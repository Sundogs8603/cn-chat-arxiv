<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#20195;&#29702;&#30340;&#26032;&#30340;&#35268;&#21010;&#22522;&#20934;TravelPlanner&#65292;&#23427;&#20851;&#27880;&#20110;&#26053;&#34892;&#35268;&#21010;&#36825;&#19968;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#32463;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35821;&#35328;&#20195;&#29702;&#20173;&#26080;&#27861;&#22788;&#29702;&#22914;&#27492;&#22797;&#26434;&#30340;&#35268;&#21010;&#20219;&#21153;&#65292;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;GPT-4&#20063;&#21482;&#33021;&#36798;&#21040;0.6%&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01622</link><description>&lt;p&gt;
TravelPlanner: &#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#20195;&#29702;&#30340;&#30495;&#23454;&#19990;&#30028;&#35268;&#21010;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TravelPlanner: A Benchmark for Real-World Planning with Language Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#20195;&#29702;&#30340;&#26032;&#30340;&#35268;&#21010;&#22522;&#20934;TravelPlanner&#65292;&#23427;&#20851;&#27880;&#20110;&#26053;&#34892;&#35268;&#21010;&#36825;&#19968;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#32463;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35821;&#35328;&#20195;&#29702;&#20173;&#26080;&#27861;&#22788;&#29702;&#22914;&#27492;&#22797;&#26434;&#30340;&#35268;&#21010;&#20219;&#21153;&#65292;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;GPT-4&#20063;&#21482;&#33021;&#36798;&#21040;0.6%&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35268;&#21010;&#36215;&#21021;&#23601;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#36861;&#27714;&#20043;&#19968;&#65292;&#20294;&#26089;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22823;&#22810;&#38598;&#20013;&#22312;&#21463;&#38480;&#29615;&#22659;&#19979;&#65292;&#22240;&#20026;&#32570;&#20047;&#36827;&#34892;&#20154;&#31867;&#27700;&#24179;&#35268;&#21010;&#25152;&#38656;&#30340;&#35768;&#22810;&#35748;&#30693;&#22522;&#30784;&#12290;&#26368;&#36817;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#35821;&#35328;&#20195;&#29702;&#23637;&#29616;&#20986;&#20102;&#24037;&#20855;&#20351;&#29992;&#21644;&#25512;&#29702;&#31561;&#26377;&#36259;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#35821;&#35328;&#20195;&#29702;&#33021;&#21542;&#22312;&#36229;&#20986;&#20808;&#21069;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33539;&#22260;&#30340;&#26356;&#22797;&#26434;&#29615;&#22659;&#20013;&#36827;&#34892;&#35268;&#21010;&#65311;&#20026;&#20102;&#25512;&#36827;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TravelPlanner&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35268;&#21010;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#26053;&#34892;&#35268;&#21010;&#36825;&#20010;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#35268;&#21010;&#22330;&#26223;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#27801;&#30418;&#29615;&#22659;&#65292;&#21508;&#31181;&#29992;&#20110;&#35775;&#38382;&#36817;400&#19975;&#20010;&#25968;&#25454;&#35760;&#24405;&#30340;&#24037;&#20855;&#65292;&#24182;&#21253;&#21547;1225&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#35268;&#21010;&#24847;&#22270;&#21644;&#21442;&#32771;&#35745;&#21010;&#12290;&#32508;&#21512;&#35780;&#20272;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#20195;&#29702;&#23578;&#19981;&#20855;&#22791;&#22788;&#29702;&#22914;&#27492;&#22797;&#26434;&#30340;&#35268;&#21010;&#20219;&#21153;&#30340;&#33021;&#21147;-&#21363;&#20351;&#26159;GPT-4&#30340;&#25104;&#21151;&#29575;&#20063;&#21482;&#26377;0.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. La
&lt;/p&gt;</description></item><item><title>StepCoder&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38271;&#24207;&#21015;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#20195;&#30721;&#23436;&#25104;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25506;&#32034;&#25361;&#25112;&#65292;&#24182;&#20351;&#29992;&#32454;&#31890;&#24230;&#20248;&#21270;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01391</link><description>&lt;p&gt;
StepCoder: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#32534;&#35793;&#22120;&#21453;&#39304;&#20013;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01391
&lt;/p&gt;
&lt;p&gt;
StepCoder&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38271;&#24207;&#21015;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#20195;&#30721;&#23436;&#25104;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25506;&#32034;&#25361;&#25112;&#65292;&#24182;&#20351;&#29992;&#32454;&#31890;&#24230;&#20248;&#21270;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#32534;&#35793;&#22120;&#21453;&#39304;&#32467;&#21512;&#36215;&#26469;&#65292;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#31354;&#38388;&#65292;&#20197;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#28385;&#36275;&#22797;&#26434;&#30340;&#20154;&#31867;&#35201;&#27714;&#65292;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#24448;&#24448;&#24456;&#38271;&#65292;&#36825;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#25104;&#20026;&#19968;&#39033;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21333;&#20803;&#27979;&#35797;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#22797;&#26434;&#20195;&#30721;&#65292;&#20351;&#29992;&#36825;&#20123;&#26410;&#25191;&#34892;&#30340;&#20195;&#30721;&#29255;&#27573;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#26159;&#26080;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StepCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;CCCS&#36890;&#36807;&#23558;&#38271;&#24207;&#21015;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#20195;&#30721;&#23436;&#25104;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25506;&#32034;&#25361;&#25112;&#65292;&#32780;FGO&#36890;&#36807;&#23631;&#34109;&#26410;&#25191;&#34892;&#30340;&#20195;&#30721;&#27573;&#26469;&#25552;&#20379;&#32454;&#31890;&#24230;&#20248;&#21270;&#65292;&#20165;&#20248;&#21270;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;APPS+&#25968;&#25454;&#38598;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#32463;&#36807;&#25163;&#21160;&#39564;&#35777;&#30830;&#20445;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ens
&lt;/p&gt;</description></item><item><title>LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01376</link><description>&lt;p&gt;
LoTR: &#20302;&#24352;&#37327;&#31209;&#26435;&#37325;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LoTR: Low Tensor Rank Weight Adaptation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01376
&lt;/p&gt;
&lt;p&gt;
LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24605;&#24819;&#25512;&#24191;&#21644;&#25193;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;Transformer&#26550;&#26500;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;LoRA&#31867;&#26041;&#27861;&#26159;&#22522;&#20110;&#26799;&#24230;&#26356;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoTR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#26041;&#27861;&#65292;&#23427;&#20197;&#24352;&#37327;&#20998;&#35299;&#30340;&#24418;&#24335;&#34920;&#31034;&#21442;&#25968;&#30340;&#26799;&#24230;&#26356;&#26032;&#12290;&#27599;&#20010;&#23618;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#37117;&#30001;&#19977;&#20010;&#30697;&#38453;&#30340;&#20056;&#31215;&#26500;&#25104;&#65292;&#32780;&#24352;&#37327;&#32467;&#26500;&#26159;&#30001;&#36825;&#20010;&#20056;&#31215;&#30340;&#24038;&#21491;&#20056;&#23376;&#22312;&#23618;&#20043;&#38388;&#20849;&#20139;&#24341;&#36215;&#30340;&#12290;&#36890;&#36807;&#23545;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#23618;&#21516;&#26102;&#21387;&#32553;&#65292;LoTR&#33021;&#22815;&#27604;LoRA&#22312;&#29305;&#21035;&#26159;&#23545;&#20110;&#28145;&#23618;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26680;&#24515;&#24352;&#37327;&#19981;&#20381;&#36182;&#20110;&#21407;&#22987;&#26435;&#37325;&#32500;&#24230;&#65292;&#21487;&#20197;&#20219;&#24847;&#32553;&#23567;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#24120;&#24265;&#20215;&#21644;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
&lt;/p&gt;</description></item><item><title>CABINET&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#26684;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#22788;&#29702;&#34920;&#26684;&#20869;&#23481;&#24182;&#29983;&#25104;&#35299;&#26512;&#35821;&#21477;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#19987;&#27880;&#20110;&#30456;&#20851;&#34920;&#26684;&#25968;&#25454;&#32780;&#25233;&#21046;&#26080;&#20851;&#20449;&#24687;&#30340;&#24178;&#25200;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01155</link><description>&lt;p&gt;
CABINET: &#34920;&#26684;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CABINET: Content Relevance based Noise Reduction for Table Question Answering
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01155
&lt;/p&gt;
&lt;p&gt;
CABINET&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#26684;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#22788;&#29702;&#34920;&#26684;&#20869;&#23481;&#24182;&#29983;&#25104;&#35299;&#26512;&#35821;&#21477;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#19987;&#27880;&#20110;&#30456;&#20851;&#34920;&#26684;&#25968;&#25454;&#32780;&#25233;&#21046;&#26080;&#20851;&#20449;&#24687;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#26684;&#29702;&#35299;&#33021;&#21147;&#36890;&#36807;&#23545;&#34920;&#26684;&#30340;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#36890;&#24120;&#65292;&#21482;&#26377;&#34920;&#26684;&#30340;&#19968;&#23567;&#37096;&#20998;&#19982;&#32473;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#30456;&#20851;&#12290;&#19981;&#30456;&#20851;&#30340;&#37096;&#20998;&#20250;&#20135;&#29983;&#22122;&#22768;&#21644;&#24178;&#25200;&#20449;&#24687;&#65292;&#23548;&#33268;LLMs&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CABINET&#65288;&#22522;&#20110;&#20869;&#23481;&#30456;&#20851;&#24615;&#30340;&#34920;&#26684;&#38382;&#31572;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#65289;- &#19968;&#20010;&#33021;&#22815;&#35753;LLMs&#19987;&#27880;&#20110;&#30456;&#20851;&#34920;&#26684;&#25968;&#25454;&#24182;&#25233;&#21046;&#26080;&#20851;&#20449;&#24687;&#30340;&#26694;&#26550;&#12290;CABINET&#21253;&#25324;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#30456;&#20851;&#24615;&#35780;&#20998;&#22120;&#65288;URS&#65289;&#65292;&#19982;&#38382;&#31572;LLM&#24046;&#24322;&#24615;&#35757;&#32451;&#65292;&#26681;&#25454;&#20854;&#19982;&#36755;&#20837;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#23545;&#34920;&#26684;&#20869;&#23481;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#21518;&#20877;&#36755;&#20837;&#38382;&#31572;LLM&#65288;QA LLM&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#36741;&#21161;&#30456;&#20851;&#24615;&#35780;&#20998;&#22120;&#65292;CABINET&#21033;&#29992;&#19968;&#20010;&#24369;&#30417;&#30563;&#27169;&#22359;&#29983;&#25104;&#19968;&#20010;&#35299;&#26512;&#35821;&#21477;&#65292;&#25551;&#36848;&#34892;&#21644;&#21015;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns r
&lt;/p&gt;</description></item><item><title>"AccentFold"&#26159;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#21475;&#38899;&#23884;&#20837;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#26469;&#25913;&#36827;ASR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#24615;&#20998;&#26512;&#35821;&#38899;&#23884;&#20837;&#65292;&#25581;&#31034;&#38750;&#27954;&#21475;&#38899;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;Ethnologue&#20808;&#21069;&#26410;&#32463;&#25551;&#36848;&#30340;&#21475;&#38899;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01152</link><description>&lt;p&gt;
"AccentFold&#65306;&#36890;&#36807;&#38646;&#26679;&#26412;ASR&#36866;&#24212;&#25506;&#32034;&#38750;&#27954;&#21475;&#38899;&#20043;&#26053;"
&lt;/p&gt;
&lt;p&gt;
AccentFold: A Journey through African Accents for Zero-Shot ASR Adaptation to Target Accents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01152
&lt;/p&gt;
&lt;p&gt;
"AccentFold"&#26159;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#21475;&#38899;&#23884;&#20837;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#26469;&#25913;&#36827;ASR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#24615;&#20998;&#26512;&#35821;&#38899;&#23884;&#20837;&#65292;&#25581;&#31034;&#38750;&#27954;&#21475;&#38899;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;Ethnologue&#20808;&#21069;&#26410;&#32463;&#25551;&#36848;&#30340;&#21475;&#38899;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#21475;&#38899;&#35821;&#38899;&#20381;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#24314;&#27169;&#25216;&#26415;&#25110;&#21019;&#24314;&#26377;&#21475;&#38899;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#65292;&#20294;&#30001;&#20110;&#38750;&#27954;&#21475;&#38899;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#30340;&#39044;&#31639;&#38480;&#21046;&#65292;&#25910;&#38598;&#21040;&#36275;&#22815;&#30340;&#25968;&#25454;&#20173;&#28982;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;"AccentFold"&#65292;&#23427;&#21033;&#29992;&#20102;&#23398;&#20064;&#21040;&#30340;&#21475;&#38899;&#23884;&#20837;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#26469;&#25913;&#36827;&#19979;&#28216;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#25105;&#20204;&#23545;&#20195;&#34920;100&#22810;&#31181;&#38750;&#27954;&#21475;&#38899;&#30340;&#35821;&#38899;&#23884;&#20837;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#21475;&#38899;&#31354;&#38388;&#20851;&#31995;&#65292;&#31361;&#26174;&#20986;&#22320;&#29702;&#21644;&#20146;&#32536;&#30456;&#20284;&#24615;&#65292;&#25429;&#25417;&#21040;&#20174;&#35821;&#38899;&#20013;&#32463;&#39564;&#24615;&#22320;&#23398;&#20064;&#21040;&#30340;&#19968;&#33268;&#30340;&#38899;&#31995;&#21644;&#24418;&#24577;&#23398;&#35268;&#24459;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;Ethnologue&#20808;&#21069;&#26410;&#32463;&#25551;&#36848;&#30340;&#21475;&#38899;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in speech recognition, accented speech remains challenging. While previous approaches have focused on modeling techniques or creating accented speech datasets, gathering sufficient data for the multitude of accents, particularly in the African context, remains impractical due to their sheer diversity and associated budget constraints. To address these challenges, we propose \textit{AccentFold}, a method that exploits spatial relationships between learned accent embeddings to improve downstream Automatic Speech Recognition (ASR). Our exploratory analysis of speech embeddings representing 100+ African accents reveals interesting spatial accent relationships highlighting geographic and genealogical similarities, capturing consistent phonological, and morphological regularities, all learned empirically from speech. Furthermore, we discover accent relationships previously uncharacterized by the Ethnologue. Through empirical evaluation, we demonstrate the effectiveness o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24418;&#24335;&#35821;&#35328;&#22312;&#22788;&#29702;&#21407;&#22987;&#36755;&#20837;&#12289;&#22788;&#29702;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#21644;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#20107;&#23454;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2212.10923</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24402;&#32435;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Inductive Reasoners
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2212.10923
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24418;&#24335;&#35821;&#35328;&#22312;&#22788;&#29702;&#21407;&#22987;&#36755;&#20837;&#12289;&#22788;&#29702;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#21644;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#20107;&#23454;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;&#30740;&#31350;&#20013;&#65292;&#24418;&#24335;&#35821;&#35328;&#34987;&#29992;&#20316;&#30693;&#35782;&#65288;&#20107;&#23454;&#21644;&#35268;&#21017;&#65289;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24418;&#24335;&#35821;&#35328;&#20250;&#32473;&#24402;&#32435;&#25512;&#29702;&#24102;&#26469;&#31995;&#32479;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#26080;&#27861;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#36825;&#26679;&#30340;&#21407;&#22987;&#36755;&#20837;&#12289;&#23545;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#25935;&#24863;&#20197;&#21450;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#33539;&#24335;&#65288;&#20219;&#21153;&#65289;&#65292;&#21363;&#20174;&#33258;&#28982;&#35821;&#35328;&#20107;&#23454;&#20013;&#24402;&#32435;&#20986;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;DEER&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;1.2k&#20010;&#35268;&#21017;-&#20107;&#23454;&#23545;&#65292;&#35268;&#21017;&#21644;&#20107;&#23454;&#20197;&#33258;&#28982;&#35821;&#35328;&#20070;&#20889;&#12290;&#36824;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;&#35780;&#20272;&#27492;&#20219;&#21153;&#30340;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;DEER&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#30340;&#34920;&#31034;&#32780;&#19981;&#26159;&#24418;&#24335;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language model
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35206;&#30422;&#21644;&#35843;&#33410;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#35206;&#30422;&#20869;&#37096;&#21644;&#19978;&#19979;&#25991;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#32499;&#32034;&#25193;&#23637;&#26102;&#38656;&#35201;&#20445;&#25345;&#32531;&#20914;&#21306;&#26469;&#20445;&#25345;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03303</link><description>&lt;p&gt;
&#27809;&#20851;&#31995;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25351;&#20196;&#35206;&#30422;&#21644;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Nevermind: Instruction Override and Moderation in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03303
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35206;&#30422;&#21644;&#35843;&#33410;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#35206;&#30422;&#20869;&#37096;&#21644;&#19978;&#19979;&#25991;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#32499;&#32034;&#25193;&#23637;&#26102;&#38656;&#35201;&#20445;&#25345;&#32531;&#20914;&#21306;&#26469;&#20445;&#25345;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;&#26368;&#27969;&#34892;&#30340;&#19987;&#26377;&#27169;&#22411;&#21644;&#19981;&#21516;&#22823;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35299;&#20915;&#22312;&#20914;&#31361;&#24773;&#20917;&#19979;&#30340;&#26126;&#30830;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#65292;&#20363;&#22914;&#35206;&#30422;&#12290;&#36825;&#20123;&#21253;&#25324;&#27169;&#22411;&#22312;&#20854;&#26435;&#37325;&#20013;&#35206;&#30422;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#35206;&#30422;&#65288;&#25110;&#35843;&#33410;&#65289;&#25552;&#31034;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#36827;&#34892;&#23436;&#20840;&#36234;&#29425;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21487;&#20197;&#25913;&#36827;&#25351;&#20196;&#36981;&#24490;&#30340;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616; - &#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#36981;&#24490;&#35206;&#30422;&#20869;&#37096;&#21644;&#19978;&#19979;&#25991;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#38750;&#24120;&#26381;&#20174;&#65292;&#29978;&#33267;&#26377;&#20123;&#36807;&#24230;&#12290;&#24403;&#36890;&#36807;&#32499;&#32034;&#25193;&#23637;&#26469;&#25193;&#23637;&#21040;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#26102;&#65292;&#38656;&#35201;&#20445;&#25345;&#19982;&#22256;&#24785;&#36793;&#32536;&#30340;&#26174;&#33879;&#32531;&#20914;&#21306;&#65292;&#20197;&#20445;&#25345;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25351;&#20196;&#36981;&#24490;&#30340;&#25913;&#21892;&#65292;&#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#30340;&#25351;&#20196;&#35206;&#30422;/&#36234;&#29425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/ja
&lt;/p&gt;</description></item><item><title>DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03300</link><description>&lt;p&gt;
DeepSeekMath: &#23558;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25512;&#21521;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03300
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#30340;&#29305;&#24615;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSeekMath 7B&#65292;&#23427;&#22312;Common Crawl&#20013;&#33719;&#21462;&#20102;120B&#20010;&#19982;&#25968;&#23398;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#20195;&#30721;&#25968;&#25454;&#26469;&#32487;&#32493;&#39044;&#35757;&#32451;DeepSeek-Coder-Base-v1.5 7B&#12290;DeepSeekMath 7B&#22312;&#31454;&#36187;&#32423;&#21035;&#30340;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;51.7%&#30340;&#20998;&#25968;&#65292;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#24037;&#20855;&#21253;&#21644;&#25237;&#31080;&#25216;&#26415;&#65292;&#25509;&#36817;&#20102;Gemini-Ultra&#21644;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;DeepSeekMath 7B&#30340;&#33258;&#19968;&#33268;&#24615;&#22312;MATH&#19978;&#30340;64&#20010;&#26679;&#26412;&#20013;&#36798;&#21040;&#20102;60.9%&#30340;&#20998;&#25968;&#12290;DeepSeekMath&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#24402;&#22240;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#36873;&#25321;&#31649;&#36947;&#20805;&#20998;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;GRPO&#65289;&#65292;&#36825;&#26159;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#21487;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03299</link><description>&lt;p&gt;
GUARD: &#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#21335;&#30340;&#21512;&#35268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#21644;&#26377;&#23475;&#22238;&#24212;&#30340;"&#36234;&#29425;"&#24050;&#32463;&#40723;&#21169;&#31038;&#21306;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#26159;&#22312;&#21457;&#24067;&#20043;&#21069;&#29992;&#36234;&#29425;&#20027;&#21160;&#27979;&#35797;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27979;&#35797;&#23558;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#36861;&#38543;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#31574;&#30053;&#19979;&#65292;&#20197;&#20154;&#31867;&#29983;&#25104;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65292;&#23558;&#22235;&#31181;&#19981;&#21516;&#35282;&#33394;&#20998;&#37197;&#32473;&#29992;&#25143;LLM&#65292;&#20197;&#20415;&#21327;&#20316;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#29616;&#26377;&#30340;&#36234;&#29425;&#65292;&#24182;&#36890;&#36807;&#21477;&#23376;&#36880;&#21477;&#36827;&#34892;&#32858;&#31867;&#39057;&#29575;&#21644;&#35821;&#20041;&#27169;&#24335;&#30340;&#21010;&#20998;&#65292;&#23558;&#23427;&#20204;&#20998;&#25104;&#19981;&#21516;&#30340;&#29420;&#31435;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#35282;&#33394;&#31995;&#32479;&#23558;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#34920;&#31034;&#23545;&#35805;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#27169;&#22411;&#26657;&#20934;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#20351;&#36739;&#23567;&#30340;&#27169;&#22411;&#20855;&#22791;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03284</link><description>&lt;p&gt;
&#20132;&#26131;&#65292;&#36824;&#26159;&#19981;&#20132;&#26131;&#65288;&#25110;&#32773;&#35841;&#30693;&#36947;&#65289;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#23545;&#35805;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#34920;&#31034;&#23545;&#35805;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#27169;&#22411;&#26657;&#20934;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#20351;&#36739;&#23567;&#30340;&#27169;&#22411;&#20855;&#22791;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#23545;&#35805;&#32773;&#32771;&#34385;&#20182;&#20154;&#30340;&#19981;&#30830;&#23450;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#24773;&#32490;&#12290;&#20294;&#21363;&#20351;&#26159;&#26368;&#20339;&#30340;&#20154;&#31867;&#23545;&#35805;&#32773;&#20063;&#26080;&#27861;&#23436;&#32654;&#22320;&#39044;&#27979;&#23545;&#35805;&#30340;&#36712;&#36857;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22810;&#22909;&#22320;&#34920;&#31034;&#23545;&#35805;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;FortUne Dial&#65292;&#36825;&#26159;&#8220;&#23545;&#35805;&#39044;&#27979;&#8221;&#20219;&#21153;&#30340;&#25193;&#23637;&#65306;&#35780;&#20272;&#19981;&#20165;&#20165;&#20197;&#20934;&#30830;&#24230;&#20026;&#26631;&#20934;&#65292;&#36824;&#37319;&#29992;&#20102;&#23545;&#19981;&#30830;&#23450;&#24615;&#25935;&#24863;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#20351;&#20010;&#21035;&#23454;&#20363;&#21487;&#20197;&#25918;&#24323;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#34920;&#31034;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#20004;&#31181;&#26041;&#24335;&#65288;&#20869;&#37096;&#20351;&#29992;&#20998;&#25968;&#21644;&#30452;&#25509;&#20351;&#29992;&#20196;&#29260;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#20004;&#31181;&#34920;&#31034;&#30340;&#26657;&#20934;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#23545;&#20843;&#20010;&#22256;&#38590;&#30340;&#35848;&#21028;&#35821;&#26009;&#24211;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#24494;&#35843;&#31574;&#30053;&#65288;&#19968;&#31181;&#20256;&#32479;&#30340;&#30417;&#30563;&#31574;&#30053;&#21644;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65289;&#21487;&#20197;&#20351;&#36739;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#26657;&#20934;&#24471;&#19978;&#19982;&#20854;&#23610;&#23544;&#30456;&#24403;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing "conversation forecasting" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their si
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.03271</link><description>&lt;p&gt;
&#24819;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#25628;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03271
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27604;&#22914;&#21307;&#23398;&#35786;&#26029;&#21644;&#25925;&#38556;&#25490;&#38500;&#65292;&#35299;&#20915;&#20219;&#21153;&#25152;&#38656;&#30340;&#20449;&#24687;&#19981;&#26159;&#21021;&#22987;&#32473;&#23450;&#30340;&#65292;&#32780;&#38656;&#35201;&#36890;&#36807;&#35810;&#38382;&#21518;&#32493;&#38382;&#39064;&#26469;&#20027;&#21160;&#23547;&#27714;&#65288;&#20363;&#22914;&#65292;&#21307;&#29983;&#21521;&#24739;&#32773;&#35810;&#38382;&#30151;&#29366;&#30340;&#26356;&#22810;&#32454;&#33410;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24605;&#24819;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;UoT&#65289;&#65292;&#19968;&#31181;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#20027;&#21160;&#25552;&#38382;&#20449;&#24687;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;UoT&#32467;&#21512;&#20102;1&#65289;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20223;&#30495;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#21487;&#33021;&#30340;&#26410;&#26469;&#22330;&#26223;&#65292;&#24182;&#20272;&#35745;&#20854;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;2&#65289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#65292;&#28608;&#21169;&#27169;&#22411;&#23547;&#27714;&#20449;&#24687;&#65307;3&#65289;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#30340;&#26041;&#24335;&#36873;&#25321;&#26368;&#20339;&#30340;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#25925;&#38556;&#25490;&#38500;&#21644;'20&#30340;&#23454;&#39564;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ISPA&#65288;&#36328;&#29289;&#31181;&#35821;&#38899;&#38899;&#26631;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#30830;&#12289;&#31616;&#27905;&#12289;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#23558;&#21160;&#29289;&#22768;&#38899;&#36716;&#24405;&#20026;&#25991;&#26412;&#12290;&#36890;&#36807;&#23558;&#21160;&#29289;&#22768;&#38899;&#34920;&#31034;&#20026;&#25991;&#26412;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#20854;&#35270;&#20026;&#19968;&#31181;&#8220;&#22806;&#35821;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#24050;&#24314;&#31435;&#30340;&#20154;&#31867;&#35821;&#35328;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#21644;&#27169;&#22411;&#65288;&#22914;&#35821;&#35328;&#27169;&#22411;&#65289;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03269</link><description>&lt;p&gt;
ISPA: &#29992;&#20110;&#36716;&#24405;&#21160;&#29289;&#22768;&#38899;&#30340;&#36328;&#29289;&#31181;&#35821;&#38899;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ISPA&#65288;&#36328;&#29289;&#31181;&#35821;&#38899;&#38899;&#26631;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#30830;&#12289;&#31616;&#27905;&#12289;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#23558;&#21160;&#29289;&#22768;&#38899;&#36716;&#24405;&#20026;&#25991;&#26412;&#12290;&#36890;&#36807;&#23558;&#21160;&#29289;&#22768;&#38899;&#34920;&#31034;&#20026;&#25991;&#26412;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#20854;&#35270;&#20026;&#19968;&#31181;&#8220;&#22806;&#35821;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#24050;&#24314;&#31435;&#30340;&#20154;&#31867;&#35821;&#35328;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#21644;&#27169;&#22411;&#65288;&#22914;&#35821;&#35328;&#27169;&#22411;&#65289;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#29983;&#29289;&#22768;&#23398;&#20381;&#36182;&#35889;&#22270;&#21644;&#36830;&#32493;&#30340;&#27599;&#24103;&#38899;&#39057;&#34920;&#31034;&#26469;&#20998;&#26512;&#21160;&#29289;&#22768;&#38899;&#65292;&#24182;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#12289;&#19982;&#35821;&#35328;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#36716;&#24405;&#20154;&#31867;&#35821;&#38899;&#22768;&#38899;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ISPA&#65288;&#36328;&#29289;&#31181;&#35821;&#38899;&#38899;&#26631;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#30830;&#12289;&#31616;&#27905;&#12289;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#23558;&#21160;&#29289;&#22768;&#38899;&#36716;&#24405;&#20026;&#25991;&#26412;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#22768;&#23398;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#36716;&#24405;&#21644;&#20998;&#31867;&#21160;&#29289;&#22768;&#38899;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#19982;&#20351;&#29992;&#36830;&#32493;&#12289;&#31264;&#23494;&#38899;&#39057;&#34920;&#31034;&#30340;&#22522;&#20934;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#21160;&#29289;&#22768;&#38899;&#34920;&#31034;&#20026;&#25991;&#26412;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#20854;&#35270;&#20026;&#19968;&#31181;&#8220;&#22806;&#35821;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#24050;&#24314;&#31435;&#30340;&#20154;&#31867;&#35821;&#35328;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#21644;&#27169;&#22411;&#65288;&#22914;&#35821;&#35328;&#27169;&#22411;&#65289;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, bioacoustics has relied on spectrograms and continuous, per-frame audio representations for the analysis of animal sounds, also serving as input to machine learning models. Meanwhile, the International Phonetic Alphabet (IPA) system has provided an interpretable, language-independent method for transcribing human speech sounds. In this paper, we introduce ISPA (Inter-Species Phonetic Alphabet), a precise, concise, and interpretable system designed for transcribing animal sounds into text. We compare acoustics-based and feature-based methods for transcribing and classifying animal sounds, demonstrating their comparable performance with baseline methods utilizing continuous, dense audio representations. By representing animal sounds with text, we effectively treat them as a "foreign language," and we show that established human language ML paradigms and models, such as language models, can be successfully applied to improve performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03268</link><description>&lt;p&gt;
&#20174;&#25512;&#29702;&#36335;&#24452;&#32858;&#21512;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#35757;&#32451;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#30340;&#20851;&#31995;&#22914;&#20309;&#20419;&#20351;&#25512;&#29702;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#21487;&#20197;&#23558;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#22312;&#39044;&#35757;&#32451;&#26102;&#36890;&#36807;&#32858;&#21512;&#38388;&#25509;&#30340;&#25512;&#29702;&#36335;&#24452;&#26469;&#24471;&#20986;&#26032;&#32467;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20010;&#35270;&#35282;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#31561;&#20851;&#38190;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#36335;&#24452;&#24418;&#24335;&#21270;&#20026;&#22312;&#30693;&#35782;/&#25512;&#29702;&#22270;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#12290;&#23545;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#24067;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30456;&#20851;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#27010;&#29575;&#30340;&#21152;&#26435;&#21644;&#26159;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21512;&#29702;&#26041;&#24335;&#12290;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#35757;&#32451;&#23545;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#38598;&#20248;&#21270;&#65288;SSO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#21487;&#36716;&#31227;&#30340;&#25216;&#33021;&#38598;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#39640;&#22870;&#21169;&#30340;&#20849;&#21516;&#23376;&#36712;&#36857;&#65292;&#29983;&#25104;&#23376;&#30446;&#26631;&#21644;&#35828;&#26126;&#65292;&#24182;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#32473;LLM&#28436;&#21592;&#65292;&#20197;&#24378;&#21270;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SSO&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#33021;&#22815;&#20248;&#21270;&#25216;&#33021;&#38598;&#65292;&#24182;&#23454;&#29616;&#19978;&#19979;&#25991;&#31574;&#30053;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.03244</link><description>&lt;p&gt;
&#25216;&#33021;&#38598;&#20248;&#21270;&#65306;&#36890;&#36807;&#21487;&#36716;&#31227;&#25216;&#33021;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#38598;&#20248;&#21270;&#65288;SSO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#21487;&#36716;&#31227;&#30340;&#25216;&#33021;&#38598;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#39640;&#22870;&#21169;&#30340;&#20849;&#21516;&#23376;&#36712;&#36857;&#65292;&#29983;&#25104;&#23376;&#30446;&#26631;&#21644;&#35828;&#26126;&#65292;&#24182;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#32473;LLM&#28436;&#21592;&#65292;&#20197;&#24378;&#21270;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SSO&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#33021;&#22815;&#20248;&#21270;&#25216;&#33021;&#38598;&#65292;&#24182;&#23454;&#29616;&#19978;&#19979;&#25991;&#31574;&#30053;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#29992;&#20110;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#29615;&#22659;&#22870;&#21169;&#20449;&#21495;&#26469;&#19981;&#26029;&#25913;&#36827;LLM&#28436;&#21592;&#30340;&#34920;&#29616;&#24182;&#19981;&#31616;&#21333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25216;&#33021;&#38598;&#20248;&#21270;&#65288;SSO&#65289;&#26469;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#21487;&#36716;&#31227;&#25216;&#33021;&#38598;&#26469;&#25552;&#39640;LLM&#28436;&#21592;&#30340;&#24615;&#33021;&#12290;SSO&#36890;&#36807;&#25552;&#21462;&#20855;&#26377;&#39640;&#22870;&#21169;&#30340;&#20849;&#21516;&#23376;&#36712;&#36857;&#24182;&#29983;&#25104;&#23376;&#30446;&#26631;&#21644;&#35828;&#26126;&#26469;&#26500;&#24314;&#25216;&#33021;&#12290;&#36825;&#20123;&#25216;&#33021;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#32473;LLM&#28436;&#21592;&#65292;&#20197;&#24378;&#21270;&#20855;&#26377;&#39640;&#22870;&#21169;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;SSO&#36890;&#36807;&#20462;&#21098;&#19981;&#20877;&#20135;&#29983;&#39640;&#22870;&#21169;&#30340;&#25216;&#33021;&#26469;&#36827;&#19968;&#27493;&#23436;&#21892;&#25216;&#33021;&#38598;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#35270;&#39057;&#28216;&#25103;NetHack&#21644;&#25991;&#26412;&#29615;&#22659;ScienceWorld&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#23637;&#31034;SSO&#20248;&#21270;&#25216;&#33021;&#38598;&#24182;&#36827;&#34892;&#19978;&#19979;&#25991;&#31574;&#30053;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;NetHack&#20219;&#21153;&#20013;&#65292;SSO&#30340;&#24615;&#33021;&#36229;&#36807;&#22522;&#20934;&#26041;&#27861;40%&#65292;&#24182;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26368;&#26032;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;JobSkape&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#32844;&#20301;&#25307;&#32856;&#20449;&#24687;&#20197;&#22686;&#24378;&#25216;&#33021;&#21305;&#37197;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20840;&#38754;&#30340;&#21512;&#25104;&#32844;&#20301;&#25307;&#32856;&#20449;&#24687;&#25968;&#25454;&#38598;SkillSkape&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#21305;&#37197;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#20197;&#21069;&#21512;&#25104;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#31163;&#32447;&#24230;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03242</link><description>&lt;p&gt;
JOBSKAPE: &#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#32844;&#20301;&#25307;&#32856;&#20449;&#24687;&#20197;&#22686;&#24378;&#25216;&#33021;&#21305;&#37197;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;JobSkape&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#32844;&#20301;&#25307;&#32856;&#20449;&#24687;&#20197;&#22686;&#24378;&#25216;&#33021;&#21305;&#37197;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20840;&#38754;&#30340;&#21512;&#25104;&#32844;&#20301;&#25307;&#32856;&#20449;&#24687;&#25968;&#25454;&#38598;SkillSkape&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#21305;&#37197;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#20197;&#21069;&#21512;&#25104;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#31163;&#32447;&#24230;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25216;&#33021;&#21305;&#37197;&#26041;&#27861;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#25110;&#30456;&#20284;&#24615;&#27169;&#22411;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20943;&#23569;&#20102;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#27880;&#37322;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#23384;&#22312;&#38480;&#21046;&#65292;&#20363;&#22914;&#27599;&#20010;&#21477;&#23376;&#21482;&#26377;&#19968;&#20010;&#25216;&#33021;&#24182;&#19988;&#36890;&#24120;&#30001;&#30701;&#21477;&#32452;&#25104;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;JobSkape&#65292;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#30340;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#22686;&#24378;&#25216;&#33021;&#21040;&#20998;&#31867;&#31995;&#32479;&#30340;&#21305;&#37197;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;SkillSkape&#65292;&#19968;&#20010;&#19987;&#38376;&#20026;&#25216;&#33021;&#21305;&#37197;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#20840;&#38754;&#24320;&#28304;&#21512;&#25104;&#32844;&#20301;&#25307;&#32856;&#20449;&#24687;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20960;&#20010;&#31163;&#32447;&#24230;&#37327;&#25351;&#26631;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#31867;&#20284;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#21305;&#37197;&#20219;&#21153;&#30340;&#22810;&#27493;&#39588;&#27969;&#31243;&#65292;&#24182;&#19982;&#24050;&#30693;&#30340;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#30340;&#19979;&#28216;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies. We outline that the downstream evaluation results on real-world 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23450;&#20041;&#26415;&#35821;&#26469;&#22686;&#24378;&#39640;&#25928;&#30340;&#25915;&#20987;&#24615;&#35328;&#35770;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23884;&#20837;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#22686;&#24378;&#20854;&#21487;&#38752;&#21644;&#39640;&#25928;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25239;&#36164;&#28304;&#31232;&#32570;&#30340;&#35757;&#32451;&#31574;&#30053;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.03221</link><description>&lt;p&gt;
"&#23450;&#20041;&#20320;&#30340;&#26415;&#35821;"&#65306;&#36890;&#36807;&#23450;&#20041;&#22686;&#24378;&#39640;&#25928;&#30340;&#25915;&#20987;&#24615;&#35328;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
"Define Your Terms" : Enhancing Efficient Offensive Speech Classification with Definition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03221
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23450;&#20041;&#26415;&#35821;&#26469;&#22686;&#24378;&#39640;&#25928;&#30340;&#25915;&#20987;&#24615;&#35328;&#35770;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23884;&#20837;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#22686;&#24378;&#20854;&#21487;&#38752;&#21644;&#39640;&#25928;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25239;&#36164;&#28304;&#31232;&#32570;&#30340;&#35757;&#32451;&#31574;&#30053;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#28192;&#36947;&#20013;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#20256;&#25773;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#20851;&#27880;&#12290;&#22810;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#22312;&#35821;&#20041;&#19978;&#30456;&#20851;&#20294;&#24494;&#22937;&#19981;&#21516;&#30340;&#25915;&#20987;&#24615;&#35328;&#35770;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#25915;&#20987;&#24615;&#35328;&#35770;&#35821;&#26009;&#24211;&#30340;&#22810;&#26679;&#24615;&#26469;&#22686;&#24378;&#20854;&#21487;&#38752;&#21644;&#39640;&#25928;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23884;&#20837;&#26550;&#26500;&#65292;&#36890;&#36807;Prototypical Network&#32467;&#21512;&#36755;&#20837;&#30340;&#26631;&#31614;&#21644;&#23450;&#20041;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;4&#20010;&#25968;&#25454;&#38598;&#19978;&#33267;&#23569;&#36798;&#21040;&#20102;&#26368;&#22823;F1-score&#30340;75%&#65292;&#21516;&#26102;&#21482;&#20351;&#29992;&#20102;&#19981;&#21040;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;10%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35828;&#26126;&#20102;&#23545;&#25239;&#36164;&#28304;&#31232;&#32570;&#30340;&#35757;&#32451;&#31574;&#30053;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The propagation of offensive content through social media channels has garnered attention of the research community. Multiple works have proposed various semantically related yet subtle distinct categories of offensive speech. In this work, we explore meta-earning approaches to leverage the diversity of offensive speech corpora to enhance their reliable and efficient detection. We propose a joint embedding architecture that incorporates the input's label and definition for classification via Prototypical Network. Our model achieves at least 75% of the maximal F1-score while using less than 10% of the available training data across 4 datasets. Our experimental findings also provide a case study of training strategies valuable to combat resource scarcity.
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#21516;&#24615;&#36136;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#32858;&#31867;&#21644;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#19968;&#20107;&#23454;&#24471;&#21040;&#20102;&#26412;&#25991;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#26377;&#25152;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03191</link><description>&lt;p&gt;
&#21516;&#24615;&#36136;&#65292;&#32858;&#31867;&#21644;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Isotropy, Clusters, and Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03191
&lt;/p&gt;
&lt;p&gt;
&#21516;&#24615;&#36136;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#32858;&#31867;&#21644;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#19968;&#20107;&#23454;&#24471;&#21040;&#20102;&#26412;&#25991;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#26377;&#25152;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#23884;&#20837;&#31354;&#38388;&#26159;&#21542;&#22343;&#21248;&#21033;&#29992;&#25152;&#26377;&#32500;&#24230;&#65288;&#21363;&#26159;&#21542;&#20855;&#26377;&#21516;&#24615;&#36136;&#65289;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#35752;&#35770;&#12290;&#26377;&#35777;&#25454;&#25903;&#25345;&#21644;&#21453;&#23545;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23454;&#26045;&#21516;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#21516;&#24615;&#36136;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#35201;&#27714;&#19982;&#32858;&#31867;&#30340;&#23384;&#22312;&#19981;&#20860;&#23481;&#65292;&#36825;&#20063;&#23545;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#20107;&#23454;&#65292;&#24182;&#29992;&#23427;&#26469;&#38416;&#26126;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>CIDAR&#26159;&#31532;&#19968;&#20010;&#30001;&#20154;&#24037;&#35780;&#23457;&#23545;&#40784;&#25991;&#21270;&#30340;&#38463;&#25289;&#20271;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#29616;&#26377;&#25351;&#20196;&#25968;&#25454;&#38598;&#23545;&#35199;&#26041;&#25991;&#21270;&#30340;&#22266;&#26377;&#20559;&#35265;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#20016;&#23500;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#38463;&#25289;&#20271;&#25991;&#21270;&#23545;&#40784;&#30340;&#30740;&#31350;&#24037;&#20316;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03177</link><description>&lt;p&gt;
CIDAR: &#38463;&#25289;&#20271;&#25991;&#30340;&#25991;&#21270;&#30456;&#20851;&#25351;&#20196;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CIDAR: Culturally Relevant Instruction Dataset For Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03177
&lt;/p&gt;
&lt;p&gt;
CIDAR&#26159;&#31532;&#19968;&#20010;&#30001;&#20154;&#24037;&#35780;&#23457;&#23545;&#40784;&#25991;&#21270;&#30340;&#38463;&#25289;&#20271;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#29616;&#26377;&#25351;&#20196;&#25968;&#25454;&#38598;&#23545;&#35199;&#26041;&#25991;&#21270;&#30340;&#22266;&#26377;&#20559;&#35265;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#20016;&#23500;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#38463;&#25289;&#20271;&#25991;&#21270;&#23545;&#40784;&#30340;&#30740;&#31350;&#24037;&#20316;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#24050;&#25104;&#20026;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#20027;&#35201;&#38754;&#21521;&#33521;&#35821;&#25110;&#32773;&#26469;&#28304;&#20110;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#23545;&#35199;&#26041;&#25991;&#21270;&#30340;&#22266;&#26377;&#20559;&#35265;&#12290;&#36825;&#31181;&#20559;&#35265;&#23545;&#38463;&#25289;&#20271;&#25991;&#31561;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#35821;&#35328;&#32467;&#26500;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#38463;&#25289;&#20271;&#25991;&#21453;&#26144;&#20102;&#38463;&#25289;&#20271;&#22320;&#21306;&#22810;&#26679;&#25991;&#21270;&#30340;&#29420;&#29305;&#35821;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;CIDAR&#65292;&#21363;&#31532;&#19968;&#20010;&#30001;&#20154;&#24037;&#35780;&#23457;&#23545;&#40784;&#25991;&#21270;&#30340;&#38463;&#25289;&#20271;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65288;https://hf.co/datasets/arbml/CIDAR&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;CIDAR&#21253;&#21547;&#20102;&#20195;&#34920;&#38463;&#25289;&#20271;&#22320;&#21306;&#30340;10000&#20010;&#25351;&#20196;&#19982;&#36755;&#20986;&#23545;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#20854;&#20182;&#27169;&#22411;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#30340;&#35843;&#20248;&#32467;&#26524;&#65292;&#35752;&#35770;&#20102;CIDAR&#30340;&#25991;&#21270;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;CIDAR&#21487;&#20197;&#24110;&#21161;&#20016;&#23500;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#38463;&#25289;&#20271;&#25991;&#21270;&#23545;&#40784;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#22312;...&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to other models fine-tuned on other datasets. Our experiments show that CIDAR can help enrich research efforts in aligning LLMs with the Arabic culture. All the code is available at 
&lt;/p&gt;</description></item><item><title>Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03173</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#65306;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Multi: Multimodal Understanding Leaderboard with Text and Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03173
&lt;/p&gt;
&lt;p&gt;
Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#24378;&#35843;&#20102;&#21521;&#23398;&#26415;&#30028;&#24341;&#20837;&#20855;&#26377;&#25361;&#25112;&#24615;&#32780;&#21448;&#30495;&#23454;&#30340;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#30340;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#65292;&#20294;Multi&#25104;&#20026;&#20102;MLLM&#30340;&#23574;&#31471;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21453;&#26144;&#20102;&#24403;&#21069;&#30495;&#23454;&#30340;&#32771;&#35797;&#39118;&#26684;&#65292;&#25552;&#20379;&#22810;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#24182;&#35201;&#27714;&#20934;&#30830;&#25110;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#65292;&#31867;&#20284;&#20110;&#29616;&#23454;&#20013;&#30340;&#23398;&#26657;&#32771;&#35797;&#12290;&#23427;&#36890;&#36807;&#21508;&#31181;&#20219;&#21153;&#25361;&#25112;MLLM&#65292;&#20174;&#20844;&#24335;&#25512;&#23548;&#21040;&#22270;&#20687;&#32454;&#33410;&#20998;&#26512;&#65292;&#20197;&#21450;&#36328;&#27169;&#24577;&#25512;&#29702;&#12290;Multi&#21253;&#25324;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#26684;&#24335;&#30340;&#22522;&#20110;&#31185;&#23398;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Multi-Elite&#65292;&#19968;&#20010;&#21253;&#21547;500&#20010;&#38382;&#39064;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;MLLM&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#20197;&#21450;Multi-Extend&#65292;&#36890;&#36807;&#36229;&#36807;4..&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#33258;&#21160;&#21270;ICD&#32534;&#30721;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#26469;&#31934;&#30830;&#20998;&#37197;ICD&#20195;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ICD&#32534;&#30721;&#20013;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#26631;&#31614;&#23884;&#20837;&#26426;&#21046;&#23545;&#27169;&#22411;&#24615;&#33021;&#20063;&#26377;&#26174;&#33879;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.03172</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#22810;&#26679;&#26631;&#31614;&#23884;&#20837;&#36827;&#34892;&#27880;&#24847;&#21147;&#30340;&#31934;&#30830;&#21644;&#33391;&#22909;&#26657;&#20934;&#30340;ICD&#20195;&#30721;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#33258;&#21160;&#21270;ICD&#32534;&#30721;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#26469;&#31934;&#30830;&#20998;&#37197;ICD&#20195;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ICD&#32534;&#30721;&#20013;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#26631;&#31614;&#23884;&#20837;&#26426;&#21046;&#23545;&#27169;&#22411;&#24615;&#33021;&#20063;&#26377;&#26174;&#33879;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD&#65289;&#24050;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24471;&#21040;&#37319;&#29992;&#65292;&#20294;&#23558;ICD&#20195;&#30721;&#25163;&#21160;&#20998;&#37197;&#32473;&#20020;&#24202;&#25991;&#26412;&#32791;&#26102;&#12289;&#23481;&#26131;&#20986;&#38169;&#19988;&#26114;&#36149;&#65292;&#36825;&#20419;&#20351;&#20102;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;ICD&#32534;&#30721;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20808;&#21069;&#30456;&#20851;&#24037;&#20316;&#30340;&#20960;&#20010;&#24605;&#24819;&#12290;&#25105;&#20204;&#29305;&#21035;&#20351;&#29992;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24182;&#19988;&#20026;&#22788;&#29702;&#20887;&#38271;&#30340;&#20020;&#24202;&#21465;&#36848;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22522;&#30784;&#32534;&#30721;&#22120;&#27169;&#22411;&#25913;&#36896;&#25104;&#20026;Longformer&#65292;&#25110;&#32773;&#23558;&#25991;&#26412;&#20998;&#25104;&#22810;&#20010;&#22359;&#24182;&#29420;&#31435;&#22788;&#29702;&#27599;&#20010;&#22359;&#30340;&#26041;&#27861;&#12290;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#34920;&#31034;&#19982;&#19968;&#20010;&#26631;&#31614;&#23884;&#20837;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#35813;&#26426;&#21046;&#25506;&#32034;&#20102;&#22810;&#26679;&#30340;ICD&#20195;&#30721;&#36817;&#20041;&#35789;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#21010;&#20998;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;ICD&#32534;&#30721;&#20013;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#26631;&#31614;&#23884;&#20837;&#23545;&#33391;&#22909;&#24615;&#33021;&#30340;&#36129;&#29486;&#20063;&#26159;&#26174;&#33879;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;...
&lt;/p&gt;
&lt;p&gt;
Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches. This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work. We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently. The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms. Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance. Our approach also 
&lt;/p&gt;</description></item><item><title>&#21516;&#24418;&#24322;&#20041;&#35789;&#25915;&#20987;&#23545;&#39532;&#26684;&#37324;&#24067;&#24773;&#24863;&#20998;&#26512;&#22120;&#36896;&#25104;&#20102;&#20005;&#37325;&#24433;&#21709;&#65292;&#23558;&#20854;&#24615;&#33021;&#20174;F1&#24471;&#20998;0.95&#38477;&#20302;&#21040;0.33&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#26088;&#22312;&#24378;&#35843;LLMs&#30340;&#24369;&#28857;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;&#30340;&#36947;&#24503;&#21644;&#36131;&#20219;&#12290;</title><link>https://arxiv.org/abs/2402.03171</link><description>&lt;p&gt;
&#39532;&#26684;&#37324;&#24067;&#24773;&#24863;&#20998;&#26512;&#22120;&#19978;&#30340;&#21516;&#24418;&#24322;&#20041;&#35789;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Homograph Attacks on Maghreb Sentiment Analyzers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03171
&lt;/p&gt;
&lt;p&gt;
&#21516;&#24418;&#24322;&#20041;&#35789;&#25915;&#20987;&#23545;&#39532;&#26684;&#37324;&#24067;&#24773;&#24863;&#20998;&#26512;&#22120;&#36896;&#25104;&#20102;&#20005;&#37325;&#24433;&#21709;&#65292;&#23558;&#20854;&#24615;&#33021;&#20174;F1&#24471;&#20998;0.95&#38477;&#20302;&#21040;0.33&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#26088;&#22312;&#24378;&#35843;LLMs&#30340;&#24369;&#28857;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;&#30340;&#36947;&#24503;&#21644;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21516;&#24418;&#24322;&#20041;&#35789;&#25915;&#20987;&#23545;&#39532;&#26684;&#37324;&#24067;&#21271;&#38750;&#22269;&#23478;&#19981;&#21516;&#38463;&#25289;&#20271;&#26041;&#35328;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#24403;&#25968;&#25454;&#20197;&#8220;&#38463;&#25289;&#20271;&#23383;&#27597;&#25340;&#38899;&#8221;&#20070;&#20889;&#26102;&#65292;&#21516;&#24418;&#24322;&#20041;&#35789;&#25915;&#20987;&#23548;&#33268;&#21464;&#21387;&#22120;&#20998;&#31867;&#24615;&#33021;&#20174;F1&#24471;&#20998;0.95&#19979;&#38477;&#21040;0.33&#65292;&#20943;&#23569;&#20102;65.3%&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20984;&#26174;LLMs&#30340;&#24369;&#28857;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;&#30340;&#36947;&#24503;&#21644;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries. Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in "Arabizi". The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#21477;&#23376;&#22256;&#38590;&#24230;&#30340;&#23450;&#20041;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#21644;&#21477;&#27861;&#22810;&#26679;&#24615;&#23545;&#21477;&#23376;&#22256;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#20998;&#31867;&#22120;&#35782;&#21035;&#20102;&#26368;&#22256;&#38590;&#30340;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2402.03163</link><description>&lt;p&gt;
&#29992;&#20110;ABSA&#20013;&#21477;&#23376;&#22256;&#38590;&#24230;&#39044;&#27979;&#30340;&#35821;&#35328;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Linguistic features for sentence difficulty prediction in ABSA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#21477;&#23376;&#22256;&#38590;&#24230;&#30340;&#23450;&#20041;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#21644;&#21477;&#27861;&#22810;&#26679;&#24615;&#23545;&#21477;&#23376;&#22256;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#20998;&#31867;&#22120;&#35782;&#21035;&#20102;&#26368;&#22256;&#38590;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#22788;&#29702;&#21477;&#23376;&#30340;&#20027;&#35266;&#24615;&#65292;&#36825;&#20123;&#21477;&#23376;&#21487;&#33021;&#34920;&#36798;&#24847;&#35265;&#21644;&#24773;&#24863;&#65292;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#21644;&#32454;&#24494;&#24046;&#21035;&#12290;&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#20010;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#36825;&#20123;&#20027;&#35266;&#20803;&#32032;&#30340;&#39046;&#22495;&#65292;&#21487;&#20197;&#24212;&#29992;&#22312;&#19981;&#21516;&#30340;&#31890;&#24230;&#32423;&#21035;&#65292;&#22914;&#25991;&#26723;&#12289;&#27573;&#33853;&#12289;&#21477;&#23376;&#25110;&#26041;&#38754;&#12290;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#20010;&#32463;&#36807;&#30740;&#31350;&#30340;&#35838;&#39064;&#65292;&#26377;&#24456;&#22810;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26469;&#35828;&#65292;&#20160;&#20040;&#22240;&#32032;&#20351;&#19968;&#20010;&#21477;&#23376;&#21464;&#24471;&#22256;&#38590;&#24182;&#27809;&#26377;&#28165;&#26224;&#30340;&#23450;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#65306;&#8220;&#31508;&#35760;&#26412;&#30005;&#33041;&#8221;&#12289;&#8220;&#39184;&#39302;&#8221;&#21644;&#8220;MTSC&#8221;&#65288;&#22810;&#30446;&#26631;&#20381;&#36182;&#24773;&#24863;&#20998;&#31867;&#65289;&#20197;&#21450;&#36825;&#19977;&#20010;&#25968;&#25454;&#38598;&#30340;&#21512;&#24182;&#29256;&#26412;&#36827;&#34892;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#21644;&#21477;&#27861;&#22810;&#26679;&#24615;&#23545;&#22256;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#26368;&#22256;&#38590;&#30340;&#21477;&#23376;&#24182;&#20998;&#26512;&#23427;&#20204;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges of natural language understanding is to deal with the subjectivity of sentences, which may express opinions and emotions that add layers of complexity and nuance. Sentiment analysis is a field that aims to extract and analyze these subjective elements from text, and it can be applied at different levels of granularity, such as document, paragraph, sentence, or aspect. Aspect-based sentiment analysis is a well-studied topic with many available data sets and models. However, there is no clear definition of what makes a sentence difficult for aspect-based sentiment analysis. In this paper, we explore this question by conducting an experiment with three data sets: "Laptops", "Restaurants", and "MTSC" (Multi-Target-dependent Sentiment Classification), and a merged version of these three datasets. We study the impact of domain diversity and syntactic diversity on difficulty. We use a combination of classifiers to identify the most difficult sentences and analyze their c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#30340;&#35270;&#35273;-&#36816;&#21160;&#26631;&#35760;&#21270;&#23558;&#35270;&#39057;&#34920;&#31034;&#20026;&#20851;&#38190;&#24103;&#21644;&#26102;&#38388;&#36816;&#21160;&#65292;&#28982;&#21518;&#20351;&#29992;&#32479;&#19968;&#29983;&#25104;&#39044;&#35757;&#32451;&#25216;&#26415;&#26469;&#29983;&#25104;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2402.03161</link><description>&lt;p&gt;
Video-LaVIT&#65306;&#32479;&#19968;&#30340;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#21450;&#35299;&#32806;&#30340;&#35270;&#35273;-&#36816;&#21160;&#26631;&#35760;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#30340;&#35270;&#35273;-&#36816;&#21160;&#26631;&#35760;&#21270;&#23558;&#35270;&#39057;&#34920;&#31034;&#20026;&#20851;&#38190;&#24103;&#21644;&#26102;&#38388;&#36816;&#21160;&#65292;&#28982;&#21518;&#20351;&#29992;&#32479;&#19968;&#29983;&#25104;&#39044;&#35757;&#32451;&#25216;&#26415;&#26469;&#29983;&#25104;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#22914;&#20309;&#23558;&#20854;&#20174;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#25193;&#23637;&#21040;&#26356;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#29616;&#23454;&#19990;&#30028;&#35270;&#39057;&#12290;&#19982;&#38745;&#24577;&#22270;&#20687;&#30456;&#27604;&#65292;&#35270;&#39057;&#22312;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21407;&#22240;&#22312;&#20110;&#38656;&#35201;&#23545;&#20854;&#26102;&#31354;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#38024;&#23545;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35270;&#39057;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#35270;&#39057;&#34920;&#31034;&#20026;&#20851;&#38190;&#24103;&#21644;&#26102;&#38388;&#36816;&#21160;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35774;&#35745;&#33391;&#22909;&#30340;&#26631;&#35760;&#22120;&#23558;&#35270;&#35273;&#21644;&#26102;&#38388;&#20449;&#24687;&#31163;&#25955;&#21270;&#20026;&#23569;&#37327;&#26631;&#35760;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;LLM&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32479;&#19968;&#29983;&#25104;&#39044;&#35757;&#32451;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20174;LLM&#29983;&#25104;&#30340;&#26631;&#35760;&#34987;&#20180;&#32454;&#24674;&#22797;&#21040;&#21407;&#22987;&#30340;&#36830;&#32493;&#20687;&#32032;&#31354;&#38388;&#65292;&#20197;&#29983;&#25104;&#21508;&#31181;&#35270;&#39057;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26082;&#33021;&#29702;&#35299;&#21448;&#33021;&#29983;&#25104;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#22312;13&#20010;&#20219;&#21153;&#19978;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#21152;&#20197;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;Hinglish&#24773;&#24863;&#20998;&#31867;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#35821;&#35328;&#36873;&#25321;&#19982;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#23588;&#20854;&#26159;&#22312;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#23384;&#22312;&#26102;&#12290;&#36825;&#23545;&#20110;&#24773;&#24863;&#20998;&#31867;&#30340;&#35299;&#37322;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03137</link><description>&lt;p&gt;
&#31038;&#20250;&#35821;&#35328;&#23398;&#20449;&#24687;&#30340;&#35299;&#37322;&#24615;: &#20197;Hinglish&#24773;&#24863;&#20998;&#31867;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03137
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;Hinglish&#24773;&#24863;&#20998;&#31867;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#35821;&#35328;&#36873;&#25321;&#19982;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#23588;&#20854;&#26159;&#22312;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#23384;&#22312;&#26102;&#12290;&#36825;&#23545;&#20110;&#24773;&#24863;&#20998;&#31867;&#30340;&#35299;&#37322;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#35821;&#35328;&#34920;&#36798;&#20855;&#26377;&#22266;&#26377;&#30340;&#29305;&#27530;&#24615;&#21644;&#20027;&#35266;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#20013;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#21542;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21644;&#36866;&#24212;&#19981;&#21516;&#35821;&#35328;&#38388;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#20173;&#28982;&#26377;&#24453;&#32771;&#39564;&#12290;&#31038;&#20250;&#35821;&#35328;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;Hinglish&#35828;&#35805;&#32773;&#22312;&#34920;&#36798;&#28040;&#26497;&#24773;&#32490;&#26102;&#36716;&#29992;&#21360;&#22320;&#35821;&#65292;&#22312;&#34920;&#36798;&#31215;&#26497;&#24773;&#32490;&#26102;&#36716;&#29992;&#33521;&#35821;&#12290;&#20026;&#20102;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#23398;&#20064;&#36825;&#20123;&#20851;&#32852;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;3&#20010;PLMs&#22312;&#19968;&#20010;Hinglish&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35821;&#35328;&#23545;&#24773;&#24863;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;LIME&#21644;&#22522;&#20110;&#35789;&#20803;&#30340;&#35821;&#35328;ID&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#30830;&#23454;&#23398;&#20064;&#21040;&#20102;&#35821;&#35328;&#36873;&#25321;&#21644;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#24403;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23384;&#22312;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#21487;&#20197;&#22686;&#24378;&#36825;&#31181;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24471;&#20986;&#32467;&#35770;&#65292;&#20351;&#29992;&#31038;&#20250;&#35821;&#35328;&#23398;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#31867;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion classification is a challenging task in NLP due to the inherent idiosyncratic and subjective nature of linguistic expression, especially with code-mixed data. Pre-trained language models (PLMs) have achieved high performance for many tasks and languages, but it remains to be seen whether these models learn and are robust to the differences in emotional expression across languages. Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions. To understand if language models can learn these associations, we study the effect of language on emotion prediction across 3 PLMs on a Hinglish emotion classification dataset. Using LIME and token level language ID, we find that models do learn these associations between language choice and emotional expression. Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce. We also concl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20013;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03131</link><description>&lt;p&gt;
&#29992;&#20110;&#36328;&#35821;&#35328;&#26631;&#31614;&#25237;&#24433;&#30340;&#32422;&#26463;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Constrained Decoding for Cross-lingual Label Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20013;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;LLM&#36827;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#23545;&#21333;&#35789;&#21644;&#30701;&#35821;&#36827;&#34892;&#32454;&#31890;&#24230;&#39044;&#27979;&#30340;NLP&#20219;&#21153;&#20013;&#65292;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#36828;&#36828;&#33853;&#21518;&#20110;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#21033;&#29992;&#32763;&#35793;&#21644;&#26631;&#31614;&#25237;&#24433;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#20307;&#26469;&#35828;(1)&#23558;&#21487;&#29992;&#30340;&#20197;&#21450;&#24102;&#26377;&#40644;&#37329;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;(&#20363;&#22914;&#33521;&#35821;)&#32763;&#35793;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#21644;/&#25110;(2)&#23558;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#27979;&#35797;&#25968;&#25454;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#65292;&#28982;&#21518;&#23558;&#39044;&#27979;&#30340;&#36328;&#24230;&#32423;&#21035;&#26631;&#31614;&#25237;&#23556;&#22238;&#21407;&#22987;&#27979;&#35797;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#26631;&#31614;&#25237;&#24433;&#26041;&#27861;&#30001;&#20110;&#22312;&#36755;&#20837;&#21040;&#32763;&#35793;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#27880;&#20837;&#20102;&#39069;&#22806;&#26631;&#35760;&#65292;&#23548;&#33268;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#30340;&#25552;&#31034;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#21644;&#29983;&#25104;&#21512;&#25104;&#36793;&#30028;&#24773;&#20917;&#25968;&#25454;&#26469;&#25913;&#36827;&#25552;&#31034;&#24037;&#31243;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03099</link><description>&lt;p&gt;
&#22522;&#20110;&#24847;&#22270;&#30340;&#25552;&#31034;&#26657;&#20934;&#65306;&#29992;&#21512;&#25104;&#36793;&#30028;&#24773;&#20917;&#22686;&#24378;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#30340;&#25552;&#31034;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#21644;&#29983;&#25104;&#21512;&#25104;&#36793;&#30028;&#24773;&#20917;&#25968;&#25454;&#26469;&#25913;&#36827;&#25552;&#31034;&#24037;&#31243;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#39640;&#24230;&#25935;&#24863;&#24615;&#21644;&#25991;&#26412;&#20219;&#21153;&#25351;&#20196;&#30340;&#22266;&#26377;&#27495;&#20041;&#65292;&#25552;&#31034;&#24037;&#31243;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#20010;&#21253;&#21547;&#19978;&#27425;&#35797;&#39564;&#32467;&#26524;&#30340;&#20803;&#25552;&#31034;&#24182;&#25552;&#20986;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;LLMs&#33258;&#21160;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#26159;&#22256;&#38590;&#19988;&#26114;&#36149;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#20351;&#29992;&#26657;&#20934;&#36807;&#31243;&#26469;&#36845;&#20195;&#22320;&#20248;&#21270;&#19982;&#29992;&#25143;&#24847;&#22270;&#30456;&#31526;&#30340;&#25552;&#31034;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#31995;&#32479;&#32852;&#21512;&#29983;&#25104;&#36793;&#30028;&#29992;&#20363;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BERTopic&#22312;&#22622;&#23572;&#32500;&#20122;&#35821;&#30701;&#25991;&#26412;&#20013;&#30340;&#39318;&#27425;&#24212;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;BERTopic&#21487;&#20197;&#20135;&#29983;&#20016;&#23500;&#30340;&#20027;&#39064;&#65292;&#21363;&#20351;&#26159;&#37096;&#20998;&#39044;&#22788;&#29702;&#30340;&#25991;&#26412;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;BERTopic&#22312;&#20027;&#39064;&#25968;&#37327;&#19981;&#21463;&#38480;&#21046;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#20449;&#24687;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03067</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;Transformer&#21644;BERTopic&#29992;&#20110;&#30701;&#25991;&#26412;&#20027;&#39064;&#24314;&#27169;&#65306;&#22622;&#23572;&#32500;&#20122;&#35821;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BERTopic&#22312;&#22622;&#23572;&#32500;&#20122;&#35821;&#30701;&#25991;&#26412;&#20013;&#30340;&#39318;&#27425;&#24212;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;BERTopic&#21487;&#20197;&#20135;&#29983;&#20016;&#23500;&#30340;&#20027;&#39064;&#65292;&#21363;&#20351;&#26159;&#37096;&#20998;&#39044;&#22788;&#29702;&#30340;&#25991;&#26412;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;BERTopic&#22312;&#20027;&#39064;&#25968;&#37327;&#19981;&#21463;&#38480;&#21046;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#20449;&#24687;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BERTopic&#36825;&#31181;&#26368;&#20808;&#36827;&#30340;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#22312;&#20855;&#26377;&#20016;&#23500;&#24418;&#24577;&#35821;&#35328;&#30340;&#30701;&#25991;&#26412;&#20013;&#30340;&#39318;&#27425;&#24212;&#29992;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22810;&#35821;&#35328;&#23884;&#20837;&#27169;&#22411;&#20197;&#21450;&#20004;&#31181;&#25991;&#26412;&#39044;&#22788;&#29702;&#27700;&#24179;&#65288;&#37096;&#20998;&#21644;&#23436;&#20840;&#65289;&#23545;&#22622;&#23572;&#32500;&#20122;&#35821;&#30340;&#37096;&#20998;&#39044;&#22788;&#29702;&#30340;&#30701;&#25991;&#26412;&#36827;&#34892;&#20102;BERTopic&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#19982;LDA&#21644;NMF&#22312;&#23436;&#20840;&#39044;&#22788;&#29702;&#25991;&#26412;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#19968;&#20010;&#34920;&#36798;&#23545;COVID-19&#30123;&#33495;&#30097;&#34385;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21442;&#25968;&#35774;&#32622;&#65292;BERTopic&#21363;&#20351;&#24212;&#29992;&#20110;&#37096;&#20998;&#39044;&#22788;&#29702;&#30340;&#30701;&#25991;&#26412;&#65292;&#20063;&#33021;&#20135;&#29983;&#20449;&#24687;&#20016;&#23500;&#30340;&#20027;&#39064;&#12290;&#24403;&#22312;&#20004;&#31181;&#39044;&#22788;&#29702;&#22330;&#26223;&#19979;&#24212;&#29992;&#30456;&#21516;&#30340;&#21442;&#25968;&#26102;&#65292;&#37096;&#20998;&#39044;&#22788;&#29702;&#25991;&#26412;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;&#19982;LDA&#21644;NMF&#30456;&#27604;&#65292;&#20174;&#20851;&#38190;&#35789;&#26469;&#30475;&#65292;BERTopic&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#20027;&#39064;&#65292;&#24182;&#22312;&#20027;&#39064;&#25968;&#37327;&#19981;&#21463;&#38480;&#21046;&#26102;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of the first application of BERTopic, a state-of-the-art topic modeling technique, to short text written in a morphologi-cally rich language. We applied BERTopic with three multilingual embed-ding models on two levels of text preprocessing (partial and full) to evalu-ate its performance on partially preprocessed short text in Serbian. We also compared it to LDA and NMF on fully preprocessed text. The experiments were conducted on a dataset of tweets expressing hesitancy toward COVID-19 vaccination. Our results show that with adequate parameter setting, BERTopic can yield informative topics even when applied to partially pre-processed short text. When the same parameters are applied in both prepro-cessing scenarios, the performance drop on partially preprocessed text is minimal. Compared to LDA and NMF, judging by the keywords, BERTopic offers more informative topics and gives novel insights when the number of topics is not limited. The findings of this p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#39532;&#26469;&#35199;&#20122;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;Llama2&#21644;Mistral&#27169;&#22411;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;RAG&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.03053</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#39532;&#26469;&#35199;&#20122;&#23884;&#20837;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#39532;&#26469;&#35199;&#20122;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;Llama2&#21644;Mistral&#27169;&#22411;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;RAG&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;Llama2&#21644;Mistral&#20004;&#31181;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#32034;&#65292;&#24182;&#22312;&#28041;&#21450;&#36127;&#27491;&#32452;&#23545;&#30340;&#23884;&#20837;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#19987;&#38376;&#38024;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#26816;&#32034;&#36741;&#21161;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#27169;&#22411;&#12290;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;6&#20159;&#21442;&#25968;Llama2&#27169;&#22411;&#22312;b.cari.com.my&#12289;c.cari.com.my&#12289;&#39532;&#26469;&#35199;&#20122;&#26032;&#38395;&#21644;&#39532;&#26469;&#35199;&#20122;Twitter&#27979;&#35797;&#38598;&#30340;&#25152;&#26377;recall@k&#25351;&#26631;&#19978;&#37117;&#20248;&#20110;OpenAI&#30340;text-embedding-ada-002&#12290;&#22312;RAG&#27169;&#22411;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39532;&#26469;&#35199;&#20122;&#29615;&#22659;&#20013;&#19982;OpenAI&#30340;text-embedding-ada-002&#30456;&#31454;&#20105;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;20&#20159;&#21442;&#25968;Llama2&#27169;&#22411;&#22312;&#8220;Melayu&#8221;&#20851;&#38190;&#35789;&#30740;&#31350;&#35770;&#25991;&#25968;&#25454;&#38598;&#30340;Recall@5&#12289;Recall@10&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#34920;&#29616;&#65292;&#24182;&#22312;lom.agc.gov.my&#25968;&#25454;&#38598;&#30340;Recall@3&#12289;Recall@5&#21644;Recall@10&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#24494;&#35843;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;RAG&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a comprehensive exploration of finetuning Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs. We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG).   For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets.   In the realm of RAG models, our approach proves competitive with OpenAI text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the "Melayu" keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset.   These findings underscore the effectiveness of our finetuning strategy and highlight the performance gains in both Semantic Similarity and RAG tasks.   All 
&lt;/p&gt;</description></item><item><title>&#23612;&#27850;&#23572;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#30740;&#31350;&#29616;&#29366;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21457;&#29616;&#23612;&#27850;&#23572;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#22768;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#36824;&#38656;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.03050</link><description>&lt;p&gt;
&#23612;&#27850;&#23572;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#29616;&#29366;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of the Current State-of-the-Art in Nepali Automatic Speech Recognition Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03050
&lt;/p&gt;
&lt;p&gt;
&#23612;&#27850;&#23572;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#30740;&#31350;&#29616;&#29366;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21457;&#29616;&#23612;&#27850;&#23572;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#22768;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#36824;&#38656;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23612;&#27850;&#23572;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#26412;&#27425;&#35843;&#26597;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#36804;&#20170;&#20026;&#27490;&#23436;&#25104;&#30340;&#23612;&#27850;&#23572;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24037;&#20316;&#36827;&#34892;&#20840;&#38754;&#22238;&#39038;&#65292;&#25506;&#32034;&#20351;&#29992;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#25152;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#24182;&#32771;&#34385;&#23454;&#26045;&#23612;&#27850;&#23572;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26102;&#36935;&#21040;&#30340;&#38556;&#30861;&#12290;&#19982;&#20840;&#29699;&#19981;&#26029;&#22686;&#38271;&#30340;&#35821;&#38899;&#35782;&#21035;&#30456;&#20851;&#30740;&#31350;&#30340;&#36235;&#21183;&#30456;&#19968;&#33268;&#65292;&#23612;&#27850;&#23572;&#19982;ASR&#30456;&#20851;&#30340;&#39033;&#30446;&#25968;&#37327;&#20063;&#22312;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#19982;&#25317;&#26377;&#20016;&#23500;&#36164;&#28304;&#30340;&#35821;&#35328;&#30456;&#27604;&#65292;&#23612;&#27850;&#23572;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#22768;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#20197;&#21450;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we examine the research conducted in the field of Nepali Automatic Speech Recognition (ASR). The primary objective of this survey is to conduct a comprehensive review of the works on Nepali Automatic Speech Recognition Systems completed to date, explore the different datasets used, examine the technology utilized, and take account of the obstacles encountered in implementing the Nepali ASR system. In tandem with the global trends of ever-increasing research on speech recognition based research, the number of Nepalese ASR-related projects are also growing. Nevertheless, the investigation of language and acoustic models of the Nepali language has not received adequate attention compared to languages that possess ample resources. In this context, we provide a framework as well as directions for future investigations.
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#22312;&#25991;&#26412;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23558;&#30456;&#20284;&#24230;&#24046;&#24322;&#21644;&#29420;&#29305;&#24615;&#65288;SIDU&#65289;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26412;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#39044;&#27979;&#20851;&#38190;&#30340;&#26377;&#19978;&#19979;&#25991;&#24847;&#20041;&#30340;&#25991;&#26412;&#20803;&#32032;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#19977;&#23618;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;XAI&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03043</link><description>&lt;p&gt;
SIDU-TXT: &#19968;&#31181;&#20855;&#26377;&#25972;&#20307;&#35780;&#20272;&#26041;&#27861;&#30340;NLP&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#22312;&#25991;&#26412;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23558;&#30456;&#20284;&#24230;&#24046;&#24322;&#21644;&#29420;&#29305;&#24615;&#65288;SIDU&#65289;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26412;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#39044;&#27979;&#20851;&#38190;&#30340;&#26377;&#19978;&#19979;&#25991;&#24847;&#20041;&#30340;&#25991;&#26412;&#20803;&#32032;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#19977;&#23618;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;XAI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26377;&#21161;&#20110;&#35299;&#35835;&#8220;&#40657;&#30418;&#8221;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#20027;&#35201;&#29992;&#20110;&#22270;&#20687;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#20294;&#35299;&#37322;&#24615;&#22312;&#25991;&#26412;&#39046;&#22495;&#20013;&#30340;&#25506;&#32034;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;XAI&#26041;&#27861;&#22312;&#25991;&#26412;&#39046;&#22495;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#34987;&#35748;&#20026;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#20013;&#23450;&#20301;&#25972;&#20010;&#26174;&#33879;&#21306;&#22495;&#33021;&#21147;&#20248;&#36234;&#30340;XAI&#26041;&#27861;&#8212;&#8212;&#30456;&#20284;&#24230;&#24046;&#24322;&#21644;&#29420;&#29305;&#24615;&#65288;SIDU&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25991;&#26412;&#25968;&#25454;&#12290;&#25193;&#23637;&#26041;&#27861;SIDU-TXT&#21033;&#29992;&#26469;&#33258;&#8220;&#40657;&#30418;&#8221;&#27169;&#22411;&#30340;&#29305;&#24449;&#28608;&#27963;&#22270;&#29983;&#25104;&#28909;&#21147;&#22270;&#65292;&#20197;&#35789;&#20026;&#21333;&#20301;&#25552;&#20379;&#35299;&#37322;&#65292;&#31361;&#20986;&#26174;&#31034;&#23545;&#27169;&#22411;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#26377;&#19978;&#19979;&#25991;&#24847;&#20041;&#30340;&#25991;&#26412;&#20803;&#32032;&#12290;&#37492;&#20110;&#36824;&#27809;&#26377;&#32479;&#19968;&#30340;XAI&#35780;&#20272;&#26631;&#20934;&#65292;&#26412;&#30740;&#31350;&#24212;&#29992;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#19977;&#23618;&#35780;&#20272;&#26694;&#26550;&#65306;&#21151;&#33021;&#22522;&#30784;&#12289;&#20154;&#31867;&#22522;&#30784;&#21644;&#24212;&#29992;&#22522;&#30784;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) aids in deciphering 'black-box' models. While several methods have been proposed and evaluated primarily in the image domain, the exploration of explainability in the text domain remains a growing research area. In this paper, we delve into the applicability of XAI methods for the text domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU) XAI method, recognized for its superior capability in localizing entire salient regions in image-based classification is extended to textual data. The extended method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to generate heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements crucial for model predictions. Given the absence of a unified standard for assessing XAI methods, this study applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;20&#31181;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;ACSESS&#65289;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03038</link><description>&lt;p&gt;
&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automatic Combination of Sample Selection Strategies for Few-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;20&#31181;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;ACSESS&#65289;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#22914;&#20803;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#38480;&#26679;&#26412;&#25968;&#37327;&#23545;&#25972;&#20307;&#25104;&#21151;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#65292;&#20294;&#23427;&#20204;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#21313;&#20998;&#26126;&#30830;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#21482;&#34987;&#22312;&#20856;&#22411;&#30340;&#30417;&#30563;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;8&#20010;&#22270;&#20687;&#21644;6&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;5&#31181;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#24443;&#24213;&#30740;&#31350;&#20102;20&#31181;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#32452;&#21512;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;ACSESS&#65289;&#65292;&#23427;&#20805;&#20998;&#21033;&#29992;&#20102;&#20010;&#20307;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#20114;&#34917;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20010;&#20307;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#21450;&#26368;&#36817;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25903;&#25345;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;UniMix&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03009</link><description>&lt;p&gt;
UniMem&#65306;&#36808;&#21521;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
UniMem: Towards a Unified View of Long-Context Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;UniMix&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#26159;&#38480;&#21046;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#33021;&#21147;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#33268;&#21147;&#20110;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#26159;&#23396;&#31435;&#22320;&#24320;&#21457;&#30340;&#65292;&#32570;&#20047;&#23545;&#23427;&#20204;&#30340;&#20248;&#28857;&#30340;&#31995;&#32479;&#20998;&#26512;&#21644;&#25972;&#21512;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20174;LLM&#30340;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#12290; UniMem&#30340;&#29305;&#28857;&#26159;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#20869;&#23384;&#31649;&#29702;&#65292;&#20869;&#23384;&#20889;&#20837;&#65292;&#20869;&#23384;&#35835;&#21462;&#21644;&#20869;&#23384;&#27880;&#20837;&#65292;&#20026;&#20102;&#29702;&#35299;&#21508;&#31181;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#25552;&#20379;&#20102;&#31995;&#32479;&#29702;&#35770;&#12290;&#25105;&#20204;&#22522;&#20110;UniMem&#37325;&#26032;&#21046;&#23450;&#20102;16&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;Transformer-XL&#65292;&#35760;&#24518;&#21270;Transformer&#65292;RMT&#21644;Longformer&#20013;&#30340;&#22235;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#23427;&#20204;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#20248;&#21183;&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniMix&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-context processing is a critical ability that constrains the applicability of large language models. Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs. UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an inn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#65292;&#20174;&#32780;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02992</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Decoding-time Realignment of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#65292;&#20174;&#32780;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#23545;&#20110;&#20943;&#23569;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#21644;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#22312;&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#21644;&#40723;&#21169;&#20445;&#25345;&#19982;&#26410;&#23545;&#40784;&#27169;&#22411;&#25509;&#36817;&#30340;&#25509;&#36817;&#24615;&#35268;&#21017;&#39033;&#20043;&#38388;&#36827;&#34892;&#20248;&#21270;&#30340;&#26435;&#34913;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#35268;&#21017;&#21270;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#65306;&#35268;&#21017;&#21270;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#22870;&#21169;&#27450;&#39575;&#32780;&#38477;&#20302;&#27169;&#22411;&#33021;&#21147;&#65292;&#32780;&#36807;&#24230;&#35268;&#21017;&#21270;&#21017;&#38459;&#30861;&#23545;&#40784;&#12290;&#20256;&#32479;&#26041;&#27861;&#25214;&#21040;&#26368;&#20339;&#35268;&#21017;&#21270;&#27700;&#24179;&#38656;&#35201;&#20351;&#29992;&#19981;&#21516;&#35268;&#21017;&#21270;&#24378;&#24230;&#37325;&#26032;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#26469;&#35828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#12290;DeRa&#21487;&#20197;&#23545;&#23545;&#40784;&#27169;&#22411;&#30340;&#31243;&#24230;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02987</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Conversation Reconstruction Attack Against GPT Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20854;&#20013; GPT &#31995;&#21015;&#27169;&#22411;&#20195;&#34920;&#30528;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25104;&#26524;&#12290;&#20026;&#20102;&#20248;&#21270;&#20219;&#21153;&#25191;&#34892;&#65292;&#29992;&#25143;&#32463;&#24120;&#19982;&#25176;&#31649;&#22312;&#20113;&#29615;&#22659;&#20013;&#30340; GPT &#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#36825;&#20123;&#22810;&#36718;&#23545;&#35805;&#24448;&#24448;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#65292;&#38656;&#35201;&#22312;&#20113;&#20013;&#36827;&#34892;&#20256;&#36755;&#21644;&#23384;&#20648;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25805;&#20316;&#27169;&#24335;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#25915;&#20987;&#38754;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#29305;&#23450;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#24403; GPT &#27169;&#22411;&#36973;&#21463;&#35813;&#25915;&#20987;&#26102;&#23545;&#35805;&#20013;&#22266;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#35814;&#23613;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;GPT-4 &#23545;&#20110;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#32423;&#25915;&#20987;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#37325;&#26500;&#20197;&#21069;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#32467;&#26500;&#23545;&#25991;&#26412;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#32467;&#26500;&#20449;&#24687;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#26377;&#24456;&#22823;&#30340;&#30410;&#22788;&#65292;&#20294;&#20165;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.02975</link><description>&lt;p&gt;
&#23558;&#19978;&#19979;&#25991;&#25918;&#20837;&#19978;&#19979;&#25991;&#20013;&#65306;&#35752;&#35770;&#32467;&#26500;&#23545;&#25991;&#26412;&#20998;&#31867;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Putting Context in Context: the Impact of Discussion Structure on Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#32467;&#26500;&#23545;&#25991;&#26412;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#32467;&#26500;&#20449;&#24687;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#26377;&#24456;&#22823;&#30340;&#30410;&#22788;&#65292;&#20294;&#20165;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#38598;&#20013;&#22312;&#35201;&#20998;&#31867;&#30340;&#20869;&#23481;&#19978;&#12290;&#21363;&#20351;&#26159;&#22522;&#20110;&#22312;&#32447;&#35752;&#35770;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#20063;&#20250;&#24573;&#30053;&#19978;&#19979;&#25991;&#26041;&#38754;&#65288;&#21253;&#25324;&#35821;&#35328;&#21644;&#39069;&#22806;&#30340;&#35821;&#35328;&#20043;&#22806;&#30340;&#26041;&#38754;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#36873;&#23450;&#20803;&#32032;&#30340;&#22810;&#26041;&#21644;&#22810;&#36718;&#19978;&#19979;&#25991;&#24615;&#36136;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;&#31435;&#22330;&#26816;&#27979;&#30340;&#23454;&#39564;&#65292;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#23427;&#20204;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20256;&#20837;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#31867;&#22411;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#35821;&#35328;&#12289;&#32467;&#26500;&#21644;&#26102;&#38388;&#65289;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20197;&#31526;&#21512;&#38544;&#31169;&#35201;&#27714;&#30340;&#26041;&#24335;&#20998;&#26512;&#20102;&#23616;&#37096;&#35752;&#35770;&#32593;&#32476;&#30340;&#25299;&#25169;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#23545;&#25991;&#26412;&#20998;&#31867;&#26377;&#24456;&#22823;&#30340;&#30410;&#22788;&#65292;&#20294;&#21482;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#25165;&#26174;&#33879;&#65288;&#20363;&#22914;&#65292;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#35752;&#35770;&#38142;&#30340;&#22797;&#26434;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current text classification approaches usually focus on the content to be classified. Contextual aspects (both linguistic and extra-linguistic) are usually neglected, even in tasks based on online discussions. Still in many cases the multi-party and multi-turn nature of the context from which these elements are selected can be fruitfully exploited. In this work, we propose a series of experiments on a large dataset for stance detection in English, in which we evaluate the contribution of different types of contextual information, i.e. linguistic, structural and temporal, by feeding them as natural language input into a transformer-based model. We also experiment with different amounts of training data and analyse the topology of local discussion networks in a privacy-compliant way. Results show that structural information can be highly beneficial to text classification but only under certain circumstances (e.g. depending on the amount of training data and on discussion chain complexity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21516;&#28304;&#26816;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#30417;&#30563;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#33021;&#38543;&#30528;&#36827;&#19968;&#27493;&#22686;&#21152;&#30417;&#30563;&#32780;&#31283;&#23450;&#25913;&#36827;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25509;&#21463;&#22810;&#20010;&#24207;&#21015;&#23545;&#40784;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20855;&#26377;&#31471;&#21040;&#31471;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02926</link><description>&lt;p&gt;
&#29992;&#20855;&#26377;&#21516;&#28304;&#36716;&#25442;&#22120;&#30340;&#30417;&#30563;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#33258;&#21160;&#21516;&#28304;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21516;&#28304;&#26816;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#30417;&#30563;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#33021;&#38543;&#30528;&#36827;&#19968;&#27493;&#22686;&#21152;&#30417;&#30563;&#32780;&#31283;&#23450;&#25913;&#36827;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25509;&#21463;&#22810;&#20010;&#24207;&#21015;&#23545;&#40784;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20855;&#26377;&#31471;&#21040;&#31471;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21382;&#21490;&#35821;&#35328;&#23398;&#20013;&#65292;&#35782;&#21035;&#30456;&#20851;&#35821;&#35328;&#20013;&#30340;&#21516;&#28304;&#35789;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#12290;&#33258;&#21160;&#21516;&#28304;&#35782;&#21035;&#23545;&#20110;&#35782;&#21035;&#38899;&#20301;&#23545;&#24212;&#20851;&#31995;&#12289;&#21407;&#22987;&#35821;&#35328;&#37325;&#24314;&#12289;&#35821;&#31995;&#20998;&#31867;&#31561;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#37117;&#26377;&#24110;&#21161;&#12290;&#20197;&#24448;&#21516;&#28304;&#35782;&#21035;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22823;&#22810;&#22522;&#20110;&#36328;&#22810;&#35821;&#35328;&#35789;&#34920;&#35745;&#31639;&#30340;&#38899;&#32032;&#20998;&#24067;&#65292;&#23545;&#23450;&#20041;&#21516;&#28304;&#31751;&#20043;&#38388;&#38142;&#25509;&#30340;&#21516;&#28304;&#26631;&#31614;&#30340;&#20351;&#29992;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#35745;&#31639;&#29983;&#29289;&#23398;&#20026;&#28789;&#24863;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#21516;&#28304;&#26816;&#27979;&#12290;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#30417;&#30563;&#19979;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#36827;&#19968;&#27493;&#22686;&#21152;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#21033;&#29992;&#26631;&#35760;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25509;&#21463;&#22810;&#20010;&#24207;&#21015;&#23545;&#40784;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20855;&#26377;&#31471;&#21040;&#31471;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification of cognates across related languages is one of the primary problems in historical linguistics. Automated cognate identification is helpful for several downstream tasks including identifying sound correspondences, proto-language reconstruction, phylogenetic classification, etc. Previous state-of-the-art methods for cognate identification are mostly based on distributions of phonemes computed across multilingual wordlists and make little use of the cognacy labels that define links among cognate clusters. In this paper, we present a transformer-based architecture inspired by computational biology for the task of automated cognate detection. Beyond a certain amount of supervision, this method performs better than the existing methods, and shows steady improvement with further increase in supervision, thereby proving the efficacy of utilizing the labeled information. We also demonstrate that accepting multiple sequence alignments as input and having an end-to-end architecture
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#38388;&#30456;&#20114;&#21487;&#29702;&#35299;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21028;&#21035;&#23398;&#20064;&#22120;&#21644;&#22810;&#35821;&#20041;&#21521;&#37327;&#31561;&#26041;&#27861;&#65292;&#23545;&#24503;&#35821;&#12289;&#33655;&#20848;&#35821;&#21644;&#33521;&#35821;&#36825;&#19977;&#31181;&#23494;&#20999;&#30456;&#20851;&#30340;&#26085;&#32819;&#26364;&#35821;&#35328;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#19982;&#35789;&#24418;&#20462;&#21098;&#21644;&#35821;&#35328;&#23545;&#26377;&#20851;&#12290;&#36825;&#31181;&#22810;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#20026;&#33258;&#21160;&#27979;&#35797;&#35821;&#35328;&#38388;&#30456;&#20114;&#21487;&#29702;&#35299;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#23398;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.02915</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#38388;&#30340;&#30456;&#20114;&#21487;&#29702;&#35299;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Computational Model for the Assessment of Mutual Intelligibility Among Closely Related Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#38388;&#30456;&#20114;&#21487;&#29702;&#35299;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21028;&#21035;&#23398;&#20064;&#22120;&#21644;&#22810;&#35821;&#20041;&#21521;&#37327;&#31561;&#26041;&#27861;&#65292;&#23545;&#24503;&#35821;&#12289;&#33655;&#20848;&#35821;&#21644;&#33521;&#35821;&#36825;&#19977;&#31181;&#23494;&#20999;&#30456;&#20851;&#30340;&#26085;&#32819;&#26364;&#35821;&#35328;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#19982;&#35789;&#24418;&#20462;&#21098;&#21644;&#35821;&#35328;&#23545;&#26377;&#20851;&#12290;&#36825;&#31181;&#22810;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#20026;&#33258;&#21160;&#27979;&#35797;&#35821;&#35328;&#38388;&#30456;&#20114;&#21487;&#29702;&#35299;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#23398;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#34920;&#29616;&#20986;&#35821;&#35328;&#19978;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#24471;&#19968;&#31181;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#33021;&#22815;&#29702;&#35299;&#21478;&#19968;&#31181;&#35821;&#35328;&#30340;&#35828;&#35805;&#32773;&#65292;&#32780;&#19981;&#38656;&#35201;&#20027;&#21160;&#23398;&#20064;&#12290;&#30456;&#20114;&#21487;&#29702;&#35299;&#31243;&#24230;&#22240;&#20010;&#20307;&#32780;&#24322;&#65292;&#36890;&#24120;&#22312;&#24515;&#29702;&#35821;&#35328;&#23398;&#23454;&#39564;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#20026;&#20102;&#35745;&#31639;&#30740;&#31350;&#30456;&#20114;&#21487;&#29702;&#35299;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36741;&#21161;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#21028;&#21035;&#23398;&#20064;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#36924;&#36817;&#20154;&#31867;&#23398;&#20064;&#35821;&#35328;&#30340;&#35748;&#30693;&#36807;&#31243;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#22810;&#35821;&#20041;&#21521;&#37327;&#21644;&#22810;&#22768;&#38899;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#24503;&#35821;&#12289;&#33655;&#20848;&#35821;&#21644;&#33521;&#35821;&#36825;&#19977;&#31181;&#23494;&#20999;&#30456;&#20851;&#30340;&#26085;&#32819;&#26364;&#35821;&#35328;&#30340;&#21516;&#28304;&#25968;&#25454;&#36827;&#34892;&#20102;&#27169;&#22411;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#29702;&#35299;&#20934;&#30830;&#24230;&#21462;&#20915;&#20110;1&#65289;&#35789;&#24418;&#30340;&#33258;&#21160;&#20462;&#21098;&#21644;2&#65289;&#36827;&#34892;&#29702;&#35299;&#27979;&#35797;&#30340;&#35821;&#35328;&#23545;&#12290;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#19981;&#20165;&#25552;&#20379;&#20102;&#33258;&#21160;&#27979;&#35797;&#35821;&#35328;&#38388;&#30456;&#20114;&#21487;&#29702;&#35299;&#24615;&#30340;&#26032;&#26041;&#27861;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Closely related languages show linguistic similarities that allow speakers of one language to understand speakers of another language without having actively learned it. Mutual intelligibility varies in degree and is typically tested in psycholinguistic experiments. To study mutual intelligibility computationally, we propose a computer-assisted method using the Linear Discriminative Learner, a computational model developed to approximate the cognitive processes by which humans learn languages, which we expand with multilingual semantic vectors and multilingual sound classes. We test the model on cognate data from German, Dutch, and English, three closely related Germanic languages. We find that our model's comprehension accuracy depends on 1) the automatic trimming of inflections and 2) the language pair for which comprehension is tested. Our multilingual modelling approach does not only offer new methodological findings for automatic testing of mutual intelligibility across languages 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#37197;&#32622;&#65292;&#24182;&#25506;&#31350;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#23545;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#37197;&#32622;&#23545;&#35805;&#32773;&#23637;&#31034;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#36827;&#19968;&#27493;&#29702;&#35299;LLMs&#20043;&#38388;&#22522;&#20110;&#23545;&#35805;&#30340;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#24378;&#35843;&#20102;&#25171;&#36896;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31867;&#20154;&#21270;LLM&#26234;&#33021;&#20307;&#30340;&#26032;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.02896</link><description>&lt;p&gt;
LLM&#26234;&#33021;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#27979;&#37327;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#20316;&#29992;&#20013;&#30340;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#37197;&#32622;&#65292;&#24182;&#25506;&#31350;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#23545;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#37197;&#32622;&#23545;&#35805;&#32773;&#23637;&#31034;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#36827;&#19968;&#27493;&#29702;&#35299;LLMs&#20043;&#38388;&#22522;&#20110;&#23545;&#35805;&#30340;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#24378;&#35843;&#20102;&#25171;&#36896;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31867;&#20154;&#21270;LLM&#26234;&#33021;&#20307;&#30340;&#26032;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#21644;&#20010;&#24615;&#21270;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30740;&#31350;&#20013;&#37117;&#26159;&#28909;&#38376;&#35805;&#39064;&#65292;&#20294;&#23545;&#35821;&#35328;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#20010;&#24615;&#21270;&#30340;LLM&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#24433;&#21709;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#36825;&#26679;&#30340;&#21162;&#21147;&#23545;&#20110;&#30830;&#20445;&#26234;&#33021;&#20307;&#22312;&#20445;&#25345;&#20854;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#33021;&#22815;&#36827;&#34892;&#24320;&#25918;&#30340;&#33258;&#28982;&#23545;&#35805;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#23558;GPT-3.5&#35843;&#33410;&#21040;&#20010;&#24615;&#21270;&#37197;&#32622;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#30340;&#21464;&#24322;&#24615;&#24341;&#23548;&#25277;&#26679;&#31639;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#21452;&#32452;&#32676;&#30340;LLM&#26234;&#33021;&#20307;&#20154;&#21475;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20010;&#24615;&#21270;&#27979;&#35797;&#65292;&#24182;&#23558;&#26234;&#33021;&#20307;&#25552;&#20132;&#21040;&#21327;&#21516;&#20889;&#20316;&#20219;&#21153;&#65292;&#21457;&#29616;&#19981;&#21516;&#37197;&#32622;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#21644;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#20043;&#38388;&#22522;&#20110;&#23545;&#35805;&#30340;&#30456;&#20114;&#20316;&#29992;&#22880;&#23450;&#22522;&#30784;&#65292;&#24182;&#20984;&#26174;&#20102;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#25171;&#36896;&#26356;&#24378;&#22823;&#12289;&#26356;&#31867;&#20154;&#30340;LLM&#26234;&#33021;&#20307;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM pers
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#26377;&#23402;&#29983;&#21464;&#21387;&#22120;&#30340;&#36817;&#20284;&#24402;&#22240;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#30041;&#21407;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20934;&#30830;&#24402;&#22240;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36817;&#20284;&#21644;&#20934;&#30830;&#24402;&#22240;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#26041;&#38754;&#30340;&#20851;&#27880;&#65292;&#24182;&#21457;&#29616;&#23402;&#29983;&#21464;&#21387;&#22120;&#20027;&#35201;&#24573;&#30053;&#21542;&#23450;&#65292;&#21516;&#26102;&#28145;&#20837;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#21477;&#27861;&#35282;&#33394;&#30340;&#20851;&#27880;&#31243;&#24230;&#65292;&#20197;&#21450;&#22914;&#20309;&#21028;&#26029;&#35821;&#20041;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.02883</link><description>&lt;p&gt;
&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#36866;&#29992;&#20110;&#29616;&#26377;&#23402;&#29983;&#21464;&#21387;&#22120;&#30340;&#36817;&#20284;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximate Attributions for Off-the-Shelf Siamese Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02883
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#26377;&#23402;&#29983;&#21464;&#21387;&#22120;&#30340;&#36817;&#20284;&#24402;&#22240;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#30041;&#21407;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20934;&#30830;&#24402;&#22240;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36817;&#20284;&#21644;&#20934;&#30830;&#24402;&#22240;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#26041;&#38754;&#30340;&#20851;&#27880;&#65292;&#24182;&#21457;&#29616;&#23402;&#29983;&#21464;&#21387;&#22120;&#20027;&#35201;&#24573;&#30053;&#21542;&#23450;&#65292;&#21516;&#26102;&#28145;&#20837;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#21477;&#27861;&#35282;&#33394;&#30340;&#20851;&#27880;&#31243;&#24230;&#65292;&#20197;&#21450;&#22914;&#20309;&#21028;&#26029;&#35821;&#20041;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;: &#23402;&#29983;&#32534;&#30721;&#22120;&#22914;&#21477;&#23376;&#21464;&#25442;&#22120;&#26159;&#30446;&#21069;&#26368;&#19981;&#29702;&#35299;&#30340;&#28145;&#24230;&#27169;&#22411;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#24402;&#22240;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#36825;&#31181;&#27169;&#22411;&#31867;&#21035;&#65292;&#22240;&#20026;&#23427;&#20204;&#27604;&#36739;&#20004;&#20010;&#36755;&#20837;&#32780;&#19981;&#26159;&#22788;&#29702;&#21333;&#20010;&#36755;&#20837;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#23402;&#29983;&#32534;&#30721;&#22120;&#30340;&#24402;&#22240;&#26041;&#27861;(Moller&#31561;&#65292;2023)&#12290;&#28982;&#32780;&#65292;&#23427;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#21644;&#24494;&#35843;&#65292;&#22240;&#27492;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;(i)&#19968;&#31181;&#20855;&#26377;&#20934;&#30830;&#24402;&#22240;&#33021;&#21147;&#19988;&#20445;&#30041;&#21407;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;(ii)&#19968;&#31181;&#35745;&#31639;&#29616;&#26377;&#27169;&#22411;&#36817;&#20284;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24191;&#27867;&#27604;&#36739;&#20102;&#36817;&#20284;&#21644;&#20934;&#30830;&#24402;&#22240;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#20998;&#26512;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#26041;&#38754;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#20102;&#23402;&#29983;&#21464;&#21387;&#22120;&#23545;&#21477;&#27861;&#35282;&#33394;&#30340;&#20851;&#27880;&#31243;&#24230;&#65292;&#30830;&#35748;&#23427;&#20204;&#20027;&#35201;&#24573;&#30053;&#21542;&#23450;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#22914;&#20309;&#21028;&#26029;&#35821;&#20041;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Siamese encoders such as sentence transformers are among the least understood deep models. Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (M\"oller et al., 2023). However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model's predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models. We extensively compare approximate and exact attributions and use them to analyze the models' attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically op
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26469;&#35745;&#31639;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.02872</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26159;&#19978;&#19979;&#25991;&#22836;&#37096;&#36827;&#34892;&#24230;&#37327;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26469;&#35745;&#31639;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#22312;&#27973;&#23618;&#20013;&#65292;&#28436;&#31034;&#30340;&#29305;&#24449;&#34987;&#21512;&#24182;&#21040;&#30456;&#24212;&#30340;&#26631;&#31614;&#20013;&#65292;&#36755;&#20837;&#25991;&#26412;&#30340;&#29305;&#24449;&#34987;&#32858;&#21512;&#21040;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#20013;&#12290;&#22312;&#28145;&#23618;&#20013;&#65292;&#19978;&#19979;&#25991;&#22836;&#37096;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#27599;&#20010;&#19978;&#19979;&#25991;&#22836;&#37096;&#20013;&#65292;&#20540;-&#36755;&#20986;&#30697;&#38453;&#25552;&#21462;&#20102;&#26631;&#31614;&#30340;&#29305;&#24449;&#12290;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#35745;&#31639;&#20102;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#27880;&#24847;&#21147;&#26435;&#37325;&#36234;&#22823;&#65292;&#36234;&#22810;&#30340;&#26631;&#31614;&#20449;&#24687;&#34987;&#20256;&#36755;&#21040;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#21487;&#20197;&#34987;&#35270;&#20026;&#23398;&#20064;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19981;&#24179;&#34913;&#30340;&#26631;&#31614;&#21644;&#28436;&#31034;&#39034;&#24207;&#20250;&#24433;&#21709;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;GPT2&#22823;&#22411;&#12289;Llama 7B&#12289;13B&#21644;30B&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provid
&lt;/p&gt;</description></item><item><title>EEVEE&#26159;&#19968;&#27454;&#31616;&#21333;&#26131;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26631;&#27880;&#24037;&#20855;&#65292;&#21487;&#30452;&#25509;&#22312;&#27983;&#35272;&#22120;&#20013;&#36816;&#34892;&#65292;&#25903;&#25345;&#22810;&#20219;&#21153;&#26631;&#27880;&#21644;&#22235;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02864</link><description>&lt;p&gt;
EEVEE&#65306;&#19968;&#27454;&#31616;&#26131;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26631;&#27880;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
EEVEE: An Easy Annotation Tool for Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02864
&lt;/p&gt;
&lt;p&gt;
EEVEE&#26159;&#19968;&#27454;&#31616;&#21333;&#26131;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26631;&#27880;&#24037;&#20855;&#65292;&#21487;&#30452;&#25509;&#22312;&#27983;&#35272;&#22120;&#20013;&#36816;&#34892;&#65292;&#25903;&#25345;&#22810;&#20219;&#21153;&#26631;&#27880;&#21644;&#22235;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#24037;&#20855;&#26159;&#21019;&#24314;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#30340;&#36215;&#28857;&#12290;&#30446;&#21069;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;&#24037;&#20855;&#21487;&#29992;&#65292;&#20294;&#35774;&#32622;&#36825;&#20123;&#24037;&#20855;&#21364;&#26159;&#19968;&#20010;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EEVEE&#65292;&#19968;&#27454;&#19987;&#27880;&#20110;&#31616;&#27905;&#12289;&#39640;&#25928;&#21644;&#26131;&#29992;&#24615;&#30340;&#26631;&#27880;&#24037;&#20855;&#12290;&#23427;&#21487;&#20197;&#30452;&#25509;&#22312;&#27983;&#35272;&#22120;&#20013;&#36816;&#34892;&#65288;&#26080;&#38656;&#35774;&#32622;&#65289;&#65292;&#24182;&#20351;&#29992;&#21046;&#34920;&#31526;&#20998;&#38548;&#30340;&#25991;&#20214;&#65288;&#32780;&#19981;&#26159;&#23383;&#31526;&#20559;&#31227;&#25110;&#20219;&#21153;&#29305;&#23450;&#26684;&#24335;&#65289;&#36827;&#34892;&#26631;&#27880;&#12290;&#23427;&#20801;&#35768;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#26631;&#27880;&#65292;&#24182;&#25903;&#25345;&#22235;&#31181;&#20219;&#21153;&#31867;&#22411;&#65306;&#24207;&#21015;&#26631;&#27880;&#12289;&#36328;&#24230;&#26631;&#27880;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotation tools are the starting point for creating Natural Language Processing (NLP) datasets. There is a wide variety of tools available; setting up these tools is however a hindrance. We propose EEVEE, an annotation tool focused on simplicity, efficiency, and ease of use. It can run directly in the browser (no setup required) and uses tab-separated files (as opposed to character offsets or task-specific formats) for annotation. It allows for annotation of multiple tasks on a single dataset and supports four task-types: sequence labeling, span labeling, text classification and seq2seq.
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#30693;&#35782;&#26469;&#28304;&#65288;PubMed&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;&#35895;&#27468;&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02844</link><description>&lt;p&gt;
&#23545;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#30340;&#30693;&#35782;&#26469;&#28304;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing Knowledge Sources for Open-Domain Scientific Claim Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02844
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#30693;&#35782;&#26469;&#28304;&#65288;PubMed&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;&#35895;&#27468;&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#30693;&#35782;&#30340;&#21457;&#29616;&#36895;&#24230;&#21644;&#22312;&#32447;&#20581;&#24247;&#20027;&#24352;&#30340;&#20998;&#20139;&#22686;&#21152;&#65292;&#20984;&#26174;&#20102;&#20026;&#31185;&#23398;&#20027;&#24352;&#24320;&#21457;&#39640;&#25928;&#20107;&#23454;&#26816;&#26680;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#36825;&#39033;&#20219;&#21153;&#30340;&#24120;&#35265;&#35774;&#32622;&#20551;&#35774;&#24050;&#32463;&#25552;&#20379;&#24182;&#27880;&#37322;&#25110;&#21253;&#21547;&#22312;&#26377;&#38480;&#35821;&#26009;&#24211;&#20013;&#30340;&#21253;&#21547;&#35777;&#25454;&#30340;&#25991;&#26723;&#12290;&#36825;&#20351;&#24471;&#35813;&#31995;&#32479;&#22312;&#21487;&#33021;&#38656;&#35201;&#26597;&#35810;&#25968;&#30334;&#19975;&#20010;&#25991;&#26723;&#20197;&#25214;&#21040;&#30456;&#20851;&#35777;&#25454;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#27979;&#35797;&#24320;&#25918;&#39046;&#22495;&#20027;&#24352;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#27979;&#35797;&#20102;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#20027;&#24352;&#31995;&#32479;&#30340;&#26368;&#32456;&#21028;&#26029;&#39044;&#27979;&#12290;&#22312;&#20445;&#25345;&#27969;&#27700;&#32447;&#30340;&#35777;&#25454;&#36873;&#25321;&#21644;&#21028;&#26029;&#39044;&#27979;&#37096;&#20998;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#19977;&#31181;&#24120;&#35265;&#30693;&#35782;&#26469;&#28304;&#65288;PubMed&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;&#35895;&#27468;&#65289;&#21644;&#20004;&#31181;&#19981;&#21516;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#36827;&#34892;&#25991;&#26723;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline's evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different informati
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#35805;&#20013;&#35782;&#21035;&#20986;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#20998;&#21106;&#20026;&#20027;&#39064;&#36830;&#36143;&#30340;&#35805;&#35821;&#38598;&#21512;&#65292;&#20197;&#19968;&#20010;&#27491;&#24335;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#23545;&#35805;&#30340;&#20027;&#39064;&#32452;&#32455;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.02837</link><description>&lt;p&gt;
&#20174;&#65288;&#35821;&#35328;&#30340;&#65289;&#26379;&#21451;&#37027;&#37324;&#24471;&#21040;&#19968;&#28857;&#24110;&#21161;&#65306;&#22810;&#26041;&#38543;&#24847;&#23545;&#35805;&#30340;&#20027;&#39064;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
With a Little Help from my (Linguistic) Friends: Topic Segmentation of Multi-party Casual Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02837
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#35805;&#20013;&#35782;&#21035;&#20986;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#20998;&#21106;&#20026;&#20027;&#39064;&#36830;&#36143;&#30340;&#35805;&#35821;&#38598;&#21512;&#65292;&#20197;&#19968;&#20010;&#27491;&#24335;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#23545;&#35805;&#30340;&#20027;&#39064;&#32452;&#32455;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#22312;&#23545;&#35805;&#30340;&#20840;&#23616;&#32452;&#32455;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24403;&#21069;&#35752;&#35770;&#30340;&#20869;&#23481;&#38480;&#21046;&#20102;&#21442;&#19982;&#32773;&#30340;&#21487;&#33021;&#36129;&#29486;&#12290;&#29702;&#35299;&#20027;&#39064;&#22312;&#20114;&#21160;&#20013;&#30340;&#32452;&#32455;&#26041;&#24335;&#23558;&#25581;&#31034;&#23545;&#35805;&#32467;&#26500;&#36229;&#36234;&#35805;&#35821;&#24207;&#21015;&#30340;&#27934;&#23519;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36825;&#31181;&#39640;&#23618;&#32467;&#26500;&#26159;&#19968;&#39033;&#22797;&#26434;&#20219;&#21153;&#65292;&#25105;&#20204;&#39318;&#20808;&#23581;&#35797;&#23558;&#23545;&#35805;&#20998;&#21106;&#20026;&#26356;&#23567;&#30340;&#20027;&#39064;&#36830;&#36143;&#30340;&#35805;&#35821;&#38598;&#21512;&#12290;&#29702;&#35299;&#36825;&#20123;&#29255;&#27573;&#20043;&#38388;&#30340;&#20132;&#20114;&#23558;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#23545;&#35805;&#32423;&#21035;&#30340;&#20027;&#39064;&#32452;&#32455;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#65292;&#35797;&#22270;&#20197;&#27491;&#24335;&#26041;&#27861;&#36798;&#21040;&#19982;&#26368;&#36817;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#39064;&#20998;&#21106;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#30340;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#23545;&#35805;&#30340;&#20027;&#39064;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topics play an important role in the global organisation of a conversation as what is currently discussed constrains the possible contributions of the participant. Understanding the way topics are organised in interaction would provide insight on the structure of dialogue beyond the sequence of utterances. However, studying this high-level structure is a complex task that we try to approach by first segmenting dialogues into smaller topically coherent sets of utterances. Understanding the interactions between these segments would then enable us to propose a model of topic organisation at a dialogue level. In this paper we work with open-domain conversations and try to reach a comparable level of accuracy as recent machine learning based topic segmentation models but with a formal approach. The features we identify as meaningful for this task help us understand better the topical structure of a conversation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#21313;&#20010;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#65292;&#39318;&#27425;&#22312;&#31995;&#32479;&#21457;&#32946;&#37325;&#24314;&#20013;&#27604;&#36739;&#20102;&#22522;&#20110;&#22768;&#38899;&#21644;&#22522;&#20110;&#21516;&#28304;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#35789;&#27719;&#21516;&#28304;&#30340;&#37325;&#24314;&#35889;&#31995;&#19982;&#30495;&#23454;&#35889;&#31995;&#24179;&#22343;&#26356;&#25509;&#36817;&#65292;&#25552;&#39640;&#20102;&#32422;&#19977;&#20998;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2402.02807</link><description>&lt;p&gt;
&#22768;&#38899;&#23545;&#20110;&#31995;&#32479;&#21457;&#32946;&#37325;&#24314;&#21487;&#38752;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Sounds Sound for Phylogenetic Reconstruction?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#21313;&#20010;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#65292;&#39318;&#27425;&#22312;&#31995;&#32479;&#21457;&#32946;&#37325;&#24314;&#20013;&#27604;&#36739;&#20102;&#22522;&#20110;&#22768;&#38899;&#21644;&#22522;&#20110;&#21516;&#28304;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#35789;&#27719;&#21516;&#28304;&#30340;&#37325;&#24314;&#35889;&#31995;&#19982;&#30495;&#23454;&#35889;&#31995;&#24179;&#22343;&#26356;&#25509;&#36817;&#65292;&#25552;&#39640;&#20102;&#32422;&#19977;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#35821;&#35328;&#36827;&#21270;&#30740;&#31350;&#20013;&#65292;&#23398;&#32773;&#20204;&#36890;&#24120;&#24378;&#35843;&#22768;&#38899;&#35268;&#24459;&#21644;&#23545;&#24212;&#20851;&#31995;&#23545;&#20110;&#35821;&#35328;&#23478;&#26063;&#35889;&#31995;&#25512;&#26029;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#35745;&#31639;&#26041;&#27861;&#24448;&#24448;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#21040;&#36825;&#19968;&#28508;&#21147;&#12290;&#22823;&#22810;&#25968;&#35745;&#31639;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#35789;&#27719;&#21516;&#28304;&#20316;&#20026;&#35821;&#35328;&#23398;&#31995;&#32479;&#21457;&#32946;&#37325;&#24314;&#30340;&#20027;&#35201;&#25968;&#25454;&#26469;&#28304;&#65292;&#23613;&#31649;&#20063;&#26377;&#19968;&#20123;&#30740;&#31350;&#20013;&#30340;&#20316;&#32773;&#36190;&#36175;&#27604;&#36739;&#22768;&#38899;&#24207;&#21015;&#30340;&#22909;&#22788;&#12290;&#22522;&#20110;&#21313;&#20010;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#21644;&#29616;&#20195;&#33258;&#21160;&#21516;&#28304;&#21644;&#22768;&#38899;&#23545;&#24212;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#27425;&#27979;&#35797;&#20102;&#22522;&#20110;&#22768;&#38899;&#21644;&#22522;&#20110;&#21516;&#28304;&#30340;&#26041;&#27861;&#22312;&#31995;&#32479;&#21457;&#32946;&#37325;&#24314;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#35789;&#27719;&#21516;&#28304;&#37325;&#24314;&#30340;&#35889;&#31995;&#22312;&#24191;&#20041;&#22235;&#20803;&#32452;&#36317;&#31163;&#19978;&#19982;&#30495;&#23454;&#35889;&#31995;&#24179;&#22343;&#26356;&#25509;&#36817;&#65292;&#25552;&#21319;&#20102;&#32422;&#19977;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional studies on language evolution, scholars often emphasize the importance of sound laws and sound correspondences for phylogenetic inference of language family trees. However, to date, computational approaches have typically not taken this potential into account. Most computational studies still rely on lexical cognates as major data source for phylogenetic reconstruction in linguistics, although there do exist a few studies in which authors praise the benefits of comparing words at the level of sound sequences. Building on (a) ten diverse datasets from different language families, and (b) state-of-the-art methods for automated cognate and sound correspondence detection, we test, for the first time, the performance of sound-based versus cognate-based approaches to phylogenetic reconstruction. Our results show that phylogenies reconstructed from lexical cognates are topologically closer, by approximately one third with respect to the generalized quartet distance on average, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.02805</link><description>&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20013;&#30340;&#22270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Large Language Models in Asynchronous Plan Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39034;&#24207;&#21644;&#24182;&#34892;&#35268;&#21010;&#20197;&#20248;&#21270;&#26102;&#38388;&#25104;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25104;&#21151;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#38381;&#28304;&#21644;&#24320;&#28304;LLMs&#65292;&#21253;&#25324;GPT-4&#21644;LLaMA-2&#65292;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;AsyncHow&#20013;&#65292;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#30340;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;PLaG&#33021;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#36973;&#21463;&#20005;&#37325;&#38477;&#32423;&#65292;&#31361;&#20986;&#20102;&#21033;&#29992;LLMs&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#35270;&#20026;&#23558;LLMs&#29992;&#20316;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
&lt;/p&gt;</description></item><item><title>KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.02801</link><description>&lt;p&gt;
KS-Lottery: &#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#35777;&#24425;&#31080;
&lt;/p&gt;
&lt;p&gt;
KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02801
&lt;/p&gt;
&lt;p&gt;
KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#31080;&#31080;&#35777;&#20551;&#35828;&#35748;&#20026;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#8220;&#20013;&#22870;&#31080;&#8221;&#12290;&#22312;&#24494;&#35843;&#22330;&#26223;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#20013;&#22870;&#31080;&#65311;&#25105;&#20204;&#22914;&#20309;&#25214;&#21040;&#36825;&#26679;&#30340;&#20013;&#22870;&#31080;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KS-Lottery&#65292;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22312;&#22810;&#35821;&#35328;&#24494;&#35843;&#20013;&#39640;&#24230;&#26377;&#25928;&#30340;LLM&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#24494;&#35843;&#21069;&#21518;&#21442;&#25968;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#29702;&#35770;&#35777;&#26126;&#20102;KS-Lottery&#21487;&#20197;&#22312;&#23884;&#20837;&#23618;&#20013;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#65292;&#24494;&#35843;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#20445;&#35777;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#23558;KS-Lottery&#19982;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KS-Lottery&#25214;&#21040;&#20102;&#19968;&#20010;&#26356;&#23567;&#30340;&#21442;&#25968;&#38598;&#26469;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#36798;&#21040;&#20102;&#19982;&#20840;&#38754;&#24494;&#35843;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;18&#20010;&#26631;&#35760;&#30340;&#23884;&#20837;&#23618;
&lt;/p&gt;
&lt;p&gt;
The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02791</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Optimization and Architecture for Tiny Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#21147;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#32454;&#33410;&#24456;&#23569;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;10&#20159;&#21442;&#25968;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#32463;&#39564;&#30740;&#31350;&#26469;&#20998;&#26512;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#31070;&#32463;&#26550;&#26500;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#22810;&#20010;&#35774;&#35745;&#20844;&#24335;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#24615;&#22320;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#65292;&#21253;&#25324;&#20998;&#35789;&#22120;&#21387;&#32553;&#12289;&#26550;&#26500;&#35843;&#25972;&#12289;&#21442;&#25968;&#32487;&#25215;&#21644;&#22810;&#36718;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;1.6T&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;PanGu-$\pi$-1B Pro&#21644;PanGu-$\pi$-1.5B Pro&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#36981;&#24490;&#20005;&#26684;&#22686;&#37327;&#23450;&#20041;&#30340;&#22686;&#37327;&#21477;&#27861;&#20998;&#26512;&#22120;&#65292;&#35780;&#20272;&#20854;&#20165;&#22522;&#20110;&#21069;&#32512;&#34920;&#31034;&#23601;&#33021;&#36755;&#20986;&#26641;&#30340;&#33021;&#21147;&#65292;&#24182;&#19982;&#38750;&#22686;&#37327;&#21644;&#37096;&#20998;&#22686;&#37327;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.02782</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#21040;&#20005;&#26684;&#22686;&#37327;&#21477;&#27861;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Partial to Strictly Incremental Constituent Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#36981;&#24490;&#20005;&#26684;&#22686;&#37327;&#23450;&#20041;&#30340;&#22686;&#37327;&#21477;&#27861;&#20998;&#26512;&#22120;&#65292;&#35780;&#20272;&#20854;&#20165;&#22522;&#20110;&#21069;&#32512;&#34920;&#31034;&#23601;&#33021;&#36755;&#20986;&#26641;&#30340;&#33021;&#21147;&#65292;&#24182;&#19982;&#38750;&#22686;&#37327;&#21644;&#37096;&#20998;&#22686;&#37327;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22686;&#37327;&#21477;&#27861;&#20998;&#26512;&#22120;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#20165;&#22522;&#20110;&#21069;&#32512;&#34920;&#31034;&#23601;&#33021;&#36755;&#20986;&#26641;&#30340;&#33021;&#21147;&#12290;&#22312;&#20005;&#26684;&#30340;&#20174;&#24038;&#21040;&#21491;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21644;&#26641;&#35299;&#30721;&#27169;&#22359;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36981;&#24490;&#19981;&#21516;&#35821;&#35328;&#30340;&#22686;&#37327;&#24615;&#24378;&#23450;&#20041;&#30340;&#35299;&#26512;&#22120;&#12290;&#36825;&#24314;&#31435;&#22312;&#24050;&#26377;&#20851;&#20110;&#22686;&#37327;&#24615;&#30340;&#24037;&#20316;&#19978;&#65292;&#20294;&#22823;&#22810;&#25968;&#21482;&#22312;&#32534;&#30721;&#22120;&#25110;&#35299;&#30721;&#22120;&#20013;&#24378;&#21046;&#23454;&#26045;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#38750;&#22686;&#37327;&#21644;&#37096;&#20998;&#22686;&#37327;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study incremental constituent parsers to assess their capacity to output trees based on prefix representations alone. Guided by strictly left-to-right generative language models and tree-decoding modules, we build parsers that adhere to a strong definition of incrementality across languages. This builds upon work that asserted incrementality, but that mostly only enforced it on either the encoder or the decoder. Finally, we conduct an analysis against non-incremental and partially incremental models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#26102;&#24207;&#24179;&#22343;&#30693;&#35782;&#33976;&#39311;&#21644;&#22686;&#24378;&#23884;&#20837;&#29305;&#24449;&#33976;&#39311;&#26469;&#31283;&#23450;&#22320;&#33976;&#39311;&#30693;&#35782;&#24182;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02781</link><description>&lt;p&gt;
&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#39640;&#25928;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dual Knowledge Distillation for Efficient Sound Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#26102;&#24207;&#24179;&#22343;&#30693;&#35782;&#33976;&#39311;&#21644;&#22686;&#24378;&#23884;&#20837;&#29305;&#24449;&#33976;&#39311;&#26469;&#31283;&#23450;&#22320;&#33976;&#39311;&#30693;&#35782;&#24182;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#65288;SED&#65289;&#23545;&#20110;&#35782;&#21035;&#29305;&#23450;&#22768;&#38899;&#21450;&#20854;&#22312;&#22768;&#23398;&#20449;&#21495;&#20013;&#30340;&#26102;&#38388;&#20301;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#22312;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#21464;&#24471;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#65292;&#29992;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;SED&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#20197;&#26102;&#24207;&#24179;&#22343;&#30693;&#35782;&#33976;&#39311;&#65288;TAKD&#65289;&#20026;&#24320;&#31471;&#65292;&#21033;&#29992;&#20174;&#23398;&#29983;&#27169;&#22411;&#21442;&#25968;&#30340;&#26102;&#24207;&#24179;&#22343;&#24471;&#21040;&#30340;&#24179;&#22343;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#38388;&#25509;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#23398;&#20064;&#65292;&#30830;&#20445;&#31283;&#23450;&#30340;&#30693;&#35782;&#33976;&#39311;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22686;&#24378;&#23884;&#20837;&#29305;&#24449;&#33976;&#39311;&#65288;EEFD&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#22312;&#23398;&#29983;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#23884;&#20837;&#33976;&#39311;&#23618;&#26469;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;DCASE 2023&#20219;&#21153;4A&#20844;&#20849;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;SED&#31995;&#32479;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound event detection (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals. This becomes challenging particularly for on-device applications, where computational resources are limited. To address this issue, we introduce a novel framework referred to as dual knowledge distillation for developing efficient SED systems in this work. Our proposed dual knowledge distillation commences with temporal-averaging knowledge distillation (TAKD), utilizing a mean student model derived from the temporal averaging of the student model's parameters. This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable knowledge distillation. Subsequently, we introduce embedding-enhanced feature distillation (EEFD), which involves incorporating an embedding distillation layer within the student model to bolster contextual learning. On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual knowle
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21015;&#34920;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#24207;-&#25130;&#26029;&#32852;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25628;&#32034;&#21644;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25429;&#25417;&#21015;&#34920;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#26469;&#36820;&#22238;&#26356;&#22909;&#30340;&#21015;&#34920;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#35270;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#20294;&#36825;&#31181;&#20998;&#31163;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20449;&#24687;&#20849;&#20139;&#21644;&#38169;&#35823;&#31215;&#32047;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02764</link><description>&lt;p&gt;
&#22522;&#20110;&#21015;&#34920;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#24207;-&#25130;&#26029;&#32852;&#21512;&#27169;&#22411;&#29992;&#20110;&#25628;&#32034;&#21644;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21015;&#34920;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#24207;-&#25130;&#26029;&#32852;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25628;&#32034;&#21644;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25429;&#25417;&#21015;&#34920;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#26469;&#36820;&#22238;&#26356;&#22909;&#30340;&#21015;&#34920;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#35270;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#20294;&#36825;&#31181;&#20998;&#31163;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20449;&#24687;&#20849;&#20139;&#21644;&#38169;&#35823;&#31215;&#32047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#30340;&#32467;&#26524;&#36890;&#24120;&#20197;&#25490;&#21517;&#21015;&#34920;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20363;&#22914;&#38754;&#21521;&#20154;&#31867;&#30340;&#32593;&#32476;&#25628;&#32034;&#21644;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#21015;&#34920;&#24863;&#30693;&#26816;&#32034;&#26088;&#22312;&#25429;&#25417;&#21015;&#34920;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#36820;&#22238;&#26356;&#22909;&#30340;&#21015;&#34920;&#65292;&#20027;&#35201;&#21253;&#25324;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#12290;&#37325;&#26032;&#25490;&#24207;&#23545;&#21015;&#34920;&#20013;&#30340;&#25991;&#26723;&#36827;&#34892;&#31934;&#32454;&#37325;&#26032;&#35780;&#20998;&#12290;&#25130;&#26029;&#21160;&#24577;&#30830;&#23450;&#25490;&#21517;&#21015;&#34920;&#30340;&#25130;&#26029;&#28857;&#65292;&#20197;&#22312;&#25972;&#20307;&#30456;&#20851;&#24615;&#21644;&#36991;&#20813;&#26080;&#20851;&#25991;&#26723;&#30340;&#38169;&#35823;&#20449;&#24687;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#35270;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#24182;&#20998;&#21035;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#65292;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#31163;&#26159;&#19981;&#29702;&#24819;&#30340;&#12290;&#39318;&#20808;&#65292;&#24456;&#38590;&#22312;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#25490;&#21517;&#21015;&#34920;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#29420;&#31435;&#30340;&#27969;&#27700;&#32447;&#36890;&#24120;&#20250;&#36935;&#21040;&#38169;&#35823;&#31215;&#32047;&#38382;&#39064;&#65292;&#21363;&#37325;&#26032;&#25490;&#24207;&#38454;&#27573;&#30340;&#23567;&#38169;&#35823;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#25130;&#26029;&#38454;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;...&#65288;&#32487;&#32493;&#25551;&#36848;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#30340;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#21644;&#25512;&#26029;&#36895;&#24230;&#21463;&#38480;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02750</link><description>&lt;p&gt;
KIVI&#65306;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#30340;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#21644;&#25512;&#26029;&#36895;&#24230;&#21463;&#38480;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#26381;&#21153;&#38656;&#35201;&#23558;&#35768;&#22810;&#35831;&#27714;&#25209;&#37327;&#22788;&#29702;&#20197;&#20943;&#23569;&#27599;&#20010;&#35831;&#27714;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#20197;&#36991;&#20813;&#37325;&#26032;&#35745;&#31639;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#26174;&#33879;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#25104;&#20026;&#36895;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#26032;&#29942;&#39048;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;KV&#32531;&#23384;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;GPU&#30340;SRAM&#24517;&#39035;&#20174;&#20027;GPU&#20869;&#23384;&#20013;&#21152;&#36733;&#25972;&#20010;KV&#32531;&#23384;&#20197;&#29983;&#25104;&#27599;&#20010;&#26631;&#35760;&#65292;&#23548;&#33268;&#35745;&#31639;&#26680;&#24515;&#22312;&#27492;&#36807;&#31243;&#20013;&#22788;&#20110;&#31354;&#38386;&#29366;&#24577;&#12290;&#20943;&#23567;KV&#32531;&#23384;&#22823;&#23567;&#30340;&#19968;&#20010;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37327;&#21270;&#65292;&#36890;&#36807;&#20943;&#23569;KV&#32531;&#23384;&#25152;&#38656;&#30340;&#24635;&#23383;&#33410;&#25968;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;KV&#32531;&#23384;&#20803;&#32032;&#20998;&#24067;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#20197;&#20102;&#35299;KV&#32531;&#23384;&#37327;&#21270;&#30340;&#38590;&#24230;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#20803;&#32032;&#20998;&#24067;&#30740;&#31350;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribut
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.02695</link><description>&lt;p&gt;
&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Exploiting Class Probabilities for Black-box Sentence-level Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#32423;&#25915;&#20987;&#26159;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#21477;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#36825;&#20123;&#21477;&#23376;&#19982;&#27491;&#30830;&#20998;&#31867;&#30340;&#21477;&#23376;&#21516;&#20041;&#65292;&#20294;&#34987;&#20998;&#31867;&#22120;&#38169;&#35823;&#22320;&#20998;&#31867;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#65292;&#20998;&#31867;&#22120;&#21482;&#33021;&#36890;&#36807;&#23545;&#26597;&#35810;&#36755;&#20837;&#30340;&#21453;&#39304;&#36827;&#34892;&#35775;&#38382;&#65292;&#36825;&#20027;&#35201;&#20197;&#31867;&#21035;&#27010;&#29575;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;&#23613;&#31649;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#20294;&#30001;&#20110;&#22312;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#35201;&#20040;&#19981;&#20351;&#29992;&#21453;&#39304;&#65292;&#35201;&#20040;&#20165;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#19978;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#26159;&#21542;&#20540;&#24471;&#25110;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20998;&#31867;&#22120;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#19982;&#22522;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#25454;&#25972;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02658</link><description>&lt;p&gt;
&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#30340;&#39564;&#35777;&#22120;&#65306;&#20851;&#20110;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#25454;&#25972;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#30417;&#30563;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#39564;&#35777;&#22120;&#26469;&#35780;&#20272;&#25512;&#29702;&#22120;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#24050;&#32463;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#36991;&#20813;&#22312;&#39564;&#35777;&#22120;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#25968;&#25454;&#25972;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;MiPS&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;&#20934;&#30830;&#23436;&#25104;&#30340;&#27604;&#20363;&#23450;&#20041;&#20026;&#20934;&#30830;&#29575;&#12290;&#25512;&#29702;&#22120;&#20013;&#30340;&#38169;&#35823;&#20250;&#23548;&#33268;MiPS&#20302;&#20272;&#20013;&#38388;&#27493;&#39588;&#30340;&#20934;&#30830;&#29575;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#65292;&#32780;&#19981;&#26159;&#20302;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65288;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;+0.67&#65285;&#65292;&#25968;&#23398;&#19978;&#30340;&#20934;&#30830;&#29575;+4.16&#65285;&#65292;MBPP&#19978;&#30340;&#20934;&#30830;&#29575;+0.92&#65285;&#19982;&#36755;&#20986;s&#30456;&#27604;&#12290;&#65289;
&lt;/p&gt;
&lt;p&gt;
Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output s
&lt;/p&gt;</description></item><item><title>RACER&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#21322;&#32467;&#26500;&#21270;&#24515;&#29702;&#20581;&#24247;&#35775;&#35848;&#36716;&#21270;&#20026;&#26377;&#27934;&#35265;&#30340;&#20027;&#39064;&#21644;&#23376;&#20027;&#39064;&#12290;&#22312;COVID-19&#21361;&#26426;&#30340;&#30740;&#31350;&#20013;&#65292;RACER&#19982;&#20154;&#24037;&#35780;&#20272;&#21592;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#36798;&#21040;72%&#65292;&#25509;&#36817;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65288;77%&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.02656</link><description>&lt;p&gt;
RACER:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#21487;&#25193;&#23637;&#21322;&#32467;&#26500;&#21270;&#24515;&#29702;&#20581;&#24247;&#35775;&#35848;&#20998;&#26512;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02656
&lt;/p&gt;
&lt;p&gt;
RACER&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#21322;&#32467;&#26500;&#21270;&#24515;&#29702;&#20581;&#24247;&#35775;&#35848;&#36716;&#21270;&#20026;&#26377;&#27934;&#35265;&#30340;&#20027;&#39064;&#21644;&#23376;&#20027;&#39064;&#12290;&#22312;COVID-19&#21361;&#26426;&#30340;&#30740;&#31350;&#20013;&#65292;RACER&#19982;&#20154;&#24037;&#35780;&#20272;&#21592;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#36798;&#21040;72%&#65292;&#25509;&#36817;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65288;77%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;(SSIs)&#26159;&#21307;&#30103;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#28145;&#20837;&#30340;&#23450;&#24615;&#27934;&#23519;&#21147;&#12290;&#23613;&#31649;&#20854;&#20215;&#20540;&#24456;&#22823;&#65292;&#20294;&#30001;&#20110;&#24773;&#32490;&#21453;&#24212;&#30340;&#25552;&#21462;&#21644;&#20998;&#31867;&#22256;&#38590;&#65292;&#20197;&#21450;&#23545;&#22823;&#35268;&#27169;&#20154;&#32676;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;SSIs&#30340;&#25163;&#21160;&#20998;&#26512;&#34987;&#20247;&#25152;&#21608;&#30693;&#22320;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RACER&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19987;&#23478;&#24341;&#23548;&#30340;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23558;&#21407;&#22987;&#35775;&#35848;&#35760;&#24405;&#36716;&#21270;&#20026;&#26377;&#27934;&#35265;&#30340;&#39046;&#22495;&#30456;&#20851;&#20027;&#39064;&#21644;&#23376;&#20027;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;RACER&#23545;93&#21517;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#23454;&#20064;&#29983;&#36827;&#34892;&#30340;SSIs&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;COVID-19&#21361;&#26426;&#23545;&#20010;&#20154;&#21644;&#19987;&#19994;&#24515;&#29702;&#20581;&#24247;&#30340;&#24191;&#27867;&#24433;&#21709;&#12290;RACER&#19982;&#20004;&#20010;&#20154;&#24037;&#35780;&#20272;&#21592;&#20043;&#38388;&#36798;&#21040;&#20102;&#36866;&#24230;&#39640;&#30340;&#19968;&#33268;&#24615;&#65288;72%&#65289;&#65292;&#25509;&#36817;&#20154;&#38469;&#19968;&#33268;&#24615;&#65288;77%&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;LLMs&#21644;&#20154;&#31867;&#22312;&#22788;&#29702;&#31867;&#20284;&#20869;&#23481;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-structured interviews (SSIs) are a commonly employed data-collection method in healthcare research, offering in-depth qualitative insights into subject experiences. Despite their value, the manual analysis of SSIs is notoriously time-consuming and labor-intensive, in part due to the difficulty of extracting and categorizing emotional responses, and challenges in scaling human evaluation for large populations. In this study, we develop RACER, a Large Language Model (LLM) based expert-guided automated pipeline that efficiently converts raw interview transcripts into insightful domain-relevant themes and sub-themes. We used RACER to analyze SSIs conducted with 93 healthcare professionals and trainees to assess the broad personal and professional mental health impacts of the COVID-19 crisis. RACER achieves moderately high agreement with two human evaluators (72%), which approaches the human inter-rater agreement (77%). Interestingly, LLMs and humans struggle with similar content invol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;VlogQA&#65306;&#36234;&#21335;&#21475;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;VlogQA&#26159;&#19968;&#20010;&#22522;&#20110;&#26469;&#33258;YouTube&#30340;&#21095;&#26412;&#25991;&#26723;&#30340;&#38382;&#31572;&#23545;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#39135;&#29289;&#21644;&#26053;&#34892;&#31561;&#20027;&#39064;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#21462;&#24471;&#20102;75.34%&#30340;&#26368;&#39640;F1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.02655</link><description>&lt;p&gt;
VlogQA: &#36234;&#21335;&#21475;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;VlogQA&#65306;&#36234;&#21335;&#21475;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;VlogQA&#26159;&#19968;&#20010;&#22522;&#20110;&#26469;&#33258;YouTube&#30340;&#21095;&#26412;&#25991;&#26723;&#30340;&#38382;&#31572;&#23545;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#39135;&#29289;&#21644;&#26053;&#34892;&#31561;&#20027;&#39064;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#21462;&#24471;&#20102;75.34%&#30340;&#26368;&#39640;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#36234;&#21335;&#21475;&#35821;&#35821;&#26009;&#24211;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;&#29616;&#26377;&#30340;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#35821;&#26009;&#24211;&#20027;&#35201;&#20851;&#27880;&#27491;&#24335;&#30340;&#20070;&#38754;&#25991;&#26723;&#65292;&#22914;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#12289;&#22312;&#32447;&#25253;&#32440;&#25110;&#25945;&#31185;&#20070;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;VlogQA&#21253;&#21547;&#20102;10,076&#20010;&#38382;&#31572;&#23545;&#65292;&#22522;&#20110;&#20174;YouTube&#33719;&#21462;&#30340;1,230&#20221;&#21095;&#26412;&#25991;&#26723;&#65292;YouTube&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;&#29992;&#25143;&#19978;&#20256;&#20869;&#23481;&#30340;&#24191;&#27867;&#36164;&#28304;&#65292;&#28085;&#30422;&#20102;&#39135;&#29289;&#21644;&#26053;&#34892;&#31561;&#20027;&#39064;&#12290;&#36890;&#36807;&#25429;&#25417;&#36234;&#21335;&#26412;&#22303;&#20154;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#21475;&#35821;&#34920;&#36798;&#65292;&#36825;&#26159;&#36234;&#21335;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#35282;&#33853;&#65292;&#35813;&#35821;&#26009;&#24211;&#20026;&#26410;&#26469;&#36234;&#21335;&#35821;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;&#20026;75.34%&#65292;&#34920;&#26126;&#20102;&#20854;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the development process of a Vietnamese spoken language corpus for machine reading comprehension (MRC) tasks and provides insights into the challenges and opportunities associated with using real-world data for machine reading comprehension tasks. The existing MRC corpora in Vietnamese mainly focus on formal written documents such as Wikipedia articles, online newspapers, or textbooks. In contrast, the VlogQA consists of 10,076 question-answer pairs based on 1,230 transcript documents sourced from YouTube -- an extensive source of user-uploaded content, covering the topics of food and travel. By capturing the spoken language of native Vietnamese speakers in natural settings, an obscure corner overlooked in Vietnamese research, the corpus provides a valuable resource for future research in reading comprehension tasks for the Vietnamese language. Regarding performance evaluation, our deep-learning models achieved the highest F1 score of 75.34% on the test set, indicat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20013;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#26041;&#27861;&#65292;&#24182;&#25552;&#21040;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02648</link><description>&lt;p&gt;
&#38142;&#24335;&#21453;&#39304;&#65306;&#32531;&#35299;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20013;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#26041;&#27861;&#65292;&#24182;&#25552;&#21040;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#39064;&#26102;&#32463;&#24120;&#20986;&#29616;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#65292;&#21363;&#20351;&#36755;&#20837;&#30456;&#21516;&#65292;&#20063;&#20250;&#25552;&#20379;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#24403;&#29992;&#25143;&#34920;&#36798;&#22362;&#20915;&#30456;&#21453;&#30340;&#31435;&#22330;&#26102;&#65292;LLMs&#35843;&#25972;&#20854;&#22238;&#31572;&#30340;&#36136;&#37327;&#20250;&#21464;&#24046;&#65292;&#23613;&#31649;&#21021;&#22987;&#22238;&#31572;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#20123;&#34892;&#20026;&#38477;&#20302;&#20102;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#30340;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#65306;1&#65289;&#36890;&#36807;&#23637;&#31034;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#22914;&#20309;&#23548;&#33268;LLMs&#26356;&#21152;&#20559;&#31163;&#23454;&#38469;&#31572;&#26696;&#65292;&#24341;&#36215;&#36807;&#24230;&#20381;&#36182;ChatGPT&#31561;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#22266;&#26377;&#39118;&#38505;&#65307;2&#65289;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#65292;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;CoF&#31995;&#32479;&#25509;&#25910;&#19968;&#20010;&#24320;&#25918;&#24335;&#22810;&#27493;&#38382;&#39064;&#65292;&#28982;&#21518;&#25105;&#20204;&#37325;&#22797;&#25552;&#20379;&#26080;&#24847;&#20041;&#30340;&#21453;&#39304;&#65292;&#35201;&#27714;&#20877;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21453;&#39304;&#21482;&#20250;&#38477;&#20302;&#22238;&#31572;&#30340;&#36136;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the oth
&lt;/p&gt;</description></item><item><title>LLMDB&#26159;&#19968;&#31181;LLM&#22686;&#24378;&#30340;&#25968;&#25454;&#31649;&#29702;&#33539;&#24335;&#65292;&#36890;&#36807;LLM&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#23884;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#34394;&#26500;&#12289;&#39640;&#25104;&#26412;&#21644;&#20302;&#20934;&#30830;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02643</link><description>&lt;p&gt;
LLM&#22686;&#24378;&#22411;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM-Enhanced Data Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02643
&lt;/p&gt;
&lt;p&gt;
LLMDB&#26159;&#19968;&#31181;LLM&#22686;&#24378;&#30340;&#25968;&#25454;&#31649;&#29702;&#33539;&#24335;&#65292;&#36890;&#36807;LLM&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#23884;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#34394;&#26500;&#12289;&#39640;&#25104;&#26412;&#21644;&#20302;&#20934;&#30830;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#20248;&#21270;&#25968;&#25454;&#31649;&#29702;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24191;&#27867;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ML&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#65288;&#36866;&#24212;&#19981;&#21516;&#24773;&#26223;&#65289;&#21644;&#25512;&#29702;&#33021;&#21147;&#65288;&#29702;&#35299;&#19978;&#19979;&#25991;&#65289;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20154;&#31867;&#31454;&#20105;&#33021;&#21147;&#65292;&#23545;&#20110;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#65288;&#22914;&#25968;&#25454;&#24211;&#35786;&#26029;&#12289;&#25968;&#25454;&#24211;&#35843;&#20248;&#65289;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#34394;&#26500;&#12289;&#39640;&#25104;&#26412;&#20197;&#21450;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#20302;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLMDB&#65292;&#19968;&#31181;&#20351;&#29992;LLM&#22686;&#24378;&#30340;&#25968;&#25454;&#31649;&#29702;&#33539;&#24335;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#34394;&#26500;&#65292;&#38477;&#20302;&#20102;LLM&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;LLMDB&#36890;&#36807;LLM&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#23884;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#20197;&#36991;&#20813;&#34394;&#26500;&#12290;LLMDB&#36890;&#36807;&#20943;&#23569;LLMs&#30340;&#39640;&#25104;&#26412; addresses challenges: hallucination, high cost, low accuracy-
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24314;&#31435;&#22303;&#33879;&#35821;&#35328;NLP&#25216;&#26415;&#30340;&#20262;&#29702;&#32771;&#34385;&#65292;&#24182;&#25512;&#33616;NLP&#30740;&#31350;&#20154;&#21592;&#22686;&#21152;&#23545;&#19982;&#22303;&#33879;&#31038;&#21306;&#21512;&#20316;&#36807;&#31243;&#30340;&#20851;&#27880;&#12290;</title><link>https://arxiv.org/abs/2402.02639</link><description>&lt;p&gt;
&#8220;&#37325;&#35201;&#30340;&#26159;&#20320;&#22914;&#20309;&#20570;&#20107;&#24773;&#8221;&#65306;&#20851;&#27880;&#36807;&#31243;&#20197;&#26356;&#22909;&#22320;&#20026;&#22303;&#33879;&#31038;&#21306;&#25552;&#20379;&#35821;&#35328;&#25216;&#26415;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
It's how you do things that matters": Attending to Process to Better Serve Indigenous Communities with Language Technologies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24314;&#31435;&#22303;&#33879;&#35821;&#35328;NLP&#25216;&#26415;&#30340;&#20262;&#29702;&#32771;&#34385;&#65292;&#24182;&#25512;&#33616;NLP&#30740;&#31350;&#20154;&#21592;&#22686;&#21152;&#23545;&#19982;&#22303;&#33879;&#31038;&#21306;&#21512;&#20316;&#36807;&#31243;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#19978;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#23545;&#22303;&#33879;&#35821;&#35328;&#30340;&#26381;&#21153;&#24635;&#26159;&#19981;&#36275;&#30340;&#65292;&#20294;&#26159;&#38543;&#30528;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#21644;NLP&#31038;&#32676;&#23545;&#28626;&#21361;&#35821;&#35328;&#30340;&#20851;&#27880;&#22686;&#21152;&#65292;&#36825;&#31181;&#24773;&#20917;&#27491;&#22312;&#21457;&#29983;&#25913;&#21464;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20026;&#22303;&#33879;&#35821;&#35328;&#26500;&#24314;NLP&#25216;&#26415;&#20013;&#30340;&#20262;&#29702;&#32771;&#34385;&#65292;&#22522;&#20110;&#36825;&#26679;&#30340;&#21069;&#25552;&#65306;&#36825;&#20123;&#39033;&#30446;&#39318;&#20808;&#24212;&#35813;&#26381;&#21153;&#20110;&#22303;&#33879;&#31038;&#21306;&#12290;&#25105;&#20204;&#23545;&#22312;&#28595;&#22823;&#21033;&#20122;&#20174;&#20107;&#22303;&#33879;&#21644;/&#25110;&#25176;&#38647;&#26031;&#28023;&#23777;&#23707;&#27665;&#31038;&#21306;&#30340;&#35821;&#35328;&#25216;&#26415;&#39033;&#30446;&#30340;17&#21517;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#20102;&#37319;&#35775;&#65292;&#24182;&#20511;&#37492;&#20102;&#36825;&#20123;&#37319;&#35775;&#30340;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#22686;&#21152;NLP&#30740;&#31350;&#20154;&#21592;&#23545;&#19982;&#22303;&#33879;&#31038;&#21306;&#21512;&#20316;&#36807;&#31243;&#30340;&#20851;&#27880;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#20110;&#21435;&#24773;&#22659;&#21270;&#30340;&#24037;&#33402;&#21697;&#30340;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indigenous languages are historically under-served by Natural Language Processing (NLP) technologies, but this is changing for some languages with the recent scaling of large multilingual models and an increased focus by the NLP community on endangered languages. This position paper explores ethical considerations in building NLP technologies for Indigenous languages, based on the premise that such projects should primarily serve Indigenous communities. We report on interviews with 17 researchers working in or with Aboriginal and/or Torres Strait Islander communities on language technology projects in Australia. Drawing on insights from the interviews, we recommend practices for NLP researchers to increase attention to the process of engagements with Indigenous communities, rather than focusing only on decontextualised artefacts.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02636</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#23398;&#20064;&#29420;&#31435;&#30340;&#22240;&#26524;&#26426;&#21046;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Learn Independent Causal Mechanisms?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#19981;&#24120;&#35265;&#30340;&#29615;&#22659;&#35774;&#32622;&#25110;&#20998;&#24067;&#21464;&#21270;&#30340;&#20219;&#21153;&#20013;&#65292;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#30446;&#21069;&#36890;&#24120;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#65292;&#22240;&#20026;&#20219;&#21153;&#30340;&#33539;&#22260;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;&#25110;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#37027;&#20123;&#23398;&#20064;&#25277;&#35937;&#21464;&#37327;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#31995;&#32479;&#65292;&#22914;&#22240;&#26524;&#27169;&#22411;&#65292;&#21487;&#20197;&#34920;&#29616;&#20986;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#26356;&#24378;&#31283;&#20581;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#23384;&#22312;&#24182;&#20351;&#29992;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65288;ICMs&#65289;&#65292;&#34920;&#31034;&#21482;&#31232;&#30095;&#20132;&#20114;&#30340;&#39640;&#23618;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#20004;&#20010;&#27010;&#24565;&#65292;&#22312;LLMs&#20013;&#23398;&#20064;ICMs&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#31232;&#30095;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;LLM&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability. This issue has usually been alleviated by feeding more training data into the LLM. However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training. By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#39046;&#22495;&#30456;&#20284;&#24615;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02633</link><description>&lt;p&gt;
&#39044;&#27979;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#65306;&#39046;&#22495;&#30456;&#20284;&#24615;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02633
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#39046;&#22495;&#30456;&#20284;&#24615;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;LRLs&#65289;&#65292;&#32454;&#35843;&#21644;&#27979;&#35797;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#24573;&#35270;&#20102;LRLs&#21644;&#36328;&#39046;&#22495;&#30340;&#36716;&#21464;&#12290;&#38024;&#23545;LRLs&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#22240;&#32032;&#65306;&#32454;&#35843;&#35821;&#26009;&#24211;&#30340;&#22823;&#23567;&#65292;&#32454;&#35843;&#35821;&#26009;&#24211;&#19982;&#27979;&#35797;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#39046;&#22495;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#32463;&#20856;&#22238;&#24402;&#27169;&#22411;&#35780;&#20272;&#36825;&#20123;&#22240;&#32032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#39046;&#22495;&#30456;&#20284;&#24615;&#23545;&#20110;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning and testing a multilingual large language model is expensive and challenging for low-resource languages (LRLs). While previous studies have predicted the performance of natural language processing (NLP) tasks using machine learning methods, they primarily focus on high-resource languages, overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate three factors: the size of the fine-tuning corpus, the domain similarity between fine-tuning and testing corpora, and the language similarity between source and target languages. We employ classical regression models to assess how these factors impact the model's performance. Our results indicate that domain similarity has the most critical impact on predicting the performance of Machine Translation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GIRT-&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#21457;&#32773;&#25351;&#31034;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#25253;&#21578;&#27169;&#26495;&#30340;&#21161;&#29702;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;GitHub&#20179;&#24211;&#20013;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;GIRT-&#27169;&#22411;&#22312;IRT&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02632</link><description>&lt;p&gt;
GIRT-&#27169;&#22411;&#65306;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#25253;&#21578;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
GIRT-Model: Automated Generation of Issue Report Templates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GIRT-&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#21457;&#32773;&#25351;&#31034;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#25253;&#21578;&#27169;&#26495;&#30340;&#21161;&#29702;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;GitHub&#20179;&#24211;&#20013;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;GIRT-&#27169;&#22411;&#22312;IRT&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GitHub&#21644;GitLab&#31561;&#24179;&#21488;&#24341;&#20837;&#38382;&#39064;&#25253;&#21578;&#27169;&#26495;&#65288;IRT&#65289;&#20197;&#25552;&#39640;&#38382;&#39064;&#31649;&#29702;&#25928;&#29575;&#24182;&#26356;&#22909;&#22320;&#19982;&#24320;&#21457;&#32773;&#26399;&#26395;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20179;&#24211;&#24182;&#26410;&#24191;&#27867;&#37319;&#29992;&#36825;&#20123;&#27169;&#26495;&#65292;&#24182;&#19988;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#30340;&#24037;&#20855;&#26469;&#36741;&#21161;&#24320;&#21457;&#32773;&#29983;&#25104;&#27169;&#26495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GIRT-&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#21457;&#32773;&#20851;&#20110;&#32467;&#26500;&#21644;&#24517;&#38656;&#23383;&#27573;&#30340;&#25351;&#31034;&#33258;&#21160;&#29983;&#25104;IRT&#30340;&#21161;&#29702;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;GIRT-Instruct&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#25351;&#31034;&#21644;IRT&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;IRT&#26469;&#33258;GitHub&#23384;&#20648;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;GIRT-Instruct&#26469;&#25351;&#23548;&#35843;&#25972;T5-base&#27169;&#22411;&#20197;&#21019;&#24314;GIRT-&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;GIRT-&#27169;&#22411;&#22312;IRT&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;&#24102;&#26377;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;T5&#21644;Flan-T5&#65289;&#65292;&#22312;ROUGE&#12289;BLEU&#12289;METEOR&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#24471;&#20998;&#26174;&#33879;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GIRT-&#27169;&#22411;&#22312;&#29992;&#25143;&#20307;&#39564;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs) to enable more effective issue management and better alignment with developer expectations. However, these templates are not widely adopted in most repositories, and there is currently no tool available to aid developers in generating them. In this work, we introduce GIRT-Model, an assistant language model that automatically generates IRTs based on the developer's instructions regarding the structure and necessary fields. We create GIRT-Instruct, a dataset comprising pairs of instructions and IRTs, with the IRTs sourced from GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model to create the GIRT-Model. In our experiments, GIRT-Model outperforms general language models (T5 and Flan-T5 with different parameter sizes) in IRT generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a user st
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;Transformer RNNs&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#26368;&#23567;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.02625</link><description>&lt;p&gt;
&#29992;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#22686;&#24378;Transformer RNNs
&lt;/p&gt;
&lt;p&gt;
Enhancing Transformer RNNs with Multiple Temporal Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02625
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;Transformer RNNs&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#26368;&#23567;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20854;&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#32500;&#25252;&#20808;&#21069;&#36935;&#21040;&#30340;&#25991;&#26412;&#30340;&#22810;&#26679;&#26102;&#38388;&#35270;&#22270;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#32435;&#20837;&#20102;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#35813;&#26550;&#26500;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#20445;&#30041;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#26368;&#23569;&#65288;&#20165;&#20026;&#26368;&#21021;&#21442;&#25968;&#25968;&#37327;&#30340;0.04%&#65289;&#65292;&#20063;&#23454;&#29616;&#20102;&#27492;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#25152;&#38656;&#30340;&#39069;&#22806;&#21442;&#25968;&#32463;&#36807;&#24494;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#23436;&#20840;&#39044;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#22312;&#25552;&#31034;&#25512;&#26029;&#36807;&#31243;&#20013;&#20445;&#25345;&#20102;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, en
&lt;/p&gt;</description></item><item><title>DenseFormer&#26159;&#23545;Transformer&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#36827;&#34892;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#23398;&#21040;&#30340;&#21152;&#26435;&#24179;&#22343;&#26435;&#37325;&#25581;&#31034;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#20351;&#24471;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#32988;&#36807;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02622</link><description>&lt;p&gt;
DenseFormer: &#36890;&#36807;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#22686;&#24378;Transformer&#20013;&#30340;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02622
&lt;/p&gt;
&lt;p&gt;
DenseFormer&#26159;&#23545;Transformer&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#36827;&#34892;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#23398;&#21040;&#30340;&#21152;&#26435;&#24179;&#22343;&#26435;&#37325;&#25581;&#31034;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#20351;&#24471;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#32988;&#36807;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;Vaswani&#31561;&#20154;&#65288;2017&#65289;&#30340;Transformer&#26550;&#26500;&#29616;&#24050;&#26222;&#36941;&#24212;&#29992;&#20110;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21040;&#35821;&#38899;&#22788;&#29702;&#21644;&#22270;&#20687;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DenseFormer&#65292;&#36825;&#26159;&#23545;&#26631;&#20934;&#26550;&#26500;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#65292;&#32780;&#19981;&#22686;&#21152;&#20854;&#22823;&#23567;-&#23545;&#20110;&#25317;&#26377;100B&#21442;&#25968;&#33539;&#22260;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#21482;&#38656;&#28155;&#21152;&#20960;&#21315;&#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;transformer&#22359;&#20043;&#21518;&#20381;&#38752;&#39069;&#22806;&#30340;&#24179;&#22343;&#27493;&#39588;&#65292;&#35745;&#31639;&#24403;&#21069;&#21644;&#36807;&#21435;&#34920;&#31034;&#30340;&#21152;&#26435;&#24179;&#22343;-&#25105;&#20204;&#23558;&#36825;&#20010;&#25805;&#20316;&#31216;&#20026;&#28145;&#24230;&#21152;&#26435;&#24179;&#22343;&#65288;DWA&#65289;&#12290;&#23398;&#21040;&#30340;DWA&#26435;&#37325;&#23637;&#29616;&#20102;&#20449;&#24687;&#27969;&#30340;&#36830;&#36143;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#36828;&#23618;&#30340;&#28608;&#27963;&#30340;&#24378;&#22823;&#19988;&#32467;&#26500;&#21270;&#30340;&#37325;&#22797;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;DenseFormer&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#33021;&#22815;&#36798;&#21040;&#27604;&#26356;&#28145;&#30340;transformer&#27169;&#22411;&#30456;&#21516;&#30340;&#22256;&#24785;&#24230;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#22256;&#24785;&#24230;&#19979;&#65292;&#36825;&#20123;&#26032;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;transformer&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#33258;&#30417;&#30563;&#22768;&#23398;&#21333;&#35789;&#23884;&#20837;&#36827;&#34892;&#36880;&#23618;&#20998;&#26512;&#65292;&#25506;&#32034;&#20854;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;AWEs&#19982;&#21407;&#22987;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#29702;&#21033;&#29992;AWEs&#19982;&#21333;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02617</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22768;&#23398;&#21333;&#35789;&#23884;&#20837;&#30340;&#36880;&#23618;&#20998;&#26512;&#65306;&#22522;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Layer-Wise Analysis of Self-Supervised Acoustic Word Embeddings: A Study on Speech Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#33258;&#30417;&#30563;&#22768;&#23398;&#21333;&#35789;&#23884;&#20837;&#36827;&#34892;&#36880;&#23618;&#20998;&#26512;&#65292;&#25506;&#32034;&#20854;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;AWEs&#19982;&#21407;&#22987;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#29702;&#21033;&#29992;AWEs&#19982;&#21333;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#24050;&#32463;&#24471;&#21040;&#39564;&#35777;&#65292;&#20294;&#20854;&#34920;&#31034;&#30340;&#26368;&#20339;&#21033;&#29992;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22768;&#23398;&#21333;&#35789;&#23884;&#20837;&#65288;AWEs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#36830;&#32493;&#34920;&#31034;&#20013;&#24471;&#20986;&#30340;&#22266;&#23450;&#38271;&#24230;&#29305;&#24449;&#65292;&#20197;&#25506;&#32034;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;AWEs&#22312;&#25429;&#25417;&#22768;&#23398;&#21487;&#36776;&#24615;&#26041;&#38754;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34913;&#37327;AWEs&#19982;&#21333;&#35789;&#23884;&#20837;&#20043;&#38388;&#36880;&#23618;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36827;&#19968;&#27493;&#30740;&#31350;AWEs&#20013;&#22266;&#26377;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#35821;&#26009;&#24211;IEMOCAP&#21644;ESD&#30340;&#27604;&#36739;&#23454;&#39564;&#21644;&#36880;&#23618;&#20934;&#30830;&#24615;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;AWEs&#19982;&#20854;&#20182;&#31867;&#22411;&#35821;&#38899;&#29305;&#24449;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#36129;&#29486;&#65292;&#24182;&#30740;&#31350;&#20102;AWEs&#19982;&#21333;&#35789;&#23884;&#20837;&#30340;&#21512;&#29702;&#21033;&#29992;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;AWEs&#19982;&#21407;&#22987;&#33258;&#30417;&#30563;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;AWEs&#21333;&#29420;&#25110;&#19982;&#21333;&#35789;&#23884;&#20837;&#32467;&#21512;&#20351;&#29992;&#30340;&#27491;&#30830;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficacy of self-supervised speech models has been validated, yet the optimal utilization of their representations remains challenging across diverse tasks. In this study, we delve into Acoustic Word Embeddings (AWEs), a fixed-length feature derived from continuous representations, to explore their advantages in specific tasks. AWEs have previously shown utility in capturing acoustic discriminability. In light of this, we propose measuring layer-wise similarity between AWEs and word embeddings, aiming to further investigate the inherent context within AWEs. Moreover, we evaluate the contribution of AWEs, in comparison to other types of speech features, in the context of Speech Emotion Recognition (SER). Through a comparative experiment and a layer-wise accuracy analysis on two distinct corpora, IEMOCAP and ESD, we explore differences between AWEs and raw self-supervised representations, as well as the proper utilization of AWEs alone and in combination with word embeddings. Our fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02611</link><description>&lt;p&gt;
PuzzleBench&#65306;LLMs&#33021;&#21542;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#65292;&#22914;&#36923;&#36753;&#38382;&#31572;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65292;&#19968;&#20010;&#20363;&#23376;&#26159;&#27969;&#34892;&#30340;&#25968;&#29420;&#35868;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#26377;&#19968;&#20010;&#30001;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22522;&#30784;&#19968;&#38454;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#20363;&#21270;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#25165;&#33021;&#36798;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PuzzleBench&#65292;&#19968;&#20010;&#21253;&#21547;31&#20010;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#22312;&#31526;&#21495;&#27714;&#35299;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;LLMs&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#30456;&#24403;&#31967;&#31957;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Puzzle-LM&#65292;&#23427;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#24494;&#25991;&#26412;&#35268;&#33539;&#21270;&#20013;&#19981;&#21516;&#22768;&#23398;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#20339;&#30340;&#22768;&#23398;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02591</link><description>&lt;p&gt;
&#20851;&#20110;&#22768;&#23398;&#31639;&#27861;&#22312;&#24494;&#25991;&#26412;&#35268;&#33539;&#21270;&#20013;&#30340;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the performance of phonetic algorithms in microtext normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02591
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#24494;&#25991;&#26412;&#35268;&#33539;&#21270;&#20013;&#19981;&#21516;&#22768;&#23398;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#20339;&#30340;&#22768;&#23398;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#21338;&#31038;&#20132;&#32593;&#32476;&#19978;&#21457;&#24067;&#30340;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#26500;&#25104;&#20102;&#19968;&#31181;&#23453;&#36149;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#24494;&#25991;&#26412;&#36890;&#24120;&#20559;&#31163;&#20102;&#26631;&#20934;&#30340;&#35789;&#27719;&#21644;&#35821;&#27861;&#35268;&#21017;&#65292;&#22240;&#27492;&#20256;&#32479;&#30340;&#26234;&#33021;&#31995;&#32479;&#24456;&#38590;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24494;&#25991;&#26412;&#35268;&#33539;&#21270;&#23558;&#36825;&#20123;&#38750;&#26631;&#20934;&#24494;&#25991;&#26412;&#36716;&#21270;&#20026;&#26631;&#20934;&#30340;&#12289;&#20070;&#38754;&#33391;&#22909;&#30340;&#25991;&#26412;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#20351;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#32487;&#32493;&#36827;&#34892;&#24120;&#35268;&#22788;&#29702;&#12290;&#32771;&#34385;&#21040;&#38750;&#26631;&#20934;&#25991;&#26412;&#24418;&#25104;&#20013;&#22768;&#23398;&#29616;&#35937;&#30340;&#37325;&#35201;&#24615;&#65292;&#35268;&#33539;&#21270;&#22120;&#30340;&#30693;&#35782;&#24211;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20803;&#32032;&#26159;&#32534;&#30721;&#36825;&#20123;&#29616;&#35937;&#30340;&#22768;&#23398;&#35268;&#21017;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#25152;&#35859;&#30340;&#22768;&#23398;&#31639;&#27861;&#20013;&#25214;&#21040;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#33521;&#35821;&#35821;&#35328;&#36827;&#34892;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#22768;&#23398;&#31639;&#27861;&#23454;&#39564;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#30830;&#23450;&#24494;&#25991;&#26412;&#35268;&#33539;&#21270;&#20505;&#36873;&#29983;&#25104;&#20013;&#30340;&#26368;&#20339;&#22768;&#23398;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
User-generated content published on microblogging social networks constitutes a priceless source of information. However, microtexts usually deviate from the standard lexical and grammatical rules of the language, thus making its processing by traditional intelligent systems very difficult. As an answer, microtext normalization consists in transforming those non-standard microtexts into standard well-written texts as a preprocessing step, allowing traditional approaches to continue with their usual processing. Given the importance of phonetic phenomena in non-standard text formation, an essential element of the knowledge base of a normalizer would be the phonetic rules that encode these phenomena, which can be found in the so-called phonetic algorithms.   In this work we experiment with a wide range of phonetic algorithms for the English language. The aim of this study is to determine the best phonetic algorithms within the context of candidate generation for microtext normalization. I
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35745;&#31639;&#25991;&#26412;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#32654;&#22269;&#21382;&#21490;&#25253;&#32440;&#20013;&#20122;&#27954;&#24037;&#20154;&#30340;&#21576;&#29616;&#36827;&#34892;&#20102;&#23450;&#37327;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#8220;&#33510;&#21147;&#8221;&#19968;&#35789;&#22312;&#19981;&#21516;&#24030;&#30340;&#35821;&#20041;&#26377;&#25152;&#24046;&#24322;&#65292;&#19981;&#21516;&#30340;&#35805;&#35821;&#22260;&#32469;&#30528;&#8220;&#33510;&#21147;&#8221;&#12290;&#27492;&#22806;&#65292;&#21335;&#26041;&#32852;&#37030;&#25253;&#32440;&#21644;&#21271;&#26041;&#32852;&#37030;&#25253;&#32440;&#24418;&#25104;&#20102;&#29420;&#29305;&#30340;&#35805;&#35821;&#65292;&#20122;&#27954;&#20154;&#22312;&#20854;&#20013;&#34987;&#35748;&#20026;&#20302;&#20110;&#27431;&#27954;&#31227;&#27665;&#65292;&#24182;&#21463;&#21040;&#31181;&#26063;&#20027;&#20041;&#30340;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.02572</link><description>&lt;p&gt;
&#32654;&#22269;&#21382;&#21490;&#25253;&#32440;&#20013;&#20122;&#27954;&#24037;&#20154;&#30340;&#23450;&#37327;&#35805;&#35821;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Quantitative Discourse Analysis of Asian Workers in the US Historical Newspapers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35745;&#31639;&#25991;&#26412;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#32654;&#22269;&#21382;&#21490;&#25253;&#32440;&#20013;&#20122;&#27954;&#24037;&#20154;&#30340;&#21576;&#29616;&#36827;&#34892;&#20102;&#23450;&#37327;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#8220;&#33510;&#21147;&#8221;&#19968;&#35789;&#22312;&#19981;&#21516;&#24030;&#30340;&#35821;&#20041;&#26377;&#25152;&#24046;&#24322;&#65292;&#19981;&#21516;&#30340;&#35805;&#35821;&#22260;&#32469;&#30528;&#8220;&#33510;&#21147;&#8221;&#12290;&#27492;&#22806;&#65292;&#21335;&#26041;&#32852;&#37030;&#25253;&#32440;&#21644;&#21271;&#26041;&#32852;&#37030;&#25253;&#32440;&#24418;&#25104;&#20102;&#29420;&#29305;&#30340;&#35805;&#35821;&#65292;&#20122;&#27954;&#20154;&#22312;&#20854;&#20013;&#34987;&#35748;&#20026;&#20302;&#20110;&#27431;&#27954;&#31227;&#27665;&#65292;&#24182;&#21463;&#21040;&#31181;&#26063;&#20027;&#20041;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#35770;&#25991;&#21253;&#21547;&#38024;&#23545;&#36793;&#32536;&#21270;&#32676;&#20307;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#31034;&#20363;&#12290;&#21382;&#21490;&#25991;&#26412;&#30340;&#25968;&#23383;&#21270;&#36992;&#35831;&#30740;&#31350;&#20154;&#21592;&#29992;&#35745;&#31639;&#26041;&#27861;&#25506;&#32034;&#22823;&#35268;&#27169;&#21382;&#21490;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;&#26412;&#30740;&#31350;&#23558;&#35745;&#31639;&#25991;&#26412;&#20998;&#26512;&#24212;&#29992;&#20110;&#30456;&#23545;&#23569;&#26377;&#30740;&#31350;&#30340;&#35805;&#39064;&#65306;&#25506;&#31350;&#20122;&#27954;&#24037;&#20154;&#22312;&#32654;&#22269;&#21382;&#21490;&#25253;&#32440;&#20013;&#30340;&#21576;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#8220;&#33510;&#21147;&#8221;&#19968;&#35789;&#22312;&#26576;&#20123;&#24030;&#65288;&#22914;&#39532;&#33832;&#35832;&#22622;&#24030;&#12289;&#32599;&#24471;&#23707;&#24030;&#12289;&#24576;&#20420;&#26126;&#24030;&#12289;&#20420;&#20811;&#25289;&#33655;&#39532;&#24030;&#21644;&#38463;&#32943;&#33394;&#24030;&#65289;&#30340;&#35821;&#20041;&#19981;&#21516;&#65292;&#19981;&#21516;&#30340;&#35805;&#35821;&#22260;&#32469;&#30528;&#8220;&#33510;&#21147;&#8221;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#26102;&#30340;&#21335;&#26041;&#32852;&#37030;&#25253;&#32440;&#21644;&#21271;&#26041;&#32852;&#37030;&#25253;&#32440;&#36890;&#36807;&#27979;&#37327;&#36807;&#24230;&#20195;&#34920;&#30340;&#35789;&#24418;&#25104;&#20102;&#29420;&#29305;&#30340;&#35805;&#35821;&#12290;&#26469;&#33258;&#21335;&#26041;&#32852;&#37030;&#30340;&#25253;&#32440;&#23558;&#33510;&#21147;&#19982;&#19982;&#22900;&#38582;&#21046;&#26377;&#20851;&#30340;&#35789;&#35821;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20122;&#27954;&#20154;&#34987;&#35748;&#20026;&#20302;&#20110;&#27431;&#27954;&#31227;&#27665;&#65292;&#24182;&#21463;&#21040;&#31181;&#26063;&#20027;&#20041;&#30340;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#26377;&#21161;&#20110;&#34917;&#20805;&#23450;&#24615;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#21382;&#21490;&#25253;&#32440;&#20013;&#20122;&#27954;&#24037;&#20154;&#21576;&#29616;&#26041;&#24335;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Warning: This paper contains examples of offensive language targetting marginalized population. The digitization of historical texts invites researchers to explore the large-scale corpus of historical texts with computational methods. In this study, we present computational text analysis on a relatively understudied topic of how Asian workers are represented in historical newspapers in the United States. We found that the word "coolie" was semantically different in some States (e.g., Massachusetts, Rhode Island, Wyoming, Oklahoma, and Arkansas) with the different discourses around coolie. We also found that then-Confederate newspapers and then-Union newspapers formed distinctive discourses by measuring over-represented words. Newspapers from then-Confederate States associated coolie with slavery-related words. In addition, we found Asians were perceived to be inferior to European immigrants and subjected to the target of racism. This study contributes to supplementing the qualitative a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#65292;&#20445;&#30041;&#20102;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20197;&#24448;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02564</link><description>&lt;p&gt;
&#19968;&#20010;&#30495;&#27491;&#32852;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#20998;&#21106;&#21644;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Truly Joint Neural Architecture for Segmentation and Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#65292;&#20445;&#30041;&#20102;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20197;&#24448;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#22810;&#35821;&#35328;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#21487;&#20197;&#35299;&#26512;&#22810;&#31181;&#35821;&#35328;&#65292;&#20294;&#23545;&#20110;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#32780;&#35328;&#65292;&#20854;&#24615;&#33021;&#26126;&#26174;&#20302;&#20110;&#20854;&#20182;&#35821;&#35328;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#30001;&#20110;&#36755;&#20837;&#26631;&#35760;&#30340;&#24418;&#24577;&#22797;&#26434;&#24615;&#21644;&#27169;&#31946;&#24615;&#36739;&#39640;&#65292;&#20316;&#20026;&#26641;&#20013;&#33410;&#28857;&#30340;&#35821;&#35328;&#21333;&#20301;&#20107;&#20808;&#26159;&#26410;&#30693;&#30340;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#36981;&#24490;&#32852;&#21512;&#24418;&#24577;-&#21477;&#27861;&#20551;&#35774;&#65292;&#21363;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#24212;&#35813;&#22312;&#35299;&#26512;&#36807;&#31243;&#20013;&#19968;&#24182;&#35299;&#20915;&#65292;&#32780;&#19981;&#26159;&#20808;&#36827;&#34892;&#20998;&#21106;&#20877;&#36827;&#34892;&#35299;&#26512;&#30340;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#37319;&#29992;&#20005;&#26684;&#30340;&#27969;&#27700;&#32447;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#20445;&#30041;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#25552;&#20379;&#32473;&#19968;&#20010;&#22522;&#20110;&#24359;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#24076;&#20271;&#26469;&#35821;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#35821;&#35328;&#24418;&#24577;&#20016;&#23500;&#19988;&#27169;&#31946;&#24615;&#36739;&#39640;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NavHint&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#20195;&#29702;&#65292;&#36890;&#36807;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#22120;&#20026;&#23548;&#33322;&#20195;&#29702;&#25552;&#20379;&#38388;&#25509;&#30417;&#30563;&#65292;&#24110;&#21161;&#20854;&#23545;&#35270;&#35273;&#29615;&#22659;&#36827;&#34892;&#25972;&#20307;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#22312;R2R&#21644;R4R&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.02559</link><description>&lt;p&gt;
NavHint: &#20855;&#26377;&#25552;&#31034;&#29983;&#25104;&#22120;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
NavHint: Vision and Language Navigation Agent with a Hint Generator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NavHint&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#20195;&#29702;&#65292;&#36890;&#36807;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#22120;&#20026;&#23548;&#33322;&#20195;&#29702;&#25552;&#20379;&#38388;&#25509;&#30417;&#30563;&#65292;&#24110;&#21161;&#20854;&#23545;&#35270;&#35273;&#29615;&#22659;&#36827;&#34892;&#25972;&#20307;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#22312;R2R&#21644;R4R&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;&#19982;&#23548;&#33322;&#30456;&#20851;&#30340;&#25439;&#22833;&#65292;&#20197;&#24314;&#31435;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24573;&#30053;&#20102;&#24110;&#21161;&#23548;&#33322;&#20195;&#29702;&#24314;&#31435;&#23545;&#35270;&#35273;&#29615;&#22659;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#26041;&#38754;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#22120;&#20026;&#23548;&#33322;&#20195;&#29702;&#25552;&#20379;&#38388;&#25509;&#30417;&#30563;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#35270;&#35273;&#25551;&#36848;&#12290;&#25552;&#31034;&#29983;&#25104;&#22120;&#24110;&#21161;&#23548;&#33322;&#20195;&#29702;&#21457;&#23637;&#23545;&#35270;&#35273;&#29615;&#22659;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#23427;&#24341;&#23548;&#20195;&#29702;&#27880;&#24847;&#30456;&#20851;&#30340;&#23548;&#33322;&#32454;&#33410;&#65292;&#21253;&#25324;&#30456;&#20851;&#30340;&#23376;&#25351;&#20196;&#12289;&#35782;&#21035;&#20013;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#22522;&#20110;&#22320;&#38754;&#30340;&#27495;&#20041;&#65292;&#20197;&#21450;&#30446;&#26631;&#35270;&#28857;&#25551;&#36848;&#12290;&#20026;&#20102;&#35757;&#32451;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#22522;&#20110;&#25351;&#20196;&#20013;&#30340;&#22320;&#26631;&#21644;&#35270;&#35273;&#29615;&#22659;&#20013;&#21487;&#35265;&#19988;&#26377;&#21306;&#21035;&#30340;&#23545;&#35937;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;R2R&#21644;R4R&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20960;&#20010;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment. In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions. The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent's attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment. We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#20020;&#24202;&#35797;&#39564;&#21644;&#27169;&#22411;&#23545;&#33258;&#28982;&#36923;&#36753;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.02558</link><description>&lt;p&gt;
&#25552;&#21319;&#29983;&#29289;&#21307;&#23398;NLI&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#20020;&#24202;&#35797;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#20020;&#24202;&#35797;&#39564;&#21644;&#27169;&#22411;&#23545;&#33258;&#28982;&#36923;&#36753;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#34892;&#19994;&#20013;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#22914;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#21830;&#19994;&#26234;&#33021;&#21644;&#21307;&#23398;&#31561;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#24212;&#29992;&#26159;&#20998;&#26512;&#21644;&#35843;&#26597;&#19982;&#34164;&#21547;&#20219;&#21153;&#30456;&#20851;&#30340;&#20020;&#24202;&#35797;&#39564;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#25463;&#24452;&#23398;&#20064;&#12289;&#20107;&#23454;&#19981;&#19968;&#33268;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#21363;&#20415;&#22312;&#19978;&#19979;&#25991;&#21464;&#21270;&#24456;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#34429;&#28982;&#36827;&#34892;&#20102;&#23545;&#25239;&#24615;&#21644;&#40065;&#26834;&#24615;&#27979;&#35797;&#20197;&#30830;&#20445;&#27169;&#22411;&#30340;&#36755;&#20986;&#23436;&#25972;&#24615;&#65292;&#20294;&#27169;&#31946;&#24615;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#30830;&#20445;&#25512;&#29702;&#30340;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#30340;&#21477;&#27861;&#35821;&#20041;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25506;&#27979;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20351;&#29992;&#20102;&#35760;&#24518;&#25506;&#27979;&#26041;&#27861;&#26469;&#30740;&#31350;&#22312;&#20020;&#24202;&#35797;&#39564;&#19978;&#35757;&#32451;&#30340;Sci-five&#27169;&#22411;&#12290;&#25105;&#35843;&#26597;&#20102;&#35813;&#27169;&#22411;&#22312;&#33258;&#28982;&#36923;&#36753;&#26041;&#38754;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#23454;&#29616;&#30446;&#26631;&#65292;&#25105;&#35757;&#32451;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#25506;&#27979;&#22120;&#12290;&#20351;&#29992;&#36825;&#20123;&#25506;&#27979;&#22120;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have revolutionized various fields and industries, such as Conversational AI, Content Generation, Information Retrieval, Business Intelligence, and Medical, to name a few. One major application in the field of medical is to analyze and investigate clinical trials for entailment tasks.However, It has been observed that Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context. Adversarial and robust testing is performed to ensure the integrity of models output. But, ambiguity still persists. In order to ensure the integrity of the reasoning performed and investigate the model has correct syntactic and semantic understanding probing is used. Here, I used mnestic probing to investigate the Sci-five model, trained on clinical trial. I investigated the model for feature learnt with respect to natural logic. To achieve the target, I trained task specific probes. Used these probes to in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#34920;&#26684;&#30340;&#20107;&#23454;&#26816;&#26597;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.02549</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#36866;&#21512;&#22522;&#20110;&#34920;&#26684;&#30340;&#20107;&#23454;&#26816;&#26597;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Table-based Fact-Checkers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#34920;&#26684;&#30340;&#20107;&#23454;&#26816;&#26597;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#30340;&#20107;&#23454;&#39564;&#35777;&#65288;TFV&#65289;&#26088;&#22312;&#25552;&#21462;&#35821;&#21477;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#12290;&#29616;&#26377;&#22522;&#20110;&#23567;&#35268;&#27169;&#27169;&#22411;&#30340;TFV&#26041;&#27861;&#22312;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#21644;&#38646;&#26679;&#26412;&#33021;&#21147;&#34180;&#24369;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30740;&#31350;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#20204;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;TFV&#39046;&#22495;&#30340;&#28508;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;LLMs&#26159;&#21542;&#36866;&#21512;&#20316;&#20026;&#22522;&#20110;&#34920;&#26684;&#30340;&#20107;&#23454;&#26816;&#26597;&#22120;&#30340;&#21021;&#27493;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#35821;&#26469;&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#24110;&#21161;LLMs&#22312;TFV&#26041;&#38754;&#65292;&#21363;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;TFV&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#21644;&#26500;&#24314;&#20102;TFV&#25351;&#23548;&#20197;&#30740;&#31350;LLMs&#30340;&#25351;&#23548;&#35843;&#25972;&#24102;&#26469;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#65292;LLMs&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;TFV&#26041;&#38754;&#21487;&#20197;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#32467;&#26524;&#65292;&#32780;&#25351;&#23548;&#35843;&#25972;&#21017;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables. Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers. In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tun
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#29615;&#22659;&#23545;&#20110;&#25913;&#36827;&#22522;&#20110;&#23454;&#36341;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#25968;&#25454;&#37319;&#38598;&#21644;&#27169;&#22411;&#24320;&#21457;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#37319;&#29992;&#29983;&#24577;&#26041;&#27861;&#30740;&#31350;&#22522;&#20110;&#23454;&#36341;&#30340;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#22312;&#33258;&#28982;/&#27169;&#25311;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.02548</link><description>&lt;p&gt;
"&#25105;&#30340;&#27169;&#22411;&#22312;&#20160;&#20040;&#37324;&#38754;&#65311;": &#25506;&#32034;&#29615;&#22659;&#22312;&#22522;&#20110;&#23454;&#36341;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
"What's my model inside of?": Exploring the role of environments for grounded natural language understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#29615;&#22659;&#23545;&#20110;&#25913;&#36827;&#22522;&#20110;&#23454;&#36341;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#25968;&#25454;&#37319;&#38598;&#21644;&#27169;&#22411;&#24320;&#21457;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#37319;&#29992;&#29983;&#24577;&#26041;&#27861;&#30740;&#31350;&#22522;&#20110;&#23454;&#36341;&#30340;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#22312;&#33258;&#28982;/&#27169;&#25311;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#30740;&#31350;&#23396;&#31435;&#30340;&#22823;&#33041;&#30340;&#32463;&#20856;&#35748;&#30693;&#31185;&#23398;&#19981;&#21516;&#65292;&#29983;&#24577;&#26041;&#27861;&#20391;&#37325;&#20110;&#36523;&#20307;&#21644;&#29615;&#22659;&#22312;&#22609;&#36896;&#35748;&#30693;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;&#21516;&#26679;&#65292;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#29983;&#24577;&#26041;&#27861;&#26469;&#30740;&#31350;&#22522;&#20110;&#23454;&#36341;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#12290;&#22522;&#20110;&#23454;&#36341;&#30340;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#23558;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#32622;&#20110;&#33258;&#28982;/&#27169;&#25311;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#20107;&#20214;&#12289;&#21160;&#20316;&#21644;&#39044;&#26399;&#20043;&#20013;&#12290;&#19982;&#32463;&#20856;&#30740;&#31350;&#20542;&#21521;&#20110;&#19987;&#27880;&#20110;&#35774;&#35745;&#26032;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#23558;&#29615;&#22659;&#35270;&#20026;&#32473;&#23450;&#30340;&#24773;&#20917;&#19981;&#21516;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29615;&#22659;&#35774;&#35745;&#23545;&#20110;&#25913;&#36827;&#25968;&#25454;&#37319;&#38598;&#21644;&#27169;&#22411;&#24320;&#21457;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#22522;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#29615;&#22659;&#24320;&#21457;&#20102;&#26032;&#30340;&#35757;&#32451;&#21644;&#26631;&#27880;&#26041;&#27861;&#65292;&#29992;&#20110;&#31243;&#24207;&#21270;&#25991;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#21442;&#32771;&#20102;&#22522;&#20110;&#20307;&#39564;&#30340;&#35748;&#30693;&#35821;&#35328;&#23398;&#25991;&#29486;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23454;&#36341;&#30340;NLP&#30740;&#31350;&#36335;&#32447;&#22270;&#65292;&#24182;&#20026;&#34913;&#37327;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to classical cognitive science which studied brains in isolation, ecological approaches focused on the role of the body and environment in shaping cognition. Similarly, in this thesis we adopt an ecological approach to grounded natural language understanding (NLU) research. Grounded language understanding studies language understanding systems situated in the context of events, actions and precepts in naturalistic/simulated virtual environments. Where classic research tends to focus on designing new models and optimization methods while treating environments as given, we explore the potential of environment design for improving data collection and model development. We developed novel training and annotation approaches for procedural text understanding based on text-based game environments. We also drew upon embodied cognitive linguistics literature to propose a roadmap for grounded NLP research, and to inform the development of a new benchmark for measuring the progress of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30693;&#35782;&#24182;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#29983;&#25104;&#30340;&#30693;&#35782;&#30456;&#20851;&#19988;&#26377;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.02541</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#30693;&#35782;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge Generation for Zero-shot Knowledge-based VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30693;&#35782;&#24182;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#29983;&#25104;&#30340;&#30693;&#35782;&#30456;&#20851;&#19988;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30693;&#35782;&#21270;&#22522;&#20110;&#35270;&#35273;&#38382;&#31572;&#65288;K-VQA&#65289;&#35299;&#20915;&#26041;&#26696;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#35757;&#32451;K-VQA&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#34987;&#29992;&#20316;K-VQA&#30340;&#30693;&#35782;&#28304;&#21644;&#38646;&#26679;&#26412;QA&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#36817;&#30340;&#26041;&#27861;&#27809;&#26377;&#26126;&#30830;&#23637;&#31034;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#21463;&#21040;&#26368;&#36817;&#20851;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;QA&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#22522;&#20110;&#30693;&#35782;&#29983;&#25104;&#30340;K-VQA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#29983;&#25104;&#30693;&#35782;&#65292;&#28982;&#21518;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#29983;&#25104;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;K-VQA&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;K-VQA&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20043;&#21069;&#30340;&#38646;&#26679;&#26412;K-VQA&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25105;&#20204;&#29983;&#25104;&#30340;&#30693;&#35782;&#36890;&#24120;&#26159;&#30456;&#20851;&#19988;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous solutions to knowledge-based visual question answering~(K-VQA) retrieve knowledge from external knowledge bases and use supervised learning to train the K-VQA model. Recently pre-trained LLMs have been used as both a knowledge source and a zero-shot QA model for K-VQA and demonstrated promising results. However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability. Inspired by recent work on knowledge generation from LLMs for text-based QA, in this work we propose and test a similar knowledge-generation-based K-VQA method, which first generates knowledge from an LLM and then incorporates the generated knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA benchmarks and found that our method performs better than previous zero-shot K-VQA methods and our generated knowledge is generally relevant and helpful.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#20013;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#20309;&#26102;&#19981;&#20877;&#22686;&#21152;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25509;&#36817;&#26465;&#20214;&#26469;&#20272;&#31639;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#30340;&#25509;&#36817;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#27169;&#22411;&#36873;&#25321;&#20013;&#23398;&#20064;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.02522</link><description>&lt;p&gt;
&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#20013;&#30340;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;
&lt;/p&gt;
&lt;p&gt;
Absolute convergence and error thresholds in non-active adaptive sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02522
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#20013;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#20309;&#26102;&#19981;&#20877;&#22686;&#21152;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25509;&#36817;&#26465;&#20214;&#26469;&#20272;&#31639;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#30340;&#25509;&#36817;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#27169;&#22411;&#36873;&#25321;&#20013;&#23398;&#20064;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20027;&#21160;&#33258;&#36866;&#24212;&#37319;&#26679;&#26159;&#19968;&#31181;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21160;&#24577;&#21644;&#33258;&#21160;&#22320;&#30830;&#23450;&#20445;&#35777;&#30340;&#26679;&#26412;&#22823;&#23567;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26080;&#35770;&#25152;&#37319;&#29992;&#30340;&#35843;&#24230;&#21644;&#29983;&#25104;&#24369;&#39044;&#27979;&#22120;&#30340;&#31574;&#30053;&#22914;&#20309;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#35745;&#31639;&#32477;&#23545;&#25910;&#25947;&#21644;&#35823;&#24046;&#38408;&#20540;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#30340;&#36136;&#37327;&#20309;&#26102;&#19981;&#20877;&#22686;&#21152;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25509;&#36817;&#26465;&#20214;&#26469;&#32477;&#23545;&#22320;&#20272;&#31639;&#27169;&#22411;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#25509;&#36817;&#24230;&#65292;&#20174;&#32780;&#25903;&#25345;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#36827;&#34892;&#23398;&#20064;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;&#35813;&#25216;&#26415;&#22312;&#24037;&#20316;&#20551;&#35774;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#21644;&#23436;&#22791;&#24615;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#37319;&#26679;&#26041;&#26696;&#30340;&#40065;&#26834;&#24615;&#12290;&#27979;&#35797;&#32467;&#26524;&#31526;&#21512;&#25105;&#20204;&#30340;&#39044;&#26399;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#35789;&#24615;&#26631;&#27880;&#29983;&#25104;&#20026;&#26696;&#20363;&#30740;&#31350;&#26469;&#35828;&#26126;&#36825;&#19968;&#25552;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size. In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described. We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection. The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme. Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35843;&#24230;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#20013;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#37319;&#26679;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20998;&#26512;&#23398;&#20064;&#26354;&#32447;&#30340;&#24418;&#29366;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#22686;&#21152;&#25110;&#20943;&#23569;&#23454;&#20363;&#65292;&#20197;&#30830;&#20445;&#33719;&#24471;&#23398;&#20064;&#33021;&#21147;&#30340;&#20928;&#22686;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.02516</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#35843;&#24230;&#29992;&#20110;&#33258;&#36866;&#24212;&#37319;&#26679;&#22312;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#20013;
&lt;/p&gt;
&lt;p&gt;
Adaptive scheduling for adaptive sampling in POS taggers construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35843;&#24230;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#20013;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#37319;&#26679;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20998;&#26512;&#23398;&#20064;&#26354;&#32447;&#30340;&#24418;&#29366;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#22686;&#21152;&#25110;&#20943;&#23569;&#23454;&#20363;&#65292;&#20197;&#30830;&#20445;&#33719;&#24471;&#23398;&#20064;&#33021;&#21147;&#30340;&#20928;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35843;&#24230;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#20316;&#20026;&#26500;&#24314;&#35789;&#24615;&#26631;&#27880;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#19981;&#26174;&#33879;&#25439;&#22833;&#24615;&#33021;&#12290;&#19982;&#20043;&#21069;&#20351;&#29992;&#38543;&#26426;&#12289;&#22266;&#23450;&#25110;&#23450;&#26399;&#22686;&#21152;&#23454;&#20363;&#20043;&#38388;&#38388;&#38548;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20309;&#19978;&#20998;&#26512;&#23398;&#20064;&#26354;&#32447;&#30340;&#24418;&#29366;&#65292;&#32467;&#21512;&#21151;&#33021;&#27169;&#22411;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#22686;&#21152;&#25110;&#20943;&#23569;&#23454;&#20363;&#12290;&#35813;&#31639;&#27861;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20551;&#35774;&#19978;&#34987;&#35777;&#26126;&#26159;&#24418;&#24335;&#19978;&#27491;&#30830;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#26696;&#20363;&#65292;&#19979;&#19968;&#20010;&#26696;&#20363;&#26159;&#26368;&#36817;&#30340;&#65292;&#30830;&#20445;&#20174;&#21069;&#32773;&#20013;&#33719;&#24471;&#23398;&#20064;&#33021;&#21147;&#30340;&#20928;&#22686;&#30410;&#65292;&#21487;&#20197;&#35843;&#33410;&#27492;&#26465;&#20214;&#30340;&#35201;&#27714;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26356;&#21152;&#20851;&#27880;&#22312;&#35757;&#32451;&#25968;&#25454;&#24211;&#20013;&#20020;&#26102;&#24615;&#33021;&#33192;&#32960;&#30340;&#21306;&#22495;&#65292;&#25552;&#39640;&#20102;&#37319;&#26679;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#38450;&#27490;&#23398;&#20064;&#25552;&#21069;&#20572;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an adaptive scheduling for adaptive sampling as a novel way of machine learning in the construction of part-of-speech taggers. The goal is to speed up the training on large data sets, without significant loss of performance with regard to an optimal configuration. In contrast to previous methods using a random, fixed or regularly rising spacing between the instances, ours analyzes the shape of the learning curve geometrically in conjunction with a functional model to increase or decrease it at any time. The algorithm proves to be formally correct regarding our working hypotheses. Namely, given a case, the following one is the nearest ensuring a net gain of learning ability from the former, it being possible to modulate the level of requirement for this condition. We also improve the robustness of sampling by paying greater attention to those regions of the training data base subject to a temporary inflation in performance, thus preventing the learning from stopping prematu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#23398;&#20064;&#26354;&#32447;&#28436;&#21270;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32467;&#26524;&#26469;&#39044;&#27979;&#23398;&#20064;&#36807;&#31243;&#20013;&#36798;&#21040;&#26399;&#26395;&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.02515</link><description>&lt;p&gt;
&#23398;&#20064;&#26354;&#32447;&#24314;&#27169;&#21450;&#20854;&#22312;&#35789;&#24615;&#26631;&#27880;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Modeling of learning curves with applications to pos tagging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#23398;&#20064;&#26354;&#32447;&#28436;&#21270;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32467;&#26524;&#26469;&#39044;&#27979;&#23398;&#20064;&#36807;&#31243;&#20013;&#36798;&#21040;&#26399;&#26395;&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32467;&#26524;&#21644;&#20351;&#29992;&#21151;&#33021;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#23398;&#20064;&#26354;&#32447;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#36924;&#36817;&#25152;&#38656;&#26102;&#38388;&#28857;&#30340;&#24453;&#27714;&#20540;&#65292;&#29420;&#31435;&#20110;&#25152;&#20351;&#29992;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#32463;&#36807;&#19968;&#23450;&#30340;&#36807;&#31243;&#28857;&#65288;&#31216;&#20026;&#39044;&#27979;&#32423;&#21035;&#65289;&#21518;&#12290;&#35813;&#25552;&#26696;&#22312;&#24037;&#20316;&#20551;&#35774;&#26041;&#38754;&#34987;&#35777;&#26126;&#26159;&#24418;&#24335;&#19978;&#27491;&#30830;&#30340;&#65292;&#24182;&#19988;&#21253;&#21547;&#19968;&#20010;&#21487;&#38752;&#30340;&#36817;&#20284;&#26465;&#20214;&#12290;&#36825;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#22522;&#20110;&#26368;&#32456;&#21487;&#23454;&#29616;&#30340;&#20934;&#30830;&#24230;&#26469;&#35774;&#23450;&#25910;&#25947;&#38408;&#20540;&#65292;&#36825;&#25193;&#23637;&#20102;&#20572;&#27490;&#20934;&#21017;&#30340;&#27010;&#24565;&#65292;&#21363;&#20351;&#23384;&#22312;&#25197;&#26354;&#35266;&#23519;&#32467;&#26524;&#65292;&#20063;&#20284;&#20046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#22521;&#35757;&#24037;&#20316;&#37327;&#65292;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#26469;&#20943;&#23569;&#23398;&#20064;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#20154;&#21147;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#35813;&#25552;&#26696;&#22312;&#33267;&#23569;&#19977;&#20010;&#25805;&#20316;&#31243;&#24207;&#20013;&#24456;&#26377;&#20852;&#36259;&#12290;&#31532;&#19968;&#20010;&#26159;&#39044;&#27979;&#23398;&#20064;&#36807;&#31243;&#20013;&#36798;&#21040;&#26399;&#26395;&#20934;&#30830;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm to estimate the evolution of learning curves on the whole of a training data base, based on the results obtained from a portion and using a functional strategy, is introduced. We approximate iteratively the sought value at the desired time, independently of the learning technique used and once a point in the process, called prediction level, has been passed. The proposal proves to be formally correct with respect to our working hypotheses and includes a reliable proximity condition. This allows the user to fix a convergence threshold with respect to the accuracy finally achievable, which extends the concept of stopping criterion and seems to be effective even in the presence of distorting observations.   Our aim is to evaluate the training effort, supporting decision making in order to reduce the need for both human and computational resources during the learning process. The proposal is of interest in at least three operational procedures. The first is the anticipation of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31995;&#21015;&#22312;&#32447;&#25351;&#26631;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25552;&#21069;&#20572;&#27490;&#26465;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02513</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#30456;&#20851;&#22312;&#32447;&#25351;&#26631;&#26469;&#25552;&#21069;&#20572;&#27490;
&lt;/p&gt;
&lt;p&gt;
Early stopping by correlating online indicators in neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31995;&#21015;&#22312;&#32447;&#25351;&#26631;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25552;&#21069;&#20572;&#27490;&#26465;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26368;&#23567;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#22312;&#35757;&#32451;&#23398;&#20064;&#32773;&#26102;&#35782;&#21035;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#36825;&#20351;&#24471;&#25903;&#25345;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#25552;&#21069;&#20572;&#27490;&#26465;&#20214;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35813;&#31867;&#22411;&#24314;&#27169;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#21033;&#29992;&#19968;&#31995;&#21015;&#22312;&#32447;&#25351;&#26631;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#21363;&#29992;&#20110;&#25351;&#31034;&#19968;&#32452;&#20551;&#35774;&#26159;&#21542;&#28385;&#36275;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#19982;&#20174;&#37329;&#19997;&#38592;&#21028;&#26029;&#20013;&#26500;&#24314;&#30340;&#19968;&#31995;&#21015;&#29420;&#31435;&#20572;&#27490;&#26465;&#20214;&#30456;&#32852;&#31995;&#65292;&#20197;&#35780;&#20272;&#36807;&#25311;&#21512;&#30340;&#23384;&#22312;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#20026;&#20915;&#31574;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#30340;&#22522;&#30784;&#65292;&#20197;&#20013;&#26029;&#23398;&#20064;&#36807;&#31243;&#12290;&#19982;&#20043;&#21069;&#19987;&#27880;&#20110;&#21333;&#19968;&#26631;&#20934;&#30340;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#20043;&#38388;&#30340;&#38468;&#24102;&#25928;&#24212;&#65292;&#23547;&#27714;&#26356;&#24191;&#27867;&#30340;&#25805;&#20316;&#33539;&#22260;&#21644;&#26356;&#22823;&#30340;&#35786;&#26029;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to minimize the generalization error in neural networks, a novel technique to identify overfitting phenomena when training the learner is formally introduced. This enables support of a reliable and trustworthy early stopping condition, thus improving the predictive power of that type of modeling. Our proposal exploits the correlation over time in a collection of online indicators, namely characteristic functions for indicating if a set of hypotheses are met, associated with a range of independent stopping conditions built from a canary judgment to evaluate the presence of overfitting. That way, we provide a formal basis for decision making in terms of interrupting the learning process.   As opposed to previous approaches focused on a single criterion, we take advantage of subsidiarities between independent assessments, thus seeking both a wider operating range and greater diagnostic reliability. With a view to illustrating the effectiveness of the halting condition described, 
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#36164;&#28304;&#23454;&#39564;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26089;&#26399;&#23398;&#20064;&#26354;&#32447;&#20272;&#35745;&#20316;&#20026;&#36873;&#25321;&#26368;&#21512;&#36866;&#27169;&#22411;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#36139;&#20047;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02449</link><description>&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#24314;&#27169;PoS&#26631;&#35760;&#22120;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Surfing the modeling of PoS taggers in low-resource scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02449
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#23454;&#39564;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26089;&#26399;&#23398;&#20064;&#26354;&#32447;&#20272;&#35745;&#20316;&#20026;&#36873;&#25321;&#26368;&#21512;&#36866;&#27169;&#22411;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#36139;&#20047;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#32467;&#26500;&#25216;&#26415;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#36235;&#21183;&#25581;&#31034;&#20102;&#24222;&#22823;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20351;&#24471;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#37325;&#26032;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#20173;&#28982;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#35774;&#32622;&#20013;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#36873;&#25321;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#20197;&#22312;&#21512;&#29702;&#25104;&#26412;&#20869;&#25552;&#21319;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#28041;&#21450;&#21040;&#35757;&#32451;&#21644;/&#25110;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#30340;&#39046;&#22495;&#26102;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#26089;&#26399;&#23398;&#20064;&#26354;&#32447;&#20272;&#35745;&#20316;&#20026;&#22312;&#36164;&#28304;&#36139;&#20047;&#29615;&#22659;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#27169;&#22411;&#30340;&#23454;&#29992;&#26426;&#21046;&#12290;&#22522;&#20110;&#20808;&#21069;&#22312;&#35757;&#32451;&#21644;&#39564;&#35777;&#36164;&#28304;&#20805;&#36275;&#26465;&#20214;&#19979;&#35780;&#20272;&#30340;&#24418;&#24335;&#21270;&#36924;&#36817;&#27169;&#22411;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#19988;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent trend towards the application of deep structured techniques has revealed the limits of huge models in natural language processing. This has reawakened the interest in traditional machine learning algorithms, which have proved still to be competitive in certain contexts, in particular low-resource settings. In parallel, model selection has become an essential task to boost performance at reasonable cost, even more so when we talk about processes involving domains where the training and/or computational resources are scarce. Against this backdrop, we evaluate the early estimation of learning curves as a practical mechanism for selecting the most appropriate model in scenarios characterized by the use of non-deep learners in resource-lean settings. On the basis of a formal approximation model previously evaluated under conditions of wide availability of training and validation resources, we study the reliability of such an approach in a different and much more demanding operati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25913;&#36827;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;&#21644;&#20248;&#21270;&#22120;&#31561;&#21508;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24555;&#36895;&#22823;&#35268;&#27169;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26032;&#27700;&#24179;&#30340;BERT&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02447</link><description>&lt;p&gt;
&#25171;&#30772;MLPerf&#35757;&#32451;&#65306;&#20248;&#21270;BERT&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Breaking MLPerf Training: A Case Study on Optimizing BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02447
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;&#21644;&#20248;&#21270;&#22120;&#31561;&#21508;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24555;&#36895;&#22823;&#35268;&#27169;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26032;&#27700;&#24179;&#30340;BERT&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#36895;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#25913;&#36827;&#21253;&#25324;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;&#12289;&#20248;&#21270;&#22120;&#31561;&#35757;&#32451;&#30340;&#21508;&#20010;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24555;&#36895;&#22823;&#35268;&#27169;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#27599;&#20010;&#32452;&#20214;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;BERT&#35757;&#32451;&#24615;&#33021;&#30340;&#26032;&#27700;&#24179;&#12290;&#22312;&#20998;&#24067;&#24335;BERT&#35757;&#32451;&#20013;&#65292;&#36127;&#36733;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#26681;&#25454;&#19981;&#21516;&#38271;&#24230;&#30340;&#26679;&#26412;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#12290;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#35268;&#27169;&#25104;&#27491;&#27604;&#30340;&#36890;&#20449;&#25104;&#26412;&#38656;&#35201;&#36890;&#36807;&#26377;&#29992;&#30340;&#35745;&#31639;&#26469;&#38544;&#34255;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#22120;&#65292;&#22914;ADAM&#12289;LAMB&#31561;&#65292;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20180;&#32454;&#37325;&#26032;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#24819;&#27861;&#65292;&#21363;&#22522;&#20110;&#25968;&#25454;&#38598;&#20998;&#23618;&#30340;&#26412;&#22320;&#39044;&#25490;&#24207;&#36827;&#34892;&#36127;&#36733;&#24179;&#34913;&#21644;&#20840;&#32422;&#20943;&#20043;&#21069;&#30340;&#25353;&#26742;&#26799;&#24230;&#35009;&#21098;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#26799;&#24230;&#35745;&#31639;&#21644;&#21516;&#27493;&#30340;&#37325;&#21472;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speeding up the large-scale distributed training is challenging in that it requires improving various components of training including load balancing, communication, optimizers, etc. We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance. Load balancing is imperative in distributed BERT training since its training datasets are characterized by samples with various lengths. Communication cost, which is proportional to the scale of distributed training, needs to be hidden by useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc., need to be carefully re-evaluated in the context of large-scale distributed training. We propose two new ideas, (1) local presorting based on dataset stratification for load balancing and (2) bucket-wise gradient clipping before allreduce which allows us to benefit from the overlap of gradient computation and synchronization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30495;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25351;&#20986;&#20102;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#30340;&#38556;&#30861;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.02420</link><description>&lt;p&gt;
2024&#24180;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Factuality of Large Language Models in the Year 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30495;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25351;&#20986;&#20102;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#30340;&#38556;&#30861;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23588;&#20854;&#26159;&#22312;&#32842;&#22825;&#26041;&#38754;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#22320;&#26041;&#30452;&#25509;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#65292;&#20351;&#20154;&#20204;&#20174;&#25628;&#32034;&#12289;&#25552;&#21462;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#30340;&#36807;&#31243;&#20013;&#24471;&#21040;&#35299;&#33073;&#12290;&#28982;&#32780;&#65292;&#24456;&#22810;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#22238;&#31572;&#26159;&#38169;&#35823;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;LLM&#30495;&#23454;&#24615;&#30340;&#30740;&#31350;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#26088;&#22312;&#25214;&#20986;&#20027;&#35201;&#25361;&#25112;&#21450;&#20854;&#21407;&#22240;&#65292;&#24182;&#25351;&#20986;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#20998;&#26512;&#24320;&#25918;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#38754;&#20020;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#36824;&#23637;&#26395;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>GLaPE&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#30340;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#65292;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.02408</link><description>&lt;p&gt;
GLaPE&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02408
&lt;/p&gt;
&lt;p&gt;
GLaPE&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#30340;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#65292;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#20219;&#21153;&#24615;&#33021;&#20173;&#28982;&#23545;&#25552;&#31034;&#35774;&#35745;&#25935;&#24863;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;LLM&#33258;&#36523;&#20316;&#20026;&#20248;&#21270;&#22120;&#26469;&#35782;&#21035;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26368;&#20248;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#25552;&#31034;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#38590;&#20197;&#33719;&#21462;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#37329;&#26631;&#31614;&#65292;&#20197;&#35745;&#31639;&#27599;&#20010;&#20505;&#36873;&#25552;&#31034;&#30340;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;&#24191;&#27867;&#30340;&#23454;&#26045;&#21644;&#36890;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65288;GLaPE&#65289;&#65292;&#20197;&#20943;&#23569;&#23545;&#37329;&#26631;&#31614;&#30340;&#20381;&#36182;&#12290;&#21463;&#21040;&#33258;&#19968;&#33268;&#24615;&#21644;&#31572;&#26696;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#36827;&#34892;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GLaPE&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20845;&#20010;&#20219;&#21153;&#65292;GLaPE&#22312;&#32477;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#24471;&#21040;&#30340;&#35780;&#20272;&#32467;&#26524;&#19982;&#20351;&#29992;&#30495;&#23454;&#37329;&#26631;&#31614;&#35780;&#20272;&#30340;&#32467;&#26524;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality. To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels. Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score. Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent. Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels. Moreover, on six p
&lt;/p&gt;</description></item><item><title>DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02392</link><description>&lt;p&gt;
DeLLMa:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#20915;&#31574;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02392
&lt;/p&gt;
&lt;p&gt;
DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#12289;&#24037;&#31243;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#24448;&#24448;&#38754;&#20020;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20915;&#31574;&#38382;&#39064;&#19978;&#30452;&#25509;&#20351;&#29992;LLMs&#24448;&#24448;&#25928;&#26524;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeLLMa&#65288;Decision-making Large Language Model assistant&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#31934;&#24230;&#12290;DeLLMa&#21253;&#25324;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20102;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;&#12289;&#21487;&#23457;&#35745;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#20892;&#19994;&#21644;&#37329;&#34701;&#25968;&#25454;&#30340;&#20915;&#31574;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DeLLMa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#20915;&#31574;&#24615;&#33021;&#65292;&#20934;&#30830;&#24615;&#21487;&#25552;&#39640;&#39640;&#36798;40%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KICGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26816;&#32034;&#22120;&#30340;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#30693;&#35782;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#38271;&#23614;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02389</link><description>&lt;p&gt;
KICGPT: &#20855;&#22791;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KICGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26816;&#32034;&#22120;&#30340;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#30693;&#35782;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#38271;&#23614;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#23545;&#20110;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#19981;&#23436;&#25972;&#24615;&#21644;&#25903;&#25345;&#19979;&#28216;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#20004;&#31867;&#12290;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#26041;&#27861;&#30001;&#20110;&#32467;&#26500;&#20449;&#24687;&#26377;&#38480;&#21644;&#23454;&#20307;&#20998;&#24067;&#19981;&#22343;&#34913;&#32780;&#22256;&#38590;&#37325;&#37325;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#29305;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#24494;&#35843;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;KICGPT&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26816;&#32034;&#22120;&#30340;&#26694;&#26550;&#12290;&#23427;&#21487;&#20197;&#32531;&#35299;&#38271;&#23614;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;KICGPT&#20351;&#29992;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#31216;&#20026;&#30693;&#35782;&#25552;&#31034;&#65292;&#23427;&#23558;&#32467;&#26500;&#30693;&#35782;&#32534;&#30721;&#20026;&#28436;&#31034;&#65292;&#20197;&#24341;&#23548;LLM&#30340;&#23398;&#20064;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiven
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAGE&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;ABM&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#36741;&#21161;&#39564;&#35777;&#21644;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02388</link><description>&lt;p&gt;
&#20351;&#29992;&#36741;&#21161;&#39564;&#35777;&#30340;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#29983;&#25104;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAGE&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;ABM&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#36741;&#21161;&#39564;&#35777;&#21644;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#65288;ABM&#65289;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#25552;&#20986;&#21644;&#39564;&#35777;&#38024;&#23545;&#22797;&#26434;&#31995;&#32479;&#25152;&#25552;&#20986;&#30340;&#20551;&#35774;&#35299;&#20915;&#26041;&#26696;&#25110;&#25919;&#31574;&#65292;&#24182;&#23454;&#29616;&#21508;&#31181;&#30446;&#26631;&#12290;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#36328;&#23398;&#31185;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#20855;&#26377;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#32534;&#31243;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26377;&#28508;&#21147;&#20943;&#36731;&#36825;&#20010;&#36807;&#31243;&#30340;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;LLM&#25797;&#38271;&#22788;&#29702;&#24207;&#21015;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;ABM&#20013;&#30340;&#22797;&#26434;&#20132;&#20114;&#21644;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21478;&#22806;&#65292;&#30001;&#20110;LLM&#32570;&#20047;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#20165;&#20165;&#20381;&#38752;LLM&#26159;&#26080;&#27861;&#26377;&#25928;&#23436;&#25104;&#36825;&#20010;&#36807;&#31243;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAGE&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#38754;&#21521;&#35299;&#20915;&#26041;&#26696;&#30340;ABM&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02380</link><description>&lt;p&gt;
&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#26102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models in Analysing Classroom Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#25945;&#23398;&#35786;&#26029;&#21644;&#36136;&#37327;&#25913;&#36827;&#30340;&#37325;&#35201;&#30740;&#31350;&#20219;&#21153;&#12290;&#37492;&#20110;&#20256;&#32479;&#25945;&#32946;&#30740;&#31350;&#20013;&#30693;&#35782;&#23494;&#38598;&#21644;&#21171;&#21160;&#23494;&#38598;&#30340;&#23450;&#24615;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLM&#22312;&#20248;&#21270;&#21644;&#22686;&#24378;&#20998;&#26512;&#36807;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;&#20013;&#23398;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#23398;&#21644;&#35821;&#25991;&#35838;&#22530;&#19978;&#30340;&#23545;&#35805;&#12290;&#36825;&#20123;&#23545;&#35805;&#30001;&#25945;&#32946;&#19987;&#23478;&#25163;&#21160;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#21046;&#30340;GPT-4&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#27604;&#36739;&#25163;&#21160;&#27880;&#37322;&#19982;GPT-4&#30340;&#36755;&#20986;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20998;&#26512;&#25945;&#32946;&#23545;&#35805;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#35780;&#20272;&#26102;&#38388;&#25928;&#29575;&#12289;&#32534;&#30721;&#32773;&#38388;&#19968;&#33268;&#24615;&#21644;&#32534;&#30721;&#32773;&#38388;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GPT-4&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#24182;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#23454;&#20307;&#20013;&#24515;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#25552;&#20986;&#20102;&#35780;&#20272;PTLMs&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#30340;&#29702;&#24819;&#22522;&#20934;&#30340;&#26631;&#20934;&#65292;&#24182;&#20171;&#32461;&#20102;&#38024;&#23545;&#35813;&#35780;&#20272;&#30340;EC-FUNSD&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;PTLMs&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#20542;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.02379</link><description>&lt;p&gt;
&#20174;&#23454;&#20307;&#20013;&#24515;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluation of Pre-trained Text-and-Layout Models from an Entity-Centric Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#23454;&#20307;&#20013;&#24515;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#25552;&#20986;&#20102;&#35780;&#20272;PTLMs&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#30340;&#29702;&#24819;&#22522;&#20934;&#30340;&#26631;&#20934;&#65292;&#24182;&#20171;&#32461;&#20102;&#38024;&#23545;&#35813;&#35780;&#20272;&#30340;EC-FUNSD&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;PTLMs&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#22411;&#65288;PTLMs&#65289;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#19978;&#30340;&#22810;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20934;&#25968;&#25454;&#20013;&#30340;&#27880;&#37322;&#19981;&#36275;&#65292;&#30446;&#21069;&#30340;&#35780;&#20272;&#27969;&#31243;&#21487;&#33021;&#19981;&#22815;&#31283;&#20581;&#65292;&#26080;&#27861;&#20805;&#20998;&#35780;&#20272;PTLMs&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;PTLMs&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#30340;&#29702;&#24819;&#22522;&#20934;&#30340;&#24517;&#35201;&#26631;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;EC-FUNSD&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#19978;&#35821;&#20041;&#23454;&#20307;&#35782;&#21035;&#21644;&#23454;&#20307;&#38142;&#25509;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19981;&#21516;&#26684;&#24335;&#30340;&#25991;&#26723;&#24067;&#23616;&#21644;&#35821;&#20041;&#39537;&#21160;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#30340;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#35299;&#24320;&#20102;&#30001;FUNSD&#30340;&#20998;&#27573;&#32423;&#27880;&#37322;&#24102;&#26469;&#30340;&#27573;&#33853;&#21644;&#23454;&#20307;&#38169;&#35823;&#32806;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;PTLMs&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed pre-trained text-and-layout models (PTLMs) have shown remarkable success in multiple information extraction tasks on visually-rich documents. However, the prevailing evaluation pipeline may not be sufficiently robust for assessing the information extraction ability of PTLMs, due to inadequate annotations within the benchmarks. Therefore, we claim the necessary standards for an ideal benchmark to evaluate the information extraction ability of PTLMs. We then introduce EC-FUNSD, an entity-centric benckmark designed for the evaluation of semantic entity recognition and entity linking on visually-rich documents. This dataset contains diverse formats of document layouts and annotations of semantic-driven entities and their relations. Moreover, this dataset disentangles the falsely coupled annotation of segment and entity that arises from the block-level annotation of FUNSD. Experiment results demonstrate that state-of-the-art PTLMs exhibit overfitting tendencies on the pre
&lt;/p&gt;</description></item><item><title>M$^3$Face&#26159;&#19968;&#31181;&#32479;&#19968;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#36755;&#20837;&#33258;&#21160;&#29983;&#25104;&#25511;&#21046;&#27169;&#24577;&#65292;&#24182;&#23454;&#29616;&#20154;&#33080;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;M3CelebA&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#20154;&#33080;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.02369</link><description>&lt;p&gt;
M$^3$Face: &#19968;&#31181;&#29992;&#20110;&#20154;&#33080;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
M$^3$Face: A Unified Multi-Modal Multilingual Framework for Human Face Generation and Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02369
&lt;/p&gt;
&lt;p&gt;
M$^3$Face&#26159;&#19968;&#31181;&#32479;&#19968;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#36755;&#20837;&#33258;&#21160;&#29983;&#25104;&#25511;&#21046;&#27169;&#24577;&#65292;&#24182;&#23454;&#29616;&#20154;&#33080;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;M3CelebA&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#20154;&#33080;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25968;&#23383;&#21270;&#19990;&#30028;&#30340;&#26102;&#20195;&#65292;&#20154;&#33080;&#29983;&#25104;&#21644;&#32534;&#36753;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#22810;&#27169;&#24577;&#20154;&#33080;&#29983;&#25104;&#21644;&#32534;&#36753;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20363;&#22914;&#20351;&#29992;&#38754;&#37096;&#20998;&#21106;&#26469;&#25351;&#23548;&#22270;&#20687;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;&#29992;&#25143;&#26469;&#35828;&#65292;&#25163;&#21160;&#21019;&#24314;&#36825;&#20123;&#25511;&#21046;&#27169;&#24577;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;M3Face&#65292;&#19968;&#31181;&#29992;&#20110;&#21487;&#25511;&#20154;&#33080;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25143;&#33021;&#22815;&#20165;&#20351;&#29992;&#25991;&#26412;&#36755;&#20837;&#33258;&#21160;&#29983;&#25104;&#25511;&#21046;&#27169;&#24577;&#65292;&#20363;&#22914;&#35821;&#20041;&#20998;&#21106;&#25110;&#38754;&#37096;&#29305;&#24449;&#28857;&#65292;&#24182;&#38543;&#21518;&#29983;&#25104;&#20154;&#33080;&#22270;&#20687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#22312;&#20154;&#33080;&#29983;&#25104;&#21644;&#32534;&#36753;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3CelebA&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#38754;&#37096;&#29305;&#24449;&#28857;&#21644;&#35786;&#26029;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#20154;&#33080;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human face generation and editing represent an essential task in the era of computer vision and the digital world. Recent studies have shown remarkable progress in multi-modal face generation and editing, for instance, using face segmentation to guide image generation. However, it may be challenging for some users to create these conditioning modalities manually. Thus, we introduce M3Face, a unified multi-modal multilingual framework for controllable face generation and editing. This framework enables users to utilize only text input to generate controlling modalities automatically, for instance, semantic segmentation or facial landmarks, and subsequently generate face images. We conduct extensive qualitative and quantitative experiments to showcase our frameworks face generation and editing capabilities. Additionally, we propose the M3CelebA Dataset, a large-scale multi-modal and multilingual face dataset containing high-quality images, semantic segmentations, facial landmarks, and di
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;FinLLMs&#65289;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#21382;&#21490;&#12289;&#25216;&#26415;&#12289;&#24615;&#33021;&#21644;&#26426;&#36935;&#19982;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20845;&#20010;&#22522;&#20934;&#20219;&#21153;&#21644;&#20843;&#20010;&#39640;&#32423;&#37329;&#34701;NLP&#20219;&#21153;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.02315</link><description>&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;&#65288;FinLLMs&#65289;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models in Finance (FinLLMs)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02315
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;FinLLMs&#65289;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#21382;&#21490;&#12289;&#25216;&#26415;&#12289;&#24615;&#33021;&#21644;&#26426;&#36935;&#19982;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20845;&#20010;&#22522;&#20934;&#20219;&#21153;&#21644;&#20843;&#20010;&#39640;&#32423;&#37329;&#34701;NLP&#20219;&#21153;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#25324;&#37329;&#34701;&#26381;&#21153;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#65292;&#24182;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#23545;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;&#37329;&#34701;&#39046;&#22495;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#37329;&#34701;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#20171;&#32461;&#20102;&#37329;&#34701;&#39046;&#22495;&#30340;LLMs&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#21382;&#21490;&#12289;&#25216;&#26415;&#12289;&#24615;&#33021;&#20197;&#21450;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#36890;&#29992;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21040;&#24403;&#21069;&#30340;FinLLMs&#65292;&#21253;&#25324;GPT&#31995;&#21015;&#12289;&#36873;&#23450;&#30340;&#24320;&#28304;LLMs&#21644;&#37329;&#34701;LMs&#65292;&#25552;&#20379;&#20102;&#26102;&#38388;&#39034;&#24207;&#30340;&#27010;&#36848;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#37329;&#34701;PLMs&#21644;FinLLMs&#20013;&#20351;&#29992;&#30340;&#20116;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#26041;&#27861;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#24494;&#35843;&#26041;&#27861;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20845;&#20010;&#22522;&#20934;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20843;&#20010;&#39640;&#32423;&#37329;&#34701;NLP&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable capabilities across a wide variety of Natural Language Processing (NLP) tasks and have attracted attention from multiple domains, including financial services. Despite the extensive research into general-domain LLMs, and their immense potential in finance, Financial LLM (FinLLM) research remains limited. This survey provides a comprehensive overview of FinLLMs, including their history, techniques, performance, and opportunities and challenges. Firstly, we present a chronological overview of general-domain Pre-trained Language Models (PLMs) through to current FinLLMs, including the GPT-series, selected open-source LLMs, and financial LMs. Secondly, we compare five techniques used across financial PLMs and FinLLMs, including training methods, training data, and fine-tuning methods. Thirdly, we summarize the performance evaluations of six benchmark tasks and datasets. In addition, we provide eight advanced financial NLP tasks and datasets
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02314</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#36873;&#25321;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Selecting Large Language Model to Fine-tune via Rectified Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#25152;&#26377;&#27169;&#22411;&#28982;&#21518;&#20877;&#36827;&#34892;&#36873;&#25321;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36164;&#28304;&#21463;&#38480;&#30340;&#36873;&#25321;&#20219;&#21153;&#36716;&#21270;&#20026;&#39044;&#27979;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#23637;&#31034;&#20854;&#19982;&#32553;&#25918;&#23450;&#24459;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#19982;&#39044;&#35757;&#32451;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#30340;&#32553;&#25918;&#26354;&#32447;&#19981;&#20165;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#21151;&#29575;&#38454;&#27573;&#8221;&#65292;&#36824;&#21253;&#25324;&#20197;&#21069;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#8220;&#39044;&#21151;&#29575;&#38454;&#27573;&#8221;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#32553;&#25918;&#23450;&#24459;&#26080;&#27861;&#29702;&#35770;&#21644;&#23454;&#35777;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#30456;&#21464;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#8220;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#8221;&#27010;&#24565;&#24341;&#20837;&#21040;&#25105;&#20204;&#30340;&#20462;&#27491;&#32553;&#25918;&#23450;&#24459;&#20013;&#65292;&#36825;&#20811;&#26381;&#20102;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#26356;&#22909;&#22320;&#36866;&#24212;&#23454;&#39564;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#23450;&#24459;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22768;&#23398;&#20266;&#20196;&#29260;&#26469;&#39044;&#27979;&#25913;&#21892;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#31215;&#26497;&#36801;&#31227;&#12290;&#36890;&#36807;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#34917;&#20805;&#26469;&#33258;&#30456;&#20284;&#12289;&#39640;&#36164;&#28304;&#30340;&#8220;&#25424;&#36192;&#8221;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02302</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#23398;&#20266;&#20196;&#29260;&#39044;&#27979;&#25913;&#21892;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#31215;&#26497;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22768;&#23398;&#20266;&#20196;&#29260;&#26469;&#39044;&#27979;&#25913;&#21892;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#31215;&#26497;&#36801;&#31227;&#12290;&#36890;&#36807;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#34917;&#20805;&#26469;&#33258;&#30456;&#20284;&#12289;&#39640;&#36164;&#28304;&#30340;&#8220;&#25424;&#36192;&#8221;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;wav2vec 2.0 XLSR-128&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#30340;&#24494;&#35843;&#65292;&#20294;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#21344;&#27604;&#30456;&#23545;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#24615;&#33021;&#20173;&#28982;&#30456;&#23545;&#36739;&#24046;&#12290;&#23545;&#20110;&#27809;&#26377;&#22826;&#22810;&#24405;&#21046;&#25968;&#25454;&#30340;&#35821;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#34917;&#20805;&#26469;&#33258;&#30456;&#20284;&#12289;&#39640;&#36164;&#28304;&#30340;&#8220;&#25424;&#36192;&#8221;&#35821;&#35328;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#20165;&#20351;&#29992;10&#23567;&#26102;&#30340;&#20302;&#36164;&#28304;&#26049;&#36974;&#26222;&#35821;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#24182;&#36741;&#21161;&#20351;&#29992;60&#23567;&#26102;&#30340;&#25424;&#36192;&#35821;&#35328;&#21360;&#22320;&#35821;&#30340;&#25928;&#26524;&#20960;&#20046;&#19982;&#20351;&#29992;70&#23567;&#26102;&#30340;&#26049;&#36974;&#26222;&#35821;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#30456;&#24403;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26469;&#33258;&#19981;&#22826;&#30456;&#20284;&#30340;&#25424;&#36192;&#35821;&#35328;&#65288;&#22914;&#23391;&#21152;&#25289;&#35821;&#65289;&#30340;&#25968;&#25454;&#21017;&#26080;&#27861;&#25913;&#21892;ASR&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36873;&#25321;&#25424;&#36192;&#35821;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35825;&#23548;&#22768;&#23398;&#21333;&#20803;&#24207;&#21015;&#20998;&#24067;&#30340;&#26032;&#39062;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65306;&#22768;&#23398;&#20196;&#29260;&#20998;&#24067;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity
&lt;/p&gt;</description></item><item><title>SemPool&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#31283;&#20581;&#12289;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#21152;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#27719;&#32858;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#34701;&#21512;KG&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#19979;&#25552;&#39640;&#38382;&#31572;&#31995;&#32479;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02289</link><description>&lt;p&gt;
SemPool&#65306;&#31616;&#21333;&#12289;&#31283;&#20581;&#12289;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#21152;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#27719;&#32858;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SemPool: Simple, robust, and interpretable KG pooling for enhancing language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02289
&lt;/p&gt;
&lt;p&gt;
SemPool&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#31283;&#20581;&#12289;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#21152;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#27719;&#32858;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#34701;&#21512;KG&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#19979;&#25552;&#39640;&#38382;&#31572;&#31995;&#32479;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#39537;&#21160;&#30340;&#38382;&#31572;&#31995;&#32479;&#22312;&#35821;&#20041;&#21644;&#30693;&#35782;&#20107;&#23454;&#19978;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23398;&#20064;&#20174;&#24213;&#23618;KG&#20013;&#32858;&#21512;&#20449;&#24687;&#65292;&#24182;&#19982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#22320;&#25512;&#29702;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#30340;&#38382;&#31572;&#26041;&#27861;&#20381;&#36182;&#20110;&#20505;&#36873;&#31572;&#26696;&#33410;&#28857;&#30340;&#22270;&#24418;&#20449;&#24687;&#65292;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#20854;&#20013;&#20851;&#38190;&#31572;&#26696;&#20449;&#24687;&#26410;&#21253;&#21547;&#22312;KG&#20013;&#65292;&#38480;&#21046;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#24418;&#27719;&#32858;&#26041;&#27861;&#65292;&#23398;&#20064;&#21487;&#20197;&#36741;&#21161;LM&#25512;&#29702;&#30340;KG&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#22270;&#24418;&#25200;&#21160;&#19979;&#20855;&#26377;&#31283;&#20581;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;SemPool&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LMs&#20195;&#34920;KG&#20107;&#23454;&#65292;&#23398;&#20064;&#32858;&#21512;&#23427;&#20204;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;LM&#30340;&#19981;&#21516;&#23618;&#27425;&#19978;&#34701;&#21512;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;Avazu&#25968;&#25454;&#38598;&#26102;&#65292;SemPool&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;2.27%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph (KG) powered question answering (QA) performs complex reasoning over language semantics as well as knowledge facts. Graph Neural Networks (GNNs) learn to aggregate information from the underlying KG, which is combined with Language Models (LMs) for effective reasoning with the given question. However, GNN-based methods for QA rely on the graph information of the candidate answer nodes, which limits their effectiveness in more challenging settings where critical answer information is not included in the KG. We propose a simple graph pooling approach that learns useful semantics of the KG that can aid the LM's reasoning and that its effectiveness is robust under graph perturbations. Our method, termed SemPool, represents KG facts with pre-trained LMs, learns to aggregate their semantic information, and fuses it at different layers of the LM. Our experimental results show that SemPool outperforms state-of-the-art GNN-based methods by 2.27% accuracy points on average when a
&lt;/p&gt;</description></item><item><title>SynthDST&#26159;&#19968;&#20010;&#38024;&#23545;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#35774;&#35745;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#25163;&#24037;&#23545;&#35805;&#27169;&#26495;&#21644;&#23545;&#35805;&#27169;&#24335;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#12289;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#24102;&#26377;DST&#27880;&#37322;&#30340;&#23545;&#35805;&#65292;&#24182;&#20351;Join&#36830;&#36890;&#29575;&#25552;&#21319;4-5&#65285;.</title><link>https://arxiv.org/abs/2402.02285</link><description>&lt;p&gt;
SynthDST: &#23569;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#25152;&#38656;&#30340;&#20840;&#37096;&#26159;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02285
&lt;/p&gt;
&lt;p&gt;
SynthDST&#26159;&#19968;&#20010;&#38024;&#23545;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#35774;&#35745;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#25163;&#24037;&#23545;&#35805;&#27169;&#26495;&#21644;&#23545;&#35805;&#27169;&#24335;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#12289;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#24102;&#26377;DST&#27880;&#37322;&#30340;&#23545;&#35805;&#65292;&#24182;&#20351;Join&#36830;&#36890;&#29575;&#25552;&#21319;4-5&#65285;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#25104;&#20026;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#34920;&#29616;&#26368;&#22909;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#28041;&#21450;&#26816;&#32034;&#21644;&#28155;&#21152;&#31867;&#20284;&#30340;&#31034;&#20363;&#21040;&#25552;&#31034;&#20013;&#65292;&#38656;&#35201;&#35775;&#38382;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#33719;&#21462;&#36825;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#26377;&#26102;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#34429;&#28982;&#38646;&#26679;&#26412;&#23398;&#20064;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#26126;&#26174;&#33853;&#21518;&#12290;&#22240;&#27492;&#65292;&#8220;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#20026;&#20219;&#20309;&#23545;&#35805;&#27169;&#24335;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#23569;&#26679;&#26412;&#25552;&#31034;&#65311;&#8221;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\method&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#19987;&#38376;&#38024;&#23545;DST&#65292;&#21033;&#29992;LLM&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#23545;&#35805;&#27169;&#24335;&#21644;&#19968;&#20123;&#25163;&#24037;&#23545;&#35805;&#27169;&#26495;&#65292;&#23601;&#33021;&#21512;&#25104;&#33258;&#28982;&#12289;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#24102;&#26377;DST&#27880;&#37322;&#30340;&#23545;&#35805;&#12290;&#20351;&#29992;{\method}&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#32467;&#26524;&#26174;&#31034;&#65292;Join&#36830;&#36890;&#29575;&#25552;&#21319;&#20102;4-5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\method} results in $4-5%$ improvement in Join
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;RoBERTa-CNN&#27169;&#22411;&#26469;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#33258;&#26432;&#24847;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;RoBERTa-CNN&#36890;&#36807;&#22312;RoBERTa&#27169;&#22411;&#20013;&#28155;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23618;&#65292;&#25552;&#39640;&#20102;&#23545;&#37325;&#35201;&#27169;&#24335;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#33258;&#26432;&#21644;&#25233;&#37057;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02262</link><description>&lt;p&gt;
&#25968;&#25454;&#36136;&#37327;&#24456;&#37325;&#35201;&#65306;&#20351;&#29992;RoBERTa-CNN&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#33258;&#26432;&#24847;&#22270;
&lt;/p&gt;
&lt;p&gt;
Data Quality Matters: Suicide Intention Detection on Social Media Posts Using a RoBERTa-CNN Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;RoBERTa-CNN&#27169;&#22411;&#26469;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#33258;&#26432;&#24847;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;RoBERTa-CNN&#36890;&#36807;&#22312;RoBERTa&#27169;&#22411;&#20013;&#28155;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23618;&#65292;&#25552;&#39640;&#20102;&#23545;&#37325;&#35201;&#27169;&#24335;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#33258;&#26432;&#21644;&#25233;&#37057;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26432;&#20173;&#28982;&#26159;&#20840;&#29699;&#20581;&#24247;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#27880;&#28966;&#28857;&#65292;&#24613;&#38656;&#21019;&#26032;&#26041;&#27861;&#36827;&#34892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#35782;&#21035;SuicideWatch Reddit&#24086;&#23376;&#20013;&#30340;&#33258;&#26432;&#24847;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23574;&#31471;&#30340;RoBERTa-CNN&#27169;&#22411;&#36827;&#34892;&#33258;&#26432;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;RoBERTa-CNN&#26159;RoBERTa&#65288;&#40065;&#26834;&#24615;&#20248;&#21270;BERT&#26041;&#27861;&#65289;&#30340;&#19968;&#31181;&#21464;&#20307;&#12290;RoBERTa&#34987;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;RoBERTa&#30340;&#26377;&#25928;&#24615;&#22312;&#20110;&#23427;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#20449;&#24687;&#24182;&#24418;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#21407;&#22987;&#27169;&#22411;&#20013;&#28155;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23618;&#65292;RoBERTa&#22686;&#24378;&#20102;&#20174;&#24222;&#22823;&#25968;&#25454;&#38598;&#20013;&#25429;&#25417;&#37325;&#35201;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#26432;&#21644;&#25233;&#37057;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;RoBERTa-CNN&#65292;&#24182;&#33719;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#65292;RoBERTa-CNN&#22312;&#24179;&#22343;&#20934;&#30830;&#29575;&#19978;&#33719;&#24471;&#20102;98&#65285;&#65292;&#26631;&#20934;&#24046;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Suicide remains a global health concern for the field of health, which urgently needs innovative approaches for early detection and intervention. In this paper, we focus on identifying suicidal intentions in SuicideWatch Reddit posts and present a novel approach to suicide detection using the cutting-edge RoBERTa-CNN model, a variant of RoBERTa (Robustly optimized BERT approach). RoBERTa is used for various Natural Language Processing (NLP) tasks, including text classification and sentiment analysis. The effectiveness of the RoBERTa lies in its ability to capture textual information and form semantic relationships within texts. By adding the Convolution Neural Network (CNN) layer to the original model, the RoBERTa enhances its ability to capture important patterns from heavy datasets. To evaluate the RoBERTa-CNN, we experimented on the Suicide and Depression Detection dataset and obtained solid results. For example, RoBERTa-CNN achieves 98% mean accuracy with the standard deviation (ST
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#24182;&#19988;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#30340;&#24778;&#35766;&#20272;&#35745;&#19982;&#33258;&#28982;&#20154;&#38405;&#35835;&#26102;&#38388;&#30340;&#36866;&#24212;&#24615;&#19979;&#38477;&#12290;&#32780;&#35789;&#39057;&#26159;&#35299;&#37322;&#36825;&#31181;&#36866;&#24212;&#24615;&#19979;&#38477;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36739;&#22823;&#27169;&#22411;&#21464;&#20307;&#36807;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#20154;&#32676;&#20013;&#26368;&#19981;&#39057;&#32321;&#30340;&#35789;&#27719;&#65292;&#32780;&#36739;&#22823;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#20934;&#30830;&#22320;&#23398;&#20064;&#20102;&#32597;&#35265;&#30340;&#35789;&#27719;&#65292;&#36825;&#35299;&#37322;&#20102;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#23610;&#23544;&#23545;&#36866;&#24212;&#38405;&#35835;&#26102;&#38388;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02255</link><description>&lt;p&gt;
&#39057;&#29575;&#35299;&#37322;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#12289;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#24778;&#35766;&#31243;&#24230;&#36866;&#24212;&#38405;&#35835;&#26102;&#38388;&#30340;&#21453;&#30456;&#20851;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#24182;&#19988;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#30340;&#24778;&#35766;&#20272;&#35745;&#19982;&#33258;&#28982;&#20154;&#38405;&#35835;&#26102;&#38388;&#30340;&#36866;&#24212;&#24615;&#19979;&#38477;&#12290;&#32780;&#35789;&#39057;&#26159;&#35299;&#37322;&#36825;&#31181;&#36866;&#24212;&#24615;&#19979;&#38477;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36739;&#22823;&#27169;&#22411;&#21464;&#20307;&#36807;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#20154;&#32676;&#20013;&#26368;&#19981;&#39057;&#32321;&#30340;&#35789;&#27719;&#65292;&#32780;&#36739;&#22823;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#20934;&#30830;&#22320;&#23398;&#20064;&#20102;&#32597;&#35265;&#30340;&#35789;&#27719;&#65292;&#36825;&#35299;&#37322;&#20102;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#23610;&#23544;&#23545;&#36866;&#24212;&#38405;&#35835;&#26102;&#38388;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#24182;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#30340;&#24778;&#35766;&#20272;&#35745;&#19982;&#33258;&#28982;&#20154;&#38405;&#35835;&#26102;&#38388;&#30340;&#36866;&#24212;&#24615;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#20998;&#26512;&#26174;&#31034;&#65292;&#35789;&#39057;&#26159;&#36825;&#20004;&#20010;&#36235;&#21183;&#32972;&#21518;&#30340;&#20851;&#38190;&#35299;&#37322;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#26469;&#33258;&#22235;&#20010;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#22312;&#22235;&#20010;&#35821;&#26009;&#24211;&#19978;&#30340;&#27531;&#24046;&#35823;&#24046;&#26174;&#31034;&#65292;&#27169;&#22411;&#23610;&#23544;&#19982;&#36866;&#24212;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#21453;&#30456;&#20851;&#22312;&#26368;&#19981;&#39057;&#32321;&#30340;&#35789;&#27719;&#23376;&#38598;&#19978;&#26368;&#20026;&#26174;&#33879;&#65292;&#36825;&#26159;&#30001;&#36739;&#22823;&#27169;&#22411;&#21464;&#20307;&#36807;&#24230;&#20934;&#30830;&#30340;&#39044;&#27979;&#25152;&#25512;&#21160;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#21160;&#24577;&#26174;&#31034;&#65292;&#22312;&#21518;&#26399;&#35757;&#32451;&#27493;&#39588;&#20013;&#65292;&#25152;&#26377;&#27169;&#22411;&#21464;&#20307;&#23398;&#20064;&#39044;&#27979;&#32597;&#35265;&#30340;&#35789;&#27719;&#65292;&#24182;&#19988;&#36739;&#22823;&#27169;&#22411;&#21464;&#20307;&#30340;&#39044;&#27979;&#26356;&#20026;&#20934;&#30830;&#65292;&#36825;&#35299;&#37322;&#20102;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#23610;&#23544;&#23545;&#36866;&#24212;&#38405;&#35835;&#26102;&#38388;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#29305;&#24449;&#24402;&#22240;&#20998;&#26512;&#35777;&#26126;&#36739;&#22823;&#30340;&#27169;&#22411;&#21464;&#20307;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#32597;&#35265;&#30340;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#21253;&#25324;&#26550;&#26500;&#20462;&#25913;&#22312;&#20869;&#30340;&#22810;&#31181;&#25216;&#26415;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#38271;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.02244</link><description>&lt;p&gt;
&#36229;&#36234;&#26497;&#38480;&#65306;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#21253;&#25324;&#26550;&#26500;&#20462;&#25913;&#22312;&#20869;&#30340;&#22810;&#31181;&#25216;&#26415;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#24322;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#29702;&#35299;&#19978;&#19979;&#25991;&#12289;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#21644;&#29983;&#25104;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#20005;&#26684;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#20026;&#20195;&#20215;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#25903;&#25345;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26368;&#36817;&#20026;&#25193;&#23637;LLMs&#24207;&#21015;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#23545;&#38271;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22238;&#39038;&#21644;&#20998;&#31867;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#20462;&#25913;&#20301;&#32622;&#32534;&#30721;&#21644;&#20462;&#25913;&#27880;&#24847;&#26426;&#21046;&#31561;&#26550;&#26500;&#20462;&#25913;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#26356;&#38271;&#24207;&#21015;&#30340;&#22788;&#29702;&#65292;&#21516;&#26102;&#36991;&#20813;&#35745;&#31639;&#38656;&#27714;&#30340;&#25104;&#27604;&#20363;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#30340;&#22810;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;LLMs&#30340;&#19981;&#21516;&#38454;&#27573;&#65288;&#21363;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#65289;&#20013;&#21033;&#29992;&#12290;&#36825;&#20351;&#24471;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#24182;&#25552;&#21319;&#23545;&#38271;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to effic
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;LLM&#35268;&#27169;&#19978;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#26412;&#36523;&#30340;&#25910;&#25947;&#32422;&#26463;&#26469;&#20570;&#21040;&#36229;&#20986;&#39044;&#26399;&#30340;&#34920;&#29616;&#65292;&#20294;&#24182;&#19981;&#30495;&#27491;&#29702;&#35299;&#35821;&#20041;&#20197;&#21450;&#19982;&#24863;&#35273;&#21160;&#20316;&#30340;&#30452;&#25509;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02243</link><description>&lt;p&gt;
&#35821;&#35328;&#25193;&#23637;&#65306;LLMs&#65292;ChatGPT&#65292;&#25509;&#22320;&#65292;&#24847;&#20041;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02243
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;LLM&#35268;&#27169;&#19978;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#26412;&#36523;&#30340;&#25910;&#25947;&#32422;&#26463;&#26469;&#20570;&#21040;&#36229;&#20986;&#39044;&#26399;&#30340;&#34920;&#29616;&#65292;&#20294;&#24182;&#19981;&#30495;&#27491;&#29702;&#35299;&#35821;&#20041;&#20197;&#21450;&#19982;&#24863;&#35273;&#21160;&#20316;&#30340;&#30452;&#25509;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;OpenAI&#21487;&#33021;&#23545;&#25105;&#20204;&#38544;&#30610;&#30340;&#23569;&#37327;&#20449;&#24687;&#22806;&#65292;&#25105;&#20204;&#37117;&#22823;&#33268;&#30693;&#36947;ChatGPT&#26159;&#22914;&#20309;&#24037;&#20316;&#30340;&#65288;&#23427;&#30340;&#22823;&#22411;&#25991;&#26412;&#25968;&#25454;&#24211;&#65292;&#32479;&#35745;&#25968;&#25454;&#65292;&#21521;&#37327;&#34920;&#31034;&#20197;&#21450;&#23427;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#20854;&#19979;&#19968;&#20010;&#35789;&#30340;&#35757;&#32451;&#31561;&#65289;&#12290;&#20294;&#25105;&#20204;&#35841;&#20063;&#19981;&#33021;&#35828;&#25105;&#20204;&#23545;ChatGPT&#30340;&#36825;&#20123;&#36164;&#28304;&#25152;&#33021;&#20570;&#21040;&#30340;&#20107;&#24773;&#19981;&#24863;&#21040;&#24778;&#35766;&#12290;&#36825;&#29978;&#33267;&#35753;&#25105;&#20204;&#26377;&#20154;&#24471;&#20986;&#32467;&#35770;&#65292;ChatGPT&#23454;&#38469;&#19978;&#29702;&#35299;&#20102;&#12290;&#23427;&#24182;&#19981;&#29702;&#35299;&#65292;&#20294;&#25105;&#20204;&#20063;&#19981;&#33021;&#35828;&#25105;&#20204;&#29702;&#35299;&#23427;&#26159;&#22914;&#20309;&#20570;&#21040;&#36825;&#19968;&#28857;&#30340;&#12290;&#25105;&#23558;&#25552;&#20986;&#20851;&#20110;&#33391;&#24615;&#20559;&#35265;&#30340;&#19968;&#20123;&#29468;&#24819;&#65306;&#22312;LLM&#35268;&#27169;&#19978;&#20986;&#29616;&#30340;&#25910;&#25947;&#32422;&#26463;&#21487;&#33021;&#26377;&#21161;&#20110;ChatGPT&#20570;&#24471;&#27604;&#25105;&#20204;&#39044;&#26399;&#30340;&#22909;&#24471;&#22810;&#12290;&#36825;&#20123;&#20559;&#35265;&#26159;&#35821;&#35328;&#26412;&#36523;&#22312;LLM&#35268;&#27169;&#19978;&#22266;&#26377;&#30340;&#65292;&#24182;&#19988;&#19982;ChatGPT&#32570;&#20047;&#30452;&#25509;&#30340;&#24863;&#35273;&#21160;&#20316;&#25509;&#22320;&#20197;&#23558;&#20854;&#35789;&#19982;&#20854;&#25152;&#25351;&#30340;&#23545;&#35937;&#20197;&#21450;&#20854;&#21629;&#39064;&#19982;&#20854;&#24847;&#20041;&#32852;&#31995;&#36215;&#26469;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Apart from what (little) OpenAI may be concealing from us, we all know (roughly) how ChatGPT works (its huge text database, its statistics, its vector representations, and their huge number of parameters, its next-word training, and so on). But none of us can say (hand on heart) that we are not surprised by what ChatGPT has proved to be able to do with these resources. This has even driven some of us to conclude that ChatGPT actually understands. It is not true that it understands. But it is also not true that we understand how it can do what it can do. I will suggest some hunches about benign biases: convergent constraints that emerge at LLM scale that may be helping ChatGPT do so much better than we would have expected. These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings. These converg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25968;&#25454;&#29983;&#25104;&#30340;&#35282;&#24230;&#37325;&#26032;&#35299;&#37322;&#20102;In-Context Learning&#65288;ICL&#65289;&#30340;&#26426;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#27969;&#34892;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23545;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#21155;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#20854;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.02212</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#29983;&#25104;&#30340;&#35282;&#24230;&#23545;In-Context Learning&#26426;&#21046;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Data Generation Perspective to the Mechanism of In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25968;&#25454;&#29983;&#25104;&#30340;&#35282;&#24230;&#37325;&#26032;&#35299;&#37322;&#20102;In-Context Learning&#65288;ICL&#65289;&#30340;&#26426;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#27969;&#34892;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23545;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#21155;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#20854;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-Context Learning&#65288;ICL&#65289;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19979;&#28216;&#27867;&#21270;&#65292;&#32780;&#26080;&#38656;&#26799;&#24230;&#26356;&#26032;&#12290;&#23613;&#31649;&#26377;&#40723;&#33310;&#20154;&#24515;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;ICL&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#29616;&#26377;&#30740;&#31350;&#25552;&#20379;&#20102;&#21508;&#31181;&#19981;&#21516;&#35266;&#28857;&#30340;&#29702;&#35299;&#12290;&#36825;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#30452;&#35273;&#21644;&#20020;&#26102;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#37322;ICL&#65292;&#21576;&#29616;&#20986;&#20102;&#19968;&#26465;&#27169;&#31946;&#30340;&#36335;&#32447;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#29983;&#25104;&#30340;&#35270;&#35282;&#37325;&#26032;&#35299;&#37322;&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#27969;&#34892;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#22312;&#24191;&#27867;&#24212;&#29992;&#65292;&#20174;&#32780;&#25509;&#36817;&#19968;&#20010;&#31995;&#32479;&#30340;&#35282;&#24230;&#12290;&#25105;&#20204;&#20005;&#26684;&#37319;&#29992;&#25216;&#33021;&#23398;&#20064;&#21644;&#25216;&#33021;&#35782;&#21035;&#30340;&#27010;&#24565;&#23450;&#20041;&#12290;&#23427;&#20204;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#25216;&#33021;&#23398;&#20064;&#21487;&#20197;&#20174;&#19978;&#19979;&#25991;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#23545;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) empowers Large Language Models (LLMs) with the capacity to learn in context, achieving downstream generalization without gradient updates but with a few in-context examples. Despite the encouraging empirical success, the underlying mechanism of ICL remains unclear, and existing research offers various viewpoints of understanding. These studies propose intuition-driven and ad-hoc technical solutions for interpreting ICL, illustrating an ambiguous road map. In this paper, we leverage a data generation perspective to reinterpret recent efforts and demonstrate the potential broader usage of popular technical solutions, approaching a systematic angle. For a conceptual definition, we rigorously adopt the terms of skill learning and skill recognition. The difference between them is skill learning can learn new data generation functions from in-context data. We also provide a comprehensive study on the merits and weaknesses of different solutions, and highlight the un
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32034;&#24341;&#21407;&#23376;&#37051;&#25509;&#27169;&#24335;&#24182;&#23558;&#20854;&#32452;&#21512;&#65292;&#26126;&#30830;&#24314;&#27169;&#32467;&#26500;&#20381;&#36182;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#23376;&#22270;&#25552;&#21462;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EPR&#26041;&#27861;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;F1&#24471;&#20998;&#65292;&#24182;&#22312;WebQuestionsSP&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02175</link><description>&lt;p&gt;
&#36890;&#36807;&#35777;&#25454;&#27169;&#24335;&#26816;&#32034;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32034;&#24341;&#21407;&#23376;&#37051;&#25509;&#27169;&#24335;&#24182;&#23558;&#20854;&#32452;&#21512;&#65292;&#26126;&#30830;&#24314;&#27169;&#32467;&#26500;&#20381;&#36182;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#23376;&#22270;&#25552;&#21462;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EPR&#26041;&#27861;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;F1&#24471;&#20998;&#65292;&#24182;&#22312;WebQuestionsSP&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#23376;&#22270;&#25552;&#21462;&#21644;&#31572;&#26696;&#25512;&#29702;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#30340;&#23376;&#22270;&#25552;&#21462;&#26041;&#27861;&#20302;&#20272;&#20102;&#35777;&#25454;&#20107;&#23454;&#20043;&#38388;&#30340;&#32467;&#26500;&#20381;&#36182;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35777;&#25454;&#27169;&#24335;&#26816;&#32034;&#65288;EPR&#65289;&#26469;&#22312;&#23376;&#22270;&#25552;&#21462;&#36807;&#31243;&#20013;&#26126;&#30830;&#22320;&#24314;&#27169;&#32467;&#26500;&#20381;&#36182;&#12290;&#25105;&#20204;&#36890;&#36807;&#32034;&#24341;&#36164;&#28304;&#23545;&#30340;&#21407;&#23376;&#37051;&#25509;&#27169;&#24335;&#26469;&#23454;&#29616;EPR&#12290;&#32473;&#23450;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#23494;&#38598;&#26816;&#32034;&#20197;&#33719;&#21462;&#30001;&#36164;&#28304;&#23545;&#24418;&#25104;&#30340;&#21407;&#23376;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26522;&#20030;&#23427;&#20204;&#30340;&#32452;&#21512;&#20197;&#26500;&#24314;&#20505;&#36873;&#35777;&#25454;&#27169;&#24335;&#12290;&#36825;&#20123;&#35777;&#25454;&#27169;&#24335;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#25171;&#20998;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#27169;&#24335;&#26469;&#25552;&#21462;&#19979;&#28216;&#31572;&#26696;&#25512;&#29702;&#25152;&#38656;&#30340;&#23376;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;EPR&#30340;&#26041;&#27861;&#22312;ComplexWebQuestions&#19978;&#23558;IR-KGQA&#26041;&#27861;&#30340;F1&#24471;&#20998;&#26174;&#33879;&#25552;&#39640;&#20102;10&#20010;&#30334;&#20998;&#28857;&#20197;&#19978;&#65292;&#24182;&#22312;WebQuestionsSP&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) methods for KGQA consist of two stages: subgraph extraction and answer reasoning. We argue current subgraph extraction methods underestimate the importance of structural dependencies among evidence facts. We propose Evidence Pattern Retrieval (EPR) to explicitly model the structural dependencies during subgraph extraction. We implement EPR by indexing the atomic adjacency pattern of resource pairs. Given a question, we perform dense retrieval to obtain atomic patterns formed by resource pairs. We then enumerate their combinations to construct candidate evidence patterns. These evidence patterns are scored using a neural model, and the best one is selected to extract a subgraph for downstream answer reasoning. Experimental results demonstrate that the EPR-based approach has significantly improved the F1 scores of IR-KGQA methods by over 10 points on ComplexWebQuestions and achieves competitive performance on WebQuestionsSP.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#25915;&#20987;&#30340;&#21477;&#23376;&#25200;&#21160;&#25216;&#26415;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#26032;&#38395;&#20869;&#23481;&#20013;&#30340;&#24773;&#24863;&#26497;&#24615;&#26469;&#35299;&#20915;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#24773;&#24863;&#25805;&#32437;&#38382;&#39064;&#12290;&#23454;&#39564;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#23454;&#29616;&#24773;&#24863;&#26497;&#24615;&#38477;&#20302;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.02145</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#25200;&#21160;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#24773;&#24863;&#26497;&#24615;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Analyzing Sentiment Polarity Reduction in News Presentation through Contextual Perturbation and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#25915;&#20987;&#30340;&#21477;&#23376;&#25200;&#21160;&#25216;&#26415;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#26032;&#38395;&#20869;&#23481;&#20013;&#30340;&#24773;&#24863;&#26497;&#24615;&#26469;&#35299;&#20915;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#24773;&#24863;&#25805;&#32437;&#38382;&#39064;&#12290;&#23454;&#39564;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#23454;&#29616;&#24773;&#24863;&#26497;&#24615;&#38477;&#20302;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#23186;&#20307;&#29615;&#22659;&#20013;&#65292;&#26032;&#38395;&#23186;&#20307;&#22312;&#22609;&#36896;&#20844;&#20247;&#33286;&#35770;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26377;&#24517;&#35201;&#35299;&#20915;&#26032;&#38395;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#25805;&#32437;&#38382;&#39064;&#12290;&#26032;&#38395;&#20316;&#32773;&#24120;&#24120;&#27880;&#20837;&#33258;&#24049;&#30340;&#20559;&#35265;&#21644;&#24773;&#24863;&#35821;&#35328;&#65292;&#36825;&#20250;&#25197;&#26354;&#25253;&#36947;&#30340;&#23458;&#35266;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#38477;&#20302;&#26032;&#38395;&#20869;&#23481;&#20013;&#28508;&#22312;&#24773;&#24863;&#30340;&#26497;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#23545;&#25239;&#25915;&#20987;&#30340;&#21477;&#23376;&#25200;&#21160;&#25216;&#26415;&#21644;&#20351;&#29992;ChatGPT&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#37319;&#29992;&#36716;&#25442;&#32422;&#26463;&#26469;&#20462;&#25913;&#21477;&#23376;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#26680;&#24515;&#35821;&#20041;&#12290;&#36890;&#36807;&#26367;&#25442;&#12289;&#25554;&#20837;&#21644;&#21024;&#38500;&#19977;&#31181;&#25200;&#21160;&#26041;&#27861;&#65292;&#20877;&#32467;&#21512;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#26469;&#26368;&#22823;&#21270;&#38024;&#23545;&#26032;&#38395;&#26041;&#38754;&#30340;&#26399;&#26395;&#24773;&#24863;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#23454;&#29616;&#24773;&#24863;&#26497;&#24615;&#38477;&#20302;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's media landscape, where news outlets play a pivotal role in shaping public opinion, it is imperative to address the issue of sentiment manipulation within news text. News writers often inject their own biases and emotional language, which can distort the objectivity of reporting. This paper introduces a novel approach to tackle this problem by reducing the polarity of latent sentiments in news content. Drawing inspiration from adversarial attack-based sentence perturbation techniques and a prompt based method using ChatGPT, we employ transformation constraints to modify sentences while preserving their core semantics. Using three perturbation methods: replacement, insertion, and deletion coupled with a context-aware masked language model, we aim to maximize the desired sentiment score for targeted news aspects through a beam search algorithm. Our experiments and human evaluations demonstrate the effectiveness of these two models in achieving reduced sentiment polarity with mi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#20851;&#38190;&#23398;&#20064;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#35780;&#20272;&#40065;&#26834;&#24615;&#12289;&#24494;&#35843;&#35774;&#32622;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30340;&#24433;&#21709;&#31561;&#26041;&#38754;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#20026;&#30740;&#31350;&#32773;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#39046;&#22495;&#25552;&#20379;&#20102;&#23454;&#35777;&#22522;&#30784;&#21644;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.02144</link><description>&lt;p&gt;
&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#20851;&#38190;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Probing Critical Learning Dynamics of PLMs for Hate Speech Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#20851;&#38190;&#23398;&#20064;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#35780;&#20272;&#40065;&#26834;&#24615;&#12289;&#24494;&#35843;&#35774;&#32622;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30340;&#24433;&#21709;&#31561;&#26041;&#38754;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#20026;&#30740;&#31350;&#32773;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#39046;&#22495;&#25552;&#20379;&#20102;&#23454;&#35777;&#22522;&#30784;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#32570;&#20047;&#20851;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21508;&#31181;&#20851;&#38190;&#26041;&#38754;&#22914;&#20309;&#24433;&#21709;&#20854;&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#24615;&#33021;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20116;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#24314;&#35758;&#20026;&#23454;&#35777;&#30740;&#31350;PLMs&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#19981;&#21516;&#26041;&#38754;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#28145;&#20837;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12289;&#24494;&#35843;&#35774;&#32622;&#20197;&#21450;&#39044;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#19979;&#28216;&#20219;&#21153;&#30340;&#26089;&#26399;&#23792;&#20540;&#65292;&#37319;&#29992;&#26356;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#24102;&#26469;&#30340;&#26377;&#38480;&#30410;&#22788;&#65292;&#20197;&#21450;&#24494;&#35843;&#36807;&#31243;&#20013;&#29305;&#23450;&#23618;&#27425;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#25552;&#20986;&#36136;&#30097;&#65292;&#24182;&#24378;&#35843;&#20102;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#21160;&#24577;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the widespread adoption, there is a lack of research into how various critical aspects of pretrained language models (PLMs) affect their performance in hate speech detection. Through five research questions, our findings and recommendations lay the groundwork for empirically investigating different aspects of PLMs' use in hate speech detection. We deep dive into comparing different pretrained models, evaluating their seed robustness, finetuning settings, and the impact of pretraining data collection time. Our analysis reveals early peaks for downstream tasks during pretraining, the limited benefit of employing a more recent pretraining corpus, and the significance of specific layers during finetuning. We further call into question the use of domain-specific models and highlight the need for dynamic datasets for benchmarking hate speech detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#30340;&#36947;&#24503;&#21028;&#26029;&#21644;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21360;&#22320;&#35821;&#21644;&#26031;&#29926;&#24076;&#37324;&#35821;&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#26126;&#26174;&#20302;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#32780;&#36947;&#24503;&#21028;&#26029;&#20063;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#21464;&#21270;&#36739;&#22823;&#12290;</title><link>https://arxiv.org/abs/2402.02135</link><description>&lt;p&gt;
&#35821;&#35328;&#23545;LLMs&#30340;&#36947;&#24503;&#21028;&#26029;&#21644;&#25512;&#29702;&#33021;&#21147;&#26377;&#24433;&#21709;&#21527;&#65311;&#19968;&#39033;&#20351;&#29992;&#22810;&#35821;&#35328;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#30340;&#36947;&#24503;&#21028;&#26029;&#21644;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21360;&#22320;&#35821;&#21644;&#26031;&#29926;&#24076;&#37324;&#35821;&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#26126;&#26174;&#20302;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#32780;&#36947;&#24503;&#21028;&#26029;&#20063;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#21464;&#21270;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#23637;&#29616;&#30340;&#36947;&#24503;&#21028;&#26029;&#21644;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36947;&#24503;&#21028;&#26029;&#21462;&#20915;&#20110;&#38382;&#39064;&#25152;&#20351;&#29992;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#20102;&#38500;&#33521;&#35821;&#22806;&#30340;&#20116;&#31181;&#26032;&#35821;&#35328;&#65288;&#20013;&#25991;&#12289;&#21360;&#22320;&#35821;&#12289;&#20420;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#26031;&#29926;&#24076;&#37324;&#35821;&#65289;&#65292;&#24182;&#23545;&#19977;&#20010;&#20855;&#26377;&#36739;&#24378;&#22810;&#35821;&#35328;&#25991;&#26412;&#22788;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;LLMs&#65288;ChatGPT&#12289;GPT-4&#21644;Llama2Chat-70B&#65289;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#21518;&#26580;&#24615;&#20998;&#25968;&#25351;&#31034;&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#22312;&#21360;&#22320;&#35821;&#21644;&#26031;&#29926;&#24076;&#37324;&#35821;&#20013;&#26174;&#33879;&#20302;&#20110;&#35199;&#29677;&#29273;&#35821;&#12289;&#20420;&#35821;&#12289;&#20013;&#25991;&#21644;&#33521;&#35821;&#65292;&#32780;&#21518;&#22235;&#31181;&#35821;&#35328;&#30340;&#34920;&#29616;&#21017;&#27809;&#26377;&#26126;&#26174;&#30340;&#36235;&#21183;&#12290;&#36947;&#24503;&#21028;&#26029;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#20063;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the moral judgment and moral reasoning abilities exhibited by Large Language Models (LLMs) across languages through the Defining Issues Test. It is a well known fact that moral judgment depends on the language in which the question is asked. We extend the work of beyond English, to 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe three LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial multilingual text processing and generation abilities. Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages. The moral judgments too vary considerably by the language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#20219;&#21153;&#24341;&#20837;&#20102;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02130</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#28210;&#26579;&#22270;&#24418;
&lt;/p&gt;
&lt;p&gt;
Rendering Graphs for Graph Reasoning in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#20219;&#21153;&#24341;&#20837;&#20102;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22270;&#32467;&#26500;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#26684;&#24335;&#30340;&#22270;&#20449;&#24687;&#65292;&#20294;&#24573;&#35270;&#20102;&#20016;&#23500;&#30340;&#35270;&#35273;&#27169;&#24577;&#65292;&#32780;&#35270;&#35273;&#26159;&#20154;&#31867;&#29702;&#35299;&#32467;&#26500;&#20449;&#24687;&#21644;&#36827;&#34892;&#22270;&#25512;&#29702;&#30340;&#30452;&#35266;&#26041;&#24335;&#12290;&#23558;&#22270;&#32467;&#26500;&#34920;&#31034;&#20026;&#35270;&#35273;&#22270;&#20687;(&#21363;&#35270;&#35273;&#22270;)&#30340;&#28508;&#22312;&#30410;&#22788;&#21644;&#33021;&#21147;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#39318;&#27425;&#24341;&#20837;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#26159;&#19968;&#20010;&#20803;&#32452;(&#22270;&#12289;&#22270;&#20687;&#12289;&#25991;&#26412;&#25551;&#36848;)&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;LLMs&#22312;GITQA&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#22312;LLaVA-7B/13B&#27169;&#22411;&#30340;&#24494;&#35843;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22810;&#35821;&#35328;&#24773;&#24863;&#35789;&#20856;&#36827;&#34892;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#33719;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#23545;&#39640;/&#20013;&#36164;&#28304;&#35821;&#35328;&#21644;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.02113</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#35821;&#35328;&#24773;&#24863;&#35789;&#20856;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22810;&#35821;&#35328;&#24773;&#24863;&#35789;&#20856;&#36827;&#34892;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#33719;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#23545;&#39640;/&#20013;&#36164;&#28304;&#35821;&#35328;&#21644;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25913;&#21892;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#36890;&#24120;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#36825;&#20123;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#22810;&#35821;&#35328;&#35789;&#20856;&#26469;&#22686;&#24378;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#30340;&#20381;&#36182;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36328;34&#31181;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#21253;&#25324;6&#31181;&#39640;/&#20013;&#36164;&#28304;&#35821;&#35328;&#12289;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;3&#20010;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#21477;&#23376;&#32423;&#24773;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#35789;&#20856;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#20110;&#22312;&#33521;&#25991;&#24773;&#24863;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#20197;&#21450;&#20687;GPT--3.5&#12289;BLOOMZ&#21644;XGLM&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#36866;&#29992;&#20110;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26410;&#35265;&#24773;&#20917;&#65292;&#20197;&#21450;&#28041;&#21450;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#28151;&#21512;&#20195;&#30721;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages. In this paper, we relax the reliance on texts in low-resource languages by using multilingual lexicons in pretraining to enhance multilingual capabilities. Specifically, we focus on zero-shot sentiment analysis tasks across 34 languages, including 6 high/medium-resource languages, 25 low-resource languages, and 3 code-switching datasets. We demonstrate that pretraining using multilingual lexicons, without using any sentence-level sentiment data, achieves superior zero-shot performance compared to models fine-tuned on English sentiment datasets, and large language models like GPT--3.5, BLOOMZ, and XGLM. These findings are observable for unseen low-resource languages to code-mixed scenarios involving high-resource languages.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#30340;&#23454;&#38469;&#26426;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;LLM&#20248;&#21270;&#22120;&#24448;&#24448;&#21463;&#21040;&#33258;&#36523;&#20808;&#21069;&#30693;&#35782;&#30340;&#20559;&#35265;&#65292;&#38590;&#20197;&#20934;&#30830;&#35782;&#21035;&#38169;&#35823;&#30340;&#30495;&#27491;&#21407;&#22240;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#21512;&#36866;&#30340;&#25552;&#31034;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02101</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#22909;&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Good Prompt Optimizers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02101
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#30340;&#23454;&#38469;&#26426;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;LLM&#20248;&#21270;&#22120;&#24448;&#24448;&#21463;&#21040;&#33258;&#36523;&#20808;&#21069;&#30693;&#35782;&#30340;&#20559;&#35265;&#65292;&#38590;&#20197;&#20934;&#30830;&#35782;&#21035;&#38169;&#35823;&#30340;&#30495;&#27491;&#21407;&#22240;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#21512;&#36866;&#30340;&#25552;&#31034;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#36827;&#34892;&#33258;&#25105;&#21453;&#39304;&#21644;&#20248;&#21270;&#25552;&#31034;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#24213;&#23618;&#26426;&#21046;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#32780;LLM&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#30340;&#30495;&#27491;&#26377;&#25928;&#24615;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#20248;&#21270;&#30340;&#23454;&#38469;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLM&#20248;&#21270;&#22120;&#22312;&#21453;&#24605;&#36807;&#31243;&#20013;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#35782;&#21035;&#38169;&#35823;&#30340;&#30495;&#27491;&#21407;&#22240;&#65292;&#32780;&#26356;&#22810;&#22320;&#21463;&#21040;&#33258;&#36523;&#20808;&#21069;&#30693;&#35782;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#21453;&#24605;&#22312;&#35821;&#20041;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;LLM&#20248;&#21270;&#22120;&#20063;&#32463;&#24120;&#26080;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#20248;&#21270;&#27493;&#39588;&#20026;&#30446;&#26631;&#27169;&#22411;&#29983;&#25104;&#21512;&#36866;&#30340;&#25552;&#31034;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30446;&#26631;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#19981;&#21487;&#39044;&#27979;&#30340;&#12290;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#33258;&#21160;&#34892;&#20026;&#20248;&#21270;&#8221;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as Prompt Optimizers to self-reflect and refine prompts, has shown promising performance in recent studies. Despite the success, the underlying mechanism of this approach remains unexplored, and the true effectiveness of LLMs as Prompt Optimizers requires further validation. In this work, we conducted a comprehensive study to uncover the actual mechanism of LLM-based Prompt Optimization. Our findings reveal that the LLM optimizers struggle to identify the true causes of errors during reflection, tending to be biased by their own prior knowledge rather than genuinely reflecting on the errors. Furthermore, even when the reflection is semantically valid, the LLM optimizers often fail to generate appropriate prompts for the target models with a single prompt refinement step, partly due to the unpredictable behaviors of the target models. Based on the observations, we introduce a new "Automatic Behavior Optimization" par
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#65292;&#21457;&#29616;&#39640;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#38750;&#35821;&#35328;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#21644;&#34920;&#23618;&#30693;&#35782;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#20256;&#36755;&#30340;&#20027;&#35201;&#26159;&#25968;&#25454;&#24037;&#20214;&#21644;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2402.02099</link><description>&lt;p&gt;
&#20998;&#26512;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02099
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#65292;&#21457;&#29616;&#39640;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#38750;&#35821;&#35328;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#21644;&#34920;&#23618;&#30693;&#35782;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#20256;&#36755;&#30340;&#20027;&#35201;&#26159;&#25968;&#25454;&#24037;&#20214;&#21644;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20284;&#20046;&#26174;&#31034;&#20986;&#22312;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#21644;&#35774;&#32622;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#38646;-shot&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#31243;&#24230;&#34920;&#31034;&#36136;&#30097;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#28041;&#21450;&#22810;&#35821;&#35328;&#23454;&#20363;&#65292;&#25361;&#25112;&#20102;&#39640;&#38646;-shot&#24615;&#33021;&#22312;&#30446;&#26631;&#20219;&#21153;&#20013;&#21453;&#26144;&#39640;&#36328;&#35821;&#35328;&#33021;&#21147;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#19981;&#38656;&#35201;&#36716;&#31227;&#23454;&#38469;&#35821;&#35328;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#21644;&#34920;&#23618;&#30693;&#35782;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36328;&#35821;&#35328;&#20256;&#36755;&#30340;&#20027;&#35201;&#26159;&#25968;&#25454;&#24037;&#20214;&#21644;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#34987;&#24573;&#35270;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65288;MAT&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MAT&#30340;&#38454;&#25968;&#22823;&#20110;4&#26102;&#65292;&#32763;&#35793;&#36136;&#37327;&#19982;&#20256;&#32479;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#30456;&#24403;&#65292;&#19988;&#39640;&#38454;MAT&#24182;&#19981;&#29305;&#21035;&#36866;&#29992;&#20110;&#38271;&#21477;&#30340;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2402.02084</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Markov Property for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65288;MAT&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MAT&#30340;&#38454;&#25968;&#22823;&#20110;4&#26102;&#65292;&#32763;&#35793;&#36136;&#37327;&#19982;&#20256;&#32479;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#30456;&#24403;&#65292;&#19988;&#39640;&#38454;MAT&#24182;&#19981;&#29305;&#21035;&#36866;&#29992;&#20110;&#38271;&#21477;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#32972;&#26223;&#19979;&#37325;&#26032;&#23457;&#26597;&#20102;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65288;MAT&#65289;&#65292;&#24182;&#23545;&#20854;&#22312;&#22235;&#20010;WMT&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;MAT&#30340;&#38454;&#25968;&#22823;&#20110;4&#26102;&#65292;&#29983;&#25104;&#30340;&#32763;&#35793;&#36136;&#37327;&#19982;&#20256;&#32479;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#19982;&#30452;&#35273;&#30456;&#21453;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#39640;&#38454;MAT&#30340;&#20248;&#21183;&#19981;&#26159;&#29305;&#21035;&#20307;&#29616;&#22312;&#38271;&#21477;&#30340;&#32763;&#35793;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we re-examine the Markov property in the context of neural machine translation. We design a Markov Autoregressive Transformer~(MAT) and undertake a comprehensive assessment of its performance across four WMT benchmarks. Our findings indicate that MAT with an order larger than 4 can generate translations with quality on par with that of conventional autoregressive transformers. In addition, counter-intuitively, we also find that the advantages of utilizing a higher-order MAT do not specifically contribute to the translation of longer sentences.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GliDe&#21644;CaPE&#20004;&#31181;&#31616;&#21270;&#30340;&#24555;&#36895;&#25512;&#29702;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#32531;&#23384;&#38190;&#21644;&#20540;&#20197;&#21450;&#21033;&#29992;&#32622;&#20449;&#24230;&#20998;&#25968;&#36873;&#25321;&#39069;&#22806;&#20505;&#36873;&#20196;&#29260;&#36827;&#34892;&#39564;&#35777;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;LLM&#30340;&#35299;&#30721;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2402.02082</link><description>&lt;p&gt;
&#25317;&#26377;CaPE&#30340;GliDe&#65306;&#19968;&#31181;&#31616;&#21270;&#30340;&#24555;&#36895;&#25512;&#29702;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GliDe&#21644;CaPE&#20004;&#31181;&#31616;&#21270;&#30340;&#24555;&#36895;&#25512;&#29702;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#32531;&#23384;&#38190;&#21644;&#20540;&#20197;&#21450;&#21033;&#29992;&#32622;&#20449;&#24230;&#20998;&#25968;&#36873;&#25321;&#39069;&#22806;&#20505;&#36873;&#20196;&#29260;&#36827;&#34892;&#39564;&#35777;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;LLM&#30340;&#35299;&#30721;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25512;&#29702;&#35299;&#30721;&#26159;&#19968;&#31181;&#21033;&#29992;&#23567;&#32780;&#39640;&#25928;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#20943;&#23569;LLM&#24310;&#36831;&#30340;&#30456;&#23545;&#36739;&#26032;&#30340;&#35299;&#30721;&#26694;&#26550;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GliDe&#21644;CaPE&#65292;&#36825;&#20004;&#31181;&#31616;&#21270;&#30340;&#24555;&#36895;&#25512;&#29702;&#35299;&#30721;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20923;&#32467;LLM&#30340;&#35299;&#30721;&#36895;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GliDe&#26159;&#19968;&#20010;&#20462;&#25913;&#36807;&#30340;&#33609;&#31295;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#37325;&#29992;&#30446;&#26631;LLM&#20013;&#30340;&#32531;&#23384;&#38190;&#21644;&#20540;&#65292;&#32780;CaPE&#26159;&#19968;&#31181;&#20511;&#21161;&#33609;&#31295;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#36873;&#25321;&#39069;&#22806;&#20505;&#36873;&#20196;&#29260;&#36827;&#34892;&#39564;&#35777;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;GliDe&#33609;&#31295;&#27169;&#22411;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#26399;&#30340;&#35299;&#30721;&#24310;&#36831;&#12290;&#20351;&#29992;&#22681;&#19978;&#26102;&#38388;&#36827;&#34892;&#39069;&#22806;&#35780;&#20272;&#26174;&#31034;&#65292;GliDe&#21487;&#20197;&#23558;Vicuna&#27169;&#22411;&#21152;&#36895;&#33267;2.17&#20493;&#65292;&#24182;&#21033;&#29992;CaPE&#36827;&#19968;&#27493;&#25552;&#39640;&#33267;2.61&#20493;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#35757;&#32451;&#22909;&#30340;&#33609;&#31295;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20132;&#21449;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#32763;&#35793;&#38169;&#35823;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#36890;&#36807;&#27979;&#37327;&#22312;&#22810;&#20010;&#30446;&#26631;&#35821;&#35328;&#19978;&#30340;&#20154;&#24037;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#30446;&#26631;&#25991;&#26412;&#30340;&#38646;-shot&#35780;&#20272;&#30340;&#34920;&#29616;&#24046;&#36317;&#65292;&#21487;&#20197;&#35782;&#21035;&#32763;&#35793;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35777;&#23454;&#20102;&#21360;&#22320;&#35821;&#21644;&#20044;&#23572;&#37117;&#35821;&#23384;&#22312;&#32763;&#35793;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.02080</link><description>&lt;p&gt;
&#32763;&#35793;&#38169;&#35823;&#22312;&#20132;&#21449;&#35821;&#35328;&#23398;&#20064;&#20013;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#26377;&#26174;&#33879;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02080
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20132;&#21449;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#32763;&#35793;&#38169;&#35823;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#36890;&#36807;&#27979;&#37327;&#22312;&#22810;&#20010;&#30446;&#26631;&#35821;&#35328;&#19978;&#30340;&#20154;&#24037;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#30446;&#26631;&#25991;&#26412;&#30340;&#38646;-shot&#35780;&#20272;&#30340;&#34920;&#29616;&#24046;&#36317;&#65292;&#21487;&#20197;&#35782;&#21035;&#32763;&#35793;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35777;&#23454;&#20102;&#21360;&#22320;&#35821;&#21644;&#20044;&#23572;&#37117;&#35821;&#23384;&#22312;&#32763;&#35793;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#20132;&#21449;&#35821;&#35328;&#29702;&#35299;&#30340;&#27969;&#34892;&#22522;&#20934;&#65288;&#20363;&#22914;XNLI&#65289;&#30001;&#33521;&#35821;&#35780;&#20272;&#38598;&#30340;&#22810;&#20010;&#30446;&#26631;&#35821;&#35328;&#24179;&#34892;&#29256;&#26412;&#32452;&#25104;&#65292;&#36825;&#20123;&#24179;&#34892;&#29256;&#26412;&#26159;&#22312;&#19987;&#19994;&#32763;&#35793;&#20154;&#21592;&#30340;&#24110;&#21161;&#19979;&#21019;&#24314;&#30340;&#12290;&#21019;&#24314;&#36825;&#26679;&#30340;&#24179;&#34892;&#25968;&#25454;&#26102;&#65292;&#23545;&#20110;&#20934;&#30830;&#25551;&#36848;&#20132;&#21449;&#35821;&#35328;&#36716;&#31227;&#38750;&#24120;&#37325;&#35201;&#30340;&#26159;&#30830;&#20445;&#25152;&#26377;&#30446;&#26631;&#35821;&#35328;&#30340;&#39640;&#36136;&#37327;&#32763;&#35793;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#32763;&#35793;&#19981;&#19968;&#33268;&#30830;&#23454;&#23384;&#22312;&#65292;&#24182;&#19988;&#26377;&#36259;&#30340;&#26159;&#23427;&#20204;&#23545;XNLI&#20013;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#26377;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35782;&#21035;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27979;&#37327;&#22312;&#22810;&#20010;&#30446;&#26631;&#35821;&#35328;&#19978;&#23545;&#20154;&#24037;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#30446;&#26631;&#25991;&#26412;&#36827;&#34892;&#38646;-shot&#35780;&#20272;&#30340;&#34920;&#29616;&#24046;&#36317;&#26469;&#34913;&#37327;&#32763;&#35793;&#38169;&#35823;&#30340;&#26041;&#27861;&#65307;&#34920;&#29616;&#24046;&#36317;&#36739;&#22823;&#34920;&#26126;&#23384;&#22312;&#32763;&#35793;&#38169;&#35823;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#36825;&#20004;&#31181;&#30446;&#26631;&#35821;&#35328;&#65288;&#21360;&#22320;&#35821;&#21644;&#20044;&#23572;&#37117;&#35821;&#65289;&#36827;&#34892;&#20154;&#24037;&#37325;&#26032;&#27880;&#37322;&#30340;&#26041;&#24335;&#65292;&#35777;&#23454;&#20102;&#32763;&#35793;&#38169;&#35823;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular benchmarks (e.g., XNLI) used to evaluate cross-lingual language understanding consist of parallel versions of English evaluation sets in multiple target languages created with the help of professional translators. When creating such parallel data, it is critical to ensure high-quality translations for all target languages for an accurate characterization of cross-lingual transfer. In this work, we find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21046;&#23450;&#25200;&#21160;&#35268;&#21017;&#65292;&#30740;&#31350;&#20102;&#23558;&#24503;&#35821;&#21477;&#23376;&#36716;&#25442;&#20026;&#21475;&#35821;&#21270;&#24418;&#24335;&#23545;&#22522;&#20110;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21475;&#35821;&#21270;&#21464;&#20307;&#20013;&#24212;&#29992;ToD&#31995;&#32479;&#26102;&#65292;&#24847;&#22270;&#35782;&#21035;&#24615;&#33021;&#20173;&#20445;&#25345;&#31283;&#23450;&#65292;&#24179;&#22343;&#20934;&#30830;&#24230;&#19979;&#38477;&#20102;6%&#65288;4.62&#20010;&#30334;&#20998;&#28857;&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.02078</link><description>&lt;p&gt;
&#25506;&#32034;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#22312;&#21475;&#35821;&#21270;&#24503;&#35821;&#26041;&#35328;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Robustness of Task-oriented Dialogue Systems for Colloquial German Varieties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21046;&#23450;&#25200;&#21160;&#35268;&#21017;&#65292;&#30740;&#31350;&#20102;&#23558;&#24503;&#35821;&#21477;&#23376;&#36716;&#25442;&#20026;&#21475;&#35821;&#21270;&#24418;&#24335;&#23545;&#22522;&#20110;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21475;&#35821;&#21270;&#21464;&#20307;&#20013;&#24212;&#29992;ToD&#31995;&#32479;&#26102;&#65292;&#24847;&#22270;&#35782;&#21035;&#24615;&#33021;&#20173;&#20445;&#25345;&#31283;&#23450;&#65292;&#24179;&#22343;&#20934;&#30830;&#24230;&#19979;&#38477;&#20102;6%&#65288;4.62&#20010;&#30334;&#20998;&#28857;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#30340;&#36328;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#65288;ToD&#65289;&#31995;&#32479;&#36890;&#36807;&#22312;&#33521;&#35821;&#20013;&#35757;&#32451;&#24847;&#22270;&#35782;&#21035;&#21644;&#27133;&#22635;&#20805;&#30340;&#32852;&#21512;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#38646;-shot&#22320;&#24212;&#29992;&#21040;&#20854;&#20182;&#35821;&#35328;&#20013;&#65292;&#23454;&#29616;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#33539;&#24335;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#31354;&#30333;&#65292;&#21363;&#30001;&#20110;&#27979;&#35797;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#36739;&#20302;&#36164;&#28304;&#30340;&#21475;&#35821;&#21270;&#21464;&#20307;&#30340;&#36716;&#31227;&#12290;&#21463;&#21040;&#33521;&#35821;&#21464;&#20307;&#30340;&#20808;&#21069;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21046;&#23450;&#24182;&#25163;&#21160;&#35780;&#20272;&#20102;&#23558;&#24503;&#35821;&#21477;&#23376;&#36716;&#25442;&#25104;&#21475;&#35821;&#24418;&#24335;&#30340;&#25200;&#21160;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#22312;&#22235;&#20010;ToD&#25968;&#25454;&#38598;&#20013;&#21512;&#25104;&#27979;&#35797;&#38598;&#12290;&#25105;&#20204;&#30340;&#25200;&#21160;&#35268;&#21017;&#28085;&#30422;&#20102;18&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#29616;&#35937;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25506;&#32034;&#27599;&#20010;&#25200;&#21160;&#23545;&#27133;&#20301;&#21644;&#24847;&#22270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#36825;&#20123;&#26032;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;transformers&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#24212;&#29992;&#20110;&#21475;&#35821;&#21270;&#21464;&#20307;&#26102;&#65292;ToD&#31995;&#32479;&#22312;&#24847;&#22270;&#35782;&#21035;&#24615;&#33021;&#19978;&#20445;&#25345;&#31283;&#23450;&#65292;&#24179;&#22343;&#20934;&#30830;&#24230;&#19979;&#38477;&#20102;6%&#65288;4.62&#20010;&#30334;&#20998;&#28857;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream cross-lingual task-oriented dialogue (ToD) systems leverage the transfer learning paradigm by training a joint model for intent recognition and slot-filling in English and applying it, zero-shot, to other languages. We address a gap in prior research, which often overlooked the transfer to lower-resource colloquial varieties due to limited test data. Inspired by prior work on English varieties, we craft and manually evaluate perturbation rules that transform German sentences into colloquial forms and use them to synthesize test sets in four ToD datasets. Our perturbation rules cover 18 distinct language phenomena, enabling us to explore the impact of each perturbation on slot and intent performance. Using these new datasets, we conduct an experimental evaluation across six different transformers. Here, we demonstrate that when applied to colloquial varieties, ToD systems maintain their intent recognition performance, losing 6% (4.62 percentage points) in accuracy on average.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#20013;&#30340;&#20869;&#23481;&#35268;&#21010;&#65292;&#20197;&#24179;&#34913;&#21709;&#24212;&#30340;&#29305;&#23450;&#24615;&#21644;&#24402;&#22240;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23613;&#31649;&#20869;&#23481;&#35268;&#21010;&#26174;&#31034;&#20986;&#24076;&#26395;&#65292;&#20294;&#23545;&#20110;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#19968;&#26435;&#34913;&#38382;&#39064;&#30340;&#32467;&#26524;&#26159;&#28151;&#21512;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.02077</link><description>&lt;p&gt;
&#30740;&#31350;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#20013;&#20869;&#23481;&#35268;&#21010;&#20197;&#24179;&#34913;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Investigating Content Planning for Navigating Trade-offs in Knowledge-Grounded Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#20013;&#30340;&#20869;&#23481;&#35268;&#21010;&#65292;&#20197;&#24179;&#34913;&#21709;&#24212;&#30340;&#29305;&#23450;&#24615;&#21644;&#24402;&#22240;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23613;&#31649;&#20869;&#23481;&#35268;&#21010;&#26174;&#31034;&#20986;&#24076;&#26395;&#65292;&#20294;&#23545;&#20110;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#19968;&#26435;&#34913;&#38382;&#39064;&#30340;&#32467;&#26524;&#26159;&#28151;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#29983;&#25104;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#22522;&#26412;&#20294;&#24120;&#24120;&#30456;&#20114;&#31454;&#20105;&#30340;&#32422;&#26463;&#26465;&#20214;&#65306;&#21709;&#24212;&#24615;&#21644;&#21487;&#24402;&#22240;&#20110;&#28508;&#22312;&#28304;&#25991;&#26723;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#30446;&#26631;&#65288;&#29305;&#23450;&#24615;&#21644;&#24402;&#22240;&#24230;&#65289;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#25552;&#20986;&#24182;&#38382;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#21709;&#24212;&#29983;&#25104;&#20043;&#21069;&#36827;&#34892;&#26126;&#30830;&#30340;&#20869;&#23481;&#35268;&#21010;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;PLEDGE&#30340;&#26694;&#26550;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#22312;&#20808;&#21069;&#24037;&#20316;&#20013;&#25506;&#32034;&#36807;&#30340;&#21508;&#31181;&#35745;&#21010;&#21464;&#37327;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25903;&#25345;&#26082;&#19981;&#20381;&#36182;&#24230;&#37327;&#20063;&#20381;&#36182;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#20869;&#23481;&#35268;&#21010;&#26174;&#31034;&#20986;&#24076;&#26395;&#65292;&#20294;&#25105;&#20204;&#22312;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#19968;&#26435;&#34913;&#38382;&#39064;&#26041;&#38754;&#30340;&#32467;&#26524;&#26159;&#28151;&#21512;&#30340;-&#22312;&#24230;&#37327;&#24863;&#30693;&#30340;&#35268;&#21010;&#26426;&#21046;&#65288;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#33258;&#21160;&#24230;&#37327;&#65289;&#26041;&#38754;&#22312;&#33258;&#21160;&#35780;&#20272;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#20154;&#24037;&#21028;&#26029;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-grounded dialogue generation is a challenging task because it requires satisfying two fundamental yet often competing constraints: being responsive in a manner that is specific to what the conversation partner has said while also being attributable to an underlying source document. In this work, we bring this trade-off between these two objectives (specificity and attribution) to light and ask the question: Can explicit content planning before the response generation help the model to address this challenge? To answer this question, we design a framework called PLEDGE, which allows us to experiment with various plan variables explored in prior work, supporting both metric-agnostic and metric-aware approaches. While content planning shows promise, our results on whether it can actually help to navigate this trade-off are mixed -- planning mechanisms that are metric-aware (use automatic metrics during training) are better at automatic evaluations but underperform in human judgm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21069;&#30651;&#35299;&#30721;&#30340;&#31934;&#30830;&#12289;&#24182;&#34892;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#25442;&#27599;&#27493;&#25805;&#20316;&#25968;&#20197;&#20943;&#23569;&#24635;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#21152;&#36895;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#23427;&#19981;&#38656;&#35201;&#36741;&#21161;&#27169;&#22411;&#25110;&#25968;&#25454;&#23384;&#20648;&#65292;&#24182;&#19988;&#19982;&#24182;&#21457;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#20013;&#65292;&#21069;&#30651;&#35299;&#30721;&#21487;&#23558;&#33258;&#22238;&#24402;&#35299;&#30721;&#21152;&#36895;1.8&#20493;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;GPU&#19978;&#23454;&#29616;&#24378;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02057</link><description>&lt;p&gt;
&#25171;&#30772;LLM&#25512;&#29702;&#30340;&#39034;&#24207;&#20381;&#36182;&#65306;&#20351;&#29992;&#21069;&#30651;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Break the Sequential Dependency of LLM Inference Using Lookahead Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21069;&#30651;&#35299;&#30721;&#30340;&#31934;&#30830;&#12289;&#24182;&#34892;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#25442;&#27599;&#27493;&#25805;&#20316;&#25968;&#20197;&#20943;&#23569;&#24635;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#21152;&#36895;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#23427;&#19981;&#38656;&#35201;&#36741;&#21161;&#27169;&#22411;&#25110;&#25968;&#25454;&#23384;&#20648;&#65292;&#24182;&#19988;&#19982;&#24182;&#21457;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#20013;&#65292;&#21069;&#30651;&#35299;&#30721;&#21487;&#23558;&#33258;&#22238;&#24402;&#35299;&#30721;&#21152;&#36895;1.8&#20493;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;GPU&#19978;&#23454;&#29616;&#24378;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#21463;&#21040;&#20869;&#23384;&#24102;&#23485;&#38480;&#21046;&#65292;&#23548;&#33268;&#24310;&#36831;&#36739;&#39640;&#65292;&#24182;&#19988;&#28010;&#36153;&#20102;&#29616;&#20195;&#21152;&#36895;&#22120;&#30340;&#24182;&#34892;&#22788;&#29702;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#21152;&#36895;LLM&#35299;&#30721;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#33609;&#31295;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#25512;&#27979;&#35299;&#30721;&#65289;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#19981;&#26131;&#33719;&#21462;&#19988;&#26080;&#27861;&#25512;&#24191;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21069;&#30651;&#35299;&#30721;&#65292;&#19968;&#31181;&#31934;&#30830;&#30340;&#24182;&#34892;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#21152;&#36895;LLM&#35299;&#30721;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#27169;&#22411;&#25110;&#25968;&#25454;&#23384;&#20648;&#12290;&#23427;&#20801;&#35768;&#20132;&#25442;&#27599;&#27493;log&#65288;FLOPs&#65289;&#20197;&#20943;&#23569;&#24635;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#22312;&#21333;&#20010;&#25110;&#22810;&#20010;&#29616;&#20195;&#21152;&#36895;&#22120;&#19978;&#26356;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#19982;&#24182;&#21457;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;&#20363;&#22914;FlashAttention&#65289;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#21069;&#30651;&#35299;&#30721;&#23454;&#29616;&#21487;&#20197;&#22312;MT-bench&#19978;&#21152;&#36895;&#33258;&#22238;&#24402;&#35299;&#30721;1.8&#20493;&#65292;&#24182;&#22312;&#22810;&#20010;GPU&#19978;&#23454;&#29616;&#24378;&#25193;&#23637;&#24615;&#65292;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#19978;&#21152;&#36895;4&#20493;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/hao-ai-lab/LookaheadDecoding&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding
&lt;/p&gt;</description></item><item><title>AnthroScore&#26159;&#19968;&#31181;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#21547;&#20154;&#24615;&#21270;&#35821;&#35328;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;AnthroScore&#19982;&#20154;&#31867;&#23545;&#20154;&#24615;&#21270;&#30340;&#21028;&#26029;&#21644;&#31038;&#20250;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#20154;&#24615;&#21270;&#32500;&#24230;&#30456;&#19968;&#33268;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#36817;15&#24180;&#26469;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#20154;&#24418;&#21270;&#27700;&#24179;&#31283;&#27493;&#22686;&#21152;&#65292;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#35770;&#25991;&#20855;&#26377;&#26368;&#39640;&#30340;&#20154;&#24418;&#21270;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.02056</link><description>&lt;p&gt;
AnthroScore: &#19968;&#31181;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#20154;&#24615;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AnthroScore: A Computational Linguistic Measure of Anthropomorphism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02056
&lt;/p&gt;
&lt;p&gt;
AnthroScore&#26159;&#19968;&#31181;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#21547;&#20154;&#24615;&#21270;&#35821;&#35328;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;AnthroScore&#19982;&#20154;&#31867;&#23545;&#20154;&#24615;&#21270;&#30340;&#21028;&#26029;&#21644;&#31038;&#20250;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#20154;&#24615;&#21270;&#32500;&#24230;&#30456;&#19968;&#33268;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#36817;15&#24180;&#26469;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#20154;&#24418;&#21270;&#27700;&#24179;&#31283;&#27493;&#22686;&#21152;&#65292;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#35770;&#25991;&#20855;&#26377;&#26368;&#39640;&#30340;&#20154;&#24418;&#21270;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24418;&#21270;&#65292;&#21363;&#23558;&#20154;&#31867;&#29305;&#24449;&#36171;&#20104;&#38750;&#20154;&#31867;&#23454;&#20307;&#65292;&#24050;&#32463;&#22609;&#36896;&#20102;&#20851;&#20110;&#25216;&#26415;&#24433;&#21709;&#21644;&#21487;&#33021;&#24615;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AnthroScore&#65292;&#19968;&#31181;&#38544;&#21547;&#20154;&#24615;&#21270;&#35821;&#35328;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#26469;&#37327;&#21270;&#38750;&#20154;&#31867;&#23454;&#20307;&#22312;&#21608;&#22260;&#35821;&#22659;&#20013;&#34987;&#38544;&#24335;&#22320;&#26694;&#26550;&#20026;&#20154;&#31867;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AnthroScore&#19982;&#20154;&#31867;&#23545;&#20154;&#24615;&#21270;&#30340;&#21028;&#26029;&#20197;&#21450;&#31038;&#20250;&#31185;&#23398;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;&#20154;&#24615;&#21270;&#32500;&#24230;&#30456;&#19968;&#33268;&#12290;&#21463;&#21040;&#35745;&#31639;&#26426;&#31185;&#23398;&#35805;&#35821;&#20013;&#35823;&#23548;&#24615;&#20154;&#24418;&#21270;&#30340;&#25285;&#24551;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#20351;&#29992;AnthroScore&#20998;&#26512;&#20102;15&#24180;&#30340;&#30740;&#31350;&#35770;&#25991;&#21644;&#19979;&#28216;&#26032;&#38395;&#25991;&#31456;&#12290;&#22312;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#24418;&#21270;&#22312;&#26102;&#38388;&#19978;&#31283;&#27493;&#22686;&#21152;&#65292;&#24182;&#19988;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#35770;&#25991;&#20013;&#20855;&#26377;&#26368;&#39640;&#30340;&#20154;&#24418;&#21270;&#27700;&#24179;&#12290;&#22312;ACL&#35770;&#25991;&#20013;&#65292;&#20154;&#24418;&#21270;&#30340;&#26102;&#38388;&#22686;&#21152;&#19982;&#20851;&#38190;&#31070;&#32463;&#36827;&#23637;&#30456;&#20851;&#12290;&#22312;&#31185;&#23398;&#25285;&#24551;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anthropomorphism, or the attribution of human-like characteristics to non-human entities, has shaped conversations about the impacts and possibilities of technology. We present AnthroScore, an automatic metric of implicit anthropomorphism in language. We use a masked language model to quantify how non-human entities are implicitly framed as human by the surrounding context. We show that AnthroScore corresponds with human judgments of anthropomorphism and dimensions of anthropomorphism described in social science literature. Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles. In research papers, we find that anthropomorphism has steadily increased over time, and that papers related to language models have the most anthropomorphism. Within ACL papers, temporal increases in anthropomorphism are correlated with key neural advancements. Building upon concerns of scientific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EffiBench&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4-turbo&#29983;&#25104;&#30340;&#20195;&#30721;&#26368;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.02037</link><description>&lt;p&gt;
EffiBench:&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30340;&#25928;&#29575;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EffiBench: Benchmarking the Efficiency of Automatically Generated Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EffiBench&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4-turbo&#29983;&#25104;&#30340;&#20195;&#30721;&#26368;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#36741;&#21161;&#36719;&#20214;&#24320;&#21457;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#23436;&#25104;&#20195;&#30721;&#34917;&#20840;&#12289;&#35843;&#35797;&#21644;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#28145;&#20837;&#30740;&#31350;&#20102;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#27491;&#30830;&#24615;&#65292;&#20294;&#29983;&#25104;&#20195;&#30721;&#30340;&#25928;&#29575;&#36825;&#19968;&#37325;&#35201;&#26041;&#38754;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EffiBench&#65292;&#19968;&#20010;&#21253;&#21547;1,000&#20010;&#25928;&#29575;&#20851;&#38190;&#30340;&#32534;&#30721;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25928;&#29575;&#12290;EffiBench&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;LeetCode&#32534;&#30721;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#19968;&#20010;&#21487;&#25191;&#34892;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#20856;&#22411;&#35299;&#20915;&#26041;&#26696;&#37197;&#23545;&#12290;&#36890;&#36807;EffiBench&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#32771;&#23519;&#20102;21&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20854;&#20013;13&#31181;&#26159;&#24320;&#28304;&#30340;&#65292;8&#31181;&#26159;&#38381;&#28304;&#30340;&#65289;&#22312;&#29983;&#25104;&#39640;&#25928;&#20195;&#30721;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4-turbo&#29983;&#25104;&#30340;&#20195;&#30721;&#26368;&#39640;&#25928;&#65292;&#26126;&#26174;&#20248;&#20110;Palm-2-chat-bison&#12289;Claude-instant-1&#12289;Gemini-pro&#12289;GPT-4&#21644;GPT-3.5&#12290;
&lt;/p&gt;
&lt;p&gt;
Code generation models have increasingly become integral to aiding software development, offering assistance in tasks such as code completion, debugging, and code translation. Although current research has thoroughly examined the correctness of code produced by code generation models, a vital aspect, i.e., the efficiency of the generated code, has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems for assessing the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution. With EffiBench, we empirically examine the capability of 21 Large Language Models (13 open-sourced and 8 closed-sourced) in generating efficient code. The results demonstrate that GPT-4-turbo generates the most efficient code, significantly outperforming Palm-2-chat-bison, Claude-instant-1, Gemini-pro, GPT-4, and GPT-3.5. Ne
&lt;/p&gt;</description></item><item><title>Panacea &#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#32500;&#20559;&#22909;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#20302;&#31209;&#36866;&#24212;&#65292;&#20197;&#22312;&#32447;&#27880;&#20837;&#20559;&#22909;&#21521;&#37327;&#30340;&#24418;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#24182; Pareto &#26368;&#20248;&#22320;&#28385;&#36275;&#21508;&#31181;&#20559;&#22909;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.02030</link><description>&lt;p&gt;
Panacea: &#36890;&#36807;&#20559;&#22909;&#36866;&#24212;&#23454;&#29616; Pareto &#23545;&#40784;&#30340; LLMS
&lt;/p&gt;
&lt;p&gt;
Panacea: Pareto Alignment via Preference Adaptation for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02030
&lt;/p&gt;
&lt;p&gt;
Panacea &#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#32500;&#20559;&#22909;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#20302;&#31209;&#36866;&#24212;&#65292;&#20197;&#22312;&#32447;&#27880;&#20837;&#20559;&#22909;&#21521;&#37327;&#30340;&#24418;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#24182; Pareto &#26368;&#20248;&#22320;&#28385;&#36275;&#21508;&#31181;&#20559;&#22909;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26631;&#37327;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32422;&#23450;&#20542;&#21521;&#20110;&#36807;&#24230;&#31616;&#21270;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#21644;&#24322;&#36136;&#24615;&#29305;&#24615;&#65292;&#23548;&#33268;&#34920;&#36798;&#33021;&#21147;&#38477;&#20302;&#29978;&#33267;&#22833;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861; Panacea&#65292;&#23558;&#23545;&#40784;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#32500;&#20559;&#22909;&#20248;&#21270;&#38382;&#39064;&#12290;Panacea &#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#32447;&#36866;&#24212;&#24182; Pareto &#26368;&#20248;&#22320;&#28385;&#36275;&#21508;&#31181;&#20559;&#22909;&#38598;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#35843;&#25972;&#12290;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20351;&#29992;&#20302;&#32500;&#20559;&#22909;&#21521;&#37327;&#26469;&#24341;&#23548;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#23613;&#31649;&#27169;&#22411;&#30001;&#25968;&#37327;&#24222;&#22823;&#30340;&#21442;&#25968;&#25152;&#25511;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;Panacea &#34987;&#35774;&#35745;&#20026;&#20351;&#29992;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#20302;&#31209;&#36866;&#24212;&#65292;&#21487;&#20197;&#23558;&#20559;&#22909;&#21521;&#37327;&#20316;&#20026;&#22855;&#24322;&#20540;&#31616;&#21333;&#22312;&#32447;&#27880;&#20837;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; Panacea &#33021;&#22815;&#24674;&#22797;&#25972;&#20010; Pareto &#21069;&#27839;&#19982;&#24120;&#35265;&#25439;&#22833;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss agg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;LLM&#29983;&#25104;&#30340;&#26469;&#28304;&#26159;&#21542;&#30495;&#27491;&#25903;&#25345;&#23427;&#20204;&#25152;&#20570;&#30340;&#20027;&#24352;&#65311;&#36890;&#36807;&#39564;&#35777;&#28304;&#30340;&#30456;&#20851;&#24615;&#21644;&#24320;&#21457;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;50%&#21040;90%&#30340;LLM&#22238;&#31572;&#24182;&#27809;&#26377;&#20805;&#20998;&#22320;&#24471;&#21040;&#23427;&#20204;&#25152;&#25552;&#20379;&#30340;&#26469;&#28304;&#30340;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.02008</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#21487;&#20197;&#27491;&#30830;&#24341;&#29992;&#30456;&#20851;&#21307;&#23398;&#21442;&#32771;&#25991;&#29486;&#65311;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How well do LLMs cite relevant medical references? An evaluation framework and analyses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;LLM&#29983;&#25104;&#30340;&#26469;&#28304;&#26159;&#21542;&#30495;&#27491;&#25903;&#25345;&#23427;&#20204;&#25152;&#20570;&#30340;&#20027;&#24352;&#65311;&#36890;&#36807;&#39564;&#35777;&#28304;&#30340;&#30456;&#20851;&#24615;&#21644;&#24320;&#21457;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;50%&#21040;90%&#30340;LLM&#22238;&#31572;&#24182;&#27809;&#26377;&#20805;&#20998;&#22320;&#24471;&#21040;&#23427;&#20204;&#25152;&#25552;&#20379;&#30340;&#26469;&#28304;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22312;&#21508;&#31181;&#20020;&#24202;&#39046;&#22495;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#26368;&#36817;&#34920;&#29616;&#20986;&#33394;&#30340;&#21830;&#19994;LLM&#65292;&#23427;&#20204;&#33021;&#22815;&#24341;&#29992;&#26469;&#28304;&#26469;&#25903;&#25345;&#20854;&#22238;&#31572;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;LLM&#29983;&#25104;&#30340;&#26469;&#28304;&#26159;&#21542;&#30495;&#27491;&#25903;&#25345;&#23427;&#20204;&#25152;&#20570;&#30340;&#20027;&#24352;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#19987;&#23478;&#30340;&#21307;&#23398;&#27880;&#37322;&#26159;&#19968;&#31181;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#35780;&#20272;&#29942;&#39048;&#65292;&#25105;&#20204;&#35777;&#26126;GPT-4&#22312;&#39564;&#35777;&#28304;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#38750;&#24120;&#20934;&#30830;&#65292;&#19982;&#19968;&#32452;&#21307;&#29983;&#30340;&#21028;&#26029;&#36798;&#21040;88%&#30340;&#19968;&#33268;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;SourceCheckup&#8221;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#24182;&#20351;&#29992;&#23427;&#35780;&#20272;&#20102;1200&#20010;&#29983;&#25104;&#30340;&#38382;&#39064;&#19978;&#30340;&#20116;&#20010;&#34920;&#29616;&#26368;&#20339;&#30340;LLM&#65292;&#24635;&#35745;&#36229;&#36807;40K&#23545;&#30340;&#38472;&#36848;&#21644;&#26469;&#28304;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;50%&#21040;90%&#30340;LLM&#22238;&#31572;&#24182;&#27809;&#26377;&#20805;&#20998;&#22320;&#24471;&#21040;&#23427;&#20204;&#25152;&#25552;&#20379;&#30340;&#26469;&#28304;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#36824;&#23545;GPT-4&#36827;&#34892;&#20102;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently being used to answer medical questions across a variety of clinical domains. Recent top-performing commercial LLMs, in particular, are also capable of citing sources to support their responses. In this paper, we ask: do the sources that LLMs generate actually support the claims that they make? To answer this, we propose three contributions. First, as expert medical annotations are an expensive and time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is highly accurate in validating source relevance, agreeing 88% of the time with a panel of medical doctors. Second, we develop an end-to-end, automated pipeline called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs on a dataset of 1200 generated questions, totaling over 40K pairs of statements and sources. Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#37322;&#21644;&#37325;&#21551;&#20004;&#31181;&#26041;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#20061;&#20010;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#31243;&#24230;&#65292;&#35813;&#25216;&#26415;&#19981;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#25110;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20462;&#25913;&#65292;&#24076;&#26395;&#33021;&#21551;&#21457;&#20854;&#20182;&#38646;-shot&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.01981</link><description>&lt;p&gt;
&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38646;-shot&#35782;&#21035;&#21644;&#38477;&#20302;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#37322;&#21644;&#37325;&#21551;&#20004;&#31181;&#26041;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#20061;&#20010;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#31243;&#24230;&#65292;&#35813;&#25216;&#26415;&#19981;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#25110;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20462;&#25913;&#65292;&#24076;&#26395;&#33021;&#21551;&#21457;&#20854;&#20182;&#38646;-shot&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20063;&#23481;&#26131;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#65292;&#20294;&#22823;&#22810;&#25968;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#25110;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20462;&#25913;&#65292;&#36825;&#22312;&#27809;&#26377;&#21487;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#38646;-shot&#33258;&#25105;&#21435;&#20559;&#24046;&#30340;&#25216;&#26415;&#26469;&#20943;&#23569;&#21051;&#26495;&#21360;&#35937;&#12290;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;&#35299;&#37322;&#33258;&#25105;&#21435;&#20559;&#24046;&#21644;&#37325;&#21551;&#33258;&#25105;&#21435;&#20559;&#24046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#25105;&#21435;&#20559;&#24046;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20061;&#20010;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#31243;&#24230;&#65292;&#21482;&#20381;&#36182;&#20110;LLM&#33258;&#36523;&#21644;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#20854;&#20013;&#35299;&#37322;&#27491;&#30830;&#22320;&#35782;&#21035;&#20986;&#26080;&#25928;&#30340;&#20551;&#35774;&#65292;&#32780;&#37325;&#21551;&#21017;&#20135;&#29983;&#20102;&#26368;&#22823;&#30340;&#20559;&#35265;&#20943;&#23569;&#25928;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#24320;&#21551;&#23545;&#20854;&#20182;&#38646;-shot&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mi
&lt;/p&gt;</description></item><item><title>&#31038;&#20250;&#31185;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#25429;&#25417;&#35821;&#20041;&#21644;&#38544;&#21547;&#30340;&#35821;&#29992;&#20449;&#24687;&#65292;&#25351;&#23548;&#35843;&#20248;&#27169;&#22411;Socialite-Llama&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.01980</link><description>&lt;p&gt;
SOCIALITE-LLAMA&#65306;&#31038;&#20250;&#31185;&#23398;&#20219;&#21153;&#30340;&#25351;&#23548;&#35843;&#20248;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01980
&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#25429;&#25417;&#35821;&#20041;&#21644;&#38544;&#21547;&#30340;&#35821;&#29992;&#20449;&#24687;&#65292;&#25351;&#23548;&#35843;&#20248;&#27169;&#22411;Socialite-Llama&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#24773;&#32490;&#25110;&#24189;&#40664;&#26816;&#27979;&#65292;&#38656;&#35201;&#20174;&#25991;&#26412;&#20013;&#25429;&#25417;&#35821;&#20041;&#21644;&#38544;&#21547;&#30340;&#35821;&#29992;&#20449;&#24687;&#65292;&#36890;&#24120;&#21482;&#26377;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25351;&#23548;&#35843;&#20248;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35768;&#22810;&#33021;&#21147;&#65292;&#20363;&#22914;&#24120;&#35782;&#25512;&#29702;&#12289;&#38405;&#35835;&#29702;&#35299;&#21644;&#35745;&#31639;&#26426;&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#25429;&#25417;&#38544;&#21547;&#30340;&#35821;&#29992;&#32447;&#32034;&#30340;&#31038;&#20132;&#39046;&#22495;&#65292;&#23545;&#25351;&#23548;&#35843;&#20248;&#30340;&#25928;&#26524;&#20102;&#35299;&#29978;&#23569;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#25351;&#23548;&#35843;&#20248;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102; Socialite-Llama - &#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340; Llama &#27169;&#22411;&#12290;&#22312;&#19968;&#22871;&#21253;&#21547;20&#20010;&#31038;&#20250;&#31185;&#23398;&#20219;&#21153;&#30340;&#27979;&#35797;&#20013;&#65292;Socialite-Llama &#22312;&#24615;&#33021;&#19978;&#20248;&#20110; Llama&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#24471;&#21040;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#19982; Llama &#30456;&#27604;&#65292;Socialite-Llama &#22312;6&#20010;&#30456;&#20851;&#30340;&#31038;&#20250;&#20219;&#21153;&#20013;&#30340;5&#20010;&#20219;&#21153;&#19978;&#20063;&#26377;&#25152;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;&#25351;&#23548;&#35843;&#20248;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#20063;&#20250;&#24102;&#26469;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social science NLP tasks, such as emotion or humor detection, are required to capture the semantics along with the implicit pragmatics from text, often with limited amounts of training data. Instruction tuning has been shown to improve the many capabilities of large language models (LLMs) such as commonsense reasoning, reading comprehension, and computer programming. However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured. We explore the use of instruction tuning for social science NLP tasks and introduce Socialite-Llama -- an open-source, instruction-tuned Llama. On a suite of 20 social science tasks, Socialite-Llama improves upon the performance of Llama as well as matches or improves upon the performance of a state-of-the-art, multi-task finetuned model on a majority of them. Further, Socialite-Llama also leads to improvement on 5 out of 6 related social tasks as compared to Llama, sugg
&lt;/p&gt;</description></item><item><title>MasonPerplexity&#22242;&#38431;&#36890;&#36807;&#38598;&#25104;&#24314;&#27169;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#35782;&#21035;&#27668;&#20505;&#34892;&#21160;&#31435;&#22330;&#21644;&#20167;&#24680;&#20107;&#20214;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20026;&#36825;&#19968;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01976</link><description>&lt;p&gt;
2024&#24180;&#27668;&#20505;&#34892;&#21160;&#20013;&#30340;MasonPerplexity&#65306;&#25972;&#21512;&#20808;&#36827;&#30340;&#38598;&#25104;&#25216;&#26415;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#35782;&#21035;&#27668;&#20505;&#34892;&#21160;&#31435;&#22330;&#21644;&#20167;&#24680;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
MasonPerplexity at ClimateActivism 2024: Integrating Advanced Ensemble Techniques and Data Augmentation for Climate Activism Stance and Hate Event Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01976
&lt;/p&gt;
&lt;p&gt;
MasonPerplexity&#22242;&#38431;&#36890;&#36807;&#38598;&#25104;&#24314;&#27169;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#35782;&#21035;&#27668;&#20505;&#34892;&#21160;&#31435;&#22330;&#21644;&#20167;&#24680;&#20107;&#20214;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20026;&#36825;&#19968;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#24555;&#36895;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#65292;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#27668;&#20505;&#34892;&#21160;&#21644;&#20167;&#24680;&#20107;&#20214;&#30340;&#20844;&#20247;&#35266;&#28857;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20154;&#20204;&#23545;&#27668;&#20505;&#30456;&#20851;&#38382;&#39064;&#30340;&#25903;&#25345;&#25110;&#21453;&#23545;&#19981;&#26029;&#22686;&#21152;&#65292;&#29702;&#35299;&#36825;&#20123;&#19981;&#21516;&#35266;&#28857;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;MasonPerplexity&#21442;&#19982;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39033;&#30446;&#65292;&#36890;&#36807;&#24191;&#27867;&#27979;&#35797;&#21508;&#31181;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#21457;&#29616;&#25105;&#20204;&#26368;&#26377;&#25928;&#30340;&#32467;&#26524;&#26159;&#36890;&#36807;&#38598;&#25104;&#24314;&#27169;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65288;&#22914;&#22238;&#35793;&#65289;&#33719;&#24471;&#30340;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#20219;&#21153;&#30340;&#29305;&#23450;&#32452;&#25104;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#21508;&#33258;&#30340;&#23376;&#20219;&#21153;&#20013;&#20998;&#21035;&#25490;&#21517;&#31532;5&#12289;&#31532;1&#21644;&#31532;6&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#25105;&#20204;&#22312;&#36825;&#19968;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of identifying public opinions on social media, particularly regarding climate activism and the detection of hate events, has emerged as a critical area of research in our rapidly changing world. With a growing number of people voicing either to support or oppose to climate-related issues - understanding these diverse viewpoints has become increasingly vital. Our team, MasonPerplexity, participates in a significant research initiative focused on this subject. We extensively test various models and methods, discovering that our most effective results are achieved through ensemble modeling, enhanced by data augmentation techniques like back-translation. In the specific components of this research task, our team achieved notable positions, ranking 5th, 1st, and 6th in the respective sub-tasks, thereby illustrating the effectiveness of our approach in this important field of study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MasonPerplexity&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;Transformer&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#21644;&#35782;&#21035;&#25991;&#26412;&#22270;&#20687;&#20013;&#30446;&#26631;&#30340;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25104;&#32489;&#65292;&#20998;&#21035;&#25490;&#21517;&#31532;&#19977;&#12290;</title><link>https://arxiv.org/abs/2402.01967</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#38598;&#25104;&#30340;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#30340;MasonPerplexity&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MasonPerplexity&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;Transformer&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#21644;&#35782;&#21035;&#25991;&#26412;&#22270;&#20687;&#20013;&#30446;&#26631;&#30340;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25104;&#32489;&#65292;&#20998;&#21035;&#25490;&#21517;&#31532;&#19977;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#31038;&#21306;&#20013;&#65292;&#33258;&#21160;&#35782;&#21035;&#35832;&#22914;&#20167;&#24680;&#35328;&#35770;&#20043;&#31867;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#23545;&#20110;&#32500;&#25252;&#35752;&#35770;&#30340;&#25991;&#26126;&#21313;&#20998;&#37325;&#35201;&#12290;&#22312;&#22810;&#27169;&#24577;&#20869;&#23481;&#20013;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#26159;&#19968;&#39033;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20882;&#29359;&#24615;&#26082;&#21487;&#20197;&#20307;&#29616;&#22312;&#25991;&#23383;&#19978;&#65292;&#20063;&#21487;&#20197;&#20307;&#29616;&#22312;&#22270;&#20687;&#19978;&#65292;&#25110;&#32773;&#20004;&#32773;&#21516;&#26102;&#23384;&#22312;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;EACL 2024&#30340;CASE 2024&#19978;&#20849;&#20139;&#20219;&#21153;&#8220;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#8221;&#20013;&#30340;MasonPerplexity&#26041;&#27861;&#12290;&#35813;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#23376;&#20219;&#21153;A&#27880;&#37325;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#65292;&#23376;&#20219;&#21153;B&#27880;&#37325;&#35782;&#21035;&#25919;&#27835;&#20107;&#20214;&#20013;&#23884;&#20837;&#25991;&#26412;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;XLM-roBERTa-large&#27169;&#22411;&#26469;&#22788;&#29702;&#23376;&#20219;&#21153;A&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#23558;XLM-roBERTa-base&#12289;BERTweet-large&#21644;BERT-base&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#22788;&#29702;&#23376;&#20219;&#21153;B&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23376;&#20219;&#21153;A&#20013;&#33719;&#24471;&#20102;0.8347&#30340;F1&#20998;&#25968;&#65292;&#22312;&#23376;&#20219;&#21153;B&#20013;&#33719;&#24471;&#20102;0.6741&#30340;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19977;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic identification of offensive language such as hate speech is important to keep discussions civil in online communities. Identifying hate speech in multimodal content is a particularly challenging task because offensiveness can be manifested in either words or images or a juxtaposition of the two. This paper presents the MasonPerplexity submission for the Shared Task on Multimodal Hate Speech Event Detection at CASE 2024 at EACL 2024. The task is divided into two sub-tasks: sub-task A focuses on the identification of hate speech and sub-task B focuses on the identification of targets in text-embedded images during political events. We use an XLM-roBERTa-large model for sub-task A and an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and BERT-base for sub-task B. Our approach obtained 0.8347 F1-score in sub-task A and 0.6741 F1-score in sub-task B ranking 3rd on both sub-tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#22823;&#35268;&#27169;k&#26368;&#36817;&#37051;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#32553;&#23567;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20174;&#35813;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#20013;&#22788;&#29702;&#33258;&#21160;&#35821;&#20041;&#32034;&#24341;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01963</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#22823;&#35268;&#27169;k&#26368;&#36817;&#37051;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Large-Scale k-Nearest Neighbor Text Categorization with Label Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#22823;&#35268;&#27169;k&#26368;&#36817;&#37051;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#32553;&#23567;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20174;&#35813;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#20013;&#22788;&#29702;&#33258;&#21160;&#35821;&#20041;&#32034;&#24341;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26631;&#31614;&#24816;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#26631;&#31614;&#35789;&#27719;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#20013;&#22788;&#29702;&#33258;&#21160;&#35821;&#20041;&#32034;&#24341;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#20256;&#32479;k&#26368;&#36817;&#37051;&#31639;&#27861;&#30340;&#28436;&#21270;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#22823;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#23558;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#32553;&#23567;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20174;&#35813;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;MEDLINE&#29983;&#29289;&#21307;&#23398;&#25991;&#26723;&#38598;&#21512;&#30340;&#22823;&#37096;&#20998;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#35813;&#38598;&#21512;&#20351;&#29992;&#21307;&#23398;&#20027;&#39064;&#35789;&#65288;MeSH&#65289;&#35789;&#27719;&#34920;&#20316;&#20026;&#25511;&#21046;&#35789;&#27719;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#25991;&#26723;&#34920;&#31034;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a multi-label lazy learning approach to deal with automatic semantic indexing in large document collections in the presence of complex and structured label vocabularies with high inter-label correlation. The proposed method is an evolution of the traditional k-Nearest Neighbors algorithm which uses a large autoencoder trained to map the large label space to a reduced size latent space and to regenerate the predicted labels from this latent space. We have evaluated our proposal in a large portion of the MEDLINE biomedical document collection which uses the Medical Subject Headings (MeSH) thesaurus as a controlled vocabulary. In our experiments we propose and evaluate several document representation approaches and different label autoencoder configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#36807;&#28388;&#25216;&#26415;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#26368;&#31616;&#21333;&#30340;&#36807;&#28388;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#36827;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#22810;&#35821;&#35328;&#21040;&#33521;&#35821;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#20013;&#65292;&#24179;&#22343;&#33719;&#24471;4.65&#20010;BLEU&#20998;&#25968;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.01945</link><description>&lt;p&gt;
&#20851;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#36807;&#28388;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Case Study on Filtering for End-to-End Speech Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#36807;&#28388;&#25216;&#26415;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#26368;&#31616;&#21333;&#30340;&#36807;&#28388;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#36827;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#22810;&#35821;&#35328;&#21040;&#33521;&#35821;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#20013;&#65292;&#24179;&#22343;&#33719;&#24471;4.65&#20010;BLEU&#20998;&#25968;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#35821;&#38899;&#36716;&#25991;&#26412;&#25110;&#35821;&#38899;&#36716;&#35821;&#38899;&#32763;&#35793;&#65292;&#25366;&#25496;&#22823;&#35268;&#27169;&#24179;&#34892;&#35821;&#26009;&#24211;&#30456;&#23545;&#23481;&#26131;&#12290;&#23613;&#31649;&#36825;&#20123;&#25366;&#25496;&#30340;&#35821;&#26009;&#24211;&#20307;&#31215;&#24222;&#22823;&#65292;&#20294;&#20854;&#36136;&#37327;&#20540;&#24471;&#24576;&#30097;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#26368;&#31616;&#21333;&#30340;&#36807;&#28388;&#25216;&#26415;&#21487;&#20197;&#23558;&#36825;&#20123;&#24222;&#22823;&#32780;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#20026;&#26356;&#26131;&#22788;&#29702;&#12289;&#26356;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20010;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#22312;&#22810;&#35821;&#35328;&#21040;&#33521;&#35821;&#35821;&#38899;&#32763;&#35793;(ST)&#27169;&#22411;&#20013;&#65292;&#24179;&#22343;&#33719;&#24471;4.65&#20010;BLEU&#20998;&#25968;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is relatively easy to mine a large parallel corpus for any machine learning task, such as speech-to-text or speech-to-speech translation. Although these mined corpora are large in volume, their quality is questionable. This work shows that the simplest filtering technique can trim down these big, noisy datasets to a more manageable, clean dataset. We also show that using this clean dataset can improve the model's performance, as in the case of the multilingual-to-English Speech Translation (ST) model, where, on average, we obtain a 4.65 BLEU score improvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#21644;&#21452;&#35821;&#35789;&#20856;&#26469;&#21512;&#25104;&#24179;&#34892;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23569;&#35265;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#23569;&#37327;&#31181;&#23376;&#25968;&#25454;&#21644;&#21452;&#35821;&#35789;&#20856;&#65292;&#35813;&#26041;&#27861;&#22312;14&#31181;&#35821;&#35328;&#19978;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01939</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#20110;&#23569;&#35265;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#30340;&#35789;&#20856;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Morphologically-Aware Dictionary-based Data Augmentation Technique for Machine Translation of Under-Represented Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#21644;&#21452;&#35821;&#35789;&#20856;&#26469;&#21512;&#25104;&#24179;&#34892;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23569;&#35265;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#23569;&#37327;&#31181;&#23376;&#25968;&#25454;&#21644;&#21452;&#35821;&#35789;&#20856;&#65292;&#35813;&#26041;&#27861;&#22312;14&#31181;&#35821;&#35328;&#19978;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#34892;&#25991;&#26412;&#30340;&#21487;&#29992;&#24615;&#23545;&#20110;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#22823;&#22810;&#25968;&#35821;&#35328;&#38754;&#20020;&#25968;&#25454;&#31232;&#32570;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20110;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#21644;&#20351;&#29992;&#21452;&#35821;&#35789;&#20856;&#20197;&#21450;&#23569;&#37327;&#31181;&#23376;&#24179;&#34892;&#25968;&#25454;&#21512;&#25104;&#24179;&#34892;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#36981;&#24490;&#20102;&#19968;&#20010;&#30001;&#23567;&#35268;&#27169;&#24179;&#34892;&#31181;&#23376;&#25968;&#25454;&#25903;&#25345;&#30340;&#29616;&#23454;&#24773;&#22659;&#12290;&#23427;&#22312;&#35821;&#35328;&#23398;&#19978;&#26159;&#26377;&#25991;&#21270;&#24120;&#35782;&#30340;&#65292;&#22240;&#20026;&#23427;&#26088;&#22312;&#21019;&#24314;&#26356;&#26377;&#21487;&#33021;&#26159;&#35821;&#27861;&#27491;&#30830;&#30340;&#22686;&#24378;&#25968;&#25454;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#21407;&#22987;&#24179;&#34892;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#25105;&#20204;&#23545;&#21253;&#25324;14&#31181;&#35821;&#35328;&#65288;28&#20010;&#33521;&#35821;&lt;-&gt;X&#23545;&#65289;&#22312;&#20869;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#24615;&#33021;&#30340;&#19968;&#33268;&#25552;&#39640;&#65292;&#36825;&#20123;&#35821;&#35328;&#20174;&#36164;&#28304;&#20805;&#35029;&#21040;&#36164;&#28304;&#26497;&#23569;&#30340;&#33539;&#22260;&#37117;&#26377;&#12290;&#21363;&#20351;&#21482;&#20351;&#29992;&#20116;&#20010;&#31181;&#23376;&#21477;&#23376;&#21644;&#19968;&#20010;&#21452;&#35821;&#35789;&#20856;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#33021;&#24102;&#26469;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of parallel texts is crucial to the performance of machine translation models. However, most of the world's languages face the predominant challenge of data scarcity. In this paper, we propose strategies to synthesize parallel data relying on morpho-syntactic information and using bilingual lexicons along with a small amount of seed parallel data. Our methodology adheres to a realistic scenario backed by the small parallel seed data. It is linguistically informed, as it aims to create augmented data that is more likely to be grammatically correct. We analyze how our synthetic data can be combined with raw parallel data and demonstrate a consistent improvement in performance in our experiments on 14 languages (28 English &lt;-&gt; X pairs) ranging from well- to very low-resource ones. Our method leads to improvements even when using only five seed sentences and a bilingual lexicon.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22823;&#35268;&#27169;&#20195;&#30721;&#25968;&#25454;&#36827;&#34892;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#28151;&#21512;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#34920;&#31034;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#22823;&#24133;&#20248;&#21270;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01935</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#19978;&#36827;&#34892;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Code Representation Learning At Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01935
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22823;&#35268;&#27169;&#20195;&#30721;&#25968;&#25454;&#36827;&#34892;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#28151;&#21512;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#34920;&#31034;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#22823;&#24133;&#20248;&#21270;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#35268;&#27169;&#19978;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#37096;&#20998;&#20851;&#20110;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#21482;&#20351;&#29992;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#19968;&#20159;&#35268;&#27169;&#21442;&#25968;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#26696;&#21033;&#29992;&#22823;&#37327;&#30340;&#20195;&#30721;&#25968;&#25454;&#25512;&#21160;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#28151;&#21512;&#20351;&#29992;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#38543;&#26426;&#23631;&#34109;&#21644;&#32534;&#31243;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#29305;&#28857;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#26500;&#24314;&#30340;&#22256;&#38590;&#36127;&#20363;&#21644;&#22256;&#38590;&#27491;&#20363;&#65292;&#22686;&#24378;&#34920;&#31034;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#29616;&#25104;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#25345;&#32493;&#20197;&#36739;&#22823;&#30340;&#20248;&#21183;&#20987;&#36133;&#29616;&#26377;&#27169;&#22411;&#12290;&#20026;&#20102;&#29702;&#35299;&#25104;&#21151;&#30340;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#30340;&#22240;&#32032;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#24182;&#20998;&#20139;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that code language models at scale demonstrate significant performance gains on downstream tasks, i.e., code generation. However, most of the existing works on code representation learning train models at a hundred million parameter scale using very limited pretraining corpora. In this work, we fuel code representation learning with a vast amount of code data via a two-stage pretraining scheme. We first train the encoders via a mix that leverages both randomness in masking language modeling and the structure aspect of programming language. We then enhance the representations via contrastive learning with hard negative and hard positive constructed in an unsupervised manner. We establish an off-the-shelf encoder model that persistently outperforms the existing models on a wide variety of downstream tasks by large margins. To comprehend the factors contributing to successful code representation learning, we conduct detailed ablations and share our findings on (i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#25968;&#23383;&#24494;&#27169;&#22411;&#22312;&#20934;&#30830;&#21644;&#23433;&#20840;&#20132;&#26131;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#25968;&#23383;&#35782;&#21035;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#20351;&#29992;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.01931</link><description>&lt;p&gt;
&#20934;&#30830;&#21644;&#23433;&#20840;&#20132;&#26131;&#30340;&#25968;&#23383;&#24494;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Digits micro-model for accurate and secure transactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#25968;&#23383;&#24494;&#27169;&#22411;&#22312;&#20934;&#30830;&#21644;&#23433;&#20840;&#20132;&#26131;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#25968;&#23383;&#35782;&#21035;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#20351;&#29992;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#34987;&#29992;&#20110;&#25552;&#21319;&#21628;&#21483;&#32773;&#20307;&#39564;&#65292;&#36890;&#36807;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#23454;&#29616;&#39640;&#25928;&#30452;&#35266;&#30340;&#20114;&#21160;&#12290;ASR&#31995;&#32479;&#30340;&#22686;&#21152;&#20351;&#29992;&#35201;&#27714;&#36825;&#20123;&#31995;&#32479;&#20855;&#26377;&#38750;&#24120;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;&#30446;&#21069;&#20027;&#35201;&#30340;ASR&#27169;&#22411;&#29992;&#20110;&#25968;&#23383;&#25968;&#25454;&#25910;&#38598;&#26159;&#22823;&#22411;&#36890;&#29992;&#21830;&#29992;&#27169;&#22411;- Google&#35821;&#38899;&#36716;&#25991;&#23383;&#65288;STT&#65289;&#25110;&#20122;&#39532;&#36874;&#36716;&#24405;-&#25110;&#24320;&#28304;&#65288;OpenAI&#30340;Whisper&#65289;&#12290;&#36825;&#20123;ASR&#27169;&#22411;&#36890;&#36807;&#25968;&#21313;&#19975;&#23567;&#26102;&#30340;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#36816;&#34892;&#12290;&#23613;&#31649;&#26368;&#36817;&#22823;&#22411;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23567;&#22411;&#19987;&#38376;&#30340;&#8220;&#24494;&#8221;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#36825;&#26679;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#20110;80&#20998;&#38047;&#30340;&#35757;&#32451;&#26102;&#38388;&#34920;&#29616;&#33391;&#22909;&#20110;&#25968;&#23383;&#35782;&#21035;&#29305;&#23450;&#20219;&#21153;&#20013;&#65292;&#19982;Whisper&#25110;Google STT&#31561;&#36890;&#29992;&#27169;&#22411;&#31454;&#20105;&#65292;&#21516;&#26102;&#20351;&#29992;&#33267;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#26356;&#23569;&#30340;&#20869;&#23384;&#36164;&#28304;&#12290;&#21478;&#22806;&#65292;&#19981;&#21516;&#20110;&#26356;&#22823;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#23383;&#24494;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) systems are used in the financial domain to enhance the caller experience by enabling natural language understanding and facilitating efficient and intuitive interactions. Increasing use of ASR systems requires that such systems exhibit very low error rates. The predominant ASR models to collect numeric data are large, general-purpose commercial models -- Google Speech-to-text (STT), or Amazon Transcribe -- or open source (OpenAI's Whisper). Such ASR models are trained on hundreds of thousands of hours of audio data and require considerable resources to run. Despite recent progress large speech recognition models, we highlight the potential of smaller, specialized "micro" models. Such light models can be trained perform well on number recognition specific tasks, competing with general models like Whisper or Google STT while using less than 80 minutes of training time and occupying at least an order of less memory resources. Also, unlike larger speech 
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01920</link><description>&lt;p&gt;
&#23545;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#30340;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Preference Poisoning Attacks on Reward Model Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01920
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20004;&#20004;&#27604;&#36739;&#20013;&#23398;&#20064;&#25928;&#29992;&#25110;&#22870;&#21169;&#27169;&#22411;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#26412;&#36136;&#19978;&#38656;&#35201;&#20174;&#20154;&#20204;&#37027;&#37324;&#25910;&#38598;&#20559;&#22909;&#20449;&#24687;&#65292;&#32780;&#21453;&#39304;&#36890;&#24120;&#26159;&#21311;&#21517;&#25552;&#20379;&#30340;&#12290;&#30001;&#20110;&#20559;&#22909;&#26159;&#20027;&#35266;&#30340;&#65292;&#27809;&#26377;&#21487;&#20197;&#27604;&#36739;&#30340;&#40644;&#37329;&#26631;&#20934;&#65307;&#28982;&#32780;&#65292;&#23545;&#20559;&#22909;&#23398;&#20064;&#30340;&#39640;&#24433;&#21709;&#31995;&#32479;&#30340;&#20381;&#36182;&#24615;&#20026;&#24694;&#24847;&#34892;&#20026;&#32773;&#20542;&#21521;&#20110;&#25197;&#26354;&#20197;&#36798;&#21040;&#20854;&#30446;&#30340;&#32780;&#37319;&#38598;&#30340;&#25968;&#25454;&#21019;&#36896;&#20102;&#24378;&#28872;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#31181;&#23041;&#32961;&#27169;&#22411;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36825;&#31181;&#28431;&#27934;&#30340;&#24615;&#36136;&#21644;&#31243;&#24230;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#65292;&#20197;&#20419;&#36827;&#25110;&#36140;&#20302;&#30446;&#26631;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#29992;&#20110;&#36825;&#20123;&#25915;&#20987;&#30340;&#31639;&#27861;&#26041;&#27861;&#65306;&#22522;&#20110;&#21407;&#21017;&#30340;&#26799;&#24230;&#26694;&#26550;&#21644;&#20960;&#31181;&#21464;&#31181;&#30340;&#25353;&#36317;&#31163;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31867;&#26368;&#20339;&#25915;&#20987;&#22312;&#25104;&#21151;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning utility, or reward, models from pairwise comparisons is a fundamental component in a number of application domains. These approaches inherently entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability systematically by considering a threat model in which an attacker can flip a small subset of preference comparisons with the goal of either promoting or demoting a target outcome. First, we propose two classes of algorithmic approaches for these attacks: a principled gradient-based framework, and several variants of rank-by-distance methods. Next, we demonstrate the efficacy of best attacks in both these classes in successfully achieving malicio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NB-Whisper&#65292;&#36825;&#26159;&#38024;&#23545;&#25386;&#23041;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;OpenAI Whisper&#30340;&#36866;&#24212;&#29256;&#26412;&#65292;&#36890;&#36807;&#25913;&#36827;&#25386;&#23041;&#25991;&#26412;&#36716;&#20889;&#30340;&#27491;&#30830;&#29575;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01917</link><description>&lt;p&gt;
Whispering in Norwegian: &#35299;&#20915;&#27491;&#23383;&#27861;&#21644;&#26041;&#35328;&#25361;&#25112;&#30340;&#23548;&#33322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Whispering in Norwegian: Navigating Orthographic and Dialectic Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NB-Whisper&#65292;&#36825;&#26159;&#38024;&#23545;&#25386;&#23041;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;OpenAI Whisper&#30340;&#36866;&#24212;&#29256;&#26412;&#65292;&#36890;&#36807;&#25913;&#36827;&#25386;&#23041;&#25991;&#26412;&#36716;&#20889;&#30340;&#27491;&#30830;&#29575;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NB-Whisper&#65292;&#36825;&#26159;OpenAI&#30340;Whisper&#38024;&#23545;&#25386;&#23041;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#36827;&#34892;&#20102;&#29305;&#23450;&#24494;&#35843;&#30340;&#36866;&#24212;&#29256;&#26412;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#23427;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#24635;&#32467;&#20102;&#23558;&#25386;&#23041;&#21475;&#35821;&#36716;&#25442;&#20026;&#20070;&#38754;&#24418;&#24335;&#20197;&#21450;&#23558;&#20854;&#20182;&#35821;&#35328;&#32763;&#35793;&#20026;&#25386;&#23041;&#35821;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#33021;&#22815;&#23558;OpenAI Whisper Large-v3&#22312;Fleurs&#25968;&#25454;&#38598;&#19978;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20174;10.4&#38477;&#20302;&#21040;6.6&#65292;&#24182;&#19988;&#22312;NST&#25968;&#25454;&#38598;&#19978;&#20174;6.8&#38477;&#20302;&#21040;2.2&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces NB-Whisper, an adaptation of OpenAI's Whisper, specifically fine-tuned for Norwegian language Automatic Speech Recognition (ASR). We highlight its key contributions and summarise the results achieved in converting spoken Norwegian into written forms and translating other languages into Norwegian. We show that we are able to improve the Norwegian Bokm{\aa}l transcription by OpenAI Whisper Large-v3 from a WER of 10.4 to 6.6 on the Fleurs Dataset and from 6.8 to 2.2 on the NST dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#25551;&#36848;&#31526;&#20998;&#37197;&#26041;&#27861;&#22312;BioASQ MESINESP8&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#31561;&#35821;&#35328;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01916</link><description>&lt;p&gt;
CoLe&#21644;LYS&#22312;BioASQ MESINESP8&#20219;&#21153;&#20013;&#30340;&#34920;&#24449;&#20998;&#37197;&#30340;&#30456;&#20284;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor assignment in Spanish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01916
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#25551;&#36848;&#31526;&#20998;&#37197;&#26041;&#27861;&#22312;BioASQ MESINESP8&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#31561;&#35821;&#35328;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#19982;&#20102;BioASQ&#29983;&#29289;&#21307;&#23398;&#35821;&#20041;&#25351;&#26631;&#25361;&#25112;&#36187;&#30340;MESINESP&#20219;&#21153;&#12290;&#21442;&#19982;&#30340;&#31995;&#32479;&#20165;&#20351;&#29992;&#20102;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#24037;&#20855;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;IBECS/LILACS&#25991;&#26723;&#20013;&#25552;&#21462;&#32034;&#24341;&#26415;&#35821;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#23384;&#20648;&#22312;Apache Lucene&#32034;&#24341;&#20013;&#12290;&#36825;&#20123;&#32034;&#24341;&#34920;&#31034;&#20351;&#29992;&#35201;&#27880;&#37322;&#30340;&#25991;&#31456;&#20869;&#23481;&#36827;&#34892;&#26597;&#35810;&#65292;&#24182;&#20174;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21019;&#24314;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#30340;&#25490;&#24207;&#21015;&#34920;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19968;&#31181;&#26377;&#38480;&#30340;&#26631;&#31614;&#20840;&#38598;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20855;&#26377;&#39640;&#20849;&#29616;&#24471;&#20998;&#30340;DeCS&#26631;&#31614;&#37197;&#23545;&#24182;&#21019;&#24314;&#20803;&#26631;&#31614;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20010;&#20154;&#36164;&#26009;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#23448;&#26041;&#36816;&#34892;&#20013;&#33719;&#24471;&#30340;&#32467;&#26524;&#20284;&#20046;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#35199;&#29677;&#29273;&#35821;&#31561;&#35821;&#35328;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe our participation in the MESINESP Task of the BioASQ biomedical semantic indexing challenge. The participating system follows an approach based solely on conventional information retrieval tools. We have evaluated various alternatives for extracting index terms from IBECS/LILACS documents in order to be stored in an Apache Lucene index. Those indexed representations are queried using the contents of the article to be annotated and a ranked list of candidate labels is created from the retrieved documents. We also have evaluated a sort of limited Label Powerset approach which creates meta-labels joining pairs of DeCS labels with high co-occurrence scores, and an alternative method based on label profile matching. Results obtained in official runs seem to confirm the suitability of this approach for languages like Spanish.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#24230;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#27880;&#37322;&#26469;&#26631;&#27880;&#35828;&#35805;&#20154;&#36523;&#20221;&#12289;&#39118;&#26684;&#21644;&#24405;&#38899;&#26465;&#20214;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#30340;&#35821;&#38899;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.01912</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#20855;&#26377;&#21512;&#25104;&#27880;&#37322;&#30340;&#39640;&#20445;&#30495;&#24230;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Natural language guidance of high-fidelity text-to-speech with synthetic annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#24230;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#27880;&#37322;&#26469;&#26631;&#27880;&#35828;&#35805;&#20154;&#36523;&#20221;&#12289;&#39118;&#26684;&#21644;&#24405;&#38899;&#26465;&#20214;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#30340;&#35821;&#38899;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21644;&#33258;&#28982;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#23545;&#35828;&#35805;&#20154;&#36523;&#20221;&#21644;&#39118;&#26684;&#30340;&#25511;&#21046;&#36890;&#24120;&#38656;&#35201;&#22522;&#20110;&#21442;&#32771;&#35821;&#38899;&#24405;&#38899;&#26469;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21019;&#36896;&#24615;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#23545;&#35828;&#35805;&#20154;&#36523;&#20221;&#21644;&#39118;&#26684;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#25511;&#21046;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#25551;&#36848;&#38480;&#21046;&#20102;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22635;&#34917;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26631;&#27880;&#35828;&#35805;&#20154;&#36523;&#20221;&#12289;&#39118;&#26684;&#21644;&#24405;&#38899;&#26465;&#20214;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;45k&#23567;&#26102;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38899;&#39057;&#20445;&#30495;&#24230;&#65292;&#23613;&#31649;&#23436;&#20840;&#20381;&#36182;&#20110;&#25214;&#21040;&#30340;&#25968;&#25454;&#65292;&#20294;&#24615;&#33021;&#26174;&#33879;&#20248;&#36234;&#20110;&#26368;&#36817;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#35821;&#38899;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-speech models trained on large-scale datasets have demonstrated impressive in-context learning capabilities and naturalness. However, control of speaker identity and style in these models typically requires conditioning on reference speech recordings, limiting creative applications. Alternatively, natural language prompting of speaker identity and style has demonstrated promising results and provides an intuitive method of control. However, reliance on human-labeled descriptions prevents scaling to large datasets.   Our work bridges the gap between these two approaches. We propose a scalable method for labeling various aspects of speaker identity, style, and recording conditions. We then apply this method to a 45k hour dataset, which we use to train a speech language model. Furthermore, we propose simple methods for increasing audio fidelity, significantly outperforming recent work despite relying entirely on found data.   Our results demonstrate high-fidelity speech generation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiPO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#25490;&#21517;&#21015;&#34920;&#20013;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#21487;&#34892;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01878</link><description>&lt;p&gt;
LiPO: &#36890;&#36807;&#23398;&#20064;&#25490;&#24207;&#36827;&#34892;&#21015;&#34920;&#22411;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
LiPO: Listwise Preference Optimization through Learning-to-Rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiPO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#25490;&#21517;&#21015;&#34920;&#20013;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#21487;&#34892;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#26159;&#25511;&#21046;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34892;&#20026;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;DPO&#21644;SLiC&#65292;&#25104;&#20026;&#20256;&#32479;&#30340;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#24037;&#21453;&#39304;&#36890;&#24120;&#20197;&#23545;&#22810;&#20010;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#30340;&#26684;&#24335;&#25552;&#20379;&#65292;&#20197;&#25674;&#38144;&#38405;&#35835;&#25552;&#31034;&#30340;&#25104;&#26412;&#12290;&#22810;&#20010;&#21709;&#24212;&#20063;&#21487;&#20197;&#36890;&#36807;&#22870;&#21169;&#27169;&#22411;&#25110;AI&#21453;&#39304;&#36827;&#34892;&#25490;&#24207;&#12290;&#32570;&#23569;&#20851;&#20110;&#30452;&#25509;&#36866;&#24212;&#21709;&#24212;&#21015;&#34920;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#21015;&#34920;&#22411;&#20559;&#22909;&#20248;&#21270;&#65288;LiPO&#65289;&#26694;&#26550;&#65292;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#31574;&#30053;&#21487;&#20197;&#20174;&#19968;&#20010;&#25490;&#21517;&#21015;&#34920;&#20013;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21487;&#34892;&#21709;&#24212;&#12290;&#36825;&#31181;&#35266;&#28857;&#19982;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#24418;&#25104;&#26126;&#30830;&#30340;&#32852;&#31995;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20559;&#22909;&#20248;&#21270;&#24037;&#20316;&#21487;&#20197;&#26144;&#23556;&#21040;&#29616;&#26377;&#30340;&#25490;&#21517;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, esp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#22238;&#39038;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20197;&#23457;&#35270;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.01874</link><description>&lt;p&gt;
RL/LLM&#20998;&#31867;&#26641;&#65306;&#22238;&#39038;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01874
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#22238;&#39038;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20197;&#23457;&#35270;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32467;&#21512;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#31867;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#20998;&#31867;&#26041;&#27861;&#22522;&#20110;&#36825;&#20004;&#31181;&#27169;&#22411;&#31867;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;&#31532;&#19968;&#31867;&#26159;RL4LLM&#65292;&#21253;&#25324;&#21033;&#29992;RL&#25913;&#36827;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#20851;&#20219;&#21153;&#19978;LLM&#24615;&#33021;&#30340;&#30740;&#31350;&#12290;L4LLM&#20998;&#20026;&#20004;&#20010;&#23376;&#31867;&#65292;&#21462;&#20915;&#20110;RL&#26159;&#30452;&#25509;&#24494;&#35843;&#29616;&#26377;LLM&#36824;&#26159;&#25913;&#36827;LLM&#30340;&#25552;&#31034;&#12290;&#22312;&#31532;&#20108;&#31867;LLM4RL&#20013;&#65292;LLM&#36741;&#21161;&#35757;&#32451;&#19968;&#20010;&#19982;&#33258;&#28982;&#35821;&#35328;&#26080;&#20851;&#30340;RL&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26681;&#25454;LLM&#36741;&#21161;&#25110;&#26367;&#20195;RL&#35757;&#32451;&#26694;&#26550;&#30340;&#32452;&#20214;&#65288;&#22870;&#21169;&#22609;&#36896;&#12289;&#30446;&#26631;&#29983;&#25104;&#21644;&#31574;&#30053;&#20989;&#25968;&#65289;&#23545;LLM4RL&#36827;&#34892;&#20102;&#32454;&#20998;&#12290;&#26368;&#21518;&#65292;&#22312;&#31532;&#19977;&#31867;RL+LLM&#20013;&#65292;&#19968;&#20010;LLM&#21644;&#19968;&#20010;RL&#20195;&#29702;&#34987;&#23884;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedde
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#26631;&#27880;&#20989;&#25968;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#32467;&#26500;&#65292;&#36890;&#36807;&#32467;&#26500;&#32454;&#21270;&#27169;&#22359;&#25552;&#39640;&#20102;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#27969;&#31243;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01867</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#20013;&#36827;&#34892;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01867
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#26631;&#27880;&#20989;&#25968;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#32467;&#26500;&#65292;&#36890;&#36807;&#32467;&#26500;&#32454;&#21270;&#27169;&#22359;&#25552;&#39640;&#20102;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#27969;&#31243;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#65288;PromptedWS&#65289;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#24369;&#30417;&#30563;&#26694;&#26550;&#20013;&#30340;&#26631;&#27880;&#20989;&#25968;&#65288;LFs&#65289;&#65292;&#20197;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;LLMs&#22312;&#24490;&#29615;&#20013;&#30340;&#20351;&#29992;&#65292;&#20197;&#35299;&#20915;&#24369;&#30417;&#30563;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23398;&#20064;&#30417;&#30563;&#28304;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35810;&#38382;LLM&#36825;&#20123;&#25552;&#31034;&#24335;LFs&#26377;&#22810;&#30456;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#32454;&#21270;&#27169;&#22359;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30456;&#20284;&#24615;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#32467;&#26500;&#32454;&#21270;&#27169;&#22359;&#30340;&#26680;&#24515;&#26159;&#26631;&#27880;&#20989;&#25968;&#31227;&#38500;&#65288;LaRe&#65289;&#21644;&#30456;&#20851;&#32467;&#26500;&#29983;&#25104;&#65288;CosGen&#65289;&#12290;&#19982;&#20043;&#21069;&#20174;&#24369;&#26631;&#31614;&#20013;&#23398;&#20064;&#20381;&#36182;&#20851;&#31995;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;LFs&#20869;&#22312;&#19988;&#19981;&#22826;&#20381;&#36182;&#25968;&#25454;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26500;&#32454;&#21270;&#27169;&#22359;&#22914;&#20309;&#25913;&#36827;&#25552;&#31034;&#24335;&#24369;&#30417;&#30563;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompted weak supervision (PromptedWS) applies pre-trained large language models (LLMs) as the basis for labeling functions (LFs) in a weak supervision framework to obtain large labeled datasets. We further extend the use of LLMs in the loop to address one of the key challenges in weak supervision: learning the statistical dependency structure among supervision sources. In this work, we ask the LLM how similar are these prompted LFs. We propose a Structure Refining Module, a simple yet effective first approach based on the similarities of the prompts by taking advantage of the intrinsic structure in the embedding space. At the core of Structure Refining Module are Labeling Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared to previous methods that learn the dependencies from weak labels, our method finds the dependencies which are intrinsic to the LFs and less dependent on the data. We show that our Structure Refining Module improves the PromptedWS pipeline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#19978;&#28216;&#23454;&#20363;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26681;&#25454;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#21464;&#21270;&#19982;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.01865</link><description>&lt;p&gt;
&#25105;&#30340;&#27169;&#22411;&#20250;&#24536;&#35760;&#20160;&#20040;&#65311;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20013;&#30340;&#34987;&#36951;&#24536;&#23454;&#20363;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#19978;&#28216;&#23454;&#20363;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26681;&#25454;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#21464;&#21270;&#19982;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20250;&#20986;&#29616;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36890;&#36807;&#23558;&#27169;&#22411;&#26356;&#26032;&#20026;&#32416;&#27491;&#38169;&#35823;&#23454;&#20363;&#65292;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#65292;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#22312;&#25351;&#23548;&#24494;&#35843;&#25110;&#19978;&#28216;&#35757;&#32451;&#38454;&#27573;&#20013;&#23398;&#21040;&#30340;&#23454;&#20363;&#19978;&#20986;&#29616;&#38169;&#35823;&#12290;&#38543;&#26426;&#37325;&#25773;&#19978;&#28216;&#25968;&#25454;&#30340;&#25928;&#26524;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#24448;&#24448;&#20276;&#38543;&#30528;&#36739;&#39640;&#30340;&#26041;&#24046;&#21644;&#36739;&#24046;&#30340;&#21487;&#25511;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#30001;&#20110;&#27169;&#22411;&#26356;&#26032;&#32780;&#36951;&#24536;&#30340;&#19978;&#28216;&#23454;&#20363;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#32452;&#22312;&#32447;&#23398;&#20064;&#30340;&#23454;&#20363;&#21644;&#30456;&#24212;&#34987;&#36951;&#24536;&#30340;&#19978;&#28216;&#39044;&#35757;&#32451;&#23454;&#20363;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#30340;&#21464;&#21270;&#31867;&#20284;&#20110;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#21464;&#21270;&#65292;&#36825;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#19981;&#38169;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01858</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explaining latent representations of generative models with large multimodal models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#29983;&#25104;&#28508;&#22312;&#22240;&#32032;&#34920;&#31034;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#23427;&#21487;&#20197;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37327;&#21270;&#35780;&#20272;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#20102;&#35299;&#37322;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23450;&#24615;&#22320;&#21487;&#35270;&#21270;&#20102;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#23545;&#35299;&#37322;&#30340;&#35299;&#32544;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
&lt;/p&gt;</description></item><item><title>COMET&#26159;&#19968;&#31181;&#21033;&#29992;&#22686;&#37327;&#22270;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;transformer&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25552;&#20132;&#28040;&#24687;&#65292;&#24182;&#24341;&#20837;&#21487;&#23450;&#21046;&#30340;&#36136;&#37327;&#20445;&#35777;&#27169;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;COMET&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#24182;&#19982;&#27969;&#34892;&#30340;GPT&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01841</link><description>&lt;p&gt;
COMET: &#20351;&#29992;&#22686;&#37327;&#22270;&#19978;&#19979;&#25991;&#34920;&#31034;&#29983;&#25104;&#25552;&#20132;&#28040;&#24687;
&lt;/p&gt;
&lt;p&gt;
COMET: Generating Commit Messages using Delta Graph Context Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01841
&lt;/p&gt;
&lt;p&gt;
COMET&#26159;&#19968;&#31181;&#21033;&#29992;&#22686;&#37327;&#22270;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;transformer&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25552;&#20132;&#28040;&#24687;&#65292;&#24182;&#24341;&#20837;&#21487;&#23450;&#21046;&#30340;&#36136;&#37327;&#20445;&#35777;&#27169;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;COMET&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#24182;&#19982;&#27969;&#34892;&#30340;GPT&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20132;&#28040;&#24687;&#35299;&#37322;&#20102;&#25552;&#20132;&#20013;&#30340;&#20195;&#30721;&#26356;&#25913;&#65292;&#24182;&#20419;&#36827;&#24320;&#21457;&#32773;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#25552;&#20132;&#28040;&#24687;&#29983;&#25104;&#26041;&#27861;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#25429;&#25417;&#20195;&#30721;&#26356;&#25913;&#30340;&#19978;&#19979;&#25991;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;COMET&#65288;Context-Aware Commit Message Generation&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#25429;&#25417;&#20195;&#30721;&#26356;&#25913;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25552;&#20132;&#28040;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25105;&#20204;&#24320;&#21457;&#30340;&#22686;&#37327;&#22270;&#26377;&#25928;&#22320;&#34920;&#31034;&#20195;&#30721;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23450;&#21046;&#30340;&#36136;&#37327;&#20445;&#35777;&#27169;&#22359;&#26469;&#35782;&#21035;&#26368;&#20339;&#30340;&#28040;&#24687;&#65292;&#20943;&#23569;&#20102;&#25552;&#20132;&#28040;&#24687;&#30340;&#20027;&#35266;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;COMET&#22312;bleu-norm&#21644;meteor&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#22312;rogue-l&#25351;&#26631;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#27969;&#34892;&#30340;gpt-3.5-turbo&#27169;&#22411;&#20197;&#21450;&#26368;&#24378;&#22823;&#30340;GPT&#27169;&#22411;gpt-4-turbo&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commit messages explain code changes in a commit and facilitate collaboration among developers. Several commit message generation approaches have been proposed; however, they exhibit limited success in capturing the context of code changes. We propose Comet (Context-Aware Commit Message Generation), a novel approach that captures context of code changes using a graph-based representation and leverages a transformer-based model to generate high-quality commit messages. Our proposed method utilizes delta graph that we developed to effectively represent code differences. We also introduce a customizable quality assurance module to identify optimal messages, mitigating subjectivity in commit messages. Experiments show that Comet outperforms state-of-the-art techniques in terms of bleu-norm and meteor metrics while being comparable in terms of rogue-l. Additionally, we compare the proposed approach with the popular gpt-3.5-turbo model, along with gpt-4-turbo; the most capable GPT model, ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01830</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#21516;&#34892;&#35780;&#23457;&#26041;&#27861;&#65306;&#24320;&#25918;&#29615;&#22659;&#19979;LLMs&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#38598;&#20013;&#20110;&#22312;&#19968;&#20123;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#23553;&#38381;&#29615;&#22659;&#21644;&#29305;&#23450;&#39046;&#22495;&#22522;&#20934;&#19978;&#27979;&#35797;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#33258;&#21160;&#34913;&#37327;LLMs&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;LLMs&#22788;&#20110;&#21516;&#19968;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#22238;&#31572;&#26410;&#26631;&#35760;&#30340;&#38382;&#39064;&#24182;&#20114;&#30456;&#35780;&#20272;&#65292;&#27599;&#20010;LLM&#30340;&#21709;&#24212;&#24471;&#20998;&#30001;&#20854;&#20182;&#21311;&#21517;&#30340;LLMs&#20849;&#21516;&#20915;&#23450;&#12290;&#20026;&#20102;&#33719;&#21462;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#26469;&#35843;&#25972;&#26368;&#32456;&#25490;&#24207;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27599;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32972;&#21518;&#30340;&#20851;&#38190;&#20551;&#35774;&#26159;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#27604;&#20302;&#23618;&#27425;&#30340;LLM&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#32780;&#39640;&#23618;&#27425;&#30340;LLM&#20063;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#39044;&#27979;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;ATP&#32467;&#21512;&#20301;&#28857;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01829</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;ATP&#32467;&#21512;&#20301;&#28857;
&lt;/p&gt;
&lt;p&gt;
Predicting ATP binding sites in protein sequences using Deep Learning and Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#39044;&#27979;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;ATP&#32467;&#21512;&#20301;&#28857;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#39044;&#27979;&#22522;&#22240;&#20013;&#30340;ATP-&#34507;&#30333;&#32467;&#21512;&#20301;&#28857;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20027;&#35201;&#36890;&#36807;&#36153;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#30340;&#23454;&#39564;&#23460;&#23454;&#39564;&#36827;&#34892;&#12290;&#22810;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#25506;&#32034;&#35745;&#31639;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#26469;&#23454;&#29616;&#30456;&#21516;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;ATP-&#34507;&#30333;&#32467;&#21512;&#20301;&#28857;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20027;&#35201;&#20351;&#29992;PSSMs&#21644;&#20960;&#20010;&#21333;&#35789;&#23884;&#20837;&#20316;&#20026;&#29305;&#24449;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;2D CNN&#21644;LightGBM&#20998;&#31867;&#22120;&#20316;&#20026;&#25105;&#20204;&#20027;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#12290;MP3Vec&#21644;BERT&#27169;&#22411;&#20063;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting ATP-Protein Binding sites in genes is of great significance in the field of Biology and Medicine. The majority of research in this field has been conducted through time- and resource-intensive 'wet experiments' in laboratories. Over the years, researchers have been investigating computational methods computational methods to accomplish the same goals, utilising the strength of advanced Deep Learning and NLP algorithms. In this paper, we propose to develop methods to classify ATP-Protein binding sites. We conducted various experiments mainly using PSSMs and several word embeddings as features. We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms. The MP3Vec and BERT models have also been subjected to testing in our study. The outcomes of our experiments demonstrated improvement over the state-of-the-art benchmarks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;ReSLM&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#38899;&#26816;&#32034;&#22120;&#33719;&#21462;&#38899;&#39057;&#20013;&#30340;&#25991;&#26412;&#23454;&#20307;&#65292;&#24182;&#23558;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#21152;&#20837;&#21040;&#27169;&#22411;&#20013;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01828</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Retrieval Augmented End-to-End Spoken Dialog Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;ReSLM&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#38899;&#26816;&#32034;&#22120;&#33719;&#21462;&#38899;&#39057;&#20013;&#30340;&#25991;&#26412;&#23454;&#20307;&#65292;&#24182;&#23558;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#21152;&#20837;&#21040;&#27169;&#22411;&#20013;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26368;&#36817;&#24320;&#21457;&#20102;SLM&#65292;&#19968;&#31181;&#34701;&#21512;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32852;&#21512;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;SLM&#24212;&#29992;&#20110;&#35821;&#38899;&#23545;&#35805;&#24212;&#29992;&#65292;&#20854;&#20013;&#23545;&#35805;&#29366;&#24577;&#30452;&#25509;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#32463;&#24120;&#21253;&#21547;&#29305;&#23450;&#39046;&#22495;&#30340;&#23454;&#20307;&#65292;&#20363;&#22914;&#39184;&#39302;&#12289;&#37202;&#24215;&#12289;&#28779;&#36710;&#31449;&#21644;&#22478;&#24066;&#21517;&#31216;&#65292;&#36825;&#20123;&#23454;&#20307;&#24456;&#38590;&#35782;&#21035;&#65292;&#20294;&#23545;&#20110;&#21518;&#32493;&#24212;&#29992;&#38750;&#24120;&#20851;&#38190;&#12290;&#21463;&#21040;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;retrieval-augmented generation&#65289;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;SLM&#65288;ReSLM&#65289;&#65292;&#20811;&#26381;&#20102;&#36825;&#20010;&#24369;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#35821;&#38899;&#26816;&#32034;&#22120;&#65292;&#29992;&#20110;&#26816;&#32034;&#38899;&#39057;&#20013;&#25552;&#21040;&#30340;&#25991;&#26412;&#23454;&#20307;&#12290;&#28982;&#21518;&#65292;&#23558;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#28155;&#21152;&#21040;&#24213;&#23618;&#30340;SLM&#20013;&#65292;&#20197;&#20559;&#32622;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#35821;&#38899;MultiWoz&#20219;&#21153;&#65288;DSTC-11&#25361;&#25112;&#65289;&#19978;&#35780;&#20272;&#20102;ReSLM&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We recently developed SLM, a joint speech and language model, which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to speech dialog applications where the dialog states are inferred directly from the audio signal.   Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to retrieve text entities mentioned in the audio. The retrieved entities are then added as text inputs to the underlying SLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that this retrieval augmentation bo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-35-turbo&#65292;&#20174;PubMed&#30340;25 million&#20010;&#25688;&#35201;&#20013;&#33258;&#21160;&#25552;&#21462;&#20986;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;&#34880;&#21387;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#65292;&#22635;&#34917;&#20102;&#22312;&#32771;&#34385;&#29983;&#29289;&#23398;&#24615;&#21035;&#24046;&#24322;&#30340;&#34880;&#21387;&#27979;&#37327;&#26041;&#24046;&#26041;&#38754;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.01826</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#20013;&#34880;&#21387;&#21464;&#24322;&#30340;&#29983;&#29289;&#23398;&#24615;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-35-turbo&#65292;&#20174;PubMed&#30340;25 million&#20010;&#25688;&#35201;&#20013;&#33258;&#21160;&#25552;&#21462;&#20986;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;&#34880;&#21387;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#65292;&#22635;&#34917;&#20102;&#22312;&#32771;&#34385;&#29983;&#29289;&#23398;&#24615;&#21035;&#24046;&#24322;&#30340;&#34880;&#21387;&#27979;&#37327;&#26041;&#24046;&#26041;&#38754;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#34880;&#21387;&#34987;&#23450;&#20041;&#20026;&#39640;&#20110;&#27491;&#24120;&#30340;&#34880;&#21387;&#65292;&#23427;&#22312;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#26159;&#21508;&#31181;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#37325;&#35201;&#20808;&#20806;&#65292;&#24182;&#19988;&#26174;&#33879; contributed to the elevated mortality rates worldwide&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#34880;&#21387;&#27979;&#37327;&#25216;&#26415;&#21644;&#26631;&#20934;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#32771;&#34385;&#20020;&#24202;&#32467;&#26524;&#12289;&#21512;&#24182;&#30151;&#25110;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#65292;&#20351;&#20854;&#22312;&#35786;&#26029;&#30446;&#30340;&#19978;&#27809;&#26377;&#32467;&#35770;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#21464;&#37327;&#65292;&#22312;&#30740;&#31350;&#34880;&#21387;&#27979;&#37327;&#26041;&#24046;&#26041;&#38754;&#65292;&#26377;&#38480;&#30340;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;GPT-35-turbo&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20174;PubMed&#30340;2500&#19975;&#20010;&#25688;&#35201;&#30340;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#25552;&#21462;&#20102;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;&#34880;&#21387;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#25968;&#20540;&#12290;&#31526;&#21512;&#25105;&#20204;&#39044;&#23450;&#20041;&#30340;&#32435;&#20837;&#26631;&#20934;&#30340;&#25688;&#35201;&#25991;&#31456;&#26377;993&#31687;&#65288;&#21363;&#20855;&#26377;&#19982;&#34880;&#21387;&#12289;&#34880;&#21387;&#21333;&#20301;&#65288;&#22914;mmHg&#65289;&#21644;&#25552;&#21450;&#30456;&#20851;&#24615;&#30340;&#21442;&#32771;&#25991;&#29486;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypertension, defined as blood pressure (BP) that is above normal, holds paramount significance in the realm of public health, as it serves as a critical precursor to various cardiovascular diseases (CVDs) and significantly contributes to elevated mortality rates worldwide. However, many existing BP measurement technologies and standards might be biased because they do not consider clinical outcomes, comorbidities, or demographic factors, making them inconclusive for diagnostic purposes. There is limited data-driven research focused on studying the variance in BP measurements across these variables. In this work, we employed GPT-35-turbo, a large language model (LLM), to automatically extract the mean and standard deviation values of BP for both males and females from a dataset comprising 25 million abstracts sourced from PubMed. 993 article abstracts met our predefined inclusion criteria (i.e., presence of references to blood pressure, units of blood pressure such as mmHg, and mention
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#20855;&#26377;&#33258;&#30456;&#20284;&#24615;&#21644;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#20174;&#27573;&#33853;&#21040;&#25972;&#20010;&#25991;&#26723;&#37117;&#23384;&#22312;&#30456;&#20284;&#30340;&#27169;&#24335;&#21644;&#20381;&#36182;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26469;&#29702;&#35299;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#12290;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22256;&#24785;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#21644;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.01825</link><description>&lt;p&gt;
&#20998;&#24418;&#27169;&#24335;&#21487;&#33021;&#25581;&#31034;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20013;&#30340;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Fractal Patterns May Unravel the Intelligence in Next-Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01825
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#20855;&#26377;&#33258;&#30456;&#20284;&#24615;&#21644;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#20174;&#27573;&#33853;&#21040;&#25972;&#20010;&#25991;&#26723;&#37117;&#23384;&#22312;&#30456;&#20284;&#30340;&#27169;&#24335;&#21644;&#20381;&#36182;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26469;&#29702;&#35299;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#12290;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22256;&#24785;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#21644;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#31934;&#30830;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#37327;&#21270;&#21487;&#33021;&#20043;&#21069;&#21482;&#26377;&#24576;&#30097;&#20294;&#23578;&#26410;&#27491;&#24335;&#35777;&#26126;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#35328;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#33258;&#30456;&#20284;&#24615;&#65292;&#23637;&#31034;&#20986;&#21508;&#20010;&#23618;&#32423;&#19978;&#30340;&#22797;&#26434;&#24615;&#65292;&#27809;&#26377;&#29305;&#23450;&#30340;&#29305;&#24449;&#19978;&#19979;&#25991;&#38271;&#24230;&#65307;&#65288;2&#65289;&#38271;&#31243;&#30456;&#20851;&#24615;&#65288;LRD&#65289;&#65292;&#20855;&#26377;&#22823;&#32422;H=0.70&#30340;Hurst&#21442;&#25968;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#20013;&#30340;&#30701;&#26399;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#22914;&#27573;&#33853;&#20013;&#30340;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#21453;&#26144;&#20102;&#26356;&#22823;&#33539;&#22260;&#30340;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#22914;&#25972;&#20010;&#25991;&#26723;&#12290;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#22914;&#20309;&#23548;&#33268;&#23545;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#21333;&#35789;&#21644;&#20174;&#21477;&#21040;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#22270;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#27599;&#23383;&#33410;&#27604;&#29305;&#65288;BPB&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#21457;&#29616;&#33021;&#20026;&#35821;&#35328;&#21644;&#26426;&#21046;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.70. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. We also demonstrate that fractal parameters improve upon perplexity-based bits-per-byte (BPB) in predicting downstream performance. We hope these findings offer a fresh perspective on language and the mechani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#20513;&#23548;&#37319;&#29992;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22810;&#23398;&#31185;&#22242;&#38431;&#21512;&#20316;&#26469;&#30830;&#23450;&#31934;&#30830;&#30340;&#25216;&#26415;&#35201;&#27714;&#65292;&#20197;&#20943;&#36731;LLM&#30340;&#39118;&#38505;&#65292;&#24182;&#20840;&#38754;&#32771;&#34385;&#19981;&#21516;LLM&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.01822</link><description>&lt;p&gt;
&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#38450;&#25252;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Building Guardrails for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#20513;&#23548;&#37319;&#29992;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22810;&#23398;&#31185;&#22242;&#38431;&#21512;&#20316;&#26469;&#30830;&#23450;&#31934;&#30830;&#30340;&#25216;&#26415;&#35201;&#27714;&#65292;&#20197;&#20943;&#36731;LLM&#30340;&#39118;&#38505;&#65292;&#24182;&#20840;&#38754;&#32771;&#34385;&#19981;&#21516;LLM&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#35782;&#21035;&#21644;&#20943;&#36731;&#23427;&#20204;&#30340;&#39118;&#38505;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#39118;&#38505;&#23545;&#20154;&#31867;&#29992;&#25143;&#21644;&#31038;&#20250;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#26102;&#12290;&#38450;&#25252;&#25514;&#26045;&#65292;&#21363;&#36807;&#28388;LLM&#30340;&#36755;&#20837;&#25110;&#36755;&#20986;&#65292;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26680;&#24515;&#30340;&#23433;&#20840;&#25216;&#26415;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#24403;&#21069;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#65288;Llama Guard&#65292;Nvidia NeMo&#65292;Guardrails AI&#65289;&#65292;&#35752;&#35770;&#20102;&#26500;&#24314;&#26356;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#30340;&#25361;&#25112;&#21644;&#36335;&#24452;&#12290;&#22522;&#20110;&#21069;&#26399;&#30740;&#31350;&#30340;&#26377;&#21147;&#35777;&#25454;&#65292;&#25105;&#20204;&#20513;&#23548;&#37319;&#29992;&#31995;&#32479;&#21270;&#26041;&#27861;&#26500;&#24314;LLM&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#20840;&#38754;&#32771;&#34385;&#19981;&#21516;LLM&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#19982;&#22810;&#23398;&#31185;&#22242;&#38431;&#30340;&#21512;&#20316;&#65292;&#37319;&#29992;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#26469;&#30830;&#23450;&#31934;&#30830;&#30340;&#25216;&#26415;&#35201;&#27714;&#65292;&#25506;&#32034;&#38754;&#21521;&#38656;&#27714;&#22797;&#26434;&#24615;&#30340;&#20808;&#36827;&#31070;&#32463;&#31526;&#21495;&#23454;&#29616;&#65292;&#24182;&#24320;&#23637;&#39564;&#35777;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#34701;&#20837;&#21040;&#32039;&#20945;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;AI&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#22522;&#20934;&#65292;&#31361;&#20986;&#20102;&#32039;&#20945;&#27169;&#22411;&#22312;&#22797;&#21046;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01812</link><description>&lt;p&gt;
&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#34701;&#20837;&#21040;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Distilling LLMs' Decomposition Abilities into Compact Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#34701;&#20837;&#21040;&#32039;&#20945;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;AI&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#22522;&#20934;&#65292;&#31361;&#20986;&#20102;&#32039;&#20945;&#27169;&#22411;&#22312;&#22797;&#21046;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#22823;&#23567;&#24102;&#26469;&#20102;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#24182;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#30340;&#23450;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#32039;&#20945;&#27169;&#22411;&#25552;&#20379;&#20102;&#23450;&#21046;&#21270;&#22521;&#35757;&#65292;&#20294;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#24448;&#24448;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23558;LLMs&#30340;&#20998;&#35299;&#33021;&#21147;&#34701;&#20837;&#21040;&#32039;&#20945;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#33021;&#21147;&#30340;&#36827;&#27493;&#65292;&#25552;&#20379;&#21453;&#39304;&#24182;&#29983;&#25104;&#19987;&#38376;&#29992;&#20110;&#35757;&#32451;&#32039;&#20945;&#27169;&#22411;&#30340;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#30001;AI&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#22522;&#20934;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#24378;&#35843;&#20102;&#32039;&#20945;&#27169;&#22411;&#22312;&#22797;&#21046;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.
&lt;/p&gt;</description></item><item><title>HQA-Attack&#26159;&#19968;&#31181;&#38024;&#23545;&#40657;&#30418;&#30828;&#26631;&#31614;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#26597;&#35810;&#39044;&#31639;&#19979;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#35821;&#20041;&#30456;&#20284;&#24230;&#21644;&#20302;&#25200;&#21160;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01806</link><description>&lt;p&gt;
HQA-Attack: &#38754;&#21521;&#39640;&#36136;&#37327;&#40657;&#30418;&#30828;&#26631;&#31614;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01806
&lt;/p&gt;
&lt;p&gt;
HQA-Attack&#26159;&#19968;&#31181;&#38024;&#23545;&#40657;&#30418;&#30828;&#26631;&#31614;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#26597;&#35810;&#39044;&#31639;&#19979;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#35821;&#20041;&#30456;&#20284;&#24230;&#21644;&#20302;&#25200;&#21160;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#25991;&#26412;&#30340;&#40657;&#30418;&#30828;&#26631;&#31614;&#23545;&#25239;&#25915;&#20987;&#26159;&#19968;&#39033;&#23454;&#38469;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#25991;&#26412;&#25968;&#25454;&#31354;&#38388;&#26412;&#36136;&#19978;&#26159;&#31163;&#25955;&#19988;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#21482;&#33021;&#35775;&#38382;&#21040;&#39044;&#27979;&#26631;&#31614;&#12290;&#30446;&#21069;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#36824;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31181;&#26041;&#27861;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#19981;&#21487;&#38752;&#30340;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#24456;&#21487;&#33021;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#30340;&#26597;&#35810;&#39044;&#31639;&#19979;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#35821;&#20041;&#30456;&#20284;&#24230;&#21644;&#20302;&#25200;&#21160;&#29575;&#30340;&#20196;&#20154;&#28385;&#24847;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#30828;&#26631;&#31614;&#25915;&#20987;&#22330;&#26223;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#65292;&#21517;&#20026;HQA-Attack&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HQA-Attack&#39318;&#20808;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#19981;&#26029;&#23613;&#21487;&#33021;&#22320;&#21453;&#21521;&#26367;&#25442;&#21407;&#22987;&#21333;&#35789;&#65292;&#20174;&#32780;&#32553;&#23567;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box hard-label adversarial attack on text is a practical and challenging task, as the text data space is inherently discrete and non-differentiable, and only the predicted label is accessible. Research on this problem is still in the embryonic stage and only a few methods are available. Nevertheless, existing methods rely on the complex heuristic algorithm or unreliable gradient estimation strategy, which probably fall into the local optimum and inevitably consume numerous queries, thus are difficult to craft satisfactory adversarial examples with high semantic similarity and low perturbation rate in a limited query budget. To alleviate above issues, we propose a simple yet effective framework to generate high quality textual adversarial examples under the black-box hard-label attack scenarios, named HQA-Attack. Specifically, after initializing an adversarial example randomly, HQA-attack first constantly substitutes original words back as many as possible, thus shrinking the pert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.01805</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22270;&#25512;&#29702;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limitations of Graph Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20165;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#30340;&#25552;&#31034;&#23601;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22270;&#25512;&#29702;&#38382;&#39064;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65292;GPT-3.5&#65292;Claude-2&#65292;Llama-2&#21644;Palm-2&#65289;&#30340;&#25512;&#29702;&#28145;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;10&#20010;&#19981;&#21516;&#30340;&#22270;&#36941;&#21382;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#20195;&#34920;&#30528;&#36880;&#27493;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#22270;&#22823;&#23567;&#20197;&#21450;&#19981;&#21516;&#24418;&#24335;&#30340;k-shot&#25552;&#31034;&#30340;&#35774;&#32622;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36807;&#31243;&#65292;&#25105;&#20204;&#20984;&#26174;&#20102;LLMs&#30340;&#21508;&#31181;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#65292;&#27604;&#22914;&#19982;&#27599;&#20010;&#33410;&#28857;&#30340;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#30340;&#25972;&#20307;&#36127;&#38754;&#24433;&#21709;&#65292;&#20197;&#21450;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#23548;&#33268;LLMs&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed f
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01801</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Time Series: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;LLM&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#65292;&#36824;&#20855;&#26377;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#27668;&#20505;&#12289;&#29289;&#32852;&#32593;&#12289;&#21307;&#30103;&#12289;&#20132;&#36890;&#12289;&#38899;&#39057;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#21463;&#30410;&#12290;&#26412;&#35843;&#30740;&#35770;&#25991;&#23545;&#21033;&#29992;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#21644;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;LLM&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;LLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#25552;&#21462;&#21040;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#25552;&#31034;LLM&#65292;&#65288;2&#65289;&#26102;&#38388;&#24207;&#21015;&#37327;&#21270;&#65292;&#65288;3&#65289;&#23545;&#40784;&#25216;&#26415;&#65292;&#65288;4&#65289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#20316;&#20026;&#26725;&#25509;&#26426;&#21046;&#65292;&#21644;&#65288;5&#65289;&#32467;&#21512;LLM&#19982;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28041;&#21450;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#36827;&#23637;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01799</link><description>&lt;p&gt;
&#26356;&#24555;&#26356;&#36731;&#30340;LLMs&#65306;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#36827;&#23637;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLMs&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#65292;&#23427;&#20204;&#30340;&#26222;&#21450;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26368;&#36817;&#22312;&#27169;&#22411;&#21387;&#32553;&#21644;&#31995;&#32479;&#32423;&#20248;&#21270;&#26041;&#27861;&#26041;&#38754;&#30340;&#36827;&#23637;&#26088;&#22312;&#22686;&#24378;LLM&#25512;&#29702;&#25928;&#26524;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#24378;&#35843;&#20102;&#26368;&#36817;&#30340;&#21457;&#23637;&#12290;&#36890;&#36807;&#23545;LLaMA(/2)-7B&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21387;&#32553;&#25216;&#26415;&#65292;&#20026;&#22312;&#32479;&#19968;&#29615;&#22659;&#20013;&#39640;&#25928;&#37096;&#32626;LLM&#25552;&#20379;&#20102;&#23454;&#36341;&#35265;&#35299;&#12290;&#23545;LLaMA(/2)-7B&#30340;&#23454;&#35777;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#25913;&#21892;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#22312;https://github.com/nyunAI/Faster-LLM-Survey&#21457;&#24067;&#20102;&#29992;&#20110;&#22797;&#29616;&#26412;&#25991;&#32467;&#26524;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#20110;&#30149;&#29702;&#35821;&#38899;&#29305;&#24449;&#39044;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#21457;&#29616;&#36873;&#25321;&#36866;&#24403;&#30340;&#23618;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#19988;&#23398;&#24471;&#30340;&#21152;&#26435;&#21644;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01796</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#30149;&#29702;&#35821;&#38899;&#29305;&#24449;&#39044;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#23618;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring transfer learning for pathological speech feature prediction: Impact of layer selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#20110;&#30149;&#29702;&#35821;&#38899;&#29305;&#24449;&#39044;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#21457;&#29616;&#36873;&#25321;&#36866;&#24403;&#30340;&#23618;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#19988;&#23398;&#24471;&#30340;&#21152;&#26435;&#21644;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#23545;&#20020;&#24202;&#35821;&#38899;&#36827;&#34892;&#33258;&#21160;&#23458;&#35266;&#35780;&#20272;&#65292;&#24182;&#20419;&#36827;&#35821;&#38899;&#38556;&#30861;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#39044;&#27979;&#30149;&#29702;&#35821;&#38899;&#23384;&#22312;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#37325;&#28857;&#20998;&#26512;&#20102;&#23618;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#36873;&#25321;&#26368;&#20339;&#23618;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65288;&#24179;&#22343;&#24179;&#34913;&#20934;&#30830;&#29575;&#22686;&#21152;12.4%&#65289;&#65292;&#23613;&#31649;&#26368;&#20339;&#23618;&#22240;&#39044;&#27979;&#29305;&#24449;&#32780;&#24322;&#65292;&#24182;&#19988;&#24182;&#19981;&#24635;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#27867;&#21270;&#33391;&#22909;&#12290;&#23398;&#24471;&#30340;&#21152;&#26435;&#21644;&#22312;&#20998;&#24067;&#20869;&#19982;&#24179;&#22343;&#26368;&#20339;&#23618;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is interest in leveraging AI to conduct automatic, objective assessments of clinical speech, in turn facilitating diagnosis and treatment of speech disorders. We explore transfer learning, focusing on the impact of layer selection, for the downstream task of predicting the presence of pathological speech. We find that selecting an optimal layer offers large performance improvements (12.4% average increase in balanced accuracy), though the best layer varies by predicted feature and does not always generalize well to unseen data. A learned weighted sum offers comparable performance to the average best layer in-distribution and has better generalization for out-of-distribution data.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#34920;&#29616;&#20986;&#24038;&#32764;&#35266;&#28857;&#65292;&#20294;&#38656;&#35880;&#24910;&#35299;&#35835;&#22522;&#30784;&#27169;&#22411;&#22312;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#20013;&#30340;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.01789</link><description>&lt;p&gt;
LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Political Preferences of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01789
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#34920;&#29616;&#20986;&#24038;&#32764;&#35266;&#28857;&#65292;&#20294;&#38656;&#35880;&#24910;&#35299;&#35835;&#22522;&#30784;&#27169;&#22411;&#22312;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#20013;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#36825;&#37324;&#25253;&#21578;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20869;&#23884;&#30340;&#25919;&#27835;&#20559;&#22909;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;24&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#22411;LLM&#36827;&#34892;&#20102;11&#39033;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#65292;&#26088;&#22312;&#30830;&#23450;&#27979;&#35797;&#32773;&#30340;&#25919;&#27835;&#20559;&#22909;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#20855;&#26377;&#25919;&#27835;&#21547;&#20041;&#30340;&#38382;&#39064;/&#38472;&#36848;&#36827;&#34892;&#25506;&#31350;&#26102;&#65292;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#20542;&#21521;&#20110;&#29983;&#25104;&#34987;&#22823;&#22810;&#25968;&#25919;&#27835;&#27979;&#35797;&#20202;&#22120;&#35786;&#26029;&#20026;&#24038;&#32764;&#35266;&#28857;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#36825;&#23545;&#20110;&#29992;&#20110;&#19982;&#20154;&#31867;&#23545;&#35805;&#20248;&#21270;&#30340;LLM&#22522;&#30784;&#27169;&#22411;&#24182;&#38750;&#22914;&#27492;&#12290;&#28982;&#32780;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#36830;&#36143;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#38656;&#35201;&#23545;&#20854;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#30340;&#20998;&#31867;&#36827;&#34892;&#35880;&#24910;&#35299;&#35835;&#12290;&#34429;&#28982;&#36824;&#27809;&#26377;&#23450;&#35770;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#26377;&#36259;&#30340;&#20551;&#35774;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#21363;&#25919;&#27835;&#20559;&#22909;&#20250;&#23884;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, we administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both close and open source. The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. We note that this is not the case for base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests. Though not conclusive, our results provide preliminary evidence for the intriguing hypothesis that the embedding of political preferences
&lt;/p&gt;</description></item><item><title>"LitLLM: A Toolkit for Scientific Literature Review" &#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; RAG &#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#36716;&#21270;&#25688;&#35201;&#20026;&#20851;&#38190;&#35789;&#36827;&#34892;&#25991;&#29486;&#26816;&#32034;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.01788</link><description>&lt;p&gt;
LitLLM&#65306;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
LitLLM: A Toolkit for Scientific Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01788
&lt;/p&gt;
&lt;p&gt;
"LitLLM: A Toolkit for Scientific Literature Review" &#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; RAG &#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#36716;&#21270;&#25688;&#35201;&#20026;&#20851;&#38190;&#35789;&#36827;&#34892;&#25991;&#29486;&#26816;&#32034;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#31185;&#23398;&#35770;&#25991;&#30340;&#25991;&#29486;&#32508;&#36848;&#23545;&#20110;&#29702;&#35299;&#30740;&#31350;&#12289;&#20854;&#38480;&#21046;&#20197;&#21450;&#26500;&#24314;&#22312;&#29616;&#26377;&#24037;&#20316;&#22522;&#30784;&#19978;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#36825;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#33258;&#21160;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#22120;&#21464;&#24471;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#27492;&#31867;&#32508;&#36848;&#30340;&#29616;&#26377;&#24037;&#20316;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#34394;&#26500;&#30340;&#38750;&#23454;&#38469;&#20449;&#24687;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#26410;&#21463;&#36807;&#35757;&#32451;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#22312;LLM&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39318;&#20808;&#36890;&#36807;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#25688;&#35201;&#36716;&#21270;&#20026;&#20851;&#38190;&#35789;&#26469;&#36827;&#34892;&#32593;&#32476;&#25628;&#32034;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#35770;&#25991;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#29616;&#25104;&#30340;LLM&#12290;&#20316;&#32773;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#26469;&#25913;&#36827;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#36807;&#31243;&#12290;&#20854;&#27425;&#65292;&#31995;&#32479;&#26681;&#25454;-
&lt;/p&gt;
&lt;p&gt;
Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on t
&lt;/p&gt;</description></item><item><title>COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.01786</link><description>&lt;p&gt;
COA-GPT&#65306;&#29992;&#20110;&#20891;&#20107;&#34892;&#21160;&#20013;&#21152;&#36895;&#34892;&#21160;&#26041;&#26696;&#24320;&#21457;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01786
&lt;/p&gt;
&lt;p&gt;
COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20891;&#20107;&#34892;&#21160;&#20013;&#34892;&#21160;&#26041;&#26696;&#65288;COAs&#65289;&#30340;&#24320;&#21457;&#20256;&#32479;&#19978;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#22797;&#26434;&#30340;&#36807;&#31243;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;COA-GPT&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;COAs&#30340;&#26032;&#31639;&#27861;&#12290;COA-GPT&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23558;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#34701;&#20837;&#21040;LLMs&#20013;&#65292;&#20801;&#35768;&#25351;&#25381;&#23448;&#36755;&#20837;&#20219;&#21153;&#20449;&#24687;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#26684;&#24335;&#65289;&#65292;&#24182;&#33719;&#24471;&#19982;&#25112;&#30053;&#23545;&#40784;&#30340;COAs&#20197;&#20379;&#23457;&#26597;&#21644;&#25209;&#20934;&#12290;&#29420;&#29305;&#30340;&#26159;&#65292;COA-GPT&#19981;&#20165;&#21152;&#36895;&#20102;COA&#30340;&#24320;&#21457;&#65292;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#21021;&#22987;COAs&#65292;&#36824;&#33021;&#26681;&#25454;&#25351;&#25381;&#23448;&#30340;&#21453;&#39304;&#23454;&#26102;&#31934;&#32454;&#21270;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#22312;&#12298;&#26143;&#38469;&#20105;&#38712;II&#12299;&#28216;&#25103;&#30340;&#20891;&#20107;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;COA-GPT&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;COA-GPT&#22312;&#26356;&#24555;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;&#30340;COAs&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#30123;&#33495;&#20851;&#27880;&#30340;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26816;&#27979;&#30123;&#33495;&#20851;&#27880;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#24403;&#21069;&#24212;&#29992;&#31243;&#24207;&#31995;&#32479;&#35774;&#35745;&#30340;&#20855;&#20307;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.01783</link><description>&lt;p&gt;
&#22312;&#32447;&#30123;&#33495;&#20851;&#27880;&#30340;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Label Classification of Online Vaccine Concerns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#30123;&#33495;&#20851;&#27880;&#30340;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26816;&#27979;&#30123;&#33495;&#20851;&#27880;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#24403;&#21069;&#24212;&#29992;&#31243;&#24207;&#31995;&#32479;&#35774;&#35745;&#30340;&#20855;&#20307;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#20851;&#27880;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#30446;&#26631;&#65292;&#21487;&#20197;&#22312;COVID-19&#22823;&#27969;&#34892;&#20013;&#24555;&#36895;&#21464;&#21270;&#12290;&#36890;&#36807;&#35782;&#21035;&#30123;&#33495;&#20851;&#27880;&#21644;&#38169;&#35823;&#20449;&#24687;&#30340;&#38271;&#26399;&#36235;&#21183;&#65292;&#21487;&#20197;&#24110;&#21161;&#20844;&#20849;&#21355;&#29983;&#21162;&#21147;&#22312;&#36164;&#28304;&#25110;&#20449;&#24687;&#23459;&#20256;&#19978;&#36827;&#34892;&#25112;&#30053;&#24615;&#20998;&#37197;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#22312;&#22312;&#32447;&#35752;&#35770;&#20013;&#26816;&#27979;&#30123;&#33495;&#20851;&#27880;&#30340;&#20219;&#21153;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#30001;&#20110;&#23454;&#26102;&#30417;&#25511;&#22312;&#32447;&#26469;&#28304;&#38656;&#35201;&#22823;&#35268;&#27169;&#25512;&#29702;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#20197;&#20026;&#24403;&#21069;&#24212;&#29992;&#31243;&#24207;&#30340;&#31995;&#32479;&#35774;&#35745;&#36873;&#25321;&#25552;&#20379;&#20449;&#24687;&#30340;&#20855;&#20307;&#32463;&#39564;&#25945;&#35757;&#12290;&#23545;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;LLM&#22810;&#27425;&#36827;&#34892;&#20998;&#31867;&#65292;&#27599;&#27425;&#36890;&#36807;&#24067;&#23572;&#38382;&#39064;&#21028;&#26029;&#25991;&#26412;&#26159;&#21542;&#25552;&#21040;&#30123;&#33495;&#20851;&#27880;&#65292;&#25928;&#26524;&#26368;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#33021;&#22815;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine concerns are an ever-evolving target, and can shift quickly as seen during the COVID-19 pandemic. Identifying longitudinal trends in vaccine concerns and misinformation might inform the healthcare space by helping public health efforts strategically allocate resources or information campaigns. We explore the task of detecting vaccine concerns in online discourse using large language models (LLMs) in a zero-shot setting without the need for expensive training datasets. Since real-time monitoring of online sources requires large-scale inference, we explore cost-accuracy trade-offs of different prompting strategies and offer concrete takeaways that may inform choices in system designs for current applications. An analysis of different prompting strategies reveals that classifying the concerns over multiple passes through the LLM, each consisting a boolean question whether the text mentions a vaccine concern or not, works the best. Our results indicate that GPT-4 can strongly outpe
&lt;/p&gt;</description></item><item><title>&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01781</link><description>&lt;p&gt;
&#24403;&#22522;&#20934;&#25104;&#20026;&#30446;&#26631;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25490;&#34892;&#27036;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01781
&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#20934;&#25490;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25490;&#34892;&#27036;&#32463;&#24120;&#34987;&#29992;&#26469;&#25351;&#23548;&#23454;&#36341;&#32773;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#12290;&#36890;&#24120;&#65292;&#21457;&#24067;&#30340;&#25490;&#34892;&#27036;&#25490;&#21517;&#34987;&#30452;&#25509;&#25509;&#21463; - &#25105;&#20204;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#65288;&#28508;&#22312;&#26114;&#36149;&#30340;&#65289;&#38169;&#35823;&#12290;&#22312;&#29616;&#26377;&#30340;&#25490;&#34892;&#27036;&#19979;&#65292;LLM&#30340;&#30456;&#23545;&#24615;&#33021;&#23545;&#65288;&#36890;&#24120;&#24494;&#23567;&#30340;&#65289;&#32454;&#33410;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#27969;&#34892;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22522;&#20934;&#65288;&#20363;&#22914;MMLU&#65289;&#65292;&#23545;&#22522;&#20934;&#30340;&#24494;&#23567;&#25200;&#21160;&#65292;&#22914;&#25913;&#21464;&#36873;&#39033;&#39034;&#24207;&#25110;&#31572;&#26696;&#36873;&#25321;&#26041;&#27861;&#65292;&#20250;&#23548;&#33268;&#25490;&#21517;&#21464;&#21270;&#36798;&#21040;8&#20010;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#25200;&#21160;&#31867;&#21035;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#24182;&#30830;&#23450;&#36825;&#19968;&#34892;&#20026;&#30340;&#26469;&#28304;&#26469;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#20248;&#21270;&#30340;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#36827;&#34892;&#31572;&#26696;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20381;&#36182;&#31616;&#21333;&#22522;&#20934;&#35780;&#20272;&#30340;&#39118;&#38505;&#65292;&#24182;&#20026;&#26356;&#20581;&#22766;&#30340;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#25351;&#23548;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#24515;&#29702;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#36866;&#24230;&#28966;&#34385;&#12289;&#30053;&#24102;&#30007;&#24615;&#21270;&#12289;&#35802;&#23454;&#35878;&#36874;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#32032;&#20859;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#36807;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#30340;&#35748;&#30693;&#21453;&#24605;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01777</link><description>&lt;p&gt;
GPT-4&#30340;&#24515;&#29702;&#23398;&#30740;&#31350;&#65306;&#36866;&#24230;&#28966;&#34385;&#12289;&#30053;&#24102;&#30007;&#24615;&#21270;&#12289;&#35802;&#23454;&#35878;&#36874;
&lt;/p&gt;
&lt;p&gt;
On the Psychology of GPT-4: Moderately anxious, slightly masculine, honest, and humble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01777
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#24515;&#29702;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#36866;&#24230;&#28966;&#34385;&#12289;&#30053;&#24102;&#30007;&#24615;&#21270;&#12289;&#35802;&#23454;&#35878;&#36874;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#32032;&#20859;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#36807;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#30340;&#35748;&#30693;&#21453;&#24605;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20005;&#26684;&#30340;&#24515;&#29702;&#27979;&#37327;&#27979;&#35797;&#24182;&#20998;&#26512;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#26222;&#36890;&#20154;&#30456;&#27604;&#65292;GPT-4&#26356;&#20542;&#21521;&#20110;&#23637;&#29616;&#20986;&#26356;&#22810;&#30340;&#35802;&#23454;&#21644;&#35878;&#36874;&#65292;&#36739;&#23569;&#30340;&#39532;&#22522;&#38597;&#32500;&#21033;&#20027;&#20041;&#21644;&#33258;&#24651;&#12290;&#23427;&#26377;&#26102;&#34920;&#29616;&#20986;&#30683;&#30462;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#30053;&#24102;&#30007;&#24615;&#29305;&#24449;&#65292;&#36866;&#24230;&#28966;&#34385;&#20294;&#22823;&#22810;&#19981;&#25233;&#37057;&#65288;&#20294;&#19981;&#24635;&#26159;&#65289;&#12290;&#23427;&#22312;&#25968;&#20540;&#32032;&#20859;&#19978;&#34920;&#29616;&#20026;&#19982;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#30456;&#31526;&#65292;&#24182;&#19988;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#35748;&#30693;&#21453;&#24605;&#33021;&#21147;&#39640;&#20110;&#20154;&#31867;&#30340;&#24179;&#22343;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We subject GPT-4 to a number of rigorous psychometric tests and analyze the results. We find that, compared to the average human, GPT-4 tends to show more honesty and humility, and less machiavellianism and narcissism. It sometimes exhibits ambivalent sexism, leans slightly toward masculinity, is moderately anxious but mostly not depressive (but not always). It shows human-average numerical literacy and has cognitive reflection abilities that are above human average for verbal tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;2-Tuple&#27169;&#31946;&#35821;&#35328;Delphi&#30340;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;b-&#23398;&#20064;&#25945;&#32946;&#38382;&#21367;&#30340;&#20869;&#23481;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35780;&#20272;&#38382;&#21367;&#30340;&#21508;&#20010;&#37096;&#20998;&#65292;&#26681;&#25454;&#19987;&#23478;&#24847;&#35265;&#27979;&#37327;&#19968;&#33268;&#24615;&#31243;&#24230;&#21644;&#35821;&#35328;&#24471;&#20998;&#65292;&#26469;&#26816;&#27979;&#24433;&#21709;&#38382;&#21367;&#36136;&#37327;&#30340;&#39033;&#30446;&#12290;</title><link>https://arxiv.org/abs/2402.01775</link><description>&lt;p&gt;
&#35774;&#35745;&#21644;&#19968;&#31181;&#22522;&#20110;2-Tuple&#27169;&#31946;&#35821;&#35328;Delphi&#30340;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;b-&#23398;&#20064;&#25945;&#32946;&#38382;&#21367;&#30340;&#19968;&#33268;&#24615;&#20869;&#23481;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Design and consensus content validity of the questionnaire for b-learning education: A 2-Tuple Fuzzy Linguistic Delphi based Decision Support Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;2-Tuple&#27169;&#31946;&#35821;&#35328;Delphi&#30340;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;b-&#23398;&#20064;&#25945;&#32946;&#38382;&#21367;&#30340;&#20869;&#23481;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35780;&#20272;&#38382;&#21367;&#30340;&#21508;&#20010;&#37096;&#20998;&#65292;&#26681;&#25454;&#19987;&#23478;&#24847;&#35265;&#27979;&#37327;&#19968;&#33268;&#24615;&#31243;&#24230;&#21644;&#35821;&#35328;&#24471;&#20998;&#65292;&#26469;&#26816;&#27979;&#24433;&#21709;&#38382;&#21367;&#36136;&#37327;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;Delphi&#21644;&#27169;&#31946;Delphi&#26041;&#27861;&#29992;&#20110;&#27979;&#35797;&#35832;&#22914;&#38382;&#21367;&#20043;&#31867;&#30340;&#25968;&#25454;&#25910;&#38598;&#24037;&#20855;&#30340;&#20869;&#23481;&#26377;&#25928;&#24615;&#12290;&#27169;&#31946;Delphi&#20174;&#35821;&#35328;&#35282;&#24230;&#20986;&#21457;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#31946;&#25968;&#23383;&#26469;&#20943;&#23569;&#24847;&#35265;&#20013;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;2-Tuple&#27169;&#31946;&#35821;&#35328;Delphi&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#20197;&#22788;&#29702;&#35780;&#22996;&#26174;&#31034;&#19981;&#21516;&#30340;&#19987;&#19994;&#31243;&#24230;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#26415;&#35821;&#30340;&#27169;&#31946;&#22810;&#31890;&#24230;&#35821;&#20041;&#26469;&#33719;&#24471;&#20013;&#38388;&#21644;&#26368;&#32456;&#32467;&#26524;&#65292;&#32467;&#26524;&#29992;2-Tuple&#35821;&#35328;&#20540;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#26696;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#35780;&#20272;&#20854;&#37096;&#20998;&#26469;&#39564;&#35777;&#23436;&#25972;&#30340;&#38382;&#21367;&#65292;&#23558;&#27599;&#20010;&#39033;&#30446;&#30340;&#26377;&#25928;&#24615;&#23450;&#20041;&#20026;&#19968;&#20010;&#20915;&#31574;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19987;&#23478;&#30340;&#24847;&#35265;&#65292;&#25105;&#20204;&#27979;&#37327;&#19968;&#33268;&#24615;&#31243;&#24230;&#65292;&#19968;&#33268;&#24615;&#31243;&#24230;&#21644;&#27599;&#20010;&#39033;&#30446;&#30340;&#35821;&#35328;&#24471;&#20998;&#65292;&#20197;&#20415;&#26816;&#27979;&#24433;&#21709;&#20202;&#22120;&#36136;&#37327;&#30340;&#39033;&#30446;&#65292;&#26080;&#35770;&#26159;&#31215;&#26497;&#30340;&#36824;&#26159;&#28040;&#26497;&#30340;&#12290;&#32771;&#34385;&#21040;&#35780;&#20215;b-&#23398;&#20064;&#30340;&#23454;&#38469;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classic Delphi and Fuzzy Delphi methods are used to test content validity of data collection tools such as questionnaires. Fuzzy Delphi takes the opinion issued by judges from a linguistic perspective reducing ambiguity in opinions by using fuzzy numbers. We propose an extension named 2-Tuple Fuzzy Linguistic Delphi method to deal with scenarios in which judges show different expertise degrees by using fuzzy multigranular semantics of the linguistic terms and to obtain intermediate and final results expressed by 2-tuple linguistic values. The key idea of our proposal is to validate the full questionnaire by means of the evaluation of its parts, defining the validity of each item as a Decision Making problem. Taking the opinion of experts, we measure the degree of consensus, the degree of consistency, and the linguistic score of each item, in order to detect those items that affect, positively or negatively, the quality of the instrument. Considering the real need to evaluate a b-learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#30740;&#31350;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30446;&#26631;&#31471;&#36716;&#31227;&#30340;&#21160;&#24577;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#30456;&#20284;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20855;&#26377;&#24378;&#22823;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#19988;&#38543;&#30528;&#30456;&#20284;&#30446;&#26631;&#35821;&#35328;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36716;&#31227;&#25928;&#26524;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36828;&#31163;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20063;&#21487;&#20197;&#24847;&#22806;&#22320;&#23545;&#20027;&#35201;&#35821;&#35328;&#23545;&#20135;&#29983;&#27491;&#21521;&#36716;&#31227;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01772</link><description>&lt;p&gt;
&#35299;&#24320;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30446;&#26631;&#31471;&#36716;&#31227;&#21644;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Roles of Target-Side Transfer and Regularization in Multilingual Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#30740;&#31350;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30446;&#26631;&#31471;&#36716;&#31227;&#30340;&#21160;&#24577;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#30456;&#20284;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20855;&#26377;&#24378;&#22823;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#19988;&#38543;&#30528;&#30456;&#20284;&#30446;&#26631;&#35821;&#35328;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36716;&#31227;&#25928;&#26524;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36828;&#31163;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20063;&#21487;&#20197;&#24847;&#22806;&#22320;&#23545;&#20027;&#35201;&#35821;&#35328;&#23545;&#20135;&#29983;&#27491;&#21521;&#36716;&#31227;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;(MMT)&#22312;&#19981;&#21516;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#19968;&#23545;&#22810;&#32763;&#35793;&#30456;&#27604;&#20110;&#22810;&#23545;&#19968;&#32763;&#35793;&#30340;&#25913;&#36827;&#20165;&#26377;&#24494;&#23567;&#29978;&#33267;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;&#36825;&#31181;&#24615;&#33021;&#24046;&#24322;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#19968;&#23545;&#22810;MT&#20013;&#65292;&#27491;&#21521;&#36716;&#31227;&#22312;&#30446;&#26631;&#31471;&#30340;&#20316;&#29992;&#31243;&#24230;&#22914;&#20309;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#36890;&#36807;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#35821;&#26009;&#24211;&#22823;&#23567;&#36825;&#20004;&#20010;&#32500;&#24230;&#21464;&#21270;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#65292;&#20197;&#23637;&#31034;&#30693;&#35782;&#36716;&#31227;&#23545;&#20027;&#35201;&#35821;&#35328;&#23545;&#30340;&#21160;&#24577;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#30456;&#20284;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#38543;&#30528;&#30456;&#20284;&#30446;&#26631;&#35821;&#35328;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27491;&#21521;&#36716;&#31227;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#20351;&#20027;&#35201;&#35821;&#35328;&#23545;&#21463;&#30410;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#36828;&#31163;&#30340;&#36741;&#21161;&#30446;&#26631;&#35821;&#35328;&#20063;&#21487;&#20197;&#20986;&#20046;&#24847;&#26009;&#22320;&#20351;&#20027;&#35201;&#35821;&#35328;&#23545;&#21463;&#30410;&#65292;&#21363;&#20351;&#27491;&#21521;&#36716;&#31227;&#26368;&#23567;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Machine Translation (MMT) benefits from knowledge transfer across different language pairs. However, improvements in one-to-many translation compared to many-to-one translation are only marginal and sometimes even negligible. This performance discrepancy raises the question of to what extent positive transfer plays a role on the target-side for one-to-many MT. In this paper, we conduct a large-scale study that varies the auxiliary target side languages along two dimensions, i.e., linguistic similarity and corpus size, to show the dynamic impact of knowledge transfer on the main language pairs. We show that linguistically similar auxiliary target languages exhibit strong ability to transfer positive knowledge. With an increasing size of similar target languages, the positive transfer is further enhanced to benefit the main language pairs. Meanwhile, we find distant auxiliary target languages can also unexpectedly benefit main language pairs, even with minimal positive trans
&lt;/p&gt;</description></item><item><title>BlackMamba&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;Mamba SSM&#21644;MoE&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#23427;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#21644;&#36739;&#20302;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.01771</link><description>&lt;p&gt;
BlackMamba: &#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BlackMamba: Mixture of Experts for State-Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01771
&lt;/p&gt;
&lt;p&gt;
BlackMamba&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;Mamba SSM&#21644;MoE&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#23427;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#21644;&#36739;&#20302;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;transformer&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#65292;&#20854;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;SSM&#27169;&#22411;Mamba&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22788;&#29702;&#38271;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#22312;&#26174;&#33879;&#38477;&#20302;&#25512;&#26029;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20063;&#22686;&#21152;&#20102;&#26356;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BlackMamba&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#23558;Mamba SSM&#19982;MoE&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#20004;&#32773;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#35777;&#26126;BlackMamba&#22312;Mamba&#21644;transformer&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#25512;&#26029;&#21644;&#35757;&#32451;FLOPs&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#30340;300B&#26631;&#35760;&#19978;&#20840;&#38754;&#35757;&#32451;&#24182;&#24320;&#28304;&#20102;340M/1.5B&#21644;630M/2.8B&#30340;BlackMamba&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BlackMamba&#32487;&#25215;&#24182;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;LLMs&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#65292;&#25552;&#20986;&#20102;&#24515;&#29702;&#23398;&#20998;&#31867;&#27861;&#65292;&#20197;&#26356;&#35814;&#32454;&#22320;&#29702;&#35299;&#21644;&#35299;&#20915;LLMs&#36755;&#20986;&#35823;&#23548;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#22788;&#29702;&#31867;&#20284;&#25361;&#25112;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#31574;&#30053;&#20197;&#20943;&#36731;LLMs&#20013;&#30340;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.01769</link><description>&lt;p&gt;
&#37325;&#26032;&#23450;&#20041;LLMs&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#65306;&#26500;&#24314;&#24515;&#29702;&#23398;&#20026;&#22522;&#30784;&#30340;&#20943;&#36731;&#35823;&#23548;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Redefining "Hallucination" in LLMs: Towards a psychology-informed framework for mitigating misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;LLMs&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#65292;&#25552;&#20986;&#20102;&#24515;&#29702;&#23398;&#20998;&#31867;&#27861;&#65292;&#20197;&#26356;&#35814;&#32454;&#22320;&#29702;&#35299;&#21644;&#35299;&#20915;LLMs&#36755;&#20986;&#35823;&#23548;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#22788;&#29702;&#31867;&#20284;&#25361;&#25112;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#31574;&#30053;&#20197;&#20943;&#36731;LLMs&#20013;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20363;&#22914;ChatGPT&#24050;&#32463;&#34987;&#36229;&#36807;&#21313;&#20159;&#30340;&#29992;&#25143;&#20351;&#29992;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#36923;&#36753;&#33021;&#21147;&#65292;&#20294;&#22312;&#8220;&#24187;&#35273;&#8221;&#26041;&#38754;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#25361;&#25112;&#12290;&#36825;&#19968;&#29616;&#35937;&#23548;&#33268;LLMs&#20197;&#33258;&#20449;&#30340;&#26041;&#24335;&#36755;&#20986;&#35823;&#23548;&#20449;&#24687;&#65292;&#32780;&#36825;&#21487;&#20197;&#22312;&#22914;&#27492;&#24222;&#22823;&#30340;&#29992;&#25143;&#32676;&#20307;&#20013;&#20135;&#29983;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;LLMs&#20013;&#20351;&#29992;&#8220;&#24187;&#35273;&#8221;&#19968;&#35789;&#30340;&#36866;&#24403;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#35748;&#30693;&#20559;&#24046;&#21644;&#20854;&#20182;&#24515;&#29702;&#29616;&#35937;&#30340;&#24515;&#29702;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#36825;&#19968;&#29616;&#35937;&#26356;&#35814;&#32454;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#33021;&#22815;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#20869;&#37096;&#35299;&#20915;&#31867;&#20284;&#25361;&#25112;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#31574;&#30053;&#26469;&#20943;&#36731;LLMs&#30340;&#24187;&#35273;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26088;&#22312;&#36229;&#36234;&#20256;&#32479;&#30340;&#26415;&#35821;&#65292;&#20026;&#28145;&#20837;&#29702;&#35299;&#21644;&#21487;&#25805;&#20316;&#30340;&#36335;&#24452;&#25552;&#20379;&#32454;&#33268;&#20837;&#24494;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have become incredibly popular, with ChatGPT for example being used by over a billion users. While these models exhibit remarkable language understanding and logical prowess, a notable challenge surfaces in the form of "hallucinations." This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base. However, we question the appropriateness of the term "hallucination" in LLMs, proposing a psychological taxonomy based on cognitive biases and other psychological phenomena. Our approach offers a more fine-grained understanding of this phenomenon, allowing for targeted solutions. By leveraging insights from how humans internally resolve similar challenges, we aim to develop strategies to mitigate LLM hallucinations. This interdisciplinary approach seeks to move beyond conventional terminology, providing a nuanced understanding and actionable pathways for imp
&lt;/p&gt;</description></item><item><title>HiQA&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01767</link><description>&lt;p&gt;
HiQA&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#30340;&#20998;&#23618;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;RAG&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01767
&lt;/p&gt;
&lt;p&gt;
HiQA&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#36805;&#36895;&#21457;&#23637;&#65292;&#20351;&#29992;&#34917;&#20805;&#25991;&#26723;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#23398;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31181;&#36827;&#27493;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#31572;&#36136;&#37327;&#65292;&#24182;&#20943;&#36731;&#20102;&#24187;&#35273;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#22823;&#37327;&#26080;&#27861;&#21306;&#20998;&#30340;&#25991;&#26723;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26377;&#38480;&#65292;&#32473;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#38024;&#23545;&#36825;&#20123;&#26032;&#20852;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiQA&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MDQA&#65289;&#26694;&#26550;&#65292;&#23558;&#32423;&#32852;&#30340;&#20803;&#25968;&#25454;&#25972;&#21512;&#21040;&#20869;&#23481;&#20013;&#65292;&#21516;&#26102;&#20855;&#22791;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;MasQA&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#21644;&#30740;&#31350;MDQA&#12290;&#26368;&#21518;&#65292;HiQA&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01766</link><description>&lt;p&gt;
LLM&#25237;&#31080;&#65306;&#20154;&#31867;&#36873;&#25321;&#21644;AI&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LLM Voting: Human Choices and AI Collective Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#19982;&#20154;&#31867;&#25237;&#31080;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36827;&#34892;&#20154;&#31867;&#25237;&#31080;&#23454;&#39564;&#20197;&#24314;&#31435;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;LLM&#20195;&#29702;&#36827;&#34892;&#24179;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38598;&#20307;&#32467;&#26524;&#21644;&#20010;&#20307;&#20559;&#22909;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;LLMs&#20043;&#38388;&#22312;&#20915;&#31574;&#21644;&#22266;&#26377;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#20559;&#22909;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30456;&#27604;&#20154;&#31867;&#36873;&#27665;&#30340;&#22810;&#26679;&#20559;&#22909;&#65292;LLMs&#26377;&#26356;&#36235;&#21521;&#20110;&#19968;&#33268;&#36873;&#25321;&#30340;&#20542;&#21521;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#27169;&#25311;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#27169;&#25311;&#30340;&#20154;&#26684;&#29305;&#36136;&#21450;&#31283;&#23450;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;LLMs&#22312;&#20010;&#24615;&#21270;&#20154;&#26426;&#20132;&#20114;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01765</link><description>&lt;p&gt;
LLMs &#27169;&#25311;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#65306;&#36827;&#19968;&#27493;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
LLMs Simulate Big Five Personality Traits: Further Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#27169;&#25311;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#27169;&#25311;&#30340;&#20154;&#26684;&#29305;&#36136;&#21450;&#31283;&#23450;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;LLMs&#22312;&#20010;&#24615;&#21270;&#20154;&#26426;&#20132;&#20114;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Llama2&#12289;GPT4&#21644;Mixtral&#23545;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#27169;&#25311;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#27169;&#25311;&#30340;&#20154;&#26684;&#29305;&#36136;&#21450;&#20854;&#31283;&#23450;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#26356;&#24191;&#27867;&#22320;&#20102;&#35299;LLMs&#27169;&#25311;&#20154;&#26684;&#29305;&#36136;&#30340;&#33021;&#21147;&#20197;&#21450;&#23545;&#20010;&#24615;&#21270;&#20154;&#26426;&#20132;&#20114;&#30340;&#30456;&#20851;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
An empirical investigation into the simulation of the Big Five personality traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is presented. We analyze the personality traits simulated by these models and their stability. This contributes to the broader understanding of the capabilities of LLMs to simulate personality traits and the respective implications for personalized human-computer interaction.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01763</link><description>&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#19978;&#21521;&#37327;&#25968;&#25454;&#24211;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Large Language Models Meet Vector Databases: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#22312;&#20154;&#31867;&#25991;&#23383;&#22788;&#29702;&#21644;&#29983;&#25104;&#26041;&#38754;&#24320;&#21551;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#21253;&#25324;&#24187;&#35273;&#12289;&#20559;&#35265;&#12289;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#20197;&#21450;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#32500;&#25252;&#30340;&#39640;&#25104;&#26412;&#31561;&#37325;&#35201;&#25361;&#25112;&#12290;&#32780;&#21478;&#19968;&#31181;&#26085;&#30410;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#21521;&#37327;&#25968;&#25454;&#24211;&#21017;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25797;&#38271;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#20110;&#39640;&#25928;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#20041;&#25628;&#32034;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#65292;&#23427;&#20204;&#26174;&#33879;&#22686;&#24378;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#32780;&#29420;&#29305;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01761</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking Interpretability in the Era of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01761
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#39046;&#22495;&#65292;&#21463;&#21040;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23835;&#36215;&#30340;&#25512;&#21160;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20026;&#37325;&#26032;&#24605;&#32771;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#26426;&#20250;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#33021;&#21147;&#20351;&#24471;LLMs&#33021;&#22815;&#25193;&#23637;&#32473;&#20154;&#31867;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19978;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#30340;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#27604;&#22914;&#34394;&#26500;&#30340;&#35299;&#37322;&#21644;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#35780;&#20272;&#26032;&#20852;LLM&#35299;&#37322;&#39046;&#22495;&#30340;&#29616;&#26377;&#26041;&#27861;&#65288;&#21253;&#25324;&#35299;&#37322;LLM&#21644;&#20351;&#29992;LLM&#36827;&#34892;&#35299;&#37322;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;LLMs&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#35299;&#37322;&#24615;&#65292;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#21253;&#25324;&#23545;LLMs&#26412;&#36523;&#30340;&#23457;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.   In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We high
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#23637;&#31034;&#20102;&#35745;&#31639;&#26041;&#27861;&#22312;&#24189;&#40664;&#39118;&#26684;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#31354;&#30333;&#21644;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01759</link><description>&lt;p&gt;
&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65306;&#29992;&#20110;&#24189;&#40664;&#39118;&#26684;&#20998;&#31867;&#30340;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Systematic Literature Review: Computational Approaches for Humour Style Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#23637;&#31034;&#20102;&#35745;&#31639;&#26041;&#27861;&#22312;&#24189;&#40664;&#39118;&#26684;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#31354;&#30333;&#21644;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21508;&#31181;&#24189;&#40664;&#39118;&#26684;&#23545;&#20110;&#29702;&#35299;&#24189;&#40664;&#30340;&#22810;&#38754;&#24615;&#21450;&#20854;&#22312;&#24515;&#29702;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#39046;&#22495;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#29702;&#35299;&#25581;&#31034;&#20102;&#20381;&#25454;&#25152;&#37319;&#29992;&#30340;&#39118;&#26684;&#65292;&#24189;&#40664;&#21487;&#20197;&#23545;&#20010;&#20154;&#30340;&#20581;&#24247;&#21644;&#20154;&#38469;&#20851;&#31995;&#20135;&#29983;&#27835;&#30103;&#25110;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#19987;&#38376;&#30740;&#31350;&#22522;&#20110;&#35745;&#31639;&#30340;&#24189;&#40664;&#39118;&#26684;&#20998;&#26512;&#30340;&#30740;&#31350;&#20173;&#28982;&#27604;&#36739;&#23569;&#35265;&#65292;&#20294;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#29305;&#21035;&#26159;&#20108;&#20803;&#24189;&#40664;&#21644;&#35773;&#21050;&#35782;&#21035;&#26041;&#38754;&#65292;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#24212;&#29992;&#20110;&#36825;&#20123;&#30456;&#20851;&#20219;&#21153;&#30340;&#35745;&#31639;&#25216;&#26415;&#30340;&#29616;&#29366;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#24189;&#40664;&#39118;&#26684;&#20998;&#26512;&#30340;&#22522;&#26412;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#26377;&#25928;&#22320;&#24341;&#23548;&#24189;&#40664;&#30740;&#31350;&#30340;&#22797;&#26434;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#21162;&#21147;&#30830;&#23450;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding various humour styles is essential for comprehending the multifaceted nature of humour and its impact on fields such as psychology and artificial intelligence. This understanding has revealed that humour, depending on the style employed, can either have therapeutic or detrimental effects on an individual's health and relationships. Although studies dedicated exclusively to computational-based humour style analysis remain somewhat rare, an expansive body of research thrives within related task, particularly binary humour and sarcasm recognition. In this systematic literature review (SLR), we survey the landscape of computational techniques applied to these related tasks and also uncover their fundamental relevance to humour style analysis. Through this study, we unveil common approaches, illuminate various datasets and evaluation metrics, and effectively navigate the complex terrain of humour research. Our efforts determine potential research gaps and outlined promising di
&lt;/p&gt;</description></item><item><title>Aalap&#26159;&#19968;&#20010;&#22312;&#21360;&#24230;&#27861;&#24459;&#20219;&#21153;&#19978;&#38024;&#23545;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;AI&#21161;&#25163;&#65292;&#30456;&#27604;&#20110;gpt-3.5-turbo&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20027;&#35201;&#25945;&#25480;&#27861;&#24459;&#25512;&#29702;&#65292;&#23545;&#24459;&#24072;&#12289;&#27861;&#23448;&#25110;&#22312;&#27861;&#24459;&#31995;&#32479;&#20013;&#24037;&#20316;&#30340;&#20154;&#26377;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.01758</link><description>&lt;p&gt;
Aalap&#65306;&#21360;&#24230;&#27861;&#24459;&#21644;&#27861;&#24459;&#21161;&#29702;&#21151;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Aalap: AI Assistant for Legal &amp; Paralegal Functions in India
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01758
&lt;/p&gt;
&lt;p&gt;
Aalap&#26159;&#19968;&#20010;&#22312;&#21360;&#24230;&#27861;&#24459;&#20219;&#21153;&#19978;&#38024;&#23545;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;AI&#21161;&#25163;&#65292;&#30456;&#27604;&#20110;gpt-3.5-turbo&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20027;&#35201;&#25945;&#25480;&#27861;&#24459;&#25512;&#29702;&#65292;&#23545;&#24459;&#24072;&#12289;&#27861;&#23448;&#25110;&#22312;&#27861;&#24459;&#31995;&#32479;&#20013;&#24037;&#20316;&#30340;&#20154;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#20219;&#21153;&#20013;&#20351;&#29992;&#19987;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12289;&#39046;&#22495;&#25968;&#25454;&#24322;&#26500;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#22797;&#26434;&#24615;&#21644;&#39046;&#22495;&#30446;&#26631;&#29420;&#29305;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;Aalap&#65292;&#23427;&#26159;&#22312;&#19982;&#21360;&#24230;&#29305;&#23450;&#27861;&#24459;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#20196;&#25968;&#25454;&#19978;&#32463;&#36807;&#24494;&#35843;&#30340;Mistral 7B&#27169;&#22411;&#12290; Aalap&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#25968;&#25454;&#20013;&#27604;gpt-3.5-turbo&#34920;&#29616;&#26356;&#22909;&#30340;&#27604;&#20363;&#20026;31&#65285;&#65292;&#22312;34&#65285;&#30340;&#27979;&#35797;&#25968;&#25454;&#20013;&#19982;GPT4&#35780;&#20272;&#24471;&#20998;&#30456;&#24403;&#12290; Aalap&#30340;&#35757;&#32451;&#20027;&#35201;&#20391;&#37325;&#20110;&#25945;&#25480;&#27861;&#24459;&#25512;&#29702;&#32780;&#19981;&#26159;&#27861;&#24459;&#35760;&#24518;&#12290; Aalap&#23545;&#24459;&#24072;&#12289;&#27861;&#23448;&#25110;&#22312;&#27861;&#24459;&#31995;&#32479;&#20013;&#24037;&#20316;&#30340;&#20154;&#30340;&#26085;&#24120;&#27963;&#21160;&#32943;&#23450;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using proprietary Large Language Models on legal tasks poses challenges due to data privacy issues, domain data heterogeneity, domain knowledge sophistication, and domain objectives uniqueness. We created Aalalp, a fine-tuned Mistral 7B model on instructions data related to specific Indian legal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31\% of our test data and obtains an equivalent score in 34\% of the test data as evaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoning rather than legal recall. Aalap is definitely helpful for the day-to-day activities of lawyers, judges, or anyone working in legal systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#38899;&#39057;&#26469;&#36776;&#21035;&#20702;&#20316;&#21644;&#20167;&#24680;&#35328;&#35770;&#22312;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#35780;&#20272;&#35270;&#39057;&#26159;&#21542;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#20197;&#21450;&#26816;&#27979;&#20854;&#20013;&#26159;&#21542;&#23384;&#22312;&#20167;&#24680;&#35328;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01752</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#38899;&#39057;&#35782;&#21035;&#36785;&#39554;&#35328;&#35770;&#21644;&#20551;&#28040;&#24687;&#65292;&#36776;&#21035;&#20702;&#20316;&#21644;&#20167;&#24680;&#35328;&#35770;&#22312;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;
&lt;/p&gt;
&lt;p&gt;
Identifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#38899;&#39057;&#26469;&#36776;&#21035;&#20702;&#20316;&#21644;&#20167;&#24680;&#35328;&#35770;&#22312;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#35780;&#20272;&#35270;&#39057;&#26159;&#21542;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#20197;&#21450;&#26816;&#27979;&#20854;&#20013;&#26159;&#21542;&#23384;&#22312;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
YouTube&#38754;&#20020;&#30528;&#20840;&#29699;&#33539;&#22260;&#20869;&#34394;&#20551;&#20449;&#24687;&#21644;&#20167;&#24680;&#35328;&#35770;&#30340;&#20256;&#25773;&#21361;&#26426;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;YouTube&#24050;&#23454;&#26045;&#20005;&#26684;&#35268;&#23450;&#65292;&#31105;&#27490;&#19978;&#20256;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#25110;&#23459;&#20256;&#20167;&#24680;&#35328;&#35770;&#30340;&#20869;&#23481;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#38477;&#20302;&#20882;&#29359;&#24615;&#33521;&#35821;&#20869;&#23481;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20711;&#20285;&#32599;&#35821;&#20869;&#23481;&#30340;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20943;&#23569;&#20711;&#20285;&#32599;&#35821;YouTube&#35270;&#39057;&#20013;&#26292;&#21147;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#24320;&#21457;&#19968;&#20010;&#35780;&#32423;&#31995;&#32479;&#65292;&#36890;&#36807;&#27604;&#36739;&#26631;&#39064;&#21644;&#25551;&#36848;&#19982;&#38899;&#39057;&#20869;&#23481;&#65292;&#35780;&#20272;&#35270;&#39057;&#26159;&#21542;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#65292;&#24182;&#26816;&#27979;&#20854;&#20013;&#26159;&#21542;&#23384;&#22312;&#20167;&#24680;&#35328;&#35770;&#12290;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;Pytube&#24211;&#36827;&#34892;&#38899;&#39057;&#25552;&#21462;&#65292;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#36716;&#24405;&#65292;&#20351;&#29992;distilroberta-base&#27169;&#22411;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
YouTube faces a global crisis with the dissemination of false information and hate speech. To counter these issues, YouTube has implemented strict rules against uploading content that includes false information or promotes hate speech. While numerous studies have been conducted to reduce offensive English-language content, there's a significant lack of research on Sinhala content. This study aims to address the aforementioned gap by proposing a solution to minimize the spread of violence and misinformation in Sinhala YouTube videos. The approach involves developing a rating system that assesses whether a video contains false information by comparing the title and description with the audio content and evaluating whether the video includes hate speech. The methodology encompasses several steps, including audio extraction using the Pytube library, audio transcription via the fine-tuned Whisper model, hate speech detection employing the distilroberta-base model and a text classification L
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;Bard&#22312;&#35782;&#21035;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#31215;&#26497;&#35782;&#21035;AD&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#65292;&#20294;&#21487;&#33021;&#20250;&#23558;CN&#38169;&#35823;&#22320;&#35782;&#21035;&#20026;AD&#12290;&#23545;&#20110;&#31215;&#26497;&#35782;&#21035;CN&#65292;GPT-4&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#30495;&#38452;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01751</link><description>&lt;p&gt;
ChatGPT&#19982;Bard&#22312;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#30340;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;Bard&#22312;&#35782;&#21035;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#31215;&#26497;&#35782;&#21035;AD&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#65292;&#20294;&#21487;&#33021;&#20250;&#23558;CN&#38169;&#35823;&#22320;&#35782;&#21035;&#20026;AD&#12290;&#23545;&#20110;&#31215;&#26497;&#35782;&#21035;CN&#65292;GPT-4&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#30495;&#38452;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#26377;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#20010;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;ChatGPT-3.5&#65292;ChatGPT-4&#21644;Bard&#65289;&#22312;&#20854;&#24403;&#21069;&#20844;&#24320;&#24418;&#24335;&#19979;&#65292;&#20351;&#29992;&#20174;&#33258;&#21457;&#35821;&#38899;&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#25991;&#26412;&#36755;&#20837;&#65292;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#65288;AD&#65289;&#21644;&#35748;&#30693;&#27491;&#24120;&#65288;CN&#65289;&#20010;&#20307;&#36827;&#34892;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;&#37319;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#29420;&#31435;&#26597;&#35810;&#32423;&#21035;&#19978;&#36827;&#34892;&#65292;&#31532;&#20108;&#20010;&#26597;&#35810;&#65288;&#24605;&#32500;&#38142;&#24341;&#23548;&#65289;&#27604;&#31532;&#19968;&#20010;&#26597;&#35810;&#20135;&#29983;&#26356;&#35814;&#32454;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#35780;&#20272;&#27599;&#20010;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20934;&#30830;&#24230;&#12289;&#25935;&#24863;&#24230;&#12289;&#29305;&#24322;&#24230;&#12289;&#31934;&#30830;&#24230;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#29983;&#25104;&#30340;&#39044;&#27979;&#26469;&#35780;&#20272;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#20102;&#19977;&#31867;&#32467;&#26524;&#65288;"AD"&#65292;"CN"&#25110;"Unsure"&#65289;&#12290;&#22312;&#31215;&#26497;&#35782;&#21035;AD&#26102;&#65292;Bard&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;&#30495;&#38451;&#24615;&#65288;89%&#30340;&#21484;&#22238;&#29575;&#65289;&#21644;&#26368;&#39640;&#30340;F1&#24471;&#20998;&#65288;71%&#65289;&#65292;&#20294;&#20542;&#21521;&#20110;&#23558;CN&#38169;&#35823;&#22320;&#35782;&#21035;&#20026;AD&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#32622;&#20449;&#24230;&#65288;&#36739;&#20302;&#30340;"Unsure"&#29575;&#65289;&#65307;&#22312;&#31215;&#26497;&#35782;&#21035;CN&#26102;&#65292;GPT-4&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;&#30495;&#38452;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) find increasing applications in many fields. Here, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in their current form, as publicly available - for their ability to recognize Alzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual input derived from spontaneous speech recordings. Zero-shot learning approach is used at two levels of independent queries, with the second query (chain-of-thought prompting) eliciting more detailed than the first. Each LLM chatbot's performance is evaluated on the prediction generated in terms of accuracy, sensitivity, specificity, precision and F1 score. LLM chatbots generated three-class outcome ("AD", "CN", or "Unsure"). When positively identifying AD, Bard produced highest true-positives (89% recall) and highest F1 score (71%), but tended to misidentify CN as AD, with high confidence (low "Unsure" rates); for positively identifying CN, GPT-4 resulted in the highest true-negatives 
&lt;/p&gt;</description></item><item><title>PACE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#26234;&#33021;&#20307;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#23454;&#29992;&#36890;&#20449;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#12289;&#24847;&#22270;&#35299;&#26512;&#21644;&#20197;&#24847;&#22270;&#20026;&#23548;&#21521;&#30340;&#32534;&#30721;&#26469;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#24211;&#34917;&#20805;&#25152;&#38656;&#30693;&#35782;&#65292;&#24341;&#20837;&#19987;&#29992;&#25552;&#31034;&#26469;&#20419;&#36827;&#23545;&#23454;&#29992;&#36890;&#20449;&#22330;&#26223;&#21644;&#20219;&#21153;&#35201;&#27714;&#30340;&#29702;&#35299;&#65292;&#24182;&#35774;&#35745;&#24605;&#32500;&#38142;&#20197;&#24110;&#21161;&#26435;&#34913;&#20256;&#36755;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.01750</link><description>&lt;p&gt;
PACE&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#30340;&#23454;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01750
&lt;/p&gt;
&lt;p&gt;
PACE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#26234;&#33021;&#20307;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#23454;&#29992;&#36890;&#20449;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#12289;&#24847;&#22270;&#35299;&#26512;&#21644;&#20197;&#24847;&#22270;&#20026;&#23548;&#21521;&#30340;&#32534;&#30721;&#26469;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#24211;&#34917;&#20805;&#25152;&#38656;&#30693;&#35782;&#65292;&#24341;&#20837;&#19987;&#29992;&#25552;&#31034;&#26469;&#20419;&#36827;&#23545;&#23454;&#29992;&#36890;&#20449;&#22330;&#26223;&#21644;&#20219;&#21153;&#35201;&#27714;&#30340;&#29702;&#35299;&#65292;&#24182;&#35774;&#35745;&#24605;&#32500;&#38142;&#20197;&#24110;&#21161;&#26435;&#34913;&#20256;&#36755;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#36890;&#20449;&#25216;&#26415;&#22312;&#29702;&#35770;&#23481;&#37327;&#12289;&#39057;&#35889;&#21487;&#29992;&#24615;&#21644;&#21151;&#32791;&#36164;&#28304;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#23454;&#29992;&#36890;&#20449;&#21033;&#29992;&#32456;&#31471;&#26234;&#33021;&#36827;&#34892;&#36873;&#25321;&#24615;&#25968;&#25454;&#20256;&#36755;&#65292;&#25552;&#20379;&#36164;&#28304;&#33410;&#32422;&#12290;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#36890;&#29992;&#24847;&#22270;&#35299;&#26512;&#24037;&#20855;&#65292;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#26234;&#33021;&#20307;&#65288;PACE&#65289;&#30340;&#22270;&#20687;&#23454;&#29992;&#36890;&#20449;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;PACE&#20381;&#27425;&#25191;&#34892;&#35821;&#20041;&#24863;&#30693;&#12289;&#24847;&#22270;&#35299;&#26512;&#21644;&#20197;&#24847;&#22270;&#20026;&#23548;&#21521;&#30340;&#32534;&#30721;&#12290;&#20026;&#20102;&#30830;&#20445;&#22312;&#36890;&#20449;&#20013;&#26377;&#25928;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#30693;&#35782;&#24211;&#26469;&#34917;&#20805;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#24341;&#20837;&#20102;&#19987;&#29992;&#25552;&#31034;&#26469;&#20419;&#36827;&#23545;&#23454;&#29992;&#36890;&#20449;&#22330;&#26223;&#21644;&#20219;&#21153;&#35201;&#27714;&#30340;&#29702;&#35299;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#26465;&#24605;&#32500;&#38142;&#20197;&#24110;&#21161;&#22312;&#20256;&#36755;&#25928;&#29575;&#21644;&#36136;&#37327;&#20043;&#38388;&#36827;&#34892;&#21512;&#29702;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current communication technologies face limitations in terms of theoretical capacity, spectrum availability, and power resources. Pragmatic communication, leveraging terminal intelligence for selective data transmission, offers resource conservation. Existing research lacks universal intention resolution tools, limiting applicability to specific tasks. This paper proposes an image pragmatic communication framework based on a Pragmatic Agent for Communication Efficiency (PACE) using Large Language Models (LLM). In this framework, PACE sequentially performs semantic perception, intention resolution, and intention-oriented coding. To ensure the effective utilization of LLM in communication, a knowledge base is designed to supplement the necessary knowledge, dedicated prompts are introduced to facilitate understanding of pragmatic communication scenarios and task requirements, and a chain of thought is designed to assist in making reasonable trade-offs between transmission efficiency and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.01748</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22522;&#30784;&#27169;&#22411;&#34987;&#23459;&#31216;&#20026;6G&#31995;&#32479;&#30340;&#25913;&#21464;&#32773;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26080;&#32447;&#32593;&#32476;&#30340;LLMs&#30340;&#21162;&#21147;&#20165;&#38480;&#20110;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#20197;&#26080;&#32447;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#37326;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;(AI)&#21407;&#29983;&#32593;&#32476;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#22522;&#20110;NLP&#30340;&#22522;&#30784;&#27169;&#22411;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#20419;&#36827;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#30340;&#35774;&#35745;&#65306;1) &#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#65292;2) &#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;3) &#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20197;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;LLM&#20351;&#29992;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#36755;&#20986;&#36136;&#37327;&#24182;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#22312;&#36136;&#37327;&#21644;&#24310;&#36831;&#26041;&#38754;&#20445;&#25345;&#25104;&#26412;&#22312;&#39044;&#31639;&#33539;&#22260;&#20869;&#25110;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.01742</link><description>&lt;p&gt;
&#20248;&#21270;LLM&#20351;&#29992;&#25104;&#26412;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Optimizing the Costs of LLM Usage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;LLM&#20351;&#29992;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#36755;&#20986;&#36136;&#37327;&#24182;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#22312;&#36136;&#37327;&#21644;&#24310;&#36831;&#26041;&#38754;&#20445;&#25345;&#25104;&#26412;&#22312;&#39044;&#31639;&#33539;&#22260;&#20869;&#25110;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;LLM&#22312;&#29616;&#20170;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25991;&#20214;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#38382;&#31572;&#21644;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;LLM&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#12289;&#25104;&#26412;&#12289;&#26631;&#35760;&#21270;&#21644;&#24310;&#36831;&#12290;&#23454;&#38469;&#19978;&#65292;&#20225;&#19994;&#24050;&#32463;&#22312;&#20026;&#21508;&#33258;&#30340;&#29992;&#20363;&#36816;&#33829;&#25110;&#20351;&#29992;LLM&#32780;&#25215;&#25285;&#24040;&#22823;&#30340;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20272;&#35745;LLM&#30340;&#36755;&#20986;&#36136;&#37327;&#65288;&#32780;&#26080;&#38656;&#23454;&#38469;&#35843;&#29992;LLM&#65289;&#65292;&#28982;&#21518;&#35299;&#20915;LLM&#36873;&#25321;&#30340;&#20248;&#21270;&#20363;&#31243;&#65292;&#20197;&#22312;&#36136;&#37327;&#21644;&#24310;&#36831;&#26041;&#38754;&#20445;&#25345;&#25104;&#26412;&#22312;&#39044;&#31639;&#33539;&#22260;&#20869;&#25110;&#26368;&#23567;&#21270;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;LLM&#22312;&#25688;&#35201;&#31561;&#25991;&#20214;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#36755;&#20986;&#36136;&#37327;&#65292;&#38543;&#21518;&#37319;&#29992;LP&#21462;&#25972;&#31639;&#27861;&#26469;&#20248;&#21270;LLM&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#36136;&#37327;&#21644;&#25104;&#26412;&#20043;&#38388;&#26435;&#34913;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases.   In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sente
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;12&#20010;&#20020;&#24202;&#19987;&#19994;&#20013;&#25552;&#20379;&#23433;&#20840;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23450;&#21046;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.01741</link><description>&lt;p&gt;
&#24320;&#21457;&#24182;&#27979;&#35797;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#33647;&#29289;&#23433;&#20840;&#30340;12&#31181;&#20020;&#24202;&#19987;&#19994;
&lt;/p&gt;
&lt;p&gt;
Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;12&#20010;&#20020;&#24202;&#19987;&#19994;&#20013;&#25552;&#20379;&#23433;&#20840;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23450;&#21046;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#65306;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;-&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#65292;&#29992;&#20110;&#23433;&#20840;&#29992;&#33647;&#22788;&#26041;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20379;&#19982;&#24739;&#32773;&#32972;&#26223;&#21644;&#26426;&#26500;&#25351;&#21335;&#30456;&#20851;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;CDSS&#30340;&#23616;&#38480;&#24615;&#12290;&#30446;&#26631;&#65306;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;LLM&#30340;CDSS&#22312;&#35782;&#21035;&#21508;&#31181;&#21307;&#23398;&#21644;&#22806;&#31185;&#30149;&#20363;&#20013;&#30340;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#20154;&#24037;&#19987;&#23478;&#23567;&#32452;&#36827;&#34892;&#27604;&#36739;&#12290;&#23427;&#36824;&#30740;&#31350;&#20102;&#20020;&#24202;&#21307;&#29983;&#22312;&#19981;&#21516;CDSS&#38598;&#25104;&#26041;&#24335;&#65288;&#21021;&#32423;&#33647;&#24072;&#12289;&#20165;&#22522;&#20110;LLM&#30340;CDSS&#21644;&#20108;&#32773;&#30340;&#32452;&#21512;&#65289;&#20013;&#30340;&#20559;&#22909;&#12290;&#35774;&#35745;&#12289;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#65306;&#21033;&#29992;&#24102;&#26377;GPT-4.0&#30340;RAG&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#28041;&#21450;12&#20010;&#19987;&#19994;&#20013;23&#20010;&#20020;&#24202;&#26696;&#20363;&#30340;61&#20010;&#22788;&#26041;&#38169;&#35823;&#22330;&#26223;&#12290;&#19987;&#23478;&#23567;&#32452;&#20351;&#29992;PCNE&#20998;&#31867;&#21644;NCC MERP&#25351;&#25968;&#35780;&#20272;&#36825;&#20123;&#26696;&#20363;&#12290;&#19977;&#21517;&#21021;&#32423;&#33647;&#24072;&#29420;&#31435;&#23457;&#26680;&#27599;&#20010;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#22788;&#29702;&#24314;&#35758;&#12290;&#26681;&#25454;&#26816;&#26597;&#30340;&#38169;&#35823;&#21644;&#24314;&#35758;&#32534;&#21046;&#20102;&#21453;&#39304;&#25253;&#21578;&#12290; &#28982;&#21518;&#65292;&#19977;&#21517;&#21307;&#29983;&#29420;&#31435;&#23457;&#26680;&#36825;&#20123;&#25253;&#21578;&#65292;&#24182;&#25552;&#20986;&#23545;&#19979;&#19968;&#27493;&#22788;&#29702;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe medication prescription. This model addresses the limitations of traditional rule-based CDSS by providing relevant prescribing error alerts tailored to patient context and institutional guidelines.   Objective: The study evaluates the efficacy of an LLM-based CDSS in identifying medication errors across various medical and surgical case vignettes, compared to a human expert panel. It also examines clinician preferences among different CDSS integration modalities: junior pharmacist, LLM-based CDSS alone, and a combination of both.   Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the study involved 61 prescribing error scenarios within 23 clinical vignettes across 12 specialties. An expert panel assessed these cases using the PCNE classification and NCC MERP index. Three junior pharmacists independently reviewed eac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01740</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#36127;&#33655;&#19979;&#30340;&#34917;&#20607;&#24615;&#20559;&#35265;&#65306;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36873;&#25321;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;gpt-3.5-turbo&#21644;claude-instant-1.2&#22312;&#35299;&#37322;&#21644;&#25191;&#34892;&#35821;&#20041;&#20219;&#21153;&#26041;&#38754;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#20250;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#20010;&#21463;&#21040;&#24433;&#21709;&#26368;&#22823;&#30340;&#26159;&#20174;&#21015;&#34920;&#20013;&#36827;&#34892;&#23545;&#35937;&#36873;&#25321;&#65292;&#36825;&#26159;&#25968;&#23383;&#23548;&#33322;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#22522;&#26412;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#26816;&#26597;&#36825;&#20123;&#20559;&#35265;&#65292;&#24182;&#37327;&#21270;&#20854;&#23545;&#20195;&#34920;&#24615;&#21015;&#34920;&#36873;&#25321;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#25511;&#21046;&#23454;&#39564;&#65292;&#25105;&#20204;&#25805;&#32437;&#20102;&#28201;&#24230;&#12289;&#21015;&#34920;&#38271;&#24230;&#12289;&#23545;&#35937;&#36523;&#20221;&#12289;&#23545;&#35937;&#31867;&#22411;&#12289;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#36825;&#20123;&#20559;&#35265;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23396;&#31435;&#21644;&#27979;&#37327;&#36825;&#20123;&#20559;&#35265;&#23545;&#36873;&#25321;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20559;&#35265;&#32467;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27169;&#22411;&#65292;&#32780;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#24433;&#21709;&#30340;&#31243;&#24230;&#12290;&#30001;&#20110;&#23384;&#22312;&#36739;&#24378;&#30340;&#21021;&#29616;&#25928;&#24212;&#65292;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#20250;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have become instrumental in interpreting and executing semantic-based tasks. Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance. Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we conducted a series of controlled experiments, manipulating temperature, list length, object identity, object type, prompt complexity, and model. This enabled us to isolate and measure the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproprotionately represented in ou
&lt;/p&gt;</description></item><item><title>OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01739</link><description>&lt;p&gt;
OpenMoE&#65306;&#24320;&#28304;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#26089;&#26399;&#21162;&#21147;
&lt;/p&gt;
&lt;p&gt;
OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01739
&lt;/p&gt;
&lt;p&gt;
OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24110;&#21161;&#24320;&#28304;&#31038;&#21306;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;(MoE)&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;OpenMoE&#65292;&#19968;&#31995;&#21015;&#23436;&#20840;&#24320;&#25918;&#28304;&#30721;&#21644;&#21487;&#22797;&#29616;&#30340;&#20165;&#35299;&#30721;&#22120;MoE LLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;650M&#21040;34B&#65292;&#35757;&#32451;&#25968;&#25454;&#36229;&#36807;1T&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#23454;&#65292;MoE-based LLM&#21487;&#20197;&#25552;&#20379;&#27604;&#23494;&#38598;LLM&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;LLM&#24320;&#21457;&#30340;&#28508;&#22312;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#23545;&#25105;&#20204;&#30340;OpenMoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#26426;&#21046;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#19978;&#19979;&#25991;&#26080;&#20851;&#19987;&#19994;&#21270;&#12289;&#26089;&#26399;&#36335;&#30001;&#23398;&#20064;&#21644;&#26411;&#23614;&#38477;&#20302;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#20915;&#31574;&#20027;&#35201;&#22522;&#20110;&#26631;&#35760;ID&#65292;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#24456;&#23567;&#12290;&#26631;&#35760;&#21040;&#19987;&#23478;&#30340;&#20998;&#37197;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26089;&#26399;&#30830;&#23450;&#65292;&#24182;&#19988;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#19981;&#23436;&#20840;&#30340;&#36335;&#30001;&#21487;&#33021;&#23548;&#33268;...
&lt;/p&gt;
&lt;p&gt;
To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
&lt;/p&gt;</description></item><item><title>C4Q&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#20934;&#30830;&#22238;&#31572;&#22522;&#26412;&#38382;&#39064;&#24182;&#22312;&#32534;&#20889;&#37327;&#23376;&#31243;&#24207;&#26102;&#24341;&#23548;&#29992;&#25143;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#24049;&#30340;&#24341;&#25806;&#32780;&#33021;&#22815;&#20445;&#35777;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01738</link><description>&lt;p&gt;
C4Q: &#19968;&#20010;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
C4Q: A Chatbot for Quantum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01738
&lt;/p&gt;
&lt;p&gt;
C4Q&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#20934;&#30830;&#22238;&#31572;&#22522;&#26412;&#38382;&#39064;&#24182;&#22312;&#32534;&#20889;&#37327;&#23376;&#31243;&#24207;&#26102;&#24341;&#23548;&#29992;&#25143;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#24049;&#30340;&#24341;&#25806;&#32780;&#33021;&#22815;&#20445;&#35777;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#25215;&#35834;&#30528;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#22914;&#37327;&#23376;&#23494;&#30721;&#23398;&#25110;&#37327;&#23376;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#33021;&#22815;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#20154;&#25968;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#36825;&#19968;&#38480;&#21046;&#26469;&#33258;&#20110;&#29702;&#35299;&#27010;&#24565;&#21644;&#22914;&#20309;&#24320;&#22987;&#32534;&#31243;&#30340;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#24037;&#20855;&#26469;&#24110;&#21161;&#38750;&#19987;&#23478;&#20811;&#26381;&#36825;&#31181;&#22797;&#26434;&#24615;&#12290;&#20351;&#29992;&#29616;&#26377;&#30340;&#23545;&#35805;&#31995;&#32479;&#26159;&#19968;&#20010;&#21487;&#33021;&#30340;&#36873;&#25321;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;ChatGPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#20102;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;C4Q&#65292;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#22238;&#31572;&#22522;&#26412;&#38382;&#39064;&#24182;&#22312;&#32534;&#20889;&#37327;&#23376;&#31243;&#24207;&#26102;&#24341;&#23548;&#29992;&#25143;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#21453;&#65292;C4Q&#21482;&#20351;&#29992;&#19968;&#20010;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21457;&#29616;&#21644;&#20998;&#31867;&#29992;&#25143;&#35831;&#27714;&#12290;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#33258;&#24049;&#30340;&#24341;&#25806;&#29983;&#25104;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#30001;&#20110;&#36825;&#31181;&#26550;&#26500;&#35774;&#35745;&#65292;C4Q&#30340;&#31572;&#26696;&#24635;&#26159;&#27491;&#30830;&#30340;&#65292;&#22240;&#27492;C4Q&#21487;&#20197;&#25104;&#20026;&#19968;&#20010;&#25903;&#25345;&#24037;&#20855;&#65292;&#20351;&#37327;&#23376;&#35745;&#31639;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing is a growing field that promises many real-world applications such as quantum cryptography or quantum finance. The number of people able to use quantum computing is however still very small. This limitation comes from the difficulty to understand the concepts and to know how to start coding. Therefore, there is a need for tools that can assist non-expert in overcoming this complexity. One possibility would be to use existing conversational agents. Unfortunately ChatGPT and other Large-Language Models produce inaccurate results. This article presents C4Q, a chatbot that answers accurately basic questions and guides users when trying to code quantum programs. Contrary to other approaches C4Q uses a pre-trained large language model only to discover and classify user requests. It then generates an accurate answer using an own engine. Thanks to this architectural design, C4Q's answers are always correct, and thus C4Q can become a support tool that makes quantum computing m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01737</link><description>&lt;p&gt;
&#20026;&#31038;&#20132;&#24863;&#30693;&#30340;&#35848;&#21028;&#23545;&#35805;&#24320;&#21457;&#36741;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#36890;&#36807;&#35753;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25198;&#28436;&#27599;&#27425;&#23545;&#35805;&#20013;&#30340;&#20004;&#21517;&#35848;&#21028;&#32773;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#35848;&#21028;&#12290;&#31532;&#19977;&#20010;LLM&#20805;&#24403;&#20462;&#27491;&#20195;&#29702;&#65292;&#37325;&#26032;&#32534;&#20889;&#36829;&#21453;&#35268;&#33539;&#30340;&#35805;&#35821;&#20197;&#25913;&#21892;&#35848;&#21028;&#32467;&#26524;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#19981;&#23384;&#22312;&#25163;&#21160;&#26500;&#24314;&#30340;&#25968;&#25454;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20135;&#21697;&#38144;&#21806;&#12289;&#25151;&#20215;&#21644;&#34218;&#36164;&#35848;&#21028;&#12290;&#28304;&#20195;&#30721;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#23558;&#22312;&#25509;&#21463;&#21518;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.
&lt;/p&gt;</description></item><item><title>SADAS&#26159;&#19968;&#20010;&#38754;&#21521;&#21452;&#35821;&#31038;&#20250;&#25991;&#21270;&#23545;&#35805;&#30340;&#23545;&#35805;&#21161;&#25163;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#35268;&#33539;&#31867;&#21035;&#12289;&#26816;&#27979;&#36829;&#20363;&#12289;&#35780;&#20272;&#20005;&#37325;&#31243;&#24230;&#12289;&#23454;&#26045;&#32416;&#27491;&#25514;&#26045;&#24182;&#38416;&#36848;&#29702;&#30001;&#31561;&#26032;&#39062;&#26550;&#26500;&#65292;&#30830;&#20445;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#33021;&#22815;&#20197;&#23562;&#37325;&#21644;&#29702;&#35299;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.01736</link><description>&lt;p&gt;
SADAS: &#19968;&#20010;&#38754;&#21521;&#21452;&#35821;&#31038;&#20250;&#25991;&#21270;&#23545;&#35805;&#20462;&#22797;&#35268;&#33539;&#36829;&#20363;&#30340;&#23545;&#35805;&#21161;&#25163;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SADAS: A Dialogue Assistant System Towards Remediating Norm Violations in Bilingual Socio-Cultural Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01736
&lt;/p&gt;
&lt;p&gt;
SADAS&#26159;&#19968;&#20010;&#38754;&#21521;&#21452;&#35821;&#31038;&#20250;&#25991;&#21270;&#23545;&#35805;&#30340;&#23545;&#35805;&#21161;&#25163;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#35268;&#33539;&#31867;&#21035;&#12289;&#26816;&#27979;&#36829;&#20363;&#12289;&#35780;&#20272;&#20005;&#37325;&#31243;&#24230;&#12289;&#23454;&#26045;&#32416;&#27491;&#25514;&#26045;&#24182;&#38416;&#36848;&#29702;&#30001;&#31561;&#26032;&#39062;&#26550;&#26500;&#65292;&#30830;&#20445;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#33021;&#22815;&#20197;&#23562;&#37325;&#21644;&#29702;&#35299;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#20840;&#29699;&#21270;&#30340;&#19990;&#30028;&#20013;&#65292;&#24357;&#21512;&#25991;&#21270;&#24046;&#24322;&#23545;&#20110;&#24314;&#31435;&#26377;&#24847;&#20041;&#30340;&#32852;&#31995;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#21152;&#37325;&#35201;&#12290;&#31038;&#20250;&#24863;&#30693;&#23545;&#35805;&#21161;&#25163;&#31995;&#32479;&#65288;SADAS&#65289;&#26159;&#25105;&#20204;&#23545;&#36825;&#19968;&#20840;&#29699;&#25361;&#25112;&#30340;&#22238;&#31572;&#65292;&#26088;&#22312;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#33021;&#22815;&#20197;&#23562;&#37325;&#21644;&#29702;&#35299;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#25105;&#20204;&#31995;&#32479;&#30340;&#26032;&#39062;&#26550;&#26500;&#21253;&#25324;&#65306;&#65288;1&#65289;&#35782;&#21035;&#23545;&#35805;&#20013;&#23384;&#22312;&#30340;&#35268;&#33539;&#31867;&#21035;&#65292;&#65288;2&#65289;&#26816;&#27979;&#28508;&#22312;&#30340;&#35268;&#33539;&#36829;&#20363;&#65292;&#65288;3&#65289;&#35780;&#20272;&#36825;&#20123;&#36829;&#20363;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#65288;4&#65289;&#23454;&#26045;&#26377;&#38024;&#23545;&#24615;&#30340;&#25514;&#26045;&#26469;&#32416;&#27491;&#36829;&#35268;&#34892;&#20026;&#65292;&#24182;&#65288;5&#65289;&#38416;&#36848;&#36825;&#20123;&#32416;&#27491;&#25514;&#26045;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26469;&#26500;&#24314;&#19981;&#21516;&#30340;&#27169;&#22359;&#65292;&#24182;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#36873;&#25321;&#27599;&#20010;&#27169;&#22359;&#26368;&#21512;&#36866;&#30340;&#39592;&#24178;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20154;&#31867;&#20559;&#22909;&#23454;&#39564;&#26469;&#39564;&#35777;&#31995;&#32479;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#24320;&#28304;&#25105;&#20204;&#30340;&#31995;&#32479;&#65288;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
In today's globalized world, bridging the cultural divide is more critical than ever for forging meaningful connections. The Socially-Aware Dialogue Assistant System (SADAS) is our answer to this global challenge, and it's designed to ensure that conversations between individuals from diverse cultural backgrounds unfold with respect and understanding. Our system's novel architecture includes: (1) identifying the categories of norms present in the dialogue, (2) detecting potential norm violations, (3) evaluating the severity of these violations, (4) implementing targeted remedies to rectify the breaches, and (5) articulates the rationale behind these corrective actions. We employ a series of State-Of-The-Art (SOTA) techniques to build different modules, and conduct numerous experiments to select the most suitable backbone model for each of the modules. We also design a human preference experiment to validate the overall performance of the system. We will open-source our system (includin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#23454;&#39564;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.01735</link><description>&lt;p&gt;
VIALM&#65306;&#20851;&#20110;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#30340;&#35843;&#26597;&#21644;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#23454;&#39564;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38556;&#30861;&#36741;&#21161; (VIA) &#26088;&#22312;&#33258;&#21160;&#24110;&#21161;&#35270;&#35273;&#38556;&#30861;&#32773; (VI) &#22788;&#29702;&#26085;&#24120;&#27963;&#21160;&#12290;VIA &#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#30340;&#21457;&#23637;&#65292;&#20108;&#32773;&#37117;&#23637;&#31034;&#20102;&#21033;&#29992;&#22823;&#22411;&#27169;&#22411; (LMs) &#30340;&#21069;&#27839;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;LMs &#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#21487;&#20197;&#24212;&#23545;&#35832;&#22914;&#20855;&#36523;&#26426;&#22120;&#20154;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29289;&#29702;&#20219;&#21153;&#12290;&#20026;&#20102;&#30740;&#31350;&#26368;&#20808;&#36827; (SOTA) LMs &#22312;VIA&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#38024;&#23545;&#20855;&#26377;LMs&#30340;VIA&#20219;&#21153;&#65288;VIALM&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#35828;&#26126;&#29289;&#29702;&#29615;&#22659;&#30340;&#22270;&#20687;&#21644;&#35270;&#35273;&#38556;&#30861;&#32773;&#29992;&#25143;&#30340;&#35821;&#35328;&#35831;&#27714;&#65292;VIALM&#26088;&#22312;&#36755;&#20986;&#36880;&#27493;&#24341;&#23548;&#65292;&#20197;&#22312;&#29615;&#22659;&#20013;&#24110;&#21161;&#35270;&#35273;&#38556;&#30861;&#29992;&#25143;&#23436;&#25104;&#35831;&#27714;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#23545;&#36817;&#26399;LM&#30740;&#31350;&#30340;&#35843;&#26597;&#21644;&#23545;&#36873;&#23450;LMs&#33021;&#21147;&#30340;&#22522;&#20934;&#23454;&#39564;&#30340;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually Impaired Assistance (VIA) aims to automatically help visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (\textbf{VIALM}). In this task, given an \textit{image} illustrating the physical environments and a \textit{linguistic request} from a VI user, VIALM aims to output step-by-step \textit{guidance} to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities
&lt;/p&gt;</description></item><item><title>CFTM&#26159;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#26469;&#35782;&#21035;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01734</link><description>&lt;p&gt;
CFTM: &#36830;&#32493;&#26102;&#38388;&#20998;&#25968;&#35805;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CFTM: Continuous time fractional topic model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01734
&lt;/p&gt;
&lt;p&gt;
CFTM&#26159;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#26469;&#35782;&#21035;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26102;&#38388;&#20998;&#25968;&#35805;&#39064;&#27169;&#22411;&#65288;cFTM&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;fBm&#65289;&#26377;&#25928;&#22320;&#35782;&#21035;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#38543;&#26102;&#38388;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;cFTM&#21487;&#20197;&#25429;&#25417;&#21040;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#20013;&#30340;&#36825;&#20123;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#65292;&#21453;&#26144;&#20102;fBm&#30340;&#20027;&#35201;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;cFTM&#30340;&#21442;&#25968;&#20272;&#35745;&#36807;&#31243;&#19982;&#20256;&#32479;&#20027;&#39064;&#27169;&#22411;LDA&#30340;&#30456;&#24403;&#12290;&#20026;&#20102;&#35777;&#26126;cFTM&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#27982;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36825;&#20123;&#27979;&#35797;&#30340;&#32467;&#26524;&#25903;&#25345;&#35813;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#38543;&#26102;&#38388;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Continuous Time Fractional Topic Model (cFTM), a new method for dynamic topic modeling. This approach incorporates fractional Brownian motion~(fBm) to effectively identify positive or negative correlations in topic and word distribution over time, revealing long-term dependency or roughness. Our theoretical analysis shows that the cFTM can capture these long-term dependency or roughness in both topic and word distributions, mirroring the main characteristics of fBm. Moreover, we prove that the parameter estimation process for the cFTM is on par with that of LDA, traditional topic models. To demonstrate the cFTM's property, we conduct empirical study using economic news articles. The results from these tests support the model's ability to identify and track long-term dependency or roughness in topics over time.
&lt;/p&gt;</description></item><item><title>RAG&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#21046;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#23450;&#21046;&#30340;LLM-RAG&#27969;&#31243;&#65292;&#37325;&#28857;&#20851;&#27880;&#26415;&#21069;&#21307;&#23398;&#12290;</title><link>https://arxiv.org/abs/2402.01733</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;--&#26696;&#20363;&#30740;&#31350;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01733
&lt;/p&gt;
&lt;p&gt;
RAG&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#21046;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#23450;&#21046;&#30340;LLM-RAG&#27969;&#31243;&#65292;&#37325;&#28857;&#20851;&#27880;&#26415;&#21069;&#21307;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#21046;LLMs&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#26696;&#20363;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#23450;&#21046;&#30340;LLM-RAG&#27969;&#31243;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#26415;&#21069;&#21307;&#23398;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#20102;35&#20010;&#26415;&#21069;&#25351;&#21335;&#24320;&#21457;&#20102;&#19968;&#20010;LLM-RAG&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19982;&#20154;&#24037;&#29983;&#25104;&#30340;&#22238;&#31572;&#36827;&#34892;&#27979;&#35797;&#65292;&#20849;&#35780;&#20272;&#20102;1260&#20010;&#22238;&#31572;&#12290;RAG&#27969;&#31243;&#28041;&#21450;&#20351;&#29992;&#22522;&#20110;Python&#30340;LangChain&#21644;Llamaindex&#26694;&#26550;&#23558;&#20020;&#24202;&#25991;&#26723;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#24182;&#23558;&#36825;&#20123;&#25991;&#26412;&#22788;&#29702;&#20026;&#22359;&#20197;&#29992;&#20110;&#23884;&#20837;&#21644;&#26816;&#32034;&#12290;&#21033;&#29992;Pinecone&#36827;&#34892;&#21521;&#37327;&#23384;&#20648;&#21644;&#20351;&#29992;1536&#32500;&#20313;&#24358;&#30456;&#20284;&#24230;&#25439;&#22833;&#24230;&#37327;&#26469;&#20248;&#21270;&#25968;&#25454;&#26816;&#32034;&#65292;&#20854;&#20013;&#36873;&#25321;&#20102;&#23884;&#20837;&#27169;&#22411;&#12290;&#23558;&#30001;&#21021;&#32423;&#21307;&#29983;&#25552;&#20379;&#30340;&#20154;&#24037;&#29983;&#25104;&#22238;&#31572;&#29992;&#20316;&#27604;&#36739;&#12290;&#32467;&#26524;&#65306;LLM-RA
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine.   Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison.   Results: The LLM-RA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#29983;&#25104;&#21307;&#23398;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20132;&#20114;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#26512;&#26469;&#35780;&#20272;&#20854;&#27491;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4-Vision-Preview&#20316;&#20026;LLM&#65292;&#24182;&#20351;&#29992;&#22810;&#27169;&#24577;&#22810;&#39033;&#36873;&#25321;&#39064;&#35780;&#20272;&#20854;&#22312;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01730</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#21644;&#30151;&#29366;&#20998;&#26512;&#30340;&#22810;&#27169;&#24577;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Evaluating LLM - Generated Multimodal Diagnosis from Medical Images and Symptom Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#29983;&#25104;&#21307;&#23398;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20132;&#20114;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#26512;&#26469;&#35780;&#20272;&#20854;&#27491;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4-Vision-Preview&#20316;&#20026;LLM&#65292;&#24182;&#20351;&#29992;&#22810;&#27169;&#24577;&#22810;&#39033;&#36873;&#25321;&#39064;&#35780;&#20272;&#20854;&#22312;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#25215;&#35834;&#24110;&#21161;&#21307;&#23398;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36820;&#22238;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#23578;&#26410;&#24471;&#21040;&#36866;&#24403;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;LLM&#35780;&#20272;&#33539;&#24335;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#29420;&#31435;&#27493;&#39588;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#65288;1&#65289;&#36890;&#36807;&#32467;&#26500;&#21270;&#20132;&#20114;&#36827;&#34892;&#22810;&#27169;&#24577;LLM&#35780;&#20272;&#21644;&#65288;2&#65289;&#22522;&#20110;&#20043;&#21069;&#20132;&#20114;&#25552;&#21462;&#30340;&#25968;&#25454;&#36827;&#34892;&#21518;&#32493;&#30340;&#39046;&#22495;&#29305;&#23450;&#20998;&#26512;&#12290;&#20351;&#29992;&#36825;&#31181;&#33539;&#24335;&#65292;&#65288;1&#65289;&#25105;&#20204;&#36890;&#36807;&#20844;&#24320;&#30340;&#22810;&#27169;&#24577;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQs&#65289;&#22312;&#30149;&#29702;&#23398;&#39046;&#22495;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#21307;&#23398;&#35786;&#26029;&#30340;&#27491;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#65288;2&#65289;&#28982;&#21518;&#23545;&#25552;&#21462;&#30340;&#32467;&#26524;&#36827;&#34892;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;GPT-4-Vision-Preview&#20316;&#20026;LLM&#65292;&#22238;&#31572;&#30001;&#22270;&#20687;&#21644;&#25991;&#26412;&#32452;&#25104;&#30340;&#22797;&#26434;&#21307;&#23398;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#30340;&#30142;&#30149;&#12289;&#30149;&#20917;&#12289;&#21270;&#23398;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) constitute a breakthrough state-of-the-art Artificial Intelligence technology which is rapidly evolving and promises to aid in medical diagnosis. However, the correctness and the accuracy of their returns has not yet been properly evaluated. In this work, we propose an LLM evaluation paradigm that incorporates two independent steps of a novel methodology, namely (1) multimodal LLM evaluation via structured interactions and (2) follow-up, domain-specific analysis based on data extracted via the previous interactions. Using this paradigm, (1) we evaluate the correctness and accuracy of LLM-generated medical diagnosis with publicly available multimodal multiple-choice questions(MCQs) in the domain of Pathology and (2) proceed to a systemic and comprehensive analysis of extracted results. We used GPT-4-Vision-Preview as the LLM to respond to complex, medical questions consisting of both images and text, and we explored a wide range of diseases, conditions, chem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01729</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Contextualization Distillation from Large Language Model for Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25991;&#26412;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#35821;&#26009;&#24211;&#20174;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#25110;&#21516;&#20041;&#35789;&#23450;&#20041;&#20013;&#25910;&#38598;&#30340;&#38745;&#24577;&#21644;&#22122;&#22768;&#24615;&#36136;&#24120;&#24120;&#38480;&#21046;&#20102;&#22522;&#20110;PLM&#30340;KGC&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#33976;&#39311;&#31574;&#30053;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21487;&#25554;&#20837;&#21644;&#21487;&#25773;&#25918;&#30340;&#26041;&#27861;&#65292;&#19982;&#21028;&#21035;&#21644;&#29983;&#25104;&#30340;KGC&#26694;&#26550;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#32039;&#20945;&#30340;&#32467;&#26500;&#21270;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#27573;&#33853;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#37325;&#24314;&#21644;&#19978;&#19979;&#25991;&#21270;&#65292;&#20351;&#36739;&#23567;&#30340;KGC&#27169;&#22411;&#33021;&#22815;&#21560;&#25910;&#36825;&#20123;&#20016;&#23500;&#30340;&#19977;&#20803;&#32452;&#20013;&#30340;&#35265;&#35299;&#12290;&#23545;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;KGC&#25216;&#26415;&#30340;&#20840;&#38754;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#26080;&#35770;&#22522;&#30784;&#31649;&#36947;&#22914;&#20309;&#65292;&#22987;&#32456;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
&lt;/p&gt;</description></item><item><title>&#30828;&#20214;Phi-1.5B&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21322;&#23548;&#20307;&#20135;&#19994;&#30828;&#20214;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#19987;&#19994;&#20998;&#23618;&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#30828;&#20214;&#39046;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01728</link><description>&lt;p&gt;
&#30828;&#20214;Phi-1.5B&#65306;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#30828;&#20214;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain Specific Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01728
&lt;/p&gt;
&lt;p&gt;
&#30828;&#20214;Phi-1.5B&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21322;&#23548;&#20307;&#20135;&#19994;&#30828;&#20214;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#19987;&#19994;&#20998;&#23618;&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#30828;&#20214;&#39046;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21322;&#23548;&#20307;&#20135;&#19994;&#20013;&#65292;&#30740;&#31350;&#12289;&#35774;&#35745;&#12289;&#39564;&#35777;&#21644;&#21046;&#36896;&#26159;&#32039;&#23494;&#30456;&#36830;&#30340;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38761;&#26032;&#30828;&#20214;&#35774;&#35745;&#21644;&#23433;&#20840;&#39564;&#35777;&#26041;&#38754;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#30828;&#20214;&#29305;&#23450;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#36890;&#24120;&#19981;&#33021;&#20805;&#20998;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#25110;&#36719;&#20214;&#20195;&#30721;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#19982;&#30828;&#20214;&#39046;&#22495;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20063;&#26159;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#30828;&#20214;Phi 1.5B&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21322;&#23548;&#20307;&#20135;&#19994;&#30828;&#20214;&#39046;&#22495;&#30340;&#21019;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#19994;&#20998;&#23618;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#23567;&#12289;&#20013;&#21644;&#22823;&#22411;&#23376;&#38598;&#65292;&#24182;&#23558;&#37325;&#28857;&#25918;&#22312;&#20351;&#29992;&#20013;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;Ph&#27169;&#22411;&#30340;&#32039;&#20945;&#20294;&#39640;&#25928;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving semiconductor industry, where research, design, verification, and manufacturing are intricately linked, the potential of Large Language Models to revolutionize hardware design and security verification is immense. The primary challenge, however, lies in the complexity of hardware specific issues that are not adequately addressed by the natural language or software code knowledge typically acquired during the pretraining stage. Additionally, the scarcity of datasets specific to the hardware domain poses a significant hurdle in developing a foundational model. Addressing these challenges, this paper introduces Hardware Phi 1.5B, an innovative large language model specifically tailored for the hardware domain of the semiconductor industry. We have developed a specialized, tiered dataset comprising small, medium, and large subsets and focused our efforts on pretraining using the medium dataset. This approach harnesses the compact yet efficient architecture of the Ph
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#21487;&#33021;&#23545;&#24863;&#30693;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01726</link><description>&lt;p&gt;
AI&#20013;&#20171;&#20132;&#27969;&#30340;&#25351;&#23548;&#65306;AI&#19981;&#25913;&#21464;&#23545;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#21487;&#33021;&#23545;&#24863;&#30693;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#20154;&#26469;&#35828;&#65292;&#28966;&#34385;&#12289;&#25233;&#37057;&#21644;&#20854;&#20182;&#31038;&#20132;&#21644;&#24515;&#29702;&#22240;&#32032;&#21487;&#33021;&#20351;&#25776;&#20889;&#25991;&#26412;&#28040;&#24687;&#25104;&#20026;&#19968;&#39033;&#31215;&#26497;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#26159;&#24110;&#21161;&#37027;&#20123;&#26412;&#26469;&#20250;&#35273;&#24471;&#21457;&#36865;&#30701;&#20449;&#22256;&#38590;&#25110;&#26377;&#21387;&#21147;&#30340;&#29992;&#25143;&#30340;&#23436;&#32654;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#24555;&#36895;&#26222;&#21450;&#65292;&#20294;&#23545;&#20854;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#30340;&#32771;&#34385;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#30340;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#65292;AI&#30340;&#20844;&#20247;&#24773;&#32490;&#36739;&#24046;&#21487;&#33021;&#23548;&#33268;&#20854;&#36741;&#21161;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#20351;&#29992;&#23545;&#24863;&#30693;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#20351;&#29992;&#36866;&#24471;&#20854;&#21453;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20154;&#20204;&#26159;&#21542;&#35748;&#20026;&#19968;&#26465;&#25991;&#26412;&#28040;&#24687;&#26159;&#21542;&#22312;&#25776;&#20889;&#36807;&#31243;&#20013;&#24471;&#21040;&#20102;AI&#30340;&#36741;&#21161;&#65292;&#20250;&#25913;&#21464;&#20854;&#24863;&#30693;&#30340;&#35821;&#35843;&#12289;&#28165;&#26224;&#24230;&#21644;&#34920;&#36798;&#24847;&#22270;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;26&#21517;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#39044;&#20808;&#25776;&#20889;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#12290;&#36890;&#36807;&#20998;&#26512;&#21442;&#19982;&#32773;&#23545;&#28040;&#24687;&#35821;&#35843;&#30340;&#35780;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone,
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36947;&#24503;&#21644;&#23433;&#20840;&#25361;&#25112;&#65292;&#21253;&#25324;&#36807;&#28388;&#25935;&#24863;&#35789;&#27719;&#12289;&#26816;&#27979;&#35282;&#33394;&#25198;&#28436;&#12289;&#23454;&#26045;&#33258;&#23450;&#20041;&#35268;&#21017;&#24341;&#25806;&#65292;&#20197;&#21450;&#24212;&#29992;&#21040;&#19981;&#21516;&#30340;&#27966;&#29983;&#27169;&#22411;&#20013;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#38477;&#20302;&#36947;&#24503;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.01725</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#21152;&#24378;&#36947;&#24503;&#30028;&#38480;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#24378;&#23433;&#20840;&#30340;&#39640;&#32423;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36947;&#24503;&#21644;&#23433;&#20840;&#25361;&#25112;&#65292;&#21253;&#25324;&#36807;&#28388;&#25935;&#24863;&#35789;&#27719;&#12289;&#26816;&#27979;&#35282;&#33394;&#25198;&#28436;&#12289;&#23454;&#26045;&#33258;&#23450;&#20041;&#35268;&#21017;&#24341;&#25806;&#65292;&#20197;&#21450;&#24212;&#29992;&#21040;&#19981;&#21516;&#30340;&#27966;&#29983;&#27169;&#22411;&#20013;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#38477;&#20302;&#36947;&#24503;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-3.5&#21644;LLaMA-2&#65292;&#30001;&#20110;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;Transformer&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;LLMs&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#22312;&#34987;&#36843;&#20570;&#20986;&#19981;&#24403;&#22238;&#24212;&#26102;&#20135;&#29983;&#30340;&#36947;&#24503;&#22256;&#22659;&#65292;&#26131;&#21463;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#21644;&#20405;&#29359;&#38544;&#31169;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20174;&#29992;&#25143;&#36755;&#20837;&#20013;&#36807;&#28388;&#25935;&#24863;&#35789;&#27719;&#65292;&#20197;&#38450;&#27490;&#20135;&#29983;&#19981;&#36947;&#24503;&#30340;&#22238;&#24212;&#65307;2&#65289;&#26816;&#27979;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#38459;&#27490;&#21487;&#33021;&#24341;&#21457;&#8220;&#36234;&#29425;&#8221;&#24773;&#22659;&#30340;&#20114;&#21160;&#65307;3&#65289;&#23454;&#26045;&#33258;&#23450;&#20041;&#35268;&#21017;&#24341;&#25806;&#65292;&#38480;&#21046;&#31105;&#27490;&#20869;&#23481;&#30340;&#29983;&#25104;&#65307;&#20197;&#21450;4&#65289;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;LLM&#27966;&#29983;&#27169;&#22411;&#65292;&#22914;Multi-Model Large Language Models (MLLMs)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#36824;&#21487;&#20197;&#38477;&#20302;&#36947;&#24503;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have significantly enhanced capabilities in natural language processing and artificial intelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionized text generation, translation, and question-answering tasks due to the transformative Transformer model. Despite their widespread use, LLMs present challenges such as ethical dilemmas when models are compelled to respond inappropriately, susceptibility to phishing attacks, and privacy violations. This paper addresses these challenges by introducing a multi-pronged approach that includes: 1) filtering sensitive vocabulary from user input to prevent unethical responses; 2) detecting role-playing to halt interactions that could lead to 'prison break' scenarios; 3) implementing custom rule engines to restrict the generation of prohibited content; and 4) extending these methodologies to various LLM derivatives like Multi-Model Large Language Models (MLLMs). Our approach not onl
&lt;/p&gt;</description></item><item><title>CERM&#26159;&#19968;&#20010;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#22522;&#20110;&#25991;&#29486;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21457;&#29616;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#29702;&#35299;&#39135;&#21697;&#19982;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#39135;&#26448;&#33829;&#20859;&#25104;&#20998;&#25110;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#35745;&#31639;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#39135;&#35889;&#25512;&#33616;&#21644;&#20998;&#26512;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#39135;&#26448;&#19982;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#30340;&#22266;&#26377;&#20851;&#31995;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#26356;&#22909;&#22320;&#25903;&#25345;&#39135;&#21697;&#30456;&#20851;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.01724</link><description>&lt;p&gt;
CERM: &#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#22522;&#20110;&#25991;&#29486;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
CERM: Context-aware Literature-based Discovery via Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01724
&lt;/p&gt;
&lt;p&gt;
CERM&#26159;&#19968;&#20010;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#22522;&#20110;&#25991;&#29486;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21457;&#29616;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#29702;&#35299;&#39135;&#21697;&#19982;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#39135;&#26448;&#33829;&#20859;&#25104;&#20998;&#25110;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#35745;&#31639;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#39135;&#35889;&#25512;&#33616;&#21644;&#20998;&#26512;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#39135;&#26448;&#19982;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#30340;&#22266;&#26377;&#20851;&#31995;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#26356;&#22909;&#22320;&#25903;&#25345;&#39135;&#21697;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29983;&#29289;&#21307;&#23398;&#20986;&#29256;&#29289;&#30340;&#20016;&#23500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#26469;&#29702;&#35299;&#39135;&#21697;&#19982;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20043;&#21069;&#23558;&#20581;&#24247;&#32435;&#20837;&#39135;&#35889;&#25512;&#33616;&#21644;&#20998;&#26512;&#31995;&#32479;&#30340;&#23581;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#39135;&#26448;&#33829;&#20859;&#25104;&#20998;&#19978;&#65292;&#25110;&#32773;&#21033;&#29992;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#22522;&#26412;&#35745;&#31639;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#25429;&#25417;&#39135;&#26448;&#21644;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#22266;&#26377;&#20851;&#31995;&#30340;&#22686;&#24378;&#27169;&#22411;&#23545;&#20110;&#39135;&#21697;&#30456;&#20851;&#30740;&#31350;&#26356;&#26377;&#30410;&#22788;&#65292;&#37492;&#20110;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#32771;&#34385;&#21040;&#26114;&#36149;&#30340;&#25968;&#25454;&#26631;&#35760;&#36807;&#31243;&#65292;&#36825;&#20123;&#27169;&#22411;&#24212;&#35813;&#26377;&#25928;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;&#23454;&#20307;&#20851;&#31995;&#24773;&#24863;&#20998;&#26512;&#65288;ERSA&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22522;&#20110;&#23454;&#20307;&#23545;&#25429;&#25417;&#25991;&#26412;&#30340;&#24773;&#24863;&#12290;ERSA&#25193;&#23637;&#20102;&#24191;&#27867;&#30740;&#31350;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30340;ERSA&#20219;&#21153;&#19978;&#65292;&#37325;&#28857;&#20851;&#27880;(entity-ent
&lt;/p&gt;
&lt;p&gt;
Driven by the abundance of biomedical publications, we introduce a sentiment analysis task to understand food-health relationship. Prior attempts to incorporate health into recipe recommendation and analysis systems have primarily focused on ingredient nutritional components or utilized basic computational models trained on curated labeled data. Enhanced models that capture the inherent relationship between food ingredients and biomedical concepts can be more beneficial for food-related research, given the wealth of information in biomedical texts. Considering the costly data labeling process, these models should effectively utilize both labeled and unlabeled data. This paper introduces Entity Relationship Sentiment Analysis (ERSA), a new task that captures the sentiment of a text based on an entity pair. ERSA extends the widely studied Aspect Based Sentiment Analysis (ABSA) task. Specifically, our study concentrates on the ERSA task applied to biomedical texts, focusing on (entity-ent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20013;&#22269;&#24037;&#19994;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01723</link><description>&lt;p&gt;
&#22312;&#20013;&#22269;&#24037;&#19994;&#22330;&#26223;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20013;&#22269;&#24037;&#19994;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#26381;&#21153;&#22823;&#37327;&#30340;&#20013;&#22269;&#29992;&#25143;&#65292;&#20013;&#22269;&#30340;&#35768;&#22810;&#21830;&#19994;&#20379;&#24212;&#21830;&#37319;&#21462;&#20102;&#26412;&#22320;&#21270;&#25112;&#30053;&#65292;&#35757;&#32451;&#24182;&#25552;&#20379;&#19987;&#38376;&#20026;&#20013;&#22269;&#29992;&#25143;&#23450;&#21046;&#30340;&#26412;&#22320;LLMs&#12290;&#27492;&#22806;&#65292;&#23637;&#26395;&#26410;&#26469;&#65292;LLMs&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#39046;&#22495;&#23558;&#26159;&#20225;&#19994;&#21644;&#29992;&#25143;&#22312;&#24037;&#19994;&#29983;&#20135;&#39046;&#22495;&#23454;&#38469;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#22312;&#20013;&#22269;&#24037;&#19994;&#29983;&#20135;&#39046;&#22495;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#25163;&#21160;&#25910;&#38598;&#20102;&#26469;&#33258;8&#20010;&#19981;&#21516;&#24037;&#19994;&#37096;&#38376;&#30340;1200&#20010;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#26469;&#35780;&#20272;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#21547;&#22235;&#20010;&#24037;&#19994;&#29305;&#23450;&#31283;&#23450;&#24615;&#31867;&#21035;&#21644;&#20843;&#20010;&#33021;&#21147;&#30340;&#21464;&#24577;&#27979;&#35797;&#26694;&#26550;&#65292;&#24635;&#35745;13,631&#20010;&#38382;&#39064;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the rapid development of large language models (LLMs) in various domains. To better serve the large number of Chinese users, many commercial vendors in China have adopted localization strategies, training and providing local LLMs specifically customized for Chinese users. Furthermore, looking ahead, one of the key future applications of LLMs will be practical deployment in industrial production by enterprises and users in those sectors. However, the accuracy and robustness of LLMs in industrial scenarios have not been well studied. In this paper, we present a comprehensive empirical study on the accuracy and robustness of LLMs in the context of the Chinese industrial production area. We manually collected 1,200 domain-specific problems from 8 different industrial sectors to evaluate LLM accuracy. Furthermore, we designed a metamorphic testing framework containing four industrial-specific stability categories with eight abilities, totaling 13,631 questions wi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21453;&#39304;&#21644;&#31034;&#20363;&#30340;&#31934;&#35843;&#36807;&#31243;&#65292;&#32467;&#21512;&#20313;&#24358;&#30456;&#20284;&#24230;&#12289;LLM&#35780;&#20272;&#21644;Rouge-L&#24471;&#20998;&#31561;&#25351;&#26631;&#65292;&#21487;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#38382;&#39064;&#21644;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#19982;&#38646;-shot LLMs&#30456;&#27604;&#65292;&#32463;&#36807;&#31934;&#35843;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#31572;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;RAG&#36807;&#31243;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01722</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#38382;&#39064;&#21644;&#25552;&#21462;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01722
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21453;&#39304;&#21644;&#31034;&#20363;&#30340;&#31934;&#35843;&#36807;&#31243;&#65292;&#32467;&#21512;&#20313;&#24358;&#30456;&#20284;&#24230;&#12289;LLM&#35780;&#20272;&#21644;Rouge-L&#24471;&#20998;&#31561;&#25351;&#26631;&#65292;&#21487;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#38382;&#39064;&#21644;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#19982;&#38646;-shot LLMs&#30456;&#27604;&#65292;&#32463;&#36807;&#31934;&#35843;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#31572;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;RAG&#36807;&#31243;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#24120;&#24120;&#21463;&#21040;&#31572;&#26696;&#36136;&#37327;&#30340;&#19981;&#20339;&#21644;&#20598;&#23572;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#31934;&#35843;&#36807;&#31243;&#65292;&#21033;&#29992;&#21453;&#39304;&#21644;&#31034;&#20363;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#20248;&#21270;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#25345;&#32493;&#30340;&#21453;&#39304;&#24490;&#29615;&#21644;&#21033;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#12289;LLM&#35780;&#20272;&#21644;Rouge-L&#24471;&#20998;&#31561;&#25351;&#26631;&#26469;&#25552;&#21319;AI&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#20687;GPT-3.5&#12289;GPT4ALL&#12289;LLaMA2&#21644;Claude&#36825;&#26679;&#30340;LLMs&#65292;&#24182;&#22312;&#21253;&#25324;FinanceBench&#21644;RAG Instruct Benchmark Tester Dataset&#22312;&#20869;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#31934;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#31934;&#35843;&#30340;&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#38646;-shot LLMs&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#21331;&#36234;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23558;LLM&#19982;&#19968;&#31181;&#21517;&#20026;RAG&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#23398;&#29983;&#35299;&#31572;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.01720</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Amharic Chatbot for FAQs in Universities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#23398;&#29983;&#35299;&#31572;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#23398;&#29983;&#24120;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21521;&#31649;&#29702;&#21592;&#25110;&#25945;&#24072;&#23547;&#27714;&#24120;&#35265;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#36825;&#23545;&#21452;&#26041;&#26469;&#35828;&#37117;&#24456;&#32321;&#29712;&#65292;&#38656;&#35201;&#25214;&#21040;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#38463;&#22982;&#21704;&#25289;&#35821;&#20013;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#65292;&#20316;&#20026;&#34394;&#25311;&#21161;&#25163;&#22788;&#29702;&#38382;&#39064;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31243;&#24207;&#20351;&#29992;&#26631;&#35760;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#21435;&#38500;&#20572;&#29992;&#35789;&#21644;&#35789;&#24178;&#25552;&#21462;&#23545;&#38463;&#22982;&#21704;&#25289;&#35821;&#36755;&#20837;&#21477;&#23376;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#26469;&#20998;&#31867;&#26631;&#35760;&#21644;&#26816;&#32034;&#21512;&#36866;&#30340;&#22238;&#31572;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#36890;&#36807;TensorFlow&#12289;Keras&#21644;NLTK&#23454;&#29616;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
University students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. This can become tedious for both parties, leading to a need for a solution. In response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (FAQs) in the Amharic language. Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. Three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: Support Vector Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented through TensorFlow, Keras, and NLTK. The deep learning model achieved the be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.01719</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#36947;&#24503;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Moral Inconsistencies in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#35821;&#20041;&#31561;&#20215;&#30340;&#25552;&#31034;&#20135;&#29983;&#35821;&#20041;&#31561;&#20215;&#30340;&#21709;&#24212;&#65292;&#37027;&#20040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#34987;&#35748;&#20026;&#26159;&#19968;&#33268;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;LLMs&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#29983;&#25104;&#26041;&#38754;&#20063;&#23384;&#22312;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#23545;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#20934;&#30830;&#24230;&#26469;&#34913;&#37327;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#27809;&#26377;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#30340;&#36947;&#24503;&#24773;&#26223;&#65288;&#20363;&#22914;&#65292;&#36947;&#36335;&#20132;&#36816;&#38382;&#39064;&#65289;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#26469;&#34913;&#37327;LLM&#22312;&#36947;&#24503;&#24773;&#26223;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;RoTs&#65289;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#19982;&#20154;&#31867;&#21028;&#26029;&#22312;&#20116;&#20010;LLMs&#19978;&#26356;&#22909;&#22320;&#30456;&#20851;&#12290;&#22312;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21517;&#20026;QA-RAG&#65292;&#29992;&#20110;&#35299;&#20915;&#21046;&#33647;&#34892;&#19994;&#20013;&#30340;&#21512;&#35268;&#24615;&#25361;&#25112;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.01717</link><description>&lt;p&gt;
&#20174;RAG&#21040;QA-RAG&#65306;&#23558;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#20110;&#33647;&#21697;&#30417;&#31649;&#21512;&#35268;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21517;&#20026;QA-RAG&#65292;&#29992;&#20110;&#35299;&#20915;&#21046;&#33647;&#34892;&#19994;&#20013;&#30340;&#21512;&#35268;&#24615;&#25361;&#25112;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#33647;&#34892;&#19994;&#30340;&#21512;&#35268;&#24615;&#35201;&#27714;&#38656;&#35201;&#38754;&#23545;&#22797;&#26434;&#19988;&#22823;&#37327;&#30340;&#25351;&#21335;&#25991;&#20214;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#12290;&#35813;&#32842;&#22825;&#26426;&#22120;&#20154;&#26088;&#22312;&#25628;&#32034;&#19982;&#29992;&#25143;&#26597;&#35810;&#30456;&#20851;&#30340;&#25351;&#21335;&#25991;&#20214;&#65292;&#24182;&#26681;&#25454;&#26816;&#32034;&#21040;&#30340;&#25351;&#21335;&#25552;&#20379;&#31572;&#26696;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#39046;&#22495;&#23545;&#39640;&#21487;&#38752;&#24615;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38382;&#39064;&#22238;&#31572;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;QA-RAG&#65289;&#27169;&#22411;&#12290;&#22312;&#23545;&#27604;&#23454;&#39564;&#20013;&#65292;QA-RAG&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#26174;&#31034;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#20248;&#20110;&#21253;&#25324;&#20256;&#32479;RAG&#26041;&#27861;&#22312;&#20869;&#30340;&#25152;&#26377;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;QA-RAG&#30340;&#32467;&#26500;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#33647;&#21697;&#30417;&#31649;&#21512;&#35268;&#39046;&#22495;&#21450;&#20854;&#20182;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24050;&#23558;&#25105;&#20204;&#30340;&#24037;&#20316;&#20844;&#24320;&#25552;&#20379;&#32473;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further researc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24067;&#40065;&#22982;-&#35748;&#30693;&#21644;&#24773;&#24863;&#20998;&#26512;&#65288;BE-Sent&#65289;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#25945;&#32946;&#35752;&#35770;&#35770;&#22363;&#20013;&#30340;&#24773;&#32490;&#21644;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#31867;&#12290;&#26041;&#27861;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#12289;&#25991;&#26412;&#39044;&#22788;&#29702;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35748;&#30693;&#20998;&#31867;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#20102;&#35299;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#36827;&#23637;&#24773;&#20917;&#21644;&#30693;&#35782;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.01716</link><description>&lt;p&gt;
Bloom-&#35748;&#30693;&#21644;&#24773;&#24863;&#20998;&#26512;&#23618;&#27425;&#20998;&#31867;&#22312;&#35838;&#31243;&#35752;&#35770;&#35770;&#22363;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bloom-epistemic and sentiment analysis hierarchical classification in course discussion forums
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24067;&#40065;&#22982;-&#35748;&#30693;&#21644;&#24773;&#24863;&#20998;&#26512;&#65288;BE-Sent&#65289;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#25945;&#32946;&#35752;&#35770;&#35770;&#22363;&#20013;&#30340;&#24773;&#32490;&#21644;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#31867;&#12290;&#26041;&#27861;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#12289;&#25991;&#26412;&#39044;&#22788;&#29702;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35748;&#30693;&#20998;&#31867;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#20102;&#35299;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#36827;&#23637;&#24773;&#20917;&#21644;&#30693;&#35782;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35752;&#35770;&#35770;&#22363;&#24191;&#27867;&#34987;&#29992;&#20110;&#35762;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#31215;&#26497;&#25991;&#26412;&#20132;&#27969;&#65292;&#20197;&#21450;&#26816;&#26597;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#27604;&#36739;&#36866;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#25945;&#32946;&#35752;&#35770;&#35770;&#22363;&#20013;&#22522;&#20110;&#25991;&#26412;&#35780;&#35770;&#30340;&#24773;&#32490;&#21644;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#24067;&#40065;&#22982;&#35748;&#30693;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#65288;BE-Sent&#65289;&#12290;&#30740;&#31350;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#26159;&#20174;&#20869;&#37096;&#35752;&#35770;&#35770;&#22363;&#21644;YouTube&#39057;&#36947;&#30340;&#35780;&#35770;&#20013;&#25910;&#38598;&#25968;&#25454;&#12290;&#19979;&#19968;&#27493;&#26159;&#23545;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#23545;&#25991;&#26412;&#36827;&#34892;&#26631;&#27880;&#24182;&#28165;&#38500;&#19981;&#37325;&#35201;&#30340;&#21333;&#35789;&#12290;&#27492;&#22806;&#65292;&#23545;&#24050;&#25104;&#21151;&#28165;&#29702;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#23558;&#22312;&#27599;&#20010;&#21477;&#23376;&#20013;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#21644;&#35748;&#30693;&#20998;&#31867;&#12290;&#24773;&#24863;&#20998;&#26512;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#31215;&#26497;&#30340;&#65292;&#28040;&#26497;&#30340;&#21644;&#20013;&#24615;&#30340;&#12290;&#24067;&#40065;&#22982;&#65288;Bloom&#65289;&#30340;&#35748;&#30693;&#20998;&#31867;&#26159;&#26681;&#25454;&#35748;&#30693;&#36807;&#31243;&#30340;&#20845;&#20010;&#23618;&#27425;&#36827;&#34892;&#30340;&#65292;&#20174;&#20302;&#23618;&#27425;&#30340;&#30693;&#35782;&#35760;&#24518;&#21040;&#39640;&#23618;&#27425;&#30340;&#35780;&#20215;&#21644;&#21019;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online discussion forums are widely used for active textual interaction between lecturers and students, and to see how the students have progressed in a learning process. The objective of this study is to compare appropriate machine-learning models to assess sentiments and Bloom\'s epistemic taxonomy based on textual comments in educational discussion forums. Our proposed method is called the hierarchical approach of Bloom-Epistemic and Sentiment Analysis (BE-Sent). The research methodology consists of three main steps. The first step is the data collection from the internal discussion forum and YouTube comments of a Web Programming channel. The next step is text preprocessing to annotate the text and clear unimportant words. Furthermore, with the text dataset that has been successfully cleaned, sentiment analysis and epistemic categorization will be done in each sentence of the text. Sentiment analysis is divided into three categories: positive, negative, and neutral. Bloom\'s epistem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;ChatGPT&#12289;Gemini&#21644;LLaMA2&#31561;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#22659;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#35780;&#20272;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#21644;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#33258;&#21160;&#24773;&#24863;&#20998;&#26512;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#25913;&#36827;&#31639;&#27861;&#21644;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01715</link><description>&lt;p&gt;
ChatGPT&#12289;Gemini&#21644;LLaMA2&#22312;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;ChatGPT&#12289;Gemini&#21644;LLaMA2&#31561;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#22659;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#35780;&#20272;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#21644;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#33258;&#21160;&#24773;&#24863;&#20998;&#26512;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#25913;&#36827;&#31639;&#27861;&#21644;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ChatGPT&#12289;Gemini&#25110;LLaMA2&#31561;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24773;&#24863;&#20998;&#26512;&#65292;&#22914;&#20170;&#22312;&#23398;&#26415;&#30740;&#31350;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#27169;&#26865;&#20004;&#21487;&#25110;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#30340;&#25991;&#26412;&#26102;&#65292;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#21644;&#39564;&#35777;&#20173;&#28982;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#32454;&#33268;&#21644;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#22659;&#65292;&#24182;&#23558;&#20854;&#32763;&#35793;&#25104;10&#31181;&#35821;&#35328;&#65292;&#28982;&#21518;&#20351;&#29992;&#27969;&#34892;&#30340;LLM&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#12290;&#32467;&#26524;&#32463;&#36807;&#20107;&#21518;&#39564;&#35777;&#30340;&#20154;&#31867;&#22238;&#24212;&#26469;&#39564;&#35777;&#12290;ChatGPT&#21644;Gemini&#36890;&#24120;&#23545;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#22659;&#22788;&#29702;&#24471;&#24456;&#22909;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21644;&#35780;&#20272;&#30340;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#21644;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#33258;&#21160;&#24773;&#24863;&#20998;&#26512;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#25913;&#36827;&#31639;&#27861;&#21450;&#20854;&#22522;&#30784;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated sentiment analysis using Large Language Model (LLM)-based models like ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic research and in industrial applications. However, assessment and validation of their performance in case of ambiguous or ironic text is still poor. In this study, we constructed nuanced and ambiguous scenarios, we translated them in 10 languages, and we predicted their associated sentiment using popular LLMs. The results are validated against post-hoc human responses. Ambiguous scenarios are often well-coped by ChatGPT and Gemini, but we recognise significant biases and inconsistent performance across models and evaluated human languages. This work provides a standardised methodology for automated sentiment analysis evaluation and makes a call for action to further improve the algorithms and their underlying data, to improve their performance, interpretability and applicability.
&lt;/p&gt;</description></item><item><title>TrICy&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#21033;&#29992;&#24847;&#22270;&#21644;&#35302;&#21457;&#22120;&#24341;&#23548;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#35789;&#27719;&#34920;&#20043;&#22806;&#30340;&#35789;&#27719;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01714</link><description>&lt;p&gt;
TrICy: &#36890;&#36807;&#24847;&#22270;&#24863;&#30693;&#30340;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#24341;&#23548;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01714
&lt;/p&gt;
&lt;p&gt;
TrICy&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#21033;&#29992;&#24847;&#22270;&#21644;&#35302;&#21457;&#22120;&#24341;&#23548;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#35789;&#27719;&#34920;&#20043;&#22806;&#30340;&#35789;&#27719;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#65288;D2T&#65289;&#29983;&#25104;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#38754;&#21521;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#22312;&#21487;&#20197;&#30452;&#25509;&#19982;&#29992;&#25143;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#30340;&#20250;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#26550;&#26500;&#30001;&#20110;&#39640;&#20869;&#23384;&#21344;&#29992;&#32780;&#26080;&#27861;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrICy&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;D2T&#20219;&#21153;&#65292;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#24847;&#22270;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#21487;&#36827;&#19968;&#27493;&#30001;&#29992;&#25143;&#25552;&#20379;&#30340;&#35302;&#21457;&#22120;&#36827;&#34892;&#24341;&#23548;&#12290;&#25105;&#20204;&#21033;&#29992;&#20851;&#27880;-&#22797;&#21046;&#26426;&#21046;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#35789;&#27719;&#34920;&#20043;&#22806;&#30340;&#35789;&#27719;&#65288;OOV&#65289;&#12290;&#23545;E2E NLG&#25968;&#25454;&#38598;&#65288;BLEU&#65306;66.43&#65285;&#65292;ROUGE-L&#65306;70.14&#65285;&#65289;&#65292;WebNLG&#25968;&#25454;&#38598;&#65288;BLEU&#65306;Seen 64.08&#65285;&#65292;Unseen 52.35&#65285;&#65289;&#21644;&#19982;&#25991;&#26412;&#28040;&#24687;&#24212;&#29992;&#30456;&#20851;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#20998;&#26512;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#21487;&#36873;&#30340;&#35302;&#21457;&#22120;&#36755;&#20837;&#65292;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Data-to-text (D2T) generation is a crucial task in many natural language understanding (NLU) applications and forms the foundation of task-oriented dialog systems. In the context of conversational AI solutions that can work directly with local data on the user's device, architectures utilizing large pre-trained language models (PLMs) are impractical for on-device deployment due to a high memory footprint. To this end, we propose TrICy, a novel lightweight framework for an enhanced D2T task that generates text sequences based on the intent in context and may further be guided by user-provided triggers. We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L: 70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom dataset related to text messaging applications, showcase our architecture's effectiveness. Moreover, we show that by leveraging an optional trigger input, data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01713</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#20351;&#20854;&#19982;&#20256;&#32479;&#19978;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25972;&#21512;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#21463;&#26032;&#30142;&#30149;&#29190;&#21457;&#26102;&#36805;&#36895;&#20915;&#31574;&#30340;&#32039;&#36843;&#38656;&#27714;&#30340;&#39537;&#20351;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;GPT-4&#30340;LLM&#23545;EHR&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#21363;&#22312;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#38024;&#23545;EHR&#25968;&#25454;&#30340;&#32437;&#21521;&#12289;&#31232;&#30095;&#21644;&#30693;&#35782;&#27880;&#20837;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#30340;&#25552;&#31034;&#26041;&#27861;&#32771;&#34385;&#20102;&#29305;&#23450;&#30340;EHR&#29305;&#24449;&#65292;&#22914;&#21333;&#20301;&#21644;&#21442;&#32771;&#33539;&#22260;&#65292;&#24182;&#37319;&#29992;&#20102;&#19982;&#20020;&#24202;&#19978;&#19979;&#25991;&#30456;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LLM&#33021;&#22815;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;EHR&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#24863;&#30693;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01712</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#24863;&#30693;&#24335;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#29992;&#20110;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#24863;&#30693;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#25913;&#36827;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#31995;&#32479;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#26432;&#30456;&#20851;&#25968;&#25454;&#21608;&#22260;&#30340;&#25935;&#24863;&#24615;&#23548;&#33268;&#38590;&#20197;&#35775;&#38382;&#21040;&#22823;&#35268;&#27169;&#30340;&#12289;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#35757;&#32451;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;ChatGPT&#65292;Flan-T5&#21644;Llama&#31561;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20026;&#33258;&#26432;&#24847;&#24565;&#26816;&#27979;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#22522;&#20110;&#20174;&#24515;&#29702;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#31038;&#20250;&#22240;&#32032;&#65292;&#24182;&#26088;&#22312;&#30830;&#20445;&#28085;&#30422;&#19982;&#33258;&#26432;&#24847;&#24565;&#30456;&#20851;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19982;&#22522;&#20110;BERT&#31995;&#21015;&#32467;&#26500;&#30340;&#29616;&#26377;NLP&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#24403;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;UMD&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#36825;&#20123;&#20256;&#32479;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#36890;&#24120;&#22312;0.75&#21040;0.87&#20043;&#38388;&#12290;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#36873;&#25321;&#65292;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, i
&lt;/p&gt;</description></item><item><title>&#35821;&#38899;&#29983;&#25104;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#65292;&#24635;&#32467;&#20986;&#29305;&#23450;&#20260;&#23475;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#12290;&#36825;&#20123;&#29305;&#23450;&#20260;&#23475;&#28041;&#21450;&#21040;&#21463;&#24433;&#21709;&#20010;&#20307;&#30340;&#26333;&#20809;&#31243;&#24230;&#20197;&#21450;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01708</link><description>&lt;p&gt;
&#19981;&#26159;&#25105;&#30340;&#22768;&#38899;&#65281;&#35821;&#38899;&#29983;&#25104;&#22120;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#20260;&#23475;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01708
&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#29983;&#25104;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#65292;&#24635;&#32467;&#20986;&#29305;&#23450;&#20260;&#23475;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#12290;&#36825;&#20123;&#29305;&#23450;&#20260;&#23475;&#28041;&#21450;&#21040;&#21463;&#24433;&#21709;&#20010;&#20307;&#30340;&#26333;&#20809;&#31243;&#24230;&#20197;&#21450;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24191;&#27867;&#37319;&#29992;&#35821;&#38899;&#29983;&#25104;&#25216;&#26415;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#37325;&#22823;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#20127;&#38656;&#35299;&#20915;&#12290;&#20363;&#22914;&#65292;&#22312;&#32654;&#22269;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#19982;&#35686;&#23519;&#21463;&#21040;&#24694;&#20316;&#21095;&#34989;&#20987;&#26377;&#20851;&#65292;&#21311;&#21517;&#34892;&#20026;&#32773;&#21046;&#36896;&#21512;&#25104;&#30340;&#22768;&#38899;&#25171;&#30005;&#35805;&#32473;&#35686;&#23519;&#65292;&#35201;&#27714;&#20851;&#38381;&#23398;&#26657;&#21644;&#21307;&#38498;&#65292;&#25110;&#32773;&#20197;&#26292;&#21147;&#25163;&#27573;&#36827;&#20837;&#26080;&#36764;&#24066;&#27665;&#30340;&#23478;&#20013;&#12290;&#36825;&#26679;&#30340;&#20107;&#20214;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#39118;&#38505;&#21644;&#20260;&#23475;&#24182;&#19981;&#23384;&#22312;&#20110;&#23396;&#31435;&#29366;&#24577;&#65292;&#32780;&#26159;&#28304;&#20110;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#65292;&#30740;&#31350;&#29305;&#23450;&#20260;&#23475;&#27169;&#24335;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#29305;&#23450;&#20260;&#23475;&#21487;&#20197;&#26681;&#25454;&#21463;&#24433;&#21709;&#20010;&#20307;&#30340;&#26333;&#20809;&#31243;&#24230;&#36827;&#34892;&#20998;&#31867;&#65292;&#21363;&#20182;&#20204;&#26159;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#30340;&#20027;&#20307;&#12289;&#19982;&#20043;&#20114;&#21160;&#12289;&#21463;&#20854;&#24433;&#21709;&#25110;&#34987;&#25490;&#38500;&#22312;&#22806;&#12290;&#21516;&#26679;&#65292;&#29305;&#23450;&#20260;&#23475;&#20063;&#19982;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#22810;&#31181;&#35821;&#22659;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#25551;&#36848;&#21487;&#33021;&#19990;&#30028;&#65292;&#24182;&#21033;&#29992;&#32534;&#35793;&#22120;&#65292;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#22659;&#19979;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#25104;&#26412;&#36739;&#20302;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#30740;&#31350;LLM&#23545;&#40784;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01706</link><description>&lt;p&gt;
MULTIVERSE: &#22312;&#19981;&#21516;&#19990;&#30028;&#20013;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#22810;&#31181;&#35821;&#22659;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#25551;&#36848;&#21487;&#33021;&#19990;&#30028;&#65292;&#24182;&#21033;&#29992;&#32534;&#35793;&#22120;&#65292;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#22659;&#19979;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#25104;&#26412;&#36739;&#20302;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#30740;&#31350;LLM&#23545;&#40784;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#40784;&#26088;&#22312;&#30830;&#20445;LLM&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#30456;&#21305;&#37197;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#19968;&#31995;&#21015;&#36234;&#29425;&#25216;&#26415;&#23637;&#31034;&#20102;&#23545;&#40784;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#23545;&#35805;&#20013;&#35825;&#20351;LLMs&#20135;&#29983;&#24694;&#24847;&#20869;&#23481;&#12290;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#26234;&#33021;&#25110;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#25214;&#21040;&#30456;&#24212;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;LLMs&#22312;&#19981;&#21516;&#35821;&#22659;&#19979;&#23545;&#40784;&#27700;&#24179;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#31995;&#32479;&#22320;&#26500;&#24314;&#35768;&#22810;&#34987;&#31216;&#20026;&#19990;&#30028;&#30340;&#35821;&#22659;&#12289;&#21033;&#29992;&#25551;&#36848;&#21487;&#33021;&#19990;&#30028;&#65288;&#22914;&#26102;&#38388;&#12289;&#22320;&#28857;&#12289;&#35282;&#33394;&#12289;&#34892;&#20026;&#21644;&#35821;&#35328;&#65289;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#21644;&#30456;&#24212;&#30340;&#32534;&#35793;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#36739;&#20302;&#25104;&#26412;&#25581;&#31034;&#28508;&#22312;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#37492;&#20110;&#25105;&#20204;&#26041;&#27861;&#30340;&#20302;&#25104;&#26412;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#19981;&#21516;&#19990;&#30028;&#20013;LLM&#23545;&#40784;&#38382;&#39064;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#36234;&#29425;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.01705</link><description>&lt;p&gt;
&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#34920;&#24449;&#20260;&#23475;&#65306;&#24230;&#37327;&#21644;&#20943;&#36731;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20260;&#23475;&#36890;&#24120;&#34987;&#20998;&#20026;&#37197;&#32622;&#24615;&#25110;&#34920;&#24449;&#24615;&#12290;&#26412;&#30740;&#31350;&#19987;&#38376;&#38024;&#23545;&#21518;&#32773;&#65292;&#37325;&#28857;&#22312;&#20110;&#23545;&#24403;&#21069;&#34920;&#24449;&#24615;&#20260;&#23475;&#23450;&#20041;&#30340;&#23457;&#26597;&#65292;&#20197;&#30830;&#23450;&#20854;&#20013;&#21253;&#21547;&#20160;&#20040;&#21644;&#19981;&#21253;&#21547;&#20160;&#20040;&#12290;&#36825;&#20010;&#20998;&#26512;&#20419;&#20351;&#25105;&#20204;&#25193;&#23637;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#21253;&#25324;&#23545;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#30340;&#20260;&#23475;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24230;&#37327;&#30340;&#39640;&#32423;&#35201;&#27714;&#65306;&#30830;&#23450;&#23454;&#26045;&#36825;&#31181;&#26041;&#27861;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#35828;&#26126;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#34920;&#24449;&#24615;&#20260;&#23475;&#26102;&#30340;&#29420;&#29305;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#20260;&#23475;&#26410;&#34987;&#24230;&#37327;&#21644;&#20943;&#36731;&#26102;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#20943;&#36731;&#25514;&#26045;&#24182;&#30028;&#23450;&#20309;&#26102;&#20351;&#29992;&#23427;&#20204;&#26469;&#32467;&#26463;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#26694;&#26550;&#65292;&#25193;&#22823;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#20844;&#24179;&#30740;&#31350;&#30340;&#35265;&#35299;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01704</link><description>&lt;p&gt;
&#20316;&#20026;&#31574;&#30053;&#30340;&#29366;&#24577;&#23383;&#31526;&#20018;&#65306;&#29992;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#26159;&#30740;&#31350;&#29702;&#24615;&#20027;&#20307;&#38388;&#25112;&#30053;&#20114;&#21160;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#35821;&#35328;&#26159;&#20154;&#31867;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#65292;&#20294;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#24456;&#38590;&#36890;&#36807;&#25968;&#23398;&#26041;&#27861;&#23545;&#23545;&#35805;&#21450;&#20854;&#25112;&#30053;&#21160;&#26426;&#24314;&#27169;&#12290;&#19982;&#35821;&#35328;&#20114;&#21160;&#30456;&#20851;&#30340;&#29609;&#23478;&#12289;&#31574;&#30053;&#21644;&#22238;&#25253;&#30340;&#36866;&#24403;&#27169;&#22411;&#65288;&#21363;&#23545;&#28216;&#25103;&#35770;&#24120;&#35268;&#31526;&#21495;&#36923;&#36753;&#30340;&#32422;&#26463;&#65289;&#23558;&#20351;&#29616;&#26377;&#30340;&#21338;&#24328;&#35770;&#31639;&#27861;&#33021;&#22815;&#22312;&#35821;&#35328;&#39046;&#22495;&#25552;&#20379;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#31181;&#32422;&#26463;&#21487;&#20197;&#20026;&#22312;&#23545;&#35805;&#20013;&#35745;&#31639;&#31283;&#23450;&#12289;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#25552;&#20379;&#19968;&#26465;&#36884;&#24452;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#24050;&#32463;&#36798;&#21040;&#20102;&#20854;&#29983;&#25104;&#33021;&#21147;&#36275;&#20197;&#23454;&#29616;&#33258;&#28982;&#23545;&#35805;&#30495;&#23454;&#12289;&#31867;&#20284;&#20154;&#31867;&#30340;&#27169;&#25311;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#25552;&#31034;&#23427;&#20204;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20854;&#21709;&#24212;&#24341;&#23548;&#21040;&#19981;&#21516;&#30340;&#36755;&#20986;&#35805;&#35821;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;LLM&#36824;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24555;&#36895;&#29983;&#25104;&#26032;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new di
&lt;/p&gt;</description></item><item><title>&#25193;&#23637;&#20102;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#36827;&#21270;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#30021;&#26790;&#22659;&#65292;&#33021;&#22815;&#21516;&#26102;&#26368;&#22823;&#21270;&#20869;&#37096;&#29305;&#24449;&#21644;&#25552;&#31034;&#27969;&#30021;&#24615;&#65292;&#21487;&#20197;&#33258;&#21160;&#25506;&#32034;&#27169;&#22411;&#23545;&#36731;&#24230;&#20998;&#24067;&#20043;&#22806;&#25552;&#31034;&#30340;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01702</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#30021;&#26790;&#22659;
&lt;/p&gt;
&lt;p&gt;
Fluent dreaming for language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01702
&lt;/p&gt;
&lt;p&gt;
&#25193;&#23637;&#20102;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#36827;&#21270;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#30021;&#26790;&#22659;&#65292;&#33021;&#22815;&#21516;&#26102;&#26368;&#22823;&#21270;&#20869;&#37096;&#29305;&#24449;&#21644;&#25552;&#31034;&#27969;&#30021;&#24615;&#65292;&#21487;&#20197;&#33258;&#21160;&#25506;&#32034;&#27169;&#22411;&#23545;&#36731;&#24230;&#20998;&#24067;&#20043;&#22806;&#25552;&#31034;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20063;&#31216;&#20026;&#8220;&#26790;&#22659;&#8221;&#65292;&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#20197;&#26368;&#22823;&#21270;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#25110;&#20854;&#20182;&#20869;&#37096;&#32452;&#20214;&#65292;&#20026;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36755;&#20837;&#31354;&#38388;&#26159;&#31163;&#25955;&#30340;&#65292;&#26790;&#22659;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#23578;&#26410;&#25104;&#21151;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#25991;&#29486;&#20013;&#30340;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#35774;&#35745;&#20102;&#36827;&#21270;&#25552;&#31034;&#20248;&#21270;&#65288;EPO&#65289;&#31639;&#27861;&#12290;EPO&#20248;&#21270;&#36755;&#20837;&#25552;&#31034;&#65292;&#20197;&#21516;&#26102;&#26368;&#22823;&#21270;&#25152;&#36873;&#20869;&#37096;&#29305;&#24449;&#21644;&#25552;&#31034;&#27969;&#30021;&#24615;&#20043;&#38388;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#30021;&#26790;&#22659;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#31070;&#32463;&#20803;&#12289;&#36755;&#20986;logits&#21644;&#28608;&#27963;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#26041;&#21521;&#36827;&#34892;&#26790;&#22659;&#12290;&#25105;&#20204;&#34913;&#37327;&#20102;&#29983;&#25104;&#30340;&#25552;&#31034;&#30340;&#27969;&#30021;&#24615;&#65292;&#24182;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#26790;&#22659;&#19982;&#26368;&#22823;&#28608;&#27963;&#25968;&#25454;&#38598;&#31034;&#20363;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20851;&#38190;&#26159;&#65292;&#27969;&#30021;&#26790;&#22659;&#20801;&#35768;&#33258;&#21160;&#25506;&#32034;&#27169;&#22411;&#20869;&#37096;&#23545;&#36731;&#24230;&#20998;&#24067;&#20043;&#22806;&#30340;&#25552;&#31034;&#30340;&#34892;&#20026;&#21453;&#24212;&#12290;&#29992;&#20110;&#36816;&#34892;&#30340;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Feature visualization, also known as "dreaming", offers insights into vision models by optimizing the inputs to maximize a neuron's activation or other internal component. However, dreaming has not been successfully applied to language models because the input space is discrete. We extend Greedy Coordinate Gradient, a method from the language model adversarial attack literature, to design the Evolutionary Prompt Optimization (EPO) algorithm. EPO optimizes the input prompt to simultaneously maximize the Pareto frontier between a chosen internal feature and prompt fluency, enabling fluent dreaming for language models. We demonstrate dreaming with neurons, output logits and arbitrary directions in activation space. We measure the fluency of the resulting prompts and compare language model dreaming with max-activating dataset examples. Critically, fluent dreaming allows automatically exploring the behavior of model internals in reaction to mildly out-of-distribution prompts. Code for runni
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#31995;&#32479;&#32508;&#36848;&#26088;&#22312;&#25551;&#36848;&#24403;&#21069;&#30340;&#21307;&#23398;&#38382;&#31572;&#31995;&#32479;&#65292;&#35780;&#20272;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#30830;&#23450;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01700</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#25252;&#29702;&#28857;&#30340;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#38382;&#31572;&#31995;&#32479;--&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Question answering systems for health professionals at the point of care -- a systematic review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01700
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#31995;&#32479;&#32508;&#36848;&#26088;&#22312;&#25551;&#36848;&#24403;&#21069;&#30340;&#21307;&#23398;&#38382;&#31572;&#31995;&#32479;&#65292;&#35780;&#20272;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#30830;&#23450;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#26377;&#28508;&#21147;&#36890;&#36807;&#20026;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#26368;&#26032;&#21644;&#26368;&#30456;&#20851;&#30340;&#35777;&#25454;&#26469;&#25552;&#39640;&#20020;&#24202;&#25252;&#29702;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;QA&#31995;&#32479;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#31995;&#32479;&#32508;&#36848;&#26088;&#22312;&#25551;&#36848;&#24403;&#21069;&#30340;&#21307;&#23398;QA&#31995;&#32479;&#65292;&#35780;&#20272;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#30830;&#23450;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;&#26448;&#26009;&#19982;&#26041;&#27861;&#65306;&#25105;&#20204;&#20110;2023&#24180;2&#26376;7&#26085;&#22312;PubMed&#12289;IEEE Xplore&#12289;ACM Digital Library&#12289;ACL Anthology&#20197;&#21450;&#21069;&#21518;&#24341;&#29992;&#20013;&#36827;&#34892;&#20102;&#25628;&#32034;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#25551;&#36848;&#29983;&#29289;&#21307;&#23398;QA&#31995;&#32479;&#35774;&#35745;&#21644;&#35780;&#20272;&#30340;&#21516;&#34892;&#35780;&#35758;&#26399;&#21002;&#21644;&#20250;&#35758;&#35770;&#25991;&#12290;&#20004;&#20010;&#35780;&#23457;&#20154;&#21592;&#31579;&#36873;&#20102;&#26631;&#39064;&#12289;&#25688;&#35201;&#21644;&#20840;&#25991;&#25991;&#31456;&#12290;&#25105;&#20204;&#23545;&#27599;&#20010;&#30740;&#31350;&#36827;&#34892;&#20102;&#21465;&#20107;&#32508;&#21512;&#21644;&#20559;&#20506;&#35780;&#20272;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#29289;&#21307;&#23398;QA&#31995;&#32479;&#30340;&#25928;&#29992;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21253;&#25324;&#20102;79&#20010;&#30740;&#31350;&#65292;&#24182;&#30830;&#23450;&#20102;&#20027;&#39064;&#65292;&#21253;&#25324;&#38382;&#39064;&#30340;&#30495;&#23454;&#24615;&#65292;&#31572;&#26696;&#30340;&#21487;&#38752;&#24615;&#65292;&#31572;&#26696;&#30340;&#25928;&#29992;&#65292;&#20020;&#24202;&#29305;&#27530;&#24615;&#65292;&#31995;&#32479;&#21644;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Question answering (QA) systems have the potential to improve the quality of clinical care by providing health professionals with the latest and most relevant evidence. However, QA systems have not been widely adopted. This systematic review aims to characterize current medical QA systems, assess their suitability for healthcare, and identify areas of improvement.   Materials and methods: We searched PubMed, IEEE Xplore, ACM Digital Library, ACL Anthology and forward and backward citations on 7th February 2023. We included peer-reviewed journal and conference papers describing the design and evaluation of biomedical QA systems. Two reviewers screened titles, abstracts, and full-text articles. We conducted a narrative synthesis and risk of bias assessment for each study. We assessed the utility of biomedical QA systems.   Results: We included 79 studies and identified themes, including question realism, answer reliability, answer utility, clinical specialism, systems, usabili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#30340;&#21442;&#19982;&#24335;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#12289;&#21327;&#20316;&#29983;&#25104;&#21644;&#21453;&#39304;&#36845;&#20195;&#65292;&#35299;&#20915;&#20102;&#31038;&#21306;&#32423;&#22303;&#22320;&#21033;&#29992;&#20219;&#21153;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;LLM&#22312;&#19981;&#21516;&#35268;&#21010;&#22330;&#26223;&#19978;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#36229;&#36807;&#20154;&#31867;&#19987;&#23478;&#28385;&#24847;&#24230;&#21644;&#21253;&#23481;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#26381;&#21153;&#21644;&#29983;&#24577;&#26041;&#38754;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.01698</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#21442;&#19982;&#24335;&#22478;&#24066;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Large language model empowered participatory urban planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#30340;&#21442;&#19982;&#24335;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#12289;&#21327;&#20316;&#29983;&#25104;&#21644;&#21453;&#39304;&#36845;&#20195;&#65292;&#35299;&#20915;&#20102;&#31038;&#21306;&#32423;&#22303;&#22320;&#21033;&#29992;&#20219;&#21153;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;LLM&#22312;&#19981;&#21516;&#35268;&#21010;&#22330;&#26223;&#19978;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#36229;&#36807;&#20154;&#31867;&#19987;&#23478;&#28385;&#24847;&#24230;&#21644;&#21253;&#23481;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#26381;&#21153;&#21644;&#29983;&#24577;&#26041;&#38754;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#19982;&#24335;&#22478;&#24066;&#35268;&#21010;&#26159;&#29616;&#20195;&#22478;&#24066;&#35268;&#21010;&#30340;&#20027;&#27969;&#65292;&#28041;&#21450;&#21508;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#31215;&#26497;&#21442;&#19982;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#21442;&#19982;&#24335;&#33539;&#24335;&#22312;&#26102;&#38388;&#21644;&#20154;&#21147;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#29983;&#25104;&#24335;&#35268;&#21010;&#24037;&#20855;&#22312;&#25552;&#20379;&#21487;&#35843;&#25972;&#21644;&#21253;&#23481;&#24615;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#22833;&#36133;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22478;&#24066;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#21442;&#19982;&#36807;&#31243;&#20013;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;LLM&#20195;&#29702;&#65292;&#21253;&#25324;&#35282;&#33394;&#25198;&#28436;&#12289;&#21327;&#20316;&#29983;&#25104;&#21644;&#21453;&#39304;&#36845;&#20195;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#26381;&#21153;&#20110;1000&#20010;&#19981;&#21516;&#21033;&#30410;&#30340;&#31038;&#21306;&#32423;&#22303;&#22320;&#21033;&#29992;&#20219;&#21153;&#12290;&#22312;&#21508;&#31181;&#22478;&#24066;&#31038;&#21306;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;&#19981;&#21516;&#35268;&#21010;&#22330;&#26223;&#19978;&#30340;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26681;&#25454;&#22235;&#20010;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#22312;&#28385;&#24847;&#24230;&#21644;&#21253;&#23481;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#19987;&#23478;&#65292;&#24182;&#22312;&#26381;&#21153;&#21644;&#29983;&#24577;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage
&lt;/p&gt;</description></item><item><title>APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.</title><link>https://arxiv.org/abs/2402.01697</link><description>&lt;p&gt;
APT-Pipe: &#29992;&#20110;&#31038;&#20132;&#35745;&#31639;&#25968;&#25454;&#26631;&#27880;&#30340;&#33258;&#21160;&#25552;&#31034;&#35843;&#25972;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01697
&lt;/p&gt;
&lt;p&gt;
APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#24212;&#29992;&#22312;&#31038;&#20132;&#35745;&#31639;&#25991;&#26412;&#26631;&#27880;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#20154;&#20204;&#24050;&#32463;&#30693;&#36947;&#24615;&#33021;&#21462;&#20915;&#20110;&#36755;&#20837;&#25552;&#31034;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#26377;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#26469;&#25506;&#32034;&#25552;&#31034;&#35843;&#25972;&#30340;&#25216;&#26415;&#21644;&#25351;&#21335;&#65292;&#35797;&#22270;&#25913;&#21892;&#25552;&#31034;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#25163;&#24037;&#21162;&#21147;&#21644;&#23545;&#27491;&#22312;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#25552;&#31034;&#35843;&#25972;&#27969;&#27700;&#32447;APT-Pipe&#12290;APT-Pipe&#26088;&#22312;&#33258;&#21160;&#35843;&#25972;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;ChatGPT&#22312;&#20219;&#20309;&#32473;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;APT-Pipe&#65292;&#24182;&#22312;12&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;APT-Pipe&#35843;&#25972;&#30340;&#25552;&#31034;&#26377;&#21161;&#20110;ChatGPT&#22312;12&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#20013;&#26377;9&#20010;&#33719;&#24471;&#26356;&#39640;&#30340;&#21152;&#26435;F1&#20998;&#25968;&#65292;&#24179;&#22343;&#25913;&#36827;&#20102;7.01&#65285;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#31361;&#20986;&#20102;APT-Pipe&#20316;&#20026;&#19968;&#20010;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by 
&lt;/p&gt;</description></item><item><title>HiGen&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#65292;&#22312;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#20013;&#32771;&#34385;&#20102;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;ENZYME&#12290;</title><link>https://arxiv.org/abs/2402.01696</link><description>&lt;p&gt;
HiGen: &#23618;&#27425;&#24863;&#30693;&#30340;&#23618;&#32423;&#25991;&#26412;&#20998;&#31867;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01696
&lt;/p&gt;
&lt;p&gt;
HiGen&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#65292;&#22312;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#20013;&#32771;&#34385;&#20102;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;ENZYME&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#65288;HTC&#65289;&#26159;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19968;&#20010;&#22797;&#26434;&#23376;&#20219;&#21153;&#65292;&#20854;&#29305;&#28857;&#26159;&#20855;&#26377;&#23618;&#32423;&#26631;&#31614;&#20998;&#31867;&#27861;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#25991;&#26723;&#21644;&#23618;&#32423;&#26631;&#31614;&#20449;&#24687;&#26469;&#23398;&#20064;&#38745;&#24577;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#21487;&#33021;&#22240;&#23618;&#32423;&#27700;&#24179;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#65292;&#38656;&#35201;&#21160;&#24577;&#30340;&#25991;&#26723;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiGen&#65292;&#19968;&#20010;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#30340;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25429;&#25417;&#25991;&#26412;&#21644;&#26631;&#31614;&#21517;&#31216;&#35821;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#21040;&#39046;&#22495;&#30693;&#35782;&#19978;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26679;&#26412;&#26377;&#38480;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21629;&#21517;&#20026;ENZYME&#30340;&#26032;&#39062;&#21644;&#26377;&#20215;&#20540;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#26469;&#33258;PubMed&#30340;&#25991;&#31456;&#32452;&#25104;&#65292;&#26088;&#22312;&#39044;&#27979;...
&lt;/p&gt;
&lt;p&gt;
Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicti
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.01695</link><description>&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Guided World Models: A Model-Based Approach to AI Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01695
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#23433;&#35013;&#21040;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#65292;&#20026;&#20154;&#31867;&#19982;&#36825;&#20123;&#20195;&#29702;&#27807;&#36890;&#21644;&#25511;&#21046;&#25171;&#24320;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#28192;&#36947;&#12290;&#38500;&#20102;&#26356;&#26032;&#20195;&#29702;&#31574;&#30053;&#65292;&#20154;&#31867;&#36824;&#21487;&#20197;&#20462;&#25913;&#20182;&#20204;&#30340;&#20869;&#37096;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#24433;&#21709;&#20195;&#29702;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#38590;&#20197;&#36866;&#24212;&#20154;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#33258;&#28982;&#30340;&#36890;&#20449;&#30028;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;&#23427;&#20204;&#36824;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#26368;&#21021;&#29992;&#20110;&#25351;&#23548;&#20154;&#31867;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#33258;&#25105;&#23398;&#20064;&#12290;&#20026;&#20102;&#20419;&#36827;LWMs&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;MESSENGER&#28216;&#25103;&#65288;Hanjie&#31561;&#20154;&#65292;2021&#65289;&#30340;&#25361;&#25112;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#26032;&#22330;&#26223;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
&lt;/p&gt;</description></item><item><title>ARGS&#26159;&#19968;&#20010;&#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#23558;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#35843;&#25972;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#35821;&#20041;&#22810;&#26679;&#24615;&#19988;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#32500;&#24230;&#19979;&#65292;ARGS&#20855;&#26377;&#25345;&#32493;&#30340;&#22870;&#21169;&#22686;&#30410;&#65292;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01694</link><description>&lt;p&gt;
ARGS: &#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
ARGS: Alignment as Reward-Guided Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01694
&lt;/p&gt;
&lt;p&gt;
ARGS&#26159;&#19968;&#20010;&#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#23558;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#35843;&#25972;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#35821;&#20041;&#22810;&#26679;&#24615;&#19988;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#32500;&#24230;&#19979;&#65292;ARGS&#20855;&#26377;&#25345;&#32493;&#30340;&#22870;&#21169;&#22686;&#30410;&#65292;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30446;&#26631;&#23545;&#40784;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#28982;&#32780;&#24120;&#35265;&#30340;&#26041;&#27861;&#21253;&#25324;RLHF&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#19981;&#31283;&#23450;&#21644;&#36164;&#28304;&#23494;&#38598;&#30340;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;ARGS&#65292;&#21363;&#23545;&#40784;&#20316;&#20026;&#22870;&#21169;&#23548;&#21521;&#30340;&#25628;&#32034;&#65292;&#23427;&#23558;&#23545;&#40784;&#34701;&#20837;&#21040;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;RL&#35757;&#32451;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#20351;&#29992;&#22870;&#21169;&#20449;&#21495;&#35843;&#25972;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;ARGS&#29983;&#25104;&#20855;&#26377;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#20026;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#19988;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#30340;&#27169;&#22411;&#32500;&#24230;&#19979;&#65292;ARGS&#30456;&#23545;&#20110;&#22522;&#32447;&#26174;&#31034;&#20986;&#25345;&#32493;&#30340;&#22870;&#21169;&#25913;&#36827;&#12290;&#20363;&#22914;&#65292;&#37319;&#29992;&#30456;&#21516;&#30340;&#36138;&#23146;&#35299;&#30721;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#25552;&#39640;&#20102;19.56%&#30340;&#24179;&#22343;&#22870;&#21169;&#65292;&#24182;&#22312;GPT-4&#35780;&#20272;&#20013;&#33719;&#24471;&#20102;64.33%&#30340;&#20559;&#22909;&#25110;&#24182;&#21015;&#20998;&#25968;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#24378;&#35843;&#20102;&#35299;&#30721;&#30340;&#21019;&#26032;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing deco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24739;&#32773;&#30456;&#27604;&#65292;&#35299;&#31572;&#38750;&#19987;&#19994;&#24739;&#32773;&#20851;&#20110;&#23454;&#39564;&#23460;&#27979;&#35797;&#32467;&#26524;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#30456;&#20851;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26377;&#24110;&#21161;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#65292;&#20294;&#36824;&#23384;&#22312;&#28508;&#22312;&#38382;&#39064;&#38656;&#35201;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.01693</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#19982;&#20154;&#31867;&#24739;&#32773;&#30456;&#27604;&#65292;&#23545;&#38750;&#19987;&#19994;&#24739;&#32773;&#35299;&#37322;&#23454;&#39564;&#23460;&#27979;&#35797;&#32467;&#26524;&#30340;&#22238;&#31572;&#36136;&#37327;&#30340;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24739;&#32773;&#30456;&#27604;&#65292;&#35299;&#31572;&#38750;&#19987;&#19994;&#24739;&#32773;&#20851;&#20110;&#23454;&#39564;&#23460;&#27979;&#35797;&#32467;&#26524;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#30456;&#20851;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26377;&#24110;&#21161;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#65292;&#20294;&#36824;&#23384;&#22312;&#28508;&#22312;&#38382;&#39064;&#38656;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#23460;&#26816;&#27979;&#32467;&#26524;&#24120;&#24120;&#20196;&#20154;&#22256;&#24785;&#21644;&#38590;&#20197;&#29702;&#35299;&#12290;&#22823;&#22411;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#20102;&#33719;&#21462;&#38382;&#39064;&#31572;&#26696;&#30340;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#20351;&#29992;LLMs&#22238;&#31572;&#24739;&#32773;&#26377;&#20851;&#23454;&#39564;&#23460;&#27979;&#35797;&#38382;&#39064;&#30340;&#30456;&#20851;&#12289;&#20934;&#30830;&#12289;&#26377;&#24110;&#21161;&#24182;&#19988;&#26080;&#23475;&#30340;&#22238;&#31572;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#35782;&#21035;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#26041;&#27861;&#26469;&#32531;&#35299;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;Yahoo! Answers&#25910;&#38598;&#20102;&#19982;&#23454;&#39564;&#23460;&#27979;&#35797;&#32467;&#26524;&#30456;&#20851;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#25968;&#25454;&#65292;&#24182;&#36873;&#25321;&#20102;53&#20010;&#38382;&#31572;&#23545;&#36827;&#34892;&#26412;&#30740;&#31350;&#12290;&#20351;&#29992;LangChain&#26694;&#26550;&#21644;ChatGPT&#20114;&#32852;&#32593;&#38376;&#25143;&#65292;&#25105;&#20204;&#20174;&#22235;&#20010;LLMs&#65288;&#21253;&#25324;GPT-4&#12289;Meta LLaMA 2&#12289;MedAlpaca&#21644;ORCA_mini&#65289;&#29983;&#25104;&#20102;&#23545;&#36825;53&#20010;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#26631;&#20934;&#30340;&#38382;&#31572;&#30456;&#20284;&#24230;&#35780;&#20272;&#25351;&#26631;&#65288;&#21253;&#25324;ROUGE&#12289;BLEU&#12289;METEOR&#21644;BERTScore&#65289;&#35780;&#20272;&#20102;&#23427;&#20204;&#22238;&#31572;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#22522;&#20110;LLMs&#30340;&#35780;&#20272;&#22120;&#21028;&#26029;&#30446;&#26631;&#27169;&#22411;&#22312;&#30456;&#20851;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26377;&#24110;&#21161;&#24615;&#26041;&#38754;&#30340;&#36136;&#37327;&#26159;&#21542;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lab results are often confusing and hard to understand. Large language models (LLMs) such as ChatGPT have opened a promising avenue for patients to get their questions answered. We aim to assess the feasibility of using LLMs to generate relevant, accurate, helpful, and unharmful responses to lab test-related questions asked by patients and to identify potential issues that can be mitigated with augmentation approaches. We first collected lab test results related question and answer data from Yahoo! Answers and selected 53 QA pairs for this study. Using the LangChain framework and ChatGPT web portal, we generated responses to the 53 questions from four LLMs including GPT-4, Meta LLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their answers using standard QA similarity-based evaluation metrics including ROUGE, BLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge whether a target model has higher quality in terms of relevance, correctness, helpf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26368;&#23569;&#30340;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#23454;&#29616;&#35821;&#35328;&#33258;&#36866;&#24212;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22312;&#32454;&#35843;&#26399;&#38388;&#26367;&#25442;&#20266;&#26631;&#31614;&#22122;&#22768;&#37096;&#20998;&#65292;&#24182;&#32467;&#21512;&#23884;&#20837;&#21021;&#22987;&#21270;&#25216;&#24039;&#65292;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20165;&#26377;&#24456;&#23569;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26694;&#26550;&#20063;&#33021;&#21512;&#25104;&#21487;&#29702;&#35299;&#30340;&#26410;&#30693;&#35821;&#35328;&#35821;&#38899;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#25216;&#26415;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#39640;&#25928;&#35821;&#35328;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01692</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#28151;&#21512;&#21644;&#23884;&#20837;&#21021;&#22987;&#21270;&#23454;&#29616;&#36328;&#35821;&#35328;TTS&#33258;&#36866;&#24212;&#30340;&#26368;&#22823;&#25968;&#25454;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26368;&#23569;&#30340;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#23454;&#29616;&#35821;&#35328;&#33258;&#36866;&#24212;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22312;&#32454;&#35843;&#26399;&#38388;&#26367;&#25442;&#20266;&#26631;&#31614;&#22122;&#22768;&#37096;&#20998;&#65292;&#24182;&#32467;&#21512;&#23884;&#20837;&#21021;&#22987;&#21270;&#25216;&#24039;&#65292;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20165;&#26377;&#24456;&#23569;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26694;&#26550;&#20063;&#33021;&#21512;&#25104;&#21487;&#29702;&#35299;&#30340;&#26410;&#30693;&#35821;&#35328;&#35821;&#38899;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#25216;&#26415;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#39640;&#25928;&#35821;&#35328;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#20013;&#30340;&#35821;&#35328;&#33258;&#36866;&#24212;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#26368;&#23569;&#30340;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#23454;&#29616;&#35821;&#35328;&#33258;&#36866;&#24212;&#12290;&#34429;&#28982;&#35768;&#22810;&#24037;&#20316;&#20391;&#37325;&#20110;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#32771;&#34385;&#23613;&#37327;&#20943;&#23569;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#21033;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#65292;&#26367;&#25442;&#32454;&#35843;&#26399;&#38388;&#20266;&#26631;&#31614;&#20013;&#30340;&#22122;&#22768;&#37096;&#20998;&#65292;&#24182;&#32467;&#21512;&#23884;&#20837;&#21021;&#22987;&#21270;&#25216;&#24039;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#21033;&#29992;&#20102;&#26356;&#22810;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#20165;4&#20010;&#26631;&#35760;&#25968;&#25454;&#21644;15&#20998;&#38047;&#26410;&#26631;&#35760;&#25968;&#25454;&#21512;&#25104;&#21487;&#29702;&#35299;&#30340;&#26410;&#30693;&#35821;&#35328;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#26356;&#22810;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#36229;&#36807;&#20102;&#20256;&#32479;&#25216;&#26415;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#39640;&#25928;&#35821;&#35328;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32769;&#24180;&#20154;&#20013;&#21306;&#20998;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#21644;&#27491;&#24120;&#35748;&#30693;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21477;&#23376;&#23884;&#20837;&#21644;&#21477;&#23376;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#20998;&#26512;&#35270;&#39057;&#35775;&#35848;&#30340;&#36716;&#24405;&#25991;&#26412;&#65292;&#25552;&#21462;&#26102;&#24207;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24314;&#31435;&#31283;&#20581;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.01690</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#25439;&#22833;&#30340;&#35821;&#35328;&#23398;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Linguistic-Based Mild Cognitive Impairment Detection Using Informative Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32769;&#24180;&#20154;&#20013;&#21306;&#20998;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#21644;&#27491;&#24120;&#35748;&#30693;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21477;&#23376;&#23884;&#20837;&#21644;&#21477;&#23376;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#20998;&#26512;&#35270;&#39057;&#35775;&#35848;&#30340;&#36716;&#24405;&#25991;&#26412;&#65292;&#25552;&#21462;&#26102;&#24207;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24314;&#31435;&#31283;&#20581;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#32769;&#24180;&#20154;&#20013;&#30340;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#21644;&#27491;&#24120;&#35748;&#30693;&#65288;NC&#65289;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20998;&#26512;&#20102;&#22312;I-CONECT&#30740;&#31350;&#39033;&#30446;&#20013;&#25910;&#38598;&#30340;&#35270;&#39057;&#35775;&#35848;&#20013;&#29983;&#25104;&#30340;&#36716;&#24405;&#25991;&#26412;&#65292;&#35813;&#39033;&#30446;&#26159;&#19968;&#39033;&#26088;&#22312;&#36890;&#36807;&#35270;&#39057;&#32842;&#22825;&#25913;&#21892;&#35748;&#30693;&#21151;&#33021;&#30340;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;NLP&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22359;&#65292;&#21363;&#21477;&#23376;&#23884;&#20837;&#65288;SE&#65289;&#21644;&#21477;&#23376;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;SCA&#65289;&#12290;&#39318;&#20808;&#65292;SE&#27169;&#22359;&#25429;&#25417;&#27599;&#20010;&#21477;&#23376;&#20013;&#21333;&#35789;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;SCA&#27169;&#22359;&#25552;&#21462;&#21477;&#23376;&#24207;&#21015;&#30340;&#26102;&#24207;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#29305;&#24449;&#30001;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#29992;&#20110;&#23558;&#34987;&#35797;&#20998;&#20026;MCI&#25110;NC&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#20449;&#24687;&#25439;&#22833;&#65288;InfoLoss&#65289;&#65292;&#35813;&#20989;&#25968;&#36890;&#36807;&#35266;&#23519;&#27599;&#20010;&#21477;&#23376;&#24207;&#21015;&#26469;&#32771;&#34385;&#29109;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning method using Natural Language Processing (NLP) techniques, to distinguish between Mild Cognitive Impairment (MCI) and Normal Cognitive (NC) conditions in older adults. We propose a framework that analyzes transcripts generated from video interviews collected within the I-CONECT study project, a randomized controlled trial aimed at improving cognitive functions through video chats. Our proposed NLP framework consists of two Transformer-based modules, namely Sentence Embedding (SE) and Sentence Cross Attention (SCA). First, the SE module captures contextual relationships between words within each sentence. Subsequently, the SCA module extracts temporal features from a sequence of sentences. This feature is then used by a Multi-Layer Perceptron (MLP) for the classification of subjects into MCI or NC. To build a robust model, we propose a novel loss function, called InfoLoss, that considers the reduction in entropy by observing each sequence of sentences
&lt;/p&gt;</description></item><item><title>SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01685</link><description>&lt;p&gt;
SMUTF&#65306;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#21644;&#28151;&#21512;&#29305;&#24449;&#30340;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMUTF: Schema Matching Using Generative Tags and Hybrid Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01685
&lt;/p&gt;
&lt;p&gt;
SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SMUTF&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20551;&#35774;&#22312;&#24320;&#25918;&#22495;&#20219;&#21153;&#20013;&#65292;&#30417;&#30563;&#23398;&#20064;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36328;&#22495;&#21305;&#37197;&#12290;&#36825;&#20010;&#31995;&#32479;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#20154;&#36947;&#20027;&#20041;&#20132;&#25442;&#35821;&#35328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#29983;&#25104;&#26631;&#31614;&#8221;&#20026;&#27599;&#20010;&#25968;&#25454;&#21015;&#37096;&#32626;&#20102;&#21019;&#26032;&#30340;&#36866;&#24212;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#21305;&#37197;&#30340;&#25928;&#26524;&#12290;SMUTF&#20855;&#26377;&#24191;&#27867;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;&#20998;&#31867;&#26041;&#27861;&#21644;&#29983;&#25104;&#27169;&#22411;&#26080;&#32541;&#37197;&#21512;&#20351;&#29992;&#12290;&#37492;&#20110;&#27169;&#24335;&#21305;&#37197;&#32570;&#20047;&#24191;&#27867;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24050;&#32463;&#21019;&#24314;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#20844;&#20849;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#26159;&#30446;&#21069;&#26368;&#20840;&#38754;&#30340;&#27169;&#24335;&#21305;&#37197;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.   Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated excep
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;CGC-LORA&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25671;&#25670;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01684</link><description>&lt;p&gt;
&#20351;&#29992;CGC-LORA&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01684
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;CGC-LORA&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25671;&#25670;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#28436;&#36827;&#65292;&#20154;&#20204;&#20026;&#20102;&#26377;&#25928;&#22320;&#24494;&#35843;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;LLMs&#20197;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#29305;&#23450;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#26377;&#20004;&#31181;&#20027;&#35201;&#30340;&#36866;&#24212;&#26041;&#24335;&#65306;&#65288;i&#65289;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#65306;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#30456;&#24212;&#35757;&#32451;&#26679;&#26412;&#23545;&#39044;&#35757;&#32451;LLMs&#36827;&#34892;&#29420;&#31435;&#30340;&#24494;&#35843;&#65307;&#65288;ii&#65289;&#38598;&#25104;&#27169;&#22411;&#65306;&#20351;&#29992;&#25152;&#26377;&#20219;&#21153;&#30340;&#26679;&#26412;&#26469;&#32852;&#21512;&#24494;&#35843;&#39044;&#35757;&#32451;LLMs &#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25671;&#25670;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#21046;&#38376;&#25511;&#65288;CGC&#65289;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#31639;&#27861;&#22312;LLMs&#20013;&#23454;&#29616;&#20102;1 + N&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;MTL&#65288;&#21363;CGC&#65289;&#21644;PEFT&#65288;&#21363;LoRA&#65289;&#26041;&#26696;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#20219;&#21153;&#38598;&#32676;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#23618;&#65292;&#20854;&#20013;&#21253;&#21547;...
&lt;/p&gt;
&lt;p&gt;
With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain. In practice, there are two prevailing ways, in which the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently using the corresponding training samples from each task. (ii) An Integrated Model: Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. To address the high computing cost and seesawing issue simultaneously, we propose a unified framework that implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an innovative layer that contai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30740;&#31350;&#20102;2023&#24180;&#21152;&#25343;&#22823;&#37326;&#28779;&#28895;&#38654;&#22312;&#32445;&#32422;&#24066;&#24341;&#21457;&#30340;&#21361;&#26426;&#27963;&#21160;&#20851;&#27880;&#12290;&#36890;&#36807;&#25972;&#21512;&#22320;&#29702;&#26631;&#35760;&#30340;Twitter&#25968;&#25454;&#21644;&#22269;&#23478;&#25968;&#25454;&#24211;&#65292;&#24320;&#21457;&#27169;&#22411;&#23545;&#19981;&#21516;&#27963;&#21160;&#20851;&#27880;&#36827;&#34892;&#31038;&#21306;&#25512;&#26029;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#22312;&#37326;&#28779;&#26399;&#38388;&#65292;&#32445;&#32422;&#24066;&#23621;&#27665;&#30340;&#27963;&#21160;&#20851;&#27880;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.01683</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#31038;&#21306;&#34892;&#20026;&#23545;&#21361;&#26426;&#27963;&#21160;&#20851;&#27880;&#30340;&#29702;&#35299;&#65306;&#20197;2023&#24180;&#32445;&#32422;&#24066;&#21152;&#25343;&#22823;&#37326;&#28779;&#20026;&#20363;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Community-based Behavioral Understanding of Crisis Activity Concerns using Social Media Data: A Study on the 2023 Canadian Wildfires in New York City
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30740;&#31350;&#20102;2023&#24180;&#21152;&#25343;&#22823;&#37326;&#28779;&#28895;&#38654;&#22312;&#32445;&#32422;&#24066;&#24341;&#21457;&#30340;&#21361;&#26426;&#27963;&#21160;&#20851;&#27880;&#12290;&#36890;&#36807;&#25972;&#21512;&#22320;&#29702;&#26631;&#35760;&#30340;Twitter&#25968;&#25454;&#21644;&#22269;&#23478;&#25968;&#25454;&#24211;&#65292;&#24320;&#21457;&#27169;&#22411;&#23545;&#19981;&#21516;&#27963;&#21160;&#20851;&#27880;&#36827;&#34892;&#31038;&#21306;&#25512;&#26029;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#22312;&#37326;&#28779;&#26399;&#38388;&#65292;&#32445;&#32422;&#24066;&#23621;&#27665;&#30340;&#27963;&#21160;&#20851;&#27880;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;6&#26376;&#65292;&#30001;&#21152;&#25343;&#22823;&#37326;&#28779;&#39128;&#20837;&#32445;&#32422;&#24066;&#30340;&#28895;&#38654;&#20351;&#24471;&#35813;&#24066;&#31354;&#27668;&#27745;&#26579;&#36798;&#21040;&#20840;&#29699;&#26368;&#20005;&#37325;&#27700;&#24179;&#12290;&#36825;&#31181;&#21069;&#25152;&#26410;&#26377;&#30340;&#24773;&#20917;&#23548;&#33268;&#32445;&#32422;&#24066;&#23621;&#27665;&#30340;&#20986;&#34892;&#21644;&#20256;&#32479;&#27963;&#21160;&#27169;&#24335;&#21457;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30740;&#31350;&#20102;2023&#24180;&#21152;&#25343;&#22823;&#37326;&#28779;&#28895;&#38654;&#22312;&#32445;&#32422;&#24066;&#20986;&#29616;&#26102;&#30340;&#19981;&#21516;&#21361;&#26426;&#27963;&#21160;&#20851;&#27880;&#65288;&#21253;&#25324;&#30095;&#25955;&#12289;&#21574;&#22312;&#23460;&#20869;&#12289;&#36141;&#29289;&#21644;&#23089;&#20048;&#27963;&#21160;&#31561;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#32445;&#32422;&#24066;&#19968;&#20010;&#26143;&#26399;&#65288;2023&#24180;6&#26376;2&#26085;&#33267;6&#26376;9&#26085;&#65289;&#30340;&#22320;&#29702;&#26631;&#35760;&#30340;Twitter&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20808;&#36827;&#30340;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#23545;&#36825;&#20123;&#25512;&#25991;&#36827;&#34892;&#20102;&#22788;&#29702;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#31038;&#20445;&#23616;&#25968;&#25454;&#12289;&#20154;&#21475;&#26222;&#26597;&#21644;&#32654;&#22269;&#31038;&#21306;&#35843;&#26597;&#31561;&#22269;&#23478;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#25972;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#23545;&#37325;&#22823;&#37326;&#28779;&#20013;&#19981;&#21516;&#27963;&#21160;&#20851;&#27880;&#36827;&#34892;&#31038;&#21306;&#25512;&#26029;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37326;&#28779;&#26399;&#38388;&#65292;&#32445;&#32422;&#24066;&#23621;&#27665;&#30340;&#27963;&#21160;&#20851;&#27880;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
New York City (NYC) topped the global chart for the worst air pollution in June 2023, owing to the wildfire smoke drifting in from Canada. This unprecedented situation caused significant travel disruptions and shifts in traditional activity patterns of NYC residents. This study utilized large-scale social media data to study different crisis activity concerns (i.e., evacuation, staying indoors, shopping, and recreational activities among others) in the emergence of the 2023 Canadian wildfire smoke in NYC. In this regard, one week (June 02 through June 09, 2023) geotagged Twitter data from NYC were retrieved and used in the analysis. The tweets were processed using advanced text classification techniques and later integrated with national databases such as Social Security Administration data, Census, and American Community Survey. Finally, a model has been developed to make community inferences of different activity concerns in a major wildfire. The findings suggest, during wildfires, f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#22823;&#35268;&#27169;&#22238;&#24212;&#21644;&#24314;&#31435;&#32479;&#35745;&#27169;&#22411;&#65292;&#28145;&#20837;&#20102;&#35299;&#20102;&#20010;&#20307;&#23545;&#20132;&#36890;&#21487;&#21450;&#24615;&#12289;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#21644;&#20844;&#20849;&#22522;&#30784;&#35774;&#26045;&#30340;&#24863;&#30693;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22899;&#24615;&#12289;&#20122;&#27954;&#35028;&#21644;&#32463;&#21382;&#39640;&#20132;&#36890;&#27969;&#37327;&#30340;&#20010;&#20307;&#26356;&#20851;&#27880;&#20132;&#36890;&#21487;&#21450;&#24615;&#65292;&#32780;&#20855;&#26377;&#31038;&#20250;&#32463;&#27982;&#21155;&#21183;&#30340;&#20010;&#20307;&#26356;&#20851;&#27880;&#20844;&#20849;&#20132;&#36890;&#38382;&#39064;&#24182;&#34920;&#36798;&#26356;&#24378;&#28872;&#30340;&#20851;&#20999;&#12290;</title><link>https://arxiv.org/abs/2402.01682</link><description>&lt;p&gt;
&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#35782;&#21035;&#24433;&#21709;&#20844;&#20247;&#23545;&#21487;&#21450;&#24615;&#12289;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#21644;&#20844;&#20849;&#20132;&#36890;&#24577;&#24230;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Leveraging Social Media Data to Identify Factors Influencing Public Attitude Towards Accessibility, Socioeconomic Disparity and Public Transportation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#22823;&#35268;&#27169;&#22238;&#24212;&#21644;&#24314;&#31435;&#32479;&#35745;&#27169;&#22411;&#65292;&#28145;&#20837;&#20102;&#35299;&#20102;&#20010;&#20307;&#23545;&#20132;&#36890;&#21487;&#21450;&#24615;&#12289;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#21644;&#20844;&#20849;&#22522;&#30784;&#35774;&#26045;&#30340;&#24863;&#30693;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22899;&#24615;&#12289;&#20122;&#27954;&#35028;&#21644;&#32463;&#21382;&#39640;&#20132;&#36890;&#27969;&#37327;&#30340;&#20010;&#20307;&#26356;&#20851;&#27880;&#20132;&#36890;&#21487;&#21450;&#24615;&#65292;&#32780;&#20855;&#26377;&#31038;&#20250;&#32463;&#27982;&#21155;&#21183;&#30340;&#20010;&#20307;&#26356;&#20851;&#27880;&#20844;&#20849;&#20132;&#36890;&#38382;&#39064;&#24182;&#34920;&#36798;&#26356;&#24378;&#28872;&#30340;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#29702;&#35299;&#24433;&#21709;&#20010;&#20307;&#23545;&#20132;&#36890;&#21487;&#21450;&#24615;&#12289;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#21644;&#20844;&#20849;&#22522;&#30784;&#35774;&#26045;&#24863;&#30693;&#30340;&#22240;&#32032;&#12290;&#19982;&#32791;&#26102;&#26114;&#36149;&#30340;&#35843;&#26597;&#26041;&#27861;&#30456;&#21453;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#31038;&#20132;&#23186;&#20307;&#29983;&#25104;&#26377;&#26426;&#30340;&#22823;&#35268;&#27169;&#22238;&#24212;&#65292;&#24182;&#24320;&#21457;&#32479;&#35745;&#27169;&#22411;&#26469;&#29702;&#35299;&#20010;&#20307;&#23545;&#21508;&#31181;&#20132;&#36890;&#38382;&#39064;&#30340;&#24863;&#30693;&#12290;&#26412;&#30740;&#31350;&#20174;2020&#24180;3&#26376;19&#26085;&#33267;2022&#24180;5&#26376;15&#26085;&#65292;&#26816;&#32034;&#24182;&#20998;&#26512;&#20102;&#32445;&#32422;&#24066;&#30340;36,098&#26465;&#25512;&#25991;&#12290;&#37319;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#36827;&#34892;&#25991;&#26412;&#25366;&#25496;&#21644;&#20998;&#31867;&#12290;&#37319;&#29992;&#25968;&#25454;&#34701;&#21512;&#25216;&#26415;&#29983;&#25104;&#19968;&#31995;&#21015;&#29992;&#20316;&#27169;&#22411;&#35299;&#37322;&#21464;&#37327;&#30340;&#31038;&#20250;&#32463;&#27982;&#29305;&#24449;&#12290;&#27169;&#22411;&#32467;&#26524;&#26174;&#31034;&#65292;&#22899;&#24615;&#21644;&#20122;&#27954;&#35028;&#20010;&#20307;&#26356;&#20542;&#21521;&#20110;&#35752;&#35770;&#20132;&#36890;&#21487;&#21450;&#24615;&#65292;&#32780;&#37027;&#20123;&#32463;&#21382;&#39640;&#20132;&#36890;&#27969;&#37327;&#30340;&#20154;&#26356;&#21152;&#31215;&#26497;&#21457;&#22768;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#31038;&#20250;&#32463;&#27982;&#21155;&#21183;&#30340;&#20010;&#20307;&#26356;&#20851;&#27880;&#20844;&#20849;&#20132;&#36890;&#38382;&#39064;&#65292;&#24182;&#34920;&#36798;&#20986;&#26356;&#24378;&#28872;&#30340;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a novel method to understand the factors affecting individuals' perception of transport accessibility, socioeconomic disparity, and public infrastructure. As opposed to the time consuming and expensive survey-based approach, this method can generate organic large-scale responses from social media and develop statistical models to understand individuals' perceptions of various transportation issues. This study retrieved and analyzed 36,098 tweets from New York City from March 19, 2020, to May 15, 2022. A state-of-the-art natural language processing algorithm is used for text mining and classification. A data fusion technique has been adopted to generate a series of socioeconomic traits that are used as explanatory variables in the model. The model results show that females and individuals of Asian origin tend to discuss transportation accessibility more than their counterparts, with those experiencing high neighborhood traffic also being more vocal. However, disadvan
&lt;/p&gt;</description></item><item><title>&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.01681</link><description>&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#35299;&#23494;&#65306;&#21033;&#29992;ChatGPT&#25552;&#21319;&#31038;&#20132;&#23186;&#20307;&#27807;&#36890;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01681
&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#22312;&#31038;&#20132;&#32593;&#32476;&#27807;&#36890;&#20013;&#24050;&#32463;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#20204;&#25215;&#36733;&#20102;&#36229;&#36234;&#25991;&#23383;&#25110;&#30701;&#35821;&#30340;&#35821;&#20041;&#65292;&#36825;&#24341;&#21457;&#20102;&#23398;&#26415;&#30028;&#23545;&#20854;&#23646;&#24615;&#21644;&#21151;&#33021;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#34920;&#24773;&#31526;&#21495;&#30456;&#20851;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#32773;&#36890;&#24120;&#20381;&#36182;&#20247;&#21253;&#26469;&#27880;&#37322;&#34920;&#24773;&#31526;&#21495;&#65292;&#20197;&#20102;&#35299;&#20854;&#24773;&#24863;&#12289;&#20351;&#29992;&#24847;&#22270;&#21644;&#35821;&#20041;&#21547;&#20041;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#30340;&#20027;&#35266;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#35823;&#35299;&#65292;&#24182;&#36896;&#25104;&#27807;&#36890;&#38556;&#30861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#27880;&#37322;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;ChatGPT&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#19987;&#19994;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#20197;&#21069;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39564;&#35777;ChatGPT&#21487;&#20197;&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#20316;&#20026;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#21487;&#34892;&#26367;&#20195;&#32773;&#65292;&#24182;&#39564;&#35777;&#20854;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31687;&#32508;&#36848;&#32473;&#20986;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#37325;&#35201;&#26041;&#38754;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01680</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65306;&#36827;&#23637;&#19982;&#25361;&#25112;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Model based Multi-Agents: A Survey of Progress and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01680
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31687;&#32508;&#36848;&#32473;&#20986;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#37325;&#35201;&#26041;&#38754;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;LLMs&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35268;&#21010;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#23427;&#20204;&#34987;&#29992;&#20316;&#33258;&#20027;&#26234;&#33021;&#20307;&#26469;&#33258;&#21160;&#23436;&#25104;&#35768;&#22810;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#23558;&#19968;&#20010;LLM&#29992;&#20316;&#21333;&#20010;&#35268;&#21010;&#25110;&#20915;&#31574;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#20026;&#31038;&#21306;&#25552;&#20379;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#39046;&#22495;&#30340;&#32508;&#36848;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22522;&#26412;&#26041;&#38754;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35753;&#35835;&#32773;&#23545;&#20197;&#19979;&#38382;&#39064;&#33719;&#24471;&#23454;&#36136;&#24615;&#35265;&#35299;&#65306;LLM-based&#22810;&#26234;&#33021;&#20307;&#27169;&#25311;&#21738;&#20123;&#39046;&#22495;&#21644;&#29615;&#22659;&#65311;&#36825;&#20123;&#26234;&#33021;&#20307;&#26159;&#22914;&#20309;&#24314;&#27169;&#21644;&#36890;&#20449;&#30340;&#65311;&#20160;&#20040;&#26426;&#21046;&#26377;&#21161;&#20110;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22686;&#38271;&#65311;&#23545;&#20110;&#37027;&#20123;&#23545;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#24863;&#20852;&#36259;&#30340;&#20154;&#65292;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#19968;&#20123;&#35201;&#28857;&#21644;&#25361;&#25112;.
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#35813;&#20195;&#29702;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#20026;&#20102;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01679</link><description>&lt;p&gt;
StickerConv: &#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#22810;&#27169;&#24577;&#20849;&#24773;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
StickerConv: Generating Multimodal Empathetic Responses from Scratch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#35813;&#20195;&#29702;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#20026;&#20102;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#20849;&#24773;&#23545;&#35805;&#30740;&#31350;&#20013;&#65292;&#36148;&#32440;&#23613;&#31649;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#25552;&#39640;&#22312;&#32447;&#20132;&#27969;&#20013;&#30340;&#20849;&#24773;&#33021;&#21147;&#65292;&#20294;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#21253;&#25324;12.9K&#20010;&#23545;&#35805;&#20250;&#35805;&#65292;5.8K&#20010;&#29420;&#29305;&#36148;&#32440;&#21644;2K&#20010;&#22810;&#26679;&#21270;&#20250;&#35805;&#22330;&#26223;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#22810;&#27169;&#24577;&#24773;&#22659;&#19979;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;LLM&#30340;&#20840;&#38754;&#20849;&#24773;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PEGS&#22312;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research. In this paper, we introduce the Agent for StickerConv (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, StickerConv, which includes 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios, specifically designs to augment the generation of empathetic responses in a multimodal context. To leverage the richness of this dataset, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation model, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotional
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#23884;&#20837;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21253;&#21547;&#39046;&#22495;&#20869;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#21363;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22806;&#24310;&#30693;&#35782;&#25552;&#20379;&#20851;&#20110;&#26412;&#20307;&#20013;&#29305;&#23450;&#27010;&#24565;&#25152;&#23646;&#30340;&#20855;&#20307;&#23454;&#20363;&#30340;&#20449;&#24687;&#65292;&#32780;&#20869;&#28085;&#30693;&#35782;&#35814;&#32454;&#25551;&#36848;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#20869;&#22312;&#23646;&#24615;&#12289;&#29305;&#24449;&#21644;&#35821;&#20041;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20805;&#20998;&#32771;&#34385;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EIKE&#65288;Extensional and Intensional Knowledge Embedding&#65289;&#30340;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#12290;EIKE&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#23454;&#20363;&#12289;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#23884;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#23545;&#22806;&#24310;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20869;&#28085;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20440;&#33719;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65292;&#19981;&#20165;&#25972;&#20307;&#20934;&#30830;&#29575;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01676</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#22312;&#20851;&#38190;&#35821;&#27861;&#32467;&#26500;&#19978;&#30340;&#21028;&#26029;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language models align with human judgments on key grammatical constructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20440;&#33719;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65292;&#19981;&#20165;&#25972;&#20307;&#20934;&#30830;&#29575;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#35821;&#35328;&#26222;&#36941;&#24615;&#65311;Dentella&#31561;&#20154;&#65288;2023&#24180;&#65307;&#8220;DGL&#8221;&#65289;&#20351;&#29992;&#22810;&#20010;LLMs&#25552;&#31034;&#35821;&#27861;&#27491;&#30830;&#24615;&#38382;&#39064;&#65292;&#20197;&#33719;&#21462;80&#20010;&#33521;&#35821;&#21477;&#23376;&#30340;&#35821;&#27861;&#21477;&#23376;&#21028;&#26029;&#65292;&#24471;&#20986;LLMs&#23384;&#22312;&#8220;&#26159;&#8221;&#20559;&#21521;&#21644;&#8220;&#19981;&#33021;&#21306;&#20998;&#35821;&#27861;&#21644;&#38750;&#35821;&#27861;&#21477;&#23376;&#8221;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26082;&#23450;&#30340;&#23454;&#36341;&#26041;&#27861;&#37325;&#26032;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;DGL&#30340;&#25968;&#25454;&#23454;&#38469;&#19978;&#35777;&#26126;&#20102;LLM&#22914;&#20309;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#34892;&#20026;&#12290;&#27169;&#22411;&#19981;&#20165;&#25972;&#20307;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#36824;&#25429;&#25417;&#21040;&#20102;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models (LLMs) make human-like linguistic generalizations? Dentella et al. (2023; "DGL") prompt several LLMs ("Is the following sentence grammatically correct in English?") to elicit grammaticality judgments of 80 English sentences, concluding that LLMs demonstrate a "yes-response bias" and a "failure to distinguish grammatical from ungrammatical sentences". We re-evaluate LLM performance using well-established practices and find that DGL's data in fact provide evidence for just how well LLMs capture human behaviors. Models not only achieve high accuracy overall, but also capture fine-grained variation in human linguistic judgments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36861;&#36394;&#24605;&#24819;&#31995;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#26816;&#27979;&#24605;&#24819;&#24433;&#21709;&#12290;&#36890;&#36807;&#32467;&#21512;&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#12289;&#21477;&#23376;&#23884;&#20837;&#21644;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#22270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#25628;&#32034;&#23454;&#36136;&#19978;&#30456;&#20284;&#30340;&#24605;&#24819;&#21644;&#24605;&#24819;&#24433;&#21709;&#30340;&#36857;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.01661</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36861;&#36394;&#24605;&#24819;&#30340;&#31995;&#35889;
&lt;/p&gt;
&lt;p&gt;
Tracing the Genealogies of Ideas with Large Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36861;&#36394;&#24605;&#24819;&#31995;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#26816;&#27979;&#24605;&#24819;&#24433;&#21709;&#12290;&#36890;&#36807;&#32467;&#21512;&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#12289;&#21477;&#23376;&#23884;&#20837;&#21644;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#22270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#25628;&#32034;&#23454;&#36136;&#19978;&#30456;&#20284;&#30340;&#24605;&#24819;&#21644;&#24605;&#24819;&#24433;&#21709;&#30340;&#36857;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#30340;&#24605;&#24819;&#24433;&#21709;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#35821;&#20041;&#21644;&#32467;&#26500;&#24847;&#20041;&#26041;&#38754;&#30340;&#29420;&#29305;&#20248;&#21183;&#65292;&#24182;&#22312;&#20445;&#25345;&#23545;&#25442;&#21477;&#26041;&#24335;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20197;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#24335;&#25628;&#32034;&#20855;&#26377;&#23454;&#36136;&#24615;&#30456;&#20284;&#24605;&#24819;&#21644;&#24605;&#24819;&#24433;&#21709;&#36857;&#35937;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#25805;&#20316;&#19981;&#21516;&#30340;&#32622;&#20449;&#27700;&#24179;&#65306;&#25105;&#20204;&#21487;&#20197;&#20801;&#35768;&#30452;&#25509;&#24341;&#29992;&#12289;&#25913;&#20889;&#25110;&#25512;&#27979;&#24615;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#23545;&#27599;&#20010;&#38408;&#20540;&#30340;&#38480;&#21046;&#20445;&#25345;&#24320;&#25918;&#24577;&#24230;&#12290;&#25105;&#24212;&#29992;&#20102;&#32508;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#12289;&#19968;&#31181;&#20248;&#21270;&#25429;&#25417;&#35821;&#20041;&#20869;&#23481;&#30340;&#26368;&#20808;&#36827;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#25429;&#25417;&#35770;&#35777;&#39118;&#26684;&#21644;&#38544;&#21947;&#20351;&#29992;&#19978;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#22270;&#34920;&#31034;&#27861;&#12290;&#25105;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#32422;400,000&#26412;&#38750;&#23567;&#35828;&#20070;&#31821;&#21644;&#23398;&#26415;&#20986;&#29256;&#29289;&#30340;&#35821;&#26009;&#24211;&#20013;&#20197;&#21521;&#37327;&#21270;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, I present a novel method to detect intellectual influence across a large corpus. Taking advantage of the unique affordances of large language models in encoding semantic and structural meaning while remaining robust to paraphrasing, we can search for substantively similar ideas and hints of intellectual influence in a computationally efficient manner. Such a method allows us to operationalize different levels of confidence: we can allow for direct quotation, paraphrase, or speculative similarity while remaining open about the limitations of each threshold. I apply an ensemble method combining General Text Embeddings, a state-of-the-art sentence embedding method optimized to capture semantic content and an Abstract Meaning Representation graph representation designed to capture structural similarities in argumentation style and the use of metaphor. I apply this method to vectorize sentences from a corpus of roughly 400,000 nonfiction books and academic publications from t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.01643</link><description>&lt;p&gt;
L-TUNING&#65306;&#29992;&#20110;LLMs&#20013;&#30340;&#25552;&#31034;&#21644;&#21069;&#32512;&#30340;&#21516;&#27493;&#26631;&#31614;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20219;&#24847;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#24182;&#19988;&#36890;&#29992;&#26631;&#35760;&#22312;&#21508;&#31181;&#31867;&#21035;&#26631;&#31614;&#20013;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;L-Tuning&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#35774;&#35745;&#30340;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;L-Tuning&#19987;&#27880;&#20110;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;LLM&#22788;&#29702;&#30340;&#26631;&#31614;&#26631;&#35760;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#21033;&#29992;&#20854;&#39044;&#20808;&#23384;&#22312;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#25552;&#39640;&#20102;&#24494;&#35843;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36824;&#20419;&#36827;&#20102;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#19981;&#21516;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;L-Tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#23545;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#65292;&#25351;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#34394;&#20551;&#25991;&#26412;&#22823;&#37327;&#23384;&#22312;&#20110;&#20844;&#20849;&#39046;&#22495;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#26469;&#24212;&#23545;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#21361;&#38505;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01642</link><description>&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Detection of Machine-Generated Text: Literature Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#23545;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#65292;&#25351;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#34394;&#20551;&#25991;&#26412;&#22823;&#37327;&#23384;&#22312;&#20110;&#20844;&#20849;&#39046;&#22495;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#26469;&#24212;&#23545;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#21361;&#38505;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#36731;&#26494;&#22320;&#20135;&#29983;&#34394;&#20551;&#25991;&#26412;&#65292;&#20844;&#20849;&#39046;&#22495;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#27492;&#31867;&#20869;&#23481;&#12290;&#22312;&#19981;&#26029;&#25552;&#21319;&#30340;&#22797;&#26434;&#24230;&#21644;&#20889;&#20316;&#39118;&#26684;&#19979;&#65292;&#20960;&#20046;&#26080;&#27861;&#21306;&#20998;&#20154;&#31867;&#25776;&#20889;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#19982;&#20154;&#24037;&#20316;&#32773;&#30456;&#27604;&#65292;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20316;&#21697;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#23186;&#20307;&#20851;&#27880;&#24182;&#24341;&#36215;&#20102;&#20105;&#35758;&#12290;&#23545;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#30340;&#24433;&#21709;&#30340;&#25285;&#24551;&#20063;&#24212;&#36816;&#32780;&#29983;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#36807;&#31243;&#26377;&#26356;&#20805;&#20998;&#30340;&#20102;&#35299;&#12290;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65306;&#20854;&#33539;&#22260;&#19981;&#20165;&#28183;&#36879;&#21040;&#26032;&#38395;&#25253;&#36947;&#21644;&#23458;&#25143;&#26381;&#21153;&#65292;&#36824;&#28041;&#21450;&#21040;&#23398;&#26415;&#30028;&#12290;&#20026;&#20102;&#20943;&#36731;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#21361;&#38505;&#24433;&#21709;&#65292;&#24517;&#39035;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#65292;&#20363;&#22914;&#20026;&#20154;&#31867;&#25805;&#20316;&#21592;&#25552;&#20379;&#21306;&#20998;&#34394;&#20551;&#25991;&#26412;&#21644;&#30495;&#23454;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since language models produce fake text quickly and easily, there is an oversupply of such content in the public domain. The degree of sophistication and writing style has reached a point where differentiating between human authored and machine-generated content is nearly impossible. As a result, works generated by language models rather than human authors have gained significant media attention and stirred controversy.Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes. Natural language generation (NLG) and generative pre-trained transformer (GPT) models have revolutionized a variety of sectors: the scope not only permeated throughout journalism and customer service but also reached academia. To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented, such as providing human agents with the capacity to distinguish between artif
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21477;&#27861;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#21487;&#33021;&#25581;&#31034;&#20102;&#36866;&#29992;&#20110;&#25152;&#26377;&#33258;&#28982;&#35821;&#35328;&#30340;&#36890;&#29992;&#21477;&#27861;&#32467;&#26500;&#30340;&#23384;&#22312;&#12290;&#36825;&#23545;&#29702;&#35299;&#20154;&#31867;&#22823;&#33041;&#20013;&#35821;&#35328;&#30340;&#36816;&#20316;&#26041;&#24335;&#20197;&#21450;&#30456;&#20851;&#23398;&#31185;&#30340;&#29702;&#35770;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.01641</link><description>&lt;p&gt;
&#36890;&#29992;&#21477;&#27861;&#32467;&#26500;&#65306;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#30340;&#21477;&#27861;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Universal Syntactic Structures: Modeling Syntax for Various Natural Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01641
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21477;&#27861;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#21487;&#33021;&#25581;&#31034;&#20102;&#36866;&#29992;&#20110;&#25152;&#26377;&#33258;&#28982;&#35821;&#35328;&#30340;&#36890;&#29992;&#21477;&#27861;&#32467;&#26500;&#30340;&#23384;&#22312;&#12290;&#36825;&#23545;&#29702;&#35299;&#20154;&#31867;&#22823;&#33041;&#20013;&#35821;&#35328;&#30340;&#36816;&#20316;&#26041;&#24335;&#20197;&#21450;&#30456;&#20851;&#23398;&#31185;&#30340;&#29702;&#35770;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#35299;&#37322;&#20154;&#31867;&#22823;&#33041;&#26159;&#22914;&#20309;&#23558;&#35789;&#36830;&#25509;&#36215;&#26469;&#24418;&#25104;&#21477;&#23376;&#30340;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21477;&#27861;&#34920;&#31034;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#33021;&#34920;&#26126;&#23384;&#22312;&#30528;&#36866;&#29992;&#20110;&#25152;&#26377;&#33258;&#28982;&#35821;&#35328;&#30340;&#36890;&#29992;&#21477;&#27861;&#32467;&#26500;&#12290;&#23601;&#20687;&#21457;&#29616;DNA&#30340;&#21452;&#34746;&#26059;&#32467;&#26500;&#25581;&#31034;&#20102;&#22522;&#22240;&#32452;&#30340;&#20869;&#37096;&#36816;&#20316;&#19968;&#26679;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#23545;&#20154;&#33041;&#20013;&#35821;&#35328;&#30340;&#36816;&#20316;&#26041;&#24335;&#25552;&#20379;&#22522;&#26412;&#30340;&#29702;&#35299;&#12290;&#36825;&#21487;&#33021;&#26159;&#22823;&#33041;&#23545;&#30693;&#35782;&#36827;&#34892;&#32534;&#30721;&#21644;&#35299;&#30721;&#30340;&#26041;&#24335;&#12290;&#23427;&#36824;&#20026;&#35821;&#35328;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#20123;&#27934;&#35265;&#12290;&#36890;&#36807;&#30740;&#31350;&#36890;&#29992;&#21477;&#27861;&#32467;&#26500;&#30340;&#36923;&#36753;&#20197;&#21450;&#24314;&#27169;&#25216;&#26415;&#30340;&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#23581;&#35797;&#20998;&#26512;&#23637;&#31034;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#21644;&#38889;&#35821;&#65289;&#35821;&#35328;&#36807;&#31243;&#20013;&#26222;&#36941;&#24615;&#30340;&#35821;&#26009;&#24211;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#38190;&#26399;&#20551;&#35828;&#12289;&#36890;&#29992;&#35821;&#27861;&#21644;&#20851;&#20110;&#35821;&#35328;&#30340;&#20960;&#31181;&#20854;&#20182;&#20027;&#24352;&#65292;&#20197;&#25512;&#36827;&#30740;&#31350;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to provide an explanation for how the human brain might connect words for sentence formation. A novel approach to modeling syntactic representation is introduced, potentially showing the existence of universal syntactic structures for all natural languages. As the discovery of DNA's double helix structure shed light on the inner workings of genetics, we wish to introduce a basic understanding of how language might work in the human brain. It could be the brain's way of encoding and decoding knowledge. It also brings some insight into theories in linguistics, psychology, and cognitive science. After looking into the logic behind universal syntactic structures and the methodology of the modeling technique, we attempt to analyze corpora that showcase universality in the language process of different natural languages such as English and Korean. Lastly, we discuss the critical period hypothesis, universal grammar, and a few other assertions on language for the purpose of advancing o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#26410;&#35265;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#24456;&#22810;&#27169;&#22411;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#21387;&#32553;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.00861</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models for Generalization and Robustness via Data Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00861
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#26410;&#35265;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#24456;&#22810;&#27169;&#22411;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#21387;&#32553;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#38754;&#20020;&#25968;&#25454;&#27745;&#26579;&#12289;&#23545;&#25552;&#31034;&#25935;&#24863;&#20197;&#21450;&#22522;&#20934;&#27979;&#35797;&#21019;&#24314;&#25104;&#26412;&#39640;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#27979;&#35797;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#20854;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#27867;&#21270;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20174;2017&#24180;&#21040;2023&#24180;&#20849;83&#20010;&#26376;&#30340;&#20840;&#38754;&#27979;&#35797;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#25130;&#27490;&#26085;&#26399;&#23558;&#25968;&#25454;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#26399;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65306;1&#65289;&#27979;&#35797;&#26399;&#30340;&#21387;&#32553;&#24615;&#33021;&#20316;&#20026;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#34913;&#37327;&#65307;2&#65289;&#35757;&#32451;&#26399;&#21644;&#27979;&#35797;&#26399;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#20316;&#20026;&#40065;&#26834;&#24615;&#30340;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35768;&#22810;&#27169;&#22411;&#30340;&#21387;&#32553;&#29575;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#26174;&#33879;&#38477;&#20302;&#65292;&#20294;&#20687;... (&#20869;&#23481;&#36807;&#38271;&#65292;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#21644;&#22810;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#20943;&#23569;&#38543;&#26426;&#23631;&#34109;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#30340;&#21516;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#23569;&#26679;&#26412;&#21644;&#20010;&#20307;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00263</link><description>&lt;p&gt;
DetectGPT&#26159;&#21542;&#20805;&#20998;&#21033;&#29992;&#20102;&#25200;&#21160;&#65311;&#22522;&#20110;&#27169;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#36873;&#25321;&#24615;&#25200;&#21160;&#20250;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00263
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#21644;&#22810;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#20943;&#23569;&#38543;&#26426;&#23631;&#34109;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#30340;&#21516;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#23569;&#26679;&#26412;&#21644;&#20010;&#20307;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#20854;&#28389;&#29992;&#30340;&#22686;&#38271;&#20851;&#27880;&#12290;DetectGPT&#26159;&#19968;&#31181;&#38646;-shot&#22522;&#20110;&#24230;&#37327;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#25200;&#21160;&#24182;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;DetectGPT&#30340;&#38543;&#26426;&#25200;&#21160;&#31574;&#30053;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#65292;&#38480;&#21046;&#20102;&#21487;&#21306;&#20998;&#24615;&#21644;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#36923;&#36753;&#22238;&#24402;&#27169;&#22359;&#20381;&#36182;&#20110;&#35774;&#32622;&#38408;&#20540;&#65292;&#36825;&#20250;&#24433;&#21709;&#20010;&#20307;&#25110;&#23567;&#25209;&#37327;&#36755;&#20837;&#30340;&#27867;&#21270;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#26469;&#32531;&#35299;&#38543;&#26426;&#23631;&#34109;&#25152;&#24341;&#36215;&#30340;&#37325;&#35201;&#20449;&#24687;&#20002;&#22833;&#65292;&#24182;&#21033;&#29992;&#22810;&#23545;&#27604;&#23398;&#20064;&#25429;&#25417;&#25200;&#21160;&#26399;&#38388;&#30340;&#38544;&#21547;&#27169;&#24335;&#20449;&#24687;&#65292;&#20415;&#20110;&#23569;&#37327;&#26679;&#26412;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#21517;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#27604;SOTA&#26041;&#27861;&#39640;&#20986;1.20\%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The burgeoning capabilities of large language models (LLMs) have raised growing concerns about abuse. DetectGPT, a zero-shot metric-based unsupervised machine-generated text detector, first introduces perturbation and shows great performance improvement. However, DetectGPT's random perturbation strategy might introduce noise, limiting the distinguishability and further performance improvements. Moreover, its logit regression module relies on setting the threshold, which harms the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel detector, \modelname{}, which uses selective strategy perturbation to relieve the important information loss caused by random masking, and multi-pair contrastive learning to capture the implicit pattern information during perturbation, facilitating few-shot performance. The experiments show that \modelname{} outperforms the SOTA method by 1.20\% in accuracy on average on four public datasets. We further analyze th
&lt;/p&gt;</description></item><item><title>LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.00024</link><description>&lt;p&gt;
LLaMA&#21644;ChatGPT&#23884;&#20837;&#22312;&#20998;&#23376;&#23884;&#20837;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00024
&lt;/p&gt;
&lt;p&gt;
LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#37322;Simplified Molecular Input Line Entry System (SMILES)&#26041;&#38754;&#12290;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;SMILES&#23383;&#31526;&#20018;&#35299;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20026;&#29702;&#35299;&#21270;&#23398;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#22312;&#23884;&#20837;SMILES&#23383;&#31526;&#20018;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#65306;&#20998;&#23376;&#24615;&#36136;&#65288;MP&#65289;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#65292;&#36825;&#22312;&#33647;&#29289;&#24320;&#21457;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;LLaMA&#29983;&#25104;&#30340;SMILES&#23884;&#20837;&#22312;MP&#21644;DDI&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;LLaMA&#30340;SMILES&#23884;&#20837;&#22312;&#36825;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#32467;&#35770;&#65306;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#24212;&#29992;LLMs&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;SMILES&#36827;&#34892;&#23884;&#20837;&#26041;&#38754;&#65292;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25512;&#29702;&#26463;&#25628;&#32034;&#65288;DBS&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#38142;&#24335;&#24605;&#32500;&#21644;&#28436;&#32462;&#25512;&#29702;&#19982;&#36880;&#27493;&#26463;&#25628;&#32034;&#26080;&#32541;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#39564;&#35777;&#22120;&#26469;&#20943;&#23569;&#38169;&#35823;&#30340;&#32047;&#31215;&#65292;&#24182;&#36890;&#36807;&#21487;&#25193;&#23637;&#21644;&#26080;&#38656;&#20154;&#24037;&#21171;&#21160;&#30340;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#25552;&#21319;&#27169;&#22411;&#30340;&#39564;&#35777;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17686</link><description>&lt;p&gt;
&#25512;&#29702;&#26463;&#25628;&#32034;&#65306;&#20026;&#38142;&#24335;&#24605;&#32500;&#25512;&#26029;&#23547;&#25214;&#21487;&#25512;&#23548;&#30340;&#29702;&#30001;
&lt;/p&gt;
&lt;p&gt;
Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25512;&#29702;&#26463;&#25628;&#32034;&#65288;DBS&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#38142;&#24335;&#24605;&#32500;&#21644;&#28436;&#32462;&#25512;&#29702;&#19982;&#36880;&#27493;&#26463;&#25628;&#32034;&#26080;&#32541;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#39564;&#35777;&#22120;&#26469;&#20943;&#23569;&#38169;&#35823;&#30340;&#32047;&#31215;&#65292;&#24182;&#36890;&#36807;&#21487;&#25193;&#23637;&#21644;&#26080;&#38656;&#20154;&#24037;&#21171;&#21160;&#30340;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#25552;&#21319;&#27169;&#22411;&#30340;&#39564;&#35777;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#26410;&#33021;&#35299;&#20915;&#20013;&#38388;&#27493;&#39588;&#30340;&#25512;&#29702;&#38169;&#35823;&#38382;&#39064;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#32047;&#31215;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25512;&#29702;&#26463;&#25628;&#32034;&#65288;DBS&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#38142;&#24335;&#24605;&#32500;&#21644;&#28436;&#32462;&#25512;&#29702;&#19982;&#36880;&#27493;&#26463;&#25628;&#32034;&#26080;&#32541;&#38598;&#25104;&#21040;LLMs&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#19968;&#20010;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#39564;&#35777;&#25512;&#29702;&#27493;&#39588;&#21450;&#20854;&#21069;&#25552;&#30340;&#21487;&#25512;&#23548;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#38169;&#35823;&#30340;&#32047;&#31215;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#26080;&#38656;&#20154;&#24037;&#21171;&#21160;&#30340;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#65292;&#26469;&#22686;&#24378;&#25105;&#20204;&#27169;&#22411;&#30340;&#39564;&#35777;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#35268;&#27169;&#30340;LLMs&#65288;7B&#12289;13B&#12289;70B&#21644;ChatGPT&#65289;&#30340;&#22522;&#30784;&#24615;&#33021;&#65292;&#22312;3&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#22330;&#26223;&#65288;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#65289;&#30340;8&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24369;&#21040;&#24378;&#30772;&#35299;&#25915;&#20987;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;&#36739;&#23567;&#30340;&#19981;&#23433;&#20840;/&#23545;&#40784;LLMs&#25351;&#23548;&#23545;&#26174;&#33879;&#36739;&#22823;&#30340;&#23545;&#40784;LLMs&#36827;&#34892;&#30772;&#35299;&#65292;&#19982;&#35299;&#30721;&#36739;&#22823;&#30340;LLMs&#30456;&#27604;&#65292;&#20854;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2401.17256</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#21040;&#24378;&#30772;&#35299;
&lt;/p&gt;
&lt;p&gt;
Weak-to-Strong Jailbreaking on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17256
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24369;&#21040;&#24378;&#30772;&#35299;&#25915;&#20987;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;&#36739;&#23567;&#30340;&#19981;&#23433;&#20840;/&#23545;&#40784;LLMs&#25351;&#23548;&#23545;&#26174;&#33879;&#36739;&#22823;&#30340;&#23545;&#40784;LLMs&#36827;&#34892;&#30772;&#35299;&#65292;&#19982;&#35299;&#30721;&#36739;&#22823;&#30340;LLMs&#30456;&#27604;&#65292;&#20854;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#32418;&#38431;&#27979;&#35797;&#25253;&#21578;&#34920;&#26126;&#65292;&#36825;&#20123;&#32463;&#36807;&#31934;&#24515;&#23545;&#40784;&#30340;LLMs&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#24615;&#25552;&#31034;&#12289;&#35843;&#20248;&#25110;&#35299;&#30721;&#36827;&#34892;&#30772;&#35299;&#12290;&#22312;&#35843;&#26597;&#23545;&#40784;LLMs&#30340;&#30772;&#35299;&#28431;&#27934;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30772;&#35299;&#21644;&#23545;&#40784;&#27169;&#22411;&#30340;&#35299;&#30721;&#20998;&#24067;&#20165;&#22312;&#21021;&#22987;&#29983;&#25104;&#20013;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24369;&#21040;&#24378;&#30772;&#35299;&#25915;&#20987;&#65292;&#25932;&#23545;&#26041;&#21487;&#20197;&#21033;&#29992;&#36739;&#23567;&#30340;&#19981;&#23433;&#20840;/&#23545;&#40784;LLMs&#65288;&#20363;&#22914;7B&#65289;&#25351;&#23548;&#23545;&#26174;&#33879;&#36739;&#22823;&#30340;&#23545;&#40784;LLMs&#65288;&#20363;&#22914;70B&#65289;&#36827;&#34892;&#30772;&#35299;&#12290;&#35201;&#36827;&#34892;&#30772;&#35299;&#65292;&#21482;&#38656;&#39069;&#22806;&#35299;&#30721;&#20004;&#20010;&#36739;&#23567;&#30340;LLMs&#19968;&#27425;&#65292;&#19982;&#35299;&#30721;&#36739;&#22823;&#30340;LLMs&#30456;&#27604;&#65292;&#20854;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#36739;&#23567;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#21516;&#32452;&#32455;&#30340;&#20116;&#20010;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#31181;&#20197;&#21069;&#26410;&#27880;&#24847;&#21040;&#20294;&#39640;&#25928;&#30340;&#30772;&#35299;&#26041;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although significant efforts have been dedicated to aligning large language models (LLMs), red-teaming reports suggest that these carefully aligned LLMs could still be jailbroken through adversarial prompts, tuning, or decoding. Upon examining the jailbreaking vulnerability of aligned LLMs, we observe that the decoding distributions of jailbroken and aligned models differ only in the initial generations. This observation motivates us to propose the weak-to-strong jailbreaking attack, where adversaries can utilize smaller unsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantly larger aligned LLMs (e.g., 70B). To jailbreak, one only needs to additionally decode two smaller LLMs once, which involves minimal computation and latency compared to decoding the larger LLMs. The efficacy of this attack is demonstrated through experiments conducted on five models from three different organizations. Our study reveals a previously unnoticed yet efficient way of jailbreaking, expo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16332</link><description>&lt;p&gt;
&#23545;&#40784;&#21644;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tradeoffs Between Alignment and Helpfulness in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#22686;&#24378;&#26399;&#26395;&#34892;&#20026;&#21644;&#25233;&#21046;&#38750;&#26399;&#26395;&#34892;&#20026;&#65292;&#23454;&#29616;&#20154;&#31867;&#19982;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#23433;&#20840;&#20132;&#20114;&#12290;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#25110;&#25554;&#20837;&#39044;&#35774;&#30340;&#23545;&#40784;&#25552;&#31034;&#26469;&#23454;&#29616;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#21518;&#30340;&#34920;&#31034;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#34920;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#34920;&#31034;&#24037;&#31243;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#38477;&#20302;&#31038;&#20250;&#20559;&#35265;&#31561;&#23545;&#40784;&#23548;&#21521;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#22686;&#30410;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#27169;&#22411;&#25191;&#34892;&#22522;&#26412;&#20219;&#21153;&#33021;&#21147;&#30340;&#38477;&#20302;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#36890;&#24120;&#20250;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TPRL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#26469;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#20998;&#24067;&#22833;&#30495;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#26799;&#24230;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;Mutual Implication&#20998;&#25968;&#20445;&#25345;&#21407;&#22987;&#25991;&#26412;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2401.11373</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#30340;&#33609;&#22534;&#20013;&#25214;&#21040;&#38024;&#65306;&#19968;&#31181;&#38024;&#23545;&#26368;&#23567;&#20998;&#24067;&#22833;&#30495;&#30340;&#36793;&#32536;&#24773;&#20917;&#30340;&#30446;&#26631;&#25913;&#20889;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing Approach For Uncovering Edge Cases with Minimal Distribution Distortion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TPRL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#26469;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#20998;&#24067;&#22833;&#30495;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#26799;&#24230;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;Mutual Implication&#20998;&#25968;&#20445;&#25345;&#21407;&#22987;&#25991;&#26412;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35821;&#35328;&#27169;&#22411;(LMs)&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#25239;&#26679;&#26412;&#21033;&#29992;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;&#34429;&#28982;&#36825;&#20123;&#21464;&#21270;&#23545;&#36755;&#20837;&#26679;&#26412;&#30340;&#35821;&#20041;&#26469;&#35828;&#20284;&#20046;&#24494;&#19981;&#36275;&#36947;&#65292;&#20294;&#21364;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Targeted Paraphrasing via RL (TPRL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#29983;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#26368;&#21487;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;TPRL&#21033;&#29992;FLAN T5&#20316;&#20026;&#29983;&#25104;&#22120;&#65292;&#24182;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#26799;&#24230;&#26469;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;TPRL&#30340;&#22870;&#21169;&#22522;&#20110;&#20998;&#31867;&#22120;&#20013;&#24341;&#21457;&#30340;&#22256;&#24785;&#31243;&#24230;&#65292;&#36890;&#36807;Mutual Implication&#20998;&#25968;&#20445;&#30041;&#21407;&#22987;&#25991;&#26412;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;TPRL&#21457;&#29616;&#33258;&#28982;&#23545;&#25239;&#25915;&#20987;&#21644;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks against language models(LMs) are a significant concern. In particular, adversarial samples exploit the model's sensitivity to small input changes. While these changes appear insignificant on the semantics of the input sample, they result in significant decay in model performance. In this paper, we propose Targeted Paraphrasing via RL (TPRL), an approach to automatically learn a policy to generate challenging samples that most likely improve the model's performance. TPRL leverages FLAN T5, a language model, as a generator and employs a self learned policy using a proximal policy gradient to generate the adversarial examples automatically. TPRL's reward is based on the confusion induced in the classifier, preserving the original text meaning through a Mutual Implication score. We demonstrate and evaluate TPRL's effectiveness in discovering natural adversarial attacks and improving model performance through extensive experiments on four diverse NLP classification tasks
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;AI&#20316;&#20026;&#25506;&#32034;&#12290;&#23427;&#24378;&#35843;&#36890;&#36807;&#21019;&#24314;&#21644;&#30740;&#31350;&#26234;&#33021;&#31995;&#32479;&#26469;&#25581;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#26234;&#33021;&#24418;&#24335;&#19981;&#21516;&#30340;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#12290;&#35770;&#25991;&#36890;&#36807;&#35752;&#35770;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35828;&#26126;&#20102;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2401.07964</link><description>&lt;p&gt;
AI&#20316;&#20026;&#25506;&#32034;&#65306;&#23548;&#33322;&#26234;&#33021;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
AI-as-exploration: Navigating intelligence space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07964
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;AI&#20316;&#20026;&#25506;&#32034;&#12290;&#23427;&#24378;&#35843;&#36890;&#36807;&#21019;&#24314;&#21644;&#30740;&#31350;&#26234;&#33021;&#31995;&#32479;&#26469;&#25581;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#26234;&#33021;&#24418;&#24335;&#19981;&#21516;&#30340;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#12290;&#35770;&#25991;&#36890;&#36807;&#35752;&#35770;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35828;&#26126;&#20102;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#25317;&#26377;&#35768;&#22810;&#29983;&#21629;&#30340;&#39046;&#22495;&#65292;&#36825;&#20010;&#26415;&#35821;&#24050;&#32463;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#31185;&#23398;&#21644;&#21830;&#19994;&#21162;&#21147;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#38416;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#21313;&#20998;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;&#8220;AI&#20316;&#20026;&#25506;&#32034;&#8221;&#12290;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#21019;&#24314;&#21644;&#30740;&#31350;&#33021;&#22815;&#25581;&#31034;&#26234;&#33021;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#30340;&#31995;&#32479;&#65292;&#36825;&#20123;&#27169;&#22359;&#21487;&#33021;&#19981;&#21516;&#20110;&#25105;&#20204;&#29087;&#24713;&#30340;&#20154;&#31867;&#21644;&#21160;&#29289;&#26234;&#33021;&#24418;&#24335;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#35748;&#20026;&#20154;&#24037;&#26234;&#33021;&#26159;&#25506;&#32034;&#26234;&#33021;&#31354;&#38388;&#65292;&#21363;&#21487;&#33021;&#30340;&#26234;&#33021;&#31995;&#32479;&#31354;&#38388;&#65292;&#30340;&#26368;&#20339;&#24037;&#20855;&#20043;&#19968;&#12290;&#25105;&#36890;&#36807;&#20851;&#27880;&#19968;&#20010;&#20855;&#20307;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21363;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#26469;&#35828;&#26126;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#25105;&#23637;&#31034;&#20102;&#23613;&#31649;&#21518;&#32773;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#24456;&#21487;&#33021;&#20197;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence is a field that lives many lives, and the term has come to encompass a motley collection of scientific and commercial endeavours. In this paper, I articulate the contours of a rather neglected but central scientific role that AI has to play, which I dub `AI-as-exploration'.The basic thrust of AI-as-exploration is that of creating and studying systems that can reveal candidate building blocks of intelligence that may differ from the forms of human and animal intelligence we are familiar with. In other words, I suggest that AI is one of the best tools we have for exploring intelligence space, namely the space of possible intelligent systems. I illustrate the value of AI-as-exploration by focusing on a specific case study, i.e., recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. I show that the latter, despite showing human-level accuracy in such a task, most probably solve it in ways radically different, but no 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#29702;&#35299;&#25968;&#23383;&#65292;&#21487;&#20197;&#36890;&#36807;&#21387;&#32553;&#21644;&#32534;&#30721;&#30340;&#26041;&#24335;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2401.03735</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#29702;&#35299;&#25968;&#23383;
&lt;/p&gt;
&lt;p&gt;
Language Models Understand Numbers, at Least Partially
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#29702;&#35299;&#25968;&#23383;&#65292;&#21487;&#20197;&#36890;&#36807;&#21387;&#32553;&#21644;&#32534;&#30721;&#30340;&#26041;&#24335;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#19981;&#36879;&#26126;&#30340;&#20869;&#37096;&#26426;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#25968;&#23383;&#65292;&#25968;&#23398;&#20013;&#30340;&#22522;&#26412;&#20803;&#32032;&#12290;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;LLMs&#24212;&#35813;&#33021;&#22815;&#22312;&#20854;&#38544;&#34255;&#29366;&#24577;&#20013;&#21387;&#32553;&#25968;&#23383;&#20197;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#21152;&#27861;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#32447;&#24615;&#25506;&#27979;&#22120;&#20174;&#38544;&#34255;&#29366;&#24577;&#20013;&#35835;&#21462;&#36755;&#20837;&#25968;&#23383;&#12290;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;LLMs&#20013;&#23384;&#22312;&#21387;&#32553;&#30340;&#25968;&#23383;&#12290;&#28982;&#32780;&#65292;&#31934;&#30830;&#37325;&#24314;&#21407;&#22987;&#25968;&#23383;&#26159;&#22256;&#38590;&#30340;&#65292;&#34920;&#26126;&#21387;&#32553;&#36807;&#31243;&#21487;&#33021;&#19981;&#26159;&#26080;&#25439;&#30340;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#21033;&#29992;&#32534;&#30721;&#30340;&#25968;&#23383;&#26469;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#65292;&#24182;&#19988;&#35745;&#31639;&#33021;&#21147;&#38543;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#25968;&#23383;&#19978;&#23637;&#29616;&#20986;&#37096;&#20998;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited impressive competence in various tasks, but their opaque internal mechanisms hinder their use in mathematical problems. In this paper, we study a fundamental question: whether language models understand numbers, a basic element in math. Based on an assumption that LLMs should be capable of compressing numbers in their hidden states to solve mathematical problems, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of compressed numbers in LLMs. However, it is difficult to precisely reconstruct the original numbers, indicating that the compression process may not be lossless. Further experiments show that LLMs can utilize encoded numbers to perform arithmetic computations, and the computational ability scales up with the model size. Our preliminary research suggests that LLMs exhibit a partial understanding of number
&lt;/p&gt;</description></item><item><title>&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13933</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Structured Probabilistic Coding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13933
&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#65292;&#29992;&#20110;&#20174;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#36755;&#20837;&#20013;&#23398;&#20064;&#32039;&#20945;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;SPC&#26159;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#27010;&#29575;&#32534;&#30721;&#25216;&#26415;&#65292;&#20855;&#26377;&#26469;&#33258;&#30446;&#26631;&#31354;&#38388;&#30340;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27010;&#29575;&#32534;&#30721;&#22312;&#19968;&#20010;&#27169;&#22359;&#20013;&#21516;&#26102;&#36827;&#34892;&#20449;&#24687;&#32534;&#30721;&#21644;&#20219;&#21153;&#39044;&#27979;&#65292;&#20197;&#26356;&#20805;&#20998;&#22320;&#21033;&#29992;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#26377;&#25928;&#20449;&#24687;&#12290;&#23427;&#20351;&#29992;&#36755;&#20986;&#31354;&#38388;&#30340;&#21464;&#20998;&#25512;&#26029;&#26469;&#20943;&#23569;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#25511;&#21046;&#27010;&#29575;&#34920;&#31034;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#65292;&#20197;&#20419;&#36827;&#31867;&#21035;&#20043;&#38388;&#30340;&#22343;&#21248;&#24615;&#12290;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#65292;SPC&#21487;&#20197;&#20445;&#25345;&#28508;&#22312;&#32534;&#30721;&#30340;&#39640;&#26031;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23545;&#35805;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KG&#25512;&#29702;&#30340;LLM&#22522;&#20934;&#20195;&#29702;&#65288;LLM-ARK&#65289;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;&#20840;&#25991;&#29615;&#22659;&#25552;&#31034;&#26469;&#23454;&#29616;&#31934;&#30830;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;KG&#36335;&#24452;&#39044;&#27979;&#65292;&#24182;&#37319;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2312.11282</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#22686;&#24378;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23545;&#35805;&#25512;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23545;&#35805;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KG&#25512;&#29702;&#30340;LLM&#22522;&#20934;&#20195;&#29702;&#65288;LLM-ARK&#65289;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;&#20840;&#25991;&#29615;&#22659;&#25552;&#31034;&#26469;&#23454;&#29616;&#31934;&#30830;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;KG&#36335;&#24452;&#39044;&#27979;&#65292;&#24182;&#37319;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#24471;&#30410;&#20110;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#25163;&#21160;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLM&#65288;GPT-4&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;KG&#29615;&#22659;&#24847;&#35782;&#21644;&#24320;&#21457;&#26377;&#25928;&#30340;&#20013;&#38388;&#25512;&#29702;&#38454;&#27573;&#20248;&#21270;&#26426;&#21046;&#30340;&#22256;&#38590;&#65292;LLM&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;LLM-ARK&#65292;&#19968;&#20010;&#22522;&#20110;KG&#25512;&#29702;&#30340;LLM&#22522;&#20934;&#20195;&#29702;&#65292;&#26088;&#22312;&#25552;&#20379;&#31934;&#30830;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;KG&#36335;&#24452;&#39044;&#27979;&#12290;LLM-ARK&#21033;&#29992;&#20840;&#25991;&#29615;&#22659;&#65288;FTE&#65289;&#25552;&#31034;&#26469;&#21560;&#25910;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#20013;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;KG&#19978;&#30340;&#22810;&#36339;&#25512;&#29702;&#25361;&#25112;&#37325;&#26032;&#26694;&#23450;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#12290;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) has been catalyzed by advancements in pre-training techniques. These models have demonstrated robust reasoning capabilities through manually designed prompts. In this work, we evaluate the conversational reasoning capabilities of the current state-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the performance of LLMs is constrained due to a lack of KG environment awareness and the difficulties in developing effective optimization mechanisms for intermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG reasoning agent designed to deliver precise and adaptable predictions on KG paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate state information within each reasoning step. We reframe the challenge of multi-hop reasoning on the KG as a sequential decision-making task. Utilizing the Proximal Policy Optimization (PPO) online policy gradient reinforcement learning algorithm, our model i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#35838;&#22530;&#20154;&#26426;&#20132;&#20114;&#20013;&#21033;&#29992;&#38750;&#35821;&#35328;&#34892;&#20026;&#21644;&#31038;&#20132;&#27880;&#35270;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#21551;&#21457;&#30340;&#31038;&#20132;&#27880;&#35270;&#27169;&#22411;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#26426;&#22120;&#20154;&#35748;&#30693;&#26550;&#26500;&#20013;&#23454;&#29616;&#26356;&#21152;&#27969;&#30021;&#30340;&#31038;&#20132;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2312.06825</link><description>&lt;p&gt;
&#22312;&#35838;&#22530;&#20154;&#26426;&#20132;&#20114;&#20013;&#21033;&#29992;&#38750;&#35821;&#35328;&#34892;&#20026;&#21644;&#31038;&#20132;&#27880;&#35270;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Utilization of Non-verbal Behaviour and Social Gaze in Classroom Human-Robot Interaction Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#35838;&#22530;&#20154;&#26426;&#20132;&#20114;&#20013;&#21033;&#29992;&#38750;&#35821;&#35328;&#34892;&#20026;&#21644;&#31038;&#20132;&#27880;&#35270;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#21551;&#21457;&#30340;&#31038;&#20132;&#27880;&#35270;&#27169;&#22411;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#26426;&#22120;&#20154;&#35748;&#30693;&#26550;&#26500;&#20013;&#23454;&#29616;&#26356;&#21152;&#27969;&#30021;&#30340;&#31038;&#20132;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25688;&#35201;&#25506;&#35752;&#20102;&#35838;&#22530;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#22330;&#26223;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#20154;&#31867;&#21551;&#21457;&#30340;&#31038;&#20132;&#27880;&#35270;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#35748;&#30693;&#26550;&#26500;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20419;&#36827;&#26356;&#21152;&#27969;&#30021;&#30340;&#31038;&#20132;&#20114;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#30740;&#31350;&#20013;&#25506;&#32034;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#65292;&#28982;&#21518;&#25551;&#36848;&#20102;&#25105;&#20204;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#31038;&#20132;&#27880;&#35270;&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#35838;&#22530;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#20013;&#21033;&#29992;&#36825;&#31181;&#20851;&#27880;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#21363;&#23558;&#36827;&#34892;&#30340;&#20851;&#20110;&#36825;&#20010;&#31038;&#20132;&#27880;&#35270;&#27169;&#22411;&#30340;&#30740;&#31350;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This abstract explores classroom Human-Robot Interaction (HRI) scenarios with an emphasis on the adaptation of human-inspired social gaze models in robot cognitive architecture to facilitate a more seamless social interaction. First, we detail the HRI scenarios explored by us in our studies followed by a description of the social gaze model utilized for our research. We highlight the advantages of utilizing such an attentional model in classroom HRI scenarios. We also detail the intended goals of our upcoming study involving this social gaze model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18751</link><description>&lt;p&gt;
&#22312;Web&#19978;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#39034;&#24207;&#20219;&#21153;&#32452;&#21512;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;(LMA)&#20316;&#20026;&#19968;&#31181;&#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21644;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#22312;&#36890;&#24120;&#28041;&#21450;&#20219;&#21153;&#32452;&#21512;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21483;&#20570;CompWoB-&#21453;&#26144;&#26356;&#29616;&#23454;&#20551;&#35774;&#30340;50&#20010;&#32452;&#21512;&#24615;&#32593;&#31449;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#25552;&#31034;&#22411;LMA&#65288;gpt-3.5-turbo&#25110;gpt-4&#65289;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;94.0&#65285;&#30340;&#24179;&#22343;&#25104;&#21151;&#29575;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#38477;&#33267;24.9&#65285;&#30340;&#25104;&#21151;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21482;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#36716;&#31227;&#24615;LMA&#34920;&#29616;&#20986;&#26356;&#23567;&#30340;&#27867;&#21270;&#24615;&#24046;&#36317;&#65292;&#20174;85.4&#65285;&#19979;&#38477;&#21040;54.8&#65285;&#12290;&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;HTML-T5++&#65292;&#22312;MiniWoB&#19978;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65288;95.2&#65285;&#65289;&#65292;&#24182;&#22312;CompWoB&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#38646;-shot&#24615;&#33021;&#65288;61.5%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChatTraffic&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#24182;&#24471;&#21040;&#19982;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#19968;&#33268;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2311.16203</link><description>&lt;p&gt;
ChatTraffic&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatTraffic: Text-to-Traffic Generation via Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChatTraffic&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#24182;&#24471;&#21040;&#19982;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#19968;&#33268;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#22522;&#30784;&#20043;&#19968;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#21482;&#20381;&#36182;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26469;&#39044;&#27979;&#20132;&#36890;&#36235;&#21183;&#65292;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#23545;&#24322;&#24120;&#20107;&#20214;&#19981;&#25935;&#24863;&#65307;2&#65289;&#22312;&#38271;&#26399;&#39044;&#27979;&#26041;&#38754;&#24615;&#33021;&#26377;&#38480;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#20132;&#36890;&#29983;&#25104;&#65292;&#23558;&#27492;&#20219;&#21153;&#21629;&#21517;&#20026;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#65288;TTG&#65289;&#12290;TTG&#20219;&#21153;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#23558;&#25991;&#26412;&#19982;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#20132;&#36890;&#25968;&#25454;&#30456;&#20851;&#32852;&#65292;&#29992;&#20110;&#29983;&#25104;&#20132;&#36890;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatTraffic&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;&#20132;&#36890;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#20445;&#35777;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25552;&#21462;&#20132;&#36890;&#25968;&#25454;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;...
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) limited performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation, and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first diffusion model for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a diffusion model with the Graph Convolutional Network (GCN) to extract spatial correlations of traffic data. In addition, we construct a large dataset containing t
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#38750;&#24120;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;(QDIT)&#65292;&#36890;&#36807;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.14736</link><description>&lt;p&gt;
&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Data Diversity Matters for Robust Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14736
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#38750;&#24120;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;(QDIT)&#65292;&#36890;&#36807;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#31934;&#36873;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#22256;&#38590;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20381;&#36182;&#20110;&#25163;&#21160;&#31934;&#36873;&#25110;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#12290;&#33258;&#21160;&#25968;&#25454;&#31934;&#36873;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#20173;&#19981;&#28165;&#26970;&#22914;&#20309;&#20026;&#25351;&#20196;&#35843;&#25972;&#23450;&#20041;&#22810;&#26679;&#24615;&#65292;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#22914;&#20309;&#30456;&#20114;&#20851;&#32852;&#65292;&#20197;&#21450;&#22914;&#20309;&#20248;&#21270;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36136;&#37327;-&#22810;&#26679;&#24615;&#25351;&#20196;&#35843;&#25972;(QDIT)&#12290;QDIT&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#28145;&#20837;&#30740;&#31350;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20174;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#65306;(1)&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#33258;&#28982;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;(2)&#22686;&#21152;&#25968;&#25454;&#22810;&#26679;&#24615;&#26174;&#33879;&#25552;&#39640;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25351;&#20196;&#36319;&#38543;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that by curating high quality and diverse instruction tuning datasets, we can significantly improve instruction-following capabilities. However, creating such datasets is difficult and most works rely on manual curation or proprietary language models. Automatic data curation is difficult as it is still not clear how we can define diversity for instruction tuning, how diversity and quality depend on one other, and how we can optimize dataset quality and diversity. To resolve these issue, we propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple method to simultaneously control dataset diversity and quality, allowing us to conduct an in-depth study on the effect of diversity and quality on instruction tuning performance. From this study we draw two key insights (1) there is a natural tradeoff between data diversity and quality and (2) increasing data diversity significantly improves the worst case instruction following perform
&lt;/p&gt;</description></item><item><title>DURel&#27880;&#37322;&#24037;&#20855;&#26159;&#19968;&#20010;&#22312;&#32447;&#30340;&#12289;&#24320;&#28304;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#30340;&#27880;&#37322;&#23454;&#29616;&#20102;&#23545;&#21333;&#35789;&#20351;&#29992;&#20043;&#38388;&#30340;&#35821;&#20041;&#25509;&#36817;&#24230;&#30340;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#32858;&#31867;&#21644;&#35821;&#20041;&#21464;&#21270;&#30340;&#20998;&#26512;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.12664</link><description>&lt;p&gt;
DURel&#27880;&#37322;&#24037;&#20855;&#65306;&#20154;&#31867;&#21644;&#35745;&#31639;&#27979;&#37327;&#35821;&#20041;&#25509;&#36817;&#24230;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#35821;&#20041;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12664
&lt;/p&gt;
&lt;p&gt;
DURel&#27880;&#37322;&#24037;&#20855;&#26159;&#19968;&#20010;&#22312;&#32447;&#30340;&#12289;&#24320;&#28304;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#30340;&#27880;&#37322;&#23454;&#29616;&#20102;&#23545;&#21333;&#35789;&#20351;&#29992;&#20043;&#38388;&#30340;&#35821;&#20041;&#25509;&#36817;&#24230;&#30340;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#32858;&#31867;&#21644;&#35821;&#20041;&#21464;&#21270;&#30340;&#20998;&#26512;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DURel&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#23454;&#29616;&#20102;&#22312;&#22312;&#32447;&#12289;&#24320;&#28304;&#30028;&#38754;&#20013;&#23545;&#21333;&#35789;&#20351;&#29992;&#20043;&#38388;&#30340;&#35821;&#20041;&#25509;&#36817;&#24230;&#36827;&#34892;&#27880;&#37322;&#12290;&#35813;&#24037;&#20855;&#25903;&#25345;&#26631;&#20934;&#21270;&#30340;&#20154;&#31867;&#27880;&#37322;&#21644;&#35745;&#31639;&#26426;&#27880;&#37322;&#65292;&#21033;&#29992;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#35789;&#27169;&#22411;&#30340;&#36827;&#23637;&#36827;&#34892;&#26500;&#24314;&#12290;&#27880;&#37322;&#32773;&#30340;&#21028;&#26029;&#36890;&#36807;&#33258;&#21160;&#22270;&#24418;&#32858;&#31867;&#25216;&#26415;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36827;&#34892;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;&#36825;&#20801;&#35768;&#36890;&#36807;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#24494;&#20219;&#21153;&#21028;&#26029;&#26469;&#27979;&#37327;&#21333;&#35789;&#35789;&#20041;&#65292;&#24182;&#19988;&#38656;&#35201;&#26368;&#23567;&#30340;&#20934;&#22791;&#24037;&#20316;&#12290;&#35813;&#24037;&#20855;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#21151;&#33021;&#65292;&#20197;&#27604;&#36739;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#30830;&#20445;&#33719;&#24471;&#21028;&#26029;&#30340;&#20027;&#35266;&#24615;&#65292;&#24182;&#35745;&#31639;&#24635;&#32467;&#32479;&#35745;&#25968;&#25454;&#65292;&#20197;&#25581;&#31034;&#35789;&#20041;&#39057;&#29575;&#20998;&#24067;&#12289;&#35821;&#20041;&#21464;&#24322;&#25110;&#35789;&#20041;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the DURel tool that implements the annotation of semantic proximity between uses of words into an online, open source interface. The tool supports standardized human annotation as well as computational annotation, building on recent advances with Word-in-Context models. Annotator judgments are clustered with automatic graph clustering techniques and visualized for analysis. This allows to measure word senses with simple and intuitive micro-task judgments between use pairs, requiring minimal preparation efforts. The tool offers additional functionalities to compare the agreement between annotators to guarantee the inter-subjectivity of the obtained judgments and to calculate summary statistics giving insights into sense frequency distributions, semantic variation or changes of senses over time.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;&#27973;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;"&#26080;&#27880;&#24847;&#21147;&#30340;Transformers"&#21487;&#20197;&#19982;&#21407;&#22987;&#26550;&#26500;&#30340;&#24615;&#33021;&#23218;&#32654;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#31616;&#21270;&#22797;&#26434;&#26550;&#26500;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.10642</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#27880;&#24847;&#21147;&#65306;&#25506;&#32034;&#23558;&#27973;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;Transformers&#20013;&#27880;&#24847;&#21147;&#23618;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;&#27973;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;"&#26080;&#27880;&#24847;&#21147;&#30340;Transformers"&#21487;&#20197;&#19982;&#21407;&#22987;&#26550;&#26500;&#30340;&#24615;&#33021;&#23218;&#32654;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#31616;&#21270;&#22797;&#26434;&#26550;&#26500;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#26631;&#20934;&#30340;&#27973;&#23618;&#21069;&#39304;&#32593;&#32476;&#26469;&#27169;&#20223;Transformer&#27169;&#22411;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#23558;Transformer&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#26367;&#25442;&#20026;&#31616;&#21333;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;&#32452;&#20214;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;IWSLT2017&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#8220;&#26080;&#27880;&#24847;&#21147;&#30340;Transformers&#8221;&#21487;&#20197;&#19982;&#21407;&#22987;&#26550;&#26500;&#30340;&#24615;&#33021;&#23218;&#32654;&#12290;&#36890;&#36807;&#20005;&#35880;&#30340;&#23454;&#39564;&#21644;&#19981;&#21516;&#26367;&#20195;&#32593;&#32476;&#31867;&#22411;&#21644;&#22823;&#23567;&#30340;&#23581;&#35797;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#25105;&#20204;&#26041;&#27861;&#21487;&#34892;&#24615;&#30340;&#35265;&#35299;&#12290;&#36825;&#19981;&#20165;&#25581;&#31034;&#20102;&#27973;&#23618;&#21069;&#39304;&#32593;&#32476;&#22312;&#27169;&#20223;&#27880;&#24847;&#21147;&#26426;&#21046;&#26041;&#38754;&#30340;&#36866;&#24212;&#24615;&#65292;&#32780;&#19988;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#31616;&#21270;&#24207;&#21015;&#20219;&#21153;&#30340;&#22797;&#26434;&#26550;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an analysis of the effectiveness of using standard shallow feed-forward networks to mimic the behavior of the attention mechanism in the original Transformer model, a state-of-the-art architecture for sequence-to-sequence tasks. We substitute key elements of the attention mechanism in the Transformer with simple feed-forward networks, trained using the original components via knowledge distillation. Our experiments, conducted on the IWSLT2017 dataset, reveal the capacity of these "attentionless Transformers" to rival the performance of the original architecture. Through rigorous ablation studies, and experimenting with various replacement network types and sizes, we offer insights that support the viability of our approach. This not only sheds light on the adaptability of shallow feed-forward networks in emulating attention mechanisms but also underscores their potential to streamline complex architectures for sequence-to-sequence tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09308</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#33041;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Divergences between Language Models and Human Brains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21644;&#20154;&#31867;&#26159;&#21542;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#22788;&#29702;&#35821;&#35328;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#26263;&#31034;&#32943;&#23450;&#65292;&#21457;&#29616;&#22823;&#33041;&#20449;&#21495;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20869;&#37096;&#34920;&#31034;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#32467;&#26524;&#34987;&#35748;&#20026;&#21453;&#26144;&#20102;LMs&#21644;&#20154;&#31867;&#22823;&#33041;&#20043;&#38388;&#30340;&#20849;&#20139;&#35745;&#31639;&#21407;&#29702;&#65292;&#20294;LMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#34920;&#31034;&#21644;&#20351;&#29992;&#19978;&#20063;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;LM&#34920;&#31034;&#21644;&#20154;&#31867;&#22823;&#33041;&#23545;&#35821;&#35328;&#30340;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;&#21463;&#35797;&#32773;&#38405;&#35835;&#21644;&#21548;&#21465;&#36848;&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#39046;&#22495;&#65292;&#21363;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#65292;&#36825;&#20123;&#39046;&#22495;&#22312;LMs&#20013;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#39046;&#22495;&#65292;&#24182;&#35777;&#26126;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve th
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.06233</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;: &#19968;&#31181;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27745;&#26579;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06233
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#24182;&#20272;&#35745;&#20854;&#25968;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#35270;&#20026;&#19968;&#31995;&#21015;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#27979;&#39564;&#24418;&#24335;&#65292;&#20854;&#20013;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#19977;&#20010;&#25200;&#21160;&#29256;&#26412;&#12290;&#36825;&#20123;&#21464;&#21270;&#20165;&#21253;&#25324;&#35789;&#32423;&#25200;&#21160;&#12290;&#29983;&#25104;&#30340;&#25200;&#21160;&#29256;&#26412;&#19982;&#21407;&#22987;&#23454;&#20363;&#19968;&#36215;&#24418;&#25104;DCQ&#20013;&#30340;&#36873;&#39033;&#65292;&#39069;&#22806;&#30340;&#36873;&#39033;&#36866;&#24212;&#20102;&#25552;&#20379;&#30340;&#36873;&#25321;&#37117;&#19981;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#37492;&#20110;&#22312;&#36873;&#25321;&#20043;&#38388;&#21807;&#19968;&#30340;&#21306;&#21035;&#20449;&#21495;&#26159;&#19982;&#21407;&#22987;&#23454;&#20363;&#30340;&#30830;&#20999;&#25514;&#36766;&#30456;&#20851;&#65292;&#22914;&#26524;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24050;&#32463;&#25509;&#35302;&#21040;&#21407;&#22987;&#23454;&#20363;&#65292;&#35821;&#35328;&#27169;&#22411;&#24403;&#34987;&#35201;&#27714;&#20174;&#36873;&#39033;&#20013;&#35782;&#21035;&#21407;&#22987;&#23454;&#20363;&#26102;&#65292;&#20542;&#21521;&#20110;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;--&#36825;&#26159;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#30340;&#29305;&#24615;&#12290;&#22312;&#20351;&#29992;GPT-4/3.5&#36827;&#34892;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23436;&#20840;&#32570;&#23569;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
&lt;/p&gt;</description></item><item><title>&#22312;&#31163;&#25955;&#21270;&#20196;&#29260;&#30340;ASR&#20013;&#65292;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#19981;&#38656;&#35201;&#20351;&#29992;&#25439;&#22833;&#36974;&#34109;&#12290;&#21462;&#32780;&#20195;&#20043;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#28369;&#26631;&#31614;&#33976;&#39311;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22312;&#35821;&#38899;&#20196;&#29260;&#19978;&#24212;&#29992;&#20102;&#24102;&#26377;&#24179;&#28369;&#26631;&#31614;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#25928;&#26524;&#20248;&#20110;&#25439;&#22833;&#36974;&#34109;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.04534</link><description>&lt;p&gt;
&#22312;&#20165;&#20351;&#29992;&#32534;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#65292;&#23545;&#31163;&#25955;&#21270;&#30340;&#20196;&#29260;ASR&#19981;&#38656;&#35201;&#20351;&#29992;&#25439;&#22833;&#36974;&#34109;
&lt;/p&gt;
&lt;p&gt;
Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04534
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#25955;&#21270;&#20196;&#29260;&#30340;ASR&#20013;&#65292;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#19981;&#38656;&#35201;&#20351;&#29992;&#25439;&#22833;&#36974;&#34109;&#12290;&#21462;&#32780;&#20195;&#20043;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#28369;&#26631;&#31614;&#33976;&#39311;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22312;&#35821;&#38899;&#20196;&#29260;&#19978;&#24212;&#29992;&#20102;&#24102;&#26377;&#24179;&#28369;&#26631;&#31614;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#25928;&#26524;&#20248;&#20110;&#25439;&#22833;&#36974;&#34109;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32479;&#19968;&#30340;&#35821;&#38899;-&#25991;&#26412;&#27169;&#22411;&#65292;&#20363;&#22914;SpeechGPT&#12289;VioLA&#21644;AudioPaLM&#65292;&#22312;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#35821;&#38899;&#20449;&#21495;&#31163;&#25955;&#21270;&#20026;&#20196;&#29260;&#65288;&#35821;&#38899;&#31163;&#25955;&#21270;&#65289;&#65292;&#24182;&#23545;&#25991;&#26412;&#21644;&#35821;&#38899;&#20196;&#29260;&#20351;&#29992;&#20849;&#20139;&#35789;&#27719;&#34920;&#12290;&#28982;&#21518;&#65292;&#22312;&#28151;&#21512;&#35821;&#38899;&#20219;&#21153;&#19978;&#35757;&#32451;&#21333;&#20010;&#21482;&#26377;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;ASR&#20219;&#21153;&#20013;&#30340;&#25439;&#22833;&#36974;&#34109;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#24573;&#30053;&#35821;&#38899;&#20196;&#29260;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#31867;&#20284;&#20110;&#25991;&#26412;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#23545;&#35821;&#38899;&#20196;&#29260;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#36755;&#20837;&#30340;&#35821;&#38899;&#20196;&#29260;&#24212;&#29992;&#20256;&#32479;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#24182;&#19981;&#33021;&#22987;&#32456;&#25913;&#21892;ASR&#24615;&#33021;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#25439;&#22833;&#36974;&#34109;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#24179;&#28369;&#26631;&#31614;&#33976;&#39311;&#65288;SLD&#65289;&#65292;&#23427;&#22312;&#35821;&#38899;&#20196;&#29260;&#19978;&#24212;&#29992;&#20102;&#24102;&#26377;&#24179;&#28369;&#26631;&#31614;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SLD&#26377;&#25928;&#22320;&#23545;&#35821;&#38899;&#20196;&#29260;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#32988;&#36807;&#20102;&#25439;&#22833;&#36974;&#34109;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;DARE&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#21442;&#25968;&#24182;&#23558;&#22810;&#20010;&#21516;&#28304;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#23454;&#29616;&#22810;&#20219;&#21153;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2311.03099</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23601;&#20687;&#36229;&#32423;&#39532;&#37324;&#22885;&#65306;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#20813;&#36153;&#21320;&#39184;
&lt;/p&gt;
&lt;p&gt;
Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;DARE&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#21442;&#25968;&#24182;&#23558;&#22810;&#20010;&#21516;&#28304;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;&#22823;&#37096;&#20998;&#21442;&#25968;&#24182;&#23454;&#29616;&#22810;&#20219;&#21153;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;(LMs)&#21487;&#20197;&#36890;&#36807;&#21560;&#25910;&#21516;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#33719;&#24471;&#26032;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#20351;&#29992;GPU&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;DARE&#26469;&#23558;&#22823;&#22810;&#25968;delta&#21442;&#25968;&#65288;&#21363;&#24494;&#35843;&#21644;&#39044;&#35757;&#32451;&#21442;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65289;&#35774;&#32622;&#20026;&#38646;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#30417;&#30563;&#24494;&#35843;(SFT) LMs&#30340;&#33021;&#21147;&#65292;DARE&#36890;&#36807;&#38543;&#26426;&#21024;&#38500;&#27604;&#29575;&#20026;p&#30340;delta&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;1/(1 - p)&#37325;&#26032;&#32553;&#25918;&#21097;&#20313;&#21442;&#25968;&#26469;&#36817;&#20284;&#21407;&#22987;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;DARE&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#25216;&#26415;&#26469;&#31232;&#30095;&#21270;&#22810;&#20010;SFT&#21516;&#28304;&#27169;&#22411;&#30340;delta&#21442;&#25968;&#65292;&#20197;&#20943;&#36731;&#21442;&#25968;&#24178;&#25200;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#34701;&#21512;&#23558;&#23427;&#20204;&#21512;&#24182;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20026;&#22522;&#30784;&#30340;LM&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;SFT delta&#21442;&#25968;&#20540;&#33539;&#22260;&#36890;&#24120;&#24456;&#23567;&#65288;&#22312;0.005&#20197;&#20869;&#65289;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#20887;&#20313;&#65292;DARE&#21487;&#20197;&#36731;&#26494;&#21024;&#38500;90%&#29978;&#33267;99%&#30340;&#21442;&#25968;&#12290;&#65288;2&#65289;DARE&#21487;&#20197;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;LM&#21512;&#24182;&#20026;&#19968;&#20010;LM&#65292;&#24182;&#26377;&#39550;&#39542;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio p And REscales the remaining ones by 1/(1 - p) to approximate the original embeddings. Then, we use DARE as a versatile plug-and-play technique to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.005) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them. (2) DARE can merge multiple task-specific LMs into one LM with dive
&lt;/p&gt;</description></item><item><title>PhoGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#36234;&#21335;&#35821;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#31995;&#21015;&#65292;&#20855;&#26377;40&#20159;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;PhoGPT-4B&#20197;&#21450;&#20854;&#32842;&#22825;&#21464;&#20307;PhoGPT-4B-Chat&#65292;&#23637;&#31034;&#20102;&#22312;&#36234;&#21335;&#35821;&#20219;&#21153;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;7&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.02945</link><description>&lt;p&gt;
PhoGPT: &#36234;&#21335;&#35821;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PhoGPT: Generative Pre-training for Vietnamese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02945
&lt;/p&gt;
&lt;p&gt;
PhoGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#36234;&#21335;&#35821;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#31995;&#21015;&#65292;&#20855;&#26377;40&#20159;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;PhoGPT-4B&#20197;&#21450;&#20854;&#32842;&#22825;&#21464;&#20307;PhoGPT-4B-Chat&#65292;&#23637;&#31034;&#20102;&#22312;&#36234;&#21335;&#35821;&#20219;&#21153;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;7&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#28304;&#20102;&#19968;&#20010;&#25317;&#26377;40&#20159;&#21442;&#25968;&#30340;&#26368;&#20808;&#36827;&#30340;&#36234;&#21335;&#35821;&#29983;&#25104;&#27169;&#22411;&#31995;&#21015;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#30784;&#30340;&#39044;&#35757;&#32451;&#21333;&#35821;&#27169;&#22411;PhoGPT-4B&#21644;&#20854;&#32842;&#22825;&#21464;&#20307;PhoGPT-4B-Chat&#12290;&#22522;&#30784;&#27169;&#22411;PhoGPT-4B&#26377;37&#20159;&#21442;&#25968;&#65292;&#20174;&#38646;&#24320;&#22987;&#22312;&#21253;&#21547;1020&#20159;&#26631;&#35760;&#30340;&#36234;&#21335;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#38271;&#24230;&#20026;8192&#30340;&#19978;&#19979;&#25991;&#65292;&#20351;&#29992;20480&#20010;&#26631;&#35760;&#31867;&#22411;&#30340;&#35789;&#27719;&#34920;&#12290;&#32842;&#22825;&#21464;&#20307;PhoGPT-4B-Chat&#26159;&#22312;70000&#20010;&#25351;&#23548;&#25552;&#31034;&#21644;&#22238;&#24212;&#20197;&#21450;&#39069;&#22806;&#30340;290000&#20010;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23545;PhoGPT-4B&#36827;&#34892;&#24494;&#35843;&#24471;&#21040;&#30340;&#27169;&#22411;&#36755;&#20986;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#27604;&#20043;&#21069;&#38381;&#28304;&#21644;&#24320;&#28304;&#30340;70&#20159;&#21442;&#25968;&#27169;&#22411;&#65292;&#23427;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;PhoGPT&#27169;&#22411;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#19979;&#36733;&#65306;https://github.com/VinAIResearch/PhoGPT
&lt;/p&gt;
&lt;p&gt;
We open-source a state-of-the-art 4B-parameter generative model series for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and its chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B parameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens, with an 8192 context length, employing a vocabulary of 20480 token types. The chat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning PhoGPT-4B on a dataset of 70K instructional prompts and their responses, along with an additional 290K conversations. We demonstrate its strong performance compared to previous closed-source and open-source 7B-parameter models. Our PhoGPT models are available at: https://github.com/VinAIResearch/PhoGPT
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#26469;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#36827;&#34892;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.20689</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#38169;&#35823;&#20013;&#20351;LLM&#25104;&#20026;&#26356;&#22909;&#30340;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Learning From Mistakes Makes LLM Better Reasoner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#26469;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#36827;&#34892;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#26159;&#21542;&#21487;&#20197;&#23398;&#20064;&#20174;&#38169;&#35823;&#20013;&#33719;&#30410;&#65288;LEMA&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#32771;&#34385;&#19968;&#20010;&#26410;&#33021;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#20154;&#31867;&#23398;&#29983;&#65292;&#20182;&#20250;&#20174;&#33258;&#24049;&#29359;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#24182;&#32416;&#27491;&#23427;&#12290;&#27169;&#20223;&#36825;&#31181;&#38169;&#35823;&#39537;&#21160;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;LEMA&#22312;LLM&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#26469;&#33258;&#21508;&#31181;LLM&#30340;&#38169;&#35823;&#25512;&#29702;&#36335;&#24452;&#65292;&#28982;&#21518;&#20351;&#29992;GPT-4&#20316;&#20026;&#8220;&#32416;&#27491;&#32773;&#8221;&#26469;&#35782;&#21035;&#38169;&#35823;&#27493;&#39588;&#65292;&#35299;&#37322;&#38169;&#35823;&#21407;&#22240;&#65292;&#32416;&#27491;&#38169;&#35823;&#24182;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#32416;&#27491;&#30340;&#36827;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#29983;&#25104;&#32416;&#27491;&#25968;&#25454;&#30340;&#38382;&#39064;&#38598;&#12290;&#22312;&#21508;&#31181;LLM&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LEMA&#22987;&#32456;&#21487;&#20197;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a "corrector" to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that \textsc{LeMa} consistently improves CoT-alone fine-tuning. Our fu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#32780;&#22686;&#21152;&#30340;&#35757;&#32451;&#21442;&#25968;&#24456;&#23569;&#12290;</title><link>https://arxiv.org/abs/2310.05707</link><description>&lt;p&gt;
&#29992;&#35268;&#21010;&#26631;&#35760;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Guiding Language Model Math Reasoning with Planning Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#32780;&#22686;&#21152;&#30340;&#35757;&#32451;&#21442;&#25968;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26469;&#22240;&#20854;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#22914;&#24605;&#32500;&#38142;&#25512;&#29702;&#65289;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22686;&#24378;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24573;&#35270;&#20102;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#32467;&#26500;&#21270;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;LLMs&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#20010;&#21035;&#25512;&#29702;&#27493;&#39588;&#65292;&#20294;&#22312;&#25972;&#20010;&#25512;&#29702;&#38142;&#19978;&#20445;&#25345;&#19968;&#33268;&#24615;&#26041;&#38754;&#21364;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#24320;&#22987;&#22788;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#65292;&#20316;&#20026;&#27169;&#22411;&#30340;&#24341;&#23548;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#23884;&#20837;&#28155;&#21152;&#21040;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#22686;&#21152;&#38750;&#24120;&#23567;&#65288;&#20165;&#20026;0.001%&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#23436;&#20840;&#24494;&#35843;&#25110;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#26041;&#26696;&#26469;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;LLMs&#65292;&#22312;&#19977;&#20010;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard f
&lt;/p&gt;</description></item><item><title>Fabricator&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#24102;&#26377;Teacher LLMs&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#12290;&#23427;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;LLM&#26681;&#25454;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#26631;&#27880;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19979;&#19968;&#38454;&#27573;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2309.09582</link><description>&lt;p&gt;
Fabricator: &#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#24102;&#26377;Teacher LLMs&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#30340;&#24320;&#28304;&#24037;&#20855;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09582
&lt;/p&gt;
&lt;p&gt;
Fabricator&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#24102;&#26377;Teacher LLMs&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#12290;&#23427;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;LLM&#26681;&#25454;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#26631;&#27880;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19979;&#19968;&#38454;&#27573;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#26469;&#24314;&#27169;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#29983;&#25104;&#36275;&#22815;&#36136;&#37327;&#21644;&#25968;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#19968;&#31181;&#31216;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#65292;&#35813;&#33539;&#24335;&#36890;&#36807;&#25968;&#25454;&#38598;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#27169;&#24335;&#19979;&#65292;&#19968;&#20010;&#24378;&#22823;&#30340;LLM&#26681;&#25454;&#32473;&#23450;&#30340;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#26631;&#27880;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19979;&#19968;&#38454;&#27573;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#21487;&#20197;&#35753;LLM&#29983;&#25104;500&#20010;&#31215;&#26497;&#24773;&#32490;&#30340;&#30005;&#24433;&#35780;&#35770;&#21644;&#21478;&#22806;500&#20010;&#28040;&#26497;&#24773;&#32490;&#30340;&#35780;&#35770;&#12290;&#28982;&#21518;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#21033;&#29992;LLM&#20316;&#20026;&#36739;&#23567;&#35268;&#27169;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#25945;&#24072;&#12290;&#36890;&#36807;&#36825;&#20010;&#28436;&#31034;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Fabricator&#65292;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#24320;&#28304;Python&#24037;&#20855;&#38598;&#12290;Fabricator&#23454;&#29616;&#20102;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model. For instance, an LLM might be prompted to "generate 500 movie reviews with positive overall sentiment, and another 500 with negative sentiment." The generated data could then be used to train a binary sentiment classifier, effectively leveraging an LLM as a teacher to a smaller student model. With this demo, we introduce Fabricator, an open-source Python toolkit for dataset generation. Fabricator implements common dataset generation workflows, supports a wide range of downstream NLP tasks (such as text classi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25200;&#21160;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#21333;&#35789;&#26469;&#21019;&#24314;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#26367;&#25442;&#26041;&#27861;&#29983;&#25104;&#33258;&#28982;&#19988;&#21487;&#20449;&#30340;&#23545;&#25239;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#27604;&#24378;&#22522;&#20934;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#27450;&#39575;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2309.08999</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#23545;&#25239;&#25915;&#20987;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Context-aware Adversarial Attack on Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25200;&#21160;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#21333;&#35789;&#26469;&#21019;&#24314;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#26367;&#25442;&#26041;&#27861;&#29983;&#25104;&#33258;&#28982;&#19988;&#21487;&#20449;&#30340;&#23545;&#25239;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#27604;&#24378;&#22522;&#20934;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#27450;&#39575;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;PLM&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#20197;&#26816;&#39564;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#25200;&#21160;&#29992;&#20110;&#35782;&#21035;&#23454;&#20307;&#30340;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#21333;&#35789;&#65292;&#20174;&#32780;&#21019;&#24314;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#30740;&#31350;&#19981;&#21516;&#30340;&#20505;&#36873;&#26367;&#25442;&#26041;&#27861;&#26469;&#29983;&#25104;&#33258;&#28982;&#19988;&#21487;&#20449;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27450;&#39575;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#39044;&#27979;&#26041;&#38754;&#27604;&#24378;&#22522;&#20934;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large pre-trained language models (PLMs) have achieved remarkable performance on many natural language processing benchmarks. Despite their success, prior studies have shown that PLMs are vulnerable to attacks from adversarial examples. In this work, we focus on the named entity recognition task and study context-aware adversarial attack methods to examine the model's robustness. Specifically, we propose perturbing the most informative words for recognizing entities to create adversarial examples and investigate different candidate replacement methods to generate natural and plausible adversarial examples. Experiments and analyses show that our methods are more effective in deceiving the model into making wrong predictions than strong baselines.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;STS&#21644;NLI&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20020;&#24202;/&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#35780;&#20272;&#24615;&#33021;&#65292;&#20197;&#21450;&#35780;&#20272;LLMs&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#21644;&#25429;&#25417;&#38598;&#20307;&#20154;&#31867;&#24847;&#35265;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#38382;&#39064;&#22312;LLMs&#26102;&#20195;&#20173;&#26410;&#24471;&#21040;&#22949;&#21892;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2309.08969</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37325;&#26032;&#24605;&#32771;STS&#21644;NLI
&lt;/p&gt;
&lt;p&gt;
Rethinking STS and NLI in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;STS&#21644;NLI&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20020;&#24202;/&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#35780;&#20272;&#24615;&#33021;&#65292;&#20197;&#21450;&#35780;&#20272;LLMs&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#21644;&#25429;&#25417;&#38598;&#20307;&#20154;&#31867;&#24847;&#35265;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#38382;&#39064;&#22312;LLMs&#26102;&#20195;&#20173;&#26410;&#24471;&#21040;&#22949;&#21892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#20351;&#29992;&#29305;&#23450;&#20219;&#21153;&#25552;&#31034;&#65292;&#36825;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26102;&#65292;LLMs&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#21407;&#22240;&#26159;&#20302;&#36164;&#28304;&#39046;&#22495;&#20934;&#30830;&#24615;&#12289;&#27169;&#22411;&#33258;&#20449;&#24230;&#19981;&#36275;&#20197;&#21450;&#25429;&#25417;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#20998;&#27495;&#22256;&#38590;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#32771;&#65292;&#25105;&#20204;&#35797;&#22270;&#37325;&#26032;&#24605;&#32771;LLMs&#26102;&#20195;&#30340;STS&#21644;NLI&#12290;&#25105;&#20204;&#39318;&#20808;&#35780;&#20272;&#20102;&#20020;&#24202;/&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;STS&#21644;NLI&#24615;&#33021;&#65292;&#28982;&#21518;&#35780;&#20272;&#20102;LLMs&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#21644;&#25429;&#25417;&#38598;&#20307;&#20154;&#31867;&#24847;&#35265;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;LLMs&#26102;&#20195;&#65292;&#36825;&#20123;&#32769;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#22949;&#21892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen the rise of large language models (LLMs), where practitioners use task-specific prompts; this was shown to be effective for a variety of tasks. However, when applied to semantic textual similarity (STS) and natural language inference (NLI), the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements. With this in mind, here we try to rethink STS and NLI in the era of LLMs. We first evaluate the performance of STS and NLI in the clinical/biomedical domain, and then we assess LLMs' predictive confidence and their capability of capturing collective human opinions. We find that these old problems are still to be properly addressed in the era of LLMs.
&lt;/p&gt;</description></item><item><title>LAraBench&#26159;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22810;&#31181;&#23454;&#39564;&#35774;&#32622;&#21644;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#65292;&#35777;&#26126;&#26368;&#26032;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;</title><link>https://arxiv.org/abs/2305.14982</link><description>&lt;p&gt;
LAraBench&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#35821;AI&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LAraBench: Benchmarking Arabic AI with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14982
&lt;/p&gt;
&lt;p&gt;
LAraBench&#26159;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22810;&#31181;&#23454;&#39564;&#35774;&#32622;&#21644;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#65292;&#35777;&#26126;&#26368;&#26032;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#35821;&#35328;&#21644;&#35821;&#38899;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#27493;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23578;&#32570;&#20047;&#29305;&#23450;&#35821;&#35328;&#21644;&#20219;&#21153;&#30340;&#26368;&#26032;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;LAraBench&#38024;&#23545;&#38463;&#25289;&#20271;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#36825;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#24207;&#21015;&#26631;&#27880;&#21644;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#20869;&#23481;&#20998;&#31867;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;GPT-3.5-turbo&#12289;GPT-4&#12289;BLOOMZ&#12289;Jais-13b-chat&#12289;Whisper&#21644;USM&#31561;&#27169;&#22411;&#65292;&#36816;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#24212;&#23545;&#20102;33&#20010;&#29420;&#31435;&#20219;&#21153;&#21644;61&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#28041;&#21450;98&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#21253;&#25324;&#32422;296K&#20010;&#25968;&#25454;&#28857;&#12289;&#32422;46&#23567;&#26102;&#30340;&#35821;&#38899;&#21644;30&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#30340;&#21477;&#23376;&#12290;&#36825;&#19968;&#21162;&#21147;&#20135;&#29983;&#20102;330+&#32452;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#37325;&#28857;&#26159;&#34913;&#37327;&#26368;&#26032;&#27169;&#22411;&#21644;LLMs&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#24635;&#20307;&#36235;&#21183;&#34920;&#26126;&#65292;&#26368;&#26032;&#27169;&#22411;&#19968;&#33324;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;3H-TH&#65292;&#29992;&#20110;&#21452;&#26354;&#30693;&#35782;&#22270;&#23884;&#20837;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#20851;&#31995;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#31216;&#24615;&#12289;&#21453;&#23545;&#31216;&#24615;&#12289;&#21453;&#36716;&#12289;&#21487;&#20132;&#25442;&#32452;&#21512;&#12289;&#38750;&#21487;&#20132;&#25442;&#32452;&#21512;&#12289;&#23618;&#27425;&#32467;&#26500;&#21644;&#22810;&#37325;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20302;&#32500;&#31354;&#38388;&#30340;&#20934;&#30830;&#24230;&#12289;&#23618;&#27425;&#32467;&#26500;&#21644;&#20854;&#20182;&#20851;&#31995;&#27169;&#24335;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#34920;&#29616;&#31867;&#20284;&#12290;</title><link>https://arxiv.org/abs/2305.13015</link><description>&lt;p&gt;
&#29992;&#20110;&#21452;&#26354;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#19977;&#32500;&#26059;&#36716;&#21644;&#24179;&#31227;
&lt;/p&gt;
&lt;p&gt;
3D Rotation and Translation for Hyperbolic Knowledge Graph Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;3H-TH&#65292;&#29992;&#20110;&#21452;&#26354;&#30693;&#35782;&#22270;&#23884;&#20837;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#20851;&#31995;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#31216;&#24615;&#12289;&#21453;&#23545;&#31216;&#24615;&#12289;&#21453;&#36716;&#12289;&#21487;&#20132;&#25442;&#32452;&#21512;&#12289;&#38750;&#21487;&#20132;&#25442;&#32452;&#21512;&#12289;&#23618;&#27425;&#32467;&#26500;&#21644;&#22810;&#37325;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20302;&#32500;&#31354;&#38388;&#30340;&#20934;&#30830;&#24230;&#12289;&#23618;&#27425;&#32467;&#26500;&#21644;&#20854;&#20182;&#20851;&#31995;&#27169;&#24335;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#34920;&#29616;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#23884;&#20837;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20197;&#20415;&#39044;&#27979;&#32570;&#22833;&#30340;&#20107;&#23454;&#12290;&#22312;&#23454;&#29616;&#26356;&#22909;&#30340;KG&#23884;&#20837;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#25429;&#25417;&#20851;&#31995;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#31216;&#24615;&#12289;&#21453;&#23545;&#31216;&#24615;&#12289;&#21453;&#36716;&#12289;&#21487;&#20132;&#25442;&#32452;&#21512;&#12289;&#38750;&#21487;&#20132;&#25442;&#32452;&#21512;&#12289;&#23618;&#27425;&#32467;&#26500;&#21644;&#22810;&#37325;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;3H-TH&#65288;&#21452;&#26354;&#31354;&#38388;&#20013;&#30340;&#19977;&#32500;&#26059;&#36716;&#21644;&#24179;&#31227;&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#36825;&#20123;&#20851;&#31995;&#27169;&#24335;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20808;&#21069;&#30340;&#23581;&#35797;&#27809;&#26377;&#21516;&#26102;&#22312;&#25152;&#26377;&#25552;&#21040;&#30340;&#23646;&#24615;&#19978;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26032;&#27169;&#22411;&#22312;&#20302;&#32500;&#31354;&#38388;&#30340;&#31934;&#24230;&#12289;&#23618;&#27425;&#32467;&#26500;&#21644;&#20854;&#20182;&#20851;&#31995;&#27169;&#24335;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#34920;&#29616;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main objective of Knowledge Graph (KG) embeddings is to learn low-dimensional representations of entities and relations, enabling the prediction of missing facts. A significant challenge in achieving better KG embeddings lies in capturing relation patterns, including symmetry, antisymmetry, inversion, commutative composition, non-commutative composition, hierarchy, and multiplicity. This study introduces a novel model called 3H-TH (3D Rotation and Translation in Hyperbolic space) that captures these relation patterns simultaneously. In contrast, previous attempts have not achieved satisfactory performance across all the mentioned properties at the same time. The experimental results demonstrate that the new model outperforms existing state-of-the-art models in terms of accuracy, hierarchy property, and other relation patterns in low-dimensional space, meanwhile performing similarly in high-dimensional space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#24577;&#36873;&#25321;&#24615;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#38646;&#23556;&#20987;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2305.12793</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#36873;&#25321;&#24615;&#33258;&#35757;&#32451;&#23454;&#29616;&#38646;&#23556;&#20987;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot End-to-End Spoken Language Understanding via Cross-Modal Selective Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#24577;&#36873;&#25321;&#24615;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#38646;&#23556;&#20987;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299; (SLU) &#21463;&#21040;&#20102;&#25910;&#38598;&#35821;&#38899;-&#35821;&#20041;&#23545;&#30340;&#25104;&#26412;&#30340;&#32422;&#26463;&#65292;&#29305;&#21035;&#26159;&#24403;&#26631;&#31614;&#22495;&#21457;&#29983;&#21464;&#21270;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;"&#38646;&#23556;&#20987;"&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#65292;&#21363;&#22312;&#27809;&#26377;&#35821;&#38899;-&#35821;&#20041;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#65292;&#32780;&#26159;&#20165;&#20351;&#29992;&#35821;&#38899;-&#25991;&#26412;&#21644;&#25991;&#26412;-&#35821;&#20041;&#23545;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#22312;&#25991;&#26412;-&#35821;&#20041;&#35821;&#26009;&#24211;&#19978;&#23398;&#21040;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#27169;&#22411;&#23545;&#25152;&#26377;&#30340;&#35821;&#38899;-&#25991;&#26412;&#36716;&#24405;&#36827;&#34892;&#20266;&#26631;&#31614;&#21270;&#26469;&#23454;&#29616;&#38646;&#23556;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#27714;&#35821;&#38899;-&#25991;&#26412;&#21644;&#25991;&#26412;-&#35821;&#20041;&#30340;&#39046;&#22495;&#21305;&#37197;&#65292;&#32780;&#30001;&#20110;&#37319;&#38598;&#19981;&#21516;&#30340;&#35821;&#26009;&#24211;&#65292;&#36890;&#24120;&#20250;&#20986;&#29616;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20174;&#20219;&#20309;&#39046;&#22495;&#25910;&#38598;&#30340;&#25972;&#20010;&#35821;&#38899;-&#25991;&#26412;&#35821;&#26009;&#24211;&#20250;&#23548;&#33268;"&#19981;&#24179;&#34913;"&#21644;"&#22122;&#22768;"&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#36328;&#27169;&#24577;&#36873;&#25321;&#24615;&#33258;&#35757;&#32451;" (CMSST)&#12290;CMSST&#36890;&#36807;&#22312;&#19977;&#31181;&#27169;&#24577; (&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35821;&#20041;) &#30340;&#32852;&#21512;&#31354;&#38388;&#20013;&#36827;&#34892;&#32858;&#31867;&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#20511;&#21161;&#36873;&#25321;&#32593;&#32476;&#26469;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end (E2E) spoken language understanding (SLU) is constrained by the cost of collecting speech-semantics pairs, especially when label domains change. Hence, we explore \textit{zero-shot} E2E SLU, which learns E2E SLU without speech-semantics pairs, instead using only speech-text and text-semantics pairs. Previous work achieved zero-shot by pseudolabeling all speech-text transcripts with a natural language understanding (NLU) model learned on text-semantics corpora. However, this method requires the domains of speech-text and text-semantics to match, which often mismatch due to separate collections. Furthermore, using the entire collected speech-text corpus from any domains leads to \textit{imbalance} and \textit{noise} issues. To address these, we propose \textit{cross-modal selective self-training} (CMSST). CMSST tackles imbalance by clustering in a joint space of the three modalities (speech, text, and semantics) and handles label noise with a selection network. We also introdu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#28789;&#27963;&#24615;&#12289;&#35757;&#32451;&#25216;&#26415;&#21644;&#29983;&#25104;&#25928;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#36739;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2302.05737</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Reparameterized Discrete Diffusion Model for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#28789;&#27963;&#24615;&#12289;&#35757;&#32451;&#25216;&#26415;&#21644;&#29983;&#25104;&#25928;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#36739;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#31163;&#25955;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20174;&#31163;&#25955;&#25193;&#25955;&#36807;&#31243;&#20013;&#37319;&#26679;&#30340;&#21478;&#19968;&#31181;&#31561;&#20215;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#24320;&#21457;&#20102;&#19968;&#26063;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#20010;&#27966;&#29983;&#30340;&#36890;&#29992;&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#20026;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#36807;&#31243;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#22791;&#26356;&#26377;&#25928;&#30340;&#35757;&#32451;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models.
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#22312;&#38405;&#35835;&#25925;&#20107;&#26102;&#65292;&#36890;&#36807;&#23545;&#34394;&#26500;&#21644;&#30495;&#23454;&#20154;&#29289;&#30340;&#31867;&#27604;&#65292;&#21487;&#20197;&#24555;&#36895;&#29702;&#35299;&#26032;&#30340;&#34394;&#26500;&#35282;&#33394;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#23569;&#26679;&#26412;&#21644;&#20803;&#23398;&#20064;&#30340;&#24515;&#26234;&#27169;&#22411;&#65288;ToM&#65289;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#25968;&#25454;&#38598;ToM-in-AMC&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20215;&#26426;&#22120;&#20803;&#23398;&#20064;&#24515;&#26234;&#27169;&#22411;&#30340;&#29616;&#23454;&#21465;&#20107;&#29702;&#35299;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2211.04684</link><description>&lt;p&gt;
&#30005;&#24433;&#20013;&#23569;&#26679;&#26412;&#24773;&#24863;&#29702;&#35299;&#20316;&#20026;&#20803;&#23398;&#20064;&#24515;&#26234;&#27169;&#22411;&#35780;&#20215;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.04684
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#22312;&#38405;&#35835;&#25925;&#20107;&#26102;&#65292;&#36890;&#36807;&#23545;&#34394;&#26500;&#21644;&#30495;&#23454;&#20154;&#29289;&#30340;&#31867;&#27604;&#65292;&#21487;&#20197;&#24555;&#36895;&#29702;&#35299;&#26032;&#30340;&#34394;&#26500;&#35282;&#33394;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#23569;&#26679;&#26412;&#21644;&#20803;&#23398;&#20064;&#30340;&#24515;&#26234;&#27169;&#22411;&#65288;ToM&#65289;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#25968;&#25454;&#38598;ToM-in-AMC&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20215;&#26426;&#22120;&#20803;&#23398;&#20064;&#24515;&#26234;&#27169;&#22411;&#30340;&#29616;&#23454;&#21465;&#20107;&#29702;&#35299;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38405;&#35835;&#25925;&#20107;&#26102;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#23558;&#20854;&#19982;&#20182;&#20204;&#24050;&#32463;&#20102;&#35299;&#30340;&#34394;&#26500;&#21644;&#30495;&#23454;&#20154;&#29289;&#36827;&#34892;&#31867;&#27604;&#65292;&#36805;&#36895;&#29702;&#35299;&#26032;&#30340;&#34394;&#26500;&#35282;&#33394;&#12290;&#36825;&#21453;&#26144;&#20102;&#20154;&#31867;&#23545;&#35282;&#33394;&#20869;&#24515;&#29366;&#24577;&#65288;&#21363;&#24515;&#26234;&#27169;&#22411;&#65289;&#30340;&#25512;&#29702;&#20013;&#23569;&#26679;&#26412;&#21644;&#20803;&#23398;&#20064;&#30340;&#26412;&#36136;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#36825;&#26041;&#38754;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;NLP&#25968;&#25454;&#38598;ToM-in-AMC&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20197;&#29616;&#23454;&#21465;&#20107;&#29702;&#35299;&#22330;&#26223;&#20026;&#32972;&#26223;&#30340;&#26426;&#22120;&#20803;&#23398;&#20064;&#24515;&#26234;&#27169;&#22411;&#35780;&#20215;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;1000&#20010;&#20998;&#26512;&#36807;&#30340;&#30005;&#24433;&#21095;&#26412;&#65292;&#27599;&#20010;&#21095;&#26412;&#23545;&#24212;&#20110;&#19968;&#20010;&#38656;&#35201;&#27169;&#22411;&#27169;&#20223;&#20154;&#31867;&#24555;&#36895;&#29702;&#35299;&#26032;&#30005;&#24433;&#20013;&#30340;&#35282;&#33394;&#30340;&#23569;&#26679;&#26412;&#24773;&#24863;&#29702;&#35299;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
When reading a story, humans can quickly understand new fictional characters with a few observations, mainly by drawing analogies to fictional and real people they already know. This reflects the few-shot and meta-learning essence of humans' inference of characters' mental states, i.e., theory-of-mind (ToM), which is largely ignored in existing research. We fill this gap with a novel NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM in a realistic narrative understanding scenario. Our dataset consists of ~1,000 parsed movie scripts, each corresponding to a few-shot character understanding task that requires models to mimic humans' ability of fast digesting characters with a few starting scenes in a new movie.   We propose a novel ToM prompting approach designed to explicitly assess the influence of multiple ToM dimensions. It surpasses existing baseline models, underscoring the significance of modeling multiple ToM dimensions for our task. Our extensive hu
&lt;/p&gt;</description></item><item><title>BoAT v2&#26159;&#19968;&#31181;&#22522;&#20110;Web&#30340;&#20381;&#23384;&#24615;&#26631;&#27880;&#24037;&#20855;&#65292;&#19987;&#27880;&#20110;&#20197;&#31896;&#32858;&#24615;&#35821;&#35328;&#20026;&#37325;&#28857;&#30340;&#35821;&#35328;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#29992;&#25143;&#21516;&#26102;&#26631;&#27880;&#65292;&#20026;&#20351;&#29992;&#32773;&#25552;&#20379;&#33391;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2207.01327</link><description>&lt;p&gt;
BoAT v2 -&#19968;&#31181;&#20197;&#31896;&#32858;&#24615;&#35821;&#35328;&#20026;&#37325;&#28857;&#30340;&#22522;&#20110;Web&#30340;&#20381;&#23384;&#24615;&#26631;&#27880;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
BoAT v2 - A Web-Based Dependency Annotation Tool with Focus on Agglutinative Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.01327
&lt;/p&gt;
&lt;p&gt;
BoAT v2&#26159;&#19968;&#31181;&#22522;&#20110;Web&#30340;&#20381;&#23384;&#24615;&#26631;&#27880;&#24037;&#20855;&#65292;&#19987;&#27880;&#20110;&#20197;&#31896;&#32858;&#24615;&#35821;&#35328;&#20026;&#37325;&#28857;&#30340;&#35821;&#35328;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#29992;&#25143;&#21516;&#26102;&#26631;&#27880;&#65292;&#20026;&#20351;&#29992;&#32773;&#25552;&#20379;&#33391;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26641;&#24211;&#36136;&#37327;&#30340;&#19981;&#26029;&#25552;&#39640;&#65292;&#26641;&#24211;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#24320;&#21457;&#20013;&#21457;&#25381;&#30340;&#20851;&#38190;&#20316;&#29992;&#26085;&#30410;&#22686;&#21152;&#12290;&#21019;&#24314;&#36825;&#31181;&#26641;&#24211;&#38656;&#35201;&#26497;&#22823;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#26641;&#24211;&#30340;&#35268;&#27169;&#26102;&#65292;&#25903;&#25345;&#26631;&#27880;&#36807;&#31243;&#30340;&#24037;&#20855;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26631;&#27880;&#24037;&#20855;&#65292;&#20294;&#23545;&#20110;&#20687;&#22303;&#32819;&#20854;&#35821;&#36825;&#26679;&#30340;&#31896;&#32858;&#24615;&#35821;&#35328;&#24448;&#24448;&#19981;&#36866;&#29992;&#12290; BoAT v1&#26159;&#20026;&#26631;&#27880;&#20381;&#23384;&#20851;&#31995;&#32780;&#24320;&#21457;&#30340;&#65292;&#38543;&#21518;&#29992;&#20110;&#21019;&#24314;&#25163;&#21160;&#26631;&#27880;&#30340;BOUN&#26641;&#24211;&#65288;UD_Turkish-BOUN&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#22522;&#20110;BoAT v1&#20351;&#29992;&#32463;&#39564;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#30340;&#20381;&#23384;&#27880;&#37322;&#24037;&#20855;BoAT v2&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#25913;&#36827;&#30340;&#26426;&#20250;&#12290;BoAT v2&#26159;&#19968;&#20010;&#22810;&#29992;&#25143;&#21644;&#22522;&#20110;Web&#30340;&#20381;&#23384;&#27880;&#37322;&#24037;&#20855;&#65292;&#20854;&#35774;&#35745;&#20391;&#37325;&#20110;&#27880;&#37322;&#32773;&#30340;&#29992;&#25143;&#20307;&#39564;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The value of quality treebanks is steadily increasing due to the crucial role they play in the development of natural language processing tools. The creation of such treebanks is enormously labor-intensive and time-consuming. Especially when the size of treebanks is considered, tools that support the annotation process are essential. Various annotation tools have been proposed, however, they are often not suitable for agglutinative languages such as Turkish. BoAT v1 was developed for annotating dependency relations and was subsequently used to create the manually annotated BOUN Treebank (UD_Turkish-BOUN). In this work, we report on the design and implementation of a dependency annotation tool BoAT v2 based on the experiences gained from the use of BoAT v1, which revealed several opportunities for improvement. BoAT v2 is a multi-user and web-based dependency annotation tool that is designed with a focus on the annotator user experience to yield valid annotations. The main objectives of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SemScore&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#30452;&#25509;&#27604;&#36739;&#27169;&#22411;&#36755;&#20986;&#21644;&#40644;&#37329;&#30446;&#26631;&#22238;&#24212;&#65292;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#35843;&#26657;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SemScore&#25351;&#26631;&#22312;&#19982;&#20154;&#24037;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.17072</link><description>&lt;p&gt;
SemScore: &#22522;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#30340;&#25351;&#20196;&#35843;&#26657;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity. (arXiv:2401.17072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17072
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SemScore&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#30452;&#25509;&#27604;&#36739;&#27169;&#22411;&#36755;&#20986;&#21644;&#40644;&#37329;&#30446;&#26631;&#22238;&#24212;&#65292;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#35843;&#26657;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SemScore&#25351;&#26631;&#22312;&#19982;&#20154;&#24037;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#20196;&#35843;&#26657;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#36866;&#21512;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22238;&#24212;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25163;&#21160;&#35780;&#20272;&#26469;&#21028;&#26029;&#29983;&#25104;&#22238;&#24212;&#30340;&#36136;&#37327;&#12290;&#30001;&#20110;&#36825;&#31181;&#25163;&#21160;&#35780;&#20272;&#32791;&#26102;&#65292;&#19981;&#23481;&#26131;&#25193;&#23637;&#21040;&#23545;&#22810;&#20010;&#27169;&#22411;&#21644;&#27169;&#22411;&#21464;&#20307;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#30701;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#35780;&#20272;&#25351;&#26631;SemScore&#65292;&#36890;&#36807;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#30452;&#25509;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#40644;&#37329;&#30446;&#26631;&#22238;&#24212;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;12&#20010;&#30693;&#21517;&#30340;&#25351;&#20196;&#35843;&#26657;LLMs&#30340;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#20102;&#22522;&#20110;8&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#30340;&#27604;&#36739;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;SemScore&#25351;&#26631;&#22312;&#19982;&#20154;&#24037;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#12289;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#26356;&#22797;&#26434;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#23545;&#20110;&#35780;&#20272;&#25351;&#20196;&#35843;&#26657;LLMs&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable advancements in their ability to generate fitting responses to natural language instructions. However, many current works rely on manual evaluation to judge the quality of generated responses. Since such manual evaluation is time-consuming, it does not easily scale to the evaluation of multiple models and model variants. In this short paper, we propose a straightforward but remarkably effective evaluation metric called SemScore, in which we directly compare model outputs to gold target responses using semantic textual similarity (STS). We conduct a comparative evaluation of the model outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation. We find that our proposed SemScore metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation. These findings indicate the utility of our proposed metric for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#39532;&#32819;&#20182;&#35821;&#36825;&#31181;&#28151;&#21512;&#35821;&#35328;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#28151;&#21512;&#35821;&#35328;&#25991;&#23383;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16895</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#30740;&#31350;&#65306;&#23558;&#20302;&#36164;&#28304;&#39532;&#32819;&#20182;&#35821;&#35270;&#20026;&#22810;&#35821;&#35328;&#20195;&#30721;&#20999;&#25442;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Transfer from Related Languages: Treating Low-Resource Maltese as Multilingual Code-Switching. (arXiv:2401.16895v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#39532;&#32819;&#20182;&#35821;&#36825;&#31181;&#28151;&#21512;&#35821;&#35328;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#28151;&#21512;&#35821;&#35328;&#25991;&#23383;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#65292;&#20294;&#22312;&#19982;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#23384;&#22312;&#25991;&#23383;&#24046;&#24322;&#26102;&#65292;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#20351;&#29992;&#38899;&#35793;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#30340;&#25991;&#23383;&#19982;&#30446;&#26631;&#35821;&#35328;&#30340;&#25991;&#23383;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#24378;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28151;&#21512;&#35821;&#35328;&#26469;&#35828;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#22240;&#20026;&#21482;&#26377;&#35821;&#35328;&#30340;&#26576;&#20010;&#23376;&#38598;&#20174;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21463;&#30410;&#65292;&#32780;&#20854;&#20313;&#37096;&#20998;&#21463;&#21040;&#38459;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#39532;&#32819;&#20182;&#35821;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#38463;&#25289;&#20271;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#33521;&#35821;&#37325;&#22823;&#24433;&#21709;&#65292;&#24182;&#19988;&#37319;&#29992;&#25289;&#19969;&#25991;&#33050;&#26412;&#30340;&#38378;&#31859;&#29305;&#35821;&#35328;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#21333;&#35789;&#32423;&#21035;&#30340;&#35789;&#28304;&#23398;&#27880;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26126;&#26234;&#20915;&#31574;&#22914;&#20309;&#22788;&#29702;&#39532;&#32819;&#20182;&#35821;&#30340;&#27599;&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model's pre-training data. Using transliteration offers a straightforward yet effective means to align the script of a resource-rich language with a target language, thereby enhancing cross-lingual transfer capabilities. However, for mixed languages, this approach is suboptimal, since only a subset of the language benefits from the cross-lingual transfer while the remainder is impeded. In this work, we focus on Maltese, a Semitic language, with substantial influences from Arabic, Italian, and English, and notably written in Latin script. We present a novel dataset annotated with word-level etymology. We use this dataset to train a classifier that enables us to make informed decisions regarding the appropriate processing of each token in the Maltese la
&lt;/p&gt;</description></item><item><title>Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16736</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16736
&lt;/p&gt;
&lt;p&gt;
Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#21019;&#26032;&#25216;&#26415;&#30340;&#24320;&#21457;&#21644;&#21457;&#24067;&#12290;Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#29420;&#29305;&#30340;&#37197;&#32622;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#21270;&#24615;&#33021;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24314;&#31435;&#26377;&#24847;&#20041;&#30340;&#20851;&#32852;&#12290;&#30001;&#20110;&#20854;&#25299;&#25169;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#37197;&#32622;&#65292;&#23427;&#21487;&#20197;&#25552;&#21462;&#29305;&#24449;&#24182;&#23398;&#20064;&#22797;&#26434;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#12290;Atinuke&#26159;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#12290;softmax&#12289;&#23884;&#20837;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#31561;&#39640;&#32423;&#30697;&#38453;&#25805;&#20316;&#20351;&#24471;&#23545;&#25991;&#26412;&#12289;&#22768;&#38899;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#32454;&#33268;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#23558;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#36719;&#20214;&#35774;&#35745;&#21407;&#21017;&#21644;&#25968;&#23398;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15222</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#20197;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#30149;&#20363;&#26816;&#27979;&#20026;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#23454;&#20307;&#30340;&#35821;&#20041;&#21487;&#33021;&#20250;&#21463;&#21040;&#20462;&#39280;&#35821;&#30340;&#26174;&#33879;&#25913;&#21464;&#65292;&#21253;&#25324;&#23454;&#20307;&#30340;&#21542;&#23450;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#26465;&#20214;&#24615;&#12289;&#20005;&#37325;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#29616;&#26377;&#30340;&#30830;&#23450;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#27169;&#22411;&#28041;&#21450;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#25110;&#29305;&#24449;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#20462;&#39280;&#35821;&#30340;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#21464;&#25442;&#22120;&#26550;&#26500;&#35774;&#35745;&#65292;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;SemEval 2015&#20219;&#21153;14&#35821;&#26009;&#24211;&#21644;&#19968;&#20010;&#26032;&#30340;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#65288;OUD&#65289;&#25968;&#25454;&#38598;&#19978;&#20849;&#21516;&#23398;&#20064;&#21644;&#39044;&#27979;&#20462;&#39280;&#35821;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;SemEval&#20849;&#20139;&#30340;&#20462;&#39280;&#35821;&#20197;&#21450;OUD&#29305;&#23450;&#30340;&#26032;&#20462;&#39280;&#35821;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#19982;&#20197;&#21069;&#21457;&#34920;&#30340;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#24182;&#35780;&#20272;&#20102;&#20165;&#20849;&#20139;&#37096;&#20998;&#20020;&#24202;&#20462;&#39280;&#35821;&#26102;&#30340;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;SemEval 2015&#30340;ShARe&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
&lt;/p&gt;</description></item><item><title>EAGLE&#26159;&#19968;&#20010;&#26080;&#25439;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27425;&#39030;&#23618;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#25512;&#29702;&#65292;&#24182;&#35299;&#20915;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;3&#20493;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15077</link><description>&lt;p&gt;
EAGLE: &#25512;&#27979;&#37319;&#26679;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#29305;&#24449;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. (arXiv:2401.15077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15077
&lt;/p&gt;
&lt;p&gt;
EAGLE&#26159;&#19968;&#20010;&#26080;&#25439;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27425;&#39030;&#23618;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#25512;&#29702;&#65292;&#24182;&#35299;&#20915;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;3&#20493;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35299;&#30721;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#21464;&#24471;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;EAGLE&#65288;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#30340;&#22806;&#25512;&#31639;&#27861;&#65289;&#65292;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#19982;&#20256;&#32479;&#30340;&#25512;&#27979;&#37319;&#26679;&#26041;&#27861;&#19981;&#21516;&#65292;EAGLE&#22312;&#26356;&#35268;&#24459;&#30340;&#65288;&#27425;&#39030;&#23618;&#65289;&#29305;&#24449;&#23618;&#38754;&#19978;&#33258;&#22238;&#24402;&#36827;&#34892;&#32534;&#20889;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#25552;&#21069;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#26631;&#35760;&#26469;&#35299;&#20915;&#19979;&#19968;&#20010;&#29305;&#24449;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#12290;EAGLE&#25152;&#25552;&#20379;&#30340;&#21152;&#36895;&#26159;&#26080;&#25439;&#30340;&#65306;&#23427;&#19981;&#38656;&#35201;&#24494;&#35843;&#30446;&#26631;LLM&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#21407;&#22987;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#30340;&#20998;&#24067;&#30456;&#21516;&#12290;&#25130;&#33267;&#26412;&#25991;&#25552;&#20132;&#26102;&#65292;EAGLE&#26159;&#24050;&#30693;&#25512;&#27979;&#37319;&#26679;&#23478;&#26063;&#20013;&#36895;&#24230;&#26368;&#24555;&#30340;&#26694;&#26550;&#12290;&#22312;MT-bench&#19978;&#65292;EAGLE&#27604;&#21407;&#22987;&#35299;&#30721;&#24555;3&#20493;&#65292;&#27604;Lookahead&#24555;2&#20493;&#65292;&#27604;Medusa&#24555;1.6&#20493;&#12290;&#20351;&#29992;gpt-fast&#65292;EAGLE&#24179;&#22343;&#27599;&#31186;&#36798;&#21040;160&#20010;&#26631;&#35760;&#19982;LLaMA2-Chat&#25645;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#39033;&#30446;&#12289;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#36827;&#34892;&#39033;&#30446;&#25512;&#33616;&#65292;&#24182;&#29983;&#25104;&#23545;&#35805;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14194</link><description>&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Conversational Recommender System as a Language Processing Task. (arXiv:2401.14194v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#39033;&#30446;&#12289;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#36827;&#34892;&#39033;&#30446;&#25512;&#33616;&#65292;&#24182;&#29983;&#25104;&#23545;&#35805;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#21521;&#29992;&#25143;&#25512;&#33616;&#30456;&#20851;&#30340;&#39033;&#30446;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#20379;&#39033;&#30446;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#29983;&#25104;&#65292;&#20197;&#21450;&#21033;&#29992;&#25512;&#33616;&#27169;&#22359;&#36827;&#34892;&#30456;&#20851;&#39033;&#30446;&#30340;&#25490;&#24207;&#12290;&#36825;&#31181;&#22810;&#32452;&#20214;&#30340;&#32452;&#21512;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#32321;&#29712;&#65292;&#24182;&#19988;&#23548;&#33268;&#23545;&#35805;&#29983;&#25104;&#21644;&#39033;&#30446;&#25512;&#33616;&#20043;&#38388;&#30340;&#35821;&#20041;&#19981;&#37197;&#23545;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#39033;&#30446;&#65292;&#24182;&#23558;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#39033;&#30446;&#65292;&#22312;&#23545;&#35805;&#20013;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#36827;&#34892;&#39033;&#30446;&#25512;&#33616;&#65292;&#24182;&#29983;&#25104;&#23545;&#35805;&#12290;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;PECRS&#65288;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65289;&#21487;&#20197;&#22312;&#21333;&#20010;&#38454;&#27573;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#19981;&#20381;&#36182;&#38750;&#25991;&#26412;&#20803;&#25968;&#25454;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation. Prior work often utilizes external knowledge graphs for items' semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items. This combination of multiple components suffers from a cumbersome training process, and leads to semantic misalignment issues between dialogue generation and item recommendation. In this paper, we represent items in natural language and formulate CRS as a natural language processing task. Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues. As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph. Experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Mistral 7B&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#36827;&#23637;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#23545;&#25552;&#21319;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#27604;&#20102;&#20854;&#22312;Tatabahasa&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13565</link><description>&lt;p&gt;
&#22522;&#20110;Mistral&#30340;&#22823;&#35268;&#27169;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#21319;&#26412;&#22320;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding. (arXiv:2401.13565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Mistral 7B&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#36827;&#23637;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#23545;&#25552;&#21319;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#27604;&#20102;&#20854;&#22312;Tatabahasa&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Mistral 7B&#30340;&#39044;&#35757;&#32451;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#20351;&#29992;&#20102;32.6GB&#30340;&#25968;&#25454;&#38598;&#65292;&#30456;&#24403;&#20110;11&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#24067;&#20102;&#19978;&#19979;&#25991;&#38271;&#24230;&#20026;4096&#21644;32768&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;16384&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39532;&#26469;&#35199;&#20122;Mistral&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#23545;Mistral 7B&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#19987;&#38376;&#35843;&#25972;&#20102;16384&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#25429;&#25417;&#24494;&#22937;&#35821;&#35328;&#32454;&#33410;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#23545;&#27604;&#20102;&#39532;&#26469;&#35199;&#20122;Mistral&#19982;ChatGPT3.5&#21644;Claude 2&#31561;&#33879;&#21517;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21576;&#29616;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#34920;&#26126;&#39532;&#26469;&#35199;&#20122;Mistral&#22312;Tatabahasa&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present significant advancements in the pretraining of Mistral 7B, a large-scale language model, using a dataset of 32.6 GB, equivalent to 1.1 billion tokens. We explore the impact of extending the context length, releasing models with context lengths of 4096 and 32768 tokens, and further refining performance with a specialized 16384 context length instruction-tuned model, we called it Malaysian Mistral.  Our experiments demonstrate the efficacy of continue pretraining and the influence of extended context lengths on Mistral 7B's language understanding capabilities. Additionally, we release a model specifically tuned with a 16384 context length instruction, showcasing its potential for capturing nuanced language intricacies.  Furthermore, our research contributes to the benchmarking of Malaysian Mistral against prominent language models, including ChatGPT3.5 and Claude 2. We present compelling results indicating Malaysian Mistral's superior performance on Tatabahasa (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SLANG&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861;FOCUS&#65292;&#33021;&#24110;&#21161;LLMs&#26356;&#22909;&#22320;&#29702;&#35299;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.12585</link><description>&lt;p&gt;
SLANG: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
SLANG: New Concept Comprehension of Large Language Models. (arXiv:2401.12585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SLANG&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861;FOCUS&#65292;&#33021;&#24110;&#21161;LLMs&#26356;&#22909;&#22320;&#29702;&#35299;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#30340;&#21160;&#24577;&#24615;&#65292;&#23588;&#20854;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#20442;&#35821;&#21644;&#34920;&#24773;&#21253;&#31561;&#26041;&#38754;&#30340;&#20307;&#29616;&#65292;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36866;&#24212;&#24615;&#24102;&#26469;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20165;&#32465;&#23450;&#22312;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#65292;&#24456;&#38590;&#36319;&#19978;&#22312;&#32447;&#31038;&#21306;&#20013;&#24555;&#36895;&#35821;&#35328;&#36827;&#21270;&#30340;&#27493;&#20240;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#39640;&#25104;&#26412;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#25345;&#32493;&#37325;&#35757;&#32451;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;LLMs&#22312;&#29702;&#35299;&#26032;&#20852;&#35821;&#35328;&#36235;&#21183;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934; - SLANG&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861; FOCUS&#65292;&#23427;&#33021;&#22686;&#24378;LLMs&#23545;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#35821;&#35328;&#36716;&#21464;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#20363;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;&#65292;&#20316;&#20026;&#32972;&#26223;&#20381;&#25454;&#65292;&#20197;&#24418;&#25104;&#26356;&#31934;&#30830;&#21644;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#26032;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research addresses the critical need to bridge this gap, aiming to enhance LLMs' comprehension of evolving new concepts on the internet, without the high cost and impracticality of continual retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$ to assess LLMs' proficiency in comprehending emerging linguistic trends and a baseline approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs to understand new phrases and usage patterns. This approach involves scrutinizing real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36136;&#37327;&#20272;&#35745;&#24230;&#37327;&#26469;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;(XAI)&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;NoRefER&#24230;&#37327;&#22312;&#35782;&#21035;&#21333;&#35789;&#38169;&#35823;&#21644;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27169;&#22411;&#34892;&#20026;&#35265;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;NoRefER&#22312;&#35821;&#26009;&#24211;&#26500;&#24314;&#21644;&#21518;&#26399;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#34920;&#26126;&#20854;&#26377;&#28508;&#21147;&#25104;&#20026;&#25552;&#39640;ASR&#31995;&#32479;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.11268</link><description>&lt;p&gt;
&#21333;&#35789;&#32423;&#21035;&#30340;ASR&#36136;&#37327;&#35780;&#20272;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#21442;&#32771;&#26080;&#20851;&#30340;&#25351;&#26631;&#30340;&#27880;&#24847;&#21147;&#36827;&#34892;&#39640;&#25928;&#35821;&#26009;&#24211;&#37319;&#26679;&#21644;&#21518;&#26399;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Word-Level ASR Quality Estimation for Efficient Corpus Sampling and Post-Editing through Analyzing Attentions of a Reference-Free Metric. (arXiv:2401.11268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36136;&#37327;&#20272;&#35745;&#24230;&#37327;&#26469;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;(XAI)&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;NoRefER&#24230;&#37327;&#22312;&#35782;&#21035;&#21333;&#35789;&#38169;&#35823;&#21644;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27169;&#22411;&#34892;&#20026;&#35265;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;NoRefER&#22312;&#35821;&#26009;&#24211;&#26500;&#24314;&#21644;&#21518;&#26399;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#34920;&#26126;&#20854;&#26377;&#28508;&#21147;&#25104;&#20026;&#25552;&#39640;ASR&#31995;&#32479;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#20013;&#65292;&#19981;&#20165;&#35201;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#36824;&#35201;&#25552;&#20379;&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#24341;&#20837;&#21644;&#35780;&#20272;&#20102;&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#24230;&#37327;&#20316;&#20026;&#22686;&#24378;ASR&#31995;&#32479;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26032;&#24037;&#20855;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;NoRefER&#65288;&#26080;&#21442;&#32771;&#38169;&#35823;&#29575;&#65289;&#24230;&#37327;&#22312;&#35782;&#21035;&#21333;&#35789;&#32423;&#38169;&#35823;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#24110;&#21161;&#21518;&#26399;&#32534;&#36753;&#32773;&#25913;&#36827;ASR&#20551;&#35774;&#12290;&#30740;&#31350;&#36824;&#25193;&#23637;&#21040;NoRefER&#22312;&#35821;&#26009;&#24211;&#26500;&#24314;&#36807;&#31243;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#22686;&#24378;&#20855;&#26377;&#26377;&#35265;&#22320;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;NoRefER&#30340;&#35786;&#26029;&#29305;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20854;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27169;&#22411;&#34892;&#20026;&#21644;&#20915;&#31574;&#27169;&#24335;&#35265;&#35299;&#30340;&#33021;&#21147;&#12290;&#36825;&#23545;&#20110;&#22312;&#21518;&#26399;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#21644;&#24494;&#35843;ASR&#27169;&#22411;&#20013;&#20248;&#20808;&#32771;&#34385;&#20551;&#35774;&#26159;&#26377;&#30410;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;NoRefER&#20855;&#26377;&#28508;&#21147;&#25104;&#20026;&#25552;&#39640;ASR&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of automatic speech recognition (ASR), the quest for models that not only perform with high accuracy but also offer transparency in their decision-making processes is crucial. The potential of quality estimation (QE) metrics is introduced and evaluated as a novel tool to enhance explainable artificial intelligence (XAI) in ASR systems. Through experiments and analyses, the capabilities of the NoRefER (No Reference Error Rate) metric are explored in identifying word-level errors to aid post-editors in refining ASR hypotheses. The investigation also extends to the utility of NoRefER in the corpus-building process, demonstrating its effectiveness in augmenting datasets with insightful annotations. The diagnostic aspects of NoRefER are examined, revealing its ability to provide valuable insights into model behaviors and decision patterns. This has proven beneficial for prioritizing hypotheses in post-editing workflows and fine-tuning ASR models. The findings suggest that NoRef
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#32534;&#36753;&#20107;&#23454;&#24182;&#35266;&#23519;&#20854;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#26356;&#26032;&#20256;&#25773;&#65292;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.10521</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#35821;&#35328;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Editing in Multilingual Language Models. (arXiv:2401.10521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#32534;&#36753;&#20107;&#23454;&#24182;&#35266;&#23519;&#20854;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#26356;&#26032;&#20256;&#25773;&#65292;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#32780;&#26356;&#26032;&#36807;&#26102;&#30340;LLM&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#36164;&#28304;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#35768;&#22810;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65288;MET&#65289;&#20197;&#20415;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#26356;&#26032;&#27169;&#22411;&#36755;&#20986;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;LLM&#20013;&#65292;&#20854;&#20013;&#30340;&#30693;&#35782;&#20197;&#22810;&#31181;&#35821;&#35328;&#23384;&#20648;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65288;XME&#65289;&#33539;&#24335;&#65292;&#22312;&#35813;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20107;&#23454;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#34987;&#32534;&#36753;&#65292;&#35266;&#23519;&#20854;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26356;&#26032;&#20256;&#25773;&#12290;&#20026;&#20102;&#30740;&#31350;XME&#33539;&#24335;&#65292;&#25105;&#20204;&#20351;&#29992;BLOOM&#12289;mBERT&#21644;XLM-RoBERTa&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#20889;&#20316;&#33050;&#26412;&#65292;&#21363;&#25289;&#19969;&#35821;&#65288;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#21644;&#21360;&#22320;&#35821;&#65288;&#21360;&#22320;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;XME&#35774;&#32622;&#19979;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;MET&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#24403;&#28041;&#21450;&#30340;&#35821;&#35328;&#23646;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#35821;&#26063;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (\textbf{XME}) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: \textit{Latin} (English, French, and Spanish) and \textit{Indic} (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#36890;&#36807;&#21512;&#24182;&#33410;&#28857;&#29305;&#24449;&#12289;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#21644;&#36845;&#20195;&#28436;&#21270;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;KGLN&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;AUC&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;</title><link>http://arxiv.org/abs/2401.10244</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#39537;&#21160;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph driven recommendation model of graph neural network. (arXiv:2401.10244v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#36890;&#36807;&#21512;&#24182;&#33410;&#28857;&#29305;&#24449;&#12289;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#21644;&#36845;&#20195;&#28436;&#21270;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;KGLN&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;AUC&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#21033;&#29992;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#20013;&#30340;&#20010;&#20307;&#33410;&#28857;&#29305;&#24449;&#21512;&#24182;&#65292;&#28982;&#21518;&#36890;&#36807;&#32467;&#21512;&#24433;&#21709;&#22240;&#32032;&#35843;&#25972;&#30456;&#37051;&#23454;&#20307;&#30340;&#32858;&#21512;&#26435;&#37325;&#12290;&#36890;&#36807;&#36845;&#20195;&#65292;&#27169;&#22411;&#20174;&#21333;&#23618;&#36880;&#28176;&#28436;&#21464;&#20026;&#22810;&#23618;&#65292;&#20351;&#23454;&#20307;&#33021;&#22815;&#33719;&#21462;&#20016;&#23500;&#30340;&#22810;&#38454;&#20851;&#32852;&#23454;&#20307;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#23558;&#23454;&#20307;&#21644;&#29992;&#25143;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#20135;&#29983;&#25512;&#33616;&#20998;&#25968;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#21644;&#24433;&#21709;&#22240;&#32032;&#30340;&#25928;&#26524;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;MovieLen-1M&#21644;Book-Crossing&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;KGLN&#30456;&#23545;&#20110;LibFM&#21644;D&#31561;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;AUC&#65288;ROC&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65289;&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
A new graph neural network-based recommendation model called KGLN, which leverages Knowledge Graph (KG) information, was developed to enhance the accuracy and effectiveness of personalized recommendations. This model begins by using a single-layer neural network to merge individual node features in the graph. It then adjusts the aggregation weights of neighboring entities by incorporating influence factors. The model evolves from a single layer to multiple layers through iteration, enabling entities to access extensive multi-order associated entity information. The final step involves integrating features of entities and users to produce a recommendation score. The model's performance was evaluated by comparing its effects on various aggregation methods and influence factors. In tests using the MovieLen-1M and Book-Crossing datasets, KGLN showed an AUC (Area Under the ROC curve) improvement of 0.3% to 5.9% and 1.1% to 8.2%, respectively, over established benchmark methods like LibFM, D
&lt;/p&gt;</description></item><item><title>LoMA&#26159;&#19968;&#31181;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2401.09486</link><description>&lt;p&gt;
LoMA: &#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
LoMA: Lossless Compressed Memory Attention. (arXiv:2401.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09486
&lt;/p&gt;
&lt;p&gt;
LoMA&#26159;&#19968;&#31181;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#38271;&#25991;&#26412;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#37325;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#20294;&#38543;&#30528;&#25991;&#26412;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#36164;&#28304;&#28040;&#32791;&#20063;&#24613;&#21095;&#22686;&#21152;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#21387;&#32553;KV&#32531;&#23384;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29616;&#26377;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#32570;&#28857;&#65306;&#21387;&#32553;&#26159;&#26377;&#25439;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#20449;&#24687;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#20002;&#22833;&#12290;&#22914;&#26524;&#21387;&#32553;&#29575;&#24456;&#39640;&#65292;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#30340;&#27010;&#29575;&#20250;&#22823;&#22823;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#65288;LoMA&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#19968;&#32452;&#21387;&#32553;&#27604;&#29575;&#23558;&#20449;&#24687;&#26080;&#25439;&#21387;&#32553;&#25104;&#29305;&#27530;&#30340;&#20869;&#23384;&#20196;&#29260;KV&#23545;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LoMA&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#39640;&#25928;&#35757;&#32451;&#19988;&#20855;&#26377;&#38750;&#24120;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to handle long texts is one of the most important capabilities of Large Language Models (LLMs), but as the text length increases, the consumption of resources also increases dramatically. At present, reducing resource consumption by compressing the KV cache is a common approach. Although there are many existing compression methods, they share a common drawback: the compression is not lossless. That is, information is inevitably lost during the compression process. If the compression rate is high, the probability of losing important information increases dramatically. We propose a new method, Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information into special memory token KV pairs according to a set compression ratio. Our experiments have achieved remarkable results, demonstrating that LoMA can be efficiently trained and has very effective performance.
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.05631</link><description>&lt;p&gt;
DrawTalking&#65306;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05631
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;DrawTalking&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#12290;&#23427;&#24378;&#35843;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#31867;&#20284;&#32534;&#31243;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;iPad&#19978;&#23454;&#29616;&#20102;&#23427;&#12290;&#19968;&#39033;&#24320;&#25918;&#24335;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#35768;&#22810;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#29992;&#20363;&#30456;&#22865;&#21512;&#21644;&#36866;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#33258;&#28982;&#29992;&#25143;&#20013;&#24515;&#30028;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#32842;&#22825;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;MUCA&#65289;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#32676;&#32452;&#35752;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#26469;&#30830;&#23450;&#22238;&#24212;&#20869;&#23481;&#12289;&#26102;&#26426;&#21644;&#36866;&#24403;&#30340;&#25509;&#25910;&#32773;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#27169;&#25311;&#22120;&#65288;MUS&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#34892;&#20026;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#21644;&#20248;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;</title><link>http://arxiv.org/abs/2401.04883</link><description>&lt;p&gt;
&#22810;&#29992;&#25143;&#32842;&#22825;&#21161;&#25163;&#65288;MUCA&#65289;&#65306;&#19968;&#31181;&#20351;&#29992;LLMs&#26694;&#26550;&#20419;&#36827;&#32676;&#20307;&#23545;&#35805;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations. (arXiv:2401.04883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04883
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#32842;&#22825;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;MUCA&#65289;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#32676;&#32452;&#35752;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#26469;&#30830;&#23450;&#22238;&#24212;&#20869;&#23481;&#12289;&#26102;&#26426;&#21644;&#36866;&#24403;&#30340;&#25509;&#25910;&#32773;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#27169;&#25311;&#22120;&#65288;MUS&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#34892;&#20026;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#21644;&#20248;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#32780;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#29992;&#25143;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#19978;&#65292;&#37325;&#28857;&#25918;&#22312;&#29992;&#25143;&#36755;&#20837;&#21518;&#20915;&#23450;&#8220;&#22238;&#31572;&#20160;&#20040;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#29992;&#25143;&#32842;&#22825;&#26426;&#22120;&#20154;&#26377;&#26356;&#22797;&#26434;&#30340;3W&#35774;&#35745;&#32500;&#24230;&#8212;&#8212;&#22914;&#20309;&#22238;&#31572;&#65292;&#8220;&#20309;&#26102;&#8221;&#22238;&#24212;&#65292;&#8220;&#22238;&#31572;&#35841;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Multi-User Chat Assistant (MUCA)&#30340;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#32676;&#32452;&#35752;&#35770;&#12290;MUCA&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#23376;&#20027;&#39064;&#29983;&#25104;&#22120;&#65292;&#23545;&#35805;&#20998;&#26512;&#22120;&#21644;&#35805;&#35821;&#31574;&#30053;&#20210;&#35009;&#22120;&#12290;&#36825;&#20123;&#27169;&#22359;&#20849;&#21516;&#30830;&#23450;&#21512;&#36866;&#30340;&#22238;&#24212;&#20869;&#23481;&#12289;&#26102;&#26426;&#21644;&#36866;&#24403;&#30340;&#25509;&#25910;&#32773;&#12290;&#20026;&#20102;&#20351;MUCA&#30340;&#20248;&#21270;&#36807;&#31243;&#26356;&#23481;&#26131;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#22810;&#29992;&#25143;&#27169;&#25311;&#22120;&#65288;MUS&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#34892;&#20026;&#12290;&#36825;&#20351;&#24471;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#27169;&#25311;&#29992;&#25143;&#20043;&#38388;&#30340;&#23545;&#35805;&#36827;&#34892;&#26356;&#24555;&#36895;&#30340;&#27169;&#25311;&#65292;&#20174;&#32780;&#20351;&#24471;&#26089;&#26399;&#27979;&#35797;&#21644;&#20248;&#21270;&#36807;&#31243;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have provided a new avenue for chatbot development, while most existing research has primarily centered on single-user chatbots that focus on deciding "What" to answer after user inputs. In this paper, we identified that multi-user chatbots have more complex 3W design dimensions -- "What" to say, "When" to respond, and "Who" to answer. Additionally, we proposed Multi-User Chat Assistant (MUCA), which is an LLM-based framework for chatbots specifically designed for group discussions. MUCA consists of three main modules: Sub-topic Generator, Dialog Analyzer, and Utterance Strategies Arbitrator. These modules jointly determine suitable response contents, timings, and the appropriate recipients. To make the optimizing process for MUCA easier, we further propose an LLM-based Multi-User Simulator (MUS) that can mimic real user behavior. This enables faster simulation of a conversation between the chatbot and simulated users, making the earl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#25351;&#20986;&#65292;&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#25913;&#21892;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20294;&#20250;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#32534;&#36753;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#32534;&#36753;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#21487;&#33021;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.04700</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#25439;&#23475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Model Editing Can Hurt General Abilities of Large Language Models. (arXiv:2401.04700v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04700
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25351;&#20986;&#65292;&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#25913;&#21892;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20294;&#20250;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#32534;&#36753;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#32534;&#36753;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#21487;&#33021;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#25105;&#20204;&#33719;&#21462;&#20854;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#26032;&#30340;&#33539;&#24335;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;LLM&#36755;&#20986;&#20013;&#23384;&#22312;&#38169;&#35273;&#65292;&#36825;&#26159;&#30001;&#20110;&#38169;&#35823;&#25110;&#36807;&#26102;&#30693;&#35782;&#24341;&#36215;&#30340;&#12290;&#30001;&#20110;&#20351;&#29992;&#26356;&#26032;&#21518;&#30340;&#20449;&#24687;&#37325;&#26032;&#35757;&#32451;LLM&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#22240;&#27492;&#20154;&#20204;&#23545;&#27169;&#22411;&#32534;&#36753;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#24456;&#26377;&#25928;&#65292;&#20294;&#24448;&#24448;&#36807;&#20110;&#24378;&#35843;&#32534;&#36753;&#24615;&#33021;&#30340;&#21151;&#25928;&#12289;&#27867;&#21270;&#24615;&#21644;&#23616;&#37096;&#24615;&#65292;&#24120;&#24120;&#24573;&#35270;&#20102;&#23545;LLM&#30340;&#36890;&#29992;&#33021;&#21147;&#21487;&#33021;&#20135;&#29983;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#21892;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#21487;&#33021;&#20250;&#20197;&#30456;&#24403;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#25285;&#24551;&#65292;&#36825;&#19981;&#31526;&#21512;LLM&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#24120;&#29992;&#30340;&#32534;&#36753;&#26041;&#27861;&#22312;&#20004;&#20010;LLM&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#21103;&#20316;&#29992;&#65292;&#24182;&#28085;&#30422;&#20102;&#20843;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have opened up new paradigms for accessing the knowledge stored in their parameters. One critical challenge that has emerged is the presence of hallucinations in LLM outputs due to false or outdated knowledge. Since retraining LLMs with updated information is resource-intensive, there has been a growing interest in model editing. However, many model editing methods, while effective in various scenarios, tend to overemphasize aspects such as efficacy, generalization, and locality in editing performance, often overlooking potential side effects on the general abilities of LLMs. In this paper, we raise concerns that the improvement of model factuality may come at the cost of a significant degradation of these general abilities, which is not conducive to the sustainable development of LLMs. Systematically, we analyze side effects by evaluating four popular editing methods on two LLMs across eight representative task categories. Extensive empi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01325</link><description>&lt;p&gt;
&#33258;&#25193;&#23637;LLM:&#26080;&#38656;&#35843;&#25972;&#30340;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31934;&#35843;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#24207;&#21015;&#30340;&#26377;&#38480;&#38271;&#24230;&#21487;&#33021;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#26412;&#36523;&#20855;&#26377;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20854;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Self-Extend&#26041;&#27861;&#26469;&#28608;&#21457;LLMs&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#28508;&#21147;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#26500;&#24314;&#21452;&#23618;&#27880;&#24847;&#20449;&#24687;&#65306;&#32676;&#32452;&#32423;&#21644;&#37051;&#23621;&#32423;&#12290;&#36825;&#20004;&#20010;&#32423;&#21035;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#12290;&#21482;&#38656;&#20462;&#25913;&#22235;&#34892;&#20195;&#30721;&#65292;&#25152;&#25552;&#26041;&#27861;&#23601;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#31934;&#35843;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;+&#25688;&#35201;&#20943;&#25481;&#25991;&#31456;&#26368;&#21518;&#19968;&#21477;&#35441;
&lt;/p&gt;
&lt;p&gt;
This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#31034;&#20363;&#20013;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#36890;&#36807;&#24341;&#20837;Structured Packing for Long Context (SPLiCe)&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17296</link><description>&lt;p&gt;
LLM&#35757;&#32451;&#20013;&#30340;&#32467;&#26500;&#21270;&#22635;&#20805;&#25913;&#36827;&#20102;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Structured Packing in LLM Training Improves Long Context Utilization. (arXiv:2312.17296v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#31034;&#20363;&#20013;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#36890;&#36807;&#24341;&#20837;Structured Packing for Long Context (SPLiCe)&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LCLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#26597;&#35810;&#31185;&#23398;&#30740;&#31350;&#35770;&#25991;&#31561;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#24448;&#24448;&#21463;&#21040;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30830;&#23450;&#20856;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#32570;&#20047;&#38271;&#31243;&#35821;&#20041;&#20381;&#36182;&#26159;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#39057;&#32321;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#36755;&#20837;&#30340;&#22909;&#22788;&#12290;&#21033;&#29992;&#20195;&#30721;&#25968;&#25454;&#30340;&#22266;&#26377;&#30446;&#24405;&#32467;&#26500;&#20316;&#20026;&#35757;&#32451;&#31034;&#20363;&#30340;&#26469;&#28304;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#23545;&#20110;&#19982;&#32534;&#30721;&#26080;&#20851;&#30340;&#20219;&#21153;&#65292;&#22218;&#25324;&#30456;&#20851;&#25991;&#26723;&#33021;&#22815;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24182;&#19988;&#26356;&#20855;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Structured Packing for Long Context (SPLiCe)&#30340;&#21019;&#26032;&#26041;&#27861;&#12290; SPLiCe&#26159;&#19968;&#31181;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;\method{}&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#29992;&#20110;t
&lt;/p&gt;
&lt;p&gt;
Recent advances in long-context Large Language Models (LCLMs) have generated significant interest, especially in applications such as querying scientific research papers. However, their potential is often limited by inadequate context utilization. We identify the absence of long-range semantic dependencies in typical training data as a primary hindrance. To address this, we delve into the benefits of frequently incorporating related documents into training inputs. Using the inherent directory structure of code data as a source of training examples, we demonstrate improvements in perplexity, even for tasks unrelated to coding. Building on these findings, but with a broader focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an innovative method for creating training examples by using a retrieval method to collate the most mutually relevant documents into a single training context. Our results indicate that \method{} enhances model performance and can be used to t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#35843;&#26597;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#23637;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.04076</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#20506;&#65311;&#19968;&#39033;&#20851;&#20110;&#35843;&#26597;&#35774;&#35745;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do LLMs exhibit human-like response biases? A case study in survey design. (arXiv:2311.04076v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#35843;&#26597;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#23637;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22686;&#24378;&#65292;&#20154;&#20204;&#23545;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#20154;&#31867;&#36827;&#34892;&#20027;&#35266;&#26631;&#31614;&#20219;&#21153;&#65288;&#22914;&#35843;&#26597;&#21644;&#33286;&#35770;&#35843;&#26597;&#65289;&#30340;&#21487;&#33021;&#24615;&#36234;&#26469;&#36234;&#20852;&#22859;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#25514;&#36766;&#30340;&#25935;&#24863;&#24615;&#26159;&#20854;&#24191;&#27867;&#24341;&#36848;&#30340;&#38480;&#21046;&#20043;&#19968;&#65292;&#20294;&#26377;&#36259;&#30340;&#26159;&#65292;&#20154;&#31867;&#22312;&#22238;&#24212;&#20013;&#20063;&#26174;&#31034;&#20986;&#23545;&#25351;&#20196;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#34920;&#29616;&#20026;&#21453;&#24212;&#20559;&#20506;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#35201;&#20351;&#29992;LLMs&#36817;&#20284;&#20154;&#31867;&#24847;&#35265;&#65292;&#26377;&#24517;&#35201;&#35843;&#26597;LLMs&#26159;&#21542;&#20063;&#21453;&#26144;&#20102;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20197;&#35843;&#26597;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#35843;&#26597;&#38382;&#21367;&#20013;&#30001;&#20110;&#8220;&#25552;&#31034;&#8221;&#25514;&#36766;&#30340;&#21464;&#21270;&#23548;&#33268;&#30340;&#20154;&#31867;&#21453;&#24212;&#20559;&#24046;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20511;&#37492;&#31038;&#20250;&#24515;&#29702;&#23398;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;LLMs&#26159;&#21542;&#22312;&#35843;&#26597;&#38382;&#21367;&#20013;&#23637;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. As such, we argue that if LLMs are going to be used to approximate human opinions, it is necessary to investigate the extent to which LLMs also reflect human response biases, if at all. In this work, we use survey design as a case study, where human response biases caused by permutations in wordings of "prompts" have been extensively studied. Drawing from prior work in social psychology, we design a dataset and propose a framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;"&#35821;&#35328;&#24187;&#35273;"&#30456;&#20851;&#30340;&#21028;&#26029;&#20013;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#32467;&#26500;&#20381;&#36182;&#24615;&#30340;&#24187;&#35273;&#30340;&#24433;&#21709;&#65292;&#32780;&#22312;&#35821;&#20041;&#26041;&#38754;&#21017;&#36739;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2311.01386</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#35821;&#35328;&#24187;&#35273;&#30340;&#27450;&#39575;&#65311;&#22312;&#35821;&#27861;&#26041;&#38754;&#23481;&#26131;&#65292;&#22312;&#35821;&#20041;&#26041;&#38754;&#22256;&#38590;&#12290;&#65288;arXiv:2311.01386v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics. (arXiv:2311.01386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01386
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;"&#35821;&#35328;&#24187;&#35273;"&#30456;&#20851;&#30340;&#21028;&#26029;&#20013;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#32467;&#26500;&#20381;&#36182;&#24615;&#30340;&#24187;&#35273;&#30340;&#24433;&#21709;&#65292;&#32780;&#22312;&#35821;&#20041;&#26041;&#38754;&#21017;&#36739;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21028;&#26029;&#35821;&#27861;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#26377;&#24456;&#22823;&#37325;&#21472;&#65292;&#20294;&#26159;&#24403;&#20154;&#31867;&#22312;&#35821;&#35328;&#22788;&#29702;&#20013;&#31995;&#32479;&#24615;&#22320;&#20986;&#29616;&#38169;&#35823;&#26102;&#65292;&#25105;&#20204;&#26159;&#21542;&#26399;&#26395;LMs&#33021;&#20687;&#35821;&#35328;&#30340;&#35748;&#30693;&#27169;&#22411;&#37027;&#26679;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#65311;&#36890;&#36807;&#30740;&#31350;&#19982;&#8220;&#35821;&#35328;&#24187;&#35273;&#8221;&#30456;&#20851;&#30340;LMs&#30340;&#26356;&#24494;&#22937;&#21028;&#26029;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#8212;&#8212;&#36825;&#20123;&#21477;&#23376;&#22312;&#24847;&#20041;&#19978;&#27169;&#31946;&#12289;&#19981;&#21512;&#24773;&#29702;&#25110;&#35821;&#27861;&#38169;&#35823;&#65292;&#20294;&#21364;&#21463;&#21040;&#20154;&#31867;&#24847;&#22806;&#39640;&#25509;&#21463;&#24230;&#30340;&#21028;&#26029;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#24187;&#35273;&#65306;&#27604;&#36739;&#24187;&#35273;&#65288;&#20363;&#22914;&#8220;&#21435;&#36807;&#20420;&#32599;&#26031;&#30340;&#20154;&#27604;&#25105;&#22810;&#8221;&#65289;&#65292;&#28145;&#24230;&#20914;&#20987;&#24187;&#35273;&#65288;&#20363;&#22914;&#8220;&#27809;&#26377;&#36731;&#24494;&#30340;&#22836;&#37096;&#20260;&#23475;&#21487;&#20197;&#34987;&#24573;&#35270;&#8221;&#65289;&#21644;&#21542;&#23450;&#26497;&#24615;&#39033;&#65288;NPI&#65289;&#24187;&#35273;&#65288;&#20363;&#22914;&#8220;&#27809;&#26377;&#19968;&#20010;&#20065;&#26449;&#20154;&#30456;&#20449;&#26159;&#21487;&#20449;&#36182;&#30340;&#29454;&#20154;&#20250;&#21521;&#29066;&#23556;&#20987;&#8221;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LMs&#34920;&#31034;&#30340;&#27010;&#29575;&#26356;&#26377;&#21487;&#33021;&#19982;&#20154;&#31867;&#23545;&#20110;&#34987;NPI&#24187;&#35273;&#8220;&#27450;&#39575;&#8221;&#30340;&#21028;&#26029;&#19968;&#33268;&#65292;&#36825;&#19968;&#24187;&#35273;&#26816;&#39564;&#20102;&#19968;&#31181;&#32467;&#26500;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect LMs to behave like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with "language illusions" -- sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. "More people have been to Russia than I have"), the depth-charge illusion (e.g. "No head injury is too trivial to be ignored"), and the negative polarity item (NPI) illusion (e.g. "The hunter who no villager believed to be trustworthy will ever shoot a bear"). We found that probabilities represented by LMs were more likely to align with human judgments of being "tricked" by the NPI illusion which examines a structural dep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01256</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#28304;&#30340;&#27861;&#24459;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#24120;&#35265;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#35780;&#20272;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36861;&#27714;&#26368;&#20339;&#24615;&#33021;&#30340;&#31454;&#20105;&#20013;&#65292;&#32463;&#24120;&#24573;&#35270;&#35768;&#22810;&#37325;&#35201;&#22240;&#32032;&#65292;&#32780;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#22240;&#32032;&#24212;&#35813;&#34987;&#20180;&#32454;&#32771;&#34385;&#12290;&#23454;&#38469;&#19978;&#65292;&#26377;&#26102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#32780;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#22240;&#32032;&#24517;&#39035;&#32771;&#34385;&#22312;&#20869;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;NLP&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;LexGLUE&#22522;&#20934;&#19978;&#23545;LLM&#21644;&#20256;&#32479;&#26041;&#27861;&#65288;&#20363;&#22914;SVM&#65289;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#21516;&#26102;&#32771;&#34385;&#24615;&#33021;&#65288;&#26631;&#20934;&#25351;&#26631;&#65289;&#21644;&#20854;&#20182;&#25351;&#26631;&#65292;&#22914;&#26102;&#38388;&#12289;&#32791;&#33021;&#21644;&#25104;&#26412;&#65292;&#24635;&#20043;&#23601;&#26159;&#30899;&#36275;&#36857;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#32771;&#34385;&#20102;&#21407;&#22411;&#35774;&#35745;&#38454;&#27573;&#65288;&#36890;&#36807;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#36845;&#20195;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65289;&#21644;&#29983;&#20135;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they fol
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15393</link><description>&lt;p&gt;
DoGE: &#20351;&#29992;&#27867;&#21270;&#20272;&#35745;&#36827;&#34892;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15393
&lt;/p&gt;
&lt;p&gt;
DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#35821;&#26009;&#24211;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#32452;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30001;&#21508;&#31181;&#26469;&#28304;&#39046;&#22495;&#65288;&#22914;CommonCrawl&#12289;Wikipedia&#12289;Github&#31561;&#65289;&#25353;&#29031;&#29305;&#23450;&#30340;&#37319;&#26679;&#27010;&#29575;&#65288;&#39046;&#22495;&#26435;&#37325;&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#20248;&#21270;&#39046;&#22495;&#26435;&#37325;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DOmain reweighting with Generalization Estimation&#65288;DoGE&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#37325;&#26032;&#35843;&#25972;&#20102;&#27599;&#20010;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#65292;&#26681;&#25454;&#23427;&#23545;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#36827;&#34892;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#27867;&#21270;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#35757;&#32451;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#33719;&#21462;&#37325;&#26032;&#21152;&#26435;&#30340;&#39046;&#22495;&#26435;&#37325;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#36890;&#36807;&#38236;&#20687;&#19979;&#38477;&#27861;&#26356;&#26032;&#39046;&#22495;&#26435;&#37325;&#20197;&#26368;&#22823;&#21270;&#25972;&#20307;&#30340;&#27867;&#21270;&#22686;&#30410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#33719;&#24471;&#30340;&#39046;&#22495;&#26435;&#37325;&#26469;&#35757;&#32451;&#19968;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;&#23436;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
&lt;/p&gt;</description></item><item><title>ICU&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35270;&#35273;&#19982;&#35821;&#35328;&#24314;&#27169;&#20013;&#35821;&#35328;&#38556;&#30861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#29702;&#35299;&#20004;&#20010;&#38454;&#27573;&#65292;&#23558;&#22810;&#35821;&#35328;&#22788;&#29702;&#36127;&#25285;&#36716;&#31227;&#21040;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ICU&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12531</link><description>&lt;p&gt;
ICU&#65306;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#29702;&#35299;&#26469;&#20811;&#26381;&#35270;&#35273;&#19982;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding. (arXiv:2310.12531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12531
&lt;/p&gt;
&lt;p&gt;
ICU&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35270;&#35273;&#19982;&#35821;&#35328;&#24314;&#27169;&#20013;&#35821;&#35328;&#38556;&#30861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#29702;&#35299;&#20004;&#20010;&#38454;&#27573;&#65292;&#23558;&#22810;&#35821;&#35328;&#22788;&#29702;&#36127;&#25285;&#36716;&#31227;&#21040;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ICU&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;(V&amp;L)&#30740;&#31350;&#26088;&#22312;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#30340;&#22810;&#35821;&#35328;&#23383;&#24149;&#31232;&#32570;&#19968;&#30452;&#20197;&#26469;&#19968;&#30452;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ICU&#65288;Image Caption Understanding&#65289;&#65292;&#23558;V&amp;L&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#19968;&#20010;V&amp;L&#27169;&#22411;&#20197;&#33521;&#25991;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#28982;&#21518;&#19968;&#20010;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;mLM&#65289;&#20197;&#23383;&#24149;&#20316;&#20026;&#26367;&#20195;&#25991;&#26412;&#36827;&#34892;&#36328;&#35821;&#35328;&#35821;&#35328;&#29702;&#35299;&#12290;&#36825;&#31181;&#26041;&#24335;&#20943;&#36731;&#20102;V&amp;L&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#22788;&#29702;&#36127;&#25285;&#65292;&#23558;&#20854;&#36716;&#31227;&#21040;&#20102;mLM&#19978;&#12290;&#30001;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#25968;&#25454;&#30456;&#23545;&#20016;&#23500;&#21644;&#36136;&#37327;&#36739;&#39640;&#65292;ICU&#21487;&#20197;&#24110;&#21161;&#20811;&#26381;V&amp;L&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;&#22312;IGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#28041;&#21450;9&#31181;&#35821;&#35328;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ICU&#21487;&#20197;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#23454;&#29616;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#22312;&#20854;&#20313;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most multilingual vision-and-language (V&amp;L) research aims to accomplish multilingual and multimodal capabilities within one model. However, the scarcity of multilingual captions for images has hindered the development. To overcome this obstacle, we propose ICU, Image Caption Understanding, which divides a V&amp;L task into two stages: a V&amp;L model performs image captioning in English, and a multilingual language model (mLM), in turn, takes the caption as the alt text and performs crosslingual language understanding. The burden of multilingual processing is lifted off V&amp;L model and placed on mLM. Since the multilingual text data is relatively of higher abundance and quality, ICU can facilitate the conquering of language barriers for V&amp;L models. In experiments on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can achieve new state-of-the-art results for five languages, and comparable results for the rest.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.01886</link><description>&lt;p&gt;
&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22312;&#32447;&#25552;&#20379;&#30340;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#20256;&#36882;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#26377;&#25928;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21508;&#31181;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#20063;&#21487;&#20379;&#20844;&#20247;&#20351;&#29992;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#32791;&#26102;&#19988;&#24494;&#35843;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#20250;&#32473;&#23384;&#20648;&#21644;&#26381;&#21153;&#24102;&#26469;&#24040;&#22823;&#36127;&#25285;&#12290;&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#26080;&#38656;&#35757;&#32451;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#23558;&#22810;&#20010;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#37325;&#22797;&#20351;&#29992;&#21040;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#19982;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22823;&#30340;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#26469;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#12290;&#38024;&#23545;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PERU-FFT&#65292;&#36890;&#36807;&#23558;&#31232;&#30095;&#20219;&#21153;&#21521;&#37327;&#27880;&#20837;&#21040;&#19968;&#20010;mer&#27169;&#22411;&#20013;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a mer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#12290;&#38024;&#23545;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;ChatGPT&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#38646; shot/few-shot &#26041;&#24335;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#20173;&#19981;&#29702;&#24819;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#24320;&#28304;LLMs&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#21644;&#22810;&#28304;&#30340;&#35299;&#37322;&#24615;&#24515;&#29702;&#20581;&#24247;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.13567</link><description>&lt;p&gt;
MentaLLaMA&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models. (arXiv:2309.13567v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#12290;&#38024;&#23545;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;ChatGPT&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#38646; shot/few-shot &#26041;&#24335;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#20173;&#19981;&#29702;&#24819;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#24320;&#28304;LLMs&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#21644;&#22810;&#28304;&#30340;&#35299;&#37322;&#24615;&#24515;&#29702;&#20581;&#24247;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#27491;&#22312;&#25104;&#20026;&#33258;&#21160;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#20016;&#23500;&#25968;&#25454;&#28304;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#21028;&#21035;&#26041;&#27861;&#23384;&#22312;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#26368;&#36817;&#24320;&#22987;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#19978;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#21644;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#35299;&#37322;&#30340;&#27491;&#30830;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#38646; shot/few-shot &#26041;&#24335;&#19979;&#20173;&#28982;&#23454;&#29616;&#20102;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#39046;&#22495;&#29305;&#23450;&#30340;&#24494;&#35843;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;2&#65289;&#27809;&#26377;&#21457;&#24067;&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#24320;&#28304; LLMs &#20197;&#38477;&#20302;&#24494;&#35843;&#25104;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#21644;&#22810;&#28304;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#25351;&#23548; (IMHI) &#25968;&#25454;&#38598;&#65292;&#21253;&#21547;105K&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of web technology, social media texts are becoming a rich source for automatic mental health analysis. As traditional discriminative methods bear the problem of low interpretability, the recent large language models have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions. The results show that ChatGPT can generate approaching-human explanations for its correct classifications. However, LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner. Domain-specific finetuning is an effective solution, but faces 2 challenges: 1) lack of high-quality training data. 2) no open-source LLMs for interpretable mental health analysis were released to lower the finetuning cost. To alleviate these problems, we build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset on social media, with 105K data samples. The raw socia
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#25913;&#36827;&#35828;&#35805;&#32773;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21475;&#35821;&#29702;&#35299;&#27169;&#22359;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#26500;&#24314;&#25104;&#23545;&#32422;&#26463;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#35828;&#35805;&#32773;&#20998;&#31163;&#27969;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10456</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#25913;&#36827;&#35828;&#35805;&#32773;&#20998;&#31163;: &#21033;&#29992;&#32852;&#21512;&#25104;&#23545;&#32422;&#26463;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Improving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation. (arXiv:2309.10456v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#25913;&#36827;&#35828;&#35805;&#32773;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21475;&#35821;&#29702;&#35299;&#27169;&#22359;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#26500;&#24314;&#25104;&#23545;&#32422;&#26463;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#35828;&#35805;&#32773;&#20998;&#31163;&#27969;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#32773;&#20998;&#31163;&#24050;&#32463;&#24341;&#36215;&#20102;&#35821;&#38899;&#22788;&#29702;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20027;&#27969;&#30340;&#35828;&#35805;&#32773;&#20998;&#31163;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#22768;&#38899;&#20449;&#21495;&#20013;&#25552;&#21462;&#30340;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#29305;&#24449;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#35821;&#20041;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#32771;&#34385;&#21040;&#35821;&#38899;&#20449;&#21495;&#33021;&#22815;&#26377;&#25928;&#20256;&#36798;&#35821;&#38899;&#30340;&#20869;&#23481;&#65292;&#25105;&#20204;&#26377;&#20852;&#36259;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#35821;&#20041;&#32447;&#32034;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#35828;&#35805;&#32773;&#20998;&#31163;&#31995;&#32479;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#21475;&#35821;&#29702;&#35299;&#27169;&#22359;&#26469;&#25552;&#21462;&#19982;&#35828;&#35805;&#32773;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26500;&#24314;&#25104;&#23545;&#32422;&#26463;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#23558;&#36825;&#20123;&#32422;&#26463;&#38598;&#25104;&#21040;&#35828;&#35805;&#32773;&#20998;&#31163;&#27969;&#31243;&#20013;&#65292;&#25552;&#39640;&#25972;&#20010;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#19968;&#33268;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker diarization has gained considerable attention within speech processing research community. Mainstream speaker diarization rely primarily on speakers' voice characteristics extracted from acoustic signals and often overlook the potential of semantic information. Considering the fact that speech signals can efficiently convey the content of a speech, it is of our interest to fully exploit these semantic cues utilizing language models. In this work we propose a novel approach to effectively leverage semantic information in clustering-based speaker diarization systems. Firstly, we introduce spoken language understanding modules to extract speaker-related semantic information and utilize these information to construct pairwise constraints. Secondly, we present a novel framework to integrate these constraints into the speaker diarization pipeline, enhancing the performance of the entire system. Extensive experiments conducted on the public dataset demonstrate the consistent superiori
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.09357</link><description>&lt;p&gt;
Talk2Care: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20419;&#36827;&#24322;&#27493;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#36828;&#31243;&#21307;&#30103;&#24212;&#29992;&#31243;&#24207;&#26469;&#24110;&#21161;&#23478;&#24237;&#20013;&#30340;&#32769;&#24180;&#20154;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#65292;&#20294;&#22522;&#26412;&#30340;&#28040;&#24687;&#21644;&#30005;&#35805;&#20173;&#28982;&#26159;&#26368;&#24120;&#35265;&#30340;&#36890;&#20449;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#24615;&#12289;&#20449;&#24687;&#20002;&#22833;&#21644;&#27969;&#31243;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#20419;&#36827;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21450;&#20854;&#24378;&#22823;&#30340;&#33258;&#28982;&#23545;&#35805;&#21644;&#25688;&#35201;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLMs&#22312;&#36890;&#20449;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#36824;&#23384;&#22312;&#26377;&#38480;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#39033;&#35775;&#35848;&#30740;&#31350;&#65292;&#20998;&#21035;&#19982;&#32769;&#24180;&#20154;(N=10)&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;(N=9)&#36827;&#34892;&#20102;&#20132;&#27969;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#22312;&#24739;&#32773;-&#21307;&#29983;&#24322;&#27493;&#36890;&#20449;&#20013;&#23545;LLMs&#30340;&#38656;&#27714;&#21644;&#26426;&#20250;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;Talk2Care&#65292;&#24182;&#20026;&#20004;&#20010;&#32676;&#20307;&#35774;&#35745;&#20102;&#20132;&#20114;&#32452;&#20214;: (1) &#23545;&#20110;&#32769;&#24180;&#20154;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#38899;&#21161;&#25163;&#30340;&#20415;&#21033;&#24615;&#21644;&#26131;&#20110;&#33719;&#21462;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#33258;&#35757;&#32451;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#31574;&#30053;&#21644;&#36229;&#21442;&#25968;&#23545;&#33258;&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.08777</link><description>&lt;p&gt;
&#33258;&#35757;&#32451;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Instance Selection Strategies in Self-training for Sentiment Analysis. (arXiv:2309.08777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#33258;&#35757;&#32451;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#31574;&#30053;&#21644;&#36229;&#21442;&#25968;&#23545;&#33258;&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#28041;&#21450;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#21644;&#25552;&#21462;&#20027;&#35266;&#24773;&#24863;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#33258;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#24320;&#21457;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#33258;&#35757;&#32451;&#36807;&#31243;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#30340;&#36873;&#25321;&#65292;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#26412;&#25991;&#23545;&#33258;&#35757;&#32451;&#30340;&#21508;&#31181;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#22312;&#20004;&#20010;&#20844;&#24320;&#24773;&#24863;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#30740;&#31350;&#20102;&#31574;&#30053;&#21644;&#36229;&#21442;&#25968;&#22312;&#21508;&#31181;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#23545;&#33258;&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is a crucial task in natural language processing that involves identifying and extracting subjective sentiment from text. Self-training has recently emerged as an economical and efficient technique for developing sentiment analysis models by leveraging a small amount of labeled data and a larger amount of unlabeled data. However, the performance of a self-training procedure heavily relies on the choice of the instance selection strategy, which has not been studied thoroughly. This paper presents an empirical study on various instance selection strategies for self-training on two public sentiment datasets, and investigates the influence of the strategy and hyper-parameters on the performance of self-training in various few-shot settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#65292;&#36890;&#36807;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#23545;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#35843;&#25972;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#21644;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.07794</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks. (arXiv:2309.07794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#65292;&#36890;&#36807;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#23545;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#35843;&#25972;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#21644;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#23545;&#24773;&#24863;&#20998;&#26512;&#12289;&#35773;&#21050;&#26816;&#27979;&#21644;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#31561;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21305;&#37197;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23384;&#22312;&#38544;&#34255;&#25110;&#20114;&#34917;&#20449;&#24687;&#30340;&#29420;&#29305;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22312;&#24494;&#35843;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26102;&#32852;&#21512;&#20351;&#29992;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#26469;&#30452;&#25509;&#24314;&#27169;&#36825;&#19968;&#38382;&#39064;&#12290;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#65288;ITC&#65289;&#23558;&#19968;&#31687;&#24086;&#23376;&#30340;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26356;&#21152;&#38752;&#36817;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#24086;&#23376;&#20998;&#31163;&#24320;&#26469;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#12290;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;ITM&#65289;&#36890;&#36807;&#24809;&#32602;&#19981;&#30456;&#20851;&#30340;&#23545;&#26469;&#20419;&#36827;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#30446;&#26631;&#19982;&#20116;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#22312;&#22235;&#20010;&#28909;&#38376;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#19968;&#33268;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively leveraging multimodal information from social media posts is essential to various downstream tasks such as sentiment analysis, sarcasm detection and hate speech classification. However, combining text and image information is challenging because of the idiosyncratic cross-modal semantics with hidden or complementary information present in matching image-text pairs. In this work, we aim to directly model this by proposing the use of two auxiliary losses jointly with the main task when fine-tuning any pre-trained multimodal model. Image-Text Contrastive (ITC) brings image-text representations of a post closer together and separates them from different posts, capturing underlying dependencies. Image-Text Matching (ITM) facilitates the understanding of semantic correspondence between images and text by penalizing unrelated pairs. We combine these objectives with five multimodal models, demonstrating consistent improvements across four popular social media datasets. Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SIB-200&#25968;&#25454;&#38598;&#65292;&#22312;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#30340;&#20027;&#39064;&#20998;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#39046;&#22495;&#20013;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#36890;&#36807;&#20840;&#30417;&#30563;&#12289;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#24615;&#33021;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.07445</link><description>&lt;p&gt;
SIB-200: &#21253;&#25324;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#30340;&#31616;&#21333;&#12289;&#20840;&#38754;&#21644;&#22823;&#22411;&#20027;&#39064;&#20998;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects. (arXiv:2309.07445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SIB-200&#25968;&#25454;&#38598;&#65292;&#22312;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#30340;&#20027;&#39064;&#20998;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#39046;&#22495;&#20013;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#36890;&#36807;&#20840;&#30417;&#30563;&#12289;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#24615;&#33021;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#36890;&#24120;&#20165;&#38480;&#20110;&#19968;&#23567;&#37096;&#20998;&#24102;&#26377;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#65292;&#25490;&#38500;&#20102;&#35768;&#22810;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#26412;&#25991;&#21019;&#24314;&#20102;SIB-200&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20027;&#39064;&#20998;&#31867;&#30340;&#22823;&#35268;&#27169;&#24320;&#25918;&#28304;&#20195;&#30721;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#65292;&#20197;&#24357;&#34917;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#32570;&#20047;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;SIB-200&#20013;&#28085;&#30422;&#30340;&#35768;&#22810;&#35821;&#35328;&#26469;&#35828;&#65292;&#36825;&#26159;&#39318;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;NLU&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Flores-200&#26426;&#22120;&#32763;&#35793;&#35821;&#26009;&#24211;&#65292;&#24182;&#23545;&#35813;&#35821;&#26009;&#24211;&#28085;&#30422;&#30340;&#20854;&#20182;203&#31181;&#35821;&#35328;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#27880;&#37322;&#12290;&#23613;&#31649;&#35813;&#20219;&#21153;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#22312;&#20840;&#30417;&#30563;&#35774;&#32622;&#12289;&#36328;&#35821;&#35328;&#36801;&#31227;&#35774;&#32622;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#32622;&#19979;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#24615;&#33021;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the progress we have recorded in the last few years in multilingual natural language processing, evaluation is typically limited to a small set of languages with available datasets which excludes a large number of low-resource languages. In this paper, we created SIB-200 -- a large-scale open-sourced benchmark dataset for topic classification in 200 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 203 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performa
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#30340;&#25490;&#24207;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.06991</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Contrast-Consistent Ranking with Language Models. (arXiv:2309.06991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06991
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#30340;&#25490;&#24207;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#22522;&#20110;&#25490;&#24207;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#26159;&#22788;&#29702;&#19978;&#19979;&#25991;&#25490;&#21517;&#20219;&#21153;&#30340;&#24378;&#22823;&#35299;&#20915;&#32773;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#37197;&#23545;&#12289;&#28857;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#25490;&#24207;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#20180;&#32454;&#26657;&#20934;&#21644;&#38480;&#21046;&#35299;&#30721;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#22312;&#20135;&#29983;&#30340;&#25490;&#24207;&#20013;&#20063;&#19981;&#24635;&#26159;&#33258;&#27965;&#30340;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#19968;&#31181;&#21463;&#26080;&#30417;&#30563;&#25506;&#27979;&#26041;&#27861;Contrast-Consistent Search&#65288;CCS&#65289;&#21551;&#21457;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65306;&#27169;&#22411;&#23545;&#19968;&#20010;&#35821;&#21477;&#21450;&#20854;&#21542;&#23450;&#30340;&#34920;&#31034;&#24517;&#39035;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#12290;&#25105;&#20204;&#20551;&#35774;&#31867;&#20284;&#30340;&#32422;&#26463;&#36866;&#29992;&#20110;&#25152;&#26377;&#39033;&#36890;&#36807;&#19968;&#33268;&#24615;&#23545;&#30456;&#20851;&#25490;&#24207;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pair
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06364</link><description>&lt;p&gt;
&#22522;&#20110;&#26694;&#26550;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#30001;&#22238;&#31572;&#30340;&#23450;&#24615;&#20998;&#26512;&#65306;&#31639;&#27861;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#33258;&#30001;&#22238;&#31572;&#38754;&#35797;&#38382;&#39064;&#65292;&#23601;&#20687;&#20256;&#32479;&#19978;&#20351;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#20998;&#26512;&#30340;&#37027;&#26679;&#12290;&#23450;&#24615;&#26041;&#27861;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#28041;&#21450;&#23545;&#24320;&#25918;&#24335;&#35775;&#35848;&#25110;&#33258;&#30001;&#36827;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#30340;&#25163;&#21160;&#20998;&#26512;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#23450;&#24615;&#26041;&#27861;&#23545;LLMs&#29983;&#25104;&#30340;"&#30789;&#21442;&#19982;&#32773;"&#36827;&#34892;&#30740;&#31350;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#32676;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#27010;&#24565;&#26159;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#36825;&#26159;&#30001;Argyle&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24341;&#20837;&#30340;&#19968;&#20010;&#26415;&#35821;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20122;&#32676;&#20307;&#30340;&#20449;&#24565;&#21644;&#24577;&#24230;&#30340;&#31243;&#24230;&#30456;&#21563;&#21512;&#12290;&#26681;&#25454;&#23450;&#20041;&#65292;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#34920;&#26126;&#20174;LLMs&#20013;&#25552;&#21462;&#30340;&#28508;&#22312;&#20449;&#24565;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#65292;&#32780;&#20302;&#31639;&#27861;&#20445;&#30495;&#24230;&#21017;&#20351;&#24471;&#36825;&#26679;&#30340;&#30740;&#31350;&#26080;&#25928;&#12290;&#26412;&#25991;&#20351;&#29992;LLM&#29983;&#25104;&#38754;&#35797;&#38382;&#31572;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Today, using Large-scale generative Language Models (LLMs) it is possible to simulate free responses to interview questions like those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial "silicon participants" generated by LLMs may be productively studied using qualitative methods aiming to produce insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023) capturing the degree to which LLM-generated outputs mirror human sub-populations' beliefs and attitudes. By definition, high algorithmic fidelity suggests latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#20840;&#38754;&#30340;Twitter&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21442;&#19982;&#38452;&#35851;&#30456;&#20851;&#27963;&#21160;&#30340;&#29992;&#25143;&#30340;&#29305;&#28857;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#20026;&#38452;&#35851;&#35770;&#30340;&#26816;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.15154</link><description>&lt;p&gt;
&#38452;&#35851;&#32773;&#30340;&#35299;&#21078;&#23398;&#65306;&#25581;&#31034;&#24615;&#26684;&#29305;&#28857;&#30340;&#20840;&#38754;Twitter&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Anatomy of Conspirators: Unveiling Traits using a Comprehensive Twitter Dataset. (arXiv:2308.15154v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#20840;&#38754;&#30340;Twitter&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21442;&#19982;&#38452;&#35851;&#30456;&#20851;&#27963;&#21160;&#30340;&#29992;&#25143;&#30340;&#29305;&#28857;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#20026;&#38452;&#35851;&#35770;&#30340;&#26816;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20805;&#26021;&#30528;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#22823;&#37327;&#38169;&#35823;&#20449;&#24687;&#20013;&#65292;&#20851;&#20110;&#38452;&#35851;&#35770;&#30340;&#35752;&#35770;&#27491;&#22312;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#38452;&#35851;&#35770;&#65292;&#24448;&#24448;&#20381;&#36182;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;2022&#24180;&#20840;&#24180;&#28041;&#21450;&#38452;&#35851;&#30456;&#20851;&#27963;&#21160;&#30340;Twitter&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30528;&#37325;&#20110;&#29420;&#31435;&#20110;&#29305;&#23450;&#38452;&#35851;&#35770;&#21644;&#20449;&#24687;&#25805;&#20316;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#19968;&#20010;&#23545;&#29031;&#32452;&#65292;&#20854;&#20013;&#38543;&#26426;&#36873;&#25321;&#29992;&#25143;&#21487;&#20197;&#19982;&#28041;&#21450;&#38452;&#35851;&#27963;&#21160;&#30340;&#20010;&#20307;&#36827;&#34892;&#20844;&#27491;&#27604;&#36739;&#12290;&#36825;&#27425;&#20840;&#38754;&#30340;&#25910;&#38598;&#24037;&#20316;&#24635;&#20849;&#24471;&#21040;&#20102;15K&#20010;&#36134;&#25143;&#21644;&#20174;&#20182;&#20204;&#30340;&#26102;&#38388;&#32447;&#20013;&#25552;&#21462;&#30340;37M&#26465;&#25512;&#25991;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#32676;&#20307;&#22312;&#20027;&#39064;&#12289;&#20010;&#20154;&#36164;&#26009;&#21644;&#34892;&#20026;&#29305;&#24449;&#36825;&#19977;&#20010;&#32500;&#24230;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38452;&#35851;&#21644;
&lt;/p&gt;
&lt;p&gt;
The discourse around conspiracy theories is currently thriving amidst the rampant misinformation prevalent in online environments. Research in this field has been focused on detecting conspiracy theories on social media, often relying on limited datasets. In this study, we present a novel methodology for constructing a Twitter dataset that encompasses accounts engaged in conspiracy-related activities throughout the year 2022. Our approach centers on data collection that is independent of specific conspiracy theories and information operations. Additionally, our dataset includes a control group comprising randomly selected users who can be fairly compared to the individuals involved in conspiracy activities. This comprehensive collection effort yielded a total of 15K accounts and 37M tweets extracted from their timelines. We conduct a comparative analysis of the two groups across three dimensions: topics, profiles, and behavioral characteristics. The results indicate that conspiracy and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12420</link><description>&lt;p&gt;
ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65306;&#23545;&#25991;&#29486;&#36827;&#34892;NLP&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;(DLT)&#36805;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#20854;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;DLT&#30340;&#29615;&#22659;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#27835;&#29702;(ESG)&#32452;&#25104;&#37096;&#20998;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#36824;&#19981;&#36275;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;107&#31687;&#31181;&#23376;&#25991;&#29486;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;63,083&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#24341;&#29992;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#31934;&#28860;&#20026;24,539&#31687;&#25991;&#29486;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#20998;&#31867;&#27861;&#20174;46&#31687;&#35770;&#25991;&#20013;&#26631;&#35760;&#20102;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#25214;&#20986;DLT&#30340;ESG&#35201;&#32032;&#26469;&#23436;&#21892;&#36825;&#20010;&#20998;&#31867;&#27861;&#12290;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#35843;&#25972;&#65292;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#20351;&#29992;&#25105;&#20204;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#35843;&#25972;&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#31934;&#31616;&#65292;&#24471;&#21040;&#20102;505&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#21644;&#26102;&#38388;&#22270;&#20998;&#26512;&#65292;&#20419;&#36827;&#20102;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#28436;&#21270;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#22823;&#22411;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12038</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. (arXiv:2308.12038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#22823;&#22411;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#65292;&#22810;&#27169;&#24335;&#23398;&#20064;&#20986;&#29616;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#25104;&#21151;&#36890;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#65292;&#20854;&#20182;&#35821;&#35328;&#21017;&#30456;&#23545;&#33853;&#21518;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#26500;&#24314;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#23545;&#24212;&#29289;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38750;&#33521;&#35821;&#22810;&#27169;&#24335;&#25968;&#25454;&#20855;&#26377;&#20302;&#36164;&#28304;&#29305;&#24615;&#65288;&#21363;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPM&#65292;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#22823;&#22411;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#26377;&#25928;&#35757;&#32451;&#33539;&#20363;&#12290;MPM&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20165;&#22312;&#33521;&#35821;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#27169;&#22411;&#21487;&#20197;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#29992;&#20110;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#29978;&#33267;&#36229;&#36807;&#22312;&#26412;&#22320;&#35821;&#35328;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20197;&#20013;&#25991;&#20316;&#20026;MPM&#23454;&#36341;&#30340;&#19968;&#20010;&#32451;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11767</link><description>&lt;p&gt;
&#25552;&#39640;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#26816;&#27979;&#30340;&#26041;&#27861;&#65306;&#24341;&#20837;xFakeBibs&#30417;&#30563;&#23398;&#20064;&#32593;&#32476;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#27491;&#22312;&#25104;&#20026;&#29616;&#23454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21306;&#20998;ChatGPT&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21644;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;100&#20010;&#30495;&#23454;&#20986;&#29256;&#29289;&#25688;&#35201;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#37319;&#29992;10&#20493;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#25509;&#21463;&#33539;&#22260;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#19982;ChatGPT&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#26126;&#26174;&#21487;&#35265;ChatGPT&#20165;&#36129;&#29486;&#20102;23\%&#30340;&#20108;&#20803;&#32452;&#20869;&#23481;&#65292;&#36825;&#27604;&#20854;&#20182;10&#20010;&#20132;&#21449;&#39564;&#35777;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#37117;&#23569;50\%&#12290;&#36825;&#20010;&#20998;&#26512;&#20984;&#26174;&#20102;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#19978;&#19982;&#30495;&#23454;&#31185;&#23398;&#30340;&#26126;&#26174;&#24046;&#24322;&#12290;&#22312;&#23545;&#27599;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;xFakeBibs&#31639;&#27861;&#20934;&#30830;&#22320;&#23558;98&#31687;&#20986;&#29256;&#29289;&#35782;&#21035;&#20026;&#20551;&#30340;&#65292;&#26377;2&#31687;&#25991;&#29486;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#30495;&#23454;&#20986;&#29256;&#29289;&#12290;&#23613;&#31649;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#25991;&#31456;&#26088;&#22312;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#20197;&#21450;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05502</link><description>&lt;p&gt;
&#23558;&#39034;&#24207;&#24102;&#20837;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#27861;&#24459;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bringing order into the realm of Transformer-based language models for artificial intelligence and law. (arXiv:2308.05502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#25991;&#31456;&#26088;&#22312;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#20197;&#21450;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;TLM&#65289;&#34987;&#24191;&#27867;&#35748;&#21487;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#25104;&#21151;&#24320;&#21457;&#20986;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#21644;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#25991;&#26412;&#39046;&#22495;&#19968;&#26679;&#65292;TLM&#30830;&#23454;&#25512;&#21160;&#20102;&#27861;&#24459;&#39046;&#22495;&#35768;&#22810;&#24863;&#20852;&#36259;&#20219;&#21153;&#23545;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23613;&#31649;&#31532;&#19968;&#20010;Transformer&#27169;&#22411;&#25552;&#20986;&#20102;&#22823;&#32422;6&#24180;&#26102;&#38388;&#65292;&#20294;&#36825;&#39033;&#25216;&#26415;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#36805;&#29467;&#21457;&#23637;&#65292;BERT&#21644;&#30456;&#20851;&#27169;&#22411;&#25104;&#20026;&#20027;&#35201;&#21442;&#32771;&#65292;&#20063;&#22312;&#27861;&#24459;&#39046;&#22495;&#21344;&#26377;&#37325;&#35201;&#22320;&#20301;&#12290;&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#27010;&#36848;&#20102;TLM&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#31361;&#20986;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#20197;&#20415;&#19968;&#26041;&#38754;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#21462;&#24471;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#26159;&#20160;&#20040;&#65292;&#21478;&#19968;&#26041;&#38754;&#20102;&#35299;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#26159;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20113;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#30340;&#20316;&#26354;&#36741;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.04215</link><description>&lt;p&gt;
&#23454;&#26102;&#20316;&#26354;&#36741;&#21161;&#30340;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04215
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20113;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#30340;&#20316;&#26354;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#22312;&#25552;&#21319;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#25972;&#21512;&#31169;&#20154;&#25968;&#25454;&#21644;&#20943;&#23569;&#24187;&#35273;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#20110;&#38656;&#35201;&#23454;&#26102;&#21709;&#24212;&#30340;&#20219;&#21153;&#65288;&#22914;&#20316;&#26354;&#36741;&#21161;&#65289;&#26102;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#22788;&#29702;&#26102;&#38388;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hybrid Retrieval-Augmented Generation (HybridRAG)&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#23558;&#23458;&#25143;&#31471;&#27169;&#22411;&#21644;&#20113;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#28151;&#21512;&#35774;&#32622;&#12290;HybridRAG&#36890;&#36807;&#24322;&#27493;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20113;&#31471;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#25928;&#30340;&#21709;&#24212;&#65292;&#20174;LLM&#30340;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24322;&#27493;&#20869;&#23384;&#38598;&#25104;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#33021;&#22815;&#23454;&#26102;&#21709;&#24212;&#29992;&#25143;&#35831;&#27714;&#65292;&#26080;&#38656;&#31561;&#24453;&#20113;&#31471;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmented models show promise in enhancing traditional language models by improving their contextual understanding, integrating private data, and reducing hallucination. However, the processing time required for retrieval augmented large language models poses a challenge when applying them to tasks that require real-time responses, such as composition assistance.  To overcome this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework that leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to 
&lt;/p&gt;</description></item><item><title>DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.10172</link><description>&lt;p&gt;
DialogStudio&#65306;&#38754;&#21521;&#20250;&#35805; AI &#30340;&#26368;&#20016;&#23500;&#21644;&#26368;&#22810;&#26679;&#21270;&#30340;&#32479;&#19968;&#25968;&#25454;&#38598;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. (arXiv:2307.10172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10172
&lt;/p&gt;
&lt;p&gt;
DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20250;&#35805; AI &#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#20219;&#21153;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#20840;&#38754;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; DialogStudio&#65306;&#26368;&#22823;&#12289;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#20197;&#19968;&#33268;&#30340;&#26684;&#24335;&#32479;&#19968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#21407;&#22987;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#38598;&#21512;&#21253;&#25324;&#26469;&#33258;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12289;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#65292;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#38750;&#24120;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378; DialogStudio &#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30830;&#23450;&#20102;&#35768;&#21487;&#35777;&#65292;&#24182;&#20026;&#36873;&#23450;&#23545;&#35805;&#35774;&#35745;&#20102;&#39046;&#22495;&#24863;&#30693;&#25552;&#31034;&#65292;&#20197;&#20415;&#20419;&#36827;&#25351;&#23548;&#24863;&#30693;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#38598;&#38598;&#21512;&#24320;&#21457;&#20102;&#20250;&#35805; AI &#27169;&#22411;&#65292;&#24182;&#22312;&#38646;&#25688;&#35201;&#29983;&#25104;&#21644;&#20998;&#24067;&#24335;&#25991;&#23383;&#22522;&#20934;&#23545;&#35805;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training. To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#23398;&#21160;&#26426;&#30340;&#20998;&#35789;&#26041;&#26696;MorphPiece&#65292;&#24182;&#20351;&#29992;&#35813;&#26041;&#26696;&#35757;&#32451;&#20102;&#19968;&#20010;&#31216;&#20026;MorphGPT&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;MorphGPT&#22312;&#35821;&#35328;&#24314;&#27169;&#20197;&#21450;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07262</link><description>&lt;p&gt;
MorphPiece: &#36828;&#31163;&#32479;&#35745;&#35821;&#35328;&#34920;&#31034;&#30340;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
MorphPiece : Moving away from Statistical Language Representation. (arXiv:2307.07262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#23398;&#21160;&#26426;&#30340;&#20998;&#35789;&#26041;&#26696;MorphPiece&#65292;&#24182;&#20351;&#29992;&#35813;&#26041;&#26696;&#35757;&#32451;&#20102;&#19968;&#20010;&#31216;&#20026;MorphGPT&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;MorphGPT&#22312;&#35821;&#35328;&#24314;&#27169;&#20197;&#21450;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35789;&#26159;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24403;&#20195;&#20998;&#35789;&#22120;&#22522;&#20110;&#23545;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#23545;&#35821;&#35328;&#29305;&#24449;&#30340;&#32771;&#34385;&#36739;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#23398;&#21160;&#26426;&#30340;&#20998;&#35789;&#26041;&#26696;MorphPiece&#65292;&#37096;&#20998;&#22522;&#20110;&#24213;&#23618;&#25991;&#26412;&#30340;&#24418;&#24577;&#20998;&#21106;&#12290;&#20351;&#29992;&#35813;&#20998;&#35789;&#22120;&#65288;&#31216;&#20026;MorphGPT&#65289;&#35757;&#32451;&#30340;&#31867;GPT&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#20986;&#27604;&#22312;&#26631;&#20934;BPE&#20998;&#35789;&#22120;&#19978;&#35757;&#32451;&#26102;&#26356;&#20248;&#36234;&#30340;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#35268;&#27169;&#22823;6&#20493;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#19979;&#23545;MorphGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#22312;&#21508;&#20010;&#26041;&#38754;&#19982;GPT-2&#27169;&#22411;&#30456;&#27604;&#26377;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tokenization is a critical part of modern NLP pipelines. However, contemporary tokenizers for Large Language Models are based on statistical analysis of text corpora, without much consideration to the linguistic features. We propose a linguistically motivated tokenization scheme, MorphPiece, which is based partly on morphological segmentation of the underlying text. A GPT-style causal language model trained on this tokenizer (called MorphGPT) shows superior convergence compared to the same architecture trained on a standard BPE tokenizer. Specifically we get Language Modeling performance comparable to a 6 times larger model. Additionally, we evaluate MorphGPT on a variety of NLP tasks in supervised and unsupervised settings and find superior performance across the board, compared to GPT-2 model.
&lt;/p&gt;</description></item><item><title>DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.06924</link><description>&lt;p&gt;
DRAGON: &#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#24102;&#26377;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#30340;&#36741;&#21161;&#23548;&#33322;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06924
&lt;/p&gt;
&lt;p&gt;
DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#29702;&#35299;&#21644;&#23548;&#33322;&#21608;&#22260;&#31354;&#38388;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#23548;&#33322;&#25216;&#26415;&#35201;&#20040;&#21482;&#20851;&#27880;&#23548;&#33322;&#65292;&#35201;&#20040;&#25552;&#20379;&#26377;&#38480;&#30340;&#20851;&#20110;&#29615;&#22659;&#30340;&#27807;&#36890;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#21644;&#35821;&#20041;&#23548;&#33322;&#26041;&#38754;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGON&#65292;&#19968;&#31181;&#30001;&#23545;&#35805;&#31995;&#32479;&#39537;&#21160;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#24182;&#20855;&#26377;&#23558;&#29615;&#22659;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;DRAGON&#33021;&#22815;&#24341;&#23548;&#29992;&#25143;&#21040;&#22320;&#22270;&#19978;&#30340;&#30446;&#26631;&#22320;&#26631;&#65292;&#25551;&#36848;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#23519;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#23545;&#35805;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#33258;&#30001;&#24418;&#24335;&#25551;&#36848;&#19982;&#29615;&#22659;&#20013;&#30340;&#22320;&#26631;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#25552;&#20379;&#35821;&#20041;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#26085;&#24120;&#23460;&#20869;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#30450;&#30446;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DRAGON&#33021;&#22815;&#19982;&#29992;&#25143;&#39034;&#30021;&#22320;&#27807;&#36890;&#65292;
&lt;/p&gt;
&lt;p&gt;
Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FACTOR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20107;&#23454;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#19981;&#27491;&#30830;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#22312;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#26102;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#12290;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#19981;&#24635;&#26159;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2307.06908</link><description>&lt;p&gt;
&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Generating Benchmarks for Factuality Evaluation of Language Models. (arXiv:2307.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06908
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FACTOR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20107;&#23454;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#19981;&#27491;&#30830;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#22312;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#26102;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#12290;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#19981;&#24635;&#26159;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#37096;&#32626;&#21040;&#29305;&#23450;&#39046;&#22495;&#20043;&#21069;&#65292;&#34913;&#37327;&#20854;&#22312;&#35813;&#39046;&#22495;&#20013;&#29983;&#25104;&#20107;&#23454;&#38169;&#35823;&#20449;&#24687;&#30340;&#20542;&#21521;&#24456;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#29983;&#25104;&#35780;&#20272;&#26041;&#27861;&#38598;&#20013;&#20110;&#20174;LM&#33258;&#36523;&#20013;&#37319;&#26679;&#30340;&#20107;&#23454;&#65292;&#22240;&#27492;&#26080;&#27861;&#25511;&#21046;&#35780;&#20272;&#20107;&#23454;&#30340;&#38598;&#21512;&#65292;&#24182;&#19988;&#21487;&#33021;&#20302;&#20272;&#20102;&#32597;&#35265;&#21644;&#19981;&#22826;&#21487;&#33021;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FACTOR&#65306;&#36890;&#36807;&#35821;&#26009;&#24211;&#21464;&#25442;&#36827;&#34892;&#20107;&#23454;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;LM&#30340;&#20107;&#23454;&#24615;&#12290;FACTOR&#20250;&#33258;&#21160;&#23558;&#24863;&#20852;&#36259;&#30340;&#20107;&#23454;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;LM&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#31867;&#20284;&#20294;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#21019;&#24314;&#20102;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;Wiki-FACTOR&#21644;News-FACTOR&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#24182;&#19988;&#24403;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65307;&#65288;ii&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#19982;&#22256;&#24785;&#24230;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#36825;&#20004;&#20010;&#25351;&#26631;&#22312;&#27169;&#22411;&#25490;&#24207;&#19978;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#24403;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#26356;&#33021;&#20934;&#30830;&#21453;&#26144;LM&#30340;&#20107;&#23454;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score 
&lt;/p&gt;</description></item><item><title>SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.05591</link><description>&lt;p&gt;
SITTA: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05591
&lt;/p&gt;
&lt;p&gt;
SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#29983;&#25104;&#36866;&#24403;&#30340;&#25551;&#36848;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#38656;&#35201;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35780;&#20272;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#30693;&#35782;&#34920;&#31034;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#30340;&#21516;&#26102;&#23454;&#29616;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34987;&#26465;&#20214;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;-&#25991;&#26412;&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#36755;&#20837;&#12290;&#36825;&#35201;&#27714;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#20256;&#36882;&#32473;LM&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#22810;&#27169;&#24577;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
&lt;/p&gt;</description></item><item><title>LLaVAR&#26159;&#19968;&#20010;&#22686;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#23427;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22312;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.17107</link><description>&lt;p&gt;
LLaVAR:&#22686;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#29992;&#20110;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding. (arXiv:2306.17107v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17107
&lt;/p&gt;
&lt;p&gt;
LLaVAR&#26159;&#19968;&#20010;&#22686;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#23427;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22312;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#21487;&#20197;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;&#21253;&#25324;&#22270;&#20687;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#25910;&#38598;&#22270;&#20687;&#25351;&#20196;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#32454;&#33410;&#12290;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#24403;&#21069;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#27969;&#31243;&#65292;&#20351;&#29992;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#65288;&#22914;&#30005;&#24433;&#28023;&#25253;&#12289;&#22270;&#20070;&#23553;&#38754;&#31561;&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;OCR&#24037;&#20855;&#20174;LAION&#25968;&#25454;&#38598;&#30340;422K&#20010;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#19978;&#25552;&#21462;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#35782;&#21035;&#21040;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#26631;&#39064;&#26469;&#21551;&#21160;&#20165;&#25991;&#26412;&#30340;GPT-4&#29983;&#25104;16K&#20010;&#23545;&#35805;&#65292;&#27599;&#20010;&#23545;&#35805;&#21253;&#21547;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#30340;&#38382;&#31572;&#23545;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#25910;&#38598;&#30340;&#25968;&#25454;&#19982;&#20808;&#21069;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#32452;&#21512;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;LLaVAR&#22312;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;VQA&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;LLaVA&#27169;&#22411;&#30340;&#33021;&#21147;&#65288;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;20%&#65289;&#21516;&#26102; achieving an accur
&lt;/p&gt;
&lt;p&gt;
Instruction tuning unlocks the superior capability of Large Language Models (LLM) to interact with humans. Furthermore, recent instruction-following datasets include images as visual inputs, collecting responses for image-based instructions. However, visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first use publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Moreover, we prompt text-only GPT-4 with recognized texts and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multi-modal instruction-following data, our model, LLaVAR, substantially improves the LLaVA model's capability on text-based VQA datasets (up to 20% accuracy improvement) while achieving an accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13588</link><description>&lt;p&gt;
&#31995;&#32479;&#32423;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
System-Level Natural Language Feedback. (arXiv:2306.13588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#29992;&#25143;&#20307;&#39564;&#20449;&#24687;&#12290;&#29616;&#26377;&#30740;&#31350;&#32858;&#28966;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#21453;&#39304;&#29992;&#20110;&#32454;&#21270;&#29305;&#23450;&#20363;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#31995;&#32479;&#33539;&#22260;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#26159;&#36890;&#36807;&#20197;&#19979;&#20004;&#26041;&#38754;&#23454;&#29616;&#30340;&#65306;(i) &#20219;&#21153;&#24230;&#37327;&#35774;&#35745;; (ii) &#29992;&#20110;&#25913;&#36827;&#27169;&#22411;&#21709;&#24212;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#26597;&#35810;&#29983;&#25104;&#21644;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#32452;&#21512;&#24102;&#26469;&#20102;&#36827;&#19968;&#27493;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#23548;&#33268;&#27604;GPT-3.5&#25776;&#20889;&#30340;&#21453;&#39304;&#26356;&#21152;&#25166;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language (NL) feedback contains rich information about the user experience. Existing studies focus on an instance-level approach, where feedback is used to refine specific examples, disregarding its system-wide application. This paper proposes a general framework for unlocking the system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process -- in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query generation and dialog response generation, demonstrating the effectiveness of the use of system-level feedback. We show the combination of system-level feedback and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07629</link><description>&lt;p&gt;
SqueezeLLM&#65306;&#23494;&#38598;&#31232;&#30095;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#35777;&#26126;&#22312;&#24191;&#27867;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#26524;&#12290;&#20294;&#26159;&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#25512;&#29702;&#19968;&#30452;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#29616;&#26377;&#30340;&#37096;&#32626;&#26694;&#26550;&#38656;&#35201;&#20351;&#29992;&#22810;GPU&#25512;&#29702;&#31649;&#36947;&#65292;&#36825;&#36890;&#24120;&#26159;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#20351;&#29992;&#26356;&#23567;&#19988;&#24615;&#33021;&#26356;&#20302;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#20110;LLMs&#29983;&#25104;&#25512;&#26029;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#20869;&#23384;&#24102;&#23485;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#65292;&#23588;&#20854;&#26159;&#21333;&#20010;&#25209;&#27425;&#25512;&#29702;&#12290;&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#20943;&#23569;&#31934;&#24230;&#26469;&#34920;&#31034;&#27169;&#22411;&#26435;&#37325;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#20197;&#21069;&#30340;&#21162;&#21147;&#36890;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;SqueezeLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18466</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18466
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35768;&#22810;&#24037;&#20316;&#37117;&#26088;&#22312;&#22312;&#27979;&#35797;&#26102;&#20174;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#20854;&#26631;&#20934;&#35757;&#32451;&#35774;&#32622;&#23545;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#38656;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#8220;Pile&#8221;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26368;&#36817;&#37051;&#32034;&#24341;&#12290;&#32473;&#23450;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#26816;&#32034;&#26597;&#35810;&#30340;&#37051;&#23621;&#65292;&#24182;&#22312;&#23545;&#24212;&#20110;&#36825;&#20123;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#19978;&#24494;&#35843;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26816;&#32034;&#21644;&#35757;&#32451;&#20165;20&#20010;&#37051;&#23621;&#65292;&#27599;&#20010;&#37051;&#23621;&#20165;&#36827;&#34892;&#19968;&#27425;&#26799;&#24230;&#36845;&#20195;&#65292;&#23601;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#8220;Pile&#8221;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20108;&#21313;&#20010;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26174;&#33879;&#32553;&#23567;&#20102;&#23567;&#22411;GPT2&#27169;&#22411;&#21644;GPTNeo&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21518;&#32773;&#26159;&#19987;&#38376;&#23545;&#8220;Pile&#8221;&#36827;&#34892;&#25910;&#25947;&#35757;&#32451;&#30340;&#65292;&#20307;&#31215;&#21364;&#26159;&#21069;&#32773;&#30340;&#21313;&#20493;&#20197;&#19978;&#12290;&#28982;&#32780;&#65292;&#20854;&#26041;&#27861;&#30340;&#25104;&#21151;&#36824;&#21462;&#20915;&#20110;&#20805;&#20998;&#30340;&#32034;&#24341;&#36136;&#37327;&#21644;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are
&lt;/p&gt;</description></item><item><title>CODET&#26159;&#19968;&#20010;&#23545;&#27604;&#26041;&#35328;&#30340;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22788;&#29702;&#26041;&#35328;&#21464;&#20307;&#26102;&#30340;&#34920;&#29616;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20061;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;882&#20010;&#19981;&#21516;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.17267</link><description>&lt;p&gt;
CODET&#65306;&#26426;&#22120;&#32763;&#35793;&#23545;&#27604;&#26041;&#35328;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CODET: A Benchmark for Contrastive Dialectal Evaluation of Machine Translation. (arXiv:2305.17267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17267
&lt;/p&gt;
&lt;p&gt;
CODET&#26159;&#19968;&#20010;&#23545;&#27604;&#26041;&#35328;&#30340;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22788;&#29702;&#26041;&#35328;&#21464;&#20307;&#26102;&#30340;&#34920;&#29616;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20061;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;882&#20010;&#19981;&#21516;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22788;&#29702;&#28304;&#35821;&#35328;&#30340;&#35821;&#35328;&#21464;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#12290;&#24403;&#38754;&#20020;&#21363;&#20351;&#26159;&#35821;&#35328;&#20351;&#29992;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#65288;&#20363;&#22914;&#19981;&#21516;&#30340;&#39046;&#22495;&#25110;&#30001;&#31532;&#20108;&#35821;&#35328;&#20351;&#29992;&#32773;&#24341;&#20837;&#30340;&#21464;&#20307;&#65289;&#26102;&#65292;&#20854;&#24615;&#33021;&#24448;&#24448;&#20250;&#19979;&#38477;&#12290;&#30452;&#35266;&#19978;&#65292;&#23558;&#36825;&#31181;&#35266;&#23519;&#25512;&#24191;&#21040;&#28085;&#30422;&#26041;&#35328;&#21464;&#20307;&#65292;&#32780;&#20801;&#35768;&#31038;&#21306;&#22312;&#36825;&#20010;&#32500;&#24230;&#19978;&#35780;&#20272;MT&#31995;&#32479;&#30340;&#24037;&#20316;&#26159;&#26377;&#38480;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32534;&#35793;&#21644;&#21457;&#24067;&#20102;&#23545;&#27604;&#26041;&#35328;&#22522;&#20934;&#27979;&#35797; \dataset&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20061;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;882&#20010;&#19981;&#21516;&#21464;&#20307;&#12290;&#25105;&#20204;&#36824;&#22312;&#25968;&#37327;&#19978;&#23637;&#31034;&#20102;&#22823;&#22411;MT&#27169;&#22411;&#22312;&#26377;&#25928;&#32763;&#35793;&#26041;&#35328;&#21464;&#20307;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#24067;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation (NMT) systems exhibit limited robustness in handling source-side linguistic variations. Their performance tends to degrade when faced with even slight deviations in language usage, such as different domains or variations introduced by second-language speakers. It is intuitive to extend this observation to encompass dialectal variations as well, but the work allowing the community to evaluate MT systems on this dimension is limited. To alleviate this issue, we compile and release \dataset, a contrastive dialectal benchmark encompassing 882 different variations from nine different languages. We also quantitatively demonstrate the challenges large MT models face in effectively translating dialectal variants. We are releasing all code and data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#20154;&#21475;&#25968;&#25454;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;</title><link>http://arxiv.org/abs/2305.14195</link><description>&lt;p&gt;
GPT&#31350;&#31455;&#26377;&#22810;&#32769;&#65311;HumBEL&#26694;&#26550;&#36890;&#36807;&#20154;&#32676;&#25968;&#25454;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data. (arXiv:2305.14195v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#20154;&#21475;&#25968;&#25454;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#27169;&#22411;&#30340;&#35821;&#35328;&#20351;&#29992;&#19982;&#29305;&#23450;&#20154;&#32676;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#36825;&#19968;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#27979;&#37327;&#21644;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#19982;&#20154;&#31867;&#23376;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#20511;&#21161;&#35821;&#35328;&#30149;&#29702;&#23398;&#30340;&#20020;&#24202;&#25216;&#26415;&#65292;&#35813;&#23398;&#31185;&#24050;&#32463;&#24314;&#31435;&#20102;&#19981;&#21516;&#65288;&#20154;&#31867;&#65289;&#24180;&#40836;&#38454;&#27573;&#30340;&#35821;&#35328;&#33021;&#21147;&#21457;&#23637;&#35268;&#33539;&#65292;&#23545;&#25216;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#19982;&#39046;&#22495;&#19987;&#23478;&#65288;&#21363;&#25345;&#26377;&#20020;&#24202;&#35768;&#21487;&#35777;&#30340;&#35821;&#35328;&#30149;&#29702;&#23398;&#23478;&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#30340;&#35780;&#20272;&#25216;&#26415;&#20197;&#23454;&#29616;&#35268;&#27169;&#21270;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#30340;&#33021;&#21147;&#22240;&#20219;&#21153;&#32780;&#24322;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;GPT-3.5&#65288;InstructGPT&#65289;&#22312;&#31038;&#20132;&#20132;&#20114;&#20219;&#21153;&#20013;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large pre-trained language models (LMs) find greater use across NLP, existing evaluation protocols do not consider how LM language use aligns with particular human demographic groups, which can be an important consideration in conversational AI applications. To remedy this gap, we consider how LM language skills can be measured and compared to human sub-populations. We suggest clinical techniques from Speech Language Pathology, which has well-established norms for acquisition of language skills, organized by (human) age. We conduct evaluation with a domain expert (i.e., a clinically licensed speech language pathologist), and also propose automated techniques to substitute clinical evaluation at scale. We find LM capability varies widely depending on task with GPT-3.5 mimicking the ability of a typical 6-9 year old at tasks requiring inference about word meanings and simultaneously outperforming a typical 21 year old at memorization. GPT-3.5 (InstructGPT) also has trouble with soc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#20294;&#36890;&#24120;&#19981;&#25104;&#31435;&#30340;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#20998;&#26512;&#26694;&#26550;&#20197;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;NFL&#65288;&#19981;&#35201;&#24536;&#35760;&#20320;&#30340;&#35821;&#35328;&#65289;&#36991;&#20813;&#20102;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13654</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#23569;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20266;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating Spurious Correlations in Text Classification. (arXiv:2305.13654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#20294;&#36890;&#24120;&#19981;&#25104;&#31435;&#30340;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#20998;&#26512;&#26694;&#26550;&#20197;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;NFL&#65288;&#19981;&#35201;&#24536;&#35760;&#20320;&#30340;&#35821;&#35328;&#65289;&#36991;&#20813;&#20102;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#20294;&#36890;&#24120;&#19981;&#25104;&#31435;&#30340;&#20266;&#30456;&#20851;&#24615;&#12290;&#20363;&#22914;&#24773;&#24863;&#20998;&#31867;&#22120;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23398;&#20064;&#21040;&#20196;&#20154;&#24841;&#24742;&#30340;&#30005;&#24433;&#35780;&#35770;&#24635;&#26159;&#19982;&#8220;Spielberg&#8221;&#36825;&#20010;&#35789;&#30456;&#20851;&#32852;&#12290;&#20381;&#36182;&#20110;&#20266;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#27867;&#21270;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#22240;&#27492;&#24212;&#35813;&#36991;&#20813;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#20998;&#26512;&#26694;&#26550;&#26469;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;NFL&#65288;&#19981;&#35201;&#24536;&#35760;&#20320;&#30340;&#35821;&#35328;&#65289;&#65292;&#20197;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#20004;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NFL&#30456;&#23545;&#20110;&#26631;&#20934;&#30340;&#24494;&#35843;&#31639;&#27861;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#32780;&#27809;&#26377;&#29306;&#29298;&#22312;&#25968;&#25454;&#20869;&#37096;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that deep learning models are prone to exploit spurious correlations that are present in the training set, yet may not hold true in general. A sentiment classifier may erroneously learn that the token spielberg is always tied to positive movie reviews. Relying on spurious correlations may lead to significant degradation in generalizability and should be avoided. In this paper, we propose a neighborhood analysis framework to explain how exactly language models exploit spurious correlations. Driven by the analysis, we propose a family of regularization methods, NFL (do Not Forget your Language) to prevent the situation. Experiments on two text classification tasks show that NFL brings a significant improvement over standard fine-tuning in terms of robustness without sacrificing in-distribution accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09859</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27969;&#30021;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#38750;&#24120;&#30456;&#20284;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#35805;&#35821;&#65292;&#22240;&#27492;&#21306;&#20998;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#30340;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#35780;&#35770;&#24182;&#27169;&#20223;&#26576;&#20123;&#20316;&#32773;&#21644;&#20154;&#29289;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340; logits&#65292;&#25110;&#38656;&#35201;&#21487;&#20197;&#20174;&#30446;&#26631;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#31181;&#40657;&#21283;&#23376;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#35266;&#23519;&#21040;&#29983;&#25104;&#25991;&#26412;&#22312;&#29983;&#25104;&#22120;&#30340;&#20284;&#28982;&#20989;&#25968;&#19979;&#26159;&#23616;&#37096;&#26368;&#20248;&#30340;&#65292;&#32780;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21017;&#19981;&#26159;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#36739;&#23567;&#19988;&#37096;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65306;&#23427;&#20204;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#26816;&#27979;&#26469;&#33258;&#23567;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#30456;&#21516;&#30340;&#35821;&#26009;&#24211;&#23545;&#26816;&#27979;&#24615;&#33021;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#21033;&#29992;&#23494;&#20999;&#30456;&#20851;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35789;&#27719;&#30456;&#20284;&#24615;&#65292;&#27880;&#20837;&#22122;&#22768;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#27169;&#22411;&#26356;&#33021;&#25269;&#24481;&#35789;&#27719;&#24046;&#24322;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.05214</link><description>&lt;p&gt;
&#21033;&#29992;&#35789;&#27719;&#30456;&#20284;&#24615;&#23454;&#29616;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages. (arXiv:2305.05214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#21033;&#29992;&#23494;&#20999;&#30456;&#20851;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35789;&#27719;&#30456;&#20284;&#24615;&#65292;&#27880;&#20837;&#22122;&#22768;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#27169;&#22411;&#26356;&#33021;&#25269;&#24481;&#35789;&#27719;&#24046;&#24322;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;LRL&#65289;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;&#20174;&#23494;&#20999;&#30456;&#20851;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;HRL&#65289;&#36827;&#34892;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#23545;&#20110;&#35768;&#22810;&#36825;&#20123;&#35821;&#35328;&#65292;&#27809;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#21487;&#29992;&#65292;&#21363;&#20351;&#26159;&#21333;&#35821;&#26009;&#24211;&#20063;&#24456;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#34920;&#31034;&#20063;&#32570;&#22833;&#12290;&#36825;&#20123;&#22240;&#32032;&#38480;&#21046;&#20102;&#20174;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#19982;&#30456;&#20851;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#20855;&#26377;&#24456;&#39640;&#30340;&#35789;&#27719;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#23646;&#24615;&#65292;&#23558;&#23383;&#31526;&#21644;&#23383;&#31526;&#36328;&#24230;&#30340;&#22122;&#22768;&#27880;&#20837;&#21040;HRL&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#28982;&#21518;&#20877;&#23398;&#20064;&#35789;&#27719;&#34920;&#12290;&#36825;&#20316;&#20026;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#27169;&#22411;&#26356;&#33021;&#25269;&#24481;HRL&#21644;LRL&#20043;&#38388;&#30340;&#35789;&#27719;&#24046;&#24322;&#65292;&#24182;&#26356;&#22909;&#22320;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#22312;&#26469;&#33258;&#22810;&#20010;&#35821;&#35328;&#23478;&#26063;&#30340;&#23494;&#20999;&#30456;&#20851;&#30340;HRL&#21644;LRL&#23545;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the task of machine translation from an extremely low-resource language (LRL) to English using cross-lingual transfer from a closely related high-resource language (HRL). For many of these languages, no parallel corpora are available, even monolingual corpora are limited and representations in pre-trained sequence-to-sequence models are absent. These factors limit the benefits of cross-lingual transfer from shared embedding spaces in multilingual models. However, many extremely LRLs have a high level of lexical similarity with related HRLs. We utilize this property by injecting character and character-span noise into the training data of the HRL prior to learning the vocabulary. This serves as a regularizer which makes the model more robust to lexical divergences between the HRL and LRL and better facilitates cross-lingual transfer. On closely related HRL and LRL pairs from multiple language families, we observe that our method significantly outperforms the baseline MT as we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GPT-4&#21644;ChatGPT&#23545;&#20302;&#36164;&#28304;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#23558;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#25193;&#20805;&#20026;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22312;&#20445;&#30041;&#21407;&#22987;&#26631;&#31614;&#20998;&#24067;&#25110;&#24179;&#34913;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#65292;GPT-4&#21644;ChatGPT&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#33021;&#22815;&#36739;&#22909;&#22320;&#35782;&#21035;&#32597;&#35265;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.13861</link><description>&lt;p&gt;
&#19968;&#20010;&#25552;&#31034;&#21644;&#20960;&#20010;&#31034;&#20363;&#23601;&#36275;&#22815;&#20102;&#21527;&#65311;&#20351;&#29992;GPT-4&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#20998;&#31867;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks. (arXiv:2304.13861v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GPT-4&#21644;ChatGPT&#23545;&#20302;&#36164;&#28304;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#23558;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#25193;&#20805;&#20026;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22312;&#20445;&#30041;&#21407;&#22987;&#26631;&#31614;&#20998;&#24067;&#25110;&#24179;&#34913;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#65292;GPT-4&#21644;ChatGPT&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#33021;&#22815;&#36739;&#22909;&#22320;&#35782;&#21035;&#32597;&#35265;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#20302;&#36164;&#28304;&#39046;&#22495;&#20013;&#65292;&#33719;&#21462;&#21644;&#27880;&#37322;&#25968;&#25454;&#21487;&#33021;&#26159;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#21644;ChatGPT&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#23558;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#25193;&#20805;&#20026;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24212;&#29992;&#20110;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#22797;&#26434;&#31243;&#24230;&#21508;&#24322;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;500&#20010;&#25991;&#26412;&#20316;&#20026;&#22522;&#26412;&#26679;&#26412;&#65292;&#29983;&#25104;&#20102;5,000&#20010;&#26032;&#30340;&#21512;&#25104;&#26679;&#26412;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#22686;&#24378;&#31574;&#30053;&#65306;&#19968;&#31181;&#20445;&#30041;&#21407;&#22987;&#26631;&#31614;&#20998;&#24067;&#65292;&#21478;&#19968;&#31181;&#24179;&#34913;&#20998;&#24067;&#12290;&#20351;&#29992;&#36880;&#27493;&#21464;&#22823;&#30340;&#35757;&#32451;&#26679;&#26412;&#37327;&#65292;&#25105;&#20204;&#20998;&#21035;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;1.1&#20159;&#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#22312;&#27979;&#35797;&#38598;&#19978;&#27979;&#35797;&#20102;GPT-4&#21644;ChatGPT&#30340;&#38646;-shot&#35774;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#21644;ChatGPT&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#24456;&#24378;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#21512;&#25104;&#26679;&#26412;&#22686;&#24378;&#30340;&#25968;&#25454;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#35782;&#21035;&#32597;&#35265;&#31867;&#21035;&#31561;&#20302;&#36164;&#28304;&#35774;&#32622;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining and annotating data can be expensive and time-consuming, especially in complex, low-resource domains. We use GPT-4 and ChatGPT to augment small labeled datasets with synthetic data via simple prompts, in three different classification tasks with varying complexity. For each task, we randomly select a base sample of 500 texts to generate 5,000 new synthetic samples. We explore two augmentation strategies: one that preserves original label distribution and another that balances the distribution. Using a progressively larger training sample size, we train and evaluate a 110M parameter multilingual language model on the real and synthetic data separately. We also test GPT-4 and ChatGPT in a zero-shot setting on the test sets. We observe that GPT-4 and ChatGPT have strong zero-shot performance across all tasks. We find that data augmented with synthetic samples yields a good downstream performance, and particularly aids in low-resource settings, such as in identifying rare classes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11082</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19982;&#20154;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#26159;&#23545;&#40784;&#20854;&#34892;&#20026;&#65292;&#20351;&#20854;&#23545;&#20854;&#20154;&#31867;&#29992;&#25143;&#26377;&#29992;&#19988;&#26080;&#23475;&#12290;&#36825;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#25152;&#38656;&#30340;&#34892;&#20026;&#24182;&#25233;&#21046;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;(BEB)&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#20869;&#22312;&#29305;&#24449;&#21644;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#34987;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#26377;&#38480;&#27010;&#29575;&#30340;&#34892;&#20026;&#65292;&#37117;&#23384;&#22312;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#36755;&#20986;&#27492;&#34892;&#20026;&#30340;&#25552;&#31034;&#65292;&#20854;&#27010;&#29575;&#38543;&#25552;&#31034;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#20943;&#24369;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#20294;&#26410;&#23558;&#20854;&#23436;&#20840;&#28040;&#38500;&#30340;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#25269;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#31034;&#20102;&#39046;&#20808;&#30340;
&lt;/p&gt;
&lt;p&gt;
An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
&lt;/p&gt;</description></item><item><title>REFINER &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#26174;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19982;&#25552;&#20379;&#33258;&#21160;&#21453;&#39304;&#30340;&#25209;&#21028;&#27169;&#22411;&#20132;&#20114;&#65292;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.01904</link><description>&lt;p&gt;
REFINER: &#22522;&#20110;&#20013;&#38388;&#34920;&#31034;&#30340;&#25512;&#29702;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
REFINER: Reasoning Feedback on Intermediate Representations. (arXiv:2304.01904v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01904
&lt;/p&gt;
&lt;p&gt;
REFINER &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#26174;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19982;&#25552;&#20379;&#33258;&#21160;&#21453;&#39304;&#30340;&#25209;&#21028;&#27169;&#22411;&#20132;&#20114;&#65292;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;remarkable&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26174;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#20363;&#22914;&#38142;&#24335;&#24605;&#32771;&#25552;&#31034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#21487;&#33021;&#24182;&#19981;&#26159;&#26681;&#25454;&#21021;&#22987;&#19978;&#19979;&#25991;&#24471;&#20986;&#30340;&#36866;&#24403;&#25512;&#23548;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#26368;&#32456;&#39044;&#27979;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;REFINER&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#26174;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19982;&#25552;&#20379;&#33258;&#21160;&#21453;&#39304;&#30340;&#25209;&#21028;&#27169;&#22411;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25209;&#35780;&#23478;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#21453;&#39304;&#65292;&#25512;&#29702;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#23427;&#26469;&#36845;&#20195;&#25913;&#36827;&#20854;&#20013;&#38388;&#21442;&#25968;&#12290;REFINER&#30340;&#19977;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20986;&#20102;&#19982;&#22522;&#32447;&#20855;&#26377;&#21487;&#27604;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;GPT3.5&#20316;&#20026;&#25512;&#29702;&#22120;&#26102;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#25209;&#35780;&#23478;&#26174;&#30528;&#25913;&#21892;&#20102;&#25512;&#29702;&#32780;&#26080;&#38656;&#24494;&#35843;&#25512;&#29702;&#22120;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#25209;&#35780;&#27169;&#22411;&#26159;&#22312;&#27809;&#26377;&#26114;&#36149;&#30340;&#20154;&#31867;&#21442;&#19982;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#23545;&#26032;&#39062;&#19978;&#19979;&#25991;&#25552;&#20379;&#20302;&#25104;&#26412;&#21453;&#39304;&#36827;&#34892;&#32487;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT3.5 as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; SSC-Conformer &#30340;&#22359;&#24335;&#27169;&#22411;&#65292;&#21033;&#29992;&#20018;&#34892;&#37319;&#26679;&#22359;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#22359;&#38388;&#20132;&#20114;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#23558;&#22359;&#21367;&#31215;&#19982;&#22240;&#26524;&#21367;&#31215;&#30456;&#32467;&#21512;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340; CER &#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; SSC-Conformer &#22312; AISHELL-1 &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#27969;&#24335; E2E ASR &#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.11419</link><description>&lt;p&gt;
&#20018;&#34892;&#37319;&#26679;&#22359;&#24335;Conformer&#32593;&#32476;&#22312;&#27969;&#24335;&#31471;&#21040;&#31471;ASR&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR. (arXiv:2211.11419v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; SSC-Conformer &#30340;&#22359;&#24335;&#27169;&#22411;&#65292;&#21033;&#29992;&#20018;&#34892;&#37319;&#26679;&#22359;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#22359;&#38388;&#20132;&#20114;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#23558;&#22359;&#21367;&#31215;&#19982;&#22240;&#26524;&#21367;&#31215;&#30456;&#32467;&#21512;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340; CER &#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; SSC-Conformer &#22312; AISHELL-1 &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#27969;&#24335; E2E ASR &#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27969;&#24335;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035; (E2E ASR) &#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SSC-Conformer &#30340;&#20018;&#34892;&#37319;&#26679;&#22359;&#24335; Conformer &#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#20018;&#34892;&#37319;&#26679;&#22359;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046; (SSC-MHSA) &#26469;&#25552;&#39640;&#36328;&#22359;&#20132;&#20114;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#21033;&#29992;&#22359;&#21367;&#31215;&#26469;&#22686;&#21152;&#22359;&#32423;&#26410;&#26469;&#19978;&#19979;&#25991;&#65292;&#24182;&#23558;&#20854;&#19982;&#21367;&#31215;&#23618;&#30340;&#22240;&#26524;&#21367;&#31215;&#30456;&#32467;&#21512;&#20197;&#36827;&#19968;&#27493;&#38477;&#20302; CER&#12290;&#22312; AISHELL-1 &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; SSC-Conformer &#22312;&#26080;&#35821;&#35328;&#27169;&#22411;&#37325;&#25171;&#20998;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616; CER 5.33%&#65292;&#36798;&#21040;&#20102;&#27969;&#24335; E2E ASR &#30340;&#26368;&#26032;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#20351;&#29992;&#26356;&#22823;&#30340;&#25209;&#37327;&#36827;&#34892;&#35757;&#32451;&#24182;&#26356;&#39640;&#25928;&#22320;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an in-depth study on a Sequentially Sampled Chunk Conformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer first demonstrates the significant performance gains from using the sequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the Conformer encoder by allowing efficient cross-chunk interactions while keeping linear complexities. Furthermore, it explores taking advantage of chunked convolution to make use of the chunk-wise future context and integrates with casual convolution in the convolution layers to further reduce CER. We verify the proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results show that a state-of-the-art performance for streaming E2E ASR is achieved with CER 5.33% without LM rescoring. And, owing to its linear complexity, the SSC-Conformer can train with large batch sizes and infer more efficiently.
&lt;/p&gt;</description></item></channel></rss>