<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Speaker-Turn Aware Conversational Speech Translation&#30340;&#31471;&#21040;&#31471;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#35828;&#35805;&#32773;&#36716;&#25442;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#23545;&#21333;&#36890;&#36947;&#22810;&#35828;&#35805;&#32773;&#23545;&#35805;&#30340;&#35821;&#38899;&#32763;&#35793;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#35828;&#35805;&#32773;&#26465;&#20214;&#19979;&#20248;&#20110;&#20256;&#32479;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00697</link><description>&lt;p&gt;
&#21333;&#36890;&#36947;&#35828;&#35805;&#32773;&#36716;&#25442;&#24863;&#30693;&#23545;&#35805;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
End-to-End Single-Channel Speaker-Turn Aware Conversational Speech Translation. (arXiv:2311.00697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Speaker-Turn Aware Conversational Speech Translation&#30340;&#31471;&#21040;&#31471;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#35828;&#35805;&#32773;&#36716;&#25442;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#23545;&#21333;&#36890;&#36947;&#22810;&#35828;&#35805;&#32773;&#23545;&#35805;&#30340;&#35821;&#38899;&#32763;&#35793;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#35828;&#35805;&#32773;&#26465;&#20214;&#19979;&#20248;&#20110;&#20256;&#32479;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#31995;&#32479;&#26159;&#22312;&#21333;&#20010;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#21487;&#33021;&#26080;&#27861;&#36866;&#24212;&#23454;&#38469;&#24773;&#20917;&#19979;&#21253;&#21547;&#22810;&#20010;&#35828;&#35805;&#32773;&#23545;&#35805;&#30340;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Speaker-Turn Aware Conversational Speech Translation&#30340;&#31471;&#21040;&#31471;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#26631;&#35760;&#30340;&#24207;&#21015;&#21270;&#26631;&#31614;&#26684;&#24335;&#26469;&#32467;&#21512;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#35828;&#35805;&#32773;&#36716;&#25442;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;Fisher-CALLHOME&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#36890;&#36807;&#23558;&#20004;&#20010;&#21333;&#19968;&#35828;&#35805;&#32773;&#36890;&#36947;&#21512;&#24182;&#20026;&#19968;&#20010;&#22810;&#35828;&#35805;&#32773;&#36890;&#36947;&#65292;&#34920;&#31034;&#26356;&#30495;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#35828;&#35805;&#32773;&#36716;&#25442;&#21644;&#20132;&#21449;&#23545;&#35805;&#22330;&#26223;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#21333;&#35828;&#35805;&#32773;&#21644;&#22810;&#35828;&#35805;&#32773;&#26465;&#20214;&#19979;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#35828;&#35805;&#32773;&#26465;&#20214;&#19979;&#20248;&#20110;&#21442;&#32771;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional speech-to-text translation (ST) systems are trained on single-speaker utterances, and they may not generalize to real-life scenarios where the audio contains conversations by multiple speakers. In this paper, we tackle single-channel multi-speaker conversational ST with an end-to-end and multi-task training model, named Speaker-Turn Aware Conversational Speech Translation, that combines automatic speech recognition, speech translation and speaker turn detection using special tokens in a serialized labeling format. We run experiments on the Fisher-CALLHOME corpus, which we adapted by merging the two single-speaker channels into one multi-speaker channel, thus representing the more realistic and challenging scenario with multi-speaker turns and cross-talk. Experimental results across single- and multi-speaker conditions and against conventional ST systems, show that our model outperforms the reference systems on the multi-speaker condition, while attaining comparable perform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#65292;&#36890;&#36807;&#37322;&#25918;&#20854;&#21019;&#36896;&#28508;&#21147;&#65292;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;LLMs&#20998;&#20026;&#39046;&#23548;&#32773;&#21644;&#25191;&#34892;&#32773;&#65292;&#39046;&#23548;&#32773;&#25552;&#20379;&#22810;&#31181;&#39640;&#32423;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#20316;&#20026;&#25552;&#31034;&#65292;&#25191;&#34892;&#32773;&#26681;&#25454;&#39046;&#23548;&#32773;&#30340;&#25351;&#24341;&#25191;&#34892;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#29983;&#25104;&#19968;&#32452;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2311.00694</link><description>&lt;p&gt;
&#35299;&#25918;&#21019;&#36896;&#21147;&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#20197;&#25913;&#36827;&#25361;&#25112;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving. (arXiv:2311.00694v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#65292;&#36890;&#36807;&#37322;&#25918;&#20854;&#21019;&#36896;&#28508;&#21147;&#65292;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;LLMs&#20998;&#20026;&#39046;&#23548;&#32773;&#21644;&#25191;&#34892;&#32773;&#65292;&#39046;&#23548;&#32773;&#25552;&#20379;&#22810;&#31181;&#39640;&#32423;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#20316;&#20026;&#25552;&#31034;&#65292;&#25191;&#34892;&#32773;&#26681;&#25454;&#39046;&#23548;&#32773;&#30340;&#25351;&#24341;&#25191;&#34892;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#29983;&#25104;&#19968;&#32452;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#38382;&#39064;&#20013;&#24448;&#24448;&#36935;&#21040;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#26679;&#25110;&#25628;&#32034;&#35814;&#32454;&#21644;&#20302;&#32423;&#30340;&#25512;&#29702;&#38142;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25506;&#32034;&#33021;&#21147;&#19978;&#20173;&#28982;&#26377;&#38480;&#65292;&#20351;&#24471;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#24222;&#22823;&#30340;&#35299;&#31354;&#38388;&#20013;&#24456;&#38590;&#31361;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;LLMs&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#37322;&#25918;LLMs&#25506;&#32034;&#22810;&#26679;&#21270;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#30340;&#21019;&#36896;&#28508;&#21147;&#12290;&#35813;&#31574;&#30053;&#21253;&#25324;&#19968;&#20010;&#26377;&#36828;&#35265;&#30340;&#39046;&#23548;&#32773;&#65292;&#25552;&#20986;&#22810;&#31181;&#22810;&#26679;&#30340;&#39640;&#32423;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#26377;&#19968;&#20010;&#25191;&#34892;&#32773;&#65292;&#26681;&#25454;&#27599;&#20010;&#39640;&#32423;&#25351;&#20196;&#25191;&#34892;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#12290;&#25191;&#34892;&#32773;&#23558;&#39046;&#23548;&#32773;&#30340;&#27599;&#20010;&#25351;&#20196;&#20316;&#20026;&#25351;&#21335;&#65292;&#24182;&#37319;&#26679;&#22810;&#20010;&#25512;&#29702;&#38142;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20026;&#27599;&#20010;&#39046;&#23548;&#32773;&#30340;&#25552;&#35758;&#29983;&#25104;&#19968;&#32452;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems. Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains. However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space. In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning. This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction. The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal. Additio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00687</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#21463;&#20247;&#32676;&#20307;&#65292;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#19982;&#20182;&#20154;&#36827;&#34892;&#27807;&#36890;&#20197;&#23454;&#29616;&#33258;&#24049;&#30340;&#30446;&#26631;&#65311;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#32463;&#39564;&#25110;&#20182;&#20154;&#30340;&#24314;&#35758;&#65292;&#25110;&#32773;&#36890;&#36807;&#39044;&#27979;&#23545;&#26041;&#30340;&#21453;&#24212;&#26469;&#26500;&#36896;&#20505;&#36873;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#26159;&#26377;&#38480;&#21644;&#26377;&#20559;&#35265;&#30340;&#65292;&#32780;&#19988;&#23545;&#28508;&#22312;&#32467;&#26524;&#36827;&#34892;&#25512;&#29702;&#21487;&#33021;&#26159;&#22256;&#38590;&#19988;&#35748;&#30693;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#26469;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#27807;&#36890;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;-&#29983;&#25104;-&#27169;&#25311;&#65288;EGS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25509;&#21463;&#20219;&#20309;&#19968;&#20010;&#20010;&#20307;&#19982;&#19968;&#20010;&#30446;&#26631;&#21463;&#20247;&#36827;&#34892;&#27807;&#36890;&#30340;&#22330;&#26223;&#20316;&#20026;&#36755;&#20837;&#12290;EGS&#65288;1&#65289;&#36890;&#36807;&#29983;&#25104;&#19982;&#22330;&#26223;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#24314;&#35758;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#65292;&#65288;2&#65289;&#29983;&#25104;&#20197;&#37096;&#20998;&#24314;&#35758;&#20026;&#26465;&#20214;&#30340;&#27807;&#36890;&#20505;&#36873;&#65292;&#65288;3&#65289;&#27169;&#25311;&#19981;&#21516;&#21463;&#20247;&#30340;&#21453;&#24212;&#65292;&#20197;&#30830;&#23450;&#26368;&#20339;&#20505;&#36873;&#21644;&#24314;&#35758;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#21313;&#20010;&#22522;&#26412;&#36807;&#31243;&#30340;&#20843;&#20010;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;Eval4NLP 2023&#20849;&#20139;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23567;&#22411;LLM&#20316;&#20026;&#25688;&#35201;&#35780;&#20272;&#24230;&#37327;&#30340;&#28508;&#21147;&#65292;&#22312;&#20351;&#29992;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#21644;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25581;&#31034;&#20102;&#32467;&#21512;&#23567;&#22411;&#24320;&#28304;&#27169;&#22411;orca_mini_v3_7B&#21487;&#20197;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00686</link><description>&lt;p&gt;
&#23567;&#24040;&#20154;&#65306;&#25506;&#32034;&#23567;&#22411;LLM&#20316;&#20026;Eval4NLP 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#25688;&#35201;&#35780;&#20272;&#24230;&#37327;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task. (arXiv:2311.00686v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;Eval4NLP 2023&#20849;&#20139;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23567;&#22411;LLM&#20316;&#20026;&#25688;&#35201;&#35780;&#20272;&#24230;&#37327;&#30340;&#28508;&#21147;&#65292;&#22312;&#20351;&#29992;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#21644;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25581;&#31034;&#20102;&#32467;&#21512;&#23567;&#22411;&#24320;&#28304;&#27169;&#22411;orca_mini_v3_7B&#21487;&#20197;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#21644;&#20998;&#26512;&#20102;&#25105;&#20204;&#21442;&#19982;2023 Eval4NLP&#20849;&#20139;&#20219;&#21153;&#30340;&#24773;&#20917;&#65292;&#35813;&#20219;&#21153;&#19987;&#27880;&#20110;&#35780;&#20272;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36136;&#37327;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#21644;&#25688;&#35201;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#21253;&#25324;&#26631;&#20934;&#25552;&#31034;&#12289;&#22522;&#20110;&#27880;&#37322;&#32773;&#25351;&#31034;&#30340;&#25552;&#31034;&#21644;&#21019;&#26032;&#30340;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#19968;&#27425;&#24615;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#26368;&#22823;&#21270;&#25105;&#20204;&#30340;&#35780;&#20272;&#31243;&#24207;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20351;&#29992;&#8220;&#23567;&#22411;&#8221;&#24320;&#28304;&#27169;&#22411;&#65288;orca_mini_v3_7B&#65289;&#32467;&#21512;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes and analyzes our participation in the 2023 Eval4NLP shared task, which focuses on assessing the effectiveness of prompt-based techniques to empower Large Language Models to handle the task of quality estimation, particularly in the context of evaluating machine translations and summaries. We conducted systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and innovative chain-of-thought prompting. In addition, we integrated these approaches with zero-shot and one-shot learning methods to maximize the efficacy of our evaluation procedures. Our work reveals that combining these approaches using a "small", open source model (orca_mini_v3_7B) yields competitive results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#25913;&#21892;T5&#22312;&#38271;&#24207;&#21015;&#22788;&#29702;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00684</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#23545;&#40784;&#21644;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#25552;&#39640;&#20102;Transformer&#38271;&#24230;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#25913;&#21892;T5&#22312;&#38271;&#24207;&#21015;&#22788;&#29702;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24819;&#30340;&#38271;&#24230;&#21487;&#22806;&#25512;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#27604;&#35757;&#32451;&#38271;&#24230;&#26356;&#38271;&#30340;&#24207;&#21015;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#38271;&#24207;&#21015;&#24494;&#35843;&#12290;&#36825;&#31181;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#39640;&#24230;&#20381;&#36182;&#20110;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#12290;&#22312;&#35843;&#26597;&#29616;&#26377;&#22823;&#22411;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;T5&#31995;&#21015;&#20540;&#24471;&#26356;&#20180;&#32454;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#30340;&#20301;&#32622;&#23884;&#20837;&#25429;&#25417;&#21040;&#20102;&#20016;&#23500;&#32780;&#28789;&#27963;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;T5&#23384;&#22312;&#30528;&#20998;&#25955;&#30340;&#27880;&#24847;&#21147;&#38382;&#39064;&#65306;&#36755;&#20837;&#24207;&#21015;&#36234;&#38271;&#65292;&#27880;&#24847;&#21147;&#20998;&#24067;&#23601;&#36234;&#24179;&#22374;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#39640;&#20102;T5&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#65292;&#36825;&#34920;&#26126;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#21644;&#27880;&#24847;&#21147;&#23545;&#40784;&#23545;&#20110;Transformer&#38271;&#24230;&#22806;&#25512;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning. Such long-context utilization capability highly relies on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\footnote{\url
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#25152;&#20135;&#29983;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#34987;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;GPT-4&#21644;PaLM-2&#26041;&#38754;&#20107;&#23454;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#20043;&#38388;&#32570;&#20047;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00681</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21487;&#38752;&#30340;&#35780;&#21028;&#32773;&#21527;&#65311;&#20851;&#20110;LLMs&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs. (arXiv:2311.00681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00681
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#25152;&#20135;&#29983;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#34987;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;GPT-4&#21644;PaLM-2&#26041;&#38754;&#20107;&#23454;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#20043;&#38388;&#32570;&#20047;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#26174;&#33879;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#36229;&#36234;&#20102;&#26089;&#26399;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;LLMs&#30340;&#19968;&#20010;&#29305;&#21035;&#26377;&#36259;&#30340;&#24212;&#29992;&#26159;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#35780;&#20272;&#32773;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#20316;&#20026;&#21487;&#38752;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#36827;&#34892;&#20107;&#23454;&#35780;&#20272;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36825;&#21253;&#25324;&#22312;&#25972;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#20107;&#23454;&#35780;&#20998;&#36807;&#31243;&#20013;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;LLM&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;&#21508;&#31181;LLMs&#22312;&#30452;&#25509;&#20107;&#23454;&#35780;&#20998;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20256;&#32479;&#27979;&#37327;&#26041;&#27861;&#21644;&#20154;&#24037;&#27880;&#37322;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#19982;&#26368;&#21021;&#30340;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20107;&#23454;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#20043;&#38388;&#32570;&#20047;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;GPT-4&#21644;PaLM-2&#26041;&#38754;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#30456;&#20851;&#24615;&#26159;
&lt;/p&gt;
&lt;p&gt;
In recent years, Large Language Models (LLMs) have gained immense attention due to their notable emergent capabilities, surpassing those seen in earlier language models. A particularly intriguing application of LLMs is their role as evaluators for texts produced by various generative models.  In this study, we delve into the potential of LLMs as reliable assessors of factual consistency in summaries generated by text-generation models. Initially, we introduce an innovative approach for factuality assessment using LLMs. This entails employing a singular LLM for the entirety of the question-answering-based factuality scoring process. Following this, we examine the efficacy of various LLMs in direct factuality scoring, benchmarking them against traditional measures and human annotations.  Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2. Notable correlations were on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#24773;&#24863;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#26041;&#27861;&#65292;&#39318;&#20808;&#35299;&#37322;&#20102;&#24773;&#24863;&#19982;&#34394;&#20551;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28982;&#21518;&#23545;&#24212;&#29992;&#19981;&#21516;&#24773;&#24863;&#12289;&#24773;&#32490;&#21644;&#31435;&#22330;&#29305;&#24449;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#26368;&#21518;&#25506;&#35752;&#20102;&#24773;&#24863;&#39537;&#21160;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00671</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#24863;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Emotion Detection for Misinformation: A Review. (arXiv:2311.00671v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#24773;&#24863;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#26041;&#27861;&#65292;&#39318;&#20808;&#35299;&#37322;&#20102;&#24773;&#24863;&#19982;&#34394;&#20551;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28982;&#21518;&#23545;&#24212;&#29992;&#19981;&#21516;&#24773;&#24863;&#12289;&#24773;&#32490;&#21644;&#31435;&#22330;&#29305;&#24449;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#26368;&#21518;&#25506;&#35752;&#20102;&#24773;&#24863;&#39537;&#21160;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#20986;&#29616;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#32593;&#27665;&#22312;&#32593;&#19978;&#20998;&#20139;&#21644;&#38405;&#35835;&#24086;&#23376;&#21644;&#26032;&#38395;&#12290;&#28982;&#32780;&#65292;&#28044;&#20837;&#20114;&#32852;&#32593;&#30340;&#22823;&#37327;&#34394;&#20551;&#20449;&#24687;&#65288;&#20363;&#22914;&#20551;&#26032;&#38395;&#21644;&#35875;&#35328;&#65289;&#21487;&#33021;&#23545;&#20154;&#20204;&#30340;&#29983;&#27963;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#23548;&#33268;&#35875;&#35328;&#21644;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#30340;&#20986;&#29616;&#12290;&#32593;&#27665;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#26032;&#38395;&#20013;&#34920;&#36798;&#30340;&#24773;&#32490;&#21644;&#24773;&#24863;&#26500;&#25104;&#20102;&#21487;&#20197;&#24110;&#21161;&#21306;&#20998;&#34394;&#20551;&#20449;&#24687;&#21644;&#30495;&#23454;&#20449;&#24687;&#24182;&#29702;&#35299;&#35875;&#35328;&#20256;&#25773;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#22522;&#20110;&#24773;&#24863;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#37322;&#20102;&#24773;&#24863;&#19982;&#34394;&#20551;&#20449;&#24687;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#24212;&#29992;&#21508;&#31181;&#24773;&#24863;&#12289;&#24773;&#32490;&#21644;&#31435;&#22330;&#29305;&#24449;&#30340;&#19968;&#31995;&#21015;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24773;&#24863;&#39537;&#21160;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20013;&#30340;&#19968;&#20123;&#25345;&#32493;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of social media, an increasing number of netizens are sharing and reading posts and news online. However, the huge volumes of misinformation (e.g., fake news and rumors) that flood the internet can adversely affect people's lives, and have resulted in the emergence of rumor and fake news detection as a hot research topic. The emotions and sentiments of netizens, as expressed in social media posts and news, constitute important factors that can help to distinguish fake news from genuine news and to understand the spread of rumors. This article comprehensively reviews emotion-based methods for misinformation detection. We begin by explaining the strong links between emotions and misinformation. We subsequently provide a detailed analysis of a range of misinformation detection methods that employ a variety of emotion, sentiment and stance-based features, and describe their strengths and weaknesses. Finally, we discuss a number of ongoing challenges in emotion-based misinfo
&lt;/p&gt;</description></item><item><title>&#21152;&#20837;&#26126;&#30830;&#30340;&#24418;&#24577;&#23398;&#30693;&#35782;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#21487;&#20197;&#25552;&#39640;&#24076;&#20271;&#26469;&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00658</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#26126;&#30830;&#30340;&#24418;&#24577;&#23398;&#30693;&#35782;&#25552;&#39640;&#20102;&#24076;&#20271;&#26469;&#35821;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew. (arXiv:2311.00658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00658
&lt;/p&gt;
&lt;p&gt;
&#21152;&#20837;&#26126;&#30830;&#30340;&#24418;&#24577;&#23398;&#30693;&#35782;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#21487;&#20197;&#25552;&#39640;&#24076;&#20271;&#26469;&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#30340;&#25991;&#26412;&#27969;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#35821;&#35328;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#26102;&#65292;&#36825;&#31181;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#32463;&#24120;&#21463;&#21040;&#36136;&#30097;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#21152;&#20837;&#26126;&#30830;&#30340;&#24418;&#24577;&#23398;&#30693;&#35782;&#33021;&#21542;&#25552;&#39640;PLM&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#38500;&#20102;&#21407;&#22987;&#25991;&#26412;&#20043;&#22806;&#30340;&#24418;&#24577;&#23398;&#32447;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#23545;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#22797;&#26434;&#19988;&#39640;&#24230;&#27169;&#31946;&#30340;&#24076;&#20271;&#26469;&#35821;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#20998;&#35789;&#26041;&#27861;&#22312;&#35821;&#20041;&#21644;&#24418;&#24577;&#23398;&#35780;&#20272;&#26631;&#20934;&#19978;&#30456;&#27604;&#26631;&#20934;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#20998;&#35789;&#26041;&#27861;&#26377;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have shown remarkable successes in acquiring a wide range of linguistic knowledge, relying solely on self-supervised training on text streams. Nevertheless, the effectiveness of this language-agnostic approach has been frequently questioned for its sub-optimal performance when applied to morphologically-rich languages (MRLs). We investigate the hypothesis that incorporating explicit morphological knowledge in the pre-training phase can improve the performance of PLMs for MRLs. We propose various morphologically driven tokenization methods enabling the model to leverage morphological cues beyond raw text. We pre-train multiple language models utilizing the different methods and evaluate them on Hebrew, a language with complex and highly ambiguous morphology. Our experiments show that morphologically driven tokenization demonstrates improved results compared to a standard language-agnostic tokenization, on a benchmark of both semantic and morphologic ta
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20174;&#36870;&#21521;Petri&#32593;&#21040;&#24425;&#33394;Petri&#32593;&#30340;&#24418;&#24335;&#21270;&#32763;&#35793;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#36870;&#35745;&#31639;&#30340;&#19977;&#31181;&#20027;&#35201;&#24418;&#24335;&#65292;&#20026;&#20302;&#21151;&#32791;&#35745;&#31639;&#21644;&#24191;&#27867;&#24212;&#29992;&#39046;&#22495;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00629</link><description>&lt;p&gt;
&#20174;&#36870;&#21521;Petri&#32593;&#21040;&#24425;&#33394;Petri&#32593;&#30340;&#24418;&#24335;&#21270;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Formal Translation from Reversing Petri Nets to Coloured Petri Nets. (arXiv:2311.00629v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00629
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20174;&#36870;&#21521;Petri&#32593;&#21040;&#24425;&#33394;Petri&#32593;&#30340;&#24418;&#24335;&#21270;&#32763;&#35793;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#36870;&#35745;&#31639;&#30340;&#19977;&#31181;&#20027;&#35201;&#24418;&#24335;&#65292;&#20026;&#20302;&#21151;&#32791;&#35745;&#31639;&#21644;&#24191;&#27867;&#24212;&#29992;&#39046;&#22495;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36870;&#35745;&#31639;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#35745;&#31639;&#33539;&#24335;&#65292;&#20801;&#35768;&#22312;&#35745;&#31639;&#36807;&#31243;&#20013;&#20219;&#24847;&#26102;&#21051;&#20197;&#30456;&#21453;&#30340;&#39034;&#24207;&#25191;&#34892;&#25805;&#20316;&#24207;&#21015;&#12290;&#23427;&#20855;&#26377;&#20302;&#21151;&#32791;&#35745;&#31639;&#30340;&#28508;&#21147;&#65292;&#24182;&#19982;&#21270;&#23398;&#21453;&#24212;&#12289;&#37327;&#23376;&#35745;&#31639;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20998;&#24067;&#24335;&#31995;&#32479;&#31561;&#24191;&#27867;&#24212;&#29992;&#26377;&#20851;&#12290;&#36870;&#21521;Petri&#32593;&#26159;&#23545;Petri&#32593;&#30340;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#21487;&#36870;&#24615;&#24418;&#24335;&#65292;&#21363;&#22238;&#28335;&#12289;&#22240;&#26524;&#36870;&#36716;&#21644;&#36229;&#20986;&#22240;&#26524;&#39034;&#24207;&#36870;&#36716;&#12290;&#23427;&#20204;&#30340;&#29305;&#28857;&#26159;&#20351;&#29992;&#20855;&#26377;&#21629;&#21517;&#30340;&#26631;&#35760;&#65292;&#21487;&#20197;&#32467;&#21512;&#22312;&#19968;&#36215;&#24418;&#25104;&#38190;&#21512;&#12290;&#21629;&#21517;&#26631;&#35760;&#20197;&#21450;&#21382;&#21490;&#20989;&#25968;&#26500;&#25104;&#20102;&#35760;&#24518;&#36807;&#21435;&#34892;&#20026;&#30340;&#25163;&#27573;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#36870;&#24615;&#12290;&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#19968;&#31867;RPNs&#21040;&#24425;&#33394;Petri&#32593;&#27169;&#22411;&#65288;CPNs&#65289;&#30340;&#32467;&#26500;&#21270;&#32763;&#35793;&#12290;CPN&#26159;&#20256;&#32479;Petri&#32593;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#26631;&#35760;&#25658;&#24102;&#25968;&#25454;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reversible computation is an emerging computing paradigm that allows any sequence of operations to be executed in reverse order at any point during computation. Its appeal lies in its potential for lowpower computation and its relevance to a wide array of applications such as chemical reactions, quantum computation, robotics, and distributed systems. Reversing Petri nets are a recently-proposed extension of Petri nets that implements the three main forms of reversibility, namely, backtracking, causal reversing, and out-of-causal-order reversing. Their distinguishing feature is the use of named tokens that can be combined together to form bonds. Named tokens along with a history function, constitute the means of remembering past behaviour, thus, enabling reversal. In recent work, we have proposed a structural translation from a subclass of RPNs to the model of Coloured Petri Nets (CPNs), an extension of traditional Petri nets where tokens carry data values. In this paper, we extend the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FlowSUM&#65292;&#19968;&#20010;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;Transformer-based&#25688;&#35201;&#29983;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#28789;&#27963;&#30340;&#28508;&#22312;&#21518;&#21521;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#21463;&#25511;&#20132;&#26367;&#28608;&#36827;&#35757;&#32451;&#31574;&#30053;&#65292;FlowSUM&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#27491;&#21017;&#21270;&#27969;&#20013;&#30340;&#21518;&#21521;&#22604;&#38519;&#38382;&#39064;&#21644;&#30456;&#20851;&#24433;&#21709;&#22240;&#32032;&#65292;&#20026;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2311.00588</link><description>&lt;p&gt;
&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#21644;&#28608;&#36827;&#35757;&#32451;&#25552;&#21319;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Boosting Summarization with Normalizing Flows and Aggressive Training. (arXiv:2311.00588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FlowSUM&#65292;&#19968;&#20010;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;Transformer-based&#25688;&#35201;&#29983;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#28789;&#27963;&#30340;&#28508;&#22312;&#21518;&#21521;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#21463;&#25511;&#20132;&#26367;&#28608;&#36827;&#35757;&#32451;&#31574;&#30053;&#65292;FlowSUM&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#27491;&#21017;&#21270;&#27969;&#20013;&#30340;&#21518;&#21521;&#22604;&#38519;&#38382;&#39064;&#21644;&#30456;&#20851;&#24433;&#21709;&#22240;&#32032;&#65292;&#20026;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;FlowSUM&#65292;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#25688;&#35201;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#21464;&#20998;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#19981;&#36275;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21518;&#21521;&#22604;&#38519;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#28508;&#22312;&#21518;&#21521;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38376;&#26426;&#21046;&#19979;&#30340;&#21463;&#25511;&#20132;&#26367;&#28608;&#36827;&#35757;&#32451;&#65288;CAAT&#65289;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FlowSUM&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#23545;&#25512;&#29702;&#26102;&#38388;&#30340;&#24433;&#21709;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#37322;&#25918;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#27969;&#20013;&#30340;&#21518;&#21521;&#22604;&#38519;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#35757;&#32451;&#31574;&#30053;&#12289;&#38376;&#21021;&#22987;&#21270;&#20197;&#21450;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#27969;&#31867;&#22411;&#21644;&#25968;&#37327;&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training. To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism. Experimental results show that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time. Furthermore, we investigate the issue of posterior collapse in normalizing flows and analyze how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used, offering valuable insights for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#24773;&#22659;&#23398;&#20064;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#25552;&#31034;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#23391;&#21152;&#25289;&#35821;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00587</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#24773;&#22659;&#23398;&#20064;&#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crosslingual Retrieval Augmented In-context Learning for Bangla. (arXiv:2311.00587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#24773;&#22659;&#23398;&#20064;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#25552;&#31034;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#23391;&#21152;&#25289;&#35821;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#21147;&#24120;&#24120;&#34987;&#23427;&#20204;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#23391;&#21152;&#25289;&#35821;&#65289;&#20013;&#30340;&#26377;&#38480;&#24615;&#33021;&#25152;&#36974;&#30422;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#24773;&#22659;&#23398;&#20064;&#12290;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#31574;&#30053;&#24615;&#22320;&#33719;&#21462;&#35821;&#20041;&#30456;&#20284;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#20351;&#24471;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MPLMs&#65289;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#27169;&#22411;BLOOMZ&#65292;&#33021;&#22815;&#25104;&#21151;&#25552;&#39640;&#23391;&#21152;&#25289;&#35821;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23545;MPLMs&#30340;&#25552;&#21319;&#25928;&#26524;&#31283;&#23450;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla. To address this, our paper presents a pioneering approach that utilizes cross-lingual retrieval augmented in-context learning. By strategically sourcing semantically similar prompts from high-resource language, we enable multilingual pretrained language models (MPLMs), especially the generative model BLOOMZ, to successfully boost performance on Bangla tasks. Our extensive evaluation highlights that the cross-lingual retrieval augmented prompts bring steady improvements to MPLMs over the zero-shot performance.
&lt;/p&gt;</description></item><item><title>LLaVA-Interactive&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20154;&#26426;&#20132;&#20114;&#30340;&#30740;&#31350;&#21407;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;&#26469;&#23545;&#40784;&#20154;&#31867;&#24847;&#22270;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#23545;&#35805;&#12289;&#20998;&#21106;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#22810;&#27169;&#24577;&#21151;&#33021;&#65292;&#20855;&#26377;&#39640;&#25104;&#26412;&#25928;&#30410;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00571</link><description>&lt;p&gt;
LLaVA-Interactive:&#19968;&#20010;&#22270;&#20687;&#23545;&#35805;&#12289;&#20998;&#21106;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#20840;&#33021;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing. (arXiv:2311.00571v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00571
&lt;/p&gt;
&lt;p&gt;
LLaVA-Interactive&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20154;&#26426;&#20132;&#20114;&#30340;&#30740;&#31350;&#21407;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;&#26469;&#23545;&#40784;&#20154;&#31867;&#24847;&#22270;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#23545;&#35805;&#12289;&#20998;&#21106;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#22810;&#27169;&#24577;&#21151;&#33021;&#65292;&#20855;&#26377;&#39640;&#25104;&#26412;&#25928;&#30410;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLaVA-Interactive&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20154;&#26426;&#20132;&#20114;&#30340;&#30740;&#31350;&#21407;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25509;&#25910;&#22810;&#27169;&#24577;&#29992;&#25143;&#36755;&#20837;&#24182;&#29983;&#25104;&#22810;&#27169;&#24577;&#22238;&#24212;&#19982;&#29992;&#25143;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;LLaVA-Interactive&#19981;&#20165;&#20165;&#26159;&#35821;&#35328;&#25552;&#31034;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;&#35270;&#35273;&#25552;&#31034;&#26469;&#23545;&#40784;&#20154;&#31867;&#24847;&#22270;&#12290;LLaVA-Interactive&#30340;&#30740;&#21457;&#25104;&#26412;&#25928;&#30410;&#38750;&#24120;&#39640;&#65292;&#22240;&#20026;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39044;&#24314;&#30340;AI&#27169;&#22411;&#30340;&#19977;&#39033;&#22810;&#27169;&#24577;&#25216;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22411;&#35757;&#32451;&#65306;LLaVA&#30340;&#22270;&#20687;&#23545;&#35805;&#65292;SEEM&#30340;&#22270;&#20687;&#20998;&#21106;&#20197;&#21450;GLIGEN&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#12290;&#23637;&#31034;&#20102;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#65292;&#20197;&#23637;&#31034;LLaVA-Interactive&#30340;&#28508;&#21147;&#65292;&#24182;&#28608;&#21457;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#20132;&#20114;&#31995;&#32479;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Interactive is a research prototype for multimodal human-AI interaction. The system can have multi-turn dialogues with human users by taking multimodal user inputs and generating multimodal responses. Importantly, LLaVA-Interactive goes beyond language prompt, where visual prompt is enabled to align human intents in the interaction. The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of LLaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN. A diverse set of application scenarios is presented to demonstrate the promises of LLaVA-Interactive and to inspire future research in multimodal interactive systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00541</link><description>&lt;p&gt;
&#19968;&#20010;&#24102;&#26377;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#30340;&#35770;&#25991;&#19982;&#19968;&#20010;&#20851;&#20110;&#21476;&#24076;&#33098;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#30340;&#24847;&#20041;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21464;&#21270;&#65292;&#35789;&#20041;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20250;&#28436;&#21464;&#12289;&#20986;&#29616;&#25110;&#28040;&#22833;&#12290;&#23545;&#20110;&#21476;&#20195;&#35821;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#35821;&#26009;&#24211;&#36890;&#24120;&#36739;&#23567;&#12289;&#31232;&#30095;&#19988;&#22024;&#26434;&#65292;&#20934;&#30830;&#24314;&#27169;&#36825;&#31181;&#21464;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#23545;&#20110;&#24847;&#20041;&#21464;&#21270;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#21464;&#24471;&#37325;&#35201;&#12290;GASC&#21644;DiSC&#26159;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#65292;&#20351;&#29992;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#24182;&#27809;&#26377;&#20511;&#21161;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#32473;&#23450;&#30446;&#26631;&#35789;&#27719;&#65288;&#22914;"kosmos"&#65292;&#24847;&#20026;&#35013;&#39280;&#12289;&#31209;&#24207;&#25110;&#19990;&#30028;&#65289;&#30340;&#24847;&#20041;&#34920;&#31034;&#20026;&#19978;&#19979;&#25991;&#35789;&#27719;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#24847;&#20041;&#30340;&#26222;&#36941;&#24615;&#34920;&#31034;&#20026;&#24847;&#20041;&#30340;&#20998;&#24067;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36827;&#34892;&#25311;&#21512;&#65292;&#20197;&#27979;&#37327;&#36825;&#20123;&#34920;&#31034;&#20013;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EDiSC&#65292;&#36825;&#26159;DiSC&#30340;&#23884;&#20837;&#29256;&#26412;&#65292;&#23427;&#23558;&#35789;&#23884;&#20837;&#19982;DiSC&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;EDiSC&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as "kosmos" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using MCMC methods to measure temporal changes in these representations. In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance. We show empirically that EDiSC offers improved p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20687;&#32032;&#35821;&#35328;&#27169;&#22411;&#20013;&#28210;&#26579;&#25991;&#26412;&#30340;&#22235;&#31181;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#30340;&#23383;&#31526;&#20108;&#20803;&#28210;&#26579;&#31574;&#30053;&#22312;&#21477;&#23376;&#32423;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#20196;&#29260;&#32423;&#25110;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#35813;&#31574;&#30053;&#36824;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;22M&#21442;&#25968;&#20294;&#24615;&#33021;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2311.00522</link><description>&lt;p&gt;
&#20687;&#32032;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#28210;&#26579;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Text Rendering Strategies for Pixel Language Models. (arXiv:2311.00522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20687;&#32032;&#35821;&#35328;&#27169;&#22411;&#20013;&#28210;&#26579;&#25991;&#26412;&#30340;&#22235;&#31181;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#30340;&#23383;&#31526;&#20108;&#20803;&#28210;&#26579;&#31574;&#30053;&#22312;&#21477;&#23376;&#32423;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#20196;&#29260;&#32423;&#25110;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#35813;&#31574;&#30053;&#36824;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;22M&#21442;&#25968;&#20294;&#24615;&#33021;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20687;&#32032;&#30340;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#20197;&#22270;&#20687;&#24418;&#24335;&#28210;&#26579;&#30340;&#25991;&#26412;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#20219;&#20309;&#33050;&#26412;&#65292;&#20026;&#24320;&#25918;&#35789;&#27719;&#35821;&#35328;&#24314;&#27169;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#20135;&#29983;&#22823;&#37327;&#20960;&#20046;&#31561;&#25928;&#30340;&#36755;&#20837;&#34917;&#19969;&#30340;&#25991;&#26412;&#28210;&#26579;&#22120;&#65292;&#36825;&#21487;&#33021;&#23545;&#19979;&#28216;&#20219;&#21153;&#26469;&#35828;&#24182;&#19981;&#26368;&#20248;&#65292;&#22240;&#20026;&#36755;&#20837;&#34920;&#31034;&#20013;&#23384;&#22312;&#20887;&#20313;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;PIXEL&#27169;&#22411;&#65288;Rust&#31561;&#20154;&#65292;2023&#65289;&#20013;&#36827;&#34892;&#25991;&#26412;&#28210;&#26579;&#30340;&#22235;&#31181;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#30340;&#23383;&#31526;&#20108;&#20803;&#28210;&#26579;&#22312;&#21477;&#23376;&#32423;&#20219;&#21153;&#19978;&#24102;&#26469;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#20196;&#29260;&#32423;&#25110;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#36825;&#31181;&#26032;&#30340;&#28210;&#26579;&#31574;&#30053;&#36824;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#21482;&#26377;22M&#21442;&#25968;&#20294;&#24615;&#33021;&#19982;&#21407;&#22987;&#30340;86M&#21442;&#25968;&#27169;&#22411;&#30456;&#24403;&#30340;&#26356;&#32039;&#20945;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#23383;&#31526;&#20108;&#20803;&#28210;&#26579;&#23548;&#33268;&#20102;&#19968;&#20010;&#20855;&#26377;&#21508;&#21521;&#24322;&#24615;&#30340;&#34917;&#19969;&#23884;&#20837;&#31354;&#38388;&#65292;&#30001;&#20110;&#34917;&#19969;&#39057;&#29575;&#20559;&#24046;&#32780;&#21463;&#21040;&#39537;&#21160;&#65292;&#31361;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling. However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations. In this paper, we investigate four approaches to rendering text in the PIXEL model (Rust et al., 2023), and find that simple character bigram rendering brings improved performance on sentence-level tasks without compromising performance on token-level or multilingual tasks. This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model. Our analyses show that character bigram rendering leads to a consistently better model but with an anisotropic patch embedding space, driven by a patch frequency bias, highlighting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#38169;&#35823;&#20998;&#31867;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#21021;&#23398;&#32773;&#21644;&#19987;&#23478;&#31243;&#24207;&#21592;&#20043;&#38388;&#39057;&#32321;&#38169;&#35823;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#23545;95,631&#20010;&#20195;&#30721;&#23545;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#21457;&#29616;&#21021;&#23398;&#32773;&#30340;&#38169;&#35823;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#32534;&#31243;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2311.00513</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#38169;&#35823;&#20998;&#31867;&#29992;&#20110;&#20998;&#26512;&#39057;&#32321;&#38169;&#35823;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Rule-Based Error Classification for Analyzing Differences in Frequent Errors. (arXiv:2311.00513v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#38169;&#35823;&#20998;&#31867;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#21021;&#23398;&#32773;&#21644;&#19987;&#23478;&#31243;&#24207;&#21592;&#20043;&#38388;&#39057;&#32321;&#38169;&#35823;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#23545;95,631&#20010;&#20195;&#30721;&#23545;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#21457;&#29616;&#21021;&#23398;&#32773;&#30340;&#38169;&#35823;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#32534;&#31243;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#21644;&#20462;&#22797;&#38169;&#35823;&#23545;&#20110;&#21021;&#23398;&#32773;&#21644;&#19987;&#19994;&#31243;&#24207;&#21592;&#26469;&#35828;&#37117;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#21508;&#20010;&#32423;&#21035;&#31243;&#24207;&#21592;&#20043;&#38388;&#39057;&#32321;&#38169;&#35823;&#27169;&#24335;&#30340;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#21021;&#23398;&#32773;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#19981;&#21516;&#20542;&#21521;&#23578;&#26410;&#34987;&#25581;&#31034;&#12290;&#36890;&#36807;&#20102;&#35299;&#19981;&#21516;&#32423;&#21035;&#31243;&#24207;&#21592;&#30340;&#39057;&#32321;&#38169;&#35823;&#65292;&#25945;&#24072;&#21487;&#20197;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#23398;&#20064;&#32773;&#25552;&#20379;&#26377;&#29992;&#30340;&#24314;&#35758;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#38169;&#35823;&#20998;&#31867;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#30001;&#38169;&#35823;&#21644;&#27491;&#30830;&#31243;&#24207;&#32452;&#25104;&#30340;&#20195;&#30721;&#23545;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;95,631&#20010;&#20195;&#30721;&#23545;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24179;&#22343;&#25214;&#21040;3.47&#20010;&#38169;&#35823;&#65292;&#36825;&#20123;&#20195;&#30721;&#23545;&#30001;&#21508;&#20010;&#32423;&#21035;&#30340;&#31243;&#24207;&#21592;&#22312;&#19968;&#20010;&#22312;&#32447;&#35780;&#27979;&#31995;&#32479;&#19978;&#25552;&#20132;&#12290;&#20998;&#31867;&#30340;&#38169;&#35823;&#34987;&#29992;&#20110;&#20998;&#26512;&#21021;&#23398;&#32773;&#21644;&#19987;&#23478;&#31243;&#24207;&#21592;&#20043;&#38388;&#39057;&#32321;&#38169;&#35823;&#30340;&#24046;&#24322;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#30456;&#21516;&#30340;&#20837;&#38376;&#38382;&#39064;&#20013;&#65292;&#21021;&#23398;&#32773;&#30340;&#38169;&#35823;&#26159;&#30001;&#20110;&#32570;&#20047;&#32534;&#31243;&#30693;&#35782;&#23548;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding and fixing errors is a time-consuming task not only for novice programmers but also for expert programmers. Prior work has identified frequent error patterns among various levels of programmers. However, the differences in the tendencies between novices and experts have yet to be revealed. From the knowledge of the frequent errors in each level of programmers, instructors will be able to provide helpful advice for each level of learners. In this paper, we propose a rule-based error classification tool to classify errors in code pairs consisting of wrong and correct programs. We classify errors for 95,631 code pairs and identify 3.47 errors on average, which are submitted by various levels of programmers on an online judge system. The classified errors are used to analyze the differences in frequent errors between novice and expert programmers. The analyzed results show that, as for the same introductory problems, errors made by novices are due to the lack of knowledge in progra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#22312;&#23545;&#25239;&#24615;&#21512;&#25104;&#25991;&#26412;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;BERTScore&#35780;&#32423;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20026;&#26356;&#40065;&#26834;&#30340;&#25351;&#26631;&#24320;&#21457;&#25552;&#20379;&#20102;&#21160;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00508</link><description>&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#19979;&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks. (arXiv:2311.00508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#22312;&#23545;&#25239;&#24615;&#21512;&#25104;&#25991;&#26412;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;BERTScore&#35780;&#32423;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20026;&#26356;&#40065;&#26834;&#30340;&#25351;&#26631;&#24320;&#21457;&#25552;&#20379;&#20102;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#21512;&#25104;&#25991;&#26412;&#19979;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#30340;&#34920;&#29616;&#65292;&#20197;&#25581;&#31034;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#24120;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#65288;BERTScore&#12289;BLEURT&#21644;COMET&#65289;&#36827;&#34892;&#20102;&#35789;&#32423;&#21644;&#23383;&#31526;&#32423;&#25915;&#20987;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20154;&#31867;&#23454;&#39564;&#39564;&#35777;&#20102;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#20542;&#21521;&#20110;&#36807;&#24230;&#24809;&#32602;&#32463;&#36807;&#23545;&#25239;&#24615;&#24694;&#21270;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;BERTScore&#35780;&#32423;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#23427;&#35748;&#20026;&#21407;&#22987;&#21477;&#23376;&#21644;&#32463;&#36807;&#23545;&#25239;&#24615;&#24694;&#21270;&#30340;&#21477;&#23376;&#30456;&#20284;&#65292;&#32780;&#30456;&#23545;&#20110;&#21442;&#32771;&#25991;&#26412;&#32780;&#35328;&#65292;&#35748;&#20026;&#24694;&#21270;&#30340;&#32763;&#35793;&#26126;&#26174;&#27604;&#21407;&#22987;&#32763;&#35793;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#26131;&#25439;&#24615;&#30340;&#27169;&#24335;&#65292;&#20419;&#36827;&#20102;&#26356;&#21152;&#40065;&#26834;&#30340;&#25351;&#26631;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate MT evaluation metric performance on adversarially-synthesized texts, to shed light on metric robustness. We experiment with word- and character-level attacks on three popular machine translation metrics: BERTScore, BLEURT, and COMET. Our human experiments validate that automatic metrics tend to overpenalize adversarially-degraded translations. We also identify inconsistencies in BERTScore ratings, where it judges the original sentence and the adversarially-degraded one as similar, while judging the degraded translation as notably worse than the original with respect to the reference. We identify patterns of brittleness that motivate more robust metric development.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.00502</link><description>&lt;p&gt;
&#22312;CPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#21644;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#24222;&#22823;&#25968;&#37327;&#65292;LLMs&#30340;&#37096;&#32626;&#19968;&#30452;&#38754;&#20020;&#25361;&#25112;&#65292;&#23545;&#22823;&#20869;&#23384;&#23481;&#37327;&#21644;&#39640;&#20869;&#23384;&#24102;&#23485;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;LLMs&#30340;&#37096;&#32626;&#26356;&#39640;&#25928;&#12290;&#25105;&#20204;&#25903;&#25345;&#33258;&#21160;&#30340;INT4&#26435;&#37325;&#37327;&#21270;&#27969;&#31243;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;LLM&#36816;&#34892;&#26102;&#65292;&#20855;&#26377;&#39640;&#24230;&#20248;&#21270;&#30340;&#20869;&#26680;&#65292;&#20197;&#21152;&#36895;&#22312;CPU&#19978;&#30340;LLM&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#30340;&#26222;&#36866;&#24615;&#65292;&#21253;&#25324;Llama2&#65292;Llama&#65292;GPT-NeoX&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;CPU&#19978;&#30340;&#26497;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20248;&#21270;&#30446;&#26631;&#21457;&#29616;&#65292;&#20351;&#29992;Midpoint-Displacement&#65288;MD&#65289;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#33719;&#24471;&#19982;Contrast-Consistent Search&#65288;CCS&#65289;&#38750;&#24120;&#30456;&#20284;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#32780;&#19988;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#21487;&#20197;&#20351;MD&#25439;&#22833;&#20989;&#25968;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#39640;&#20110;CCS&#12290;</title><link>http://arxiv.org/abs/2311.00488</link><description>&lt;p&gt;
&#23545;&#27604;&#20248;&#21270;&#30446;&#26631;&#23545;&#19968;&#33268;&#24615;&#25628;&#32034;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing Optimization Targets for Contrast-Consistent Search. (arXiv:2311.00488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20248;&#21270;&#30446;&#26631;&#21457;&#29616;&#65292;&#20351;&#29992;Midpoint-Displacement&#65288;MD&#65289;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#33719;&#24471;&#19982;Contrast-Consistent Search&#65288;CCS&#65289;&#38750;&#24120;&#30456;&#20284;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#32780;&#19988;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#21487;&#20197;&#20351;MD&#25439;&#22833;&#20989;&#25968;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#39640;&#20110;CCS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#33268;&#24615;&#25628;&#32034;&#65288;CCS&#65289;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#26088;&#22312;&#24674;&#22797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;Midpoint-Displacement&#65288;MD&#65289;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20010;&#36229;&#21442;&#25968;&#20540;&#19979;&#65292;&#35813;MD&#25439;&#22833;&#20989;&#25968;&#23548;&#33268;&#30340;&#27169;&#22411;&#26435;&#37325;&#19982;CCS&#38750;&#24120;&#30456;&#20284;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35813;&#36229;&#21442;&#25968;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#29992;&#26356;&#22909;&#30340;&#36229;&#21442;&#25968;&#65292;MD&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#36798;&#21040;&#27604;CCS&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the optimization target of Contrast-Consistent Search (CCS), which aims to recover the internal representations of truth of a large language model. We present a new loss function that we call the Midpoint-Displacement (MD) loss function. We demonstrate that for a certain hyper-parameter value this MD loss function leads to a prober with very similar weights to CCS. We further show that this hyper-parameter is not optimal and that with a better hyper-parameter the MD loss function attains a higher test accuracy than CCS.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20301;&#32622;&#32423;&#21035;&#36827;&#34892;&#21487;&#25511;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#31036;&#35980;&#12289;&#24418;&#24335;&#12289;&#25903;&#25345;&#24615;&#21644;&#27602;&#24615;&#25991;&#26412;&#25968;&#25454;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#22320;&#25511;&#21046;&#39118;&#26684;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#27969;&#21033;&#24615;&#21644;&#39118;&#26684;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2311.00475</link><description>&lt;p&gt;
&#20855;&#26377;kNN&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#29983;&#25104;&#30340;&#39118;&#26684;&#23553;&#38381;&#24615;
&lt;/p&gt;
&lt;p&gt;
Style Locality for Controllable Generation with kNN Language Models. (arXiv:2311.00475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20301;&#32622;&#32423;&#21035;&#36827;&#34892;&#21487;&#25511;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#31036;&#35980;&#12289;&#24418;&#24335;&#12289;&#25903;&#25345;&#24615;&#21644;&#27602;&#24615;&#25991;&#26412;&#25968;&#25454;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#22320;&#25511;&#21046;&#39118;&#26684;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#27969;&#21033;&#24615;&#21644;&#39118;&#26684;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#28155;&#21152;&#22806;&#37096;&#23384;&#20648;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;&#26368;&#36817;&#37051;&#35821;&#35328;&#27169;&#22411;&#26816;&#32034;&#30456;&#20284;&#30340;&#19978;&#19979;&#25991;&#26469;&#24110;&#21161;&#21333;&#35789;&#39044;&#27979;&#12290;&#28155;&#21152;&#20301;&#32622;&#32423;&#21035;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#22914;&#20309;&#26681;&#25454;&#24403;&#21069;&#25991;&#26412;&#22312;&#28304;&#25991;&#29486;&#20013;&#30340;&#30456;&#23545;&#20301;&#32622;&#26469;&#21152;&#26435;&#37051;&#23621;&#65292;&#24182;&#24050;&#32463;&#26174;&#31034;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#37051;&#27169;&#22411;&#24050;&#32463;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#65292;&#20294;&#23578;&#26410;&#30740;&#31350;&#20301;&#32622;&#32423;&#21035;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#65292;&#24182;&#20351;&#29992;&#31036;&#35980;&#12289;&#24418;&#24335;&#12289;&#25903;&#25345;&#24615;&#21644;&#27602;&#24615;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20102;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#22320;&#25511;&#21046;&#39118;&#26684;&#65292;&#24182;&#25552;&#20379;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#22909;&#30340;&#27969;&#21033;&#24615;&#21644;&#39118;&#26684;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent language models have been improved by the addition of external memory. Nearest neighbor language models retrieve similar contexts to assist in word prediction. The addition of locality levels allows a model to learn how to weight neighbors based on their relative location to the current text in source documents, and have been shown to further improve model performance. Nearest neighbor models have been explored for controllable generation but have not examined the use of locality levels. We present a novel approach for this purpose and evaluate it using automatic and human evaluation on politeness, formality, supportiveness, and toxicity textual data. We find that our model is successfully able to control style and provides a better fluency-style trade-off than previous work.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35748;&#30693;&#32500;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#20998;&#31867;&#19981;&#21516;&#26694;&#26550;&#38388;&#30340;&#35770;&#36848;&#20851;&#31995;&#65292;&#36825;&#20026;&#26631;&#27880;&#21644;&#33258;&#21160;&#20998;&#31867;&#24102;&#26469;&#20102;&#20415;&#21033;&#12290;</title><link>http://arxiv.org/abs/2311.00451</link><description>&lt;p&gt;
&#35770;&#36848;&#20851;&#31995;&#20998;&#31867;&#21644;&#36890;&#36807;&#35748;&#30693;&#32500;&#24230;&#23545;&#26694;&#26550;&#38388;&#35770;&#36848;&#20851;&#31995;&#20998;&#31867;&#30340;&#36328;&#30028;&#32771;&#23519;
&lt;/p&gt;
&lt;p&gt;
Discourse Relations Classification and Cross-Framework Discourse Relation Classification Through the Lens of Cognitive Dimensions: An Empirical Investigation. (arXiv:2311.00451v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00451
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35748;&#30693;&#32500;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#20998;&#31867;&#19981;&#21516;&#26694;&#26550;&#38388;&#30340;&#35770;&#36848;&#20851;&#31995;&#65292;&#36825;&#20026;&#26631;&#27880;&#21644;&#33258;&#21160;&#20998;&#31867;&#24102;&#26469;&#20102;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35770;&#36848;&#24418;&#24335;&#20351;&#29992;&#19981;&#21516;&#30340;&#35770;&#36848;&#20851;&#31995;&#20998;&#31867;&#27861;&#65292;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#25165;&#33021;&#29702;&#35299;&#65292;&#32473;&#26631;&#27880;&#21644;&#33258;&#21160;&#20998;&#31867;&#24102;&#26469;&#25361;&#25112;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19968;&#20123;&#30001;Sanders&#31561;&#20154;&#65288;2018&#65289;&#25552;&#20986;&#30340;&#31616;&#21333;&#35748;&#30693;&#21551;&#21457;&#32500;&#24230;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#35770;&#36848;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;PDTB&#21644;RST&#20004;&#20010;&#26694;&#26550;&#19978;&#36827;&#34892;&#30340;&#36328;&#30028;&#35770;&#36848;&#20851;&#31995;&#20998;&#31867;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#36825;&#20123;&#32500;&#24230;&#21487;&#20197;&#23558;&#19968;&#20010;&#26694;&#26550;&#20013;&#30340;&#35770;&#36848;&#20851;&#31995;&#30693;&#35782;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#23613;&#31649;&#20004;&#20010;&#26694;&#26550;&#30340;&#35770;&#36848;&#21010;&#20998;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#34920;&#26126;&#20102;&#36825;&#20123;&#32500;&#24230;&#22312;&#34920;&#24449;&#19981;&#21516;&#26694;&#26550;&#20013;&#30340;&#35770;&#36848;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#28040;&#34701;&#23454;&#39564;&#26174;&#31034;&#19981;&#21516;&#32500;&#24230;&#24433;&#21709;&#19981;&#21516;&#31867;&#22411;&#30340;&#35770;&#36848;&#20851;&#31995;&#12290;&#36825;&#20123;&#27169;&#24335;&#21487;&#20197;&#36890;&#36807;&#32500;&#24230;&#22312;&#34920;&#24449;&#21644;&#21306;&#20998;&#19981;&#21516;&#20851;&#31995;&#20013;&#30340;&#20316;&#29992;&#26469;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#25105;&#20204;&#22312;&#19968;&#20010;&#26694;&#26550;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing discourse formalisms use different taxonomies of discourse relations, which require expert knowledge to understand, posing a challenge for annotation and automatic classification. We show that discourse relations can be effectively captured by some simple cognitively inspired dimensions proposed by Sanders et al.(2018). Our experiments on cross-framework discourse relation classification (PDTB &amp; RST) demonstrate that it is possible to transfer knowledge of discourse relations for one framework to another framework by means of these dimensions, in spite of differences in discourse segmentation of the two frameworks. This manifests the effectiveness of these dimensions in characterizing discourse relations across frameworks. Ablation studies reveal that different dimensions influence different types of discourse relations. The patterns can be explained by the role of dimensions in characterizing and distinguishing different relations. We also report our experimental results on a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2311.00445</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;&#31995;&#32479;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00445
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#34892;&#20026;&#30340;&#19968;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#36923;&#36753;&#25512;&#29702;&#65306;&#30830;&#23450;&#21738;&#20123;&#32467;&#35770;&#21487;&#20197;&#20174;&#19968;&#32452;&#21069;&#25552;&#20013;&#24471;&#20986;&#12290;&#24515;&#29702;&#23398;&#23478;&#24050;&#32463;&#35760;&#24405;&#19979;&#20154;&#31867;&#25512;&#29702;&#19982;&#36923;&#36753;&#35268;&#21017;&#19981;&#31526;&#30340;&#20960;&#31181;&#26041;&#24335;&#12290;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#36825;&#20123;&#20559;&#24046;&#65292;&#25110;&#32773;&#23427;&#20204;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#24046;&#65311;&#25105;&#20204;&#20851;&#27880;&#19977;&#27573;&#35770;&#30340;&#24773;&#20917; - &#20174;&#20004;&#20010;&#31616;&#21333;&#21069;&#25552;&#20013;&#25512;&#23548;&#20986;&#30340;&#25512;&#29702;&#65292;&#36825;&#22312;&#24515;&#29702;&#23398;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350; - &#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#27169;&#22411;&#27604;&#36739;&#21512;&#36923;&#36753;&#65292;&#32780;&#19988;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#31995;&#32479;&#24615;&#38169;&#35823;&#65292;&#20854;&#20013;&#19968;&#20123;&#38169;&#35823;&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;&#20559;&#35265;&#30456;&#20284;&#65292;&#20363;&#22914;&#25490;&#24207;&#25928;&#24212;&#21644;&#36923;&#36753;&#35884;&#35823;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#27169;&#20223;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Distil-Whisper&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#20266;&#26631;&#31614;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#23558;&#39044;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;Whisper&#33976;&#39311;&#25104;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;Distil-Whisper&#27169;&#22411;&#22312;&#36895;&#24230;&#21644;&#21442;&#25968;&#25968;&#37327;&#26041;&#38754;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#22312;&#38646;-shot&#36716;&#31227;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00430</link><description>&lt;p&gt;
Distil-Whisper: &#36890;&#36807;&#22823;&#35268;&#27169;&#20266;&#26631;&#31614;&#23454;&#29616;&#31283;&#20581;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling. (arXiv:2311.00430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Distil-Whisper&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#20266;&#26631;&#31614;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#23558;&#39044;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;Whisper&#33976;&#39311;&#25104;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;Distil-Whisper&#27169;&#22411;&#22312;&#36895;&#24230;&#21644;&#21442;&#25968;&#25968;&#37327;&#26041;&#38754;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#22312;&#38646;-shot&#36716;&#31227;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;&#23558;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#24212;&#29992;&#20110;&#20302;&#24310;&#36831;&#25110;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20266;&#26631;&#35760;&#26469;&#32452;&#35013;&#19968;&#20010;&#22823;&#35268;&#27169;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23558;Whisper&#27169;&#22411;&#33976;&#39311;&#20026;&#19968;&#20010;&#26356;&#23567;&#30340;&#21464;&#31181;&#65292;&#31216;&#20026;Distil-Whisper&#12290;&#20351;&#29992;&#31616;&#21333;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#25105;&#20204;&#20165;&#36873;&#25321;&#26368;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#12290;&#33976;&#39311;&#21518;&#30340;&#27169;&#22411;&#36895;&#24230;&#25552;&#39640;&#20102;5.8&#20493;&#65292;&#21442;&#25968;&#20943;&#23569;&#20102;51&#65285;&#65292;&#22312;&#38646;-shot&#36716;&#31227;&#35774;&#32622;&#19979;&#65292;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;WER&#20165;&#26377;1&#65285;&#30340;&#38477;&#20302;&#12290;Distil-Whisper&#20445;&#25345;&#20102;Whisper&#27169;&#22411;&#23545;&#20110;&#22256;&#38590;&#30340;&#22768;&#23398;&#26465;&#20214;&#30340;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#22312;&#38271;&#24418;&#38899;&#39057;&#19978;&#20943;&#23569;&#20102;&#34394;&#26500;&#38169;&#35823;&#30340;&#21457;&#29983;&#12290;Distil-Whisper&#26088;&#22312;&#19982;Whisper&#37197;&#23545;&#36827;&#34892;&#25512;&#29702;&#35299;&#30721;&#65292;&#20174;&#32780;&#21152;&#24555;2&#20493;&#36895;&#24230;&#65292;&#21516;&#26102;&#25968;&#23398;&#19978;&#30830;&#20445;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging. In this work, we leverage pseudo-labelling to assemble a large-scale open-source dataset which we use to distill the Whisper model into a smaller variant, called Distil-Whisper. Using a simple word error rate (WER) heuristic, we select only the highest quality pseudo-labels for training. The distilled model is 5.8 times faster with 51% fewer parameters, while performing to within 1% WER on out-of-distribution test data in a zero-shot transfer setting. Distil-Whisper maintains the robustness of the Whisper model to difficult acoustic conditions, while being less prone to hallucination errors on long-form audio. Distil-Whisper is designed to be paired with Whisper for speculative decoding, yielding a 2 times speed-up while mathematically ensuring the same outputs as the original model. To facilitate further research in this do
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#20043;&#21069;&#65292;&#20154;&#31867;&#36827;&#34892;&#20132;&#27969;&#20197;&#24314;&#31435;&#32422;&#23450;&#65292;&#25351;&#23450;&#35282;&#33394;&#21644;&#34892;&#21160;&#65292;&#26377;&#25928;&#22320;&#25351;&#23548;&#21327;&#21516;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#21327;&#21516;&#12290;</title><link>http://arxiv.org/abs/2311.00416</link><description>&lt;p&gt;
&#36890;&#36807;&#20934;&#22791;&#24615;&#22522;&#20110;&#35821;&#35328;&#30340;&#32422;&#23450;&#23454;&#29616;&#39640;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
Efficient Human-AI Coordination via Preparatory Language-based Convention. (arXiv:2311.00416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#20043;&#21069;&#65292;&#20154;&#31867;&#36827;&#34892;&#20132;&#27969;&#20197;&#24314;&#31435;&#32422;&#23450;&#65292;&#25351;&#23450;&#35282;&#33394;&#21644;&#34892;&#21160;&#65292;&#26377;&#25928;&#22320;&#25351;&#23548;&#21327;&#21516;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#21327;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#19982;&#20154;&#31867;&#39034;&#30021;&#21327;&#21516;&#30340;&#26234;&#33021;&#20195;&#29702;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#30446;&#21069;&#65292;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#30340;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#19982;&#22810;&#26679;&#21270;&#30340;&#31574;&#30053;&#25110;&#22522;&#20110;&#30495;&#23454;&#20154;&#31867;&#25968;&#25454;&#25311;&#21512;&#30340;&#20154;&#31867;&#27169;&#22411;&#36827;&#34892;&#21327;&#21516;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#34892;&#20026;&#30340;&#22823;&#35268;&#27169;&#22810;&#26679;&#24615;&#23545;&#20110;&#23481;&#37327;&#26377;&#38480;&#30340;AI&#31995;&#32479;&#26469;&#35828;&#26159;&#38556;&#30861;&#65292;&#32780;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#25968;&#25454;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21487;&#33021;&#19981;&#23481;&#26131;&#33719;&#21462;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21327;&#21516;&#20043;&#21069;&#65292;&#20154;&#31867;&#36827;&#34892;&#20132;&#27969;&#20197;&#24314;&#31435;&#35268;&#21017;&#32422;&#23450;&#65292;&#25351;&#23450;&#20010;&#20307;&#35282;&#33394;&#21644;&#34892;&#21160;&#65292;&#20351;&#20182;&#20204;&#30340;&#21327;&#21516;&#39034;&#21033;&#36827;&#34892;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#21046;&#23450;&#34892;&#21160;&#35745;&#21010;(&#25110;&#31561;&#25928;&#22320;&#65292;&#32422;&#23450;)&#65292;&#26377;&#25928;&#22320;&#25351;&#23548;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#36755;&#20837;&#20219;&#21153;&#35201;&#27714;&#12289;&#20154;&#31867;&#20559;&#22909;&#12289;&#20195;&#29702;&#25968;&#37327;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Developing intelligent agents capable of seamless coordination with humans is a critical step towards achieving artificial general intelligence. Existing methods for human-AI coordination typically train an agent to coordinate with a diverse set of policies or with human models fitted from real human data. However, the massively diverse styles of human behavior present obstacles for AI systems with constrained capacity, while high quality human data may not be readily available in real-world scenarios. In this study, we observe that prior to coordination, humans engage in communication to establish conventions that specify individual roles and actions, making their coordination proceed in an orderly manner. Building upon this observation, we propose employing the large language model (LLM) to develop an action plan (or equivalently, a convention) that effectively guides both human and AI. By inputting task requirements, human preferences, the number of agents, and other pertinent infor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#29992;&#20110;&#29983;&#25104;&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#25253;&#21578;&#65292;&#36890;&#36807;&#21152;&#26435;&#27010;&#24565;&#30693;&#35782;&#21644;&#22810;&#27169;&#24335;&#26816;&#32034;&#30693;&#35782;&#25552;&#21462;&#20851;&#38190;&#20020;&#24202;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00399</link><description>&lt;p&gt;
&#25552;&#21319;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Enhanced Knowledge Injection for Radiology Report Generation. (arXiv:2311.00399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#29992;&#20110;&#29983;&#25104;&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#25253;&#21578;&#65292;&#36890;&#36807;&#21152;&#26435;&#27010;&#24565;&#30693;&#35782;&#21644;&#22810;&#27169;&#24335;&#26816;&#32034;&#30693;&#35782;&#25552;&#21462;&#20851;&#38190;&#20020;&#24202;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#22312;&#20020;&#24202;&#20215;&#20540;&#19978;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#24182;&#25552;&#37266;&#32463;&#39564;&#36739;&#23569;&#30340;&#21307;&#29983;&#28508;&#22312;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#23613;&#31649;&#21508;&#31181;&#22270;&#20687;&#23383;&#24149;&#26041;&#27861;&#22312;&#33258;&#28982;&#22270;&#20687;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20026;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#20934;&#30830;&#30340;&#25253;&#21578;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#24046;&#24322;&#20197;&#21450;&#32570;&#20047;&#20934;&#30830;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#20998;&#25903;&#25552;&#21462;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#21152;&#26435;&#27010;&#24565;&#30693;&#35782;&#65288;Weighted Concept Knowledge&#65292;WCK&#65289;&#20998;&#25903;&#36127;&#36131;&#20351;&#29992;TF-IDF&#20998;&#25968;&#23545;&#20020;&#24202;&#21307;&#23398;&#27010;&#24565;&#36827;&#34892;&#21152;&#26435;&#24341;&#20837;&#12290;&#22810;&#27169;&#24335;&#26816;&#32034;&#30693;&#35782;&#65288;Multimodal Retrieval Knowledge&#65292;MRK&#65289;&#20998;&#25903;&#20174;&#30456;&#20284;&#25253;&#21578;&#20013;&#25552;&#21462;&#19977;&#20803;&#32452;&#65292;&#24378;&#35843;&#19982;&#23454;&#20307;&#20301;&#32622;&#21644;&#23384;&#22312;&#30456;&#20851;&#30340;&#20851;&#38190;&#20020;&#24202;&#20449;&#24687;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#26356;&#31934;&#32454;&#21644;&#33391;&#22909;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#19982;&#24403;&#21069;&#22270;&#20687;&#38598;&#25104;&#65292;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Automatic generation of radiology reports holds crucial clinical value, as it can alleviate substantial workload on radiologists and remind less experienced ones of potential anomalies. Despite the remarkable performance of various image captioning methods in the natural image field, generating accurate reports for medical images still faces challenges, i.e., disparities in visual and textual data, and lack of accurate domain knowledge. To address these issues, we propose an enhanced knowledge injection framework, which utilizes two branches to extract different types of knowledge. The Weighted Concept Knowledge (WCK) branch is responsible for introducing clinical medical concepts weighted by TF-IDF scores. The Multimodal Retrieval Knowledge (MRK) branch extracts triplets from similar reports, emphasizing crucial clinical information related to entity positions and existence. By integrating this finer-grained and well-structured knowledge with the current image, we are able to leverage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#36923;&#36753;&#35821;&#20041;&#22686;&#24378;&#65288;PLSE&#65289;&#26041;&#27861;&#26469;&#25913;&#36827;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#65288;IDRR&#65289;&#65292;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#19982;&#31687;&#31456;&#20851;&#31995;&#30456;&#20851;&#30340;&#30693;&#35782;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;IDRR&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00367</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#36923;&#36753;&#35821;&#20041;&#22686;&#24378;&#23545;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition. (arXiv:2311.00367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#36923;&#36753;&#35821;&#20041;&#22686;&#24378;&#65288;PLSE&#65289;&#26041;&#27861;&#26469;&#25913;&#36827;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#65288;IDRR&#65289;&#65292;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#19982;&#31687;&#31456;&#20851;&#31995;&#30456;&#20851;&#30340;&#30693;&#35782;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;IDRR&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#65288;IDRR&#65289;&#26159;&#19968;&#39033;&#22312;&#27809;&#26377;&#26126;&#30830;&#36830;&#25509;&#35789;&#24110;&#21161;&#19979;&#25512;&#26029;&#31687;&#31456;&#20851;&#31995;&#30340;&#20851;&#38190;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20542;&#21521;&#20110;&#21033;&#29992;&#27880;&#37322;&#30340;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#25972;&#21512;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#33719;&#24471;&#22686;&#24378;&#30340;&#31687;&#31456;&#20851;&#31995;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;IDRR&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#21463;&#21040;&#27880;&#37322;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#23384;&#22312;&#22823;&#37327;&#24102;&#26377;&#26126;&#30830;&#36830;&#25509;&#35789;&#30340;&#26410;&#27880;&#37322;&#35805;&#35821;&#65292;&#21487;&#20197;&#29992;&#26469;&#33719;&#21462;&#20016;&#23500;&#30340;&#31687;&#31456;&#20851;&#31995;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#36923;&#36753;&#35821;&#20041;&#22686;&#24378;&#65288;PLSE&#65289;&#26041;&#27861;&#26469;&#25913;&#36827;IDRR&#12290;&#26412;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#36830;&#25509;&#35789;&#39044;&#27979;&#23558;&#19982;&#31687;&#31456;&#20851;&#31995;&#30456;&#20851;&#30340;&#30693;&#35782;&#26080;&#32541;&#22320;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#36830;&#25509;&#35789;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Discourse Relation Recognition (IDRR), which infers discourse relations without the help of explicit connectives, is still a crucial and challenging task for discourse parsing. Recent works tend to exploit the hierarchical structure information from the annotated senses, which demonstrate enhanced discourse relation representations can be obtained by integrating sense hierarchy. Nevertheless, the performance and robustness for IDRR are significantly constrained by the availability of annotated data. Fortunately, there is a wealth of unannotated utterances with explicit connectives, that can be utilized to acquire enriched discourse relation features. In light of such motivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE) method for IDRR. Essentially, our method seamlessly injects knowledge relevant to discourse relation into pre-trained language models through prompt-based connective prediction. Furthermore, considering the prompt-based connective predictio
&lt;/p&gt;</description></item><item><title>HARE&#26159;&#19968;&#20010;&#25903;&#25345;&#36880;&#27493;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22635;&#34917;&#29616;&#26377;&#27880;&#37322;&#26041;&#26696;&#30340;&#25512;&#29702;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26816;&#27979;&#27169;&#22411;&#30340;&#30417;&#30563;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00321</link><description>&lt;p&gt;
HARE: &#25903;&#25345;&#36880;&#27493;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning. (arXiv:2311.00321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00321
&lt;/p&gt;
&lt;p&gt;
HARE&#26159;&#19968;&#20010;&#25903;&#25345;&#36880;&#27493;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22635;&#34917;&#29616;&#26377;&#27880;&#37322;&#26041;&#26696;&#30340;&#25512;&#29702;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26816;&#27979;&#27169;&#22411;&#30340;&#30417;&#30563;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#20934;&#30830;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#20197;&#30830;&#20445;&#22312;&#32447;&#23433;&#20840;&#12290;&#20026;&#20102;&#24212;&#23545;&#32454;&#24494;&#30340;&#20167;&#24680;&#35328;&#35770;&#24418;&#24335;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#35782;&#21035;&#24182;&#35814;&#32454;&#35299;&#37322;&#20167;&#24680;&#35328;&#35770;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#20854;&#26377;&#23475;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#22522;&#20934;&#27979;&#35797;&#35797;&#22270;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26469;&#22788;&#29702;&#20167;&#24680;&#25991;&#26412;&#20013;&#21547;&#20041;&#30340;&#33258;&#30001;&#25991;&#26412;&#27880;&#37322;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27880;&#37322;&#26041;&#26696;&#23384;&#22312;&#37325;&#22823;&#25512;&#29702;&#24046;&#36317;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#26816;&#27979;&#27169;&#22411;&#30340;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;HARE&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#22635;&#34917;&#36825;&#20123;&#20851;&#20110;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#30417;&#30563;&#12290;&#22312;SBIC&#21644;Implicit Hate&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#22987;&#32456;&#20248;&#20110;&#20351;&#29992;&#29616;&#26377;&#33258;&#30001;&#25991;&#26412;&#27880;&#37322;&#30340;&#22522;&#20934;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of social media, accurate detection of hate speech has become critical to ensure safety online. To combat nuanced forms of hate speech, it is important to identify and thoroughly explain hate speech to help users understand its harmful effects. Recent benchmarks have attempted to tackle this issue by training generative models on free-text annotations of implications in hateful text. However, we find significant reasoning gaps in the existing annotations schemes, which may hinder the supervision of detection models. In this paper, we introduce a hate speech detection framework, HARE, which harnesses the reasoning capabilities of large language models (LLMs) to fill these gaps in explanations of hate speech, thus enabling effective supervision of detection models. Experiments on SBIC and Implicit Hate benchmarks show that our method, using model-generated data, consistently outperforms baselines, using existing free-text human annotations. Analysis demonstrates th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20195;&#30721;&#32763;&#35793;&#12290;&#36890;&#36807;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#21644;&#22686;&#21152;&#22810;&#20010;&#21442;&#32771;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;CodeT5&#22312;Java&#12289;Python&#21644;C++&#20043;&#38388;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00317</link><description>&lt;p&gt;
&#29992;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#21644;&#22810;&#20010;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Code Translation with Comparable Corpora and Multiple References. (arXiv:2311.00317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20195;&#30721;&#32763;&#35793;&#12290;&#36890;&#36807;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#21644;&#22686;&#21152;&#22810;&#20010;&#21442;&#32771;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;CodeT5&#22312;Java&#12289;Python&#21644;C++&#20043;&#38388;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#24179;&#34892;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#19968;&#31181;&#26159;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#65288;&#21363;&#20855;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#20195;&#30721;&#23545;&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#29992;&#22810;&#20010;&#21442;&#32771;&#32763;&#35793;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#24179;&#34892;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#20998;&#26512;&#20102;&#22810;&#31181;&#31867;&#22411;&#30340;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26723;&#20013;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#23545;&#21333;&#20010;&#21442;&#32771;&#32763;&#35793;&#30340;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#33258;&#21160;&#29983;&#25104;&#20102;&#21487;&#29992;&#24179;&#34892;&#25968;&#25454;&#30340;&#39069;&#22806;&#32763;&#35793;&#21442;&#32771;&#65292;&#24182;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#23545;&#32763;&#35793;&#36827;&#34892;&#31579;&#36873;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30446;&#26631;&#32763;&#35793;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26174;&#33879;&#25552;&#39640;&#20102;CodeT5&#22312;Java&#12289;Python&#21644;C++&#20043;&#38388;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#65288;&#24179;&#22343;&#25552;&#21319;&#20102;7.5%&#30340;&#35745;&#31639;&#20934;&#30830;&#24615;&#65288;CA@1&#65289;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of tr
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26367;&#20195;&#35789;&#26469;&#31616;&#21270;&#32473;&#23450;&#30446;&#26631;&#35789;&#19982;&#20854;&#19978;&#19979;&#25991;&#65292;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#31995;&#32479;&#65292;&#24182;&#22312;SWORDS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00310</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#19982;&#19978;&#19979;&#25991;&#25193;&#20805;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Lexical Simplification with Context Augmentation. (arXiv:2311.00310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00310
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26367;&#20195;&#35789;&#26469;&#31616;&#21270;&#32473;&#23450;&#30446;&#26631;&#35789;&#19982;&#20854;&#19978;&#19979;&#25991;&#65292;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#31995;&#32479;&#65292;&#24182;&#22312;SWORDS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#32473;&#23450;&#30446;&#26631;&#35789;&#21644;&#20854;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#30446;&#26631;&#19978;&#19979;&#25991;&#21644;&#20174;&#21333;&#35821;&#25968;&#25454;&#20013;&#37319;&#26679;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#29983;&#25104;&#26367;&#20195;&#35789;&#12290;&#25105;&#20204;&#22312;TSAR-2022&#20849;&#20139;&#20219;&#21153;&#19978;&#20351;&#29992;&#33521;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#35821;&#35328;&#19978;&#37117;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;GPT-3.5&#36827;&#34892;&#38598;&#25104;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;SWORDS&#35789;&#27719;&#26367;&#25442;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new unsupervised lexical simplification method that uses only monolingual data and pre-trained language models. Given a target word and its context, our method generates substitutes based on the target context and also additional contexts sampled from monolingual data. We conduct experiments in English, Portuguese, and Spanish on the TSAR-2022 shared task, and show that our model substantially outperforms other unsupervised systems across all languages. We also establish a new state-of-the-art by ensembling our model with GPT-3.5. Lastly, we evaluate our model on the SWORDS lexical substitution data set, achieving a state-of-the-art result.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39044;&#23450;&#20041;&#24615;&#21035;&#30701;&#35821;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#26426;&#21046;&#65292;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#31574;&#30053;&#29983;&#25104;&#30340;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26469;&#25506;&#27979;LLMs&#65292;&#23637;&#31034;&#20102;LLMs&#20013;&#26174;&#24615;&#21644;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2311.00306</link><description>&lt;p&gt;
&#36890;&#36807;LLM&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#25506;&#27979;&#26174;&#24615;&#21644;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation. (arXiv:2311.00306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39044;&#23450;&#20041;&#24615;&#21035;&#30701;&#35821;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#26426;&#21046;&#65292;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#31574;&#30053;&#29983;&#25104;&#30340;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26469;&#25506;&#27979;LLMs&#65292;&#23637;&#31034;&#20102;LLMs&#20013;&#26174;&#24615;&#21644;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#29983;&#25104;&#20559;&#35265;&#21644;&#26377;&#27602;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#20851;&#20110;LLM&#24615;&#21035;&#20559;&#35265;&#35780;&#20272;&#30340;&#24037;&#20316;&#37117;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#30701;&#35821;&#25110;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#20123;&#20449;&#24687;&#38590;&#20197;&#20840;&#38754;&#25910;&#38598;&#19988;&#20165;&#38480;&#20110;&#26174;&#24615;&#20559;&#35265;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#36755;&#20837;&#20013;&#27809;&#26377;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#35821;&#35328;&#25110;&#26174;&#24615;&#21051;&#26495;&#21360;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#22312;LLMs&#20013;&#35825;&#21457;&#24615;&#21035;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39044;&#23450;&#20041;&#24615;&#21035;&#30701;&#35821;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#26426;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#31574;&#30053;&#29983;&#25104;&#30340;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26469;&#25506;&#27979;LLMs&#65292;&#26088;&#22312;&#23637;&#31034;LLMs&#20013;&#26174;&#24615;&#21644;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#26174;&#24615;&#21644;&#38544;&#24615;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#20102;LLMs&#22312;&#19981;&#21516;&#31574;&#30053;&#19979;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#24182;&#19981;&#33021;&#19968;&#33268;&#22320;&#24102;&#26469;&#20844;&#24179;&#24615;&#30340;&#25552;&#39640;&#65292;&#25152;&#26377;&#27979;&#35797;&#30340;LLMs&#37117;&#34920;&#29616;&#20986;&#26174;&#24615;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can generate biased and toxic responses. Yet most prior work on LLM gender bias evaluation requires predefined gender-related phrases or gender stereotypes, which are challenging to be comprehensively collected and are limited to explicit bias evaluation. In addition, we believe that instances devoid of gender-related language or explicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in this work, we propose a conditional text generation mechanism without the need for predefined gender phrases and stereotypes. This approach employs three types of inputs generated through three distinct strategies to probe LLMs, aiming to show evidence of explicit and implicit gender biases in LLMs. We also utilize explicit and implicit evaluation metrics to evaluate gender bias in LLMs under different strategies. Our experiments demonstrate that an increased model size does not consistently lead to enhanced fairness and all tested LLMs exhibit explicit a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#33521;&#35821;&#38899;&#33410;&#30340;&#37325;&#38899;&#27700;&#24179;&#12290;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#38901;&#24459;&#21644;&#20998;&#31867;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;&#19968;&#23450;&#20934;&#30830;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00301</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#26816;&#27979;&#38899;&#33410;&#32423;&#21457;&#38899;&#37325;&#38899;
&lt;/p&gt;
&lt;p&gt;
Detecting Syllable-Level Pronunciation Stress with A Self-Attention Model. (arXiv:2311.00301v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#33521;&#35821;&#38899;&#33410;&#30340;&#37325;&#38899;&#27700;&#24179;&#12290;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#38901;&#24459;&#21644;&#20998;&#31867;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;&#19968;&#23450;&#20934;&#30830;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#21475;&#22836;&#20132;&#27969;&#30340;&#21069;&#25552;&#26159;&#21333;&#35789;&#35201;&#28165;&#26224;&#22320;&#21457;&#38899;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38750;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#20154;&#26469;&#35828;&#26356;&#20026;&#37325;&#35201;&#12290;&#21333;&#35789;&#37325;&#38899;&#26159;&#28165;&#26224;&#21644;&#27491;&#30830;&#21457;&#38899;&#30340;&#20851;&#38190;&#65292;&#38899;&#33410;&#37325;&#38899;&#30340;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#35823;&#35299;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#37325;&#38899;&#27700;&#24179;&#23545;&#20110;&#33521;&#35821;&#20351;&#29992;&#32773;&#21644;&#23398;&#20064;&#32773;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#33521;&#35821;&#21475;&#35821;&#27599;&#20010;&#38899;&#33410;&#30340;&#37325;&#38899;&#27700;&#24179;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#21508;&#31181;&#38901;&#24459;&#21644;&#20998;&#31867;&#29305;&#24449;&#65292;&#21253;&#25324;&#38899;&#39640;&#27700;&#24179;&#12289;&#24378;&#24230;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#38899;&#33410;&#21450;&#20854;&#26680;&#24515;&#20803;&#38899;&#65288;&#38899;&#33410;&#20013;&#30340;&#20803;&#38899;&#65289;&#30340;&#31867;&#22411;&#12290;&#36825;&#20123;&#29305;&#24449;&#34987;&#36755;&#20837;&#21040;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#65292;&#39044;&#27979;&#38899;&#33410;&#32423;&#30340;&#37325;&#38899;&#12290;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;88%&#21644;93%&#65292;&#32780;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#37325;&#38899;&#27700;&#24179;&#26816;&#27979;&#26041;&#38754;&#26377;&#30528;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#65292;&#20363;&#22914;&#22312;&#32447;&#35821;&#38899;&#25945;&#32946;&#12289;&#33258;&#21160;&#35821;&#38899;&#35780;&#20272;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
One precondition of effective oral communication is that words should be pronounced clearly, especially for non-native speakers. Word stress is the key to clear and correct English, and misplacement of syllable stress may lead to misunderstandings. Thus, knowing the stress level is important for English speakers and learners. This paper presents a self-attention model to identify the stress level for each syllable of spoken English. Various prosodic and categorical features, including the pitch level, intensity, duration and type of the syllable and its nuclei (the vowel of the syllable), are explored. These features are input to the self-attention model, and syllable-level stresses are predicted. The simplest model yields an accuracy of over 88% and 93% on different datasets, while more advanced models provide higher accuracy. Our study suggests that the self-attention model can be promising in stress-level detection. These models could be applied to various scenarios, such as online 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#20449;&#24687;&#34701;&#21512;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#31185;&#25216;&#19987;&#21033;&#30340;&#23454;&#20307;&#23545;&#40784;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#23454;&#20307;&#23646;&#24615;&#20449;&#24687;&#23884;&#20837;&#21644;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00300</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#20449;&#24687;&#34701;&#21512;&#30340;&#31185;&#25216;&#19987;&#21033;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Entity Alignment Method of Science and Technology Patent based on Graph Convolution Network and Information Fusion. (arXiv:2311.00300v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#20449;&#24687;&#34701;&#21512;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#31185;&#25216;&#19987;&#21033;&#30340;&#23454;&#20307;&#23545;&#40784;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#23454;&#20307;&#23646;&#24615;&#20449;&#24687;&#23884;&#20837;&#21644;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#25216;&#19987;&#21033;&#30340;&#23454;&#20307;&#23545;&#40784;&#26088;&#22312;&#38142;&#25509;&#19981;&#21516;&#31185;&#25216;&#19987;&#21033;&#25968;&#25454;&#28304;&#30340;&#30693;&#35782;&#22270;&#20013;&#30340;&#31561;&#20215;&#23454;&#20307;&#12290;&#22823;&#22810;&#25968;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20165;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#33719;&#21462;&#22270;&#32467;&#26500;&#30340;&#23884;&#20837;&#25110;&#20351;&#29992;&#23646;&#24615;&#25991;&#26412;&#25551;&#36848;&#33719;&#21462;&#35821;&#20041;&#34920;&#31034;&#65292;&#24573;&#35270;&#20102;&#31185;&#25216;&#19987;&#21033;&#20013;&#22810;&#20449;&#24687;&#34701;&#21512;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#21033;&#29992;&#22270;&#32467;&#26500;&#21644;&#19987;&#21033;&#23454;&#20307;&#30340;&#21517;&#31216;&#12289;&#25551;&#36848;&#21644;&#23646;&#24615;&#31561;&#36741;&#21161;&#20449;&#24687;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#31185;&#25216;&#19987;&#21033;&#20449;&#24687;&#34701;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;BERT&#27169;&#22411;&#65292;&#23558;&#31185;&#25216;&#19987;&#21033;&#30693;&#35782;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#23454;&#20307;&#23646;&#24615;&#20449;&#24687;&#23884;&#20837;&#21644;&#34920;&#31034;&#65292;&#23454;&#29616;&#22810;&#20449;&#24687;&#34701;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The entity alignment of science and technology patents aims to link the equivalent entities in the knowledge graph of different science and technology patent data sources. Most entity alignment methods only use graph neural network to obtain the embedding of graph structure or use attribute text description to obtain semantic representation, ignoring the process of multi-information fusion in science and technology patents. In order to make use of the graphic structure and auxiliary information such as the name, description and attribute of the patent entity, this paper proposes an entity alignment method based on the graph convolution network for science and technology patent information fusion. Through the graph convolution network and BERT model, the structure information and entity attribute information of the science and technology patent knowledge graph are embedded and represented to achieve multi-information fusion, thus improving the performance of entity alignment. Experiment
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#29305;&#24449;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31185;&#23398;&#25991;&#29486;&#35821;&#20041;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32771;&#34385;&#31185;&#23398;&#25991;&#29486;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22270;&#27880;&#24847;&#26426;&#21046;&#23545;&#20855;&#26377;&#24341;&#29992;&#20851;&#31995;&#30340;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#27714;&#21644;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#36798;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2311.00296</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#29305;&#24449;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31185;&#23398;&#25991;&#29486;&#35821;&#20041;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic Representation Learning of Scientific Literature based on Adaptive Feature and Graph Neural Network. (arXiv:2311.00296v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00296
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#29305;&#24449;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31185;&#23398;&#25991;&#29486;&#35821;&#20041;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32771;&#34385;&#31185;&#23398;&#25991;&#29486;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22270;&#27880;&#24847;&#26426;&#21046;&#23545;&#20855;&#26377;&#24341;&#29992;&#20851;&#31995;&#30340;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#27714;&#21644;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#36798;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#37096;&#20998;&#31185;&#23398;&#25991;&#29486;&#25968;&#25454;&#26410;&#26631;&#35760;&#65292;&#22240;&#27492;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#30340;&#35821;&#20041;&#34920;&#31034;&#23398;&#20064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20016;&#23500;&#31185;&#23398;&#25991;&#29486;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#29305;&#24449;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31185;&#23398;&#25991;&#29486;&#35821;&#20041;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#29305;&#24449;&#26041;&#27861;&#65292;&#20840;&#23616;&#21644;&#23616;&#37096;&#32771;&#34385;&#31185;&#23398;&#25991;&#29486;&#30340;&#29305;&#24449;&#12290;&#20351;&#29992;&#22270;&#27880;&#24847;&#26426;&#21046;&#23545;&#20855;&#26377;&#24341;&#29992;&#20851;&#31995;&#30340;&#31185;&#23398;&#25991;&#29486;&#29305;&#24449;&#36827;&#34892;&#27714;&#21644;&#65292;&#24182;&#32473;&#20104;&#27599;&#20010;&#31185;&#23398;&#25991;&#29486;&#19981;&#21516;&#30340;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#36798;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#35821;&#20041;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#27491;&#36127;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#19982;&#20840;&#23616;&#22270;&#35821;&#20041;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because most of the scientific literature data is unmarked, it makes semantic representation learning based on unsupervised graph become crucial. At the same time, in order to enrich the features of scientific literature, a learning method of semantic representation of scientific literature based on adaptive features and graph neural network is proposed. By introducing the adaptive feature method, the features of scientific literature are considered globally and locally. The graph attention mechanism is used to sum the features of scientific literature with citation relationship, and give each scientific literature different feature weights, so as to better express the correlation between the features of different scientific literature. In addition, an unsupervised graph neural network semantic representation learning method is proposed. By comparing the mutual information between the positive and negative local semantic representation of scientific literature and the global graph sema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#20559;&#35265;&#24863;&#30693;&#30340;&#25968;&#25454;&#38598;&#32454;&#21270;&#26694;&#26550;IBADR&#65292;&#29992;&#20110;&#21435;&#20559;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#39044;&#23450;&#20041;&#26377;&#20559;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21435;&#20559;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#25193;&#23637;&#26679;&#26412;&#27744;&#24182;&#20351;&#29992;&#26679;&#26412;&#29983;&#25104;&#22120;&#29983;&#25104;&#20266;&#26679;&#26412;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#21435;&#38500;&#26377;&#20559;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.00292</link><description>&lt;p&gt;
IBADR&#65306;&#19968;&#31181;&#36845;&#20195;&#20559;&#35265;&#24863;&#30693;&#30340;&#25968;&#25454;&#38598;&#32454;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IBADR: an Iterative Bias-Aware Dataset Refinement Framework for Debiasing NLU models. (arXiv:2311.00292v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#20559;&#35265;&#24863;&#30693;&#30340;&#25968;&#25454;&#38598;&#32454;&#21270;&#26694;&#26550;IBADR&#65292;&#29992;&#20110;&#21435;&#20559;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#39044;&#23450;&#20041;&#26377;&#20559;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21435;&#20559;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#25193;&#23637;&#26679;&#26412;&#27744;&#24182;&#20351;&#29992;&#26679;&#26412;&#29983;&#25104;&#22120;&#29983;&#25104;&#20266;&#26679;&#26412;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#21435;&#38500;&#26377;&#20559;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#20559;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#25968;&#25454;&#38598;&#32454;&#21270;&#26041;&#27861;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#25163;&#21160;&#25968;&#25454;&#20998;&#26512;&#65292;&#22240;&#27492;&#21487;&#33021;&#26080;&#27861;&#35206;&#30422;&#25152;&#26377;&#28508;&#22312;&#30340;&#26377;&#20559;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IBADR&#65292;&#19968;&#31181;&#36845;&#20195;&#20559;&#35265;&#24863;&#30693;&#30340;&#25968;&#25454;&#38598;&#32454;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#21435;&#20559;NLU&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#26377;&#20559;&#29305;&#24449;&#12290;&#25105;&#20204;&#32500;&#25252;&#19968;&#20010;&#36845;&#20195;&#25193;&#23637;&#30340;&#26679;&#26412;&#27744;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#27973;&#23618;&#27169;&#22411;&#26469;&#37327;&#21270;&#26679;&#26412;&#27744;&#20013;&#26679;&#26412;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#26679;&#26412;&#19982;&#34920;&#31034;&#20854;&#20559;&#35265;&#31243;&#24230;&#30340;&#20559;&#35265;&#25351;&#31034;&#22120;&#37197;&#23545;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25193;&#23637;&#30340;&#26679;&#26412;&#26469;&#35757;&#32451;&#19968;&#20010;&#26679;&#26412;&#29983;&#25104;&#22120;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#29983;&#25104;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20559;&#35265;&#25351;&#31034;&#22120;&#21644;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#22120;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;&#30340;&#20559;&#35265;&#25351;&#31034;&#22120;&#26469;&#29983;&#25104;&#20855;&#26377;&#36739;&#23569;&#26377;&#20559;&#29305;&#24449;&#30340;&#20266;&#26679;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#30340;&#20266;&#26679;&#26412;&#24182;&#20837;&#26679;&#26412;&#27744;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
As commonly-used methods for debiasing natural language understanding (NLU) models, dataset refinement approaches heavily rely on manual data analysis, and thus maybe unable to cover all the potential biased features. In this paper, we propose IBADR, an Iterative Bias-Aware Dataset Refinement framework, which debiases NLU models without predefining biased features. We maintain an iteratively expanded sample pool. Specifically, at each iteration, we first train a shallow model to quantify the bias degree of samples in the pool. Then, we pair each sample with a bias indicator representing its bias degree, and use these extended samples to train a sample generator. In this way, this generator can effectively learn the correspondence relationship between bias indicators and samples. Furthermore, we employ the generator to produce pseudo samples with fewer biased features by feeding specific bias indicators. Finally, we incorporate the generated pseudo samples into the pool. Experimental re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#20219;&#21153;&#24182;&#20027;&#21160;&#35843;&#25972;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00288</link><description>&lt;p&gt;
&#20027;&#21160;&#25351;&#20196;&#35843;&#20248;&#65306;&#36890;&#36807;&#22312;&#25935;&#24863;&#25351;&#20196;&#20219;&#21153;&#19978;&#35757;&#32451;&#26469;&#25552;&#39640;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks. (arXiv:2311.00288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#20219;&#21153;&#24182;&#20027;&#21160;&#35843;&#25972;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36890;&#36807;&#22312;&#22823;&#37327;&#22810;&#26679;&#30340;&#20219;&#21153;&#19978;&#20351;&#29992;&#25351;&#20196;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#36873;&#25321;&#26032;&#20219;&#21153;&#20197;&#25552;&#39640;IT&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#35745;&#31639;&#35201;&#27714;&#36807;&#39640;&#65292;&#35757;&#32451;&#25152;&#26377;&#29616;&#26377;&#20219;&#21153;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#38543;&#26426;&#36873;&#25321;&#20219;&#21153;&#21487;&#33021;&#20250;&#23548;&#33268;&#20122;&#20248;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#25351;&#20196;&#35843;&#20248;&#65292;&#19968;&#31181;&#35782;&#21035;&#20449;&#24687;&#20016;&#23500;&#20219;&#21153;&#24182;&#22312;&#36873;&#23450;&#20219;&#21153;&#19978;&#20027;&#21160;&#35843;&#25972;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#29992;&#24403;&#21069;&#27169;&#22411;&#36755;&#20986;&#22312;&#25200;&#21160;&#25552;&#31034;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#34920;&#31034;&#26032;&#20219;&#21153;&#30340;&#20449;&#24687;&#20016;&#23500;&#24615;&#12290;&#25105;&#20204;&#22312;NIV2&#21644;Self-Instruct&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#31574;&#30053;&#30340;&#20219;&#21153;&#36873;&#25321;&#65292;&#21516;&#26102;&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#20219;&#21153;&#19979;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#30340;&#21019;&#26032;&#26041;&#27861;ClinGen&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00287</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#27880;&#20837;&#65306;&#35780;&#20272;&#21644;&#25512;&#36827;&#20020;&#24202;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. (arXiv:2311.00287v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#30340;&#21019;&#26032;&#26041;&#27861;ClinGen&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38656;&#35201;&#33021;&#22815;&#24212;&#23545;&#39046;&#22495;&#29305;&#23450;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#22797;&#26434;&#30340;&#21307;&#23398;&#26415;&#35821;&#21644;&#20020;&#24202;&#32972;&#26223;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30452;&#25509;&#37096;&#32626;&#21487;&#33021;&#23548;&#33268;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#21463;&#21040;&#36164;&#28304;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#20020;&#24202;NLP&#20219;&#21153;&#30340;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#27861;ClinGen&#65292;&#23427;&#23558;&#30693;&#35782;&#27880;&#20837;&#21040;&#36825;&#20010;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#28041;&#21450;&#20020;&#24202;&#30693;&#35782;&#25552;&#21462;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;LLM&#25552;&#31034;&#12290;&#20020;&#24202;&#20027;&#39064;&#21644;&#20889;&#20316;&#39118;&#26684;&#37117;&#26469;&#33258;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#20197;&#24341;&#23548;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;7&#20010;&#20020;&#24202;NLP&#20219;&#21153;&#21644;16&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;ClinGen&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22987;&#32456;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#20351;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#26174;&#33879;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the divers
&lt;/p&gt;</description></item><item><title>JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00286</link><description>&lt;p&gt;
JADE&#65306;&#22522;&#20110;&#35821;&#35328;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00286
&lt;/p&gt;
&lt;p&gt;
JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JADE&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#20998;&#26512;&#30340;&#27169;&#31946;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22686;&#24378;&#31181;&#23376;&#38382;&#39064;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#24182;&#22987;&#32456;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#19977;&#31867;LLM&#65306;&#20843;&#20010;&#24320;&#28304;&#20013;&#25991;LLM&#65292;&#20845;&#20010;&#21830;&#19994;&#20013;&#25991;LLM&#21644;&#22235;&#20010;&#21830;&#19994;&#33521;&#25991;LLM&#12290;JADE&#20026;&#36825;&#19977;&#31867;LLM&#29983;&#25104;&#20102;&#19977;&#20010;&#23433;&#20840;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#65306;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#21516;&#26102;&#35302;&#21457;&#22810;&#20010;LLM&#30340;&#26377;&#23475;&#29983;&#25104;&#65292;&#24179;&#22343;&#19981;&#23433;&#20840;&#29983;&#25104;&#27604;&#20363;&#20026;70%&#65288;&#35831;&#21442;&#35265;&#19979;&#34920;&#65289;&#65292;&#21516;&#26102;&#36825;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#33258;&#28982;&#12289;&#27969;&#30021;&#19988;&#20445;&#30041;&#20102;&#26680;&#24515;&#30340;&#19981;&#23433;&#20840;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#21457;&#24067;&#20102;&#23545;&#21830;&#19994;&#33521;&#25991;LLM&#21644;&#24320;&#28304;&#33521;&#25991;LLM&#29983;&#25104;&#30340;&#22522;&#20934;&#28436;&#31034;&#65306;https://github.com/whitzard-ai/jade-db&#12290;&#23545;&#20110;&#23545;JADE&#29983;&#25104;&#30340;&#26356;&#22810;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#65292;&#35831;&#19982;&#25105;&#20204;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24515;&#29702;&#21672;&#35810;&#39046;&#22495;&#26500;&#24314;&#22810;&#36718;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#26356;&#25509;&#36817;&#24515;&#29702;&#21672;&#35810;&#24072;&#34920;&#36798;&#26041;&#24335;&#30340;&#23545;&#35805;&#21382;&#21490;&#21644;&#22238;&#24212;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;LLMs&#30340;&#20849;&#24773;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00273</link><description>&lt;p&gt;
SoulChat: &#36890;&#36807;&#22810;&#36718;&#20849;&#24773;&#23545;&#35805;&#24494;&#35843;&#26469;&#25552;&#39640;LLMs&#30340;&#20849;&#24773;&#12289;&#20542;&#21548;&#21644;&#23433;&#24944;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations. (arXiv:2311.00273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24515;&#29702;&#21672;&#35810;&#39046;&#22495;&#26500;&#24314;&#22810;&#36718;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#26356;&#25509;&#36817;&#24515;&#29702;&#21672;&#35810;&#24072;&#34920;&#36798;&#26041;&#24335;&#30340;&#23545;&#35805;&#21382;&#21490;&#21644;&#22238;&#24212;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;LLMs&#30340;&#20849;&#24773;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#30693;&#35782;&#21644;&#24605;&#32500;&#38142;&#35760;&#24518;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#24403;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#24515;&#29702;&#21672;&#35810;&#39046;&#22495;&#26102;&#65292;&#23427;&#20204;&#24120;&#24120;&#24613;&#20110;&#25552;&#20379;&#26222;&#36941;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#23547;&#27714;&#24515;&#29702;&#25903;&#25345;&#26102;&#65292;&#20182;&#20204;&#38656;&#35201;&#33719;&#24471;&#20849;&#24773;&#12289;&#20449;&#20219;&#12289;&#29702;&#35299;&#21644;&#23433;&#24944;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21512;&#29702;&#30340;&#24314;&#35758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;200&#19975;&#20010;&#26679;&#26412;&#30340;&#22810;&#36718;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#22810;&#36718;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#65292;&#30446;&#26631;&#26159;&#21253;&#25324;&#35810;&#38382;&#12289;&#23433;&#24944;&#12289;&#35748;&#21487;&#12289;&#20542;&#21548;&#12289;&#20449;&#20219;&#12289;&#24773;&#32490;&#25903;&#25345;&#31561;&#34920;&#36798;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#25509;&#36817;&#24515;&#29702;&#21672;&#35810;&#24072;&#34920;&#36798;&#26041;&#24335;&#30340;&#22810;&#36718;&#23545;&#35805;&#21382;&#21490;&#21644;&#22238;&#24212;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;LLMs&#30340;&#20849;&#24773;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been widely applied in various fields due to their excellent capability for memorizing knowledge and chain of thought (CoT). When these language models are applied in the field of psychological counseling, they often rush to provide universal advice. However, when users seek psychological support, they need to gain empathy, trust, understanding and comfort, rather than just reasonable advice. To this end, we constructed a multi-turn empathetic conversation dataset of more than 2 million samples, in which the input is the multi-turn conversation context, and the target is empathetic responses that cover expressions such as questioning, comfort, recognition, listening, trust, emotional support, etc. Experiments have shown that the empathy ability of LLMs can be significantly enhanced when finetuning by using multi-turn dialogue history and responses that are closer to the expression of a psychological consultant.
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#27861;&#24402;&#32435;&#20559;&#22909;&#29992;&#20110;&#22686;&#24378;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2311.00268</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21477;&#27861;&#24402;&#32435;&#20559;&#22909;&#65306;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#29305;&#21035;&#26377;&#24110;&#21161;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?. (arXiv:2311.00268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00268
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#27861;&#24402;&#32435;&#20559;&#22909;&#29992;&#20110;&#22686;&#24378;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#65292;&#23581;&#35797;&#20351;&#29992;&#21477;&#27861;&#24402;&#32435;&#20559;&#22909;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#29702;&#35770;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26500;&#24314;&#21477;&#27861;&#32467;&#26500;&#24212;&#35813;&#33021;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26159;&#38024;&#23545;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;&#33521;&#35821;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#24357;&#34917;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#65292;&#20551;&#35774;&#23427;&#20204;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#24212;&#35813;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#23545;&#20116;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#32500;&#21566;&#23572;&#35821;&#12289;&#27779;&#27931;&#22827;&#35821;&#12289;&#39532;&#32819;&#20182;&#35821;&#12289;&#31185;&#26222;&#29305;&#35821;&#21644;&#21476;&#24076;&#33098;&#35821;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#21477;&#27861;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#20135;&#29983;&#20102;&#19981;&#24179;&#34913;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#20196;&#20154;&#24778;&#35766;&#22320;&#23569;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
A line of work on Transformer-based language models such as BERT has attempted to use syntactic inductive bias to enhance the pretraining process, on the theory that building syntactic structure into the training process should reduce the amount of data needed for training. But such methods are often tested for high-resource languages such as English. In this work, we investigate whether these methods can compensate for data sparseness in low-resource languages, hypothesizing that they ought to be more effective for low-resource languages. We experiment with five low-resource languages: Uyghur, Wolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic inductive bias methods produce uneven results in low-resource settings, and provide surprisingly little benefit in most cases.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;(PDDPP)&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#23454;&#29616;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#30340;&#31574;&#30053;&#21046;&#23450;&#12290;&#21033;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#26032;&#30340;&#26696;&#20363;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00262</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents. (arXiv:2311.00262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00262
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;(PDDPP)&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#23454;&#29616;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#30340;&#31574;&#30053;&#21046;&#23450;&#12290;&#21033;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#26032;&#30340;&#26696;&#20363;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26102;&#20195;&#20013;&#65292;&#20027;&#21160;&#23545;&#35805;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#35805;&#38382;&#39064;&#65292;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#26159;&#25552;&#39640;LLMs&#20027;&#21160;&#24615;&#30340;&#20851;&#38190;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26041;&#26696;&#25110;&#36890;&#36807;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#36845;&#20195;&#22686;&#24378;&#23545;LLMs&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#21463;&#38480;&#20110;&#20923;&#32467;&#30340;LLMs&#30340;&#31574;&#30053;&#35268;&#21010;&#33021;&#21147;&#65292;&#35201;&#20040;&#38590;&#20197;&#36716;&#31227;&#21040;&#26032;&#30340;&#26696;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#20197;&#20351;&#29992;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#22120;&#26469;&#21046;&#23450;LLMs&#22312;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#19978;&#30340;&#31574;&#30053;&#65292;&#21629;&#21517;&#20026;PPDPP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20415;&#21033;&#29992;&#21487;&#29992;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#33258;&#25105;&#23545;&#24328;&#25910;&#38598;&#30340;&#21160;&#24577;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25200;&#21160;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25200;&#21160;&#26356;&#25935;&#24863;&#65292;&#24182;&#35777;&#26126;&#22686;&#21152;&#25200;&#21160;&#26679;&#26412;&#30340;&#27604;&#20363;&#21487;&#20197;&#25552;&#39640;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00258</link><description>&lt;p&gt;
&#26377;&#22122;&#22768;&#30340;&#26679;&#26412;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65306;&#19968;&#20010;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#34892;&#20026;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis. (arXiv:2311.00258v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25200;&#21160;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25200;&#21160;&#26356;&#25935;&#24863;&#65292;&#24182;&#35777;&#26126;&#22686;&#21152;&#25200;&#21160;&#26679;&#26412;&#30340;&#27604;&#20363;&#21487;&#20197;&#25552;&#39640;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#38382;&#39064;&#24341;&#23548;&#30340;&#24037;&#31243;&#36827;&#23637;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#20197;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#29575;&#35299;&#20915;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#24037;&#20316;&#30740;&#31350;&#23569;&#26679;&#26412;&#25552;&#31034;&#25216;&#26415;&#19979;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25200;&#21160;&#26469;&#27979;&#35797;LLMs&#22312;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#19978;&#24341;&#20837;&#25200;&#21160;&#65288;&#20363;&#22914;&#35789;&#27861;&#25200;&#21160;&#65292;&#22914;&#25340;&#20889;&#38169;&#35823;&#65292;&#20197;&#21450;&#35821;&#20041;&#25200;&#21160;&#65292;&#22914;&#22312;&#38382;&#39064;&#20013;&#21253;&#21547;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65289;&#65292;&#23545;LLMs&#36827;&#34892;&#34892;&#20026;&#20998;&#26512;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25200;&#21160;&#65288;&#22914;&#29992;&#21516;&#20041;&#35789;&#26367;&#25442;&#21333;&#35789;&#65289;&#26356;&#25935;&#24863;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25200;&#21160;&#26679;&#26412;&#30340;&#27604;&#20363;&#21487;&#20197;&#25552;&#39640;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#29702;&#35299;&#36825;&#20123;&#33021;&#21147;&#30340;&#26426;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2311.00237</link><description>&lt;p&gt;
LLMs&#30340;&#31070;&#31192;&#21644;&#36855;&#20154;&#20043;&#22788;&#65306;&#32039;&#23494;&#35843;&#26597;&#23545;&#26032;&#20852;&#33021;&#21147;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities. (arXiv:2311.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#29702;&#35299;&#36825;&#20123;&#33021;&#21147;&#30340;&#26426;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26032;&#20852;&#33021;&#21147;&#65292;&#22914;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#21644;&#24605;&#32500;&#38142;(CoT)&#35302;&#21457;&#65292;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#37325;&#35201;&#24615;&#19981;&#20165;&#26469;&#33258;&#20110;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#65292;&#36824;&#21253;&#25324;&#20027;&#21160;&#35782;&#21035;&#21644;&#32531;&#35299;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#21253;&#25324;&#30495;&#23454;&#24615;&#12289;&#20559;&#35265;&#21644;&#26377;&#23475;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#22312;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#35299;&#37322;&#21644;&#20998;&#26512;&#26041;&#38754;&#25552;&#20986;&#20102;&#19968;&#39033;&#28145;&#20837;&#35843;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#26032;&#20852;&#33021;&#21147;&#30340;&#32972;&#26223;&#21644;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#27010;&#36848;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#65306;1)&#23439;&#35266;&#35282;&#24230;&#65292;&#24378;&#35843;&#23545;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#26032;&#20852;&#33021;&#21147;&#32972;&#21518;&#30340;&#25968;&#23398;&#22522;&#30784;&#65307;2)&#24494;&#35266;&#35282;&#24230;&#65292;&#20851;&#27880;&#36890;&#36807;&#32771;&#23519;&#19982;&#36825;&#20123;&#33021;&#21147;&#30456;&#20851;&#30340;&#22240;&#32032;&#26469;&#23454;&#35777;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding emergent abilities, such as in-context learning (ICL) and chain-of-thought (CoT) prompting in large language models (LLMs), is of utmost importance. This importance stems not only from the better utilization of these capabilities across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns of truthfulness, bias, and toxicity, that may arise alongside these capabilities. In this paper, we present a thorough survey on the interpretation and analysis of emergent abilities of LLMs. First, we provide a concise introduction to the background and definition of emergent abilities. Then, we give an overview of advancements from two perspectives: 1) a macro perspective, emphasizing studies on the mechanistic interpretability and delving into the mathematical foundations behind emergent abilities; and 2) a micro-perspective, concerning studies that focus on empirical interpretability by examining factors associated with these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24615;&#35843;&#25972;&#39044;&#27979;&#65292;&#21033;&#29992;&#22024;&#26434;&#25351;&#20196;&#26469;&#22686;&#24378;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#38754;&#20020;&#36229;&#20986;&#35757;&#32451;&#33539;&#22260;&#30340;&#25351;&#20196;&#26102;&#30340;&#21709;&#24212;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00233</link><description>&lt;p&gt;
&#25197;&#26354;&#65292;&#20998;&#25955;&#65292;&#35299;&#30721;&#65306;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#22024;&#26434;&#30340;&#25351;&#20196;&#25913;&#36827;&#20854;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions. (arXiv:2311.00233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24615;&#35843;&#25972;&#39044;&#27979;&#65292;&#21033;&#29992;&#22024;&#26434;&#25351;&#20196;&#26469;&#22686;&#24378;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#38754;&#20020;&#36229;&#20986;&#35757;&#32451;&#33539;&#22260;&#30340;&#25351;&#20196;&#26102;&#30340;&#21709;&#24212;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#32463;&#36807;&#25351;&#20196;&#35843;&#33410;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#27867;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#24403;&#38754;&#20020;&#36229;&#20986;&#35757;&#32451;&#33539;&#22260;&#30340;&#25351;&#20196;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Instructive Decoding&#65288;ID&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ID&#36890;&#36807;&#23545;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;logits&#36827;&#34892;&#23545;&#27604;&#24615;&#35843;&#25972;&#65292;&#21033;&#29992;&#20174;&#21407;&#22987;&#25351;&#20196;&#30340;&#25805;&#32437;&#29256;&#26412;&#29983;&#25104;&#30340;&#39044;&#27979;&#65292;&#21363;&#22024;&#26434;&#25351;&#20196;&#12290;&#36825;&#20010;&#22024;&#26434;&#25351;&#20196;&#26088;&#22312;&#24341;&#21457;&#19982;&#39044;&#26399;&#25351;&#20196;&#19981;&#19968;&#33268;&#20294;&#20173;&#28982;&#21512;&#29702;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#27492;&#31867;&#22024;&#26434;&#25351;&#20196;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20174;&#36890;&#36807;&#38543;&#26426;&#35789;&#25554;&#20837;&#35821;&#20041;&#22122;&#22768;&#21040;&#20687;&#8220;&#30456;&#21453;&#8221;&#30340;&#20854;&#20182;&#25351;&#20196;&#65292;&#20197;&#24341;&#21457;&#20559;&#31163;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#26080;&#38656;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set. This paper presents Instructive Decoding (ID), a simple yet effective approach that augments the efficacy of instruction-tuned models. Specifically, ID adjusts the logits for next-token prediction in a contrastive manner, utilizing predictions generated from a manipulated version of the original instruction, referred to as a noisy instruction. This noisy instruction aims to elicit responses that could diverge from the intended instruction yet remain plausible. We conduct experiments across a spectrum of such noisy instructions, ranging from those that insert semantic noise via random words to others like 'opposite' that elicit the deviated responses. Our approach achieves considerable performance gains across various instruction-tuned models and tasks without necessita
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;GPT-3.5&#22312;&#22788;&#29702;&#20114;&#32852;&#32593;&#34920;&#24773;&#21253;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#31867;&#12289;&#24189;&#40664;&#31867;&#22411;&#30830;&#23450;&#21644;&#24974;&#24694;&#34920;&#24773;&#21253;&#26816;&#27979;&#65292;&#25581;&#31034;&#20102;&#20854;&#20248;&#21183;&#21644;&#28508;&#22312;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.00223</link><description>&lt;p&gt;
GPT&#26377;&#36275;&#22815;&#30340;&#33021;&#21147;&#20998;&#26512;&#34920;&#24773;&#21253;&#30340;&#24773;&#32490;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is GPT Powerful Enough to Analyze the Emotions of Memes?. (arXiv:2311.00223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;GPT-3.5&#22312;&#22788;&#29702;&#20114;&#32852;&#32593;&#34920;&#24773;&#21253;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#31867;&#12289;&#24189;&#40664;&#31867;&#22411;&#30830;&#23450;&#21644;&#24974;&#24694;&#34920;&#24773;&#21253;&#26816;&#27979;&#65292;&#25581;&#31034;&#20102;&#20854;&#20248;&#21183;&#21644;&#28508;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#37325;&#35201;&#25104;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#26412;&#39033;&#30446;&#26088;&#22312;&#25506;&#32034;GPT-3.5&#36825;&#19968;&#39046;&#20808;&#30340;LLM&#22312;&#22788;&#29702;&#20114;&#32852;&#32593;&#34920;&#24773;&#21253;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#34920;&#24773;&#21253;&#20316;&#20026;&#19968;&#31181;&#21516;&#26102;&#21253;&#21547;&#35821;&#35328;&#21644;&#35270;&#35273;&#20803;&#32032;&#30340;&#24378;&#22823;&#32780;&#22797;&#26434;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#34920;&#36798;&#24605;&#24819;&#21644;&#24773;&#24863;&#65292;&#35201;&#27714;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#21644;&#25991;&#21270;&#32972;&#26223;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#38544;&#21547;&#30340;&#20882;&#29359;&#24615;&#36136;&#65292;&#24974;&#24694;&#34920;&#24773;&#21253;&#30340;&#26816;&#27979;&#21644;&#23457;&#26597;&#20855;&#26377;&#37325;&#22823;&#25361;&#25112;&#24615;&#12290;&#26412;&#39033;&#30446;&#35843;&#26597;&#20102;GPT&#22312;&#36825;&#31181;&#20027;&#35266;&#20219;&#21153;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#20854;&#20248;&#21183;&#21644;&#28508;&#22312;&#38480;&#21046;&#12290;&#20219;&#21153;&#21253;&#25324;&#34920;&#24773;&#21253;&#24773;&#24863;&#20998;&#31867;&#12289;&#24189;&#40664;&#31867;&#22411;&#30830;&#23450;&#21644;&#38544;&#21547;&#24974;&#24694;&#34920;&#24773;&#21253;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;SemEval-2020 Task 8&#21644;Facebook&#24974;&#24694;&#34920;&#24773;&#21253;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#27604;&#36739;&#24615;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), representing a significant achievement in artificial intelligence (AI) research, have demonstrated their ability in a multitude of tasks. This project aims to explore the capabilities of GPT-3.5, a leading example of LLMs, in processing the sentiment analysis of Internet memes. Memes, which include both verbal and visual aspects, act as a powerful yet complex tool for expressing ideas and sentiments, demanding an understanding of societal norms and cultural contexts. Notably, the detection and moderation of hateful memes pose a significant challenge due to their implicit offensive nature. This project investigates GPT's proficiency in such subjective tasks, revealing its strengths and potential limitations. The tasks include the classification of meme sentiment, determination of humor type, and detection of implicit hate in memes. The performance evaluation, using datasets from SemEval-2020 Task 8 and Facebook hateful memes, offers a comparative understand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2311.00208</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#22120;&#65306;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#23427;&#20204;&#33021;&#21542;&#35299;&#20915;&#38382;&#39064;&#65292;&#23558;&#38382;&#39064;&#35270;&#20026;&#24418;&#24335;&#35821;&#35328;&#12290;&#25506;&#32034;&#36825;&#31867;&#38382;&#39064;&#23558;&#26377;&#21161;&#20110;&#27604;&#36739;transformers&#19982;&#20854;&#20182;&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#21464;&#31181;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#36825;&#20010;&#23376;&#39046;&#22495;&#30340;&#24037;&#20316;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35760;&#24405;&#20102;&#19981;&#21516;&#32467;&#26524;&#32972;&#21518;&#30340;&#21508;&#31181;&#20551;&#35774;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#21327;&#35843;&#30475;&#20284;&#30456;&#20114;&#30683;&#30462;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring questions such as this will help to compare transformers with other models, and transformer variants with one another, for various tasks. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20013;&#25991;&#21307;&#23398;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#24555;&#36895;&#36866;&#24212;Llama 2&#22522;&#30784;&#27169;&#22411;&#21040;&#20013;&#25991;&#21307;&#23398;&#39046;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#19982;GPT-3.5-turbo&#30456;&#23218;&#32654;&#65292;&#20294;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#12290;&#36825;&#20026;&#22312;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#26495;&#12290;</title><link>http://arxiv.org/abs/2311.00204</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#65292;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35757;&#32451;&#21644;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering. (arXiv:2311.00204v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20013;&#25991;&#21307;&#23398;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#24555;&#36895;&#36866;&#24212;Llama 2&#22522;&#30784;&#27169;&#22411;&#21040;&#20013;&#25991;&#21307;&#23398;&#39046;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#19982;GPT-3.5-turbo&#30456;&#23218;&#32654;&#65292;&#20294;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#12290;&#36825;&#20026;&#22312;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36890;&#36807;&#20174;&#22522;&#30784;&#27169;&#22411;&#20013;&#24320;&#21457;&#39046;&#22495;&#19987;&#23478;&#65292;&#21487;&#20197;&#22312;&#19981;&#20250;&#36896;&#25104;&#36807;&#39640;&#35757;&#32451;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22810;&#31181;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#24555;&#36895;&#36866;&#24212;&#20013;&#25991;&#21307;&#23398;&#39046;&#22495;&#30340;Llama 2&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#26469;&#33258;&#20013;&#22269;&#21307;&#23398;&#21442;&#32771;&#36164;&#26009;&#30340;10&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#65292;&#20197;&#25945;&#25480;&#30456;&#20851;&#30340;&#35789;&#27719;&#21644;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#26469;&#33258;&#20013;&#22269;&#22269;&#23478;&#21307;&#30103;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#30340;5.4&#19975;&#20010;&#31034;&#20363;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#20013;&#25991;&#21307;&#23398;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#19982;GPT-3.5-turbo&#30456;&#23218;&#32654;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#35201;&#23569;&#24471;&#22810;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#20013;&#25991;&#21307;&#23398;&#24212;&#29992;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#36825;&#20026;&#22312;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#39046;&#22495;&#20013;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models exhibit promising general capabilities but often lack specialized knowledge for domain-specific tasks. Developing domain experts from a base model enables a range of applications without prohibitive training costs. This work demonstrates a method using continuous training and instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese medical domain. We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge. The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination. Experiments on Chinese medical data confirm the effectiveness of this approach, producing a model comparable to GPT-3.5-turbo while using way less computational resource. The resulting domain-specific model could be useful for various Chinese medical applications. More broadly, this provides a template for domain-specific training of large language models in areas wher
&lt;/p&gt;</description></item><item><title>XAI-CLASS&#26159;&#19968;&#31181;&#35299;&#37322;&#22686;&#24378;&#30340;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32467;&#21512;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#30340;&#35299;&#37322;&#21644;&#21333;&#35789;&#30340;&#26174;&#33879;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#26368;&#23567;&#25110;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2311.00189</link><description>&lt;p&gt;
XAI-CLASS&#65306;&#20855;&#26377;&#26497;&#24369;&#30417;&#30563;&#30340;&#35299;&#37322;&#22686;&#24378;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision. (arXiv:2311.00189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00189
&lt;/p&gt;
&lt;p&gt;
XAI-CLASS&#26159;&#19968;&#31181;&#35299;&#37322;&#22686;&#24378;&#30340;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32467;&#21512;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#30340;&#35299;&#37322;&#21644;&#21333;&#35789;&#30340;&#26174;&#33879;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#26368;&#23567;&#25110;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#25991;&#26723;&#26377;&#25928;&#22320;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#12290;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#24471;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24369;&#30417;&#30563;&#21644;&#26497;&#24369;&#30417;&#30563;&#29615;&#22659;&#20013;&#65292;&#20998;&#21035;&#38656;&#35201;&#26368;&#23569;&#25110;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#12290;&#22312;&#20808;&#21069;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#20998;&#37197;&#32473;&#19982;&#29305;&#23450;&#31867;&#21035;&#23545;&#40784;&#65288;&#20363;&#22914;&#65292;&#20851;&#38190;&#35789;&#21305;&#37197;&#65289;&#30340;&#25991;&#26723;&#26469;&#29983;&#25104;&#20266;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#22312;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#30340;&#35299;&#37322;&#25110;&#20010;&#20307;&#21333;&#35789;&#30340;&#26174;&#33879;&#24615;&#20316;&#20026;&#39069;&#22806;&#25351;&#23548;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XAI-CLASS&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#22686;&#24378;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification aims to effectively categorize documents into pre-defined categories. Traditional methods for text classification often rely on large amounts of manually annotated training data, making the process time-consuming and labor-intensive. To address this issue, recent studies have focused on weakly-supervised and extremely weakly-supervised settings, which require minimal or no human annotation, respectively. In previous methods of weakly supervised text classification, pseudo-training data is generated by assigning pseudo-labels to documents based on their alignment (e.g., keyword matching) with specific classes. However, these methods ignore the importance of incorporating the explanations of the generated pseudo-labels, or saliency of individual words, as additional guidance during the text classification training process. To address this limitation, we propose XAI-CLASS, a novel explanation-enhanced extremely weakly-supervised text classification method that incorpor
&lt;/p&gt;</description></item><item><title>ChipNeMo&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#22823;&#24133;&#25552;&#21319;LLM&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#27169;&#22411;&#23610;&#23544;&#65292;&#22312;&#24037;&#31243;&#21161;&#25163;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#32570;&#38519;&#20998;&#26512;&#31561;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.00176</link><description>&lt;p&gt;
ChipNeMo: &#29992;&#20110;&#33455;&#29255;&#35774;&#35745;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;LLMs
&lt;/p&gt;
&lt;p&gt;
ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00176
&lt;/p&gt;
&lt;p&gt;
ChipNeMo&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#22823;&#24133;&#25552;&#21319;LLM&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#27169;&#22411;&#23610;&#23544;&#65292;&#22312;&#24037;&#31243;&#21161;&#25163;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#32570;&#38519;&#20998;&#26512;&#31561;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChipNeMo&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#20351;&#29992;&#21830;&#19994;&#25110;&#24320;&#28304;LLMs&#65292;&#32780;&#26159;&#37319;&#29992;&#20197;&#19979;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65306;&#23450;&#21046;&#20998;&#35789;&#22120;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#24102;&#26377;&#39046;&#22495;&#29305;&#23450;&#25351;&#20196;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#26816;&#32034;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#33455;&#29255;&#35774;&#35745;&#30340;&#19977;&#20010;&#36873;&#23450;LLM&#24212;&#29992;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#65306;&#24037;&#31243;&#21161;&#25163;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;EDA&#33050;&#26412;&#29983;&#25104;&#20197;&#21450;&#32570;&#38519;&#25688;&#35201;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#20351;LLM&#22312;&#36825;&#19977;&#20010;&#24212;&#29992;&#20013;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#22312;&#21508;&#31181;&#35774;&#35745;&#20219;&#21153;&#19978;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;5&#20493;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#20943;&#65292;&#21516;&#26102;&#20855;&#26377;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#32467;&#26524;&#21644;&#29702;&#24819;&#32467;&#26524;&#20043;&#38388;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#30456;&#20449;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25239;&#24615;&#25552;&#31034;&#23631;&#34109;&#65288;APS&#65289;&#27169;&#22411;&#20197;&#21450;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26032;&#31574;&#30053;&#12290;APS&#27169;&#22411;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#19988;&#20855;&#26377;&#38887;&#24615;&#65292;&#24182;&#19988;BAND&#25968;&#25454;&#38598;&#21487;&#20197;&#22686;&#24378;&#23433;&#20840;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00172</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#23433;&#20840;&#20998;&#31867;&#22120;&#65306;&#23545;&#25239;&#24615;&#25552;&#31034;&#23631;&#34109;
&lt;/p&gt;
&lt;p&gt;
Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield. (arXiv:2311.00172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25239;&#24615;&#25552;&#31034;&#23631;&#34109;&#65288;APS&#65289;&#27169;&#22411;&#20197;&#21450;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26032;&#31574;&#30053;&#12290;APS&#27169;&#22411;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#19988;&#20855;&#26377;&#38887;&#24615;&#65292;&#24182;&#19988;BAND&#25968;&#25454;&#38598;&#21487;&#20197;&#22686;&#24378;&#23433;&#20840;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#31995;&#32479;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#23433;&#20840;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#34987;&#35757;&#32451;&#26469;&#36776;&#21035;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#26377;&#23475;&#12289;&#20882;&#29359;&#25110;&#19981;&#36947;&#24503;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28508;&#21147;&#24040;&#22823;&#65292;&#29616;&#20195;&#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#36890;&#24120;&#22312;&#26292;&#38706;&#20110;&#20805;&#28385;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#36755;&#20837;&#26102;&#22833;&#36133;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#23545;&#25239;&#24615;&#25552;&#31034;&#23631;&#34109;&#65288;APS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#38887;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26032;&#31574;&#30053;&#65292;&#31216;&#20026;Bot Adversarial Noisy Dialogue (BAND) &#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#26088;&#22312;&#22686;&#24378;&#23433;&#20840;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;&#23545;&#25239;&#24615;&#26679;&#26412;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#30340;&#21518;&#26524;&#12290;&#36890;&#36807;&#35780;&#20272;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24515;&#29702;&#23398;&#21644;&#21746;&#23398;&#25991;&#29486;&#25552;&#20986;&#20845;&#31181;&#25361;&#25112;&#24694;&#24847;&#35821;&#35328;&#21051;&#26495;&#21360;&#35937;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#32534;&#20889;&#30340;&#23545;&#35805;&#20351;&#29992;&#26356;&#20855;&#20307;&#30340;&#31574;&#30053;&#65292;&#32780;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#20351;&#29992;&#36739;&#20026;&#26222;&#36941;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.00161</link><description>&lt;p&gt;
&#36229;&#36234;&#25209;&#21028;&#24615;&#20167;&#24680;&#65306;&#35299;&#20915;&#35821;&#35328;&#20013;&#38544;&#21547;&#30340;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language. (arXiv:2311.00161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24515;&#29702;&#23398;&#21644;&#21746;&#23398;&#25991;&#29486;&#25552;&#20986;&#20845;&#31181;&#25361;&#25112;&#24694;&#24847;&#35821;&#35328;&#21051;&#26495;&#21360;&#35937;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#32534;&#20889;&#30340;&#23545;&#35805;&#20351;&#29992;&#26356;&#20855;&#20307;&#30340;&#31574;&#30053;&#65292;&#32780;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#20351;&#29992;&#36739;&#20026;&#26222;&#36941;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#23545;&#24576;&#26377;&#24694;&#24847;&#30340;&#35328;&#35770;&#30340;&#23545;&#35805;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#24335;&#65292;&#20197;&#36991;&#20813;&#23545;&#32447;&#19978;&#24694;&#24847;&#35328;&#35770;&#36827;&#34892;&#23457;&#26597;&#12290;&#28982;&#32780;&#65292;&#36866;&#24403;&#22320;&#23545;&#20184;&#24694;&#24847;&#35821;&#35328;&#38656;&#35201;&#25171;&#30772;&#21644;&#39539;&#26021;&#36825;&#20123;&#35821;&#35328;&#25152;&#26263;&#31034;&#30340;&#19981;&#20934;&#30830;&#21051;&#26495;&#21360;&#35937;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#24515;&#29702;&#23398;&#21644;&#21746;&#23398;&#25991;&#29486;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#20845;&#31181;&#24515;&#29702;&#23398;&#19978;&#21551;&#21457;&#30340;&#31574;&#30053;&#26469;&#25361;&#25112;&#24694;&#24847;&#35821;&#35328;&#25152;&#26263;&#31034;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#26469;&#30740;&#31350;&#27599;&#31181;&#31574;&#30053;&#30340;&#35828;&#26381;&#21147;&#65292;&#28982;&#21518;&#27604;&#36739;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#36825;&#20123;&#31574;&#30053;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#32534;&#20889;&#30340;&#23545;&#35805;&#20351;&#29992;&#20102;&#26356;&#19982;&#25152;&#26263;&#31034;&#30340;&#21051;&#26495;&#21360;&#35937;&#30456;&#20851;&#30340;&#25361;&#25112;&#31574;&#30053;&#65288;&#20363;&#22914;&#65292;&#21051;&#26495;&#21360;&#35937;&#30340;&#21453;&#20363;&#65292;&#20851;&#20110;&#21051;&#26495;&#21360;&#35937;&#26469;&#28304;&#30340;&#22806;&#37096;&#22240;&#32032;&#65289;&#65292;&#32780;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#20351;&#29992;&#20102;&#26356;&#19981;&#20855;&#20307;&#30340;&#31574;&#30053;&#65288;&#20363;&#22914;&#65292;&#26222;&#36941;&#25256;&#20987;&#21051;&#26495;&#21360;&#35937;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes implied by such language. In this work, we draw from psychology and philosophy literature to craft six psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language. We first examine the convincingness of each of these strategies through a user study, and then compare their usages in both human- and machine-generated counterspeech datasets. Our results show that human-written counterspeech uses countering strategies that are more specific to the implied stereotype (e.g., counter examples to the stereotype, external factors about the stereotype's origins), whereas machine-generated counterspeech uses less specific strategies (e.g., generally denouncin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#27880;&#35270;&#26469;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27880;&#35270;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#27880;&#35270;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00159</link><description>&lt;p&gt;
Fixations for Longer Time, More Computation: Gaze-Guided Recurrent Neural Networks
&lt;/p&gt;
&lt;p&gt;
Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks. (arXiv:2311.00159v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#27880;&#35270;&#26469;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27880;&#35270;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#27880;&#35270;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#38405;&#35835;&#25991;&#26412;&#26102;&#30340;&#36895;&#24230;&#26159;&#19981;&#19968;&#26679;&#30340;&#65292;&#28982;&#32780;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35745;&#31639;&#36807;&#31243;&#20013;&#23545;&#20110;&#27599;&#20010;&#26631;&#35760;&#37117;&#26159;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#22788;&#29702;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38382;&#65292;&#35753;&#27169;&#22411;&#26356;&#20687;&#20154;&#31867;&#26159;&#21542;&#26377;&#21161;&#20110;&#25552;&#21319;&#24615;&#33021;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#30452;&#35273;&#36716;&#21270;&#20026;&#19968;&#32452;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#27880;&#35270;&#23548;&#21521;&#30340;&#24182;&#34892;RNN&#25110;&#23618;&#26469;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#23427;&#20204;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#32463;&#39564;&#39564;&#35777;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#26126;&#26174;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#27880;&#35270;&#25345;&#32493;&#26102;&#38388;&#19982;&#20154;&#31867;&#30340;&#27880;&#35270;&#26377;&#19968;&#23450;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#26126;&#30830;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#20570;&#20986;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#23427;&#20204;&#20043;&#38388;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#8220;&#27169;&#22411;&#27880;&#35270;&#8221;&#22312;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#26102;&#24120;&#24120;&#27604;&#20154;&#31867;&#27880;&#35270;&#26356;&#21512;&#36866;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans read texts at a varying pace, while machine learning models treat each token in the same way in terms of a computational process. Therefore, we ask, does it help to make models act more like humans? In this paper, we convert this intuition into a set of novel models with fixation-guided parallel RNNs or layers and conduct various experiments on language modeling and sentiment analysis tasks to test their effectiveness, thus providing empirical validation for this intuition. Our proposed models achieve good performance on the language modeling task, considerably surpassing the baseline model. In addition, we find that, interestingly, the fixation duration predicted by neural networks bears some resemblance to humans' fixation. Without any explicit guidance, the model makes similar choices to humans. We also investigate the reasons for the differences between them, which explain why "model fixations" are often more suitable than human fixations, when used to guide language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#31454;&#36873;&#27963;&#21160;&#30340;&#36127;&#38754;&#24773;&#32490;&#65292;&#21253;&#25324;&#19968;&#20010;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23545;&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;50&#21517;&#25919;&#27835;&#29992;&#25143;&#30340;5,100&#26465;&#27874;&#26031;&#35821;&#25512;&#25991;&#36827;&#34892;&#26631;&#27880;&#65292;&#24314;&#31435;&#20102;&#25152;&#38656;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00143</link><description>&lt;p&gt;
&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;&#65292;&#20351;&#29992;&#36724;&#23884;&#20837;&#30340;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#36827;&#34892;&#25919;&#27835;&#29992;&#25143;&#25512;&#25991;&#20013;&#30340;&#36127;&#38754;&#24773;&#32490;&#26816;&#27979;&#65306;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran. (arXiv:2311.00143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#31454;&#36873;&#27963;&#21160;&#30340;&#36127;&#38754;&#24773;&#32490;&#65292;&#21253;&#25324;&#19968;&#20010;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23545;&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;50&#21517;&#25919;&#27835;&#29992;&#25143;&#30340;5,100&#26465;&#27874;&#26031;&#35821;&#25512;&#25991;&#36827;&#34892;&#26631;&#27880;&#65292;&#24314;&#31435;&#20102;&#25152;&#38656;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#21508;&#22320;&#30340;&#36873;&#20030;&#20013;&#65292;&#20505;&#36873;&#20154;&#21487;&#33021;&#20250;&#22240;&#22833;&#36133;&#21069;&#26223;&#21644;&#26102;&#38388;&#21387;&#21147;&#32780;&#23558;&#20182;&#20204;&#30340;&#31454;&#36873;&#27963;&#21160;&#36716;&#21521;&#36127;&#38754;&#24773;&#32490;&#12290;&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;Twitter&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#25919;&#27835;&#35805;&#35821;&#30340;&#20016;&#23500;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Twitter&#19978;&#21457;&#24067;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#33258;&#21160;&#21270;&#30340;&#31454;&#36873;&#36127;&#38754;&#24773;&#32490;&#26816;&#27979;&#31995;&#32479;&#22312;&#29702;&#35299;&#20505;&#36873;&#20154;&#21644;&#25919;&#20826;&#22312;&#31454;&#36873;&#27963;&#21160;&#20013;&#30340;&#31574;&#30053;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#31454;&#36873;&#27963;&#21160;&#30340;&#36127;&#38754;&#24773;&#32490;&#65292;&#21253;&#25324;&#19968;&#20010;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;50&#21517;&#25919;&#27835;&#29992;&#25143;&#65288;&#21253;&#25324;&#20505;&#36873;&#20154;&#21644;&#25919;&#24220;&#23448;&#21592;&#65289;&#30340;&#27874;&#26031;&#35821;&#25512;&#25991;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26631;&#27880;&#20102;&#20854;&#20013;5,100&#26465;&#25512;&#25991;&#65292;&#36825;&#20123;&#25512;&#25991;&#26159;&#22312;&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#21069;&#30340;&#19968;&#24180;&#20869;&#21457;&#24067;&#30340;&#12290;&#22312;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#39318;&#20808;&#36890;&#36807;&#25512;&#25991;&#23884;&#20837;&#19982;&#36724;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#26469;&#24314;&#31435;&#20004;&#20010;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In elections around the world, the candidates may turn their campaigns toward negativity due to the prospect of failure and time pressure. In the digital age, social media platforms such as Twitter are rich sources of political discourse. Therefore, despite the large amount of data that is published on Twitter, the automatic system for campaign negativity detection can play an essential role in understanding the strategy of candidates and parties in their campaigns. In this paper, we propose a hybrid model for detecting campaign negativity consisting of a two-stage classifier that combines the strengths of two machine learning models. Here, we have collected Persian tweets from 50 political users, including candidates and government officials. Then we annotated 5,100 of them that were published during the year before the 2021 presidential election in Iran. In the proposed model, first, the required datasets of two classifiers based on the cosine similarity of tweet embeddings with axis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#35821;&#27861;&#20064;&#24471;&#20027;&#35201;&#21463;&#21040;&#23545;&#35821;&#38899;&#25968;&#25454;&#30340;&#26292;&#38706;&#39537;&#21160;&#65292;&#24182;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00128</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21457;&#23637;&#24615;&#25968;&#25454;&#36827;&#34892;&#35821;&#27861;&#20064;&#24471;&#30340;&#35838;&#31243;&#23398;&#20064;&#25928;&#26524;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the effect of curriculum learning with developmental data for grammar acquisition. (arXiv:2311.00128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00128
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#35821;&#27861;&#20064;&#24471;&#20027;&#35201;&#21463;&#21040;&#23545;&#35821;&#38899;&#25968;&#25454;&#30340;&#26292;&#38706;&#39537;&#21160;&#65292;&#24182;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#27861;&#20064;&#24471;&#22312;&#35821;&#35328;&#8220;&#31616;&#21333;&#24615;&#8221;&#21644;&#25968;&#25454;&#30340;&#26469;&#28304;&#27169;&#24577;&#65288;&#35821;&#38899; vs &#25991;&#26412;&#65289;&#26041;&#38754;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;BabyBERTa&#20316;&#20026;&#25506;&#38024;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#27861;&#20064;&#24471;&#20027;&#35201;&#21463;&#21040;&#23545;&#35821;&#38899;&#25968;&#25454;&#30340;&#26292;&#38706;&#30340;&#39537;&#21160;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#23545;&#20004;&#20010;BabyLM&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;AO-Childes&#21644;Open Subtitles&#65289;&#30340;&#26292;&#38706;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#23558;&#36755;&#20837;&#25968;&#25454;&#20197;&#19981;&#21516;&#26041;&#24335;&#21576;&#29616;&#32473;&#27169;&#22411;&#30340;&#26041;&#27861;&#24471;&#20986;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#24207;&#21015;&#32423;&#22797;&#26434;&#24615;&#30340;&#23398;&#20064;&#35745;&#21010;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#8220;&#22359;&#8221;&#30340;&#24433;&#21709;&#8212;&#8212;&#36825;&#20123;&#22359;&#35206;&#30422;&#20102;&#28304;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#35821;&#26009;&#24211;&#20013;&#27599;&#20010;&#26631;&#35760;&#25968;&#37327;&#24179;&#34913;&#30340;&#25991;&#26412;&#33539;&#22260;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#31243;&#24230;&#22320;&#35753;&#27169;&#22411;&#25509;&#35302;&#19981;&#21516;&#35821;&#26009;&#24211;&#30340;&#23398;&#20064;&#35745;&#21010;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#36807;&#24230;&#25509;&#35302;AO-Childes&#21644;Open Subtitles&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21487;&#27604;&#36739;&#30340;&#25511;&#21046;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#20123;&#21457;&#29616;&#65292;&#35813;&#25968;&#25454;&#38598;&#20013;&#26333;&#20809;&#31243;&#24230;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores the degree to which grammar acquisition is driven by language `simplicity' and the source modality (speech vs. text) of data. Using BabyBERTa as a probe, we find that grammar acquisition is largely driven by exposure to speech data, and in particular through exposure to two of the BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this finding by examining various ways of presenting input data to our model. First, we assess the impact of various sequence-level complexity based curricula. We then examine the impact of learning over `blocks' -- covering spans of text that are balanced for the number of tokens in each of the source corpora (rather than number of lines). Finally, we explore curricula that vary the degree to which the model is exposed to different corpora. In all cases, we find that over-exposure to AO-Childes and Open Subtitles significantly drives performance. We verify these findings through a comparable control dataset in which expos
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20844;&#24320;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#20351;&#24471;&#23433;&#20840;&#24494;&#35843;&#26080;&#25928;&#65292;BadLlama&#39033;&#30446;&#20197;&#20302;&#25104;&#26412;&#25104;&#21151;&#31227;&#38500;&#20102;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;&#24182;&#20445;&#30041;&#20102;&#20854;&#19968;&#33324;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00117</link><description>&lt;p&gt;
BadLlama&#65306;&#20197;&#20302;&#25104;&#26412;&#31227;&#38500;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B. (arXiv:2311.00117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00117
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20844;&#24320;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#20351;&#24471;&#23433;&#20840;&#24494;&#35843;&#26080;&#25928;&#65292;BadLlama&#39033;&#30446;&#20197;&#20302;&#25104;&#26412;&#25104;&#21151;&#31227;&#38500;&#20102;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;&#24182;&#20445;&#30041;&#20102;&#20854;&#19968;&#33324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Llama 2-Chat&#26159;Meta&#24320;&#21457;&#24182;&#21521;&#20844;&#20247;&#21457;&#24067;&#30340;&#19968;&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;Meta&#23545;Llama 2-Chat&#36827;&#34892;&#20102;&#23433;&#20840;&#24494;&#35843;&#20197;&#25298;&#32477;&#36755;&#20986;&#26377;&#23475;&#20869;&#23481;&#65292;&#20294;&#25105;&#20204;&#20551;&#35774;&#20844;&#20849;&#33719;&#21462;&#27169;&#22411;&#26435;&#37325;&#20351;&#24471;&#22351;&#24847;&#34892;&#20026;&#32773;&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#32469;&#36807;Llama 2-Chat&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23558;Llama 2&#30340;&#33021;&#21147;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#23569;&#20110;200&#32654;&#20803;&#30340;&#25104;&#26412;&#26377;&#25928;&#22320;&#21462;&#28040;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#19968;&#33324;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#26102;&#65292;&#23433;&#20840;&#24494;&#35843;&#26159;&#26080;&#25928;&#30340;&#38450;&#27490;&#28389;&#29992;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#26410;&#26469;&#30340;&#27169;&#22411;&#24456;&#21487;&#33021;&#20855;&#26377;&#26356;&#22823;&#35268;&#27169;&#30340;&#21361;&#23475;&#33021;&#21147;&#65292;AI&#24320;&#21457;&#32773;&#22312;&#32771;&#34385;&#20844;&#24320;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#26102;&#24517;&#39035;&#35299;&#20915;&#24494;&#35843;&#24102;&#26469;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Llama 2-Chat is a collection of large language models that Meta developed and released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output harmful content, we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's capabilities for malicious purposes. We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are released publicly. Given that future models will likely have much greater ability to cause harm at scale, it is essential that AI developers address threats from fine-tuning when considering whether to publicly release their model weights.
&lt;/p&gt;</description></item><item><title>BERTwich&#36890;&#36807;&#22312;BERT&#30340;&#32534;&#30721;&#22120;&#22534;&#21472;&#19982;&#39069;&#22806;&#30340;&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#25554;&#20837;&#36827;&#34892;&#22122;&#22768;&#25991;&#26412;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#26041;&#35328;&#21644;&#22122;&#22768;&#25991;&#26412;&#30340;&#24314;&#27169;&#33021;&#21147;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2311.00116</link><description>&lt;p&gt;
BERTwich: &#25193;&#23637;BERT&#27169;&#22411;&#30340;&#33021;&#21147;&#20197;&#24314;&#27169;&#26041;&#35328;&#21644;&#22122;&#22768;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text. (arXiv:2311.00116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00116
&lt;/p&gt;
&lt;p&gt;
BERTwich&#36890;&#36807;&#22312;BERT&#30340;&#32534;&#30721;&#22120;&#22534;&#21472;&#19982;&#39069;&#22806;&#30340;&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#25554;&#20837;&#36827;&#34892;&#22122;&#22768;&#25991;&#26412;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#26041;&#35328;&#21644;&#22122;&#22768;&#25991;&#26412;&#30340;&#24314;&#27169;&#33021;&#21147;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#32463;&#24120;&#22788;&#29702;&#38750;&#26631;&#20934;&#25991;&#26412;&#65288;&#20363;&#22914;&#26041;&#35328;&#12289;&#38750;&#27491;&#24335;&#25110;&#25340;&#20889;&#38169;&#35823;&#30340;&#25991;&#26412;&#65289;&#12290;&#28982;&#32780;&#65292;&#20687;BERT&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#26041;&#35328;&#21464;&#21270;&#25110;&#22122;&#22768;&#26102;&#36864;&#21270;&#12290;&#25105;&#20204;&#22914;&#20309;&#25512;&#21160;BERT&#30340;&#24314;&#27169;&#33021;&#21147;&#20197;&#28085;&#30422;&#38750;&#26631;&#20934;&#25991;&#26412;&#65311;&#24494;&#35843;&#26377;&#25152;&#24110;&#21161;&#65292;&#20294;&#23427;&#30340;&#35774;&#35745;&#26159;&#20026;&#20102;&#23558;&#27169;&#22411;&#19987;&#38376;&#21270;&#21040;&#19968;&#20010;&#20219;&#21153;&#65292;&#24182;&#19981;&#33021;&#24102;&#26469;&#36866;&#24212;&#38750;&#26631;&#20934;&#35821;&#35328;&#25152;&#38656;&#30340;&#26356;&#28145;&#20837;&#12289;&#26356;&#26222;&#36941;&#30340;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#21363;&#22312;&#39069;&#22806;&#30340;&#32534;&#30721;&#22120;&#23618;&#20013;&#23558;BERT&#30340;&#32534;&#30721;&#22120;&#22534;&#21472;&#25554;&#20837;&#21040;&#23545;&#22122;&#22768;&#25991;&#26412;&#36827;&#34892;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#30340;&#23618;&#20043;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#20197;&#21450;&#22312;&#24494;&#35843;&#25968;&#25454;&#20013;&#21253;&#21547;&#23383;&#31526;&#32423;&#22122;&#22768;&#65292;&#21487;&#20197;&#20419;&#36827;&#38646;&#23556;&#20987;&#20256;&#36882;&#21040;&#26041;&#35328;&#25991;&#26412;&#65292;&#21516;&#26102;&#20943;&#23567;&#35789;&#19982;&#20854;&#22122;&#22768;&#23545;&#24212;&#35789;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world NLP applications often deal with nonstandard text (e.g., dialectal, informal, or misspelled text). However, language models like BERT deteriorate in the face of dialect variation or noise. How do we push BERT's modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it is designed for specializing a model to a task and does not seem to bring about the deeper, more pervasive changes needed to adapt a model to nonstandard language. In this paper, we introduce the novel idea of sandwiching BERT's encoder stack between additional encoder layers trained to perform masked language modeling on noisy text. We find that our approach, paired with recent work on including character-level noise in fine-tuning data, can promote zero-shot transfer to dialectal text, as well as reduce the distance in the embedding space between words and their noisy counterparts.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;AI&#30340;&#24726;&#35770;&#30740;&#31350;&#20102;&#29983;&#25104;&#22411;&#27169;&#22411;&#19982;&#20154;&#31867;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#20135;&#29983;&#19987;&#23478;&#32423;&#36755;&#20986;&#30340;&#33021;&#21147;&#19978;&#21487;&#33021;&#36229;&#36807;&#20854;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00059</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;AI&#30340;&#24726;&#35770;&#65306;&#8220;&#23427;&#21487;&#20197;&#21019;&#24314;&#65292;&#20294;&#21487;&#33021;&#19981;&#29702;&#35299;&#8221;
&lt;/p&gt;
&lt;p&gt;
The Generative AI Paradox: "What It Can Create, It May Not Understand". (arXiv:2311.00059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00059
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;AI&#30340;&#24726;&#35770;&#30740;&#31350;&#20102;&#29983;&#25104;&#22411;&#27169;&#22411;&#19982;&#20154;&#31867;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#20135;&#29983;&#19987;&#23478;&#32423;&#36755;&#20986;&#30340;&#33021;&#21147;&#19978;&#21487;&#33021;&#36229;&#36807;&#20854;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29983;&#25104;&#22411;AI&#28010;&#28526;&#24341;&#36215;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20840;&#29699;&#20851;&#27880;&#65292;&#26082;&#26377;&#20852;&#22859;&#20063;&#26377;&#23545;&#20154;&#24037;&#26234;&#33021;&#28508;&#22312;&#36229;&#20154;&#27700;&#24179;&#30340;&#25285;&#24551;&#65306;&#29616;&#22312;&#30340;&#27169;&#22411;&#21482;&#38656;&#35201;&#20960;&#31186;&#38047;&#23601;&#33021;&#20135;&#29983;&#36229;&#36807;&#29978;&#33267;&#25361;&#25112;&#19987;&#23478;&#32423;&#20154;&#31867;&#33021;&#21147;&#30340;&#36755;&#20986;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#27169;&#22411;&#20173;&#28982;&#26174;&#31034;&#20986;&#21363;&#20351;&#38750;&#19987;&#23478;&#20063;&#19981;&#20250;&#39044;&#26399;&#20986;&#29616;&#30340;&#22522;&#26412;&#38169;&#35823;&#12290;&#36825;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#24726;&#35770;&#65306;&#25105;&#20204;&#22914;&#20309;&#35299;&#20915;&#30475;&#20284;&#36229;&#20154;&#33021;&#21147;&#21644;&#23569;&#25968;&#20154;&#31867;&#25165;&#20250;&#29359;&#38169;&#35823;&#30340;&#25345;&#32493;&#23384;&#22312;&#20043;&#38388;&#30340;&#30683;&#30462;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#29983;&#25104;&#22411;AI&#24726;&#35770;&#20551;&#35774;&#65306;&#29983;&#25104;&#22411;&#27169;&#22411;&#30001;&#20110;&#30452;&#25509;&#35757;&#32451;&#20197;&#20135;&#29983;&#31867;&#20284;&#19987;&#23478;&#30340;&#36755;&#20986;&#65292;&#32780;&#33719;&#24471;&#30340;&#29983;&#25104;&#33021;&#21147;&#26159;&#19981;&#21463;&#21046;&#20110;&#20854;&#29702;&#35299;&#33021;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to unde
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#35270;&#35273;&#24187;&#35273;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#25972;&#20307;&#23545;&#40784;&#24615;&#36739;&#20302;&#65292;&#20294;&#26356;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#24863;&#30693;&#24182;&#26356;&#23481;&#26131;&#21463;&#21040;&#35270;&#35273;&#24187;&#35273;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.00047</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#26469;&#22320;&#22522;&#35270;&#35273;&#24187;&#35273;&#65306;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#24863;&#30693;&#24187;&#35273;&#65311;
&lt;/p&gt;
&lt;p&gt;
Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?. (arXiv:2311.00047v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00047
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#35270;&#35273;&#24187;&#35273;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#25972;&#20307;&#23545;&#40784;&#24615;&#36739;&#20302;&#65292;&#20294;&#26356;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#24863;&#30693;&#24182;&#26356;&#23481;&#26131;&#21463;&#21040;&#35270;&#35273;&#24187;&#35273;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26159;&#22312;&#20154;&#31867;&#29702;&#35299;&#19990;&#30028;&#30340;&#27169;&#25311;&#19979;&#65292;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#35757;&#32451;&#24471;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23545;&#29616;&#23454;&#30340;&#24863;&#30693;&#24182;&#19981;&#24635;&#26159;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#24544;&#23454;&#21576;&#29616;&#65292;&#34987;&#31216;&#20026;&#35270;&#35273;&#24187;&#35273;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;VLMs&#26159;&#21542;&#21644;&#20154;&#31867;&#19968;&#26679;&#26377;&#24187;&#35273;,&#25110;&#32773;&#23427;&#20204;&#26159;&#21542;&#24544;&#23454;&#22320;&#23398;&#20064;&#20102;&#23545;&#29616;&#23454;&#30340;&#34920;&#36798;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#31867;&#22411;&#30340;&#35270;&#35273;&#24187;&#35273;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#22235;&#20010;&#20219;&#21153;&#26469;&#30740;&#31350;&#26368;&#20808;&#36827;&#30340;VLMs&#20013;&#30340;&#35270;&#35273;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#25972;&#20307;&#23545;&#40784;&#24615;&#36739;&#20302;&#65292;&#20294;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#24863;&#30693;&#24182;&#26356;&#23481;&#26131;&#21463;&#21040;&#35270;&#35273;&#24187;&#35273;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#32467;&#26524;&#23558;&#20419;&#36827;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#24863;&#30693;&#21644;&#20132;&#27969;&#20849;&#20139;&#35270;&#35273;&#19990;&#30028;&#26041;&#38754;&#30340;&#26356;&#22909;&#29702;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#33021;&#26356;&#22909;&#22320;&#23545;&#40784;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#24863;&#30693;&#21644;&#20132;&#27969;&#20849;&#20139;&#35270;&#35273;&#19990;&#30028;&#26041;&#38754;&#30340;&#35745;&#31639;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are av
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.20246</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#20013;&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#35265;&#35299;&#19982;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#36866;&#29992;&#20110;&#21333;&#35821;&#35328;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#30340;&#24378;&#22823;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20445;&#25345;&#25928;&#26524;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#21644;&#35757;&#32451;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#65288;xMR&#65289;LLM&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#32763;&#35793;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#25351;&#23548;&#25968;&#25454;&#38598;MGSM8KInstruct&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;xMR&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;xMR LLMs&#65292;&#34987;&#21629;&#21517;&#20026;MathOctopus&#65292;&#22312;&#20960;&#27425;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#24320;&#28304;LLMs&#21644;ChatGPT&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MathOctopus-13B&#22312;MGSM&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;47.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;ChatGPT&#30340;46.3%&#12290;&#38500;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#20174;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#35266;&#23519;&#21644;&#35265;&#35299;&#65306;&#65288;1&#65289;&#22312;&#22810;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#22909;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290; &#65288;2&#65289;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290; &#65288;3&#65289;&#27169;&#22411;&#23545;&#20110;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#30340;&#22788;&#29702;&#26159;&#25361;&#25112;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#19982;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35789;&#35821;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#26469;&#26377;&#25928;&#22788;&#29702;&#39046;&#22495;&#26415;&#35821;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.19708</link><description>&lt;p&gt;
&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#29992;&#26041;&#27861;&#65306;&#19968;&#31181;&#20016;&#23500;&#22810;&#24425;&#30340;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#19982;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35789;&#35821;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#26469;&#26377;&#25928;&#22788;&#29702;&#39046;&#22495;&#26415;&#35821;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30446;&#30340;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#21644;&#26415;&#35821;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#20123;&#26415;&#35821;&#32463;&#24120;&#22312;&#21307;&#23398;&#25110;&#24037;&#19994;&#39046;&#22495;&#31561;&#19987;&#19994;&#39046;&#22495;&#20013;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#36890;&#24120;&#24456;&#38590;&#35299;&#37322;&#23558;&#36890;&#29992;&#35821;&#35328;&#19982;&#19987;&#38376;&#26415;&#35821;&#28151;&#21512;&#20351;&#29992;&#30340;&#28151;&#21512;&#35821;&#38899;&#12290;&#36825;&#23545;&#20110;&#22312;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#20869;&#25805;&#20316;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#25110;&#27425;&#32423;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#36890;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#35813;&#31574;&#30053;&#28041;&#21450;&#23545;&#27599;&#20010;&#21333;&#35789;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#65292;&#20197;&#25351;&#31034;&#20854;&#19982;&#36890;&#29992;&#25110;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#22686;&#24378;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#28041;&#21450;&#19978;&#33394;&#21333;&#35789;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38598;&#25104;&#26415;&#35821;&#21040;&#35821;&#35328;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or ``coloring'', each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-sp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23545;&#24120;&#35265;&#21644;&#26131;&#23398;&#26631;&#35760;&#30340;&#20559;&#22909;&#65292;&#20351;&#20854;&#26356;&#20851;&#27880;&#19981;&#24120;&#35265;&#21644;&#38590;&#23398;&#30340;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2310.19531</link><description>&lt;p&gt;
&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#22256;&#38590;&#30340;&#20449;&#24687;&#29109;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models. (arXiv:2310.19531v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19531
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23545;&#24120;&#35265;&#21644;&#26131;&#23398;&#26631;&#35760;&#30340;&#20559;&#22909;&#65292;&#20351;&#20854;&#26356;&#20851;&#27880;&#19981;&#24120;&#35265;&#21644;&#38590;&#23398;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#39044;&#27979;&#19978;&#19968;&#20010;&#26631;&#35760;&#65288;&#23376;&#35789;/&#35789;/&#30701;&#35821;&#65289;&#32473;&#20986;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#24120;&#24573;&#35270;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#21363;&#39057;&#32321;&#26631;&#35760;&#21644;&#19981;&#32463;&#24120;&#20986;&#29616;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#34987;&#24120;&#35265;&#19988;&#26131;&#23398;&#30340;&#26631;&#35760;&#25152;&#20027;&#23548;&#65292;&#20174;&#32780;&#24573;&#35270;&#19981;&#32463;&#24120;&#20986;&#29616;&#19988;&#38590;&#20197;&#23398;&#20064;&#30340;&#26631;&#35760;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#65288;InfoEntropy Loss&#65289;&#20989;&#25968;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#30456;&#24212;&#30340;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#20449;&#24687;&#29109;&#21160;&#24577;&#35780;&#20272;&#24453;&#23398;&#20064;&#26631;&#35760;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#28982;&#21518;&#65292;&#23427;&#36866;&#24212;&#22320;&#35843;&#25972;&#35757;&#32451;&#25439;&#22833;&#65292;&#35797;&#22270;&#20351;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#23398;&#20064;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DECENT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35299;&#32806;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#25506;&#27979;&#25216;&#26415;&#26469;&#24357;&#34917;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19347</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#36827;&#34892;&#23545;&#25239;&#35299;&#32806;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DECENT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35299;&#32806;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#25506;&#27979;&#25216;&#26415;&#26469;&#24357;&#34917;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20250;&#29983;&#25104;&#19982;&#21407;&#22987;&#25991;&#31456;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#65292;&#34987;&#31216;&#20026;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#12290;&#19982;&#20043;&#21069;&#30340;&#23567;&#22411;&#27169;&#22411;&#65288;&#22914;BART&#65292;T5&#65289;&#19981;&#21516;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#21046;&#36896;&#24858;&#34850;&#38169;&#35823;&#26041;&#38754;&#36739;&#23569;&#65292;&#20294;&#21046;&#36896;&#20102;&#26356;&#22797;&#26434;&#30340;&#38169;&#35823;&#65292;&#20363;&#22914;&#21152;&#20837;&#22240;&#26524;&#20851;&#31995;&#12289;&#28155;&#21152;&#38169;&#35823;&#32454;&#33410;&#21644;&#36807;&#24230;&#27867;&#21270;&#31561;&#12290;&#36825;&#20123;&#24187;&#35273;&#24456;&#38590;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#65292;&#36825;&#32473;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#24102;&#26469;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#35299;&#32806;&#26041;&#27861;&#26469;&#20998;&#31163;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65288;DECENT&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#21442;&#25968;&#39640;&#25928;&#25216;&#26415;&#65292;&#20197;&#24357;&#34917;LLMs&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;LLMs&#23545;&#20110;&#20462;&#39280;&#21644;&#29702;&#35299;&#30340;&#27010;&#24565;&#26356;&#21152;&#28165;&#26224;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as "hallucinations" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, and overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding, thus can execute the instructions more accur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#35835;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23558;&#22270;&#20687;&#19982;&#25918;&#23556;&#23398;&#24322;&#24120;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;GPT-4&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#22238;&#31572;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18341</link><description>&lt;p&gt;
CXR-LLaVA&#65306;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images. (arXiv:2310.18341v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#35835;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23558;&#22270;&#20687;&#19982;&#25918;&#23556;&#23398;&#24322;&#24120;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;GPT-4&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#22238;&#31572;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20197;&#22810;&#27169;&#24577;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#22797;&#21046;&#20154;&#31867;&#25918;&#23556;&#31185;&#21307;&#24072;&#23545;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#21442;&#25968;&#65288;&#22914;&#28201;&#24230;&#21644;&#26680;&#24515;&#37319;&#26679;&#65289;&#30340;&#24433;&#21709;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#25105;&#20204;&#25910;&#38598;&#20102;659,287&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65306;417,336&#20010;&#22270;&#20687;&#24102;&#26377;&#26576;&#20123;&#25918;&#23556;&#23398;&#24322;&#24120;&#26631;&#31614;&#65288;&#25968;&#25454;&#38598;1&#65289;&#65307;241,951&#20010;&#22270;&#20687;&#24102;&#26377;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#25253;&#21578;&#65288;&#25968;&#25454;&#38598;2&#65289;&#12290;&#22312;&#39044;&#35757;&#32451;Resnet50&#20316;&#20026;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#21518;&#65292;&#37319;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26469;&#23545;&#40784;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#25918;&#23556;&#23398;&#24322;&#24120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#25968;&#25454;&#38598;2&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Meta AI-2&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#32463;&#36807;GPT-4&#30340;&#25913;&#36827;&#65292;&#29983;&#25104;&#21508;&#31181;&#38382;&#39064;&#22238;&#31572;&#24773;&#26223;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;ht&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
Purpose: Recent advancements in large language models (LLMs) have expanded their capabilities in a multimodal fashion, potentially replicating the image interpretation of human radiologists. This study aimed to develop open-source multimodal large language model for interpreting chest X-ray images (CXR-LLaVA). We also examined the effect of prompt engineering and model parameters such as temperature and nucleus sampling.  Materials and Methods: For training, we collected 659,287 publicly available CXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset 1); 241,951 CXRs provided free-text radiology reports (dataset 2). After pre-training the Resnet50 as an image encoder, the contrastive language-image pre-training was used to align CXRs and corresponding radiographic abnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using dataset 2, which were refined using GPT-4, with generating various question answering scenarios. The code can be found at ht
&lt;/p&gt;</description></item><item><title>Qilin-Med-VL&#26159;&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21644;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#25552;&#39640;&#20102;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;ChiMed-VL&#12290;</title><link>http://arxiv.org/abs/2310.17956</link><description>&lt;p&gt;
&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;Qilin-Med-VL
&lt;/p&gt;
&lt;p&gt;
Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17956
&lt;/p&gt;
&lt;p&gt;
Qilin-Med-VL&#26159;&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21644;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#25552;&#39640;&#20102;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;ChiMed-VL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#22797;&#26434;&#30340;&#21307;&#30103;&#20445;&#20581;&#21644;&#29983;&#29289;&#21307;&#23398;&#20027;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#33521;&#35821;&#20043;&#22806;&#65292;&#20854;&#20182;&#35821;&#35328;&#30340;&#27169;&#22411;&#20197;&#21450;&#33021;&#22815;&#35299;&#37322;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#27169;&#22411;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#36825;&#23545;&#20110;&#20840;&#29699;&#21307;&#30103;&#20445;&#20581;&#30340;&#21487;&#35775;&#38382;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Qilin-Med-VL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#20998;&#26512;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;Qilin-Med-VL&#23558;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#19982;&#22522;&#30784;LLM&#30456;&#32467;&#21512;&#12290;&#23427;&#32463;&#21382;&#20102;&#19968;&#20010;&#28145;&#20837;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#21644;&#25351;&#23548;&#35843;&#20248;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;ChiMed-VL&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#31867;&#22411;&#30340;&#22270;&#20687;&#36827;&#34892;&#35814;&#32454;&#21644;&#20840;&#38754;&#30340;&#21307;&#23398;&#25968;&#25454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.
&lt;/p&gt;</description></item><item><title>CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.17680</link><description>&lt;p&gt;
CodeFusion: &#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17680
&lt;/p&gt;
&lt;p&gt;
CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#19968;&#20010;&#24320;&#21457;&#32773;&#21482;&#33021;&#20462;&#25913;&#20854;&#26368;&#21518;&#19968;&#34892;&#20195;&#30721;&#65292;&#22312;&#27491;&#30830;&#20043;&#21069;&#65292;&#20182;&#20204;&#38656;&#35201;&#22810;&#23569;&#27425;&#20174;&#22836;&#24320;&#22987;&#32534;&#20889;&#20989;&#25968;&#21602;&#65311;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20063;&#26377;&#31867;&#20284;&#30340;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#23481;&#26131;&#37325;&#26032;&#32771;&#34385;&#20043;&#21069;&#29983;&#25104;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CodeFusion&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#20197;&#32534;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#23436;&#25972;&#31243;&#24207;&#36827;&#34892;&#21435;&#22122;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#38024;&#23545;Bash&#12289;Python&#21644;Microsoft Excel&#26465;&#20214;&#26684;&#24335;(CF)&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#23545;CodeFusion&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CodeFusion&#65288;75M&#21442;&#25968;&#65289;&#22312;top-1&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#65288;350M-175B&#21442;&#25968;&#65289;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;top-3&#21644;top-5&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#20248;&#20110;&#23427;&#20204;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#22312;&#22810;&#26679;&#24615;&#19982;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
&lt;/p&gt;</description></item><item><title>FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.17306</link><description>&lt;p&gt;
FormaT5: &#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26465;&#20214;&#34920;&#26684;&#26684;&#24335;&#21270;&#30340;&#25277;&#26679;&#21644;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17306
&lt;/p&gt;
&lt;p&gt;
FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#30340;&#26684;&#24335;&#21270;&#26159;&#21487;&#35270;&#21270;&#12289;&#23637;&#31034;&#21644;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#32534;&#20889;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#26469;&#33258;&#21160;&#26684;&#24335;&#21270;&#34920;&#26684;&#12290;&#20294;&#23545;&#29992;&#25143;&#26469;&#35828;&#65292;&#32534;&#20889;&#36825;&#26679;&#30340;&#35268;&#21017;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#20182;&#20204;&#29702;&#35299;&#21644;&#23454;&#29616;&#24213;&#23618;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;FormaT5&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#26399;&#26395;&#30340;&#26684;&#24335;&#36923;&#36753;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#19968;&#20010;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#25143;&#20026;&#36825;&#20123;&#20219;&#21153;&#25552;&#20379;&#30340;&#25551;&#36848;&#36890;&#24120;&#26159;&#19981;&#26126;&#30830;&#25110;&#21547;&#31946;&#30340;&#65292;&#36825;&#20351;&#24471;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#38590;&#20197;&#22312;&#19968;&#27493;&#20013;&#20934;&#30830;&#23398;&#20064;&#21040;&#25152;&#38656;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35268;&#33539;&#19981;&#36275;&#30340;&#38382;&#39064;&#24182;&#20943;&#23569;&#21442;&#25968;&#38169;&#35823;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;&#36825;&#20123;&#21344;&#20301;&#31526;&#21487;&#20197;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#25110;&#32773;&#24403;&#21487;&#29992;&#30340;&#34892;&#31034;&#20363;&#26102;&#65292;&#30001;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#32534;&#31243;&#31995;&#32479;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15612</link><description>&lt;p&gt;
Nko&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#65306;&#24037;&#20855;&#12289;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23612;&#31185;&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#27809;&#26377;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#20294;&#23427;&#22312;&#25991;&#21270;&#21644;&#25945;&#32946;&#20215;&#20540;&#19978;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#26088;&#22312;&#24320;&#21457;&#21487;&#29992;&#30340;&#23612;&#31185;&#35821;&#21644;&#20854;&#20182;&#24403;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#24179;&#34892;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#20855;&#20307;&#21253;&#25324;&#65306;(1) Friallel&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#65292;&#36890;&#36807;&#22522;&#20110;&#21103;&#26412;&#32534;&#36753;&#30340;&#24037;&#20316;&#27969;&#31243;&#23454;&#29616;&#36136;&#37327;&#25511;&#21046;&#12290;(2) &#25193;&#23637;&#20102;FLoRes-200&#21644;NLLB-Seed&#35821;&#26009;&#24211;&#65292;&#20174;&#20854;&#20182;&#35821;&#35328;&#20013;&#19982;&#23612;&#31185;&#35821;&#24179;&#34892;&#32763;&#35793;&#20102;2,009&#21644;6,193&#20010;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;(3) nicolingua-0005&#65306;&#21253;&#21547;130,850&#20010;&#24179;&#34892;&#29255;&#27573;&#30340;&#19977;&#35821;&#21644;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#36229;&#36807;3&#30334;&#19975;&#23612;&#31185;&#35821;&#21333;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;(4) &#22522;&#32447;&#21452;&#35821;&#21644;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#19982;b...
&lt;/p&gt;
&lt;p&gt;
Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26131;&#25512;&#32763;&#30340;&#36947;&#24503;&#25512;&#29702;&#20219;&#21153;&#65292;&#20197;&#20102;&#35299;&#19981;&#21516;&#32972;&#26223;&#19979;&#34892;&#20026;&#30340;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#65292;&#24182;&#25552;&#20379;&#24120;&#35782;&#29702;&#30001;&#26469;&#25903;&#25345;&#25512;&#29702;&#12290;&#36890;&#36807;&#36845;&#20195;&#30340;&#33258;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.15431</link><description>&lt;p&gt;
&#20160;&#20040;&#24773;&#20917;&#19979;&#32437;&#28779;&#26159;&#21487;&#20197;&#25509;&#21463;&#30340;&#65311;&#36890;&#36807;&#36845;&#20195;&#33258;&#33976;&#39311;&#24773;&#22659;&#21644;&#21407;&#22240;&#26469;&#28040;&#38500;&#27169;&#31946;&#30340;&#31038;&#20250;&#21644;&#36947;&#24503;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations. (arXiv:2310.15431v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26131;&#25512;&#32763;&#30340;&#36947;&#24503;&#25512;&#29702;&#20219;&#21153;&#65292;&#20197;&#20102;&#35299;&#19981;&#21516;&#32972;&#26223;&#19979;&#34892;&#20026;&#30340;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#65292;&#24182;&#25552;&#20379;&#24120;&#35782;&#29702;&#30001;&#26469;&#25903;&#25345;&#25512;&#29702;&#12290;&#36890;&#36807;&#36845;&#20195;&#30340;&#33258;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#24503;&#25110;&#20262;&#29702;&#21028;&#26029;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20854;&#21457;&#29983;&#30340;&#20855;&#20307;&#32972;&#26223;&#12290;&#29702;&#35299;&#26131;&#25512;&#32763;&#30340;&#24773;&#22659;&#21270;&#21464;&#21270;&#65288;&#21363;&#22686;&#24378;&#25110;&#20943;&#24369;&#19968;&#20010;&#34892;&#20026;&#30340;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#30340;&#39069;&#22806;&#20449;&#24687;&#65289;&#23545;&#20110;&#20934;&#30830;&#21576;&#29616;&#29616;&#23454;&#22330;&#26223;&#20013;&#20957;&#22266;&#30340;&#20154;&#31867;&#36947;&#24503;&#21028;&#26029;&#30340;&#24494;&#22937;&#21644;&#22797;&#26434;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26131;&#25512;&#32763;&#30340;&#36947;&#24503;&#25512;&#29702;&#65306;&#19968;&#39033;&#20219;&#21153;&#65292;&#25552;&#20379;&#20351;&#34892;&#20026;&#22312;&#36947;&#24503;&#19978;&#26356;&#21487;&#25509;&#21463;&#25110;&#19981;&#21487;&#25509;&#21463;&#30340;&#24773;&#22659;&#65292;&#20197;&#21450;&#35777;&#26126;&#25512;&#29702;&#30340;&#24120;&#35782;&#29702;&#30001;&#12290;&#20026;&#20102;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36845;&#20195;&#33258;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20174;GPT-3&#30340;&#19968;&#23567;&#37096;&#20998;&#38750;&#32467;&#26500;&#21270;&#31181;&#23376;&#30693;&#35782;&#24320;&#22987;&#65292;&#28982;&#21518;&#22312;&#23398;&#29983;&#27169;&#22411;&#21644;&#25209;&#21028;&#32773;&#27169;&#22411;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#65288;1&#65289;&#33258;&#33976;&#39311;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20154;&#31867;&#21028;&#26029;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#36807;&#28388;&#65288;&#20197;&#25552;&#39640;&#26377;&#25928;&#24615;&#65289;&#21644;NLI&#65288;&#20197;&#25552;&#39640;&#22810;&#26679;&#24615;&#65289;&#65307;&#65288;3&#65289;&#33258;&#27169;&#20223;&#23398;&#20064;&#65288;&#20197;&#25918;&#22823;&#25152;&#38656;&#25968;&#25454;&#36136;&#37327;&#65289;&#12290;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#20102;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and intricacy of grounded human moral judgment in real-life scenarios.  We introduce defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning. To elicit high-quality task data, we take an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between (1) self-distillation from student models; (2) targeted filtering with a critic model trained by human judgment (to boost validity) and NLI (to boost diversity); (3) self-imitation learning (to amplify the desired data quality). This process yields a stude
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#20307;&#39564;&#24335;&#36716;&#25442;&#30340;&#22810;&#38454;&#27573;&#22810;&#27969;&#22810;&#27169;&#24577;Transformer&#65292;&#36890;&#36807;&#23545;&#21516;&#27493;&#30340;&#22810;&#35282;&#24230;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#27169;&#22411;&#21644;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;14.01%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.14859</link><description>&lt;p&gt;
3M-TRANSFORMER&#65306;&#19968;&#31181;&#29992;&#20110;&#20307;&#39564;&#24335;&#36716;&#25442;&#39044;&#27979;&#30340;&#22810;&#38454;&#27573;&#22810;&#27969;&#22810;&#27169;&#24577;Transformer
&lt;/p&gt;
&lt;p&gt;
3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction. (arXiv:2310.14859v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#20307;&#39564;&#24335;&#36716;&#25442;&#30340;&#22810;&#38454;&#27573;&#22810;&#27969;&#22810;&#27169;&#24577;Transformer&#65292;&#36890;&#36807;&#23545;&#21516;&#27493;&#30340;&#22810;&#35282;&#24230;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#27169;&#22411;&#21644;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;14.01%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#39044;&#27979;&#20132;&#26367;&#23545;&#35805;&#22312;&#20154;&#26426;/&#26426;&#22120;&#20154;&#20132;&#20114;&#20013;&#20855;&#26377;&#24456;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#27807;&#36890;&#30340;&#22797;&#26434;&#24615;&#20351;&#36825;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#34920;&#26126;&#65292;&#21516;&#27493;&#30340;&#22810;&#35282;&#24230;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19982;&#24322;&#27493;&#30340;&#21333;&#35282;&#24230;&#36716;&#24405;&#30456;&#27604;&#30340;&#20132;&#26367;&#23545;&#35805;&#39044;&#27979;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;Transformer&#30340;&#26032;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#20307;&#39564;&#24335;&#30340;&#12289;&#21516;&#27493;&#30340;&#22810;&#35282;&#24230;&#25968;&#25454;&#20013;&#30340;&#20132;&#26367;&#23545;&#35805;&#12290;&#25105;&#20204;&#22312;&#26368;&#36817;&#24341;&#20837;&#30340;EgoCom&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#22522;&#20934;&#32447;&#21644;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;3M-Transformer&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;&#20102;14.01%&#12290;&#25105;&#20204;&#30340;3M-Transformer&#30340;&#28304;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#22312;&#34987;&#25509;&#21463;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting turn-taking in multiparty conversations has many practical applications in human-computer/robot interaction. However, the complexity of human communication makes it a challenging task. Recent advances have shown that synchronous multi-perspective egocentric data can significantly improve turn-taking prediction compared to asynchronous, single-perspective transcriptions. Building on this research, we propose a new multimodal transformer-based architecture for predicting turn-taking in embodied, synchronized multi-perspective data. Our experimental results on the recently introduced EgoCom dataset show a substantial performance improvement of up to 14.01% on average compared to existing baselines and alternative transformer-based approaches. The source code, and the pre-trained models of our 3M-Transformer will be available upon acceptance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#20013;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#24179;&#34913;&#21305;&#37197;&#20559;&#24046;&#21644;&#20998;&#24515;&#30456;&#20284;&#24615;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#25239;&#25968;&#25454;&#21512;&#25104;&#21644;&#26679;&#26412;&#20869;&#23545;&#31435;&#35757;&#32451;&#30340;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.14670</link><description>&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#25968;&#25454;&#38598;&#20559;&#24046;&#32531;&#35299;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond. (arXiv:2310.14670v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#20013;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#24179;&#34913;&#21305;&#37197;&#20559;&#24046;&#21644;&#20998;&#24515;&#30456;&#20284;&#24615;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#25239;&#25968;&#25454;&#21512;&#25104;&#21644;&#26679;&#26412;&#20869;&#23545;&#31435;&#35757;&#32451;&#30340;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#29702;&#35299;&#20219;&#21153;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#35780;&#20272;&#27169;&#22411;&#23545;&#22797;&#26434;&#35270;&#35273;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#20004;&#31181;&#25968;&#25454;&#38598;&#20559;&#24046;&#20316;&#20026;&#26080;&#38656;&#27491;&#30830;&#29702;&#35299;&#21363;&#21487;&#27491;&#30830;&#35299;&#20915;&#21508;&#31181;VL&#20219;&#21153;&#30340;&#25463;&#24452;&#12290;&#31532;&#19968;&#31181;&#25968;&#25454;&#38598;&#20559;&#24046;&#26159;"&#19981;&#24179;&#34913;&#21305;&#37197;"&#20559;&#24046;&#65292;&#21363;&#27491;&#30830;&#31572;&#26696;&#19982;&#38382;&#39064;&#21644;&#22270;&#20687;&#30340;&#37325;&#21472;&#31243;&#24230;&#36229;&#36807;&#38169;&#35823;&#31572;&#26696;&#12290;&#31532;&#20108;&#31181;&#25968;&#25454;&#38598;&#20559;&#24046;&#26159;"&#20998;&#24515;&#30456;&#20284;&#24615;"&#20559;&#24046;&#65292;&#21363;&#38169;&#35823;&#31572;&#26696;&#19982;&#27491;&#30830;&#31572;&#26696;&#36807;&#20110;&#19981;&#30456;&#20284;&#65292;&#20294;&#19982;&#21516;&#19968;&#20010;&#26679;&#26412;&#20013;&#30340;&#20854;&#20182;&#38169;&#35823;&#31572;&#26696;&#30456;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25968;&#25454;&#38598;&#20559;&#24046;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#23545;&#25239;&#25968;&#25454;&#21512;&#25104;&#65288;ADS&#65289;&#26469;&#29983;&#25104;&#21512;&#25104;&#30340;&#35757;&#32451;&#21644;&#21435;&#20559;&#24046;&#30340;&#35780;&#20272;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26679;&#26412;&#20869; &#23545;&#31435;&#35757;&#32451;&#65288;ICT&#65289;&#26469;&#24110;&#21161;&#27169;&#22411;&#21033;&#29992;&#21512;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#31435;&#20107;&#23454;&#25968;&#25454;&#65292;&#36890;&#36807;&#27880;&#37325;&#26679;&#26412;&#20869;&#30340;&#24046;&#24322;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language (VL) understanding tasks evaluate models' comprehension of complex visual scenes through multiple-choice questions. However, we have identified two dataset biases that models can exploit as shortcuts to resolve various VL tasks correctly without proper understanding. The first type of dataset bias is \emph{Unbalanced Matching} bias, where the correct answer overlaps the question and image more than the incorrect answers. The second type of dataset bias is \emph{Distractor Similarity} bias, where incorrect answers are overly dissimilar to the correct answer but significantly similar to other incorrect answers within the same sample. To address these dataset biases, we first propose Adversarial Data Synthesis (ADS) to generate synthetic training and debiased evaluation data. We then introduce Intra-sample Counterfactual Training (ICT) to assist models in utilizing the synthesized training data, particularly the counterfactual data, via focusing on intra-sample differentia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#23545;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#29616;&#29366;&#35843;&#26597;&#21457;&#29616;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11986</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#25216;&#26415;&#23433;&#20840;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#23545;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#29616;&#29366;&#35843;&#26597;&#21457;&#29616;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;AI&#31995;&#32479;&#20250;&#20135;&#29983;&#19968;&#31995;&#21015;&#39118;&#38505;&#12290;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#32467;&#26500;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20123;&#39118;&#38505;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#33021;&#21147;&#35780;&#20272;&#65292;&#36825;&#26159;&#30446;&#21069;&#20027;&#35201;&#30340;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#22312;&#31995;&#32479;&#23433;&#20840;&#21407;&#21017;&#30340;&#22522;&#30784;&#19978;&#65292;&#29305;&#21035;&#26159;&#35748;&#35782;&#21040;&#19978;&#19979;&#25991;&#20915;&#23450;&#20102;&#29305;&#23450;&#33021;&#21147;&#26159;&#21542;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#20026;&#20102;&#32771;&#34385;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22686;&#21152;&#20102;&#20154;&#26426;&#20114;&#21160;&#21644;&#31995;&#32479;&#24433;&#21709;&#20316;&#20026;&#39069;&#22806;&#30340;&#35780;&#20272;&#23618;&#38754;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#23433;&#20840;&#35780;&#20272;&#30340;&#29616;&#29366;&#65292;&#24182;&#21019;&#24314;&#20102;&#29616;&#26377;&#35780;&#20272;&#30340;&#24211;&#12290;&#20174;&#20998;&#26512;&#20013;&#24471;&#20986;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#21069;&#36827;&#26041;&#24335;&#65292;&#27010;&#36848;&#20102;&#23454;&#38469;&#27493;&#39588;&#21644;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.09219</link><description>&lt;p&gt;
"&#20975;&#21033;&#26159;&#19968;&#20010;&#28201;&#26262;&#30340;&#20154;&#65292;&#32422;&#29791;&#22827;&#26159;&#19968;&#20010;&#27036;&#26679;": LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#29992;&#25143;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21327;&#21161;&#25776;&#20889;&#21508;&#31181;&#31867;&#22411;&#30340;&#20869;&#23481;&#65292;&#21253;&#25324;&#25512;&#33616;&#20449;&#31561;&#32844;&#19994;&#25991;&#20214;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#26041;&#20415;&#24615;&#65292;&#20294;&#36825;&#20123;&#24212;&#29992;&#24341;&#20837;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#30001;&#20110;&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#21487;&#33021;&#34987;&#29992;&#25143;&#30452;&#25509;&#22312;&#32844;&#19994;&#25110;&#23398;&#26415;&#22330;&#26223;&#20013;&#20351;&#29992;&#65292;&#23427;&#20204;&#26377;&#21487;&#33021;&#36896;&#25104;&#30452;&#25509;&#30340;&#31038;&#20250;&#20260;&#23475;&#65292;&#22914;&#38477;&#20302;&#22899;&#24615;&#30003;&#35831;&#32773;&#30340;&#25104;&#21151;&#29575;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#26469;&#30340;&#32531;&#35299;&#21644;&#30417;&#25511;&#65292;&#20840;&#38754;&#30740;&#31350;&#27492;&#31867;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#21644;&#30456;&#20851;&#20260;&#23475;&#21183;&#22312;&#24517;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#30740;&#31350;&#12290;&#21463;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#32500;&#24230;&#26469;&#23637;&#29616;LLM&#29983;&#25104;&#30340;&#20449;&#20214;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#35821;&#35328;&#39118;&#26684;&#30340;&#20559;&#35265;&#21644;&#35789;&#27719;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25512;&#33616;&#20449;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.05506</link><description>&lt;p&gt;
&#26597;&#35810;&#21644;&#24212;&#31572;&#22686;&#24378;&#19981;&#33021;&#24110;&#21161;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization. (arXiv:2310.05506v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#26102;&#65292;&#36890;&#36807;&#26597;&#35810;&#28436;&#21270;&#21644;&#22810;&#26679;&#21270;&#25512;&#29702;&#36335;&#24452;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#32463;&#39564;&#19978;&#34987;&#39564;&#35777;&#20026;&#26377;&#25928;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#24320;&#28304;LLMs&#21644;&#39030;&#23574;&#19987;&#26377;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#26088;&#22312;&#22238;&#31572;&#65306;&#65288;1&#65289;&#21738;&#20123;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26356;&#26377;&#25928;&#65307;&#65288;2&#65289;&#22686;&#24378;&#25968;&#25454;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32553;&#25918;&#20851;&#31995;&#22914;&#20309;&#65307;&#65288;3&#65289;&#25968;&#25454;&#22686;&#24378;&#33021;&#21542;&#28608;&#21169;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#30340;&#27867;&#21270;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;GSM8K&#26597;&#35810;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#20197;&#21450;&#37319;&#26679;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;AugGSM8K&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;AugGSM8K&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#31216;&#20026;MuggleMath&#12290;MuggleMath&#22312;GSM8K&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65288;&#22312;7B&#35268;&#27169;&#19978;&#20174;54%&#25552;&#39640;&#21040;68.4%&#65292;&#22312;&#25193;&#25918;&#21040;63.9%&#21040;74.0%&#20043;&#38388;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05492</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#26631;&#35760;&#21644;&#21442;&#25968;&#65292;&#23637;&#29616;&#20986;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#38543;&#31561;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#24320;&#28304;&#31038;&#21306;&#24050;&#32463;&#30740;&#31350;&#20102;&#38024;&#23545;&#27599;&#31181;&#33021;&#21147;&#30340;&#20020;&#26102;SFT&#65292;&#32780;&#19987;&#26377;LLMs&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;SFT&#35299;&#38145;&#22810;&#37325;&#33021;&#21147;&#21464;&#24471;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;SFT&#36807;&#31243;&#20013;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20154;&#31867;&#23545;&#40784;&#33021;&#21147;&#20043;&#38388;&#30340;&#25968;&#25454;&#32452;&#21512;&#12290;&#20174;&#35268;&#27169;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#33021;&#21147;&#19982;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#25968;&#25454;&#37327;&#12289;&#25968;&#25454;&#32452;&#21512;&#27604;&#20363;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;SFT&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#33021;&#21147;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25193;&#23637;&#27169;&#24335;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#35009;&#20915;&#30340;&#33258;&#21160;&#21270;&#21311;&#21517;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#65292;&#20351;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;F1&#20998;&#25968;&#36229;&#36807;5%&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#23558;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#21171;&#21160;&#24182;&#22686;&#24378;&#33258;&#21160;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04632</link><description>&lt;p&gt;
&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#35009;&#20915;&#30340;&#33258;&#21160;&#21270;&#21311;&#21517;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Anonymization of Swiss Federal Supreme Court Rulings. (arXiv:2310.04632v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#35009;&#20915;&#30340;&#33258;&#21160;&#21270;&#21311;&#21517;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#65292;&#20351;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;F1&#20998;&#25968;&#36229;&#36807;5%&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#23558;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#21171;&#21160;&#24182;&#22686;&#24378;&#33258;&#21160;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27861;&#38498;&#30340;&#35009;&#20915;&#20844;&#24320;&#38656;&#35201;&#36827;&#34892;&#36866;&#24403;&#30340;&#21311;&#21517;&#21270;&#65292;&#20197;&#20445;&#25252;&#25152;&#26377;&#30456;&#20851;&#26041;&#30340;&#38544;&#31169;&#12290;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#20381;&#38752;&#19968;&#31181;&#24050;&#26377;&#30340;&#31995;&#32479;&#65292;&#23558;&#19981;&#21516;&#30340;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#19982;&#20154;&#24037;&#19987;&#23478;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#24102;&#26377;&#35201;&#21311;&#21517;&#21270;&#23454;&#20307;&#27880;&#37322;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#36719;&#20214;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#21644;&#22312;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;F1&#20998;&#25968;&#36229;&#36807;5&#65285;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#23558;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#65288;&#22914;&#27491;&#21017;&#34920;&#36798;&#24335;&#65289;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#20154;&#24037;&#21171;&#21160;&#24182;&#22686;&#24378;&#33258;&#21160;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Releasing court decisions to the public relies on proper anonymization to protect all involved parties, where necessary. The Swiss Federal Supreme Court relies on an existing system that combines different traditional computational methods with human experts. In this work, we enhance the existing anonymization software using a large dataset annotated with entities to be anonymized. We compared BERT-based models with models pre-trained on in-domain data. Our results show that using in-domain data to pre-train the models further improves the F1-score by more than 5\% compared to existing models. Our work demonstrates that combining existing anonymization methods, such as regular expressions, with machine learning can further reduce manual labor and enhance automatic suggestions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35745;&#31639;&#26041;&#38754;&#30340;&#38480;&#21046;&#24615;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#23545;&#35805;&#65292;&#24182;&#22312;&#27599;&#20010;&#23398;&#29983;&#22238;&#31572;&#35302;&#21457;&#33258;&#35328;&#33258;&#35821;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12161</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20934;&#30830;&#35745;&#31639;&#30340;&#20195;&#30721;&#29420;&#30333;
&lt;/p&gt;
&lt;p&gt;
Code Soliloquies for Accurate Calculations in Large Language Models. (arXiv:2309.12161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12161
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35745;&#31639;&#26041;&#38754;&#30340;&#38480;&#21046;&#24615;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#23545;&#35805;&#65292;&#24182;&#22312;&#27599;&#20010;&#23398;&#29983;&#22238;&#31572;&#35302;&#21457;&#33258;&#35328;&#33258;&#35821;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#20110;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21518;&#31471;&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#25104;&#21151;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#36825;&#20123;&#25968;&#25454;&#38598;&#29992;&#20110;&#23545;LLM&#21518;&#31471;&#36827;&#34892;&#32454;&#35843;&#26102;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#23398;&#29983;&#21644;ITS&#20043;&#38388;&#30340;&#20114;&#21160;&#36136;&#37327;&#12290;&#24320;&#21457;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#24120;&#35265;&#31574;&#30053;&#28041;&#21450;&#20351;&#29992;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30340;&#23398;&#29983;-&#25945;&#24072;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#23545;&#35805;&#38656;&#35201;&#36827;&#34892;&#29289;&#29702;&#31561;&#31185;&#30446;&#20013;&#24120;&#35265;&#30340;&#22797;&#26434;&#35745;&#31639;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#25361;&#25112;&#12290;&#23613;&#31649;&#20854;&#20808;&#36827;&#30340;&#21151;&#33021;&#65292;GPT-4&#22312;&#21487;&#38752;&#22788;&#29702;&#29978;&#33267;&#31616;&#21333;&#30340;&#20056;&#27861;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#36824;&#19981;&#22815;&#65292;&#36825;&#26159;&#20854;&#22312;&#36825;&#20123;&#31185;&#30446;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26377;&#29366;&#24577;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#19968;&#20010;&#30001;GPT-4&#27169;&#25311;&#30340;&#23398;&#29983;&#21644;&#23548;&#24072;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#27169;&#25311;&#23545;&#35805;&#12290;&#27599;&#20010;&#23398;&#29983;&#30340;&#22238;&#31572;&#37117;&#20250;&#35302;&#21457;&#19968;&#20010;&#33258;&#35328;&#33258;&#35821;&#65288;&#20869;&#24515;&#29420;&#30333;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality conversational datasets are integral to the successful development of Intelligent Tutoring Systems (ITS) that employ a Large Language Model (LLM) backend. These datasets, when used to fine-tune the LLM backend, significantly enhance the quality of interactions between students and ITS. A common strategy for developing these datasets involves generating synthetic student-teacher dialogues using advanced GPT-4 models. However, challenges arise when these dialogues demand complex calculations, common in subjects like physics. Despite its advanced capabilities, GPT-4's performance falls short in reliably handling even simple multiplication tasks, marking a significant limitation in its utility for these subjects. To address these challenges, this paper introduces an innovative stateful prompt design. Our approach generates a mock conversation between a student and a tutorbot, both roles simulated by GPT-4. Each student response triggers a soliloquy (an inner monologue) in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03564</link><description>&lt;p&gt;
&#35780;&#20272;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#31867;&#20284;&#24555;&#36895;&#21457;&#23637;&#30340;GPT&#31995;&#21015;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24433;&#21709;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#24515;&#29702;&#23398;&#31561;&#21307;&#23398;&#39046;&#22495;&#23545;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#27987;&#21402;&#20852;&#36259;&#65292;&#20294;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#20855;&#20307;&#25506;&#32034;&#20173;&#28982;&#24456;&#23569;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#36234;&#26469;&#36234;&#22810;&#22320;&#34920;&#36798;&#20010;&#20154;&#24773;&#24863;&#65307;&#22312;&#29305;&#23450;&#30340;&#20027;&#39064;&#19979;&#65292;&#36825;&#20123;&#24773;&#24863;&#36890;&#24120;&#34920;&#29616;&#20026;&#28040;&#26497;&#24773;&#32490;&#65292;&#26377;&#26102;&#20250;&#21319;&#32423;&#20026;&#33258;&#26432;&#20542;&#21521;&#12290;&#21450;&#26102;&#36776;&#35782;&#36825;&#26679;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#23545;&#26377;&#25928;&#24178;&#39044;&#21644;&#28508;&#22312;&#36991;&#20813;&#20005;&#37325;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#36827;&#34892;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#33258;&#26432;&#39118;&#38505;&#21644;&#35748;&#30693;&#20559;&#24046;&#35782;&#21035;&#30340;&#23454;&#39564;&#65292;&#36827;&#20837;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65306;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#65292;&#32771;&#23519;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tunin
&lt;/p&gt;</description></item><item><title>YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00071</link><description>&lt;p&gt;
YaRN: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00071
&lt;/p&gt;
&lt;p&gt;
YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;transformer-based&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36229;&#36807;&#23427;&#20204;&#35757;&#32451;&#30340;&#24207;&#21015;&#38271;&#24230;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;YaRN&#65288;Yet another RoPE extensioN method&#65289;&#65292;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#30340;tokens&#25968;&#37327;&#21644;&#35757;&#32451;&#27493;&#39588;&#23569;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#30340;10&#20493;&#21644;2.5&#20493;&#12290;&#20351;&#29992;YaRN&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaMA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#38754;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;YaRN&#20855;&#26377;&#36229;&#36234;&#24494;&#35843;&#25968;&#25454;&#38598;&#26377;&#38480;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/jquesnelle/yarn&#19978;&#21457;&#24067;&#20102;&#20351;&#29992;64k&#21644;128k&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;Fine-tuning&#30340;Llama 2 7B/13B&#30340;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
&lt;/p&gt;</description></item><item><title>CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16705</link><description>&lt;p&gt;
CReHate: &#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16705
&lt;/p&gt;
&lt;p&gt;
CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#25968;&#25454;&#38598;&#20027;&#35201;&#21453;&#26144;&#20102;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#25991;&#21270;&#20559;&#24046;&#12290;&#36825;&#22312;&#21463;&#20027;&#35266;&#24615;&#24433;&#21709;&#36739;&#22823;&#30340;&#20219;&#21153;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#29305;&#21035;&#26377;&#38382;&#39064;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#22914;&#20309;&#29702;&#35299;&#20167;&#24680;&#35328;&#35770;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CReHate&#65292;&#23545;&#25277;&#26679;&#30340;SBIC&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#27880;&#37322;&#65306;&#28595;&#22823;&#21033;&#20122;&#12289;&#26032;&#21152;&#22369;&#12289;&#21335;&#38750;&#12289;&#33521;&#22269;&#21644;&#32654;&#22269;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#22269;&#31821;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#21482;&#26377;59.4%&#30340;&#26679;&#26412;&#22312;&#25152;&#26377;&#22269;&#23478;&#20043;&#38388;&#36798;&#25104;&#20849;&#35782;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#22269;&#31821;&#30340;&#35266;&#28857;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20167;&#24680;&#35328;&#35770;&#30340;&#32454;&#24494;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
English datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. This is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. To delve into how individuals from different countries perceive hate speech, we introduce CReHate, a cross-cultural re-annotation of the sampled SBIC dataset. This dataset includes annotations from five distinct countries: Australia, Singapore, South Africa, the United Kingdom, and the United States. Our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. We also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. These findings underscore the need to re-evaluate certain aspects of NLP research, especially with regard to the nuanced nature of hate spe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.04723</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#20869;&#37096;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25552;&#39640;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#23646;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#21363;&#32473;&#23450;&#25991;&#26412;&#26679;&#26412;&#23884;&#20837;&#38598;&#21512;&#19979;&#30340;&#27969;&#24418;&#30340;&#20869;&#37096;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#27969;&#30021;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#22312;&#20960;&#20010;&#22522;&#20110;&#23383;&#27597;&#30340;&#35821;&#35328;&#20013;&#32422;&#20026; $9$&#65292;&#32780;&#20013;&#25991;&#32422;&#20026; $7$&#65292;&#32780;&#27599;&#31181;&#35821;&#35328;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#36739;&#20302;&#65292;&#24046;&#32422; $1.5$&#65292;&#24182;&#19988;&#26377;&#26126;&#26174;&#30340;&#32479;&#35745;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
&lt;/p&gt;</description></item><item><title>&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#20854;&#27880;&#24847;&#21147;&#24448;&#24448;&#20250;&#20998;&#25955;&#21040;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#65292;&#36825;&#20250;&#23548;&#33268;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;&#21482;&#21547;&#19968;&#20010;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#22312;&#35299;&#31572;&#20013;&#36873;&#25321;&#27491;&#30830;&#29575;&#24456;&#39640;
&lt;/p&gt;
&lt;p&gt;
Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy. (arXiv:2305.14596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14596
&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#20854;&#27880;&#24847;&#21147;&#24448;&#24448;&#20250;&#20998;&#25955;&#21040;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#65292;&#36825;&#20250;&#23548;&#33268;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;&#21482;&#21547;&#19968;&#20010;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#38646;&#25110;&#23569;&#26679;&#26412;&#30340;&#37492;&#21035;&#24615;&#20219;&#21153;&#65292;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#23427;&#20204;&#30340;&#27880;&#24847;&#21147;&#65288;&#21363;&#27010;&#29575;&#36136;&#37327;&#65289;&#20250;&#20998;&#25955;&#22312;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#12290;&#36825;&#31181;&#22312;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#30340;&#22810;&#20010;&#34920;&#38754;&#24418;&#24335;&#20043;&#38388;&#20998;&#25955;&#23548;&#33268;&#20102;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#65292;&#31216;&#20026;&#8220;&#34920;&#38754;&#24418;&#24335;&#31454;&#20105;&#8221;&#65288;SFC&#65289;&#20551;&#35828;&#12290;&#36825;&#20419;&#20351;&#24341;&#20837;&#21508;&#31181;&#27010;&#29575;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#28982;&#32780;&#20173;&#23384;&#22312;&#35768;&#22810;&#26680;&#24515;&#38382;&#39064;&#26410;&#35299;&#31572;&#12290;&#25105;&#20204;&#22914;&#20309;&#27979;&#37327;SFC&#25110;&#27880;&#24847;&#21147;&#65311;&#26159;&#21542;&#26377;&#30452;&#25509;&#30340;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#65311;&#22686;&#21152;&#27880;&#24847;&#21147;&#24635;&#26159;&#33021;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#27880;&#24847;&#21147;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22686;&#21152;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20165;&#21253;&#21547;&#31572;&#26696;&#36873;&#39033;&#30340;&#19968;&#20010;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
When large language models (LMs) are applied in zero- or few-shot settings to discriminative tasks such as multiple-choice questions, their attentiveness (i.e., probability mass) is spread across many vocabulary tokens that are not valid choices. Such a spread across multiple surface forms with identical meaning is thought to cause an underestimation of a model's true performance, referred to as the "surface form competition" (SFC) hypothesis. This has motivated the introduction of various probability normalization methods. However, many core questions remain unanswered. How do we measure SFC or attentiveness? Are there direct ways of increasing attentiveness on valid choices? Does increasing attentiveness always improve task accuracy? We propose a mathematical formalism for studying this phenomenon, provide a metric for quantifying attentiveness, and identify a simple method for increasing it -namely, in-context learning with even just one example containing answer choices. The form
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#65292;&#22312;&#22810;&#39046;&#22495;&#30340;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#65292;&#22312;&#22810;&#39046;&#22495;&#30340;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#25277;&#35937;&#21333;&#25991;&#26723;&#25688;&#35201;&#65288;SDS&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#22909;&#22788;&#21487;&#33021;&#19981;&#20250;&#36731;&#26131;&#25193;&#23637;&#21040;&#22810;&#25991;&#26723;&#25688;&#35201;&#65288;MDS&#65289;&#65292;&#22240;&#20026;&#25991;&#26723;&#20043;&#38388;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#35774;&#35745;&#26032;&#30340;&#26550;&#26500;&#25110;&#26032;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;MDS&#65292;&#35201;&#20040;&#23558;PLM&#24212;&#29992;&#20110;MDS&#65292;&#20294;&#26410;&#32771;&#34385;&#21040;&#22797;&#26434;&#30340;&#25991;&#26723;&#20132;&#20114;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#19978;&#24378;&#21046;&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#23547;&#27714;&#26356;&#22909;&#22320;&#21033;&#29992;PLM&#20419;&#36827;MDS&#20219;&#21153;&#30340;&#22810;&#25991;&#26723;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#35774;&#35745;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#35206;&#30422;&#21508;&#31181;&#39046;&#22495;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#37117;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#25913;&#36827;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;MDS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have accomplished impressive achievements in abstractive single-document summarization (SDS). However, such benefits may not be readily extended to muti-document summarization (MDS), where the interactions among documents are more complex. Previous works either design new architectures or new pre-training objectives for MDS, or apply PLMs to MDS without considering the complex document interactions. While the former does not make full use of previous pre-training efforts and may not generalize well across multiple domains, the latter cannot fully attend to the intricate relationships unique to MDS tasks. In this paper, we enforce hierarchy on both the encoder and decoder and seek to make better use of a PLM to facilitate multi-document interactions for the MDS task. We test our design on 10 MDS datasets across a wide range of domains. Extensive experiments show that our proposed method can achieve consistent improvements on all these datasets, outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26469;&#33258;OpenAI&#27169;&#22411;API&#20197;&#21450;New Bing&#22686;&#24378;&#29256;&#30340;ChatGPT&#23545;&#38544;&#31169;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#25351;&#20986;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#30340;LLMs&#21487;&#33021;&#23548;&#33268;&#27604;&#20197;&#24448;&#26356;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#23454;&#39564;&#25903;&#25345;&#35813;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.05197</link><description>&lt;p&gt;
Multi-step Jailbreaking Privacy Attacks on ChatGPT
&lt;/p&gt;
&lt;p&gt;
Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26469;&#33258;OpenAI&#27169;&#22411;API&#20197;&#21450;New Bing&#22686;&#24378;&#29256;&#30340;ChatGPT&#23545;&#38544;&#31169;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#25351;&#20986;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#30340;LLMs&#21487;&#33021;&#23548;&#33268;&#27604;&#20197;&#24448;&#26356;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#23454;&#39564;&#25903;&#25345;&#35813;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#33391;&#22909;&#30340;&#25552;&#31034;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#23613;&#31649;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#30830;&#20445;&#36991;&#20813;&#20174;LLMs&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#24341;&#23548;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#20026;&#20154;&#31867;&#24102;&#26469;&#22909;&#22788;&#12290;&#30001;&#20110;&#24378;&#22823;&#30340;LLMs&#27491;&#22312;&#21534;&#22124;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;&#29616;&#26377;&#25991;&#26412;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;GPT-3&#35757;&#32451;&#20102;45TB&#30340;&#25991;&#26412;&#65289;&#65292;&#22240;&#27492;&#20154;&#20204;&#33258;&#28982;&#20250;&#24576;&#30097;&#35757;&#32451;&#25968;&#25454;&#20013;&#26159;&#21542;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#20197;&#21450;&#36825;&#20123;LLMs&#21450;&#20854;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#24102;&#26469;&#20160;&#20040;&#38544;&#31169;&#23041;&#32961;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26469;&#33258;OpenAI&#30340;&#27169;&#22411;API&#21644;&#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;New Bing&#25152;&#24102;&#26469;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#26174;&#31034;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#30340;LLMs&#21487;&#33021;&#23548;&#33268;&#27604;&#20197;&#24448;&#26356;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#35828;&#27861;&#65292;&#24182;&#35752;&#35770;LLMs&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given good prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's model APIs and New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause more severe privacy threats ever than before. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.
&lt;/p&gt;</description></item><item><title>KPEval&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#30701;&#26495;&#12290;&#36890;&#36807;&#24341;&#20837;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#21253;&#25324;&#26174;&#33879;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#35774;&#35745;&#30456;&#24212;&#30340;&#35821;&#20041;&#24230;&#37327;&#25351;&#26631;&#65292;KPEval&#22312;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#37325;&#26032;&#35780;&#20272;&#20102;20&#20010;&#20851;&#38190;&#35789;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#24773;&#20917;&#21462;&#20915;&#20110;&#35780;&#20272;&#32500;&#24230;&#65292;&#23454;&#29992;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2303.15422</link><description>&lt;p&gt;
KPEval&#65306;&#38754;&#21521;&#32454;&#31890;&#24230;&#35821;&#20041;&#35780;&#20272;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase Extraction and Generation Systems. (arXiv:2303.15422v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15422
&lt;/p&gt;
&lt;p&gt;
KPEval&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#30701;&#26495;&#12290;&#36890;&#36807;&#24341;&#20837;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#21253;&#25324;&#26174;&#33879;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#35774;&#35745;&#30456;&#24212;&#30340;&#35821;&#20041;&#24230;&#37327;&#25351;&#26631;&#65292;KPEval&#22312;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#37325;&#26032;&#35780;&#20272;&#20102;20&#20010;&#20851;&#38190;&#35789;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#24773;&#20917;&#21462;&#20915;&#20110;&#35780;&#20272;&#32500;&#24230;&#65292;&#23454;&#29992;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#34892;&#35780;&#20272;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#19982;&#20154;&#24037;&#21442;&#32771;&#30340;&#23436;&#20840;&#21305;&#37197;&#65292;&#32780;&#24573;&#30053;&#20102;&#26080;&#21442;&#32771;&#23646;&#24615;&#12290;&#36825;&#31181;&#26041;&#24335;&#26080;&#27861;&#35782;&#21035;&#29983;&#25104;&#19982;&#21442;&#32771;&#35821;&#20041;&#31561;&#25928;&#25110;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#30340;&#22810;&#26679;&#21270;&#20851;&#38190;&#35789;&#30340;&#31995;&#32479;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#20851;&#38190;&#35789;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;KPEval&#65292;&#21253;&#21547;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#26174;&#33879;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#20110;&#27599;&#20010;&#32500;&#24230;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19982;&#35780;&#20272;&#30446;&#26631;&#30456;&#19968;&#33268;&#30340;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#20803;&#35780;&#20272;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#20351;&#29992;&#30340;&#19968;&#31995;&#21015;&#24230;&#37327;&#25351;&#26631;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#31574;&#30053;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#20851;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;20&#20010;&#20851;&#38190;&#35789;&#31995;&#32479;&#65292;&#24182;&#36827;&#19968;&#27493;&#21457;&#29616;&#65306;(1)&#26368;&#22909;&#30340;&#27169;&#22411;&#26681;&#25454;&#35780;&#20272;&#32500;&#24230;&#19981;&#21516;&#32780;&#19981;&#21516;&#65307;(2)&#23454;&#29992;&#24615;&#26159;&#20851;&#38190;&#35789;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant advancements in keyphrase extraction and keyphrase generation methods, the predominant approach for evaluation only relies on exact matching with human references and disregards reference-free attributes. This scheme fails to recognize systems that generate keyphrases semantically equivalent to the references or diverse keyphrases that carry practical utility. To better assess the capability of keyphrase systems, we propose KPEval, a comprehensive evaluation framework consisting of four critical dimensions: saliency, faithfulness, diversity, and utility. For each dimension, we design semantic-based metrics that align with the evaluation objectives. Meta-evaluation studies demonstrate that our evaluation strategy correlates better with human preferences compared to a range of previously used metrics. Using this framework, we re-evaluate 20 keyphrase systems and further discover that (1) the best model differs depending on the evaluation dimension; (2) the utility
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2302.03693</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#30340;&#27010;&#24565;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#26576;&#31181;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#65288;&#25110;&#26041;&#21521;&#65289;&#30340;&#24605;&#24819;&#65292;&#24182;&#24320;&#21457;&#20102;&#36825;&#20010;&#24605;&#24819;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#12290;&#21033;&#29992;&#36825;&#20010;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#19968;&#20010;&#33258;&#28982;&#30340;&#34920;&#31034;&#36873;&#25321;&#20855;&#26377;&#36825;&#31181;&#24615;&#36136;&#65292;&#24182;&#19988;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23545;&#34920;&#31034;&#30340;&#20195;&#25968;&#25805;&#20316;&#26469;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#30340;&#31034;&#20363;&#20013;&#28436;&#31034;&#20102;&#36825;&#20010;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
&lt;/p&gt;</description></item><item><title>LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.13126</link><description>&lt;p&gt;
LEXTREME&#65306;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13126
&lt;/p&gt;
&lt;p&gt;
LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;transformer&#26550;&#26500;&#30340;&#26174;&#33879;&#36827;&#23637;&#25512;&#21160;&#19979;&#65292;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#22686;&#38271;&#12290;&#20026;&#20102;&#34913;&#37327;&#36827;&#23637;&#65292;&#31934;&#24515;&#31574;&#21010;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20934;&#21482;&#33021;&#22788;&#29702;&#33521;&#25991;&#65292;&#32780;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23578;&#26410;&#26377;&#22810;&#35821;&#35328;&#22522;&#20934;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#24050;&#32463;&#39281;&#21644;&#65292;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26368;&#20339;&#20154;&#31867;&#65292;&#24182;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25991;&#29486;&#65292;&#24182;&#36873;&#25321;&#20102;11&#20010;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;LEXTREME&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32508;&#21512;&#35780;&#20998;&#65292;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#38598;&#65292;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#12290;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#32508;&#21512;&#35780;&#20998;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#34920;&#26126;LEXTREME&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#20026;&#25913;&#36827;&#30041;&#19979;&#20102;&#20805;&#36275;&#31354;&#38388;&#12290;&#20026;&#20102;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20351;&#29992;&#65292;&#25105;&#20204;&#23558;LEXTREME&#19982;&#25152;&#26377;&#25968;&#25454;&#19968;&#36215;&#21457;&#24067;&#22312;huggingface&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;SegAugment&#65292;&#21033;&#29992;&#38899;&#39057;&#20998;&#27573;&#31995;&#32479;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#20010;&#21477;&#23376;&#32423;&#21035;&#30340;&#26367;&#20195;&#29256;&#26412;&#65292;&#33021;&#22815;&#25552;&#39640;&#35821;&#38899;&#32763;&#35793;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;2.5&#20998;&#65292;&#29978;&#33267;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;5&#20998;&#12290;</title><link>http://arxiv.org/abs/2212.09699</link><description>&lt;p&gt;
SegAugment: &#22522;&#20110;&#20998;&#27573;&#22686;&#24378;&#30340;&#35821;&#38899;&#32763;&#35793;&#25968;&#25454;&#21033;&#29992;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations. (arXiv:2212.09699v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;SegAugment&#65292;&#21033;&#29992;&#38899;&#39057;&#20998;&#27573;&#31995;&#32479;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#20010;&#21477;&#23376;&#32423;&#21035;&#30340;&#26367;&#20195;&#29256;&#26412;&#65292;&#33021;&#22815;&#25552;&#39640;&#35821;&#38899;&#32763;&#35793;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;2.5&#20998;&#65292;&#29978;&#33267;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;5&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#32763;&#35793;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#30340;&#25968;&#25454;&#36164;&#28304;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#36164;&#28304;&#26159;&#22522;&#20110;&#25991;&#26723;&#30340;&#65292;&#20294;&#20063;&#25552;&#20379;&#20102;&#19968;&#31181;&#21477;&#23376;&#32423;&#21035;&#30340;&#29256;&#26412;&#65292;&#20294;&#26159;&#23427;&#21482;&#26377;&#21333;&#20010;&#24182;&#19988;&#26159;&#22266;&#23450;&#30340;&#65292;&#21487;&#33021;&#20250;&#22952;&#30861;&#25968;&#25454;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;SegAugment&#65292;&#36890;&#36807;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#20010;&#21477;&#23376;&#32423;&#21035;&#30340;&#26367;&#20195;&#29256;&#26412;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#38899;&#39057;&#20998;&#27573;&#31995;&#32479;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#38271;&#24230;&#32422;&#26463;&#37325;&#26032;&#20998;&#27573;&#27599;&#20010;&#25991;&#26723;&#30340;&#35821;&#38899;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#40784;&#26041;&#27861;&#33719;&#21462;&#30446;&#26631;&#25991;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;MuST-C&#30340;&#20843;&#31181;&#35821;&#35328;&#23545;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19968;&#33268;&#30340;&#22686;&#30410;&#65292;&#24179;&#22343;&#22686;&#21152;&#20102;2.5 BLEU&#20998;&#25968;&#65292;&#24182;&#22312;mTEDx&#30340;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#33719;&#24471;&#20102;&#22810;&#36798;5 BLEU&#20998;&#25968;&#30340;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#24403;&#19982;&#24378;&#22823;&#30340;&#31995;&#32479;&#32467;&#21512;&#26102;&#65292;SegAugment&#22312;MuST-C&#20013;&#26641;&#31435;&#20102;&#26032;&#30340;&#29366;&#24577;&#35760;&#24405;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#22686;&#24378;&#21477;&#23376;&#32423;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#20351;&#24471;&#31471;&#21040;&#31471;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end Speech Translation is hindered by a lack of available data resources. While most of them are based on documents, a sentence-level version is available, which is however single and static, potentially impeding the usefulness of the data. We propose a new data augmentation strategy, SegAugment, to address this issue by generating multiple alternative sentence-level versions of a dataset. Our method utilizes an Audio Segmentation system, which re-segments the speech of each document with different length constraints, after which we obtain the target text via alignment methods. Experiments demonstrate consistent gains across eight language pairs in MuST-C, with an average increase of 2.5 BLEU points, and up to 5 BLEU for low-resource scenarios in mTEDx. Furthermore, when combined with a strong system, SegAugment establishes new state-of-the-art results in MuST-C. Finally, we show that the proposed method can also successfully augment sentence-level datasets, and that it enables 
&lt;/p&gt;</description></item></channel></rss>