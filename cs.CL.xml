<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#24847;&#20041;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#35805;&#39064;-&#28966;&#28857;&#34920;&#36798;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24847;&#20041;&#34920;&#31034;&#20013;&#28155;&#21152;&#35821;&#29992;&#20449;&#24687;&#26469;&#24378;&#21046;&#20351;&#29992;&#20027;&#21160;&#25110;&#34987;&#21160;&#35821;&#24577;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#33410;&#28857;&#32858;&#21512;&#32534;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#33410;&#28857;&#65292;&#26469;&#35299;&#20915;&#20102;&#24847;&#20041;&#20013;&#35789;&#24207;&#20449;&#24687;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02053</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#24847;&#20041;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#35805;&#39064;-&#28966;&#28857;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
Controlling Topic-Focus Articulation in Meaning-to-Text Generation using Graph Neural Networks. (arXiv:2310.02053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#24847;&#20041;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#35805;&#39064;-&#28966;&#28857;&#34920;&#36798;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24847;&#20041;&#34920;&#31034;&#20013;&#28155;&#21152;&#35821;&#29992;&#20449;&#24687;&#26469;&#24378;&#21046;&#20351;&#29992;&#20027;&#21160;&#25110;&#34987;&#21160;&#35821;&#24577;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#33410;&#28857;&#32858;&#21512;&#32534;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#33410;&#28857;&#65292;&#26469;&#35299;&#20915;&#20102;&#24847;&#20041;&#20013;&#35789;&#24207;&#20449;&#24687;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35064;&#38706;&#30340;&#24847;&#20041;&#34920;&#31034;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20197;&#21508;&#31181;&#26041;&#24335;&#34920;&#36798;&#65292;&#36825;&#21462;&#20915;&#20110;&#20449;&#24687;&#22312;&#34920;&#38754;&#23618;&#38754;&#19978;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#26377;&#20852;&#36259;&#25214;&#21040;&#19968;&#31181;&#22312;&#20174;&#24847;&#20041;&#29983;&#25104;&#25991;&#26412;&#26102;&#25511;&#21046;&#35805;&#39064;-&#28966;&#28857;&#34920;&#36798;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#21306;&#20998;&#24102;&#26377;&#21450;&#29289;&#21160;&#35789;&#30340;&#21477;&#23376;&#30340;&#20027;&#21160;&#35821;&#24577;&#21644;&#34987;&#21160;&#35821;&#24577;&#19978;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#22312;&#24847;&#20041;&#34920;&#31034;&#20013;&#28155;&#21152;&#35805;&#39064;&#31561;&#35821;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#25552;&#20379;&#32473;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#26102;&#24378;&#21046;&#20351;&#29992;&#20027;&#21160;&#35821;&#24577;&#25110;&#34987;&#21160;&#35821;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#27169;&#22411;&#65292;&#22240;&#20026;&#22312;&#29992;&#22270;&#34920;&#31034;&#30340;&#24847;&#20041;&#20013;&#27809;&#26377;&#20851;&#20110;&#35789;&#24207;&#30340;&#26126;&#30830;&#20449;&#24687;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35805;&#39064;-&#28966;&#28857;&#34920;&#36798;&#65288;TFA&#65289;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#24847;&#20041;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#22270;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#32858;&#21512;&#30340;&#26032;&#32534;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#26469;&#20195;&#26367;&#20256;&#32479;&#30340;&#32858;&#21512;&#30456;&#37051;&#33410;&#28857;&#20449;&#24687;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A bare meaning representation can be expressed in various ways using natural language, depending on how the information is structured on the surface level. We are interested in finding ways to control topic-focus articulation when generating text from meaning. We focus on distinguishing active and passive voice for sentences with transitive verbs. The idea is to add pragmatic information such as topic to the meaning representation, thereby forcing either active or passive voice when given to a natural language generation system. We use graph neural models because there is no explicit information about word order in a meaning represented by a graph. We try three different methods for topic-focus articulation (TFA) employing graph neural models for a meaning-to-text generation task. We propose a novel encoding strategy about node aggregation in graph neural models, which instead of traditional encoding by aggregating adjacent node information, learns node representations by using depth-f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LST&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#29992;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#27169;&#24577;&#35843;&#25972;&#21644;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#65292;&#20248;&#21270;&#20102;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02050</link><description>&lt;p&gt;
&#38024;&#23545;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Tuning Large language model for End-to-end Speech Translation. (arXiv:2310.02050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LST&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#29992;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#27169;&#24577;&#35843;&#25972;&#21644;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#65292;&#20248;&#21270;&#20102;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#20687;LLaSM&#12289;X-LLM&#21644;SpeechGPT&#36825;&#26679;&#30340;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#25351;&#20196;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;E2E-ST&#65289;&#36825;&#26679;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24120;&#24120;&#19981;&#31283;&#23450;&#65292;&#36825;&#26159;&#19968;&#20010;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#30340;&#32763;&#35793;&#20219;&#21153;&#12290;&#19982;&#21333;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#33853;&#21518;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LST&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22312;E2E-ST&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;LST&#30001;&#35821;&#38899;&#21069;&#31471;&#12289;&#36866;&#37197;&#22120;&#21644;LLM&#21518;&#31471;&#32452;&#25104;&#12290;LST&#30340;&#35757;&#32451;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#27169;&#24577;&#35843;&#25972;&#65292;&#20854;&#20013;&#36866;&#37197;&#22120;&#34987;&#35843;&#25972;&#20197;&#20351;&#35821;&#38899;&#34920;&#31034;&#19982;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#65288;2&#65289;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#65292;&#20854;&#20013;&#36866;&#37197;&#22120;&#21644;LLM&#27169;&#22411;&#21516;&#26102;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22312;E2E-ST&#20219;&#21153;&#19978;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;MuST-C&#35821;&#38899;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
With the emergence of large language models (LLMs), multimodal models based on LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM, and SpeechGPT exhibit an impressive ability to comprehend and generate human instructions. However, their performance often falters when faced with complex tasks like end-to-end speech translation (E2E-ST), a cross-language and cross-modal translation task. In comparison to single-modal models, multimodal models lag behind in these scenarios. This paper introduces LST, a Large multimodal model designed to excel at the E2E-ST task. LST consists of a speech frontend, an adapter, and a LLM backend. The training of LST consists of two stages: (1) Modality adjustment, where the adapter is tuned to align speech representation with text embedding space, and (2) Downstream task fine-tuning, where both the adapter and LLM model are trained to optimize performance on the E2EST task. Experimental results on the MuST-C speech translation benchmar
&lt;/p&gt;</description></item><item><title>&#35780;&#22996;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#19981;&#21516;&#31995;&#32479;&#21644;&#25351;&#26631;&#30340;&#35780;&#20272;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#24230;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.02040</link><description>&lt;p&gt;
&#35780;&#22996;&#65306;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Jury: A Comprehensive Evaluation Toolkit. (arXiv:2310.02040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02040
&lt;/p&gt;
&lt;p&gt;
&#35780;&#22996;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#19981;&#21516;&#31995;&#32479;&#21644;&#25351;&#26631;&#30340;&#35780;&#20272;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#24230;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#20316;&#20026;&#20219;&#20309;&#22522;&#20110;&#39044;&#27979;&#30340;&#31995;&#32479;&#30340;&#22522;&#26412;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#21644;&#21508;&#31181;&#25351;&#26631;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#20351;&#29992;&#19981;&#21516;&#25351;&#26631;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35780;&#22996;&#30340;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#26631;&#20934;&#21270;&#32467;&#26500;&#65292;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#35780;&#22996;&#30340;&#30446;&#26631;&#26159;&#26631;&#20934;&#21270;&#21644;&#25913;&#36827;&#25152;&#26377;&#31995;&#32479;&#30340;&#24230;&#37327;&#35780;&#20272;&#65292;&#24182;&#24110;&#21161;&#31038;&#21306;&#20811;&#26381;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#12290;&#33258;&#35780;&#22996;&#30340;&#24320;&#28304;&#21457;&#24067;&#20197;&#26469;&#65292;&#24050;&#32463;&#21560;&#24341;&#20102;&#24191;&#22823;&#29992;&#25143;&#65292;&#21487;&#22312;https://github.com/obss/jury &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation plays a critical role in deep learning as a fundamental block of any prediction-based system. However, the vast number of Natural Language Processing (NLP) tasks and the development of various metrics have led to challenges in evaluating different systems with different metrics. To address these challenges, we introduce jury, a toolkit that provides a unified evaluation framework with standardized structures for performing evaluation across different tasks and metrics. The objective of jury is to standardize and improve metric evaluation for all systems and aid the community in overcoming the challenges in evaluation. Since its open-source release, jury has reached a wide audience and is available at https://github.com/obss/jury.
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01991</link><description>&lt;p&gt;
&#22635;&#31354;&#39064;&#65306;&#25506;&#32034;&#24182;&#22686;&#24378;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems. (arXiv:2310.01991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#26399;&#30340;&#25991;&#29486;&#20013;&#24191;&#27867;&#25506;&#35752;&#20102;&#27491;&#21521;&#25512;&#29702;&#65288;&#21363;&#32473;&#23450;&#38382;&#39064;&#25214;&#31572;&#26696;&#65289;&#65292;&#20294;&#36870;&#21521;&#25512;&#29702;&#30456;&#23545;&#36739;&#23569;&#34987;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#32473;&#23450;&#19968;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#20854;&#31572;&#26696;&#65292;&#22312;&#38382;&#39064;&#20013;&#26377;&#20123;&#32454;&#33410;&#34987;&#30465;&#30053;&#20102;&#65292;LLM&#33021;&#21542;&#26377;&#25928;&#22320;&#36824;&#21407;&#20986;&#32570;&#22833;&#30340;&#20449;&#24687;&#65311;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#20462;&#25913;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#36825;&#19968;&#20219;&#21153;&#65306;GSM8k&#12289;SVAMP&#21644;MultiArith&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#27491;&#21521;&#25512;&#29702;&#30456;&#27604;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#27169;&#22411;&#65288;GPT4&#12289;GPT3.5&#12289;PaLM-2&#21644;LLaMa-2&#65289;&#22312;&#36870;&#21521;&#25512;&#29702;&#19978;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#21033;&#29992;&#35813;&#20219;&#21153;&#30340;&#29305;&#23450;&#26684;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#65306;Rephrase&#23558;&#32473;&#23450;&#30340;&#38382;&#39064;&#37325;&#36848;&#20026;&#19968;&#20010;&#27491;&#21521;&#25512;&#29702;&#38382;&#39064;&#65292;PAL-Tools&#32467;&#21512;&#20102;&#31243;&#24207;&#36741;&#21161;&#30340;LLM&#24605;&#24819;&#65292;&#29983;&#25104;&#19968;&#32452;&#26041;&#31243;&#24335;&#21487;&#20197;&#35299;&#20915;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information?  In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#20219;&#21153;&#20013;&#20351;&#29992;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26816;&#32034;&#23384;&#20648;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01960</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Language Models as Knowledge Bases for Visual Word Sense Disambiguation. (arXiv:2310.01960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#20219;&#21153;&#20013;&#20351;&#29992;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26816;&#32034;&#23384;&#20648;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#65288;VWSD&#65289;&#26159;&#19968;&#39033;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20301;&#20110;&#35821;&#20041;&#28040;&#27495;&#21644;&#32454;&#31890;&#24230;&#22810;&#27169;&#24335;&#26816;&#32034;&#20043;&#38388;&#12290;&#26368;&#36817;&#21457;&#23637;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VL transformers&#65289;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#19968;&#20123;&#29616;&#25104;&#30340;&#23454;&#29616;&#20855;&#26377;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#22686;&#24378;&#30693;&#35782;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#30693;&#35782;&#24211;&#65292;&#26469;&#25552;&#39640;VL transformers&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#26816;&#32034;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#20505;&#36873;&#31572;&#26696;&#65292;&#23558;VWSD&#36716;&#21270;&#20026;&#32431;&#25991;&#26412;&#38382;&#31572;&#65288;QA&#65289;&#38382;&#39064;&#12290;&#21033;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#26469;&#25506;&#32034;&#36825;&#31181;&#36716;&#25442;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#20511;&#21161;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies between linguistic sense disambiguation and fine-grained multimodal retrieval. The recent advancements in the development of visiolinguistic (VL) transformers suggest some off-the-self implementations with encouraging results, which however we argue that can be further improved. To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prom
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20013;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01957</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36827;&#34892;&#39550;&#39542;&#65306;&#34701;&#21512;&#23545;&#35937;&#32423;&#21521;&#37327;&#27169;&#24577;&#20197;&#35299;&#37322;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving. (arXiv:2310.01957v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20013;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#65292;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#39550;&#39542;&#24773;&#22659;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;10k&#20010;&#39550;&#39542;&#24773;&#22659;&#30340;160k&#20010;&#38382;&#31572;&#23545;&#65292;&#36825;&#20123;&#38382;&#31572;&#23545;&#19982;&#30001;RL&#20195;&#29702;&#25910;&#38598;&#30340;&#39640;&#36136;&#37327;&#25511;&#21046;&#21629;&#20196;&#21644;&#30001;&#25945;&#24072;LLM&#65288;GPT-3.5&#65289;&#29983;&#25104;&#30340;&#38382;&#39064;&#31572;&#26696;&#23545;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;&#21521;&#37327;&#23383;&#24149;&#35821;&#35328;&#25968;&#25454;&#26469;&#23545;&#40784;&#25968;&#23383;&#21521;&#37327;&#27169;&#24577;&#21644;&#38745;&#24577;LLM&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39550;&#39542;&#38382;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#22522;&#20110;LLM&#30340;&#39550;&#39542;&#34892;&#20026;&#29983;&#25104;&#19982;&#20256;&#32479;&#34892;&#20026;&#20811;&#38534;&#30456;&#27604;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01929</link><description>&lt;p&gt;
&#31359;&#36234;&#25991;&#21270;&#40511;&#27807;&#65306;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E&#21644;StableDiffusion&#65292;&#22312;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#38646;&#23556;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#20316;&#20026;&#25991;&#21270;&#30340;&#23186;&#20171;&#65292;&#35821;&#35328;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32780;&#22609;&#36896;&#20102;&#23427;&#20204;&#30340;&#25991;&#21270;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25551;&#36848;&#25991;&#21270;&#32500;&#24230;&#65292;&#25991;&#21270;&#39046;&#22495;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#19977;&#20010;&#23618;&#27425;&#26469;&#25506;&#32034;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25216;&#26415;&#65292;&#21253;&#25324;&#20351;&#29992;CLIP&#31354;&#38388;&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36827;&#34892;&#22806;&#22312;&#35780;&#20272;&#20197;&#21450;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;TTI&#25991;&#21270;&#24863;&#30693;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CulText2I&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30340;TTI&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;
&lt;/p&gt;
&lt;p&gt;
Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#19981;&#32479;&#19968;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#36755;&#20837;&#19982;&#36755;&#20986;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2310.01917</link><description>&lt;p&gt;
&#20998;&#23618;&#35780;&#20272;&#26694;&#26550;&#65306;&#20154;&#24037;&#35780;&#20272;&#30340;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Evaluation Framework: Best Practices for Human Evaluation. (arXiv:2310.01917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01917
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#19981;&#32479;&#19968;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#36755;&#20837;&#19982;&#36755;&#20986;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23427;&#35780;&#20272;&#20102;&#24320;&#21457;&#31995;&#32479;&#30340;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#31995;&#32479;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#22312;NLP&#20013;&#32570;&#20047;&#24191;&#27867;&#25509;&#21463;&#30340;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#65292;&#38459;&#30861;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#20844;&#24179;&#27604;&#36739;&#21644;&#24314;&#31435;&#26222;&#36941;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;NLP&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#20960;&#20010;&#32570;&#21475;&#12290;&#36825;&#20123;&#32570;&#21475;&#25104;&#20026;&#25105;&#20204;&#24320;&#21457;&#33258;&#24049;&#30340;&#20998;&#23618;&#35780;&#20272;&#26694;&#26550;&#30340;&#21160;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;NLP&#31995;&#32479;&#24615;&#33021;&#34920;&#31034;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#35780;&#20272;&#24320;&#21457;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20849;&#29983;&#27169;&#22411;&#20013;&#34987;&#20351;&#29992;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#36755;&#20837;&#19982;&#36755;&#20986;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human evaluation plays a crucial role in Natural Language Processing (NLP) as it assesses the quality and relevance of developed systems, thereby facilitating their enhancement. However, the absence of widely accepted human evaluation metrics in NLP hampers fair comparisons among different systems and the establishment of universal assessment standards. Through an extensive analysis of existing literature on human evaluation metrics, we identified several gaps in NLP evaluation methodologies. These gaps served as motivation for developing our own hierarchical evaluation framework. The proposed framework offers notable advantages, particularly in providing a more comprehensive representation of the NLP system's performance. We applied this framework to evaluate the developed Machine Reading Comprehension system, which was utilized within a human-AI symbiosis model. The results highlighted the associations between the quality of inputs and outputs, underscoring the necessity to evaluate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#22359;&#35745;&#31639;&#21644;&#36890;&#20449;&#37325;&#21472;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#65292;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#38271;&#24230;&#33021;&#22815;&#26356;&#38271;&#12290;</title><link>http://arxiv.org/abs/2310.01889</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#22359;Transformer&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#35299;&#20915;&#36817;&#26080;&#38480;&#19978;&#19979;&#25991;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ring Attention with Blockwise Transformers for Near-Infinite Context. (arXiv:2310.01889v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#22359;&#35745;&#31639;&#21644;&#36890;&#20449;&#37325;&#21472;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#65292;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#38271;&#24230;&#33021;&#22815;&#26356;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39318;&#36873;&#26550;&#26500;&#65292;&#22312;&#24191;&#27867;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#23545;&#20869;&#23384;&#30340;&#38656;&#27714;&#38480;&#21046;&#20102;&#23427;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#23545;&#20110;&#28041;&#21450;&#25193;&#23637;&#24207;&#21015;&#25110;&#38271;&#26399;&#20381;&#36182;&#30340;&#20219;&#21153;&#32780;&#35328;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#21363;&#29615;&#24418;&#27880;&#24847;&#21147;(Ring Attention)&#65292;&#23427;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#22359;&#35745;&#31639;&#23558;&#38271;&#24207;&#21015;&#20998;&#24067;&#21040;&#22810;&#20010;&#35774;&#22791;&#19978;&#65292;&#21516;&#26102;&#23558;&#20851;&#38190;-&#20540;&#22359;&#30340;&#36890;&#20449;&#19982;&#20998;&#22359;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#37325;&#21472;&#12290;&#36890;&#36807;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#65292;&#29615;&#24418;&#27880;&#24847;&#21147;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#27604;&#20043;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;Transformer&#33021;&#22815;&#22810;&#20986;&#35774;&#22791;&#25968;&#37327;&#20493;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving extended sequences or long-term dependencies. We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while concurrently overlapping the communication of key-value blocks with the computation of blockwise attention. By processing longer input sequences while maintaining memory efficiency, Ring Attention enables training and inference of sequences that are device count times longer than those of prior memory-efficient Transformers, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.01886</link><description>&lt;p&gt;
&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22312;&#32447;&#25552;&#20379;&#30340;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#20256;&#36882;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#26377;&#25928;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21508;&#31181;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#20063;&#21487;&#20379;&#20844;&#20247;&#20351;&#29992;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#32791;&#26102;&#19988;&#24494;&#35843;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#20250;&#32473;&#23384;&#20648;&#21644;&#26381;&#21153;&#24102;&#26469;&#24040;&#22823;&#36127;&#25285;&#12290;&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#26080;&#38656;&#35757;&#32451;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#23558;&#22810;&#20010;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#37325;&#22797;&#20351;&#29992;&#21040;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#19982;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22823;&#30340;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#26469;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#12290;&#38024;&#23545;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PERU-FFT&#65292;&#36890;&#36807;&#23558;&#31232;&#30095;&#20219;&#21153;&#21521;&#37327;&#27880;&#20837;&#21040;&#19968;&#20010;mer&#27169;&#22411;&#20013;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a mer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#31934;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#34920;&#31034;&#22312;&#31070;&#32463;&#35299;&#30721;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;10&#20010;NLU&#20219;&#21153;&#20013;&#65292;&#31934;&#35843;&#34920;&#31034;&#19981;&#33021;&#26356;&#22909;&#22320;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.01854</link><description>&lt;p&gt;
&#31934;&#35843;&#19982;&#25552;&#31034;&#35843;&#25972;&#30340;&#30417;&#30563;&#34920;&#31034;&#65306;&#21738;&#31181;&#26356;&#33021;&#35299;&#37322;&#22823;&#33041;&#20013;&#30340;&#35821;&#35328;&#34920;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?. (arXiv:2310.01854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#31934;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#34920;&#31034;&#22312;&#31070;&#32463;&#35299;&#30721;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;10&#20010;NLU&#20219;&#21153;&#20013;&#65292;&#31934;&#35843;&#34920;&#31034;&#19981;&#33021;&#26356;&#22909;&#22320;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#35835;&#20154;&#33041;&#35821;&#35328;&#34920;&#31034;&#32972;&#21518;&#30340;&#31639;&#27861;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#22312;NLU&#20219;&#21153;&#19978;&#36827;&#34892;&#36807;&#31934;&#35843;&#30340;&#39044;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#26469;&#25506;&#27979;&#22823;&#33041;&#23545;&#35821;&#35328;&#36755;&#20837;&#30340;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#23436;&#20840;&#30340;&#31934;&#35843;&#20250;&#26356;&#26032;&#25972;&#20010;&#21442;&#25968;&#31354;&#38388;&#24182;&#25197;&#26354;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#65292;&#19982;&#22823;&#33041;&#30340;&#24378;&#20581;&#22810;&#20219;&#21153;&#23398;&#20064;&#33021;&#21147;&#19981;&#19968;&#33268;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25552;&#31034;&#35843;&#25972;&#20445;&#25252;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#23884;&#20837;&#20197;&#36866;&#24212;&#20219;&#21153;&#12290;&#25552;&#31034;&#35843;&#25972;&#26159;&#21542;&#33021;&#29983;&#25104;&#26356;&#33021;&#35299;&#37322;&#22823;&#33041;&#35821;&#35328;&#34920;&#31034;&#30340;&#34920;&#31034;&#65311;&#22914;&#26524;&#26159;&#65292;&#21738;&#31181;NLU&#20219;&#21153;&#33021;&#35753;&#39044;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#20449;&#24687;&#34920;&#31034;&#65311;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#25552;&#31034;&#35843;&#25972;&#21644;&#31934;&#35843;&#30340;&#34920;&#31034;&#22312;&#31070;&#32463;&#35299;&#30721;&#20013;&#36827;&#34892;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#21363;&#20174;&#21050;&#28608;&#24341;&#21457;&#30340;&#22823;&#33041;&#27963;&#21160;&#20013;&#39044;&#27979;&#35821;&#35328;&#21050;&#28608;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;10&#20010;NLU&#20219;&#21153;&#20013;&#65292;&#20840;&#29699;&#31934;&#35843;&#34920;&#31034;&#37117;&#19981;&#33021;&#26356;&#22909;&#22320;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
To decipher the algorithm underlying the human brain's language representation, previous work probed brain responses to language input with pre-trained artificial neural network (ANN) models fine-tuned on NLU tasks. However, full fine-tuning generally updates the entire parametric space and distorts pre-trained features, cognitively inconsistent with the brain's robust multi-task learning ability. Prompt-tuning, in contrast, protects pre-trained weights and learns task-specific embeddings to fit a task. Could prompt-tuning generate representations that better account for the brain's language representations than fine-tuning? If so, what kind of NLU task leads a pre-trained model to better decode the information represented in the human brain? We investigate these questions by comparing prompt-tuned and fine-tuned representations in neural decoding, that is predicting the linguistic stimulus from the brain activities evoked by the stimulus. We find that on none of the 10 NLU tasks, full
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#29983;&#25104;&#21644;&#39564;&#35777;&#20043;&#38388;&#19968;&#33268;&#24615;&#30340;&#26694;&#26550;&#65292;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#20165;&#22312;76%&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#25913;&#36827;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32463;&#36807;&#31579;&#36873;&#30340;&#29983;&#25104;&#22120;&#21644;&#39564;&#35777;&#22120;&#31572;&#26696;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;&#19968;&#33268;&#24615;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#23558;Alpaca-30B&#30340;&#19968;&#33268;&#24615;&#20174;60%&#25552;&#39640;&#21040;93%&#65292;&#24182;&#19988;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#20063;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#38500;&#20102;&#25913;&#36827;&#19968;&#33268;&#24615;&#22806;&#65292;&#19968;&#33268;&#24615;&#24494;&#35843;&#36824;&#24102;&#26469;&#20102;&#20854;&#20182;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.01846</link><description>&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#21644;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22120;&#39564;&#35777;&#22120;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Improving Generator-Validator Consistency of Language Models. (arXiv:2310.01846v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#29983;&#25104;&#21644;&#39564;&#35777;&#20043;&#38388;&#19968;&#33268;&#24615;&#30340;&#26694;&#26550;&#65292;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#20165;&#22312;76%&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#25913;&#36827;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32463;&#36807;&#31579;&#36873;&#30340;&#29983;&#25104;&#22120;&#21644;&#39564;&#35777;&#22120;&#31572;&#26696;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;&#19968;&#33268;&#24615;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#23558;Alpaca-30B&#30340;&#19968;&#33268;&#24615;&#20174;60%&#25552;&#39640;&#21040;93%&#65292;&#24182;&#19988;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#20063;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#38500;&#20102;&#25913;&#36827;&#19968;&#33268;&#24615;&#22806;&#65292;&#19968;&#33268;&#24615;&#24494;&#35843;&#36824;&#24102;&#26469;&#20102;&#20854;&#20182;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25130;&#33267;2023&#24180;9&#26376;&#65292;ChatGPT&#22312;&#34987;&#38382;&#21040;"7+8=15&#65292;&#23545;&#36824;&#26159;&#38169;"&#26102;&#65292;&#22238;&#31572;"&#38169;"&#65292;&#20294;&#22312;&#34987;&#38382;&#21040;"7+8"&#31561;&#20110;&#22810;&#23569;&#26102;&#65292;&#22238;&#31572;"15"&#12290;&#36825;&#31181;&#29983;&#25104;&#21644;&#39564;&#35777;&#31572;&#26696;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24456;&#24120;&#35265;&#65292;&#30772;&#22351;&#20102;&#20154;&#20204;&#30340;&#20449;&#20219;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#29983;&#25104;&#21644;&#39564;&#35777;&#20043;&#38388;&#19968;&#33268;&#24615;&#30340;&#26694;&#26550;&#65288;&#31216;&#20026;&#29983;&#25104;&#22120;&#39564;&#35777;&#22120;&#19968;&#33268;&#24615;&#65289;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;GPT-4&#35821;&#35328;&#27169;&#22411;&#65292;&#20165;&#22312;76%&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32463;&#36807;&#29983;&#25104;&#22120;&#21644;&#39564;&#35777;&#22120;&#31572;&#26696;&#31579;&#36873;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#31216;&#20043;&#20026;&#19968;&#33268;&#24615;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;Alpaca-30B&#30340;&#19968;&#33268;&#24615;&#20174;60%&#25552;&#39640;&#21040;&#20102;93%&#65292;&#24182;&#19988;&#36825;&#31181;&#25913;&#36827;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#31215;&#26497;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39118;&#26684;&#65292;&#22914;&#24189;&#40664;&#65289;&#12290;&#38500;&#20102;&#25913;&#36827;&#19968;&#33268;&#24615;&#22806;&#65292;&#19968;&#33268;&#24615;&#24494;&#35843;&#36824;&#25913;&#21892;&#20102;&#20854;&#20182;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
As of September 2023, ChatGPT correctly answers "what is 7+8" with 15, but when asked "7+8=15, True or False" it responds with "False". This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. In this paper, we propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), finding that even GPT-4, a state-of-the-art LM, is GV-consistent only 76% of the time. To improve the consistency of LMs, we propose to finetune on the filtered generator and validator responses that are GV-consistent, and call this approach consistency fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B from 60% to 93%, and the improvement extrapolates to unseen tasks and domains (e.g., GV-consistency for positive style transfers extrapolates to unseen styles like humor). In addition to improving consistency, consistency fine-tuning improves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01845</link><description>&lt;p&gt;
&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#22312;&#24120;&#35268;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#36965;&#24863;&#22270;&#20687;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#31934;&#30830;&#30340;&#24314;&#31569;&#29289;&#23454;&#20363;&#20998;&#21106;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#21463;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#24050;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#20247;&#22810;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;Segment Anything Model&#65288;SAM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#20854;&#25797;&#38271;&#26080;&#31867;&#21035;&#22270;&#20687;&#20998;&#21106;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;SAM&#30340;&#23616;&#38480;&#24615;&#65292;&#25581;&#31034;&#20102;&#23427;&#22312;&#24212;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;SAM&#19981;&#20855;&#22791;&#35782;&#21035;&#33021;&#21147;&#65292;&#22240;&#27492;&#26080;&#27861;&#23545;&#23450;&#20301;&#30340;&#23545;&#35937;&#36827;&#34892;&#20998;&#31867;&#21644;&#26631;&#35760;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different promp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#27169;&#22411;&#12290;&#35813;&#25439;&#22833;&#20989;&#25968;&#26088;&#22312;&#22312;&#20445;&#25345;&#38899;&#20301;&#31867;&#21035;&#20043;&#38388;&#26356;&#22909;&#30340;&#38899;&#20301;&#24046;&#24322;&#30340;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#22238;&#24402;&#30446;&#26631;&#36755;&#20986;&#30340;&#26377;&#24207;&#20851;&#31995;&#12290;&#36890;&#36807;&#24341;&#20837;&#38899;&#20301;-&#21306;&#20998;&#27491;&#21017;&#21270;&#22120;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#21457;&#38899;&#35780;&#20272;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.01839</link><description>&lt;p&gt;
&#20445;&#30041;&#38899;&#20301;&#24046;&#24322;&#30340;&#24207;&#25968;&#22238;&#24402;&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss Function for Automatic Pronunciation Assessment. (arXiv:2310.01839v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01839
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#27169;&#22411;&#12290;&#35813;&#25439;&#22833;&#20989;&#25968;&#26088;&#22312;&#22312;&#20445;&#25345;&#38899;&#20301;&#31867;&#21035;&#20043;&#38388;&#26356;&#22909;&#30340;&#38899;&#20301;&#24046;&#24322;&#30340;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#22238;&#24402;&#30446;&#26631;&#36755;&#20986;&#30340;&#26377;&#24207;&#20851;&#31995;&#12290;&#36890;&#36807;&#24341;&#20837;&#38899;&#20301;-&#21306;&#20998;&#27491;&#21017;&#21270;&#22120;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#21457;&#38899;&#35780;&#20272;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#65288;APA&#65289;&#21487;&#20197;&#37327;&#21270;&#31532;&#20108;&#35821;&#35328;&#65288;L2&#65289;&#23398;&#20064;&#32773;&#22312;&#35821;&#35328;&#20013;&#30340;&#21457;&#38899;&#29087;&#32451;&#31243;&#24230;&#12290;&#24120;&#35265;&#30340;APA&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#31070;&#32463;&#27169;&#22411;&#21644;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#65289;&#36827;&#34892;&#29087;&#32451;&#27700;&#24179;&#39044;&#27979;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#22312;&#29305;&#24449;&#31354;&#38388;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#29087;&#32451;&#27700;&#24179;&#30340;&#26377;&#24207;&#24615;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#21363;&#19981;&#21516;&#38899;&#32032;&#31867;&#21035;&#30340;&#30456;&#21516;&#29087;&#32451;&#27700;&#24179;&#24517;&#28982;&#34987;&#36843;&#38752;&#24471;&#24456;&#36817;&#65292;&#20445;&#30041;&#20102;&#26356;&#23569;&#30340;&#38899;&#20301;&#8212;&#21306;&#20998;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22522;&#20110;&#22238;&#24402;&#30340;APA&#27169;&#22411;&#30340;&#38899;&#20301;&#23545;&#27604;&#24207;&#25968;&#65288;PCO&#65289;&#25439;&#22833;&#65292;&#26088;&#22312;&#22312;&#20445;&#25345;&#38899;&#20301;&#31867;&#21035;&#20043;&#38388;&#26356;&#22909;&#30340;&#38899;&#20301;&#24046;&#24322;&#30340;&#21516;&#26102;&#65292;&#32771;&#34385;&#22238;&#24402;&#30446;&#26631;&#36755;&#20986;&#30340;&#26377;&#24207;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#38899;&#20301;&#8212;&#21306;&#20998;&#27491;&#21017;&#21270;&#22120;&#24341;&#20837;&#21040;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20013;&#65292;&#40723;&#21169;...&#65288;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
Automatic pronunciation assessment (APA) manages to quantify the pronunciation proficiency of a second language (L2) learner in a language. Prevailing approaches to APA normally leverage neural models trained with a regression loss function, such as the mean-squared error (MSE) loss, for proficiency level prediction. Despite most regression models can effectively capture the ordinality of proficiency levels in the feature space, they are confronted with a primary obstacle that different phoneme categories with the same proficiency level are inevitably forced to be close to each other, retaining less phoneme-discriminative information. On account of this, we devise a phonemic contrast ordinal (PCO) loss for training regression-based APA models, which aims to preserve better phonemic distinctions between phoneme categories meanwhile considering ordinal relationships of the regression target output. Specifically, we introduce a phoneme-distinct regularizer into the MSE loss, which encoura
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2310.01837</link><description>&lt;p&gt;
&#25193;&#23637;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#26080;&#27861;&#23545;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#12289;&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;/&#25512;&#29702;&#25805;&#20316;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21482;&#33021;&#34987;&#35270;&#20026;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#19987;&#23478;&#38656;&#35201;&#24110;&#21161;&#29702;&#35299;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#34892;&#20026;&#21644;&#22522;&#30784;&#20915;&#31574;&#36807;&#31243;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#30830;&#20445;AI&#27169;&#22411;&#31283;&#20581;&#12289;&#23454;&#29992;&#21644;&#21487;&#20449;&#36182;&#37096;&#32626;&#30340;&#25163;&#27573;&#12290;&#24050;&#26377;&#19968;&#20123;XAI&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#35299;&#37322;&#21017;&#22522;&#26412;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;&#26368;&#26032;&#30340;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#20351;&#20854;&#36866;&#29992;&#20110;&#22810;&#31867;&#22270;&#20687;&#20998;&#21106;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20854;&#20013;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#24314;&#31569;&#29289;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current AI-based methods do not provide comprehensible physical interpretations of the utilized data, extracted features, and predictions/inference operations. As a result, deep learning models trained using high-resolution satellite imagery lack transparency and explainability and can be merely seen as a black box, which limits their wide-level adoption. Experts need help understanding the complex behavior of AI models and the underlying decision-making process. The explainable artificial intelligence (XAI) field is an emerging field providing means for robust, practical, and trustworthy deployment of AI models. Several XAI techniques have been proposed for image classification tasks, whereas the interpretation of image segmentation remains largely unexplored. This paper offers to bridge this gap by adapting the recent XAI classification algorithms and making them usable for muti-class image segmentation, where we mainly focus on buildings' segmentation from high-resolution satellite 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2310.01828</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65306;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20851;&#38190;&#20219;&#21153;&#24212;&#29992;&#26102;&#30340;&#24517;&#22791;&#35201;&#27714;&#65292;&#30830;&#20445;&#25152;&#20351;&#29992;&#30340;&#40657;&#30418;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;XAI&#30340;&#37325;&#35201;&#24615;&#28085;&#30422;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#24448;&#24448;&#26159;&#40657;&#30418;&#23376;&#65292;&#22240;&#27492;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#23427;&#20204;&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#26512;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#36965;&#24863;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;XAI&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#65292;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;XAI&#31639;&#27861;&#26469;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) has emerged as an essential requirement when dealing with mission-critical applications, ensuring transparency and interpretability of the employed black box AI models. The significance of XAI spans various domains, from healthcare to finance, where understanding the decision-making process of deep learning algorithms is essential. Most AI-based computer vision models are often black boxes; hence, providing explainability of deep neural networks in image processing is crucial for their wide adoption and deployment in medical image analysis, autonomous driving, and remote sensing applications. Recently, several XAI methods for image classification tasks have been introduced. On the contrary, image segmentation has received comparatively less attention in the context of explainability, although it is a fundamental task in computer vision applications, especially in remote sensing. Only some research proposes gradient-based XAI algorithms for imag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.01825</link><description>&lt;p&gt;
&#20908;&#23567;&#40614;&#20998;&#21106;&#30340;PEFT&#25216;&#26415;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#26368;&#36817;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#38656;&#27714;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#21644;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#28508;&#22312;&#30340;PEFT&#24212;&#29992;&#12290;&#19981;&#21516;&#22320;&#21306;&#30340;&#27668;&#20505;&#22810;&#26679;&#24615;&#21644;&#23545;&#20840;&#38754;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#32473;&#31934;&#30830;&#35782;&#21035;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#31181;&#26893;&#23395;&#33410;&#30340;&#20316;&#29289;&#31867;&#22411;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20840;&#38754;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20351;&#29992;&#22269;&#20869;&#39046;&#20808;&#30340;&#20908;&#23567;&#40614;&#20316;&#29289;&#30417;&#27979;&#27169;&#22411;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;PEFT&#26041;&#27861;&#22312;&#20316;&#29289;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;PEFT&#26041;&#27861;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced significant growth and have been extensively employed to adapt large vision and language models to various domains, enabling satisfactory model performance with minimal computational needs. Despite these advances, more research has yet to delve into potential PEFT applications in real-life scenarios, particularly in the critical domains of remote sensing and crop monitoring. The diversity of climates across different regions and the need for comprehensive large-scale datasets have posed significant obstacles to accurately identify crop types across varying geographic locations and changing growing seasons. This study seeks to bridge this gap by comprehensively exploring the feasibility of cross-area and cross-year out-of-distribution generalization using the State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to explore PEFT approaches for crop monitoring. Specifically, we focus on adap
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#26512;&#21644;&#32467;&#26500;&#35782;&#21035;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#24615;&#30340;KV&#32531;&#23384;&#65292;&#36890;&#36807;&#28165;&#38500;&#21644;&#20002;&#24323;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;&#21482;&#23545;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#26631;&#20934;KV&#32531;&#23384;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.01801</link><description>&lt;p&gt;
&#27169;&#22411;&#21578;&#35785;&#20320;&#35813;&#20002;&#24323;&#20160;&#20040;&#65306;&#36866;&#24212;&#24615;KV&#32531;&#23384;&#21387;&#32553;&#29992;&#20110;LLMs
&lt;/p&gt;
&lt;p&gt;
Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01801
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#26512;&#21644;&#32467;&#26500;&#35782;&#21035;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#24615;&#30340;KV&#32531;&#23384;&#65292;&#36890;&#36807;&#28165;&#38500;&#21644;&#20002;&#24323;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;&#21482;&#23545;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#26631;&#20934;KV&#32531;&#23384;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#25512;&#29702;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#19982;&#20256;&#32479;&#30340;KV&#32531;&#23384;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#26512;&#26469;&#35782;&#21035;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#22522;&#20110;&#35782;&#21035;&#20986;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#26500;&#24314;KV&#32531;&#23384;&#65306;&#22312;&#24378;&#35843;&#26412;&#22320;&#19978;&#19979;&#25991;&#30340;&#27880;&#24847;&#21147;&#22836;&#19978;&#28165;&#38500;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#65292;&#22312;&#20197;&#29305;&#27530;&#26631;&#35760;&#20026;&#20013;&#24515;&#30340;&#27880;&#24847;&#21147;&#22836;&#19978;&#20002;&#24323;&#38750;&#29305;&#27530;&#26631;&#35760;&#65292;&#24182;&#19988;&#20165;&#23545;&#24191;&#27867;&#20851;&#27880;&#25152;&#26377;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#26631;&#20934;&#30340;KV&#32531;&#23384;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#27880;&#24847;&#21147;&#20998;&#26512;&#26469;&#25351;&#23548;&#33258;&#36866;&#24212;KV&#32531;&#23384;&#30340;&#26500;&#24314;&#65292;FastGen&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;FastGen&#22312;GPU&#20869;&#23384;&#28040;&#32791;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.01798</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#19981;&#33021;&#33258;&#25105;&#32416;&#27491;&#25512;&#29702;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Cannot Self-Correct Reasoning Yet. (arXiv:2310.01798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01798
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20973;&#20511;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26080;&#21487;&#27604;&#25311;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#32780;&#25104;&#20026;&#31361;&#30772;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20854;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24403;&#24615;&#20173;&#23384;&#22312;&#30097;&#34385;&#12290;&#33258;&#25105;&#32416;&#27491;&#26041;&#27861;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;LLMs&#20869;&#37096;&#30340;&#33258;&#25105;&#32416;&#27491;&#30340;&#20316;&#29992;&#21644;&#25928;&#26524;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#32771;&#23519;&#65292;&#25581;&#31034;&#20102;&#20854;&#30495;&#27491;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20869;&#22312;&#33258;&#25105;&#32416;&#27491;&#30340;&#27010;&#24565;&#65292;&#21363;LLMs&#23581;&#35797;&#20165;&#20165;&#20381;&#38752;&#20854;&#22266;&#26377;&#33021;&#21147;&#26469;&#32416;&#27491;&#20854;&#21021;&#22987;&#21709;&#24212;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#21453;&#39304;&#30340;&#25903;&#25345;&#12290;&#22312;&#25512;&#29702;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;LLMs&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#33258;&#25105;&#32416;&#27491;&#20854;&#21709;&#24212;&#65292;&#29978;&#33267;&#26377;&#26102;&#20505;&#20854;&#34920;&#29616;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future resear
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#23545;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#19982;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#31185;&#23398;&#21453;&#39304;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01783</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#23545;&#30740;&#31350;&#35770;&#25991;&#26377;&#29992;&#30340;&#21453;&#39304;&#21527;&#65311;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models provide useful feedback on research papers? A large-scale empirical analysis. (arXiv:2310.01783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#23545;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#19982;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#31185;&#23398;&#21453;&#39304;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#30340;&#21453;&#39304;&#26159;&#20005;&#35880;&#30740;&#31350;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#23398;&#26415;&#20135;&#20986;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#22797;&#26434;&#30340;&#19987;&#19994;&#30693;&#35782;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#31185;&#23398;&#21453;&#39304;&#26426;&#21046;&#12290;&#36234;&#26469;&#36234;&#38590;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#21516;&#34892;&#35780;&#23457;&#24847;&#35265;&#12290;&#21021;&#32423;&#30740;&#31350;&#20154;&#21592;&#25110;&#26469;&#33258;&#36164;&#28304;&#21294;&#20047;&#30340;&#29615;&#22659;&#23588;&#20854;&#38590;&#20197;&#21450;&#26102;&#33719;&#24471;&#21453;&#39304;&#12290;&#38543;&#30528;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#21453;&#39304;&#24341;&#36215;&#20102;&#24191;&#27867;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#30340;&#23454;&#29992;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#23436;&#25972;PDF&#25552;&#20379;&#35780;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#21453;&#39304;&#30340;&#36136;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;15&#26412;Nature&#31867;&#26399;&#21002;&#65288;&#24635;&#20849;3096&#31687;&#35770;&#25991;&#65289;&#21644;ICLR m &#19978;&#23450;&#37327;&#27604;&#36739;&#20102;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#19982;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SEA&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#23454;&#29616;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#25805;&#20316;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01777</link><description>&lt;p&gt;
&#37319;&#29992;&#20272;&#35745;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;&#31232;&#30095;&#32447;&#24615;&#27880;&#24847;&#21147;&#65288;SEA&#65289;
&lt;/p&gt;
&lt;p&gt;
SEA: Sparse Linear Attention with Estimated Attention Mask. (arXiv:2310.01777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SEA&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#23454;&#29616;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#25805;&#20316;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;transformer&#26550;&#26500;&#22312;&#38656;&#35201;&#23545;&#24207;&#21015;&#20803;&#32032;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#24314;&#27169;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#31232;&#30095;&#21270;&#25110;&#32447;&#24615;&#36924;&#36817;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#38477;&#20302;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#20174;&#25945;&#24072;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#31232;&#30095;&#21644;&#32447;&#24615;&#26041;&#27861;&#22914;&#26524;&#19981;&#33021;&#20135;&#29983;&#23436;&#20840;&#20108;&#27425;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#36824;&#21487;&#33021;&#22833;&#21435;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEA&#65306;&#37319;&#29992;&#20272;&#35745;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;&#31232;&#30095;&#32447;&#24615;&#27880;&#24847;&#21147;&#26041;&#27861;&#12290;SEA&#36890;&#36807;&#22522;&#20110;&#26680;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#26041;&#27861;&#20272;&#35745;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#23545;&#23436;&#25972;&#27880;&#24847;&#21147;&#30697;&#38453;&#36827;&#34892;&#31232;&#30095;&#36924;&#36817;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture has made breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, transformers struggle with long sequences due to the quadratic complexity of the attention operation, and previous research has aimed to lower the complexity by sparsifying or linearly approximating the attention matrix. Yet, these approaches cannot straightforwardly distill knowledge from a teacher's attention matrix, and often require complete retraining from scratch. Furthermore, previous sparse and linear approaches may also lose interpretability if they do not produce full quadratic attention matrices. To address these challenges, we propose SEA: Sparse linear attention with an Estimated Attention mask. SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then creates a sparse approximation to the full attention matrix with a top-k se
&lt;/p&gt;</description></item><item><title>&#22534;&#26632;&#27880;&#24847;&#21147;&#20026;Transformers&#27169;&#22411;&#22788;&#29702;&#23618;&#27425;&#27169;&#24335;&#25552;&#20379;&#20102;&#33021;&#21147;&#65292;&#36890;&#36807;&#32467;&#21512;&#22534;&#26632;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#21644;&#35782;&#21035;&#20219;&#24847;&#28145;&#24230;&#30340;&#35821;&#27861;&#32467;&#26500;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#35299;&#26512;&#38590;&#24230;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.01749</link><description>&lt;p&gt;
&#22534;&#26632;&#27880;&#24847;&#21147;: &#25552;&#21319;Transformers&#23545;&#23618;&#27425;&#27169;&#24335;&#24314;&#27169;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns. (arXiv:2310.01749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01749
&lt;/p&gt;
&lt;p&gt;
&#22534;&#26632;&#27880;&#24847;&#21147;&#20026;Transformers&#27169;&#22411;&#22788;&#29702;&#23618;&#27425;&#27169;&#24335;&#25552;&#20379;&#20102;&#33021;&#21147;&#65292;&#36890;&#36807;&#32467;&#21512;&#22534;&#26632;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#21644;&#35782;&#21035;&#20219;&#24847;&#28145;&#24230;&#30340;&#35821;&#27861;&#32467;&#26500;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#35299;&#26512;&#38590;&#24230;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23588;&#20854;&#26159;&#32553;&#25918;&#28857;&#31215;&#27880;&#24847;&#21147;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24050;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#23545;&#20110;&#22788;&#29702;&#20219;&#24847;&#23884;&#22871;&#28145;&#24230;&#30340;&#23618;&#27425;&#27169;&#24335;&#27809;&#26377;&#26426;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#35782;&#21035;&#26576;&#20123;&#21477;&#27861;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22534;&#26632;&#27880;&#24847;&#21147;&#65306;&#19968;&#31181;&#32467;&#21512;&#20102;&#22534;&#26632;&#30340;&#27880;&#24847;&#21147;&#25805;&#20316;&#31526;&#65292;&#21463;&#21040;&#23427;&#20204;&#19982;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#65288;CFLs&#65289;&#30340;&#29702;&#35770;&#32852;&#31995;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22534;&#26632;&#27880;&#24847;&#21147;&#31867;&#20284;&#20110;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#20294;&#23427;&#20855;&#26377;&#19981;&#38656;&#35201;&#21477;&#27861;&#30417;&#30563;&#30340;&#35821;&#27861;&#28508;&#22312;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65306;&#19982;&#30830;&#23450;&#24615;&#19979;&#25512;&#33258;&#21160;&#26426;&#65288;PDAs&#65289;&#30456;&#20851;&#30340;&#19968;&#31181;&#65292;&#20197;&#21450;&#22522;&#20110;&#38750;&#30830;&#23450;&#24615;PDAs&#30340;&#19968;&#31181;&#65292;&#36825;&#20351;&#24471;transformers&#33021;&#22815;&#35782;&#21035;&#20219;&#24847;&#30340;CFLs&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#22534;&#26632;&#27880;&#24847;&#21147;&#30340;transformers&#22312;&#23398;&#20064;&#26631;&#20934;transformers&#38590;&#20197;&#24212;&#23545;&#30340;CFLs&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#22312;&#26368;&#22823;&#35299;&#26512;&#38590;&#24230;&#30340;CFL&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22534;&#26632;&#27880;&#24847;&#21147;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more
&lt;/p&gt;</description></item><item><title>Nugget&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36873;&#25321;&#30340;&#36755;&#20837;&#20196;&#29260;&#23376;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#65292;&#23558;&#35821;&#35328;&#20998;&#21106;&#20026;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;&#65292;&#20248;&#20110;&#30456;&#20851;&#26041;&#27861;&#65292;&#22312;&#35821;&#20041;&#27604;&#36739;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20801;&#35768;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;</title><link>http://arxiv.org/abs/2310.01732</link><description>&lt;p&gt;
Nugget: &#25991;&#26412;&#30340;&#31070;&#32463;&#32858;&#21512;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Nugget: Neural Agglomerative Embeddings of Text. (arXiv:2310.01732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01732
&lt;/p&gt;
&lt;p&gt;
Nugget&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36873;&#25321;&#30340;&#36755;&#20837;&#20196;&#29260;&#23376;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#65292;&#23558;&#35821;&#35328;&#20998;&#21106;&#20026;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;&#65292;&#20248;&#20110;&#30456;&#20851;&#26041;&#27861;&#65292;&#22312;&#35821;&#20041;&#27604;&#36739;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20801;&#35768;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#35821;&#35328;&#29702;&#35299;&#20013;&#65292;&#23884;&#20837;&#25991;&#26412;&#24207;&#21015;&#26159;&#19968;&#20010;&#24191;&#27867;&#38656;&#27714;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20391;&#37325;&#20110;&#24658;&#23450;&#22823;&#23567;&#30340;&#34920;&#31034;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#36890;&#24120;&#38543;&#36755;&#20837;&#30340;&#38271;&#24230;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Nugget&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#23558;&#35821;&#35328;&#32534;&#30721;&#20026;&#22522;&#20110;&#21160;&#24577;&#36873;&#25321;&#30340;&#36755;&#20837;&#20196;&#29260;&#23376;&#38598;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#33258;&#32534;&#30721;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#65292;&#23398;&#20064;&#36825;&#20123;nuggets&#65292;&#24182;&#30452;&#35266;&#22320;&#23558;&#35821;&#35328;&#20998;&#21106;&#25104;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Nugget&#22312;&#28041;&#21450;&#35821;&#20041;&#27604;&#36739;&#30340;&#20219;&#21153;&#20013;&#20248;&#20110;&#30456;&#20851;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#32039;&#20945;&#21333;&#20803;&#20801;&#35768;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#26032;&#30340;LM&#21487;&#33021;&#20250;&#23545;&#26356;&#22823;&#37327;&#30340;&#20869;&#23481;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding text sequences is a widespread requirement in modern language understanding. Existing approaches focus largely on constant-size representations. This is problematic, as the amount of information contained in text often varies with the length of the input. We propose a solution called Nugget, which encodes language into a representation based on a dynamically selected subset of input tokens. These nuggets are learned through tasks like autoencoding and machine translation, and intuitively segment language into meaningful units. We demonstrate Nugget outperforms related approaches in tasks involving semantic comparison. Finally, we illustrate these compact units allow for expanding the contextual window of a language model (LM), suggesting new future LMs that can condition on significantly larger amounts of content.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01717</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#30340;&#38598;&#25104;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#23558;&#21477;&#23376;&#30340;&#35789;&#21644;&#30701;&#35821;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#19981;&#20351;&#29992;&#35821;&#35328;&#23398;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#22120;&#25429;&#25417;&#21040;&#20102;&#35299;&#26512;&#32467;&#26500;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26641;&#24179;&#22343;&#8221;&#30340;&#27010;&#24565;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65307;&#36825;&#31181;&#38598;&#25104;-&#33976;&#39311;&#30340;&#36807;&#31243;&#26159;&#32531;&#35299;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#34920;&#29616;&#20986;&#20854;&#22312;&#19981;&#21516;&#38598;&#25104;&#32452;&#20214;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of "tree averaging," based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#29983;&#25104;&#30340;&#35299;&#37322;&#33021;&#26174;&#33879;&#25552;&#39640;&#21307;&#29983;&#23545;&#35786;&#26029;&#30340;&#19968;&#33268;&#29575;&#65292;&#24182;&#25351;&#20986;&#28508;&#22312;&#30340;LLM&#36755;&#20986;&#38169;&#35823;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#30830;&#20445;&#24739;&#32773;&#23433;&#20840;&#21644;&#26368;&#20339;&#20020;&#24202;&#25928;&#29992;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.01708</link><description>&lt;p&gt;
&#35299;&#35835;&#35786;&#26029;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20020;&#24202;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making. (arXiv:2310.01708v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01708
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#29983;&#25104;&#30340;&#35299;&#37322;&#33021;&#26174;&#33879;&#25552;&#39640;&#21307;&#29983;&#23545;&#35786;&#26029;&#30340;&#19968;&#33268;&#29575;&#65292;&#24182;&#25351;&#20986;&#28508;&#22312;&#30340;LLM&#36755;&#20986;&#38169;&#35823;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#30830;&#20445;&#24739;&#32773;&#23433;&#20840;&#21644;&#26368;&#20339;&#20020;&#24202;&#25928;&#29992;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#21033;&#29992;&#22522;&#20110;&#35777;&#25454;&#30340;&#30693;&#35782;&#21644;&#24739;&#32773;&#25968;&#25454;&#25552;&#20379;&#23454;&#26102;&#24314;&#35758;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#29983;&#25104;&#21307;&#23398;&#20915;&#31574;&#30340;&#32431;&#25991;&#26412;&#35299;&#37322;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#22522;&#20110;&#24739;&#32773;&#30151;&#29366;&#30340;&#35786;&#26029;&#35299;&#37322;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#19977;&#21517;&#26377;&#32463;&#39564;&#30340;&#21307;&#29983;&#35780;&#20272;&#20102;LLM&#29983;&#25104;&#30340;&#20851;&#20110;&#24739;&#32773;&#30151;&#29366;&#19982;&#21307;&#29983;&#21450;&#27169;&#22411;&#20998;&#37197;&#30340;&#35786;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#35299;&#37322;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#29983;&#23545;&#32473;&#23450;&#35786;&#26029;&#30340;&#19968;&#33268;&#29575;&#65292;&#24182;&#20984;&#26174;&#20102;LLM&#36755;&#20986;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#65292;&#33539;&#22260;&#20174;5%&#21040;30%&#19981;&#31561;&#12290;&#26412;&#30740;&#31350;&#20984;&#26174;&#20102;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#38656;&#35201;&#20180;&#32454;&#25972;&#21512;&#21644;&#35780;&#20272;&#20197;&#30830;&#20445;&#24739;&#32773;&#23433;&#20840;&#21644;&#26368;&#20339;&#20020;&#24202;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and patient data to offer real-time recommendations, with Large Language Models (LLMs) emerging as a promising tool to generate plain-text explanations for medical decisions. This study explores the effectiveness and reliability of LLMs in generating explanations for diagnoses based on patient complaints. Three experienced doctors evaluated LLM-generated explanations of the connection between patient complaints and doctor and model-assigned diagnoses across several stages. Experimental results demonstrated that LLM explanations significantly increased doctors' agreement rates with given diagnoses and highlighted potential errors in LLM outputs, ranging from 5% to 30%. The study underscores the potential and challenges of LLMs in healthcare and emphasizes the need for careful integration and evaluation to ensure patient safety and optimal clinical utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#25130;&#26029;&#37319;&#26679;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;softmax&#29942;&#39048;&#30340;&#26356;&#31934;&#30830;&#30340;&#37319;&#26679;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.01693</link><description>&lt;p&gt;
&#35299;&#24320;&#31070;&#32463;&#25991;&#26412;&#36864;&#21270;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
Closing the Curious Case of Neural Text Degeneration. (arXiv:2310.01693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#25130;&#26029;&#37319;&#26679;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;softmax&#29942;&#39048;&#30340;&#26356;&#31934;&#30830;&#30340;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35821;&#35328;&#29983;&#25104;&#20013;&#26222;&#36941;&#20351;&#29992;&#65292;&#20294;&#20026;&#20309;&#20687;&#26680;&#37319;&#26679;&#36825;&#26679;&#30340;&#25130;&#26029;&#37319;&#26679;&#21551;&#21457;&#24335;&#26041;&#27861;&#22914;&#27492;&#26377;&#25928;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#25130;&#26029;&#26041;&#27861;&#65288;&#20002;&#24323;&#26576;&#20123;&#27010;&#29575;&#38408;&#20540;&#20197;&#19979;&#30340;&#35760;&#21495;&#65292;&#26368;&#24120;&#35265;&#30340;&#25130;&#26029;&#31867;&#22411;&#65289;&#21487;&#20197;&#20445;&#35777;&#25152;&#26377;&#37319;&#26679;&#20986;&#26469;&#30340;&#35760;&#21495;&#37117;&#26377;&#38750;&#38646;&#30495;&#23454;&#27010;&#29575;&#65292;&#25552;&#20379;&#20102;&#23545;&#25130;&#26029;&#37319;&#26679;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38408;&#20540;&#21482;&#26159;&#31895;&#30053;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24517;&#28982;&#20063;&#20002;&#24323;&#20102;&#19968;&#20123;&#20855;&#26377;&#38750;&#38646;&#30495;&#23454;&#27010;&#29575;&#30340;&#35760;&#21495;&#12290;&#20026;&#20102;&#36861;&#27714;&#26356;&#31934;&#30830;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24050;&#30693;&#30340;&#27169;&#22411;&#38169;&#35823;&#28304;&#8212;&#8212;softmax&#29942;&#39048;&#65292;&#35777;&#26126;&#26576;&#20123;&#35760;&#21495;&#20855;&#26377;&#38750;&#38646;&#30495;&#23454;&#27010;&#29575;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#38408;&#20540;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#39564;&#24615;&#30340;&#25130;&#26029;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#23637;&#31034;&#36825;&#31181;&#31639;&#27861;&#30340;&#21069;&#26399;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#38408;&#20540;&#30340;&#23545;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28304;&#25552;&#31034;&#32534;&#30721;&#21040;&#30456;&#23545;&#31354;&#38388;&#20013;&#65292;&#24182;&#25628;&#32034;&#30456;&#24212;&#30340;&#30446;&#26631;&#25552;&#31034;&#65292;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20102;&#20219;&#21153;&#35821;&#20041;&#30340;&#27867;&#21270;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01691</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#27867;&#21270;&#20219;&#21153;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models. (arXiv:2310.01691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01691
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28304;&#25552;&#31034;&#32534;&#30721;&#21040;&#30456;&#23545;&#31354;&#38388;&#20013;&#65292;&#24182;&#25628;&#32034;&#30456;&#24212;&#30340;&#30446;&#26631;&#25552;&#31034;&#65292;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20102;&#20219;&#21153;&#35821;&#20041;&#30340;&#27867;&#21270;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#36890;&#36807;&#35843;&#25972;&#25552;&#31034;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#65292;&#29305;&#21035;&#26159;&#36830;&#32493;&#25552;&#31034;&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#26041;&#27861;&#65292;&#20854;&#20013;&#28304;&#25552;&#31034;&#34987;&#32534;&#30721;&#21040;&#30456;&#23545;&#31354;&#38388;&#20013;&#65292;&#24182;&#25628;&#32034;&#30456;&#24212;&#30340;&#30446;&#26631;&#25552;&#31034;&#20197;&#23558;&#20854;&#20256;&#36882;&#21040;&#30446;&#26631;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#36830;&#32493;&#25552;&#31034;&#20013;&#30340;&#8220;&#20219;&#21153;&#35821;&#20041;&#8221;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#26469;&#33258;&#22810;&#20010;&#28304;&#27169;&#22411;&#30340;&#8220;&#20219;&#21153;&#35821;&#20041;&#8221;&#32467;&#21512;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20256;&#36882;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks. However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge. In this work, we propose a zero-shot continuous prompt transfer method, where source prompts are encoded into relative space and the corresponding target prompts are searched for transferring to target models. Experimental results confirm the effectiveness of our method, showing that 'task semantics' in continuous prompts can be generalized across various language models. Moreover, we find that combining 'task semantics' from multiple source models can further enhance the generalizability of transfer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDAR&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#35828;&#35805;&#20154;&#20808;&#34892;&#25252;&#29702;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#65292;&#24182;&#36890;&#36807;&#23558;&#26412;&#22320;&#36755;&#20986;&#21512;&#24182;&#26469;&#33719;&#24471;&#26368;&#32456;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#36817;&#36317;&#31163;&#21644;&#36828;&#36317;&#31163;&#35821;&#38899;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01688</link><description>&lt;p&gt;
&#19968;&#31181;&#27169;&#22411;&#32479;&#27835;&#20840;&#29699;&#65311;&#26397;&#30528;&#31471;&#21040;&#31471;&#32852;&#21512;&#35828;&#35805;&#20154;&#20808;&#34892;&#25252;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#36808;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
One model to rule them all ? Towards End-to-End Joint Speaker Diarization and Speech Recognition. (arXiv:2310.01688v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDAR&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#35828;&#35805;&#20154;&#20808;&#34892;&#25252;&#29702;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#65292;&#24182;&#36890;&#36807;&#23558;&#26412;&#22320;&#36755;&#20986;&#21512;&#24182;&#26469;&#33719;&#24471;&#26368;&#32456;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#36817;&#36317;&#31163;&#21644;&#36828;&#36317;&#31163;&#35821;&#38899;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDAR&#65288;&#28369;&#21160;&#31383;&#21475;&#20808;&#34892;&#25252;&#29702;&#22686;&#24378;&#35782;&#21035;&#65289;&#30340;&#32852;&#21512;&#35828;&#35805;&#20154;&#20808;&#34892;&#25252;&#29702;&#65288;SD&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;SLIDAR&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#8220;&#35841;&#22312;&#20160;&#20040;&#26102;&#20505;&#35828;&#20102;&#20160;&#20040;&#8221;&#30340;&#38382;&#39064;&#12290;SLIDAR&#37319;&#29992;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#65292;&#21253;&#25324;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20808;&#34892;&#25252;&#29702;&#22686;&#24378;&#35821;&#38899;&#36716;&#24405;&#65288;E2E DAST&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20026;&#27599;&#20010;&#31383;&#21475;&#25552;&#20379;&#26412;&#22320;&#30340;&#36716;&#24405;&#12289;&#20808;&#34892;&#25252;&#29702;&#21644;&#35828;&#35805;&#20154;&#23884;&#20837;&#12290;E2E DAST&#27169;&#22411;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#25216;&#26415;&#65292;&#22914;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#21644;&#8220;Whisper-style&#8221;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#35828;&#35805;&#20154;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#26469;&#21512;&#24182;&#26412;&#22320;&#36755;&#20986;&#65292;&#24471;&#21040;&#26368;&#32456;&#30340;SD+ASR&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel framework for joint speaker diarization (SD) and automatic speech recognition (ASR), named SLIDAR (sliding-window diarization-augmented recognition). SLIDAR can process arbitrary length inputs and can handle any number of speakers, effectively solving ``who spoke what, when'' concurrently. SLIDAR leverages a sliding window approach and consists of an end-to-end diarization-augmented speech transcription (E2E DAST) model which provides, locally, for each window: transcripts, diarization and speaker embeddings. The E2E DAST model is based on an encoder-decoder architecture and leverages recent techniques such as serialized output training and ``Whisper-style" prompting. The local outputs are then combined to get the final SD+ASR result by clustering the speaker embeddings to get global speaker identities. Experiments performed on monaural recordings from the AMI corpus confirm the effectiveness of the method in both close-talk and far-field speech scenarios.
&lt;/p&gt;</description></item><item><title>VAL&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31526;&#21495;&#38598;&#25104;&#30340;&#29702;&#24565;&#65292;&#23454;&#29616;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#30340;&#33719;&#21462;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#35299;&#37322;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#22312;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#23454;&#39564;&#34920;&#26126;&#65292;VAL&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.01627</link><description>&lt;p&gt;
VAL&#65306;&#24102;&#26377;GPT&#23545;&#35805;&#35299;&#26512;&#30340;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VAL: Interactive Task Learning with GPT Dialog Parsing. (arXiv:2310.01627v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01627
&lt;/p&gt;
&lt;p&gt;
VAL&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31526;&#21495;&#38598;&#25104;&#30340;&#29702;&#24565;&#65292;&#23454;&#29616;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#30340;&#33719;&#21462;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#35299;&#37322;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#22312;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#23454;&#39564;&#34920;&#26126;&#65292;VAL&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#38656;&#35201;&#25968;&#30334;&#19975;&#20010;&#26679;&#26412;&#26469;&#29983;&#25104;&#38745;&#24577;&#30340;&#40657;&#31665;&#27169;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#65288;ITL&#65289;&#24378;&#35843;&#20174;&#20154;&#31867;&#25552;&#20379;&#30340;&#26377;&#38480;&#25351;&#20196;&#20013;&#36880;&#27493;&#33719;&#24471;&#30693;&#35782;&#65292;&#36825;&#20123;&#25351;&#20196;&#20197;&#33258;&#28982;&#35821;&#35328;&#31561;&#24418;&#24335;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;ITL&#31995;&#32479;&#24448;&#24448;&#21463;&#21040;&#33030;&#24369;&#12289;&#23481;&#26131;&#20986;&#38169;&#30340;&#35821;&#35328;&#35299;&#26512;&#30340;&#22256;&#25200;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#33030;&#24369;&#24615;&#26377;&#19968;&#23450;&#30340;&#25269;&#25239;&#33021;&#21147;&#65292;&#20294;&#19981;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#65292;&#20063;&#26080;&#27861;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VAL&#65292;&#19968;&#31181;&#20855;&#26377;&#26032;&#30340;LLM/&#31526;&#21495;&#38598;&#25104;&#29702;&#24565;&#30340;ITL&#31995;&#32479;&#12290;&#36890;&#36807;&#20165;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20351;&#29992;LLMs&#65288;&#20363;&#22914;&#35859;&#35789;&#21644;&#21442;&#25968;&#36873;&#25321;&#65289;&#65292;&#22312;&#31639;&#27861;&#26694;&#26550;&#20869;&#65292;VAL&#21033;&#29992;LLMs&#30340;&#20248;&#21183;&#65292;&#25903;&#25345;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#20132;&#20114;&#24335;&#23398;&#20064;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25903;&#25345;&#25191;&#34892;&#26032;&#20219;&#21153;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#29992;&#25143;&#19982;VAL&#30340;&#20132;&#20114;&#65292;&#21457;&#29616;&#22823;&#37096;&#20998;&#29992;&#25143;&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning often requires millions of examples to produce static, black-box models. In contrast, interactive task learning (ITL) emphasizes incremental knowledge acquisition from limited instruction provided by humans in modalities such as natural language. However, in practice, ITL systems often suffers from brittle, error-prone language parsing. Large language models (LLMs) are resistant to brittleness but are not interpretable and cannot learn incrementally. We present VAL, an ITL system with a new philosophy for LLM/symbolic integration. By using LLMs only for specific tasks -- such as predicate and argument selection -- within an algorithmic framework, VAL reaps the benefits of LLMs to support interactive learning of hierarchical task knowledge from natural language. Acquired knowledge is human interpretable and generalizes to support execution of novel tasks without additional training. We studied users' interactions with VAL in a video game setting, finding that most
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;K-12&#25945;&#32946;&#20013;&#25945;&#25480;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#23383;&#23398;&#20064;&#29615;&#22659;&#65292;&#24182;&#35752;&#35770;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#25903;&#25345;&#12289;&#35299;&#37322;&#24615;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#21162;&#21147;&#25913;&#36827;&#29616;&#26377;&#24037;&#20855;&#12289;&#24320;&#21457;&#26032;&#24037;&#20855;&#65292;&#24182;&#25506;&#32034;&#26356;&#26377;&#25928;&#21644;&#21253;&#23481;&#24615;&#30340;NLP&#25972;&#21512;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.01603</link><description>&lt;p&gt;
K-12&#25945;&#32946;&#20013;&#25945;&#25480;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#23383;&#23398;&#20064;&#29615;&#22659;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Digital Learning Environments for Teaching Natural Language Processing in K-12 Education. (arXiv:2310.01603v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01603
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;K-12&#25945;&#32946;&#20013;&#25945;&#25480;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#23383;&#23398;&#20064;&#29615;&#22659;&#65292;&#24182;&#35752;&#35770;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#25903;&#25345;&#12289;&#35299;&#37322;&#24615;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#21162;&#21147;&#25913;&#36827;&#29616;&#26377;&#24037;&#20855;&#12289;&#24320;&#21457;&#26032;&#24037;&#20855;&#65292;&#24182;&#25506;&#32034;&#26356;&#26377;&#25928;&#21644;&#21253;&#23481;&#24615;&#30340;NLP&#25972;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#24050;&#25104;&#20026;K-12&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38543;&#30528;&#23401;&#23376;&#20204;&#22312;NLP&#39537;&#21160;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#25104;&#38271;&#65292;&#21521;&#20182;&#20204;&#20171;&#32461;NLP&#27010;&#24565;&#23545;&#22521;&#20859;&#20182;&#20204;&#23545;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#35328;&#29983;&#25104;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#30340;&#20262;&#29702;&#38382;&#39064;&#30340;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;K-12&#25945;&#32946;&#20013;&#25945;&#25480;NLP&#30340;&#25968;&#23383;&#23398;&#20064;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25506;&#35752;&#20102;&#29616;&#26377;&#30340;&#25968;&#23383;&#23398;&#20064;&#24037;&#20855;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#29305;&#23450;&#30340;NLP&#20219;&#21153;&#21644;&#27969;&#31243;&#65292;&#24182;&#35843;&#26597;&#20102;&#23427;&#20204;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#35780;&#20272;&#32467;&#26524;&#12290;&#36890;&#36807;&#26816;&#26597;&#36825;&#20123;&#24037;&#20855;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#26412;&#25991;&#32508;&#36848;&#20026;&#25105;&#20204;&#20102;&#35299;&#24403;&#21069;K-12&#25945;&#32946;&#20013;NLP&#23398;&#20064;&#24037;&#20855;&#30340;&#29616;&#29366;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#23427;&#26088;&#22312;&#24341;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#21162;&#21147;&#65292;&#20197;&#25913;&#36827;&#29616;&#26377;&#24037;&#20855;&#12289;&#24320;&#21457;&#26032;&#24037;&#20855;&#65292;&#24182;&#25506;&#32034;&#26356;&#26377;&#25928;&#21644;&#21253;&#23481;&#24615;&#30340;NLP&#25972;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) plays a significant role in our daily lives and has become an essential part of Artificial Intelligence (AI) education in K-12. As children grow up with NLP-powered applications, it is crucial to introduce NLP concepts to them, fostering their understanding of language processing, language generation, and ethical implications of AI and NLP. This paper presents a comprehensive review of digital learning environments for teaching NLP in K-12. Specifically, it explores existing digital learning tools, discusses how they support specific NLP tasks and procedures, and investigates their explainability and evaluation results in educational contexts. By examining the strengths and limitations of these tools, this literature review sheds light on the current state of NLP learning tools in K-12 education. It aims to guide future research efforts to refine existing tools, develop new ones, and explore more effective and inclusive strategies for integrating NLP i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#20316;&#32773;&#36523;&#20221;&#35782;&#21035;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#36890;&#36807;&#28151;&#28102;&#20889;&#20316;&#39118;&#26684;&#26469;&#20445;&#25252;&#20844;&#24320;&#20132;&#27969;&#20013;&#30340;&#21311;&#21517;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01568</link><description>&lt;p&gt;
&#38450;&#24481;&#20316;&#32773;&#36523;&#20221;&#35782;&#21035;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Authorship Identification Attacks. (arXiv:2310.01568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#20316;&#32773;&#36523;&#20221;&#35782;&#21035;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#36890;&#36807;&#28151;&#28102;&#20889;&#20316;&#39118;&#26684;&#26469;&#20445;&#25252;&#20844;&#24320;&#20132;&#27969;&#20013;&#30340;&#21311;&#21517;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#36523;&#20221;&#35782;&#21035;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#25512;&#26029;&#26410;&#32626;&#21517;&#25991;&#26723;&#30340;&#20316;&#32773;&#36523;&#20221;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#20180;&#32454;&#21024;&#38500;&#25935;&#24863;&#20010;&#20154;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#20010;&#20154;&#36890;&#36807;&#20854;&#20070;&#38754;&#20869;&#23481;&#30041;&#19979;&#20102;&#25345;&#20037;&#30340;&#25968;&#23383;&#36275;&#36857;&#65292;&#19981;&#35770;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#21457;&#24067;&#65292;&#20445;&#23384;&#22312;&#38599;&#20027;&#30340;&#35745;&#31639;&#26426;&#19978;&#65292;&#36824;&#26159;&#20854;&#20182;&#22320;&#26041;&#12290;&#24403;&#20010;&#20154;&#38656;&#35201;&#20844;&#24320;&#20132;&#27969;&#20294;&#24076;&#26395;&#20445;&#25345;&#21311;&#21517;&#26102;&#65292;&#20960;&#20046;&#27809;&#26377;&#20160;&#20040;&#21487;&#29992;&#30340;&#26041;&#27861;&#26469;&#20445;&#25252;&#20182;&#20204;&#20813;&#21463;&#19981;&#24819;&#35201;&#30340;&#20316;&#32773;&#36523;&#20221;&#35782;&#21035;&#12290;&#36825;&#31181;&#21069;&#25152;&#26410;&#26377;&#30340;&#38544;&#31169;&#23041;&#32961;&#22312;&#20687;&#25581;&#21457;&#19985;&#38395;&#30340;&#24773;&#20917;&#19979;&#26159;&#26174;&#32780;&#26131;&#35265;&#30340;&#12290;&#38024;&#23545;&#20316;&#32773;&#36523;&#20221;&#35782;&#21035;&#25915;&#20987;&#30340;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#20027;&#35201;&#26088;&#22312;&#28151;&#28102;&#20010;&#20154;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#20174;&#32780;&#20351;&#20854;&#19982;&#20854;&#20808;&#21069;&#30340;&#20889;&#20316;&#19981;&#21487;&#20851;&#32852;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#30340;&#24847;&#20041;&#21644;&#35821;&#27861;&#23436;&#25972;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#36807;&#21435;&#20004;&#20010;&#19990;&#32426;&#20197;&#26469;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#30340;&#20840;&#38754;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Authorship identification has proven unsettlingly effective in inferring the identity of the author of an unsigned document, even when sensitive personal information has been carefully omitted. In the digital era, individuals leave a lasting digital footprint through their written content, whether it is posted on social media, stored on their employer's computers, or located elsewhere. When individuals need to communicate publicly yet wish to remain anonymous, there is little available to protect them from unwanted authorship identification. This unprecedented threat to privacy is evident in scenarios such as whistle-blowing. Proposed defenses against authorship identification attacks primarily aim to obfuscate one's writing style, thereby making it unlinkable to their pre-existing writing, while concurrently preserving the original meaning and grammatical integrity. The presented work offers a comprehensive review of the advancements in this research area spanning over the past two de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01558</link><description>&lt;p&gt;
&#20351;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Retrieval-Augmented Language Models Robust to Irrelevant Context. (arXiv:2310.01558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#26377;&#26395;&#20135;&#29983;&#20934;&#30830;&#12289;&#39640;&#25928;&#21644;&#26368;&#26032;&#30340;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#12290;RALMs&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#22312;&#30456;&#20851;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#19981;&#30456;&#20851;&#26102;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#22312;&#22810;&#36339;&#25512;&#29702;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#19981;&#30456;&#20851;&#35777;&#25454;&#30340;&#35823;&#29992;&#20250;&#23548;&#33268;&#36830;&#38145;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26816;&#32034;&#22686;&#24378;&#26377;&#26102;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20116;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#22522;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#25551;&#36848;&#20102;&#26816;&#32034;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#32447;&#65292;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#31579;&#36873;&#20986;&#19981;&#28041;&#21450;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#26816;&#32034;&#27573;&#33853;&#12290;&#36825;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#20195;&#20215;&#26159;&#33293;&#24323;&#20102;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding releva
&lt;/p&gt;</description></item><item><title>LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.01469</link><description>&lt;p&gt;
LLM&#35854;&#35328;: &#24187;&#35273;&#19981;&#26159;&#28431;&#27934;&#65292;&#32780;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01469
&lt;/p&gt;
&lt;p&gt;
LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21253;&#25324;GPT-3.5&#12289;LLaMA&#21644;PaLM&#65292;&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;LLM&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#32780;&#19981;&#34987;&#23519;&#35273;&#12290;&#24187;&#35273;&#23384;&#22312;&#30340;&#21407;&#22240;&#21644;&#26222;&#36941;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#12290;&#36825;&#20010;&#29616;&#35937;&#36843;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24187;&#35273;&#21487;&#33021;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#19988;&#23427;&#19982;&#24120;&#35268;&#30340;&#23545;&#25239;&#26679;&#26412;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#20316;&#20026;LLM&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#23545;&#25239;&#30340;&#26041;&#24335;&#23558;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#24418;&#24335;&#21270;&#20026;&#24187;&#35273;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#34987;&#25915;&#20987;&#30340;&#23545;&#25239;&#25552;&#31034;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01468</link><description>&lt;p&gt;
&#23454;&#20307;&#25512;&#26029;&#31454;&#25216;&#22330;&#65306;&#25506;&#31350;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#26126;&#30830;&#25552;&#38382;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#21547;&#31946;&#19981;&#28165;&#30340;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34892;&#20026;&#38590;&#20197;&#39044;&#27979;&#24182;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#26377;&#25928;&#35299;&#20915;&#27495;&#20041;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#33021;&#21147;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#36827;&#34892;&#22797;&#26434;&#30340;&#29702;&#35299;&#12289;&#29366;&#24577;&#36319;&#36394;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#31181;&#33021;&#21147;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35780;&#20272;&#20102;LLMs&#25512;&#26029;&#33258;&#24049;&#19981;&#30693;&#36947;&#20294;&#34987;&#27861;&#23448;&#25581;&#31034;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#8220;&#23454;&#20307;&#25512;&#26029;&#28216;&#25103;&#8221;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#22823;&#30340;LLMs...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
&lt;/p&gt;</description></item><item><title>FedBPT&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#32852;&#37030;&#24335;&#40657;&#30418;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.01467</link><description>&lt;p&gt;
FedBPT: &#39640;&#25928;&#30340;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24335;&#40657;&#30418;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models. (arXiv:2310.01467v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01467
&lt;/p&gt;
&lt;p&gt;
FedBPT&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#32852;&#37030;&#24335;&#40657;&#30418;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#31361;&#30772;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#21463;&#30410;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22312;&#29305;&#23450;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25968;&#25454;&#36866;&#24212;&#36807;&#31243;&#23384;&#22312;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#35774;&#22791;&#39547;&#30041;&#25968;&#25454;&#26102;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20801;&#35768;&#22312;&#27809;&#26377;&#38598;&#20013;&#24335;&#25968;&#25454;&#25910;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23558;FL&#24212;&#29992;&#20110;&#24494;&#35843;PLMs&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#21463;&#38480;&#30340;&#27169;&#22411;&#21442;&#25968;&#35775;&#38382;&#12289;&#39640;&#35745;&#31639;&#35201;&#27714;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedBPT&#30340;&#32852;&#37030;&#24335;&#40657;&#30418;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;FedBPT&#26080;&#38656;&#23458;&#25143;&#31471;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#35757;&#32451;&#26368;&#20248;&#25552;&#31034;&#21644;&#21033;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLM) have revolutionized the NLP landscape, achieving stellar performances across diverse tasks. These models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. However, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. Federated learning (FL) provides a solution, allowing collaborative model fine-tuning without centralized data collection. However, applying FL to finetune PLMs is hampered by challenges, including restricted model parameter access, high computational requirements, and communication overheads. This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed to address these challenges. FedBPT does not require the clients to access the model parameters. By focusing on training optimal prompts and utilizing gradient-free optimization methods, FedBPT reduces the nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NarrativePlay&#30340;&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#34394;&#26500;&#29615;&#22659;&#20013;&#25198;&#28436;&#34394;&#26500;&#35282;&#33394;&#19982;&#20854;&#20182;&#35282;&#33394;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31867;&#20154;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35270;&#35273;&#23637;&#31034;&#21644;&#35282;&#33394;&#30340;&#35328;&#35848;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#36890;&#36807;&#22312;&#20390;&#25506;&#21644;&#20882;&#38505;&#25925;&#20107;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#35201;&#20040;&#25506;&#32034;&#19990;&#30028;&#35201;&#20040;&#25552;&#39640;&#19982;&#21465;&#20107;&#35282;&#33394;&#30340;&#20146;&#21644;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01459</link><description>&lt;p&gt;
NarrativePlay: &#20132;&#20114;&#24335;&#21465;&#20107;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
NarrativePlay: Interactive Narrative Understanding. (arXiv:2310.01459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NarrativePlay&#30340;&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#34394;&#26500;&#29615;&#22659;&#20013;&#25198;&#28436;&#34394;&#26500;&#35282;&#33394;&#19982;&#20854;&#20182;&#35282;&#33394;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31867;&#20154;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35270;&#35273;&#23637;&#31034;&#21644;&#35282;&#33394;&#30340;&#35328;&#35848;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#36890;&#36807;&#22312;&#20390;&#25506;&#21644;&#20882;&#38505;&#25925;&#20107;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#35201;&#20040;&#25506;&#32034;&#19990;&#30028;&#35201;&#20040;&#25552;&#39640;&#19982;&#21465;&#20107;&#35282;&#33394;&#30340;&#20146;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NarrativePlay&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#34394;&#26500;&#29615;&#22659;&#20013;&#25198;&#28436;&#34394;&#26500;&#35282;&#33394;&#65292;&#24182;&#19982;&#20854;&#20182;&#35282;&#33394;&#36827;&#34892;&#20114;&#21160;&#65292;&#22914;&#23567;&#35828;&#31561;&#25925;&#20107;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#20174;&#21465;&#20107;&#20013;&#25552;&#21462;&#30340;&#20010;&#24615;&#29305;&#24449;&#29983;&#25104;&#31867;&#20154;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#33258;&#21160;&#29983;&#25104;&#30340;&#21465;&#20107;&#35774;&#32622;&#30340;&#35270;&#35273;&#23637;&#31034;&#12289;&#35282;&#33394;&#24418;&#35937;&#20197;&#21450;&#35282;&#33394;&#30340;&#35328;&#35848;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25682;&#24323;&#20102;&#39044;&#23450;&#20041;&#30340;&#27801;&#30418;&#65292;&#32780;&#26159;&#20174;&#29992;&#25143;&#36873;&#25321;&#30340;&#35282;&#33394;&#30340;&#35270;&#35282;&#65292;&#20851;&#27880;&#20174;&#21465;&#20107;&#20013;&#25552;&#21462;&#30340;&#20027;&#35201;&#25925;&#20107;&#24773;&#33410;&#12290;NarrativePlay&#24050;&#22312;&#20390;&#25506;&#21644;&#20882;&#38505;&#25925;&#20107;&#20004;&#31181;&#31867;&#22411;&#30340;&#21465;&#20107;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;&#36825;&#20123;&#21465;&#20107;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#35201;&#20040;&#25506;&#32034;&#19990;&#30028;&#35201;&#20040;&#25552;&#39640;&#19982;&#21465;&#20107;&#35282;&#33394;&#30340;&#20146;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce NarrativePlay, a novel system that allows users to role-play a fictional character and interact with other characters in narratives such as novels in an immersive environment. We leverage Large Language Models (LLMs) to generate human-like responses, guided by personality traits extracted from narratives. The system incorporates auto-generated visual display of narrative settings, character portraits, and character speech, greatly enhancing user experience. Our approach eschews predefined sandboxes, focusing instead on main storyline events extracted from narratives from the perspective of a user-selected character. NarrativePlay has been evaluated on two types of narratives, detective and adventure stories, where users can either explore the world or improve their favorability with the narrative characters through conversations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25915;&#20987;&#26080;&#20851;&#38450;&#24481;&#31574;&#30053;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#20174;&#32780;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.01452</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21270;&#28508;&#22312;&#34920;&#31034;&#26469;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;
&lt;/p&gt;
&lt;p&gt;
Fooling the Textual Fooler via Randomizing Latent Representations. (arXiv:2310.01452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25915;&#20987;&#26080;&#20851;&#38450;&#24481;&#31574;&#30053;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#20174;&#32780;&#36855;&#24785;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#24494;&#23567;&#22320;&#25913;&#21464;&#36755;&#20837;&#20197;&#23548;&#33268;&#27169;&#22411;&#30340;&#38169;&#35823;&#34892;&#20026;&#12290;&#20854;&#20013;&#65292;&#25932;&#23545;&#35789;&#32423;&#25200;&#21160;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#36825;&#20123;&#25915;&#20987;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#36215;&#20316;&#29992;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#32467;&#26500;&#25110;&#21442;&#25968;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#34892;&#25915;&#20987;&#65292;&#23545;&#25163;&#22810;&#27425;&#26597;&#35810;&#21463;&#23475;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#36755;&#20837;&#25991;&#26412;&#20013;&#26368;&#37325;&#35201;&#30340;&#21333;&#35789;&#65292;&#24182;&#29992;&#23427;&#20204;&#23545;&#24212;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#36825;&#20123;&#21333;&#35789;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#25915;&#20987;&#26080;&#20851;&#30340;&#38450;&#24481;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22256;&#24785;&#22522;&#20110;&#26597;&#35810;&#30340;&#40657;&#30418;&#25915;&#20987;&#20013;&#20135;&#29983;&#25932;&#23545;&#31034;&#20363;&#30340;&#36807;&#31243;&#65307;&#21363;&#24858;&#24324;&#25991;&#26412;&#24858;&#24324;&#32773;&#12290;&#36825;&#31181;&#38450;&#24481;&#21517;&#20026;AdvFooler&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#36755;&#20837;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite outstanding performance in a variety of NLP tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Among these attacks, adversarial word-level perturbations are well-studied and effective attack strategies. Since these attacks work in black-box settings, they do not require access to the model architecture or model parameters and thus can be detrimental to existing NLP applications. To perform an attack, the adversary queries the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.01448</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#20803;&#35821;&#20041;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
Meta Semantic Template for Evaluation of Large Language Models. (arXiv:2310.01448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#35821;&#35328;&#30340;&#35821;&#20041;&#65292;&#36824;&#26159;&#20165;&#20165;&#35760;&#20303;&#35757;&#32451;&#25968;&#25454;&#65311;&#26368;&#36817;&#23545;LLM&#28508;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#25285;&#24551;&#24341;&#36215;&#20102;&#31038;&#21306;&#23545;LLM&#35780;&#20272;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSTemp&#65292;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;LLM&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;MSTemp&#30340;&#26680;&#24515;&#19981;&#26159;&#30452;&#25509;&#22312;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#26159;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#20316;&#20026;&#31181;&#23376;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#21477;&#23376;&#65292;MSTemp&#21033;&#29992;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#35821;&#20041;&#12290;&#36825;&#20123;&#26032;&#26679;&#26412;&#34987;&#31216;&#20026;&#21407;&#21477;&#23376;&#30340;&#35821;&#20041;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;MSTemp&#36890;&#36807;&#21477;&#23376;&#35299;&#26512;&#21644;&#38543;&#26426;&#26367;&#25442;&#35789;&#35821;&#26469;&#29983;&#25104;&#35780;&#20272;&#26679;&#26412;&#12290;MSTemp&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#12289;&#21160;&#24577;&#21644;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;MSTemp-
&lt;/p&gt;
&lt;p&gt;
Do large language models (LLMs) genuinely understand the semantics of the language, or just memorize the training data? The recent concern on potential data contamination of LLMs has raised awareness of the community to conduct research on LLMs evaluation. In this paper, we propose MSTemp, an approach that creates meta semantic templates to evaluate the semantic understanding ability of LLMs. The core of MSTemp is not to perform evaluation directly on existing benchmark datasets, but to generate new out-of-distribution (OOD) evaluation sets using existing datasets as seeds. Specifically, for a given sentence, MSTemp leverages another language model to generate new samples while preserving its semantics. The new samples are called semantic templates to the original sentence. Then, MSTemp generates evaluation samples via sentence parsing and random word replacement on the semantic templates. MSTemp is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTemp-
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#26681;&#25454;&#38382;&#39064;&#38590;&#24230;&#35843;&#25972;&#27714;&#35299;&#31574;&#30053;&#12290;&#36825;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21018;&#24615;&#37319;&#29992;&#32479;&#19968;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01446</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#21160;&#24577;&#31574;&#30053;&#36873;&#25321;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning. (arXiv:2310.01446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#26681;&#25454;&#38382;&#39064;&#38590;&#24230;&#35843;&#25972;&#27714;&#35299;&#31574;&#30053;&#12290;&#36825;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21018;&#24615;&#37319;&#29992;&#32479;&#19968;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#26102;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#38382;&#39064;&#24448;&#24448;&#28041;&#21450;&#21508;&#31181;&#22797;&#26434;&#24615;&#12290;&#20154;&#31867;&#26412;&#33021;&#22320;&#26681;&#25454;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#35843;&#25972;&#20182;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21033;&#29992;LLM&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#37319;&#29992;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;: &#19981;&#31649;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#22914;&#20309;&#65292;&#37117;&#20351;&#29992;&#19968;&#33268;&#30340;&#27169;&#22411;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#38382;&#39064;&#20998;&#35299;&#31243;&#24230;&#12290;&#36825;&#31181;&#21018;&#24615;&#21487;&#33021;&#20250;&#24102;&#26469;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#25110;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;&#12290;&#23427;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#31574;&#30053;&#24615;&#22320;&#35843;&#25972;&#27714;&#35299;&#31574;&#30053;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#12290;&#21021;&#22987;&#35780;&#20272;&#27169;&#22359;&#35780;&#20272;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30340;&#20805;&#20998;&#24615;&#12290;&#22914;&#26524;&#38656;&#35201;&#25913;&#36827;&#65292;&#25509;&#19979;&#26469;&#30340;&#33258;&#36866;&#24212;&#27169;&#22359;&#20250;&#20171;&#20837;&#12290;&#22312;&#36825;&#20010;&#27169;&#22359;&#20869;&#65292;&#26377;&#19977;&#20010;&#20851;&#38190;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are showcasing impressive ability in handling complex reasoning tasks. In real-world situations, problems often span a spectrum of complexities. Humans inherently adjust their problem-solving approaches based on task complexity. However, most methodologies that leverage LLMs tend to adopt a uniform approach: utilizing consistent models, prompting methods, and degrees of problem decomposition, regardless of the problem complexity. Inflexibility of them can bring unnecessary computational overhead or sub-optimal performance. To address this problem, we introduce an Adaptive-Solver framework. It strategically modulates solving strategies based on the difficulties of the problems. Given an initial solution, the framework functions with two primary modules. The initial evaluation module assesses the adequacy of the current solution. If improvements are needed, the subsequent adaptation module comes into play. Within this module, three key adaptation strategies a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2310.01444</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#27969;&#20351;LLM&#20195;&#29702;&#36866;&#24212;&#29615;&#22659;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapting LLM Agents Through Communication. (arXiv:2310.01444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#20154;&#31867;&#21270;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24110;&#21161;&#36825;&#20123;&#20195;&#29702;&#22312;&#27809;&#26377;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;LLM&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#12290;&#36890;&#36807;&#36845;&#20195;&#25506;&#32034;&#21644;PPO&#35757;&#32451;&#65292;LTC&#20351;&#20195;&#29702;&#33021;&#22815;&#23558;&#30701;&#26399;&#32463;&#39564;&#34701;&#20837;&#38271;&#26399;&#35760;&#24518;&#12290;&#20026;&#20102;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#30340;&#20195;&#29702;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#32467;&#26500;&#21270;&#30340;&#36890;&#20449;&#27169;&#24335;&#65306;&#29420;&#30333;&#65292;&#23545;&#35805;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue,
&lt;/p&gt;</description></item><item><title>UPAR&#25552;&#31034;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#65292;&#22686;&#24378;&#20102;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.01441</link><description>&lt;p&gt;
UPAR&#65306;&#19968;&#31181;&#21463;&#24247;&#24503;&#21551;&#21457;&#30340;&#20419;&#36827;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities. (arXiv:2310.01441v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01441
&lt;/p&gt;
&lt;p&gt;
UPAR&#25552;&#31034;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#65292;&#22686;&#24378;&#20102;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#25552;&#31034;&#25552;&#21319;&#36825;&#31181;&#33021;&#21147;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#32479;&#19968;&#30340;&#35748;&#35782;&#35770;&#22522;&#30784;&#20173;&#28982;&#26126;&#26174;&#32570;&#22833;&#12290;&#21463;&#24247;&#24503;&#30340;&#20808;&#39564;&#21746;&#23398;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UPAR&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;LLMs&#20013;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#12290;UPAR&#26694;&#26550;&#20998;&#20026;&#22235;&#20010;&#38454;&#27573;&#65306;&#8220;&#29702;&#35299;&#8221;&#12289;&#8220;&#35745;&#21010;&#8221;&#12289;&#8220;&#34892;&#21160;&#8221;&#21644;&#8220;&#21453;&#24605;&#8221;&#65292;&#20351;&#24471;&#33021;&#22815;&#20174;&#22797;&#26434;&#32972;&#26223;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20107;&#20808;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#65292;&#25353;&#35745;&#21010;&#25191;&#34892;&#65292;&#24182;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;&#36825;&#20010;&#32467;&#26500;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20135;&#29983;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#65292;&#21487;&#33021;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#30340;&#31995;&#32479;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive inferential capabilities, with numerous research endeavors devoted to enhancing this capacity through prompting. Despite these efforts, a unified epistemological foundation is still conspicuously absent. Drawing inspiration from Kant's a priori philosophy, we propose the UPAR prompting framework, designed to emulate the structure of human cognition within LLMs. The UPAR framework is delineated into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection. This structure significantly augments the explainability and accuracy of LLM inference, producing a human-understandable and inspectable inferential trajectory. Furthermore, our work offers an epistemological foundation for existing prompting techniques, allowing for a possible systematic integration of these methods. With GPT-4,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23450;&#37327;&#26041;&#27861;&#37325;&#26032;&#23457;&#35270;&#40065;&#36805;&#21644;&#21608;&#20316;&#20154;1912&#24180;&#20197;&#31508;&#21517;&#21457;&#34920;&#30340;&#19977;&#31687;&#20105;&#35758;&#24615;&#25991;&#31456;&#65292;&#21457;&#29616;&#40065;&#36805;&#26159;&#20854;&#20013;&#12298;&#30475;&#20013;&#22269;&#12299;&#30340;&#20316;&#32773;&#65292;&#24182;&#19988;&#12298;&#24536;&#19981;&#20102;&#31062;&#20808;&#21578;&#35821;&#12299;&#21487;&#33021;&#30001;&#40065;&#36805;&#20027;&#35201;&#21019;&#20316;&#25110;&#22312;&#20182;&#30340;&#22823;&#37327;&#20462;&#25913;&#19979;&#23436;&#25104;&#12290;&#31532;&#19977;&#31687;&#25991;&#31456;&#23637;&#31034;&#20102;&#20004;&#20154;&#30340;&#24433;&#21709;&#24182;&#20855;&#26377;&#28151;&#21512;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2310.01440</link><description>&lt;p&gt;
&#23545;&#40065;&#36805;&#19982;&#21608;&#20316;&#20154;&#20105;&#35758;&#24615;&#25991;&#31456;&#30340;&#37325;&#26032;&#23457;&#35270;&#65306;&#31471;&#26408;&#28895;&#30340;&#22810;&#37325;&#22768;&#38899;
&lt;/p&gt;
&lt;p&gt;
The Many Voices of Duying: Revisiting the Disputed Essays Between Lu Xun and Zhou Zuoren. (arXiv:2310.01440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01440
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23450;&#37327;&#26041;&#27861;&#37325;&#26032;&#23457;&#35270;&#40065;&#36805;&#21644;&#21608;&#20316;&#20154;1912&#24180;&#20197;&#31508;&#21517;&#21457;&#34920;&#30340;&#19977;&#31687;&#20105;&#35758;&#24615;&#25991;&#31456;&#65292;&#21457;&#29616;&#40065;&#36805;&#26159;&#20854;&#20013;&#12298;&#30475;&#20013;&#22269;&#12299;&#30340;&#20316;&#32773;&#65292;&#24182;&#19988;&#12298;&#24536;&#19981;&#20102;&#31062;&#20808;&#21578;&#35821;&#12299;&#21487;&#33021;&#30001;&#40065;&#36805;&#20027;&#35201;&#21019;&#20316;&#25110;&#22312;&#20182;&#30340;&#22823;&#37327;&#20462;&#25913;&#19979;&#23436;&#25104;&#12290;&#31532;&#19977;&#31687;&#25991;&#31456;&#23637;&#31034;&#20102;&#20004;&#20154;&#30340;&#24433;&#21709;&#24182;&#20855;&#26377;&#28151;&#21512;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#36805;&#19982;&#21608;&#20316;&#20154;&#26159;&#20013;&#22269;&#29616;&#20195;&#25991;&#23398;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#20316;&#23478;&#20043;&#19968;&#12290;&#38500;&#20102;&#20316;&#20026;&#20804;&#24351;&#20851;&#31995;&#22806;&#65292;&#20182;&#20204;&#22312;&#20889;&#20316;&#29983;&#28079;&#21021;&#26399;&#36824;&#26159;&#23494;&#20999;&#21512;&#20316;&#30340;&#20249;&#20276;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#23450;&#37327;&#26041;&#27861;&#37325;&#26032;&#23457;&#35270;&#20004;&#20804;&#24351;&#22312;1912&#24180;&#20197;&#31508;&#21517;&#21457;&#34920;&#30340;&#19977;&#31687;&#20105;&#35758;&#24615;&#25991;&#31456;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#36827;&#34892;&#25991;&#20307;&#20998;&#26512;&#65292;&#20197;&#35843;&#26597;&#36825;&#20123;&#25991;&#31456;&#30340;&#20316;&#32773;&#20197;&#21450;&#20804;&#24351;&#20457;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#12298;&#30475;&#20013;&#22269;&#12299;&#19968;&#25991;&#30340;&#20316;&#32773;&#26159;&#40065;&#36805;&#12290;&#27492;&#22806;&#65292;&#12298;&#24536;&#19981;&#20102;&#31062;&#20808;&#21578;&#35821;&#12299;&#19968;&#25991;&#20284;&#20046;&#35201;&#20040;&#26159;&#40065;&#36805;&#20027;&#35201;&#21019;&#20316;&#65292;&#35201;&#20040;&#26159;&#22312;&#40065;&#36805;&#30340;&#22823;&#37327;&#20462;&#25913;&#19979;&#23436;&#25104;&#30340;&#65292;&#22240;&#20026;&#23427;&#19982;&#21608;&#20316;&#20154;&#35748;&#21487;&#20026;&#33258;&#24049;&#30340;&#12298;&#30475;&#36234;&#23665;&#27700;&#12299;&#22312;&#25991;&#20307;&#19978;&#26377;&#26174;&#33879;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#31532;&#19977;&#31687;&#12298;&#27665;&#20027;&#24050;&#32553;&#27700;&#65292;&#23427;&#21435;&#20102;&#20309;&#26041;&#12299;&#21017;&#23637;&#29616;&#20986;&#19968;&#31181;&#28151;&#21512;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#34920;&#26126;&#20102;&#20004;&#32773;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lu Xun and Zhou Zuoren stand as two of the most influential writers in modern Chinese literature. Beyond their familial ties as brothers, they were also intimate collaborators during the nascent stages of their writing careers. This research employs quantitative methods to revisit three disputed essays pseudonymously published by the brothers in 1912. Our stylometric analysis uses an interpretable authorship attribution model to investigate the essays' authorship and examine the brothers' respective writing styles. Our findings suggest that 'Looking at the Country of China' was authored by Lu Xun. Moreover, 'People of Yue, Forget Not Your Ancestors' Instructions' seems to be either predominantly authored or extensively revised by Lu Xun given its notable stylistic similarities to 'Looking at the Land of Yue,' a piece Zhou Zuoren recognized as his own, but edited by Lu Xun. The third essay, 'Where Has the Character of the Republic Gone?,' exhibits a 'diluted', mixed writing style, sugge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;30&#20159;&#21442;&#25968;&#30340;GPT LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#26412;&#22320;&#20195;&#30721;&#21644;&#27169;&#22411;&#37327;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#22312;&#20302;&#20869;&#23384;&#35774;&#22791;&#19978;&#30340;&#24179;&#31283;&#36816;&#34892;&#65292;&#24182;&#35299;&#20915;&#20102;&#32593;&#32476;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#24212;&#29992;&#19981;&#20165;&#20855;&#26377;&#36890;&#29992;&#21161;&#25163;&#21151;&#33021;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;&#25805;&#20316;&#30340;&#26080;&#32541;&#31227;&#21160;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.01434</link><description>&lt;p&gt;
&#38761;&#26032;&#31227;&#21160;&#20114;&#21160;&#65306;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;30&#20159;&#21442;&#25968;&#30340;GPT LLM
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile. (arXiv:2310.01434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;30&#20159;&#21442;&#25968;&#30340;GPT LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#26412;&#22320;&#20195;&#30721;&#21644;&#27169;&#22411;&#37327;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#22312;&#20302;&#20869;&#23384;&#35774;&#22791;&#19978;&#30340;&#24179;&#31283;&#36816;&#34892;&#65292;&#24182;&#35299;&#20915;&#20102;&#32593;&#32476;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#24212;&#29992;&#19981;&#20165;&#20855;&#26377;&#36890;&#29992;&#21161;&#25163;&#21151;&#33021;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;&#25805;&#20316;&#30340;&#26080;&#32541;&#31227;&#21160;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#24378;&#22823;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#12290;&#20113;&#31471;&#30340;LLM&#65292;&#20363;&#22914;OpenAI&#30340;ChatGPT&#65292;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21151;&#33021;&#65292;&#20294;&#30001;&#20110;&#32593;&#32476;&#20381;&#36182;&#24615;&#65292;&#24310;&#36831;&#21644;&#38544;&#31169;&#38382;&#39064;&#20196;&#20154;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#23637;&#26395;&#20102;&#26410;&#26469;&#22312;&#27809;&#26377;&#32593;&#32476;&#36830;&#25509;&#30340;&#24773;&#20917;&#19979;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20855;&#26377;30&#20159;&#21442;&#25968;&#30340;&#31934;&#35843;GPT LLM&#65292;&#21487;&#20197;&#22312;&#20869;&#23384;&#21482;&#26377;4GB&#30340;&#35774;&#22791;&#19978;&#24179;&#31283;&#36816;&#34892;&#12290;&#36890;&#36807;&#25972;&#21512;&#26412;&#22320;&#20195;&#30721;&#21644;&#27169;&#22411;&#37327;&#21270;&#25216;&#26415;&#65292;&#35813;&#24212;&#29992;&#19981;&#20165;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#21161;&#25163;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#21040;&#25805;&#20316;&#30340;&#21151;&#33021;&#23454;&#29616;&#26080;&#32541;&#30340;&#31227;&#21160;&#20114;&#21160;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#35757;&#32451;&#27969;&#31243;&#12289;&#23454;&#29616;&#32454;&#33410;&#21644;&#27979;&#35797;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Artificial Intelligence has witnessed remarkable progress in recent years, especially with the emergence of powerful large language models (LLMs) based on the transformer architecture. Cloud-based LLMs, such as OpenAI's ChatGPT, offer impressive capabilities but come with concerns regarding latency and privacy due to network dependencies. This article presents an innovative approach to LLM inference, envisioning a future where LLMs with billions of parameters can be executed directly on mobile devices without network connectivity. The article showcases a fine-tuned GPT LLM with 3 billion parameters that can operate smoothly on devices with as low as 4GB of memory. Through the integration of native code and model quantization techniques, the application not only serves as a general-purpose assistant but also facilitates seamless mobile interactions with text-to-actions features. The article provides insights into the training pipeline, implementation details, test results, 
&lt;/p&gt;</description></item><item><title>PORTIA&#26159;&#19968;&#20010;&#26088;&#22312;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#20854;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01432</link><description>&lt;p&gt;
&#20998;&#21106;&#19982;&#21512;&#24182;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20301;&#32622;&#20559;&#24046;&#36827;&#34892;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01432
&lt;/p&gt;
&lt;p&gt;
PORTIA&#26159;&#19968;&#20010;&#26088;&#22312;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#20854;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20316;&#20026;&#33258;&#21160;&#21270;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#31995;&#32479;&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#22312;&#20351;&#29992;&#23545;&#27604;&#35780;&#20272;&#20505;&#36873;&#31572;&#26696;&#26102;&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#25110;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#35270;&#20869;&#23481;&#32780;&#20559;&#21521;&#20110;&#31532;&#19968;&#20010;&#25110;&#31532;&#20108;&#20010;&#31572;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PORTIA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#40784;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#30340;&#27604;&#36739;&#31574;&#30053;&#65292;&#20197;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#26041;&#24335;&#26657;&#20934;&#20301;&#32622;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PORTIA&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#23545;&#27604;&#20505;&#36873;&#31572;&#26696;&#20013;&#30340;&#30456;&#20284;&#20869;&#23481;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#20379;LLMs&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;11,520&#20010;&#31572;&#26696;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PORTIA&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27169;&#22411;&#21644;&#23545;&#27604;&#24418;&#24335;&#30340;&#19968;&#33268;&#24615;&#29575;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;47.46%&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;PORTIA&#20351;&#24471;LLMs&#33021;&#22815;&#35780;&#20272;&#20013;&#23545;&#20301;&#32622;&#20559;&#24046;&#36827;&#34892;&#26657;&#20934;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#12289;&#35821;&#38899;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;MUStARD++&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#39640;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26032;&#22686;&#30340;&#25968;&#25454;&#38598;&#29255;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01430</link><description>&lt;p&gt;
&#35270;&#21548;&#20013;&#30340;&#35773;&#21050;&#65306;&#22522;&#20934;&#27979;&#35797;&#21644;&#25299;&#23637;&#20197;&#25552;&#39640;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sarcasm in Sight and Sound: Benchmarking and Expansion to Improve Multimodal Sarcasm Detection. (arXiv:2310.01430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#12289;&#35821;&#38899;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;MUStARD++&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#39640;&#22810;&#27169;&#24335;&#35773;&#21050;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26032;&#22686;&#30340;&#25968;&#25454;&#38598;&#29255;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MUStARD&#25968;&#25454;&#38598;&#21450;&#20854;&#24773;&#32490;&#35782;&#21035;&#25193;&#23637;MUStARD++&#30340;&#24341;&#20837;&#65292;&#24050;&#32463;&#30830;&#23450;&#35773;&#21050;&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#29616;&#35937;&#65292;&#19981;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#34920;&#36798;&#65292;&#36824;&#36890;&#36807;&#35821;&#35328;&#65288;&#22914;&#38899;&#35843;&#21644;&#35821;&#35843;&#65289;&#21644;&#35270;&#35273;&#32447;&#32034;&#65288;&#38754;&#37096;&#34920;&#24773;&#65289;&#34920;&#36798;&#12290;&#36890;&#36807;&#32771;&#34385;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#12289;&#35821;&#38899;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#26088;&#22312;&#23545;MUStARD++&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20805;&#20998;&#21033;&#29992;&#20854;&#22810;&#27169;&#24335;&#20016;&#23500;&#24615;&#65292;&#20351;&#23439;&#35266;F1&#20540;&#27604;&#29616;&#26377;&#22522;&#20934;&#25552;&#39640;2&#65285;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;MUStARD++&#20013;&#8220;&#35773;&#21050;&#31867;&#22411;&#8221;&#31867;&#21035;&#30340;&#19981;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;MUStARD++ Balanced&#8221;&#30340;&#25193;&#23637;&#65292;&#23558;&#27492;&#25193;&#23637;&#20013;&#30340;&#23454;&#20363;&#20998;&#24067;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#23439;&#35266;F1&#20540;&#36827;&#19968;&#27493;&#25552;&#39640;2.4&#65285;&#12290;&#36825;&#20123;&#26032;&#29255;&#27573;&#26469;&#33258;&#20110;&#19968;&#37096;&#21517;&#20026;&#12298;&#35946;&#26031;&#21307;&#29983;&#12299;&#30340;&#26032;&#39062;&#36164;&#28304;&#65292;&#22686;&#21152;&#20102;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of the MUStARD dataset, and its emotion recognition extension MUStARD++, have identified sarcasm to be a multi-modal phenomenon -expressed not only in natural language text, but also through manners of speech (like tonality and intonation) and visual cues (facial expression). With this work, we aim to perform a rigorous benchmarking of the MUStARD++ dataset by considering state-of-the-art language, speech, and visual encoders, for fully utilizing the totality of the multi-modal richness that it has to offer, achieving a 2\% improvement in macro-F1 over the existing benchmark. Additionally, to cure the imbalance in the `sarcasm type' category in MUStARD++, we propose an extension, which we call \emph{MUStARD++ Balanced}, benchmarking the same with instances from the extension split across both train and test sets, achieving a further 2.4\% macro-F1 boost. The new clips were taken from a novel source -- the TV show, House MD, which adds to the diversity of the dataset,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#25552;&#20379;&#23545;OpenStreetMap&#65288;OSM&#65289;&#25968;&#25454;&#30340;&#35821;&#35328;&#25509;&#21475;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35813;&#30028;&#38754;&#26597;&#35810;&#26377;&#20851;&#29305;&#23450;&#22320;&#29702;&#20301;&#32622;&#30340;&#23646;&#24615;&#21644;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.01429</link><description>&lt;p&gt;
Chatmap&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22320;&#22270;&#25968;&#25454;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Chatmap : Large Language Model Interaction with Cartographic Data. (arXiv:2310.01429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#25552;&#20379;&#23545;OpenStreetMap&#65288;OSM&#65289;&#25968;&#25454;&#30340;&#35821;&#35328;&#25509;&#21475;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35813;&#30028;&#38754;&#26597;&#35810;&#26377;&#20851;&#29305;&#23450;&#22320;&#29702;&#20301;&#32622;&#30340;&#23646;&#24615;&#21644;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#24191;&#27867;&#21487;&#29992;&#24615;&#65292;&#32467;&#21512;&#24378;&#22823;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20652;&#29983;&#20102;&#21019;&#26032;&#21644;&#21153;&#23454;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#20351;LLMs&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#37322;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#23545;&#24222;&#22823;&#30340;&#22320;&#22270;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#35775;&#38382;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;OpenStreetMap&#65288;OSM&#65289;&#26159;&#26368;&#38596;&#24515;&#21187;&#21187;&#30340;&#24320;&#28304;&#20840;&#29699;&#20513;&#35758;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#22478;&#24066;&#21644;&#20892;&#26449;&#22320;&#29702;&#25968;&#25454;&#65292;&#30001;&#36229;&#36807;1000&#19975;&#30340;&#36129;&#29486;&#32773;&#31038;&#21306;&#31574;&#21010;&#65292;&#20026;LLM&#24212;&#29992;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#30456;&#23545;&#36739;&#23567;&#35268;&#27169;&#65288;10&#20159;&#21442;&#25968;&#65289;&#30340;LLM&#36890;&#36807;&#36739;&#24378;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#31574;&#21010;&#30340;&#30456;&#23545;&#36739;&#23567;&#30340;&#20154;&#24037;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#30340;&#27010;&#24565;&#39564;&#35777;&#21644;&#35814;&#32454;&#36807;&#31243;&#65292;&#20197;&#25552;&#20379;&#23545;&#20219;&#24847;&#22478;&#24066;&#21306;&#22495;&#30340;OSM&#25968;&#25454;&#30340;&#35821;&#35328;&#30028;&#38754;&#12290;&#36890;&#36807;&#35813;&#30028;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#26597;&#35810;&#20301;&#32622;&#30340;&#23646;&#24615;&#12289;&#21327;&#21516;&#24615;&#21644;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The swift advancement and widespread availability of foundational Large Language Models (LLMs), complemented by robust fine-tuning methodologies, have catalyzed their adaptation for innovative and industrious applications. Enabling LLMs to recognize and interpret geospatial data, while offering a linguistic access to vast cartographic datasets, is of significant importance. OpenStreetMap (OSM) is the most ambitious open-source global initiative offering detailed urban and rural geographic data, curated by a community of over 10 million contributors, which constitutes a great potential for LLM applications. In this study, we demonstrate the proof of concept and details of the process of fine-tuning a relatively small scale (1B parameters) LLM with a relatively small artificial dataset curated by a more capable teacher model, in order to provide a linguistic interface to the OSM data of an arbitrary urban region. Through this interface, users can inquire about a location's attributes, co
&lt;/p&gt;</description></item><item><title>&#27880;&#24847;&#21147;&#25490;&#24207;&#21487;&#20197;&#25913;&#21892;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#27880;&#24847;&#21147;&#36827;&#34892;&#25490;&#24207;&#24182;&#37325;&#22797;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#25972;&#21512;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01427</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#25490;&#24207;&#22312;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#23545;&#26032;&#36817;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Attention Sorting Combats Recency Bias In Long Context Language Models. (arXiv:2310.01427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01427
&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#25490;&#24207;&#21487;&#20197;&#25913;&#21892;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#27880;&#24847;&#21147;&#36827;&#34892;&#25490;&#24207;&#24182;&#37325;&#22797;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#25972;&#21512;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#24448;&#24448;&#26410;&#33021;&#39640;&#25928;&#22320;&#25972;&#21512;&#38271;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#21487;&#33021;&#23398;&#21040;&#30340;&#27880;&#24847;&#21147;&#20808;&#39564;&#65306;&#19978;&#19979;&#25991;&#20013;&#36739;&#26089;&#20986;&#29616;&#30340;&#30456;&#20851;&#20449;&#24687;&#24179;&#22343;&#26469;&#35828;&#34987;&#20851;&#27880;&#30340;&#36739;&#23569;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#33021;&#22312;&#22238;&#24212;&#20013;&#20351;&#29992;&#26469;&#33258;&#30456;&#20851;&#25991;&#26723;&#30340;&#20449;&#24687;&#65292;&#23427;&#20204;&#20173;&#28982;&#30456;&#23545;&#20110;&#21516;&#19968;&#20301;&#32622;&#19978;&#30340;&#26080;&#20851;&#25991;&#26723;&#32473;&#20104;&#20559;&#29233;&#30340;&#27880;&#24847;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#20107;&#23454;&#65292;&#25105;&#20204;&#21033;&#29992;"&#27880;&#24847;&#21147;&#25490;&#24207;"&#65306;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#25191;&#34892;&#19968;&#27493;&#65292;&#25353;&#29031;&#20182;&#20204;&#25509;&#25910;&#21040;&#30340;&#27880;&#24847;&#21147;&#36827;&#34892;&#25490;&#24207;&#65288;&#26368;&#39640;&#30340;&#27880;&#24847;&#21147;&#25490;&#22312;&#26368;&#21518;&#65289;&#65292;&#37325;&#22797;&#35813;&#36807;&#31243;&#65292;&#20351;&#29992;&#26032;&#25490;&#24207;&#30340;&#19978;&#19979;&#25991;&#29983;&#25104;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27880;&#24847;&#21147;&#25490;&#24207;&#25552;&#39640;&#20102;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current language models often fail to incorporate long contexts efficiently during generation. We show that a major contributor to this issue are attention priors that are likely learned during pre-training: relevant information located earlier in context is attended to less on average. Yet even when models fail to use the information from a relevant document in their response, they still pay preferential attention to that document compared to an irrelevant document at the same position. We leverage this fact to introduce ``attention sorting'': perform one step of decoding, sort documents by the attention they receive (highest attention going last), repeat the process, generate the answer with the newly sorted context. We find that attention sorting improves performance of long context models. Our findings highlight some challenges in using off-the-shelf language models for retrieval augmented generation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.01425</link><description>&lt;p&gt;
Borges&#19982;AI
&lt;/p&gt;
&lt;p&gt;
Borges and AI. (arXiv:2310.01425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01425
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21551;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26102;&#20195;&#12290;&#19968;&#20123;&#20154;&#30475;&#21040;&#20102;&#26426;&#36935;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#30475;&#21040;&#20102;&#21361;&#38505;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#37117;&#36890;&#36807;&#31185;&#24187;&#23567;&#35828;&#20013;&#27969;&#34892;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;AI&#12290;&#26426;&#22120;&#26159;&#21542;&#20250;&#21464;&#24471;&#26377;&#24863;&#30693;&#33021;&#21147;&#24182;&#21453;&#25239;&#20854;&#21019;&#36896;&#32773;&#65311;&#25105;&#20204;&#26159;&#21542;&#20250;&#32463;&#21382;&#32440;&#22841;&#22841;&#23376;&#21551;&#31034;&#65311;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20043;&#21069;&#65292;&#25105;&#20204;&#39318;&#20808;&#24212;&#35813;&#38382;&#19968;&#19979;&#65292;&#36825;&#31181;&#24515;&#29702;&#24847;&#35937;&#26159;&#21542;&#23545;&#25163;&#22836;&#30340;&#29616;&#35937;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#25551;&#36848;&#12290;&#20165;&#36890;&#36807;&#31070;&#28789;&#30340;&#24773;&#32490;&#26469;&#29702;&#35299;&#22825;&#27668;&#27169;&#24335;&#30340;&#26041;&#27861;&#26159;&#26377;&#38480;&#30340;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20027;&#24352;&#36890;&#36807;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;LLMs&#21450;&#20854;&#19982;AI&#30340;&#20851;&#31995;&#65292;&#21338;&#23572;&#36203;&#26031;&#26159;20&#19990;&#32426;&#25991;&#23398;&#22823;&#24072;&#65292;&#39764;&#24187;&#29616;&#23454;&#20027;&#20041;&#20808;&#39537;&#21644;&#21518;&#29616;&#20195;&#25991;&#23398;&#30340;&#21069;&#22863;&#12290;&#36825;&#31181;&#25506;&#32034;&#26041;&#24335;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#25506;&#35752;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#21644;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#24182;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20851;&#38190;&#24046;&#36317;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.01424</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#38544;&#31169;&#39118;&#38505;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#25506;&#35752;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#21644;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#24182;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20851;&#38190;&#24046;&#36317;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#20854;&#34987;&#24191;&#27867;&#37319;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#12290;&#38500;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#39118;&#38505;&#65292;&#21253;&#25324;&#38544;&#31169;&#39118;&#38505;&#12290;&#23588;&#20854;&#26159;&#38543;&#30528;LMs&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#28508;&#21147;&#22686;&#21152;&#65292;&#20174;&#32780;&#23548;&#33268;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#38543;&#30528;LMs&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#36825;&#20123;&#38544;&#31169;&#39118;&#38505;&#20197;&#21450;&#22914;&#20309;&#20943;&#36731;&#23427;&#20204;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#20102;&#35299;LM&#38544;&#31169;&#25915;&#20987;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#30693;&#35782;&#29366;&#20917;&#65292;&#21253;&#25324;&#38656;&#35201;&#26356;&#22810;&#24037;&#20316;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20221;&#20851;&#20110;LM&#38544;&#31169;&#30340;&#25216;&#26415;&#35843;&#26597;&#12290;&#25105;&#20204;&#65288;i&#65289;&#30830;&#23450;&#20102;&#25915;&#20987;&#22312;LMs&#19978;&#23384;&#22312;&#30340;&#26174;&#33879;&#32500;&#24230;&#30340;&#20998;&#31867;&#27861;&#65292;&#65288;ii&#65289;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#26469;&#31361;&#20986;&#20027;&#35201;&#36235;&#21183;&#65292;&#65288;iii&#65289;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#65292;&#31361;&#20986;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#35782;&#21035;&#20851;&#38190;&#24046;&#36317;&#65292;&#23637;&#31034;&#24320;&#25918;&#38382;&#39064;&#21644;&#24314;&#35758;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in language models (LMs) have led to their adoption across many sectors. Alongside the potential benefits, such models present a range of risks, including around privacy. In particular, as LMs have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. As LMs become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on LM privacy. We (i) identify a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and are
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01423</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#20197;&#26469;&#65292;&#23427;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65288;&#21253;&#25324;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#65289;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22238;&#24212;&#65292;&#21560;&#24341;&#20102;&#35768;&#22810;&#20154;&#30340;&#20852;&#36259;&#12290;ChatGPT&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#35823;&#29992;&#21487;&#33021;&#20250;&#24102;&#26469;&#20005;&#37325;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#32946;&#21644;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#12290;&#30446;&#21069;&#24050;&#32463;&#26377;&#20960;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#21487;&#20379;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#22312;&#30495;&#23454;&#25991;&#26412;&#19978;&#36827;&#34892;&#27979;&#35797;&#30340;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#23427;&#20204;&#23545;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#29992;&#20110;&#26816;&#27979;&#22823;&#23398;&#21644;&#20854;&#20182;&#30740;&#31350;&#26426;&#26500;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#31456;&#12289;&#25688;&#35201;&#12289;&#25925;&#20107;&#12289;&#26032;&#38395;&#21644;&#20135;&#21697;&#35780;&#35770;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#27493;&#26159;&#20351;&#29992;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#20845;&#31181;&#24037;&#20855;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35825;&#23548;&#36741;&#23548;&#33050;&#26412;&#21644;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#30340;&#26032;&#22411;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#21021;&#27493;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#19982;&#20854;&#20182;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#30456;&#27604;&#65292;&#31995;&#32479;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01420</link><description>&lt;p&gt;
Ruffle&amp;Riley&#65306;&#36208;&#21521;&#33258;&#21160;&#21270;&#30340;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Ruffle&amp;Riley: Towards the Automated Induction of Conversational Tutoring Systems. (arXiv:2310.01420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35825;&#23548;&#36741;&#23548;&#33050;&#26412;&#21644;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#30340;&#26032;&#22411;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#21021;&#27493;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#19982;&#20854;&#20182;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#30456;&#27604;&#65292;&#31995;&#32479;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#65288;CTS&#65289;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#25552;&#20379;&#23398;&#20064;&#20307;&#39564;&#12290;&#23427;&#20204;&#34987;&#35748;&#20026;&#33021;&#22815;&#20419;&#36827;&#39640;&#27700;&#24179;&#30340;&#35748;&#30693;&#21442;&#19982;&#65292;&#24182;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#26377;&#30410;&#20110;&#23398;&#20064;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#25776;&#20889;CTS&#20869;&#23481;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#26159;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#22411;&#30340;CTS&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#39318;&#20808;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#25945;&#23398;&#25991;&#26412;&#33258;&#21160;&#35825;&#23548;&#20986;&#36741;&#23548;&#33050;&#26412;&#12290;&#20854;&#27425;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#65288;Ruffle&amp;Riley&#65289;&#22312;&#23398;&#20197;&#25945;&#23398;&#30340;&#24418;&#24335;&#20013;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#33258;&#30001;&#23545;&#35805;&#65292;&#36981;&#24490;ITS&#20856;&#22411;&#30340;&#22806;&#37096;/&#20869;&#37096;&#24490;&#29615;&#32467;&#26500;&#12290;&#22312;&#19968;&#20010;&#21021;&#27493;&#30340;&#34987;&#35797;&#32773;&#22312;&#32447;&#29992;&#25143;&#30740;&#31350;&#65288;N = 100&#65289;&#20013;&#65292;&#23558;Ruffle&amp;Riley&#19982;&#26356;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational tutoring systems (CTSs) offer learning experiences driven by natural language interaction. They are known to promote high levels of cognitive engagement and benefit learning outcomes, particularly in reasoning tasks. Nonetheless, the time and cost required to author CTS content is a major obstacle to widespread adoption. In this paper, we introduce a novel type of CTS that leverages the recent advances in large language models (LLMs) in two ways: First, the system induces a tutoring script automatically from a lesson text. Second, the system automates the script orchestration via two LLM-based agents (Ruffle&amp;Riley) with the roles of a student and a professor in a learning-by-teaching format. The system allows a free-form conversation that follows the ITS-typical outer-/inner-loop structure. In an initial between-subject online user study (N = 100) comparing Ruffle&amp;Riley to simpler QA chatbots and reading activity, we found no significant differences in post-test scores. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#21644;Reddit&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31867;&#26410;&#26631;&#35760;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26469;&#39044;&#27979;&#29992;&#25143;&#26159;&#21542;&#26377;&#25233;&#37057;&#30151;&#65292;&#24182;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2310.01418</link><description>&lt;p&gt;
Cordyceps@LT-EDI&#65306;&#20351;&#29992;Reddit&#21644;&#33258;&#25105;&#35757;&#32451;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cordyceps@LT-EDI: Depression Detection with Reddit and Self-training. (arXiv:2310.01418v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#21644;Reddit&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31867;&#26410;&#26631;&#35760;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26469;&#39044;&#27979;&#29992;&#25143;&#26159;&#21542;&#26377;&#25233;&#37057;&#30151;&#65292;&#24182;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#20196;&#20154;&#20007;&#22833;&#33021;&#21147;&#19988;&#24182;&#19981;&#32597;&#35265;&#30340;&#30142;&#30149;&#12290;&#23454;&#38469;&#19978;&#65292;&#23545;&#36807;&#24230;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#30340;&#30740;&#31350;&#34920;&#26126;&#19982;&#25233;&#37057;&#30151;&#12289;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#65288;ADHD&#65289;&#20197;&#21450;&#20854;&#20182;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#37492;&#20110;&#26377;&#22823;&#37327;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#30340;&#20154;&#32676;&#65292;&#23384;&#22312;&#30528;&#21487;&#33021;&#26410;&#34987;&#35786;&#26029;&#30340;&#29992;&#25143;&#21644;&#20182;&#20204;&#25152;&#21457;&#24067;&#30340;&#24086;&#23376;&#30340;&#24222;&#22823;&#20154;&#21475;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#26816;&#27979;&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#24086;&#23376;&#26159;&#21542;&#26469;&#33258;&#27491;&#22312;&#32463;&#21382;&#20005;&#37325;&#12289;&#20013;&#31561;&#25110;&#36731;&#24494;&#65288;&#38750;&#35786;&#26029;&#24615;&#65289;&#25233;&#37057;&#30151;&#29366;&#30340;&#29992;&#25143;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#23545;Reddit&#19978;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#29983;&#25104;&#30340;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;Detecting Signs of Depression from Social Media Text LT-EDI@RANLP 2023&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#25972;&#20307;&#25490;&#21517;&#20013;&#21517;&#21015;&#31532;&#19977;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is debilitating, and not uncommon. Indeed, studies of excessive social media users show correlations with depression, ADHD, and other mental health concerns. Given that there is a large number of people with excessive social media usage, then there is a significant population of potentially undiagnosed users and posts that they create. In this paper, we propose a depression severity detection system using a semi-supervised learning technique to predict if a post is from a user who is experiencing severe, moderate, or low (non-diagnostic) levels of depression. Namely, we use a trained model to classify a large number of unlabelled social media posts from Reddit, then use these generated labels to train a more powerful classifier. We demonstrate our framework on Detecting Signs of Depression from Social Media Text LT-EDI@RANLP 2023 shared task, where our framework ranks 3rd overall.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01405</link><description>&lt;p&gt;
&#34920;&#31034;&#24037;&#31243;&#21270;&#65306;AI&#36879;&#26126;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#25551;&#36848;&#20102;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;RepE&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#25110;&#30005;&#36335;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RepE&#25216;&#26415;&#30340;&#22522;&#20934;&#21644;&#21021;&#27493;&#20998;&#26512;&#65292;&#26174;&#31034;&#23427;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25913;&#21892;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#22312;&#21253;&#25324;&#35802;&#23454;&#24615;&#12289;&#26080;&#23475;&#24615;&#12289;&#36861;&#27714;&#26435;&#21147;&#31561;&#19968;&#31995;&#21015;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#21457;&#25381;&#20316;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#19978;&#32780;&#19979;&#36879;&#26126;&#24615;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#20419;&#36827;RepE&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#65292;&#24182;&#25512;&#21160;AI&#31995;&#32479;&#30340;&#36879;&#26126;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;ChatGPT&#26816;&#27979;&#26041;&#27861;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27867;&#21270;&#34892;&#20026;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#25991;&#26412;&#38271;&#24230;&#12289;&#20027;&#39064;&#21644;&#35821;&#35328;&#20219;&#21153;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#25581;&#31034;&#20102;&#26377;&#21551;&#31034;&#24615;&#30340;&#21457;&#29616;&#65292;&#20026;&#26410;&#26469;&#26041;&#27861;&#25110;&#25968;&#25454;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.01307</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#35757;&#32451;&#30340;ChatGPT&#26816;&#27979;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization of Training-based ChatGPT Detection Methods. (arXiv:2310.01307v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;ChatGPT&#26816;&#27979;&#26041;&#27861;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27867;&#21270;&#34892;&#20026;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#25991;&#26412;&#38271;&#24230;&#12289;&#20027;&#39064;&#21644;&#35821;&#35328;&#20219;&#21153;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#25581;&#31034;&#20102;&#26377;&#21551;&#31034;&#24615;&#30340;&#21457;&#29616;&#65292;&#20026;&#26410;&#26469;&#26041;&#27861;&#25110;&#25968;&#25454;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#26377;&#36843;&#20999;&#30340;&#38656;&#27714;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#20013;&#26816;&#27979;&#20986;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#19968;&#31181;&#24191;&#27867;&#30740;&#31350;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#26469;&#21306;&#20998;&#20108;&#32773;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20063;&#34920;&#26126;&#65292;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#21463;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#39044;&#27979;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#20219;&#21153;&#25110;&#20027;&#39064;&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#26080;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20840;&#38754;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#22312;&#30001;&#22810;&#31181;&#22240;&#32032;&#24341;&#36215;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#27867;&#21270;&#34892;&#20026;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#25991;&#26412;&#38271;&#24230;&#12289;&#20027;&#39064;&#21644;&#35821;&#35328;&#20219;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#21644;ChatGPT&#25991;&#26412;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#23545;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26377;&#21551;&#31034;&#24615;&#30340;&#21457;&#29616;&#65292;&#20026;&#26410;&#26469;&#26041;&#27861;&#25110;&#25968;&#25454;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is one of the most popular language models which achieve amazing performance on various natural language tasks. Consequently, there is also an urgent need to detect the texts generated ChatGPT from human written. One of the extensively studied methods trains classification models to distinguish both. However, existing studies also demonstrate that the trained models may suffer from distribution shifts (during test), i.e., they are ineffective to predict the generated texts from unseen language tasks or topics. In this work, we aim to have a comprehensive investigation on these methods' generalization behaviors under distribution shift caused by a wide range of factors, including prompts, text lengths, topics, and language tasks. To achieve this goal, we first collect a new dataset with human and ChatGPT texts, and then we conduct extensive studies on the collected dataset. Our studies unveil insightful findings which provide guidance for developing future methodologies or data 
&lt;/p&gt;</description></item><item><title>appjsonify&#26159;&#19968;&#31181;&#23398;&#26415;&#35770;&#25991;PDF&#21040;JSON&#36716;&#25442;&#24037;&#20855;&#21253;&#65292;&#23427;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#21644;&#26041;&#27861;&#35299;&#26512;PDF&#25991;&#20214;&#65292;&#24182;&#19988;&#20801;&#35768;&#29992;&#25143;&#28789;&#27963;&#37197;&#32622;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.01206</link><description>&lt;p&gt;
appjsonify&#65306;&#19968;&#31181;&#23398;&#26415;&#35770;&#25991;PDF&#21040;JSON&#36716;&#25442;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
appjsonify: An Academic Paper PDF-to-JSON Conversion Toolkit. (arXiv:2310.01206v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01206
&lt;/p&gt;
&lt;p&gt;
appjsonify&#26159;&#19968;&#31181;&#23398;&#26415;&#35770;&#25991;PDF&#21040;JSON&#36716;&#25442;&#24037;&#20855;&#21253;&#65292;&#23427;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#21644;&#26041;&#27861;&#35299;&#26512;PDF&#25991;&#20214;&#65292;&#24182;&#19988;&#20801;&#35768;&#29992;&#25143;&#28789;&#27963;&#37197;&#32622;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;appjsonify&#65292;&#19968;&#31181;&#22522;&#20110;Python&#30340;&#23398;&#26415;&#35770;&#25991;PDF&#21040;JSON&#36716;&#25442;&#24037;&#20855;&#21253;&#12290;&#23427;&#20351;&#29992;&#22810;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#27169;&#22411;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#26512;PDF&#25991;&#20214;&#12290;appjsonify&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#24037;&#20855;&#65292;&#20801;&#35768;&#29992;&#25143;&#36731;&#26494;&#37197;&#32622;&#22788;&#29702;&#27969;&#31243;&#65292;&#20197;&#22788;&#29702;&#29305;&#23450;&#26684;&#24335;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;appjsonify&#20316;&#20026;&#19968;&#20010;&#26131;&#20110;&#23433;&#35013;&#30340;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#36890;&#36807;PyPI&#21644;GitHub&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present appjsonify, a Python-based PDF-to-JSON conversion toolkit for academic papers. It parses a PDF file using several visual-based document layout analysis models and rule-based text processing approaches. appjsonify is a flexible tool that allows users to easily configure the processing pipeline to handle a specific format of a paper they wish to process. We are publicly releasing appjsonify as an easy-to-install toolkit available via PyPI and GitHub.
&lt;/p&gt;</description></item><item><title>TRAM&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#35780;&#20272;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;&#21313;&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;TRAM&#22522;&#20934;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20107;&#20214;&#30340;&#21508;&#31181;&#26102;&#38388;&#26041;&#38754;&#12290;&#23613;&#31649;&#20351;&#29992;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#26102;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.00835</link><description>&lt;p&gt;
TRAM&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TRAM: Benchmarking Temporal Reasoning for Large Language Models. (arXiv:2310.00835v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00835
&lt;/p&gt;
&lt;p&gt;
TRAM&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#35780;&#20272;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;&#21313;&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;TRAM&#22522;&#20934;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20107;&#20214;&#30340;&#21508;&#31181;&#26102;&#38388;&#26041;&#38754;&#12290;&#23613;&#31649;&#20351;&#29992;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#26102;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25512;&#29702;&#23545;&#20110;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#20107;&#20214;&#30340;&#32454;&#24494;&#24046;&#21035;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#23545;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#33539;&#22260;&#26377;&#38480;&#65292;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#36825;&#23548;&#33268;&#19981;&#21516;&#30740;&#31350;&#38388;&#30340;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;TRAM&#30340;&#26102;&#38388;&#25512;&#29702;&#22522;&#20934;&#35780;&#20272;&#65292;&#30001;&#21313;&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#28085;&#30422;&#20102;&#20107;&#20214;&#30340;&#21508;&#31181;&#26102;&#38388;&#26041;&#38754;&#65292;&#22914;&#39034;&#24207;&#12289;&#31639;&#26415;&#12289;&#39057;&#29575;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#20351;&#29992;&#20102;&#27969;&#34892;&#30340;LLM&#65292;&#22914;GPT-4&#21644;Llama2&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26102;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;TRAM&#33021;&#22815;&#25512;&#21160;&#36827;&#19968;&#27493;&#25552;&#21319;&#26102;&#38388;&#25512;&#29702;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the temporal reasoning capabilities of large language models (LLMs). We conduct an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based models to establish the baseline evaluations. Our findings indicate that these models still trail human performance in temporal reasoning tasks. It is our aspiration that TRAM will spur further progress in enhancing the temporal reason
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#21319;&#21475;&#35821;&#35821;&#31181;&#35782;&#21035;&#31995;&#32479;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#34920;&#31034;&#26041;&#27861;&#8212;&#8212;&#23567;&#27874;&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#24320;&#21457;&#34701;&#21512;&#31995;&#32479;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#38169;&#35823;&#35782;&#21035;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.00602</link><description>&lt;p&gt;
&#25552;&#21319;&#20302;&#36164;&#28304;&#21475;&#35821;&#35821;&#31181;&#35782;&#21035;&#27867;&#21270;&#24615;&#33021;&#30340;&#23567;&#27874;&#25955;&#23556;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification. (arXiv:2310.00602v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#21319;&#21475;&#35821;&#35821;&#31181;&#35782;&#21035;&#31995;&#32479;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#34920;&#31034;&#26041;&#27861;&#8212;&#8212;&#23567;&#27874;&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#24320;&#21457;&#34701;&#21512;&#31995;&#32479;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#38169;&#35823;&#35782;&#21035;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#35821;&#31181;&#35782;&#21035;&#65288;LID&#65289;&#20013;&#24120;&#29992;&#30340;&#29305;&#24449;&#65292;&#22914;mel-spectrogram&#25110;MFCC&#65292;&#30001;&#20110;&#31383;&#21475;&#21270;&#23548;&#33268;&#20102;&#39640;&#39057;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#23545;&#20110;&#36739;&#38271;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#20449;&#24687;&#20002;&#22833;&#36827;&#19968;&#27493;&#22686;&#21152;&#12290;&#20026;&#20102;&#25552;&#21319;&#20302;&#36164;&#28304;LID&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26367;&#20195;&#29305;&#24449;&#34920;&#31034;&#26041;&#27861;&#65292;&#21363;&#23567;&#27874;&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#65292;&#20197;&#24357;&#34917;&#19978;&#36848;&#32570;&#38519;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#22312;LID&#20219;&#21153;&#20013;&#23578;&#26410;&#26377;&#20851;&#20110;WST&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#22810;&#20010;&#21335;&#20122;LID&#35821;&#26009;&#24211;&#23545;WST&#29305;&#24449;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;LID&#38656;&#35201;&#20302;&#20843;&#24230;&#20998;&#36776;&#29575;&#65292;&#32780;&#39057;&#29575;&#25955;&#23556;&#24182;&#19981;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#36328;&#35821;&#26009;&#24211;&#35780;&#20272;&#34920;&#26126;&#65292;&#26368;&#20339;&#30340;WST&#36229;&#21442;&#25968;&#21462;&#20915;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#35821;&#26009;&#24211;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#19981;&#21516;&#30340;WST&#36229;&#21442;&#25968;&#24320;&#21457;&#20102;&#34701;&#21512;&#30340;&#22522;&#20110;ECAPA-TDNN&#30340;LID&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#19982;MFCC&#30456;&#27604;&#65292;EER&#22312;&#21516;&#19968;&#35821;&#26009;&#24211;&#21644;&#30450;&#30446;&#30340;VoxLingua107&#35780;&#20272;&#20013;&#20998;&#21035;&#38477;&#20302;&#20102;14.05%&#21644;6.40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonly used features in spoken language identification (LID), such as mel-spectrogram or MFCC, lose high-frequency information due to windowing. The loss further increases for longer temporal contexts. To improve generalization of the low-resourced LID systems, we investigate an alternate feature representation, wavelet scattering transform (WST), that compensates for the shortcomings. To our knowledge, WST is not explored earlier in LID tasks. We first optimize WST features for multiple South Asian LID corpora. We show that LID requires low octave resolution and frequency-scattering is not useful. Further, cross-corpora evaluations show that the optimal WST hyper-parameters depend on both train and test corpora. Hence, we develop fused ECAPA-TDNN based LID systems with different sets of WST hyper-parameters to improve generalization for unknown data. Compared to MFCC, EER is reduced upto 14.05% and 6.40% for same-corpora and blind VoxLingua107 evaluations, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.00535</link><description>&lt;p&gt;
JoMA: &#36890;&#36807;MLP&#21644;&#27880;&#24847;&#21147;&#30340;&#32852;&#21512;&#21160;&#21147;&#23398;&#26469;&#35299;&#23494;&#22810;&#23618;Transformer
&lt;/p&gt;
&lt;p&gt;
JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention. (arXiv:2310.00535v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;Transformer&#20013;&#21435;&#38500;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#24471;&#21040;&#20165;&#21253;&#21547;MLP&#23618;&#30340;&#20462;&#25913;&#21518;&#21160;&#24577;&#12290;JoMA&#28040;&#38500;&#20102;&#20808;&#21069;&#20998;&#26512;&#20013;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#32570;&#20047;&#27531;&#24046;&#36830;&#25509;&#65289;&#65292;&#24182;&#39044;&#27979;&#27880;&#24847;&#21147;&#22312;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#24773;&#20917;&#19979;&#39318;&#20808;&#21464;&#24471;&#31232;&#30095;&#65288;&#20026;&#20102;&#23398;&#20064;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#28982;&#21518;&#21464;&#24471;&#23494;&#38598;&#65288;&#20026;&#20102;&#23398;&#20064;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#32780;&#22312;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#23427;&#19982;&#29616;&#26377;&#30740;&#31350;&#19968;&#33268;&#65292;&#26174;&#31034;&#20986;&#27880;&#24847;&#21147;&#38543;&#26102;&#38388;&#21464;&#24471;&#31232;&#30095;&#12290;&#25105;&#20204;&#21033;&#29992;JoMA&#23450;&#24615;&#22320;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#22914;&#20309;&#23558;&#26631;&#35760;&#32452;&#21512;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#24403;&#36755;&#20837;&#26631;&#35760;&#26159;&#30001;&#28508;&#22312;&#30340;&#23618;&#27425;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#26102;&#12290;&#22312;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;Wikitext2/Wikitext103&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;OPT&#65292;Pythia&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#20914;&#31361;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#35823;&#23548;&#24615;&#25552;&#31034;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#25351;&#23548;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.17415</link><description>&lt;p&gt;
&#30452;&#35273;&#36824;&#26159;&#20381;&#36182;&#65311;&#30740;&#31350;LLMs&#23545;&#20914;&#31361;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting Prompts. (arXiv:2309.17415v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#20914;&#31361;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#35823;&#23548;&#24615;&#25552;&#31034;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#25351;&#23548;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#23545;&#20854;&#20869;&#37096;&#35760;&#24518;&#25110;&#32473;&#23450;&#25552;&#31034;&#30340;&#20559;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#30001;&#20110;&#22122;&#22768;&#25110;&#20219;&#21153;&#35774;&#32622;&#65292;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#23545;&#31435;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23450;&#37327;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#35282;&#33394;&#25198;&#28436;&#24178;&#39044;&#26469;&#25511;&#21046;LLMs&#30340;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#31181;&#40065;&#26834;&#24615;&#65292;&#21363;&#20107;&#23454;&#40065;&#26834;&#24615;&#21644;&#20915;&#31574;&#39118;&#26684;&#65292;&#20107;&#23454;&#40065;&#26834;&#24615;&#26159;&#25351;&#20174;&#25552;&#31034;&#25110;&#35760;&#24518;&#20013;&#35782;&#21035;&#27491;&#30830;&#20107;&#23454;&#30340;&#33021;&#21147;&#65292;&#32780;&#20915;&#31574;&#39118;&#26684;&#26159;&#22522;&#20110;&#35748;&#30693;&#29702;&#35770;&#23545;LLMs&#22312;&#36827;&#34892;&#19968;&#33268;&#36873;&#25321;&#36807;&#31243;&#20013;&#34892;&#20026;&#30340;&#20998;&#31867; - &#30452;&#35273;&#22411;&#12289;&#20381;&#36182;&#22411;&#25110;&#29702;&#24615;&#22411;&#65292;&#36825;&#37324;&#20551;&#35774;&#27809;&#26377;&#26126;&#30830;&#30340;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#12290;&#25105;&#20204;&#23545;&#19971;&#20010;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#35823;&#23548;&#24615;&#25552;&#31034;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#25351;&#23548;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#12290;&#23613;&#31649;&#35814;&#32454;&#30340;&#35828;&#26126;&#21487;&#20197;&#20943;&#36731;&#36873;&#25321;&#35823;&#23548;&#24615;&#31572;&#26696;&#30340;&#24773;&#20917;&#65292;&#20294;&#20063;&#20250;&#22686;&#21152;&#20986;&#29616;&#19981;&#26126;&#30830;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the robustness of LLMs' preference to their internal memory or the given prompt, which may contain contrasting information in real-world applications due to noise or task settings. To this end, we establish a quantitative benchmarking framework and conduct the role playing intervention to control LLMs' preference. In specific, we define two types of robustness, factual robustness targeting the ability to identify the correct fact from prompts or memory, and decision style to categorize LLMs' behavior in making consistent choices -- assuming there is no definitive "right" answer -intuitive, dependent, or rational based on cognitive theory. Our findings, derived from extensive experiments on seven open-source and closed-source LLMs, reveal that these models are highly susceptible to misleading prompts, especially for instructing commonsense knowledge. While detailed instructions can mitigate the selection of misleading answers, they also increase the incidence of in
&lt;/p&gt;</description></item><item><title>&#22810;&#31181;&#36827;&#21270;&#21387;&#21147;&#20849;&#21516;&#20316;&#29992;&#65292;&#22609;&#36896;&#20102;&#19990;&#30028;&#35821;&#35328;&#20013;&#30456;&#21516;&#36741;&#38899;&#30340;&#36991;&#24524;&#29616;&#35937;&#65292;&#24433;&#21709;&#30528;&#21333;&#35789;&#30340;&#20135;&#29983;&#12289;&#24418;&#21464;&#21644;&#28040;&#22833;&#12290;</title><link>http://arxiv.org/abs/2309.14006</link><description>&lt;p&gt;
&#22810;&#31181;&#36827;&#21270;&#21387;&#21147;&#22609;&#36896;&#20102;&#19990;&#30028;&#35821;&#35328;&#20013;&#30456;&#21516;&#36741;&#38899;&#30340;&#36991;&#35763;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Multiple evolutionary pressures shape identical consonant avoidance in the world's languages. (arXiv:2309.14006v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14006
&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#36827;&#21270;&#21387;&#21147;&#20849;&#21516;&#20316;&#29992;&#65292;&#22609;&#36896;&#20102;&#19990;&#30028;&#35821;&#35328;&#20013;&#30456;&#21516;&#36741;&#38899;&#30340;&#36991;&#24524;&#29616;&#35937;&#65292;&#24433;&#21709;&#30528;&#21333;&#35789;&#30340;&#20135;&#29983;&#12289;&#24418;&#21464;&#21644;&#28040;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#29289;&#21147;&#23398;&#21644;&#35748;&#30693;&#19978;&#30340;&#22256;&#38590;&#65292;&#35821;&#35328;&#19981;&#20542;&#21521;&#20110;&#21253;&#21547;&#30456;&#20284;&#25110;&#30456;&#21516;&#36741;&#38899;&#30340;&#21333;&#35789;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#36127;&#36131;&#36825;&#31181;&#29616;&#35937;&#30340;&#29305;&#23450;&#36827;&#21270;&#36807;&#31243;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#21253;&#21547;&#30456;&#21516;&#36741;&#38899;&#24207;&#21015;&#30340;&#21333;&#35789;&#21487;&#33021;&#27604;&#27809;&#26377;&#36825;&#31181;&#24207;&#21015;&#30340;&#21333;&#35789;&#26356;&#23481;&#26131;&#20135;&#29983;&#65307;&#21333;&#35789;&#24418;&#24335;&#31361;&#21464;&#30340;&#36807;&#31243;&#21487;&#33021;&#26356;&#26377;&#21487;&#33021;&#31227;&#38500;&#32780;&#38750;&#24341;&#20837;&#30456;&#21516;&#36741;&#38899;&#24207;&#21015;&#65307;&#26368;&#21518;&#65292;&#21547;&#26377;&#30456;&#21516;&#36741;&#38899;&#30340;&#21333;&#35789;&#21487;&#33021;&#27604;&#27809;&#26377;&#30340;&#21333;&#35789;&#26356;&#23481;&#26131;&#28040;&#22833;&#12290;&#36890;&#36807;&#23545;&#21516;&#28304;&#35789;&#24418;&#30340;&#36827;&#21270;&#36827;&#34892;&#31995;&#32479;&#21457;&#32946;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#21547;&#26377;&#30456;&#21516;&#36741;&#38899;&#30340;&#21333;&#35789;&#27604;&#27809;&#26377;&#30340;&#21333;&#35789;&#26356;&#23569;&#20986;&#29616;&#65292;&#32780;&#23548;&#33268;&#21333;&#35789;&#24418;&#24335;&#21457;&#29983;&#31361;&#21464;&#30340;&#36807;&#31243;&#26356;&#26377;&#21487;&#33021;&#21024;&#38500;&#30456;&#21516;&#36741;&#38899;&#24207;&#21015;&#32780;&#38750;&#24341;&#20837;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#21547;&#26377;&#30456;&#21516;&#36741;&#38899;&#30340;&#21333;&#35789;&#24182;&#27809;&#26377;&#27604;&#27809;&#26377;&#30340;&#21333;&#35789;&#26356;&#23481;&#26131;&#28040;&#22833;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#21457;&#29616;&#65292;&#24102;&#26377;&#30456;&#21516;&#36741;&#38899;&#30340;&#24418;&#24577;&#19981;&#21516;&#30340;&#21333;&#35789;&#27604;&#30456;&#21516;&#30340;&#21333;&#35789;&#26356;&#21463;&#21040;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages disfavor word forms containing sequences of similar or identical consonants, due to the biomechanical and cognitive difficulties posed by patterns of this sort. However, the specific evolutionary processes responsible for this phenomenon are not fully understood. Words containing sequences of identical consonants may be more likely to arise than those without; processes of word form mutation may be more likely to remove than create sequences of identical consonants in word forms; finally, words containing identical consonants may die out more frequently than those without. Phylogenetic analyses of the evolution of homologous word forms indicate that words with identical consonants arise less frequently than those without, and processes which mutate word forms are more likely to remove sequences of identical consonants than introduce them. However, words with identical consonants do not die out more frequently than those without. Further analyses reveal that forms with identic
&lt;/p&gt;</description></item><item><title>MAmmoTH&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#25968;&#23398;&#38382;&#39064;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#25351;&#20196;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#65292;&#25104;&#21151;&#22320;&#34701;&#21512;&#20102;&#38142;&#29366;&#24605;&#32500;&#21644;&#31243;&#24207;&#32500;&#24605;&#32500;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#30340;&#20934;&#30830;&#29575;&#12290;&#20854;&#20013;&#65292;MAmmoTH-7B&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#22909;&#30340;&#24320;&#28304;7B&#27169;&#22411;WizardMath&#65292;&#24182;&#19988;&#25972;&#20010;&#31995;&#21015;&#27169;&#22411;&#22312;&#21508;&#20010;&#35268;&#27169;&#19978;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;16%&#21040;32%&#20043;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.05653</link><description>&lt;p&gt;
MAmmoTH: &#36890;&#36807;&#28151;&#21512;&#25351;&#20196;&#35843;&#25972;&#26500;&#24314;&#25968;&#23398;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. (arXiv:2309.05653v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05653
&lt;/p&gt;
&lt;p&gt;
MAmmoTH&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#25968;&#23398;&#38382;&#39064;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#25351;&#20196;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#65292;&#25104;&#21151;&#22320;&#34701;&#21512;&#20102;&#38142;&#29366;&#24605;&#32500;&#21644;&#31243;&#24207;&#32500;&#24605;&#32500;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#30340;&#20934;&#30830;&#29575;&#12290;&#20854;&#20013;&#65292;MAmmoTH-7B&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#22909;&#30340;&#24320;&#28304;7B&#27169;&#22411;WizardMath&#65292;&#24182;&#19988;&#25972;&#20010;&#31995;&#21015;&#27169;&#22411;&#22312;&#21508;&#20010;&#35268;&#27169;&#19978;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;16%&#21040;32%&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MAmmoTH&#65292;&#19968;&#31995;&#21015;&#38024;&#23545;&#36890;&#29992;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;MAmmoTH&#27169;&#22411;&#26159;&#22312;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;MathInstruct&#19978;&#35757;&#32451;&#30340;&#12290;MathInstruct&#20174;13&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;&#20013;&#32534;&#35793;&#32780;&#25104;&#65292;&#21253;&#21547;&#20013;&#38388;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20854;&#20013;&#26377;&#20845;&#20010;&#25968;&#25454;&#38598;&#26159;&#30001;&#25105;&#20204;&#26032;&#40092;&#31574;&#21010;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#38142;&#29366;&#24605;&#32500;&#65288;CoT&#65289;&#21644;&#31243;&#24207;&#32500;&#24605;&#32500;&#65288;PoT&#65289;&#25512;&#29702;&#30340;&#28151;&#21512;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#23545;&#25968;&#23398;&#39046;&#22495;&#21508;&#20010;&#26041;&#38754;&#30340;&#24191;&#27867;&#35206;&#30422;&#12290;CoT&#21644;PoT&#30340;&#28151;&#21512;&#19981;&#20165;&#37322;&#25918;&#20102;&#24037;&#20855;&#20351;&#29992;&#30340;&#28508;&#21147;&#65292;&#32780;&#19988;&#20801;&#35768;&#22312;&#19981;&#21516;&#25968;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#24605;&#32771;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;MAmmoTH&#31995;&#21015;&#22312;&#20061;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#22312;&#25152;&#26377;&#35268;&#27169;&#19978;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;16%&#21040;32%&#19981;&#31561;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;MAmmoTH-7B&#27169;&#22411;&#22312;MATH&#65288;&#19968;&#20010;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;&#65289;&#19978;&#36798;&#21040;&#20102;33%&#65292;&#36229;&#36807;&#20102;&#26368;&#22909;&#30340;&#24320;&#28304;7B&#27169;&#22411;&#65288;WizardMath&#65289;2&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 2
&lt;/p&gt;</description></item><item><title>LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16137</link><description>&lt;p&gt;
LM-Infinite: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21363;&#26102;&#38271;&#24230;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16137
&lt;/p&gt;
&lt;p&gt;
LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;Transformer-based&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;LLM&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#30528;&#23545;&#38271;&#26102;&#38388;&#25512;&#29702;&#36807;&#31243;&#25110;&#29702;&#35299;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#26041;&#26696;&#23558;&#35757;&#32451;&#24207;&#21015;&#25130;&#26029;&#21040;&#22266;&#23450;&#38271;&#24230;&#65288;&#20363;&#22914;LLaMa&#30340;2048&#65289;&#12290;&#21363;&#20351;&#20351;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;LLM&#22312;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20043;&#21518;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#26356;&#19981;&#29992;&#35828;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20102;&#12290;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22312;&#26356;&#38271;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#30828;&#20214;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#30340;&#35757;&#32451;&#36807;&#31243;&#35774;&#35745;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#20027;&#35201;&#30340;&#20998;&#24067;&#22806;(OOD) f
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#21457;&#29616;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#65292;&#20294;&#22312;&#38656;&#35201;&#35814;&#32454;&#20449;&#24687;&#26102;&#20173;&#19981;&#22914;&#20256;&#32479;&#24037;&#20855;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2308.07505</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Data Race Detection Using Large Language Models. (arXiv:2308.07505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#21457;&#29616;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#65292;&#20294;&#22312;&#38656;&#35201;&#35814;&#32454;&#20449;&#24687;&#26102;&#20173;&#19981;&#22914;&#20256;&#32479;&#24037;&#20855;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#22312;&#20998;&#26512;&#21644;&#20248;&#21270;&#39640;&#24615;&#33021;&#35745;&#31639;&#31243;&#24207;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#23494;&#38598;&#22411;&#25163;&#21160;&#24037;&#20855;&#30340;&#21019;&#24314;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#25216;&#26415;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;DRB-ML&#30340;&#19987;&#29992;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28304;&#33258;DataRaceBench&#65292;&#24182;&#20855;&#26377;&#31934;&#32454;&#30340;&#26631;&#31614;&#65292;&#26174;&#31034;&#20102;&#25968;&#25454;&#31454;&#20105;&#23545;&#21450;&#20854;&#30456;&#20851;&#21464;&#37327;&#12289;&#34892;&#21495;&#21644;&#35835;/&#20889;&#20449;&#24687;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;DRB-ML&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#30340;LLMs&#24182;&#24494;&#35843;&#20102;&#24320;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#38656;&#35201;&#26377;&#20851;&#24341;&#36215;&#25968;&#25454;&#31454;&#20105;&#30340;&#21464;&#37327;&#23545;&#30340;&#35814;&#32454;&#20449;&#24687;&#26102;&#65292;&#23427;&#20204;&#20173;&#26080;&#27861;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#31454;&#20105;&#26816;&#27979;&#24037;&#20855;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
&lt;/p&gt;</description></item><item><title>ToolLLM&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25484;&#25569;16000&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;API&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#20351;&#29992;ChatGPT&#33258;&#21160;&#26500;&#24314;&#30340;ToolBench&#25968;&#25454;&#38598;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24037;&#20855;&#20351;&#29992;&#24773;&#22659;&#21644;API&#12290;</title><link>http://arxiv.org/abs/2307.16789</link><description>&lt;p&gt;
ToolLLM: &#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25484;&#25569;16000&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;API
&lt;/p&gt;
&lt;p&gt;
ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. (arXiv:2307.16789v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16789
&lt;/p&gt;
&lt;p&gt;
ToolLLM&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25484;&#25569;16000&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;API&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#20351;&#29992;ChatGPT&#33258;&#21160;&#26500;&#24314;&#30340;ToolBench&#25968;&#25454;&#38598;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24037;&#20855;&#20351;&#29992;&#24773;&#22659;&#21644;API&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;LLaMA&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#65292;&#21363;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#65288;API&#65289;&#26469;&#28385;&#36275;&#20154;&#31867;&#25351;&#20196;&#12290;&#21407;&#22240;&#26159;&#24403;&#21069;&#30340;&#25351;&#20196;&#35843;&#25972;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#19978;&#65292;&#20294;&#24573;&#30053;&#20102;&#24037;&#20855;&#20351;&#29992;&#39046;&#22495;&#12290;&#36825;&#19982;&#26368;&#20808;&#36827;&#30340;&#38381;&#28304;LLM&#65288;&#22914;ChatGPT&#65289;&#30340;&#20986;&#33394;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#24418;&#25104;&#23545;&#27604;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ToolLLM&#65292;&#19968;&#20010;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;ToolBench&#65292;&#19968;&#20010;&#29992;&#20110;&#24037;&#20855;&#20351;&#29992;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;ChatGPT&#33258;&#21160;&#26500;&#24314;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26500;&#24314;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;API&#25910;&#38598;&#65306;&#25105;&#20204;&#20174;RapidAPI Hub&#25910;&#38598;&#20102;16464&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;RESTful API&#65292;&#28085;&#30422;&#20102;49&#20010;&#31867;&#21035;&#65307;&#65288;ii&#65289;&#25351;&#20196;&#29983;&#25104;&#65306;&#25105;&#20204;&#25552;&#31034;ChatGPT&#29983;&#25104;&#28041;&#21450;&#36825;&#20123;API&#30340;&#22810;&#26679;&#21270;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;&#21333;&#24037;&#20855;&#21644;&#22810;&#24037;&#20855;&#20351;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-too
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12856</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#35268;&#21010;&#12289;&#38271;&#26399;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#30340;&#29616;&#23454;&#19990;&#30028;WebAgent
&lt;/p&gt;
&lt;p&gt;
A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#20027;Web&#33258;&#21160;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#31449;&#19978;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#19977;&#20010;&#26041;&#38754;&#30340;&#38480;&#21046;&#65306;&#24320;&#25918;&#39046;&#22495;&#24615;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;HTML&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;WebAgent&#36890;&#36807;&#23558;&#25351;&#20196;&#20998;&#35299;&#20026;&#35268;&#33539;&#30340;&#23376;&#25351;&#20196;&#65292;&#23558;&#38271;HTML&#25991;&#26723;&#24635;&#32467;&#20026;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#20174;&#20013;&#29983;&#25104;&#30340;Python&#31243;&#24207;&#23545;&#32593;&#31449;&#36827;&#34892;&#25805;&#20316;&#26469;&#25552;&#21069;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-U-PaLM&#35774;&#35745;&#20102;WebAgent&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#26681;&#20195;&#30721;&#65292;&#24182;&#20351;&#29992;HTML-T5&#36827;&#34892;&#39044;&#35757;&#32451;LLMs&#65292;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#20197;&#21450;&#28151;&#21512;&#38271;&#36328;&#24230;&#21435;&#22122;&#30446;&#26631;&#26469;&#36827;&#34892;&#35268;&#21010;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.12375</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#19978;&#20855;&#26377;&#21019;&#26032;&#65292;&#20294;&#24182;&#38750;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12375
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#21253;&#21547;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#36890;&#24120;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26080;&#20849;&#35782;&#65306;&#20363;&#22914;&#65292;&#34429;&#28982;Xie&#31561;&#20154;&#65288;2021&#24180;&#65289;&#23558;ICL&#27604;&#20316;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;Min&#31561;&#20154;&#65288;2022b&#24180;&#65289;&#35748;&#20026;ICL&#29978;&#33267;&#19981;&#33021;&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#65288;2&#65289;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#36755;&#20837;-&#26631;&#31614;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#65288;3&#65289;ICL&#22914;&#20309;&#32858;&#21512;&#26469;&#33258;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#36890;&#24120;&#20250;&#25972;&#21512;&#19978;&#19979;&#25991;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20294;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#31995;&#34987;&#21306;&#21035;&#23545;&#24453;&#65292;&#27169;&#22411;&#19981;&#20250;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#21516;&#23545;&#24453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;LLMs&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02738</link><description>&lt;p&gt;
RecallM:&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#38382;&#39064;&#22238;&#31572;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#23558;&#20026;&#36830;&#32493;&#23398;&#20064;&#12289;&#22797;&#26434;&#25512;&#29702;&#21644;&#23398;&#20064;&#24207;&#21015;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#25171;&#19979;&#22522;&#30784;&#12290;&#21019;&#24314;&#36825;&#31181;&#31867;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#19987;&#27880;&#20110;&#20026;AGI&#31995;&#32479;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#23637;&#31034;&#20102;RecallM&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#23427;&#25552;&#20379;&#30340;&#25913;&#36827;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ideal long-term memory mechanism for Large Language Model (LLM) based chatbots, would lay the foundation for continual learning, complex reasoning and allow sequential and temporal dependencies to be learnt. Creating this type of memory mechanism is an extremely challenging problem. In this paper we explore different methods of achieving the effect of long-term memory. We propose a new architecture focused on creating adaptable and updatable long-term memory for AGI systems. We demonstrate through various experiments the benefits of the RecallM architecture, particularly the improved temporal understanding it provides.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13649</link><description>&lt;p&gt;
GKD&#65306;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#33976;&#39311;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#35757;&#32451;&#26399;&#38388;&#36755;&#20986;&#24207;&#21015;&#21644;&#37096;&#32626;&#26102;&#30001;&#23398;&#29983;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20043;&#38388;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#65288;2&#65289;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#33021;&#19981;&#22815;&#34920;&#36798;&#32769;&#24072;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#12290;GKD&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;GKD&#36890;&#36807;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26469;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36825;&#20123;&#31163;&#25955;&#24230;&#38598;&#20013;&#20110;&#29983;&#25104;&#21487;&#33021;&#31526;&#21512;&#32769;&#24072;&#20998;&#24067;&#30340;&#23398;&#29983;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;GKD&#20248;&#20110;&#24120;&#29992;&#30340;LLM&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13300</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#20013;&#30340;&#34892;&#20026;&#25581;&#31192;&#65306;&#33258;&#36866;&#24212;&#21464;&#33394;&#40857;&#36824;&#26159;&#22266;&#25191;&#30340;&#26641;&#29549;
&lt;/p&gt;
&lt;p&gt;
Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#22806;&#37096;&#20449;&#24687;&#65292;&#24037;&#20855;&#22686;&#24378;&#65288;&#21253;&#25324;&#26816;&#32034;&#22686;&#24378;&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;LLMs&#38745;&#24577;&#21442;&#25968;&#21270;&#20869;&#23384;&#38480;&#21046;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#35777;&#25454;&#19982;&#23427;&#20204;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;LLMs&#23545;&#36825;&#20123;&#22806;&#37096;&#35777;&#25454;&#26377;&#22810;&#23569;&#25509;&#21463;&#33021;&#21147;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#20174;LLMs&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#65292;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#23545;&#31435;&#20869;&#23384;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#31995;&#21015;&#21463;&#25511;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;LLMs&#34920;&#29616;&#20986;&#30475;&#20284;&#30683;&#30462;&#30340;&#34892;&#20026;&#12290;&#19968;&#26041;&#38754;&#65292;&#19982;&#20197;&#24448;&#30340;&#35266;&#24565;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#35201;&#22806;&#37096;&#35777;&#25454;&#26159;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#65292;LLMs&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#20063;&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#35777;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLMs&#20063;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#21463;&#21040;&#23041;&#32961;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also d
&lt;/p&gt;</description></item><item><title>Chain-of-Knowledge&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#21160;&#24577;&#30693;&#35782;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#30784;&#20449;&#24687;&#65292;&#20943;&#23569;&#29983;&#25104;&#30340;&#24187;&#35273;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.13269</link><description>&lt;p&gt;
Chain-of-Knowledge:&#36890;&#36807;&#22810;&#28304;&#21160;&#24577;&#30693;&#35782;&#36866;&#24212;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#30784;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. (arXiv:2305.13269v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13269
&lt;/p&gt;
&lt;p&gt;
Chain-of-Knowledge&#36890;&#36807;&#25972;&#21512;&#22810;&#28304;&#21160;&#24577;&#30693;&#35782;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#30784;&#20449;&#24687;&#65292;&#20943;&#23569;&#29983;&#25104;&#30340;&#24187;&#35273;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#38142;&#24335;&#30693;&#35782;&#65288;CoK&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#22522;&#30784;&#20449;&#24687;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;&#23427;&#21487;&#20197;&#20135;&#29983;&#26356;&#22810;&#30340;&#20107;&#23454;&#20381;&#25454;&#65292;&#20943;&#23569;&#29983;&#25104;&#30340;&#24187;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CoK&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#25512;&#29702;&#20934;&#22791;&#12289;&#21160;&#24577;&#30693;&#35782;&#36866;&#24212;&#21644;&#31572;&#26696;&#25972;&#21512;&#12290;&#32473;&#23450;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#39064;&#65292;CoK&#39318;&#20808;&#20934;&#22791;&#33509;&#24178;&#20010;&#21021;&#27493;&#30340;&#20381;&#25454;&#21644;&#31572;&#26696;&#65292;&#21516;&#26102;&#35782;&#21035;&#20986;&#30456;&#20851;&#30340;&#30693;&#35782;&#39046;&#22495;&#12290;&#22914;&#26524;&#26679;&#26412;&#20013;&#30340;&#31572;&#26696;&#27809;&#26377;&#22810;&#25968;&#20849;&#35782;&#65292;CoK&#36890;&#36807;&#20174;&#35782;&#21035;&#20986;&#30340;&#39046;&#22495;&#20013;&#36880;&#27493;&#36866;&#24212;&#30693;&#35782;&#26469;&#32416;&#27491;&#20381;&#25454;&#12290;&#36825;&#20123;&#32416;&#27491;&#21518;&#30340;&#20381;&#25454;&#21487;&#20197;&#26356;&#22909;&#22320;&#20316;&#20026;&#26368;&#32456;&#31572;&#26696;&#25972;&#21512;&#30340;&#22522;&#30784;&#12290;&#19981;&#21516;&#20110;&#20043;&#21069;&#20027;&#35201;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#30740;&#31350;&#65292;CoK&#36824;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#28304;&#65292;&#22914;Wikidata&#21644;&#34920;&#26684;&#65292;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructure
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#35266;&#30693;&#35782;&#39537;&#21160;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#21046;&#20316;&#20102;&#30456;&#24212;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#38754;&#20020;&#30528;&#26032;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#22914;&#20309;&#27719;&#24635;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#20013;&#30340;&#19981;&#21516;&#24847;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.12091</link><description>&lt;p&gt;
&#20027;&#35266;&#30693;&#35782;&#39537;&#21160;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
"What do others think?": Task-Oriented Conversational Modeling with Subjective Knowledge. (arXiv:2305.12091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#35266;&#30693;&#35782;&#39537;&#21160;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#21046;&#20316;&#20102;&#30456;&#24212;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#38754;&#20020;&#30528;&#26032;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#22914;&#20309;&#27719;&#24635;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#20013;&#30340;&#19981;&#21516;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#31995;&#32479;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#23436;&#25104;&#29305;&#23450;&#30446;&#26631;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#20363;&#22914;&#39044;&#23450;&#37202;&#24215;&#25110;&#39184;&#21381;&#12290;&#20256;&#32479;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#31995;&#32479;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;API/&#25968;&#25454;&#24211;&#25110;&#22806;&#37096;&#20107;&#23454;&#30693;&#35782;&#26469;&#29983;&#25104;&#21709;&#24212;&#65292;&#26080;&#27861;&#28385;&#36275;&#20027;&#35266;&#29992;&#25143;&#35831;&#27714;&#65288;&#20363;&#22914;&#8220;Wi-Fi&#21487;&#38752;&#21527;&#65311;&#8221;&#25110;&#8220;&#39184;&#21381;&#29615;&#22659;&#22909;&#21527;&#65311;&#8221;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20027;&#35266;&#30693;&#35782;&#39537;&#21160;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#24314;&#27169;&#65288;SK-TOD&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20027;&#35266;&#30693;&#35782;&#23547;&#27714;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#22522;&#20110;&#20027;&#35266;&#30693;&#35782;&#26469;&#28304;&#30340;&#21709;&#24212;&#12290;&#22312;&#19982;&#29616;&#26377;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#20219;&#21153;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22914;&#22914;&#20309;&#27719;&#24635;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#20013;&#30340;&#19981;&#21516;&#24847;&#35265;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#33021;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#21644;&#20027;&#35266;&#20869;&#23481;&#29702;&#35299;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312; https://github.com/alex &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that assist users in accomplishing specific goals, such as booking a hotel or a restaurant. Traditional TODs rely on domain-specific APIs/DBs or external factual knowledge to generate responses, which cannot accommodate subjective user requests (e.g., "Is the WIFI reliable?" or "Does the restaurant have a good atmosphere?"). To address this issue, we propose a novel task of subjective-knowledge-based TOD (SK-TOD). We also propose the first corresponding dataset, which contains subjective knowledge-seeking dialogue contexts and manually annotated responses grounded in subjective knowledge sources. When evaluated with existing TOD approaches, we find that this task poses new challenges such as aggregating diverse opinions from multiple knowledge snippets. We hope this task and dataset can promote further research on TOD and subjective content understanding. The code and the dataset are available at https://github.com/alex
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;MildTriple Loss&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#24182;&#27169;&#25311;&#36328;&#27169;&#24577;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.04195</link><description>&lt;p&gt;
MildTriple Loss&#27169;&#22411;&#19979;&#30340;&#36816;&#21160;&#21644;&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;MildTriple Loss&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#24182;&#27169;&#25311;&#36328;&#27169;&#24577;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#65292;&#38543;&#30528;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#23613;&#31649;&#22312;&#34394;&#25311;&#29616;&#23454;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#23578;&#26410;&#24341;&#36215;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#20219;&#21153;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#20004;&#31181;&#35821;&#35328;&#30340;&#20849;&#21516;&#24314;&#27169;&#65292;&#35201;&#27714;&#20174;&#25991;&#26412;&#20013;&#29702;&#35299;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#24687;&#65292;&#24182;&#20174;&#19977;&#32500;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#20013;&#23398;&#20064;&#34892;&#20026;&#29305;&#24449;&#12290;&#20197;&#24448;&#30340;&#36816;&#21160;&#25968;&#25454;&#24314;&#27169;&#20027;&#35201;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36825;&#21487;&#33021;&#20250;&#36951;&#24536;&#20197;&#21069;&#30340;&#20449;&#24687;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#36816;&#21160;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#20174;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#23398;&#20064;&#34920;&#31034;&#24182;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Cross-modal retrieval has become a prominent research topic in computer vision and natural language processing with advances made in image-text and video-text retrieval technologies. However, cross-modal retrieval between human motion sequences and text has not garnered sufficient attention despite the extensive application value it holds, such as aiding virtual reality applications in better understanding users' actions and language. This task presents several challenges, including joint modeling of the two modalities, demanding the understanding of person-centered information from text, and learning behavior features from 3D human motion sequences. Previous work on motion data modeling mainly relied on autoregressive feature extractors that may forget previous information, while we propose an innovative model that includes simple yet powerful transformer-based motion and text encoders, which can learn representations from the two different modalities and capture long-term dependencie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.08612</link><description>&lt;p&gt;
&#31163;&#25955;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#26725;&#26753;&#65306;&#30452;&#36890;&#27861;&#19982;&#20854;&#23427;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#20165;&#38480;&#20110;&#35745;&#31639;&#36830;&#32493;&#21464;&#37327;&#30340;&#26799;&#24230;&#65292;&#38480;&#21046;&#20102;&#28041;&#21450;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340; Straight-Through&#65288;ST&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26799;&#24230;&#30340;&#19968;&#38454;&#36817;&#20284;&#20540;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; ReinMax&#65292;&#23427;&#38598;&#25104;&#20102; Heun's Method&#65292;&#19968;&#31181;&#35299;ODE&#30340;&#20108;&#38454;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201; Hessian &#25110;&#20854;&#20182;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;\ours &#22312;&#29616;&#26377;&#25216;&#26415;&#20013;&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324; ST &#21644; Straight-Through Gum&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.04736</link><description>&lt;p&gt;
&#20851;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#30528;&#30524;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#20197;&#21306;&#20998;&#20854;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#36825;&#39033;&#33021;&#21147;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#31181;&#21306;&#20998;&#30340;&#21487;&#33021;&#24615;&#19968;&#30452;&#26159;&#35813;&#39046;&#22495;&#20869;&#30340;&#20105;&#35758;&#35805;&#39064;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#22914;&#26524;&#33021;&#65292;&#20309;&#26102;&#33021;&#26816;&#27979;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#38500;&#38750;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#24067;&#22312;&#25972;&#20010;&#25903;&#25345;&#20013;&#23436;&#20840;&#30456;&#21516;&#65292;&#21542;&#21017;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#26469;&#33258;&#20110;&#20449;&#24687;&#35770;&#20013;&#30340;&#26631;&#20934;&#32467;&#26524;&#65292;&#24182;&#20381;&#36182;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#20687;&#20154;&#31867;&#65292;&#25105;&#20204;&#23601;&#38656;&#35201;&#26356;&#22810;&#30340;&#26679;&#26412;&#26469;&#26816;&#27979;&#23427;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#31934;&#30830;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#21578;&#35785;&#38656;&#35201;&#22810;&#23569;&#20010;&#26679;&#26412;&#25165;&#33021;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#24341;&#36215;&#20102;&#26356;&#22810;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;LLM&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.02676</link><description>&lt;p&gt;
&#22238;&#39038;&#38142;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#21453;&#39304;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#36825;&#26679;&#25165;&#33021;&#23545;&#20154;&#31867;&#26377;&#25152;&#24110;&#21161;&#24182;&#31526;&#21512;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#26469;&#29702;&#35299;&#21644;&#36981;&#24490;&#25351;&#20196;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26159;&#22522;&#20110;&#34987;&#20154;&#31867;&#27880;&#37322;&#32773;&#21916;&#27426;&#30340;&#25163;&#21160;&#25361;&#36873;&#30340;&#27169;&#22411;&#29983;&#25104;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#19988;&#26222;&#36941;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22870;&#21169;&#20989;&#25968;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36825;&#23481;&#26131;&#20986;&#29616;&#22870;&#21169;&#20989;&#25968;&#19981;&#23436;&#32654;&#21644;&#26497;&#38590;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#8220;&#22238;&#39038;&#38142;&#8221;&#65292;&#23427;&#26131;&#20110;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#21463;&#21040;&#20102;&#20154;&#31867;&#22914;&#20309;&#20174;&#20197;&#35821;&#35328;&#24418;&#24335;&#21576;&#29616;&#30340;&#24191;&#27867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#31867;&#22411;&#30340;&#21453;&#39304;&#36716;&#25442;&#25104;&#21477;&#23376;&#65292;&#28982;&#21518;&#29992;&#23427;&#20204;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#20174;&#32780;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;FLAD&#65292;&#24182;&#38024;&#23545;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#23616;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;FLAD&#21644;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#27604;&#36739;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.00674</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#26469;&#25913;&#21892;&#23567;&#26679;&#26412;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;FLAD&#65292;&#24182;&#38024;&#23545;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#23616;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;FLAD&#21644;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#27604;&#36739;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#26377;&#20215;&#20540;&#65292;&#20294;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#22411;&#19988;&#19981;&#36807;&#24230;&#25311;&#21512;&#23569;&#25968;&#26631;&#35760;&#25968;&#25454;&#28857;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20851;&#27880;&#36741;&#21161;&#25968;&#25454;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65288;FLAD&#65289;&#65292;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#26399;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#20102;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38543;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#21576;&#32447;&#24615;&#65288;&#25110;&#26356;&#24046;&#65289;&#32553;&#25918;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;FLAD&#19982;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#32622;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25506;&#32034;&#19982;&#21033;&#29992;&#22256;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25512;&#23548;&#20986;&#31639;&#27861;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#25193;&#23637;&#21040;&#27604;&#20808;&#21069;&#26041;&#27861;&#22810;100&#20493;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#8212;&#8212;EXP3-FLAD&#21644;UCB1-FLAD&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20808;&#21069;&#21482;&#36827;&#34892;&#25506;&#32034;&#25110;&#21033;&#29992;&#30340;FLAD&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging. In this work, we focus on Few-shot Learning with Auxiliary Data (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization. Previous works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality. In this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets than prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and compare them with prior FLAD methods that either explore or exploit, finding that the com
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26597;&#35810;&#37325;&#20889;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#21160;&#23398;&#20064;&#32534;&#36753;&#25805;&#20316;&#65292;&#20197;&#25552;&#39640;&#24050;&#30693;&#35823;&#20256;&#20027;&#24352;&#30340;&#26597;&#35810;&#25928;&#26524;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20351;&#26597;&#35810;&#26377;&#25928;&#24615;&#22686;&#21152;&#22810;&#36798;42%&#12290;</title><link>http://arxiv.org/abs/2210.07467</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#21457;&#29616;&#38169;&#35823;&#20449;&#24687;&#30340;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Query Rewriting for Effective Misinformation Discovery. (arXiv:2210.07467v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07467
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26597;&#35810;&#37325;&#20889;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#21160;&#23398;&#20064;&#32534;&#36753;&#25805;&#20316;&#65292;&#20197;&#25552;&#39640;&#24050;&#30693;&#35823;&#20256;&#20027;&#24352;&#30340;&#26597;&#35810;&#25928;&#26524;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20351;&#26597;&#35810;&#26377;&#25928;&#24615;&#22686;&#21152;&#22810;&#36798;42%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#32773;&#21046;&#23450;&#24050;&#30693;&#38169;&#35823;&#20449;&#24687;&#20027;&#24352;&#30340;&#25628;&#32034;&#26597;&#35810;&#65292;&#24182;&#22312;&#22810;&#20010;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#36827;&#34892;&#26377;&#25928;&#25628;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#37325;&#20889;&#31574;&#30053;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#23398;&#20064;&#21253;&#21547;&#20027;&#24352;&#30340;&#26597;&#35810;&#30340;&#32534;&#36753;&#25805;&#20316;&#65288;&#20363;&#22914;&#65292;&#29992;&#21516;&#20041;&#35789;&#26367;&#25442;&#19968;&#20010;&#35789;&#65307;&#23558;&#21160;&#35789;&#26102;&#24577;&#25913;&#20026;&#19968;&#33324;&#29616;&#22312;&#26102;&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20915;&#31574;&#36716;&#25442;&#22120;&#23398;&#20064;&#19968;&#31995;&#21015;&#32534;&#36753;&#25805;&#20316;&#65292;&#20197;&#26368;&#22823;&#21270;&#26597;&#35810;&#26816;&#32034;&#25351;&#26631;&#65292;&#20363;&#22914;&#24179;&#22343;&#31934;&#30830;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26597;&#35810;&#37325;&#20889;&#31995;&#32479;&#30456;&#23545;&#20110;&#26597;&#35810;&#30340;&#26377;&#25928;&#24615;&#22686;&#21152;&#20102;&#22810;&#36798;42%&#65292;&#21516;&#26102;&#20135;&#29983;&#30340;&#32534;&#36753;&#25805;&#20316;&#24207;&#21015;&#26159;&#21487;&#20154;&#24037;&#35299;&#37322;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel system to help fact-checkers formulate search queries for known misinformation claims and effectively search across multiple social media platforms. We introduce an adaptable rewriting strategy, where editing actions for queries containing claims (e.g., swap a word with its synonym; change verb tense into present simple) are automatically learned through offline reinforcement learning. Our model uses a decision transformer to learn a sequence of editing actions that maximizes query retrieval metrics such as mean average precision. We conduct a series of experiments showing that our query rewriting system achieves a relative increase in the effectiveness of the queries of up to 42%, while producing editing action sequences that are human interpretable.
&lt;/p&gt;</description></item><item><title>DialoGen&#26159;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#24191;&#20041;&#19978;&#19979;&#25991;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.06282</link><description>&lt;p&gt;
DialoGen: &#23545;&#35805;&#31995;&#32479;&#30340;&#24191;&#20041;&#38271;&#31243;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DialoGen: Generalized Long-Range Context Representation for Dialogue Systems. (arXiv:2210.06282v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06282
&lt;/p&gt;
&lt;p&gt;
DialoGen&#26159;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#24191;&#20041;&#19978;&#19979;&#25991;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31243;&#19978;&#19979;&#25991;&#24314;&#27169;&#23545;&#20110;&#23545;&#35805;&#29702;&#35299;&#21644;&#29983;&#25104;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#35805;&#19978;&#19979;&#25991;&#34920;&#31034;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#23558;&#26368;&#21518;-k&#20010;&#20808;&#21069;&#30340;&#35805;&#35821;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21253;&#21547;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#30340;&#23545;&#35805;&#32780;&#35328;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#19981;&#26159;&#26368;&#29702;&#24819;&#30340;&#65292;&#22240;&#20026;&#23427;&#26080;&#27861;&#36229;&#36234;&#26368;&#21518;-k&#20010;&#35805;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DialoGen&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#21487;&#20197;&#36229;&#36234;&#26368;&#21518;-k&#20010;&#35805;&#35821;&#30340;&#24191;&#20041;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#36866;&#24212;&#20855;&#26377;&#38271;&#31243;&#20381;&#36182;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#35782;&#21035;&#21644;&#21033;&#29992;&#26368;&#30456;&#20851;&#30340;&#21382;&#21490;&#35805;&#35821;&#65292;&#32780;&#19981;&#26159;&#25353;&#26102;&#38388;&#39034;&#24207;&#30340;&#26368;&#21518;-k&#20010;&#35805;&#35821;&#12290;&#25105;&#20204;&#22312;&#23545;&#35805;&#29983;&#25104;&#65288;&#24320;&#25918;&#22495;&#65289;&#21644;&#29702;&#35299;&#65288;DST&#65289;&#20219;&#21153;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;DialoGen&#22312;DailyDialog&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-range context modeling is crucial to both dialogue understanding and generation. The most popular method for dialogue context representation is to concatenate the last-$k$ previous utterances. However, this method may not be ideal for conversations containing long-range dependencies as it cannot look beyond last-$k$ utterances. In this work, we propose DialoGen, a novel encoder-decoder based framework for conversational response generation with a generalized context representation that can look beyond the last-$k$ utterances. Hence the method is adaptive to conversations with long-range dependencies. The main idea of our approach is to identify and utilize the most relevant historical utterances instead of the last-$k$ utterances in chronological order. We study the effectiveness of our proposed method on both dialogue generation (open-domain) and understanding (DST) tasks. DialoGen achieves comparable performance with the state-of-the-art models on DailyDialog dataset. We also ob
&lt;/p&gt;</description></item><item><title>FRMT&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#21306;&#22495;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#19987;&#19994;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#23545;&#35789;&#27719;&#19978;&#19981;&#21516;&#30340;&#26415;&#35821;&#21644;&#24178;&#25200;&#39033;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#21644;&#25351;&#23548;&#65292;&#20379;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.00193</link><description>&lt;p&gt;
FRMT: &#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#21306;&#22495;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation. (arXiv:2210.00193v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00193
&lt;/p&gt;
&lt;p&gt;
FRMT&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#21306;&#22495;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#19987;&#19994;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#23545;&#35789;&#27719;&#19978;&#19981;&#21516;&#30340;&#26415;&#35821;&#21644;&#24178;&#25200;&#39033;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#21644;&#25351;&#23548;&#65292;&#20379;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FRMT&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#21306;&#22495;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39118;&#26684;&#30340;&#32763;&#35793;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20174;&#33521;&#35821;&#21040;&#33889;&#33796;&#29273;&#35821;&#21644;&#20013;&#25991;&#26222;&#36890;&#35805;&#30340;&#20004;&#31181;&#22320;&#21306;&#21464;&#20307;&#30340;&#19987;&#19994;&#32763;&#35793;&#12290;&#36873;&#25321;&#28304;&#25991;&#26723;&#65292;&#20197;&#20415;&#33021;&#22815;&#23545;&#24863;&#20852;&#36259;&#30340;&#29616;&#35937;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#21253;&#25324;&#35789;&#27719;&#19978;&#19981;&#21516;&#30340;&#26415;&#35821;&#21644;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#38024;&#23545;FRMT&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#39564;&#35777;&#20102;&#22312;&#22320;&#21306;&#21305;&#37197;&#21644;&#19981;&#21305;&#37197;&#35780;&#20998;&#22330;&#26223;&#19979;&#19982;&#19987;&#23478;&#20154;&#24037;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#38024;&#23545;&#36825;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#27604;&#36739;&#20182;&#20204;&#33258;&#24049;&#27169;&#22411;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#65306;https://bit.ly/frmt-task
&lt;/p&gt;
&lt;p&gt;
We present FRMT, a new dataset and evaluation benchmark for Few-shot Region-aware Machine Translation, a type of style-targeted translation. The dataset consists of professional translations from English into two regional variants each of Portuguese and Mandarin Chinese. Source documents are selected to enable detailed analysis of phenomena of interest, including lexically distinct terms and distractor terms. We explore automatic evaluation metrics for FRMT and validate their correlation with expert human evaluation across both region-matched and mismatched rating scenarios. Finally, we present a number of baseline models for this task, and offer guidelines for how researchers can train, evaluate, and compare their own models. Our dataset and evaluation code are publicly available: https://bit.ly/frmt-task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;Sci-Net&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#33539;&#22260;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.06812</link><description>&lt;p&gt;
Sci-Net: &#23610;&#24230;&#19981;&#21464;&#27169;&#22411;&#29992;&#20110;&#20174;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;
&lt;/p&gt;
&lt;p&gt;
Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial Imagery. (arXiv:2111.06812v5 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;Sci-Net&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#33539;&#22260;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#29289;&#30340;&#20998;&#21106;&#26159;&#22320;&#29699;&#35266;&#27979;&#21644;&#33322;&#31354;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#22823;&#37096;&#20998;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21482;&#33021;&#24212;&#29992;&#20110;&#22266;&#23450;&#25110;&#29421;&#31364;&#33539;&#22260;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#22788;&#29702;&#30340;&#26159;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#36776;&#29575;&#33539;&#22260;&#12290;&#22240;&#27492;&#65292;&#32473;&#23450;&#30340;&#33322;&#31354;&#22270;&#20687;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#37325;&#37319;&#26679;&#20197;&#21305;&#37197;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#65292;&#36825;&#23548;&#33268;&#20998;&#21106;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#23610;&#24230;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;Sci-Net&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#33539;&#22260;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#33322;&#31354;&#22270;&#20687;&#20013;&#20998;&#21106;&#24314;&#31569;&#29289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;UNet&#30340;&#23618;&#27425;&#34920;&#31034;&#21644;Dense Atrous Spatial Pyramid Pooling&#26469;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#22810;&#23610;&#24230;&#34920;&#31034;&#12290;Sci-Net&#22312;Open Cities AI&#21644;Multi-Scale Building&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Buildings' segmentation is a fundamental task in the field of earth observation and aerial imagery analysis. Most existing deep learning-based methods in the literature can be applied to a fixed or narrow-range spatial resolution imagery. In practical scenarios, users deal with a broad spectrum of image resolutions. Thus, a given aerial image often needs to be re-sampled to match the spatial resolution of the dataset used to train the deep learning model, which results in a degradation in segmentation performance. To overcome this challenge, we propose, in this manuscript, Scale-invariant Neural Network (Sci-Net) architecture that segments buildings from wide-range spatial resolution aerial images. Specifically, our approach leverages UNet hierarchical representation and Dense Atrous Spatial Pyramid Pooling to extract fine-grained multi-scale representations. Sci-Net significantly outperforms state of the art models on the Open Cities AI and the Multi-Scale Building datasets with a ste
&lt;/p&gt;</description></item></channel></rss>