<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>CHARM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20840;&#38754;&#28145;&#20837;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;LLM&#30340;&#35821;&#35328;&#23548;&#21521;&#24615;&#21644;&#20219;&#21153;&#39046;&#22495;&#20250;&#24433;&#21709;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#19968;&#20123;LLMs&#22312;&#35760;&#24518;&#20013;&#25991;&#24120;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;LLMs&#22312;&#25512;&#29702;&#19978;&#34920;&#29616;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.14112</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65306;&#20174;&#20013;&#25991;&#29305;&#23450;&#21040;&#25512;&#29702;-&#35760;&#24518;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14112
&lt;/p&gt;
&lt;p&gt;
CHARM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20840;&#38754;&#28145;&#20837;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;LLM&#30340;&#35821;&#35328;&#23548;&#21521;&#24615;&#21644;&#20219;&#21153;&#39046;&#22495;&#20250;&#24433;&#21709;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#19968;&#20123;LLMs&#22312;&#35760;&#24518;&#20013;&#25991;&#24120;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;LLMs&#22312;&#25512;&#29702;&#19978;&#34920;&#29616;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CHARM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20840;&#38754;&#28145;&#20837;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25991;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20840;&#29699;&#24050;&#30693;&#21644;&#20013;&#25991;&#29305;&#26377;&#30340;&#24120;&#35782;&#12290;&#22312;CHARM&#19978;&#35780;&#20272;&#20102;7&#20010;&#33521;&#25991;&#21644;12&#20010;&#20013;&#25991;&#23450;&#21521;LLMs&#65292;&#37319;&#29992;&#20102;5&#31181;&#20195;&#34920;&#24615;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#27604;&#22914;&#24605;&#32500;&#38142;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#30340;&#35821;&#35328;&#23548;&#21521;&#24615;&#21644;&#20219;&#21153;&#39046;&#22495;&#24433;&#21709;&#20102;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20016;&#23500;&#20102;&#20197;&#24448;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#32039;&#23494;&#20851;&#32852;&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#19968;&#20123;LLMs&#22312;&#35760;&#24518;&#20013;&#25991;&#24120;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;LLMs&#22312;&#25512;&#29702;&#19978;&#34920;&#29616;&#23384;&#22312;&#24046;&#24322;&#65292;&#23613;&#31649;&#35760;&#24518;&#34920;&#29616;&#30456;&#20284;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;LLMs&#30340;&#19982;&#35760;&#24518;&#26080;&#20851;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#20998;&#26512;&#20102;&#20856;&#22411;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14112v1 Announce Type: new  Abstract: We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study pr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.09611</link><description>&lt;p&gt;
MM1&#65306;&#22810;&#27169;&#24335;LLM&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12289;&#20998;&#26512;&#19982;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20180;&#32454;&#21644;&#20840;&#38754;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#32463;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24335;&#39044;&#35757;&#32451;&#20351;&#29992;&#20180;&#32454;&#28151;&#21512;&#30340;&#22270;&#20687;&#26631;&#39064;&#12289;&#20132;&#26367;&#22270;&#20687;&#25991;&#26412;&#21644;&#20165;&#25991;&#26412;&#25968;&#25454;&#23545;&#20110;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#26032;&#28526;&#65288;SOTA&#65289;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#39044;&#35757;&#32451;&#32467;&#26524;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#22270;&#20687;&#32534;&#30721;&#22120;&#36830;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#22270;&#20687;&#26631;&#35760;&#35745;&#25968;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#35774;&#35745;&#30456;&#23545;&#37325;&#35201;&#24615;&#36739;&#23567;&#12290;&#36890;&#36807;&#25193;&#22823;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MM1&#65292;&#19968;&#20010;&#22810;&#27169;&#24335;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#20195;&#30721;&#28151;&#21512;&#31561;&#25163;&#27573;&#65292;&#38477;&#20302;&#22312;&#24403;&#20195;LLMs&#20013;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#20219;&#21153;&#30340;&#25104;&#26412;&#65292;&#20197;&#30830;&#20445;&#39044;&#27979;&#21644;&#29983;&#25104;&#36136;&#37327;&#19981;&#21463;&#25439;&#12290;</title><link>https://arxiv.org/abs/2403.05434</link><description>&lt;p&gt;
&#20351;&#29992;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#20219;&#21153;&#30340;&#25104;&#26412;&#19982;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#20195;&#30721;&#28151;&#21512;&#31561;&#25163;&#27573;&#65292;&#38477;&#20302;&#22312;&#24403;&#20195;LLMs&#20013;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#20219;&#21153;&#30340;&#25104;&#26412;&#65292;&#20197;&#30830;&#20445;&#39044;&#27979;&#21644;&#29983;&#25104;&#36136;&#37327;&#19981;&#21463;&#25439;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;/&#23569;&#36718;&#25512;&#29702;&#21644;&#29983;&#25104;&#36136;&#37327;&#12290;&#20854;&#20013;&#26377;&#19968;&#20123;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;(LRLs)&#19978;&#35757;&#32451;&#24182;&#34920;&#29616;&#20986;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#35757;&#32451;LLMs&#30340;&#25104;&#26412;&#26497;&#39640;&#65292;&#23427;&#20204;&#36890;&#24120;&#34987;&#29992;&#20316;&#32593;&#32476;&#26381;&#21153;&#65292;&#23458;&#25143;&#26681;&#25454;&#36755;&#20837;&#21644;&#36755;&#20986;&#20196;&#29260;&#30340;&#25968;&#37327;&#20184;&#36153;&#12290;&#20196;&#29260;&#25968;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#33050;&#26412;&#21644;&#35821;&#35328;&#65292;&#20197;&#21450;LLM&#30340;&#23376;&#35789;&#27719;&#34920;&#12290;&#25105;&#20204;&#34920;&#26126;LRLs&#22312;&#23450;&#20215;&#19978;&#22788;&#20110;&#19981;&#21033;&#20301;&#32622;&#65292;&#22240;&#20026;&#20247;&#25152;&#21608;&#30693;&#65292;&#23545;&#20110;LRLs&#65292;&#30693;&#21517;LLMs&#20135;&#29983;&#30340;&#20196;&#29260;&#27604;HRLs&#22810;&#12290;&#36825;&#26159;&#22240;&#20026;&#30446;&#21069;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;LLMs&#37117;&#38024;&#23545;HRL&#35789;&#27719;&#34920;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#20445;&#35777;&#39044;&#27979;&#21644;&#29983;&#25104;&#36136;&#37327;&#19981;&#21463;&#25439;&#30340;&#21516;&#26102;&#65292;&#35843;&#25972;&#24179;&#34913;&#65306;&#38477;&#20302;&#22312;&#24403;&#20195;LLMs&#20013;&#22788;&#29702;LRLs&#30340;&#25104;&#26412;&#12290;&#20316;&#20026;&#20943;&#23569;LLM&#22788;&#29702;&#30340;&#20196;&#29260;&#25968;&#37327;&#30340;&#25163;&#27573;&#65292;&#25105;&#20204;&#32771;&#34385;&#20195;&#30721;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05434v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit impressive zero/few-shot inference and generation quality for high-resource languages(HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The number of tokens strongly depends on the script and language, as well as the LLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs. This is because most currently popular LLMs are optimized for HRL vocabularies. Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary LLMs while ensuring that predictive and generative qualities are not compromised. As means to reduce the number of tokens processed by the LLM, we consider code-mixing,
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.05020</link><description>&lt;p&gt;
&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#25104;&#21151;&#24615;&#30340;&#35823;&#23548;&#24615;&#65306;&#20197;LLMs&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#31038;&#20132;&#27169;&#25311;&#26356;&#21152;&#20016;&#23500;&#65292;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30740;&#31350;&#21508;&#31181;&#31038;&#20132;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#22312;&#36825;&#20123;&#27169;&#25311;&#20013;&#37319;&#29992;&#20102;&#19968;&#31181;&#20840;&#30693;&#30340;&#36879;&#35270;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;LLM&#29983;&#25104;&#25152;&#26377;&#20132;&#35848;&#32773;&#65289;&#65292;&#36825;&#19982;&#20154;&#31867;&#20855;&#26377;&#30340;&#38750;&#20840;&#30693;&#12289;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#20114;&#21160;&#26681;&#26412;&#19981;&#31526;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#23450;&#65288;&#20840;&#30693;&#12289;&#38750;&#20840;&#30693;&#65289;&#20013;&#20351;&#29992;LLMs&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#20840;&#30693;&#26041;&#24335;&#27169;&#25311;&#30340;&#20132;&#35848;&#32773;&#22312;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#26041;&#38754;&#27604;&#38750;&#20840;&#30693;&#20195;&#29702;&#20154;&#26356;&#25104;&#21151;&#65292;&#23613;&#31649;&#21518;&#32773;&#26356;&#31526;&#21512;&#29616;&#23454;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#20174;&#20840;&#30693;&#27169;&#25311;&#20013;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#20132;&#20114;&#30340;&#33258;&#28982;&#24615;&#65292;&#20294;&#22312;&#21512;&#20316;&#22330;&#26223;&#20013;&#20960;&#20046;&#19981;&#33021;&#22686;&#24378;&#30446;&#26631;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05020v1 Announce Type: cross  Abstract: Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena with LLM-based agents. However, most work has used an omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient simulations improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00199</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Socratic Question Generation using Data Augmentation and Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#26159;&#19968;&#31181;&#24341;&#23548;&#23398;&#29983;&#29420;&#31435;&#35299;&#20915;&#38382;&#39064;&#32780;&#19981;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33487;&#26684;&#25289;&#24213;&#38382;&#39064;&#65292;&#20197;&#20943;&#36731;&#25945;&#24072;&#30340;&#32321;&#37325;&#24037;&#20316;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#28041;&#21450;&#25552;&#31034;&#36825;&#20123;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26377;&#26102;&#20250;&#20135;&#29983;&#26080;&#25928;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#25110;&#25552;&#20379;&#26080;&#20851;&#25110;&#36807;&#26089;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#20016;&#23500;&#29616;&#26377;&#30340;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#25968;&#25454;&#38598;&#65307;&#20854;&#27425;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#24320;&#28304;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;LLama 2&#65292;&#20197;&#26356;&#20542;&#21521;&#20110;&#22320;&#38754;&#30495;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00199v1 Announce Type: new  Abstract: The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;</title><link>https://arxiv.org/abs/2402.17205</link><description>&lt;p&gt;
&#27979;&#37327;&#31070;&#32463;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;STEM&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Measuring Vision-Language STEM Skills of Neural Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#32467;&#21512;STEM&#65288;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#65289;&#30693;&#35782;&#26469;&#35299;&#20915;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#25361;&#25112;&#24615;&#38382;&#39064;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#23427;&#21253;&#25324;448&#39033;&#25216;&#33021;&#21644;1,073,146&#20010;&#36328;&#36234;&#25152;&#26377;STEM&#31185;&#30446;&#30340;&#38382;&#39064;&#12290;&#19982;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#39564;&#19987;&#23478;&#27700;&#24179;&#33021;&#21147;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#22522;&#30784;&#25216;&#33021;&#21644;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;GPT-3.5-Turbo&#65292;&#28155;&#21152;&#21040;&#25105;&#20204;&#30340;&#22522;&#20934;&#20013;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#36827;&#23637;&#21482;&#26377;&#21161;&#20110;&#25484;&#25569;&#25968;&#25454;&#38598;&#20013;&#38750;&#24120;&#26377;&#38480;&#25968;&#37327;&#30340;&#20302;&#24180;&#32423;&#25216;&#33021;&#65288;&#19977;&#24180;&#32423;&#20013;&#30340;2.5%&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#36828;&#27809;&#26377;&#23436;&#20840;&#25484;&#25569;&#23398;&#21069;&#25945;&#32946;&#38454;&#27573;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.16880</link><description>&lt;p&gt;
BESA: &#20351;&#29992;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#12289;&#25991;&#26412;&#38382;&#31572;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#21442;&#25968;&#36896;&#25104;&#30340;&#35745;&#31639;&#21344;&#29992;&#21487;&#33021;&#26159;&#31105;&#38178;&#30340;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;SparseGPT&#21644;Wanda&#65289;&#23581;&#35797;&#36890;&#36807;&#26435;&#37325;&#20462;&#21098;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36880;&#23618;&#26041;&#27861;&#20250;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#26174;&#33879;&#25200;&#21160;&#65292;&#24182;&#38656;&#35201;&#32454;&#33268;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#20462;&#21098;&#36895;&#29575;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20462;&#21098;&#25216;&#26415;&#65292;&#31216;&#20026;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#65288;BESA&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#12290;&#19982;&#20856;&#22411;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#30456;&#27604;&#65292;BESA&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;i&#65289;&#23427;&#23450;&#20301;&#20110;&#25972;&#20307;&#20462;&#21098;&#35823;&#24046;&#30456;&#23545;&#20110;&#27599;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16880v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to indi
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.16139</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
What Generative Artificial Intelligence Means for Terminological Definitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16139
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#21019;&#24314;&#21644;&#28040;&#36153;&#30340;&#24433;&#21709;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;GenAI&#24037;&#20855;&#19982;&#20256;&#32479;&#26415;&#35821;&#36164;&#28304;&#30456;&#27604;&#65292;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#30410;&#22788;&#21644;&#25361;&#25112;&#12290;ChatGPT&#22312;&#20197;&#20132;&#20114;&#24335;&#21644;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#29305;&#23450;&#35821;&#22659;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#35782;&#21035;&#36164;&#28304;&#20013;&#30340;&#26415;&#35821;&#23450;&#20041;&#21487;&#33021;&#20250;&#22240;&#20854;&#21487;&#38752;&#24615;&#32780;&#32487;&#32493;&#23384;&#22312;&#12290;&#20174;&#26415;&#35821;&#23398;&#23478;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#24037;&#20855;&#20351;&#24471;AI&#36741;&#21161;&#30340;&#26415;&#35821;&#32534;&#32386;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#21518;&#26399;&#32534;&#36753;&#26415;&#35821;&#32534;&#32386;&#65292;&#23558;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#23450;&#20041;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16139v1 Announce Type: cross  Abstract: This paper examines the impact of Generative Artificial Intelligence (GenAI) on the creation and consumption of terminological definitions. GenAI tools like ChatGPT present a mix of benefits and drawbacks compared to traditional terminological resources. ChatGPT excels in providing context-specific meanings in an interactive and customized fashion but faces challenges with accuracy. Terminological definitions in recognized resources will likely survive because of their reliability. From the point of view of the terminologist, tools like ChatGPT enable AI-assisted terminography, including post-editing terminography, as an approach blending AI efficiency with human expertise for faster definition creation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14891</link><description>&lt;p&gt;
LLMBind: &#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMBind: A Unified Modality-Task Integration Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#38598;&#25104;&#33021;&#21147;&#26377;&#38480;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24102;&#22836;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;LLMBind&#21487;&#20197;&#20197;&#22810;&#31181;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#35299;&#37322;&#36755;&#20837;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#30340;&#21327;&#20316;&#23454;&#29616;&#19981;&#21516;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26377;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;40&#19975;&#26465;&#25351;&#20196;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35299;&#38145;&#20102;&#20132;&#20114;&#24335;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31687;&#32508;&#36848;&#32473;&#20986;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#37325;&#35201;&#26041;&#38754;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01680</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65306;&#36827;&#23637;&#19982;&#25361;&#25112;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Model based Multi-Agents: A Survey of Progress and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01680
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31687;&#32508;&#36848;&#32473;&#20986;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#37325;&#35201;&#26041;&#38754;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;LLMs&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35268;&#21010;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#23427;&#20204;&#34987;&#29992;&#20316;&#33258;&#20027;&#26234;&#33021;&#20307;&#26469;&#33258;&#21160;&#23436;&#25104;&#35768;&#22810;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#23558;&#19968;&#20010;LLM&#29992;&#20316;&#21333;&#20010;&#35268;&#21010;&#25110;&#20915;&#31574;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#20026;&#31038;&#21306;&#25552;&#20379;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#39046;&#22495;&#30340;&#32508;&#36848;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22522;&#26412;&#26041;&#38754;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35753;&#35835;&#32773;&#23545;&#20197;&#19979;&#38382;&#39064;&#33719;&#24471;&#23454;&#36136;&#24615;&#35265;&#35299;&#65306;LLM-based&#22810;&#26234;&#33021;&#20307;&#27169;&#25311;&#21738;&#20123;&#39046;&#22495;&#21644;&#29615;&#22659;&#65311;&#36825;&#20123;&#26234;&#33021;&#20307;&#26159;&#22914;&#20309;&#24314;&#27169;&#21644;&#36890;&#20449;&#30340;&#65311;&#20160;&#20040;&#26426;&#21046;&#26377;&#21161;&#20110;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22686;&#38271;&#65311;&#23545;&#20110;&#37027;&#20123;&#23545;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#24863;&#20852;&#36259;&#30340;&#20154;&#65292;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#19968;&#20123;&#35201;&#28857;&#21644;&#25361;&#25112;.
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize t
&lt;/p&gt;</description></item><item><title>&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#26159;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#32467;&#21512;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#38382;&#24182;&#19982;&#31995;&#32479;&#21160;&#24577;&#20132;&#20114;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2209.01621</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interactive Question Answering Systems: Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.01621
&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#26159;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#32467;&#21512;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#38382;&#24182;&#19982;&#31995;&#32479;&#21160;&#24577;&#20132;&#20114;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01621v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328;  &#25688;&#35201;: &#38382;&#31572;&#31995;&#32479;&#34987;&#20844;&#35748;&#20026;&#22312;&#32593;&#32476;&#19978;&#23547;&#27714;&#20449;&#24687;&#30340;&#27969;&#34892;&#19988;&#26377;&#25928;&#30340;&#25163;&#27573;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#20449;&#24687;&#23547;&#25214;&#32773;&#21487;&#20197;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#38382;&#39064;&#26469;&#33719;&#24471;&#31616;&#27905;&#30340;&#22238;&#31572;&#12290;&#20132;&#20114;&#24335;&#38382;&#31572;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#24182;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20301;&#20110;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#20132;&#38598;&#22788;&#12290;&#19968;&#26041;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#26222;&#36890;&#35821;&#35328;&#25552;&#38382;&#24182;&#25214;&#21040;&#22905;&#38382;&#39064;&#30340;&#23454;&#38469;&#22238;&#31572;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#21021;&#22987;&#35831;&#27714;&#20013;&#23384;&#22312;&#22810;&#20010;&#21487;&#33021;&#30340;&#22238;&#22797;&#12289;&#24456;&#23569;&#25110;&#27169;&#26865;&#20004;&#21487;&#65292;&#31995;&#32479;&#21487;&#20197;&#23558;&#38382;&#31572;&#20250;&#35805;&#24310;&#38271;&#20026;&#23545;&#35805;&#12290;&#36890;&#36807;&#20801;&#35768;&#29992;&#25143;&#25552;&#20986;&#26356;&#22810;&#38382;&#39064;&#65292;&#20132;&#20114;&#24335;&#38382;&#31572;&#20351;&#29992;&#25143;&#33021;&#22815;&#21160;&#24577;&#22320;&#19982;&#31995;&#32479;&#20132;&#20114;&#24182;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01621v2 Announce Type: replace-cross  Abstract: Question answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their query by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to dynamically interact with the system and receive more precise results. This survey offers a detailed overview of the interactive question-ans
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15127</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness. (arXiv:2401.15127v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#20851;&#20110;&#26032;&#20852;&#23041;&#32961;&#30340;&#30693;&#35782;&#20849;&#20139;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#26500;&#25104;&#20102;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#26426;&#36935;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#12289;GPT4all&#12289;Dolly&#12289;Stanford Alpaca&#12289;Alpaca-LoRA&#21644;Falcon&#31561;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35782;&#21035;&#24320;&#28304;&#24773;&#25253;&#20013;&#19982;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20316;&#20026;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20174;Twitter&#25910;&#38598;&#30340;&#32463;&#36807;&#20805;&#20998;&#39564;&#35777;&#30340;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#26469;&#28304;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#26041;&#38754;&#65292;&#21830;&#19994;&#27169;&#22411;Chatbot GPT-4&#23454;&#29616;&#20102;&#21487;&#25509;&#21463;&#30340;F1&#20998;&#25968;0.94&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;GPT4all&#23454;&#29616;&#20102;F1&#20998;&#25968;0.90&#12290;&#28982;&#32780;&#65292;&#23601;&#32593;&#32476;&#23433;&#20840;&#23454;&#20307;&#35782;&#21035;&#32780;&#35328;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence. In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study explores the capability of chatbots such as ChatGPT, GPT4all, Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify cybersecurity-related text within Open Source Intelligence. We assess the capabilities of existing chatbot models for Natural Language Processing tasks. We consider binary classification and Named Entity Recognition as tasks. This study analyzes well-established data collected from Twitter, derived from previous research efforts. Regarding cybersecurity binary classification, Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94, and the open-source GPT4all model achieved an F1-score of 0.90. However, concerning cybersecurity entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#30340;&#30740;&#31350;&#21019;&#26032;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.14724</link><description>&lt;p&gt;
&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#30340;&#35843;&#26597;&#65306;&#24517;&#35201;&#24615;&#12289;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions. (arXiv:2310.14724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#30340;&#30740;&#31350;&#21019;&#26032;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#22797;&#26434;&#35821;&#35328;&#30340;&#24378;&#22823;&#33021;&#21147;&#20351;&#24471;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#28044;&#20837;&#21040;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#24182;&#24471;&#21040;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#25509;&#21463;&#12290;&#38543;&#30528;LLMs&#30340;&#19981;&#26029;&#25193;&#23637;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#26816;&#27979;&#22120;&#12290;&#36825;&#23545;&#20110;&#20943;&#23569;LLMs&#28508;&#22312;&#30340;&#35823;&#29992;&#65292;&#24182;&#20445;&#25252;&#33402;&#26415;&#34920;&#36798;&#21644;&#31038;&#20132;&#32593;&#32476;&#31561;&#39046;&#22495;&#20813;&#21463;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#26377;&#23475;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#26088;&#22312;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#21542;&#30001;LLM&#29983;&#25104;&#65292;&#23454;&#36136;&#19978;&#26159;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#26816;&#27979;&#22120;&#25216;&#26415;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#25512;&#21160;&#22240;&#32032;&#21253;&#25324;&#27700;&#21360;&#25216;&#26415;&#12289;&#38646;&#26679;&#26412;&#26041;&#27861;&#12289;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#12289;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#12289;&#23558;LLMs&#20316;&#20026;&#26816;&#27979;&#22120;&#20197;&#21450;&#20154;&#31867;&#36741;&#21161;&#26041;&#27861;&#30340;&#21019;&#26032;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#30340;&#30740;&#31350;&#31361;&#30772;&#65292;&#24182;&#24378;&#35843;&#20102;&#36843;&#20999;&#30340;&#38656;&#27714;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, zero-shot methods, fine-turning LMs methods, adversarial learning methods, LLMs as detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SocREval&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;GPT-4&#21644;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22797;&#26434;&#25512;&#29702;&#27169;&#22411;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.00074</link><description>&lt;p&gt;
SocREval&#65306;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation. (arXiv:2310.00074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SocREval&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;GPT-4&#21644;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22797;&#26434;&#25512;&#29702;&#27169;&#22411;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#35780;&#20272;&#23427;&#20204;&#30340;&#36880;&#27493;&#25512;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#35780;&#20272;&#25351;&#26631;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#25512;&#29702;&#38142;&#26469;&#35780;&#20272;&#27169;&#22411;&#23548;&#20986;&#30340;&#25512;&#29702;&#38142;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#20154;&#24037;&#32534;&#20889;&#30340;&#25512;&#29702;&#38142;&#21487;&#33021;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#20854;&#33719;&#21462;&#36890;&#24120;&#26159;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#12290;&#29616;&#26377;&#30340;&#26080;&#21442;&#32771;&#25512;&#29702;&#25351;&#26631;&#28040;&#38500;&#20102;&#20154;&#24037;&#21046;&#20316;&#25512;&#29702;&#38142;&#30340;&#38656;&#27714;&#20316;&#20026;&#21442;&#32771;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22312;&#20855;&#26377;&#20154;&#24037;&#25512;&#29702;&#38142;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#22797;&#26434;&#21270;&#20102;&#27969;&#31243;&#24182;&#24341;&#21457;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#33258;&#21160;&#35780;&#20272;&#25512;&#29702;&#38142;&#36136;&#37327;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#21046;&#20316;&#21442;&#32771;&#30340;&#38656;&#27714;&#12290;&#21033;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#25552;&#31034;&#26469;&#22686;&#24378;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#36825;&#23601;&#26159;&#25105;&#20204;&#31216;&#20043;&#20026;SocREval&#65288;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comprehensively assess the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains to assess the model-derived chains. However, such ``gold-standard'' human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning metrics eliminate the need for human-crafted reasoning chains as references, but they typically require fine-tuning on datasets with human-derived reasoning chains, which complicates the process and raises concerns regarding generalizability across diverse datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, obviating the need for human-crafted references. Leveraging the Socratic method, we devise tailored prompts to enhance reference-free reasoning evaluation, which we term SocREval (Socratic metho
&lt;/p&gt;</description></item><item><title>&#29992;&#20154;&#24037;&#26234;&#33021;&#32763;&#35793;&#24037;&#20855;&#21487;&#20197;&#35299;&#20915;&#30740;&#31350;&#26089;&#26399;&#31185;&#23398;&#25991;&#29486;&#30340;&#25289;&#19969;&#25991;&#32763;&#35793;&#38590;&#39064;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21457;&#29616;ChatGPT&#31639;&#27861;&#22312;&#34920;&#29616;&#19978;&#26356;&#20986;&#33394;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32422;&#32752;&#183;&#20271;&#21162;&#21033;&#32473;&#27431;&#25289;&#30340;&#19968;&#23553;&#20449;&#30340;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2307.07520</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#26234;&#33021;&#32763;&#35793;&#25289;&#19969;&#25991;
&lt;/p&gt;
&lt;p&gt;
Translating Latin with Artificial Intelligence. (arXiv:2307.07520v1 [math.HO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07520
&lt;/p&gt;
&lt;p&gt;
&#29992;&#20154;&#24037;&#26234;&#33021;&#32763;&#35793;&#24037;&#20855;&#21487;&#20197;&#35299;&#20915;&#30740;&#31350;&#26089;&#26399;&#31185;&#23398;&#25991;&#29486;&#30340;&#25289;&#19969;&#25991;&#32763;&#35793;&#38590;&#39064;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21457;&#29616;ChatGPT&#31639;&#27861;&#22312;&#34920;&#29616;&#19978;&#26356;&#20986;&#33394;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32422;&#32752;&#183;&#20271;&#21162;&#21033;&#32473;&#27431;&#25289;&#30340;&#19968;&#23553;&#20449;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26089;&#26399;&#31185;&#23398;&#25991;&#29486;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#25289;&#19969;&#25991;&#20316;&#21697;&#30340;&#29616;&#20195;&#35821;&#35328;&#32763;&#35793;&#30340;&#21487;&#33719;&#24471;&#24615;&#12290;&#36825;&#23588;&#20854;&#36866;&#29992;&#20110;&#27431;&#25289;&#30340;&#20316;&#21697;&#65292;&#20182;&#25776;&#20889;&#20102;&#32422;850&#20221;&#25163;&#31295;&#65292;&#20889;&#20102;&#19968;&#21315;&#23553;&#20449;&#24182;&#25910;&#21040;&#20102;&#36817;&#20004;&#21315;&#23553;&#22238;&#20449;&#12290;&#36825;&#20123;&#25163;&#31295;&#12289;&#20070;&#31821;&#21644;&#20449;&#20214;&#30340;&#32763;&#35793;&#24050;&#32463;&#22312;&#36807;&#21435;&#20004;&#20010;&#19990;&#32426;&#20013;&#21457;&#24067;&#22312;&#21508;&#31181;&#26469;&#28304;&#20013;&#65292;&#20294;&#36824;&#26377;&#35768;&#22810;&#23578;&#26410;&#20986;&#29256;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#29616;&#22914;&#20170;&#21487;&#20197;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#32763;&#35793;&#25216;&#26415;&#26469;&#35299;&#20915;&#32763;&#35793;&#22914;&#27492;&#22823;&#37327;&#25991;&#26412;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#24037;&#20855;&#65292;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#20197;&#27604;&#36739;&#20004;&#31181;&#27969;&#34892;&#30340;&#20154;&#24037;&#26234;&#33021;&#32763;&#35793;&#31639;&#27861;&#65292;&#21363;&#35895;&#27468;&#32763;&#35793;&#21644;ChatGPT&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#21457;&#29616;ChatGPT&#22312;&#36825;&#20123;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#27492;&#35813;&#32763;&#35793;&#25903;&#25345;&#24037;&#20855;&#38543;&#21518;&#34987;&#29992;&#20110;&#23558;&#32422;&#32752;&#183;&#20271;&#21162;&#21033;&#22312;1739&#24180;&#20889;&#32473;&#27431;&#25289;&#30340;&#19968;&#23553;&#20449;&#30340;&#25688;&#24405;&#36827;&#34892;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
The major hindrance in the study of earlier scientific literature is the availability of Latin translations into modern languages. This is particular true for the works of Euler who authored about 850 manuscripts and wrote a thousand letters and received back almost two thousand more. The translation of many of these manuscripts, books and letters have been published in various sources over the last two centuries, but many more have not yet appeared. Fortunately, nowadays, the artificial intelligence AI translation can be used to circumvent the challenges of translating such substantial number of texts. To validate this tool, benchmark tests have been performed to compare the performance of two popular AI translating algorithms, namely Google Translate and ChatGPT. Since it was found that ChatGPT performed better on these tests, this translating support was then used on an excerpt of a 1739 letter from Johann Bernoulli to Euler, where he notifies that he was sending to Euler the first 
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#19968;&#20010;&#30693;&#35782;&#28170;&#21338;&#20294;&#32463;&#39564;&#19981;&#36275;&#30340;LLM&#65292;&#33021;&#22815;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#65292;&#20294;&#22312;&#26576;&#20123;&#31867;&#22411;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2303.16421</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#30693;&#35782;&#28170;&#21338;&#20294;&#32463;&#39564;&#19981;&#36275;&#30340;&#38382;&#39064;&#27714;&#35299;&#22120;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24120;&#35782;&#38382;&#39064;&#30340;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models. (arXiv:2303.16421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16421
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#30693;&#35782;&#28170;&#21338;&#20294;&#32463;&#39564;&#19981;&#36275;&#30340;LLM&#65292;&#33021;&#22815;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#65292;&#20294;&#22312;&#26576;&#20123;&#31867;&#22411;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#22312;NLP&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35760;&#24518;&#12289;&#34920;&#36798;&#21644;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#33021;&#21147;&#19968;&#30452;&#26159;LLMs&#30340;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#30171;&#28857;&#12290;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#20197;&#19979;&#20960;&#28857;&#65306;&#65288;1&#65289;GPT&#33021;&#21542;&#26377;&#25928;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#65311;&#65288;2&#65289;GPT&#23545;&#24120;&#35782;&#30693;&#35782;&#26159;&#21542;&#31934;&#36890;&#65311;&#65288;3&#65289;GPT&#26159;&#21542;&#20102;&#35299;&#29992;&#20110;&#22238;&#31572;&#29305;&#23450;&#38382;&#39064;&#30340;&#24213;&#23618;&#24120;&#35782;&#30693;&#35782;&#65311;&#65288;4&#65289;GPT&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#24120;&#35782;&#22238;&#31572;&#38382;&#39064;&#65311;&#20026;&#20102;&#35780;&#20272;&#20197;&#19978;&#24120;&#35782;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#35780;&#20272;ChatGPT&#30340;&#24120;&#35782;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;(1) GPT&#22312;&#24120;&#35782;&#20219;&#21153;&#20013;&#33021;&#22815;&#33719;&#24471;&#33391;&#22909;&#30340;&#38382;&#31572;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#26576;&#20123;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;(2) ChatGPT&#20855;&#26377;&#23398;&#35782;&#28170;&#21338;&#65292;&#21487;&#20197;&#20351;&#29992;&#30693;&#35782;&#25552;&#31034;&#20934;&#30830;&#22320;&#20135;&#29983;&#22823;&#37096;&#20998;&#24120;&#35782;&#30693;&#35782;&#12290;(3)&#23613;&#31649;&#20855;&#26377;&#30693;&#35782;&#65292;ChatGPT&#26159;&#19968;&#20010;&#32570;&#20047;&#32463;&#39564;&#30340;&#24120;&#35782;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#26576;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT and GPT-4 have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point for LLMs. It remains unclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are GPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question? (4) Can GPTs effectively leverage commonsense for answering questions? To evaluate the above commonsense problems, we conduct a series of experiments to evaluate ChatGPT's commonsense abilities, and the experimental results show that: (1) GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cann
&lt;/p&gt;</description></item></channel></rss>