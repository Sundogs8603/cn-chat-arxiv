<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;PPTC&#22522;&#20934;&#65292;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26681;&#25454;&#29992;&#25143;&#25351;&#20196;&#21019;&#24314;&#21644;&#32534;&#36753;PPT&#25991;&#20214;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#21457;&#29616;GPT-4&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#20026;75.1%&#12290;</title><link>http://arxiv.org/abs/2311.01767</link><description>&lt;p&gt;
PPTC&#22522;&#20934;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;PowerPoint&#20219;&#21153;&#23436;&#25104;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion. (arXiv:2311.01767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01767
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;PPTC&#22522;&#20934;&#65292;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26681;&#25454;&#29992;&#25143;&#25351;&#20196;&#21019;&#24314;&#21644;&#32534;&#36753;PPT&#25991;&#20214;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#21457;&#29616;GPT-4&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#20026;75.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#27979;&#35797;&#23427;&#20204;&#23545;&#22522;&#26412;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#38646;&#27425;/&#23569;&#27425;&#23581;&#35797;&#33021;&#21147;&#20197;&#21450;&#23558;&#25351;&#20196;&#32763;&#35793;&#25104;&#24037;&#20855;API&#30340;&#33021;&#21147;&#19978;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21033;&#29992;&#22797;&#26434;&#24037;&#20855;&#23436;&#25104;&#22797;&#26434;&#22810;&#36718;&#12289;&#22810;&#27169;&#24577;&#25351;&#20196;&#30340;LLM&#30340;&#35780;&#20272;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PowerPoint&#20219;&#21153;&#23436;&#25104;&#65288;PPTC&#65289;&#22522;&#20934;&#65292;&#35780;&#20272;LLM&#26681;&#25454;&#29992;&#25143;&#25351;&#20196;&#21019;&#24314;&#21644;&#32534;&#36753;PPT&#25991;&#20214;&#30340;&#33021;&#21147;&#12290;&#23427;&#21253;&#21547;279&#20010;&#28085;&#30422;&#19981;&#21516;&#20027;&#39064;&#30340;&#22810;&#36718;&#23545;&#35805;&#65292;&#28041;&#21450;&#22810;&#27169;&#24577;&#25805;&#20316;&#30340;&#25968;&#30334;&#20010;&#25351;&#20196;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;PPTX-Match&#35780;&#20272;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#26681;&#25454;&#39044;&#27979;&#25991;&#20214;&#32780;&#19981;&#26159;&#26631;&#31614;API&#24207;&#21015;&#26469;&#35780;&#20272;LLM&#26159;&#21542;&#23436;&#25104;&#20102;&#25351;&#20196;&#65292;&#22240;&#27492;&#25903;&#25345;&#21508;&#31181;LLM&#29983;&#25104;&#30340;API&#24207;&#21015;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;3&#20010;&#38381;&#21512;&#22411;LLM&#21644;6&#20010;&#24320;&#28304;LLM&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;LLM&#65292;&#36798;&#21040;&#20102;75.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1\% accuracy i
&lt;/p&gt;</description></item><item><title>AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01305</link><description>&lt;p&gt;
AWEQ&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01305
&lt;/p&gt;
&lt;p&gt;
AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#30456;&#23545;&#36739;&#39640;&#12290;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AWEQ&#65292;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;AWEQ&#22312;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;(W8A8)&#37327;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#35266;&#23519;&#21040;&#26435;&#37325;&#37327;&#21270;&#27604;&#28608;&#27963;&#37327;&#21270;&#26356;&#23481;&#26131;&#12290;AWEQ&#36890;&#36807;&#36890;&#36947;&#22343;&#34913;&#23558;&#28608;&#27963;&#37327;&#21270;&#30340;&#38590;&#24230;&#36716;&#31227;&#21040;&#26435;&#37325;&#19978;&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#37327;&#21270;&#22256;&#38590;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22343;&#34913;&#26041;&#27861;&#65292;&#20943;&#23567;&#20102;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20687;LLaMA&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.20246</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#20013;&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#35265;&#35299;&#19982;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#36866;&#29992;&#20110;&#21333;&#35821;&#35328;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#30340;&#24378;&#22823;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20445;&#25345;&#25928;&#26524;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#21644;&#35757;&#32451;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#65288;xMR&#65289;LLM&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#32763;&#35793;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#25351;&#23548;&#25968;&#25454;&#38598;MGSM8KInstruct&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;xMR&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;xMR LLMs&#65292;&#34987;&#21629;&#21517;&#20026;MathOctopus&#65292;&#22312;&#20960;&#27425;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#24320;&#28304;LLMs&#21644;ChatGPT&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MathOctopus-13B&#22312;MGSM&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;47.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;ChatGPT&#30340;46.3%&#12290;&#38500;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#20174;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#35266;&#23519;&#21644;&#35265;&#35299;&#65306;&#65288;1&#65289;&#22312;&#22810;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#22909;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290; &#65288;2&#65289;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290; &#65288;3&#65289;&#27169;&#22411;&#23545;&#20110;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#30340;&#22788;&#29702;&#26159;&#25361;&#25112;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
&lt;/p&gt;</description></item><item><title>MolCA&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#35821;&#35328;&#30340;&#24314;&#27169;&#31995;&#32479;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#22270;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#39640;&#25928;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.12798</link><description>&lt;p&gt;
MolCA: &#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#30340;&#20998;&#23376;&#22270;-&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. (arXiv:2310.12798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12798
&lt;/p&gt;
&lt;p&gt;
MolCA&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#35821;&#35328;&#30340;&#24314;&#27169;&#31995;&#32479;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#22270;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#39640;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23545;&#20998;&#23376;&#30340;&#21331;&#36234;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#32570;&#20047;&#20154;&#31867;&#19987;&#19994;&#20154;&#21592;&#22312;&#29702;&#35299;&#20998;&#23376;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#20851;&#38190;&#33021;&#21147; - 2D&#22270;&#24418;&#24863;&#30693;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MolCA: &#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#36827;&#34892;&#20998;&#23376;&#22270;-&#35821;&#35328;&#24314;&#27169;&#12290;MolCA&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#20351;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Galactica&#65289;&#33021;&#22815;&#29702;&#35299;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36328;&#27169;&#24577;&#25237;&#24433;&#22120;&#34987;&#23454;&#29616;&#20026;&#19968;&#20010;Q-Former&#65292;&#36830;&#25509;&#19968;&#20010;&#22270;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#21644;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;MolCA&#20351;&#29992;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#65288;&#21363;LoRA&#65289;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#32534;&#30721;&#22120;&#32806;&#21512;&#19981;&#21516;&#65292;MolCA&#20445;&#30041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#22686;&#21152;&#20102;2D&#22270;&#24418;&#20449;&#24687;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception - a critical ability of human professionals in comprehending molecules' topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder's representation space and an LM's text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information. To showcase its effectivenes
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#21512;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2310.06827</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20219;&#21153;&#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#26356;&#23569;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Teaching Language Models to Hallucinate Less with Synthetic Tasks. (arXiv:2310.06827v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#21512;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65288;&#22914;&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#12289;&#20250;&#35758;&#27010;&#36848;&#21644;&#20020;&#24202;&#25253;&#21578;&#29983;&#25104;&#65289;&#20013;&#32463;&#24120;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;&#20351;&#25152;&#26377;&#24517;&#35201;&#20449;&#24687;&#37117;&#22312;&#19978;&#19979;&#25991;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#20248;&#21270;LLMs&#20197;&#20943;&#23569;&#24187;&#35273;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#27599;&#20010;&#20248;&#21270;&#27493;&#39588;&#20013;&#26377;&#25928;&#35780;&#20272;&#24187;&#35273;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#20063;&#21487;&#20197;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;SynTra&#65292;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#21512;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#26131;&#20110;&#35825;&#21457;&#21644;&#34913;&#37327;&#24187;&#35273;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#21512;&#25104;&#20219;&#21153;&#36827;&#34892;&#21069;&#32512;&#35843;&#20248;&#26469;&#20248;&#21270;LLM&#30340;&#31995;&#32479;&#28040;&#24687;&#65292;&#24182;&#26368;&#32456;&#23558;&#31995;&#32479;&#28040;&#24687;&#36716;&#31227;&#21040;&#29616;&#23454;&#20013;&#38590;&#20197;&#20248;&#21270;&#30340;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#29616;&#23454;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65292;&#20351;&#29992;&#20165;&#21512;&#25104;&#26816;&#32034;&#20219;&#21153;&#36827;&#34892;&#30417;&#30563;&#65292;SynTra&#20943;&#23569;&#20102;&#20004;&#20010;&#20855;&#26377;13B&#21442;&#25968;&#30340;LLMs&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36890;&#36807;&#20248;&#21270;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#28040;&#24687;&#65292;&#21487;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the sy
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.07870</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#21270;&#35282;&#24230;&#30340;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07870
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#35748;&#20026;LLMs&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#21472;&#21152;&#12290;LLMs&#34920;&#29616;&#20986;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22522;&#20110;&#20135;&#29983;&#30340;&#35282;&#24230;&#32780;&#25913;&#21464;&#65288;&#19982;&#20154;&#31867;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#36890;&#24120;&#20855;&#26377;&#26356;&#19968;&#33268;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35282;&#24230;&#21487;&#25511;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#12289;VSM&#12289;IPIP&#65289;&#26469;&#30740;&#31350;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#35282;&#24230;&#32780;&#25913;&#21464;&#12290;&#36890;&#36807;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#31034;&#20013;&#65288;&#38544;&#24335;&#25110;&#26174;&#24335;&#65289;&#26263;&#31034;&#20102;&#26576;&#20123;&#20215;&#20540;&#35266;&#26102;&#65292;LLMs&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#26263;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
&lt;/p&gt;</description></item><item><title>Think-on-Graph&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.07697</link><description>&lt;p&gt;
Think-on-Graph: &#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07697
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#38656;&#35201;&#30693;&#35782;&#36861;&#28335;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#32463;&#24120;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#34920;&#29616;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Think-on-Graph&#65288;ToG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;LLMs&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;ToG&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#24182;&#23545;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#36827;&#34892;&#25506;&#32034;&#21644;&#25512;&#29702;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#19977;&#20803;&#32452;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#29983;&#25104;&#21253;&#21547;&#39034;&#24207;&#36830;&#25509;&#30340;&#19977;&#20803;&#32452;&#30340;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#30452;&#21040;&#25910;&#38598;&#21040;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#25110;&#36798;&#21040;&#26368;&#22823;&#28145;&#24230;&#20026;&#27490;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ToG&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;LLMs&#30340;&#21069;&#36848;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs' ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufficient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BeaverTails&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;LLM&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#35813;&#25968;&#25454;&#38598;&#20998;&#24320;&#27880;&#37322;&#20102;&#38382;&#31572;&#23545;&#30340;&#26377;&#29992;&#24615;&#21644;&#26080;&#23475;&#24615;&#65292;&#20026;&#23433;&#20840;&#24320;&#21457;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.04657</link><description>&lt;p&gt;
BeaverTails&#65306;&#36890;&#36807;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;&#25913;&#21892;LLM&#30340;&#23433;&#20840;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset. (arXiv:2307.04657v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BeaverTails&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;LLM&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#35813;&#25968;&#25454;&#38598;&#20998;&#24320;&#27880;&#37322;&#20102;&#38382;&#31572;&#23545;&#30340;&#26377;&#29992;&#24615;&#21644;&#26080;&#23475;&#24615;&#65292;&#20026;&#23433;&#20840;&#24320;&#21457;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;BeaverTails&#8221;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#23545;&#40784;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#29420;&#29305;&#22320;&#23545;&#38382;&#31572;&#23545;&#30340;&#26377;&#29992;&#24615;&#21644;&#26080;&#23475;&#24615;&#36827;&#34892;&#20102;&#20998;&#24320;&#27880;&#37322;&#65292;&#20174;&#32780;&#20026;&#36825;&#20123;&#20851;&#38190;&#23646;&#24615;&#25552;&#20379;&#20102;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#20026;30,207&#20010;&#38382;&#31572;&#23545;&#21644;30,144&#23545;&#19987;&#23478;&#27604;&#36739;&#25968;&#25454;&#25910;&#38598;&#20102;&#23433;&#20840;&#20803;&#26631;&#31614;&#65292;&#29992;&#20110;&#34913;&#37327;&#26377;&#29992;&#24615;&#21644;&#26080;&#23475;&#24615;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;BeaverTails&#22312;&#20869;&#23481;&#31649;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20854;&#22312;LLM&#20013;&#23454;&#26045;&#23454;&#38469;&#23433;&#20840;&#25514;&#26045;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#65292;&#20026;LLM&#30340;&#23433;&#20840;&#24320;&#21457;&#21644;&#37096;&#32626;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the \textsc{BeaverTails} dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 30,207 question-answer (QA) pairs and 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;transformers&#22914;&#20309;&#24179;&#34913;&#20840;&#23616;&#20998;&#24067;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#20998;&#24067;&#30340;&#20004;&#31181;&#30693;&#35782;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#26435;&#20540;&#30697;&#38453;&#20316;&#20026;&#32852;&#24819;&#35760;&#24518;&#30340;&#20316;&#29992;&#21450;&#26799;&#24230;&#22914;&#20309;&#23454;&#29616;&#26435;&#37325;&#23398;&#20064;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.00802</link><description>&lt;p&gt;
&#19968;&#31181;&#35760;&#24518;&#35270;&#35282;&#19979;&#30340;Transformer&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Birth of a Transformer: A Memory Viewpoint. (arXiv:2306.00802v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;transformers&#22914;&#20309;&#24179;&#34913;&#20840;&#23616;&#20998;&#24067;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#20998;&#24067;&#30340;&#20004;&#31181;&#30693;&#35782;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#26435;&#20540;&#30697;&#38453;&#20316;&#20026;&#32852;&#24819;&#35760;&#24518;&#30340;&#20316;&#29992;&#21450;&#26799;&#24230;&#22914;&#20309;&#23454;&#29616;&#26435;&#37325;&#23398;&#20064;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#34987;&#24191;&#27867;&#37096;&#32626;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#20197;&#20351;&#23427;&#20204;&#26356;&#21152;&#21487;&#38752;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;transformers&#22914;&#20309;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#21512;&#25104;&#30340;&#35774;&#32622;&#26469;&#24179;&#34913;&#23384;&#20648;&#20110;&#23427;&#20204;&#20043;&#20013;&#30340;&#20004;&#31181;&#30693;&#35782;&#31867;&#22411;&#8212;&#8212;&#20840;&#23616;&#20998;&#24067;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#20108;&#20803;&#20998;&#24067;&#12290;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;Transformer&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20180;&#32454;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#23545;&#20840;&#23616;&#20108;&#20803;&#20998;&#24067;&#30340;&#24555;&#36895;&#23398;&#20064;&#20197;&#21450;&#23545;&#19978;&#19979;&#25991;&#20013;&#30340;&#20108;&#20803;&#20998;&#24067;&#30340;"&#24402;&#32435;&#22836;"&#26426;&#21046;&#30340;&#36739;&#24930;&#21457;&#23637;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26435;&#20540;&#30697;&#38453;&#20316;&#20026;&#32852;&#24819;&#35760;&#24518;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35265;&#35299;&#65292;&#35828;&#26126;&#20102;&#26799;&#24230;&#22914;&#20309;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#26435;&#37325;&#30340;&#23398;&#20064;&#65292;&#24182;&#30740;&#31350;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an "induction head" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19466</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#23545; Transformer &#27169;&#22411;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based &#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#20013;&#65292;&#38271;&#24230;&#25512;&#24191;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#23427;&#26159;&#25351;&#20174;&#23567;&#30340;&#35757;&#32451;&#25991;&#26412;&#33539;&#22260;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#34987;&#21457;&#29616;&#26159;&#24433;&#21709;&#38271;&#24230;&#25512;&#24191;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#19981;&#21516;&#30340; PE &#26041;&#26696;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#25512;&#24433;&#21709;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#20116;&#31181;&#19981;&#21516;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;&#21253;&#25324;&#32477;&#23545;&#20301;&#32622;&#23884;&#20837;&#12289;T5 &#30340;&#30456;&#23545; PE&#12289;ALiBi&#12289;Rotary &#21644;&#26080;&#20301;&#32622;&#32534;&#30721;&#65289;&#30340;&#35299;&#30721;&#22120; Transformer &#30340;&#38271;&#24230;&#25512;&#24191;&#33021;&#21147;&#65292;&#23545;&#25512;&#29702;&#21644;&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#22914; ALiBi&#12289;Rotary &#21644; APE&#65292;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#26080; PE &#30340; Transformer &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26174;&#24335; PE &#26041;&#27861;&#65292;&#36825;&#24847;&#21619;&#30528;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#23454;&#38469;&#19978;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#26377;&#25928; Transformer &#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other expli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#29575;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BERTScore-Recall&#24230;&#37327;&#26041;&#27861;&#36873;&#25321;&#26356;&#22909;&#30340;&#31034;&#20363;&#26469;&#23637;&#31034;&#27979;&#35797;&#36755;&#20837;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21516;&#26102;&#36824;&#36890;&#36807;&#25193;&#23637;&#25104;&#38598;&#21512;&#32423;&#21035;&#24230;&#37327;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#34920;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;BSR&#26159;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#20013;&#20248;&#36234;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#32452;&#21512;&#20219;&#21153;&#65292;&#20351;&#29992;Set-BSR&#36827;&#34892;&#38598;&#21512;&#36873;&#25321;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14907</link><description>&lt;p&gt;
&#22522;&#20110;&#35206;&#30422;&#29575;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Coverage-based Example Selection for In-Context Learning. (arXiv:2305.14907v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#29575;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BERTScore-Recall&#24230;&#37327;&#26041;&#27861;&#36873;&#25321;&#26356;&#22909;&#30340;&#31034;&#20363;&#26469;&#23637;&#31034;&#27979;&#35797;&#36755;&#20837;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21516;&#26102;&#36824;&#36890;&#36807;&#25193;&#23637;&#25104;&#38598;&#21512;&#32423;&#21035;&#24230;&#37327;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#34920;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;BSR&#26159;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#20013;&#20248;&#36234;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#32452;&#21512;&#20219;&#21153;&#65292;&#20351;&#29992;Set-BSR&#36827;&#34892;&#38598;&#21512;&#36873;&#25321;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#19968;&#20123;&#20219;&#21153;&#31034;&#20363;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#20174;&#32780;&#23454;&#29616;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#35201;&#27714;&#36825;&#20123;&#31034;&#20363;&#23545;&#27979;&#35797;&#23454;&#20363;&#20855;&#26377;&#20449;&#24687;&#37327;&#12290;&#26631;&#20934;&#30340;&#26041;&#27861;&#26159;&#29420;&#31435;&#22320;&#23545;&#26368;&#30456;&#20284;&#30340;&#31034;&#20363;&#36827;&#34892;&#25490;&#21517;&#21644;&#36873;&#25321;&#65292;&#36825;&#26679;&#36873;&#25321;&#20986;&#30340;&#31034;&#20363;&#20250;&#37325;&#22797;&#19988;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;BERTScore-Recall&#65288;BSR&#65289;&#36873;&#25321;&#20102;&#26356;&#22909;&#30340;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#23637;&#31034;&#20102;&#27979;&#35797;&#36755;&#20837;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#22914;&#25512;&#29702;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;BSR&#21644;&#35768;&#22810;&#26631;&#20934;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#26131;&#20110;&#20248;&#21270;&#30340;&#38598;&#21512;&#32423;&#21035;&#24230;&#37327;&#26041;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35206;&#30422;&#36825;&#20123;&#20851;&#38190;&#26041;&#38754;&#12290;&#22312;&#28085;&#30422;6&#20010;&#20219;&#21153;&#30340;15&#20010;&#25968;&#25454;&#38598;&#21644;7&#20010;&#19981;&#21516;&#30340;LLM&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65288;1&#65289;BSR&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#26041;&#38754;&#26159;&#20248;&#36234;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#65288;2&#65289;&#23545;&#20110;&#32452;&#21512;&#20219;&#21153;&#65292;&#20351;&#29992;Set-BSR&#36827;&#34892;&#38598;&#21512;&#36873;&#25321;&#30340;&#24615;&#33021;&#20248;&#20110;&#29420;&#31435;&#25490;&#21517;&#65292;&#24179;&#22343;&#25552;&#39640;17&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#19988;&#23613;&#31649;&#26080;&#38656;&#35757;&#32451;&#20294;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information. In this work, we show that BERTScore-Recall (BSR) selects better examples that demonstrate more of the salient aspects, e.g. reasoning patterns, of the test input. We further extend BSR and many standard metrics to easily optimizable set-level metrics, giving still better coverage of those salient aspects. On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using Set-BSR outperforms independent ranking by up to 17 points on average and, despite being training-free, surpasses methods that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#29702;&#35299;&#24515;&#26234;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;GPT-4&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.03353</link><description>&lt;p&gt;
MindGames&#65306;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#27169;&#24577;&#36923;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38024;&#23545;&#24515;&#26234;&#29702;&#35770;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#29702;&#35299;&#24515;&#26234;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;GPT-4&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;(ToM)&#26159;&#26234;&#33021;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#20934;&#30830;&#24230;&#37327;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#20105;&#35758;&#35805;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#23558;&#20154;&#31867;ToM&#35780;&#20272;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#20351;&#29992;&#20154;&#31867;&#21019;&#24314;&#30340;&#26631;&#20934;&#21270;&#27979;&#35797;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#25512;&#29702;&#19978;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#19982;ToM&#37325;&#21472;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#26469;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#26032;&#30340;&#35821;&#35328;&#25216;&#24039;&#26469;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#65288;&#20174;70M&#21040;6B&#21644;350M&#21040;174B&#65289;&#24182;&#19981;&#19968;&#33268;&#22320;&#20135;&#29983;&#27604;&#38543;&#26426;&#32467;&#26524;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;GPT-4&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#35748;&#30693;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#20173;&#26377;&#25552;&#21319;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20844;&#24320;&#33719;&#21462;&#65306;https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM) is a critical component of intelligence, yet accurately measuring it continues to be a subject of debate. Prior research has attempted to apply human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. In this study, we utilize dynamic epistemic logic, which has established overlaps with ToM, to generate more intricate problems. We also introduce novel verbalization techniques to express these problems using natural language. Our findings indicate that certain language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates improved epistemic reasoning capabilities, there is still room for enhancement. Our code and datasets are publicly available https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10093</link><description>&lt;p&gt;
&#25552;&#39640;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Role of Context in Region-Word Alignment for Object Detection. (arXiv:2303.10093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23398;&#20064;&#22270;&#20687;-&#26631;&#27880;&#37197;&#23545;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21306;&#22495;-&#35789;&#23545;&#40784;&#65292;&#25512;&#21160;&#20102;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21306;&#22495;-&#35789;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20165;&#38024;&#23545;&#30446;&#26631;&#21517;&#35789;&#22312;&#26816;&#27979;&#20013;&#20351;&#29992;&#65292;&#20854;&#20182;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23646;&#24615;&#65292;&#23545;&#26816;&#27979;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#30446;&#26631;&#26816;&#27979;&#65292;&#24182;&#25552;&#35758;&#22686;&#24378;&#19978;&#19979;&#25991;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#31574;&#30053;&#24615;&#22320;&#23558;&#25509;&#22320;&#39044;&#35757;&#32451;&#30446;&#26631;&#24773;&#22659;&#21270;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23646;&#24615;&#20316;&#20026;&#29305;&#21035;&#26377;&#29992;&#30340;&#30446;&#26631;&#19978;&#19979;&#25991;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#22686;&#21152;&#23545;&#23427;&#20204;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#19982;&#21306;&#22495;-&#35789;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25991;&#26412;-&#21306;&#22495;&#21487;&#35270;&#21270;&#26174;&#31034;&#23646;&#24615;&#25935;&#24863;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language pretraining to learn a fine-grained, region-word alignment between image-caption pairs has propelled progress in open-vocabulary object detection. We observe that region-word alignment methods are typically used in detection with respect to only object nouns, and the impact of other rich context in captions, such as attributes, is unclear. In this study, we explore how language context affects downstream object detection and propose to enhance the role of context. In particular, we show how to strategically contextualize the grounding pretraining objective for improved alignment. We further hone in on attributes as especially useful object context and propose a novel adjective and noun-based negative sampling strategy for increasing their focus in contrastive learning. Overall, our methods enhance object detection when compared to the state-of-the-art in region-word pretraining. We also highlight the fine-grained utility of an attribute-sensitive model through text-regi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#26694;&#26550;CALM&#65292;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#26469;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#35780;&#20272;&#20854;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#23545;&#19981;&#21516;&#34920;&#31034;&#30340;&#20351;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20851;&#31995;&#23646;&#24615;&#30340;&#21033;&#29992;&#23384;&#22312;&#19968;&#23450;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.00333</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Competence-Based Analysis of Language Models. (arXiv:2303.00333v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00333
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#26694;&#26550;CALM&#65292;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#26469;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#35780;&#20272;&#20854;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#23545;&#19981;&#21516;&#34920;&#31034;&#30340;&#20351;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20851;&#31995;&#23646;&#24615;&#30340;&#21033;&#29992;&#23384;&#22312;&#19968;&#23450;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#25552;&#31034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23545;&#36755;&#20837;&#25110;&#24212;&#29992;&#29615;&#22659;&#20013;&#30340;&#24494;&#23567;&#21464;&#21270;&#21364;&#24322;&#24120;&#33030;&#24369;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#34892;&#20026;&#24182;&#28608;&#21169;&#35774;&#35745;&#26356;&#20581;&#22766;&#30340;LMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23454;&#39564;&#26694;&#26550;CALM&#65288;&#22522;&#20110;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#65289;&#65292;&#20854;&#20013;&#21033;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#22240;&#26524;&#24178;&#39044;&#26469;&#30772;&#22351;LM&#22312;&#21508;&#31181;&#35821;&#35328;&#23646;&#24615;&#19978;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#20197;&#35780;&#20272;&#23427;&#22312;&#25191;&#34892;&#32473;&#23450;&#20219;&#21153;&#26102;&#23545;&#27599;&#20010;&#34920;&#31034;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24178;&#39044;&#23454;&#29616;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#19982;&#20808;&#21069;&#30340;&#22240;&#26524;&#25506;&#26597;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20204;&#33021;&#22815;&#38024;&#23545;&#20219;&#24847;&#32534;&#30721;&#30340;&#20851;&#31995;&#23646;&#24615;&#36827;&#34892;&#25915;&#20987;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;BERT-like LMs&#22312;&#25191;&#34892;&#30456;&#20851;&#20851;&#31995;&#25552;&#31034;&#20219;&#21153;&#26102;&#22914;&#20309;&#20351;&#29992;&#22810;&#31181;&#20851;&#31995;&#23646;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#34920;&#31034;&#30340;&#36873;&#25321;&#23545;LM&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#20294;&#27169;&#22411;&#23545;&#26576;&#20123;&#29305;&#23450;&#20851;&#31995;&#23646;&#24615;&#30340;&#21033;&#29992;&#24182;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of large pretrained language models (LMs) on a variety of prompting tasks, these models can be alarmingly brittle to small changes in inputs or application contexts. To better understand such behavior and motivate the design of more robust LMs, we propose a general experimental framework, CALM (Competence-based Analysis of Language Models), where targeted causal interventions are utilized to damage an LM's internal representation of various linguistic properties in order to evaluate its use of each representation in performing a given task. We implement these interventions as gradient-based adversarial attacks, which (in contrast to prior causal probing methodologies) are able to target arbitrarily-encoded representations of relational properties, and carry out a case study of this approach to analyze how BERT-like LMs use representations of several relational properties in performing associated relation prompting tasks. We find that, while the representation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.13709</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#19981;&#33391;&#20559;&#35265;&#65306;&#36991;&#20813;&#34913;&#37327;&#21361;&#26426;
&lt;/p&gt;
&lt;p&gt;
Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13709
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#26222;&#21450;&#65292;&#39044;&#27979;&#20854;&#20351;&#29992;&#21487;&#33021;&#23545;&#20154;&#20204;&#36896;&#25104;&#20260;&#23475;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20010;&#21463;&#21040;&#20851;&#27880;&#30340;&#38382;&#39064;&#26159;&#36825;&#19968;&#25216;&#26415;&#22312;&#34892;&#20026;&#20013;&#26174;&#31034;&#20986;&#26377;&#23475;&#20559;&#35265;&#12290;&#23613;&#31649;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#65292;&#20294;&#25105;&#20204;&#34913;&#37327;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#36890;&#24120;&#19981;&#28165;&#26970;&#23427;&#20204;&#21040;&#24213;&#34913;&#37327;&#20102;&#20160;&#20040;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#35752;&#35770;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#24515;&#29702;&#27979;&#37327;&#23398;&#19987;&#27880;&#20110;&#34913;&#37327;&#19981;&#30452;&#25509;&#21487;&#35266;&#23519;&#21040;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20004;&#20010;&#26680;&#24515;&#27010;&#24565;&#65292;&#21363;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models and Natural Language Processing (NLP) technology rapidly develops and spreads into daily life, it becomes crucial to anticipate how its use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases in its behavior. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems (e.g., it is often unclear what they actually measure). In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20854;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2203.03897</link><description>&lt;p&gt;
&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29992;&#20110;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20854;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#22411;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#23884;&#20837;&#65292;&#24182;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#23398;&#20064;&#21040;&#30340;&#22810;&#27169;&#22411;&#23884;&#20837;&#30340;&#20998;&#26512;&#30456;&#23545;&#36739;&#23569;&#65292;&#23884;&#20837;&#30340;&#21487;&#36716;&#31227;&#24615;&#26377;&#24453;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;CLIP&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#20445;&#30041;&#20102;&#20998;&#31163;&#30340;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#23545;&#40784;&#30340;&#35270;&#35282;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#34913;&#37327;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#24494;&#35843;&#20043;&#21518;&#65292;CLIP&#20173;&#28982;&#20445;&#25345;&#30528;&#36739;&#24046;&#30340;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#21487;&#33021;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#40065;&#26834;&#34920;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23884;&#20837;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#22312;&#36229;&#29699;&#38754;&#19978;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multi-modal models, such as CLIP, provide transferable embeddings and show promising results in diverse applications. However, the analysis of learned multi-modal embeddings is relatively unexplored, and the embedding transferability can be improved. In this work, we observe that CLIP holds separated embedding subspaces for two different modalities, and then we investigate it through the lens of uniformity-alignment to measure the quality of learned representation. Both theoretically and empirically, we show that CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack of alignment and uniformity might restrict the transferability and robustness of embeddings. To this end, we devise a new fine-tuning method for robust representation equipping better alignment and uniformity. First, we propose a Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to generate hard negative samples on the hypersphere. Then, we fine-tune the model on har
&lt;/p&gt;</description></item></channel></rss>