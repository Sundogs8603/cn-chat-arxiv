<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13089</link><description>&lt;p&gt;
GIMLET&#65306;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#25351;&#20196;&#20998;&#23376;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#23454;&#39564;&#36896;&#25104;&#30340;&#26631;&#31614;&#19981;&#36275;&#38382;&#39064;&#23558;&#26159;&#20854;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#30693;&#35782;&#36827;&#34892;&#20219;&#21153;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#20998;&#23376;-&#25991;&#26412;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22788;&#29702;&#25351;&#20196;&#19981;&#36275;&#20197;&#21450;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIMLET&#65292;&#23427;&#32479;&#19968;&#20102;&#22270;&#24418;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#37319;&#29992;&#24191;&#20041;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#25193;&#23637;&#20197;&#32534;&#30721;&#22270;&#24418;&#32467;&#26500;&#21644;&#25351;&#20196;&#25991;&#26412;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22270;&#24418;&#32534;&#30721;&#27169;&#22359;&#12290;GIMLET&#36824;&#22312;&#27880;&#24847;&#26426;&#21046;&#20013;&#35299;&#32806;&#20102;&#22270;&#24418;&#30340;&#32534;&#30721;&#21644;&#20219;&#21153;&#25351;&#20196;&#65292;&#22686;&#24378;&#20102;&#36328;&#26032;&#20219;&#21153;&#30340;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#21644;&#21629;&#21517;&#30740;&#31350;&#20027;&#39064;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;NCI&#22312;&#25918;&#23556;&#31185;&#23398;&#20013;&#30340;210&#20159;&#32654;&#20803;&#36164;&#21161;21&#24180;&#65292;&#21457;&#29616;&#27835;&#30103;&#21644;&#29289;&#29702;&#23398;&#20026;&#22522;&#30784;&#30340;&#30740;&#31350;&#30340;&#36164;&#21161;&#24050;&#32463;&#36229;&#36807;&#20102;&#22522;&#20110;&#35786;&#26029;&#21644;&#29983;&#29289;&#23398;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.13075</link><description>&lt;p&gt;
NCI&#25918;&#23556;&#31185;&#23398;&#36164;&#21161;&#39033;&#30446;&#20013;&#30740;&#31350;&#20027;&#39064;&#21644;&#36235;&#21183;&#30340;&#21322;&#33258;&#21160;&#25552;&#21462;&#65288;2000-2020&#24180;&#65289;
&lt;/p&gt;
&lt;p&gt;
Semi-automated extraction of research topics and trends from NCI funding in radiological sciences from 2000-2020. (arXiv:2306.13075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#21644;&#21629;&#21517;&#30740;&#31350;&#20027;&#39064;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;NCI&#22312;&#25918;&#23556;&#31185;&#23398;&#20013;&#30340;210&#20159;&#32654;&#20803;&#36164;&#21161;21&#24180;&#65292;&#21457;&#29616;&#27835;&#30103;&#21644;&#29289;&#29702;&#23398;&#20026;&#22522;&#30784;&#30340;&#30740;&#31350;&#30340;&#36164;&#21161;&#24050;&#32463;&#36229;&#36807;&#20102;&#22522;&#20110;&#35786;&#26029;&#21644;&#29983;&#29289;&#23398;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#21592;&#12289;&#36164;&#21161;&#26041;&#21644;&#20844;&#20247;&#37117;&#24819;&#20102;&#35299;&#20844;&#20849;&#36164;&#37329;&#36164;&#21161;&#30340;&#30740;&#31350;&#20027;&#39064;&#21644;&#36235;&#21183;&#65292;&#20294;&#30446;&#21069;&#25163;&#21160;&#20998;&#31867;&#30340;&#21162;&#21147;&#22312;&#35268;&#27169;&#21644;&#29702;&#35299;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#21644;&#21629;&#21517;&#30740;&#31350;&#20027;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25918;&#23556;&#31185;&#23398;&#20013;210&#20159;&#32654;&#20803;&#30340;NCI&#36164;&#21161;21&#24180;&#20197;&#30830;&#23450;&#24494;&#35266;&#21644;&#23439;&#35266;&#30740;&#31350;&#20027;&#39064;&#21644;&#36164;&#21161;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#29616;&#26377;&#29983;&#29289;&#21307;&#23398;&#35789;&#21521;&#37327;&#30340;&#24207;&#21015;&#32858;&#31867;&#12289;&#19987;&#23478;&#36827;&#34892;&#21629;&#21517;&#21644;&#21487;&#35270;&#21270;&#20197;&#21457;&#29616;&#23439;&#35266;&#20027;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;15&#20010;&#21644;60&#20010;&#32858;&#31867;&#20027;&#39064;&#30340;&#32467;&#26524;&#65292;&#21457;&#29616;&#25320;&#27454;&#23884;&#20837;&#30340;2D&#25237;&#24433;&#26174;&#31034;&#20004;&#20010;&#20027;&#23548;&#36724;&#65306;&#29289;&#29702;-&#29983;&#29289;&#23398;&#21644;&#27835;&#30103;-&#35786;&#26029;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#27835;&#30103;&#21644;&#29289;&#29702;&#23398;&#20026;&#22522;&#30784;&#30340;&#30740;&#31350;&#30340;&#36164;&#21161;&#24050;&#32463;&#36229;&#36807;&#20102;&#22522;&#20110;&#35786;&#26029;&#21644;&#29983;&#29289;&#23398;&#30340;&#30740;&#31350;&#12290;&#24076;&#26395;&#36825;&#20123;&#32467;&#26524;&#33021;&#22815;&#65306;&#65288;1&#65289;&#25552;&#20379;NCI&#22312;&#25918;&#23556;&#31185;&#23398;&#20013;&#30740;&#31350;&#32463;&#36153;&#20998;&#37197;&#30340;&#35265;&#35299;&#65307;&#65288;2&#65289;&#20419;&#36827;&#30740;&#31350;&#32773;&#21644;&#38750;&#19987;&#23478;&#23545;&#36825;&#20010;&#39046;&#22495;&#30340;&#20851;&#27880;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigators, funders, and the public desire knowledge on topics and trends in publicly funded research but current efforts in manual categorization are limited in scale and understanding. We developed a semi-automated approach to extract and name research topics, and applied this to \$1.9B of NCI funding over 21 years in the radiological sciences to determine micro- and macro-scale research topics and funding trends. Our method relies on sequential clustering of existing biomedical-based word embeddings, naming using subject matter experts, and visualization to discover trends at a macroscopic scale above individual topics. We present results using 15 and 60 cluster topics, where we found that 2D projection of grant embeddings reveals two dominant axes: physics-biology and therapeutic-diagnostic. For our dataset, we found that funding for therapeutics- and physics-based research have outpaced diagnosticsand biology-based research, respectively. We hope these results may (1) give in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23601;&#19981;&#38656;&#35201;&#24494;&#35843;&#27169;&#22411;&#25110;&#35775;&#38382;&#19987;&#26377;&#20449;&#24687;&#30340;&#26041;&#27861;&#36827;&#34892;&#32622;&#20449;&#24230;&#24341;&#23548;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;LLMs&#24448;&#24448;&#23637;&#29616;&#20986;&#39640;&#24230;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;</title><link>http://arxiv.org/abs/2306.13063</link><description>&lt;p&gt;
LLM&#33021;&#34920;&#36798;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#21527;&#65311;LMM&#33258;&#20449;&#24515;&#35780;&#20272;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. (arXiv:2306.13063v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23601;&#19981;&#38656;&#35201;&#24494;&#35843;&#27169;&#22411;&#25110;&#35775;&#38382;&#19987;&#26377;&#20449;&#24687;&#30340;&#26041;&#27861;&#36827;&#34892;&#32622;&#20449;&#24230;&#24341;&#23548;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;LLMs&#24448;&#24448;&#23637;&#29616;&#20986;&#39640;&#24230;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#20934;&#30830;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#30340;&#33021;&#21147;&#65292;&#21363;&#32622;&#20449;&#24230;&#24341;&#23548;&#20219;&#21153;&#65292;&#23545;&#30830;&#20445;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#20915;&#31574;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#38656;&#35201;&#24494;&#35843;&#27169;&#22411;&#25110;&#35775;&#38382;&#19987;&#26377;&#20449;&#24687;&#30340;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#19977;&#31181;&#26041;&#27861;&#65306;&#22522;&#20110;&#34920;&#36848;&#12289;&#22522;&#20110;&#19968;&#33268;&#24615;&#12289;&#20197;&#21450;&#23427;&#20204;&#30340;&#28151;&#21512;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#22312;&#20116;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#21644;&#22235;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#19978;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of empowering large language models (LLMs) to accurately express their confidence, referred to as confidence elicitation, is essential in ensuring reliable and trustworthy decision-making processes. Previous methods, which primarily rely on model logits, have become less suitable for LLMs and even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM APIs). This leads to a growing need to explore the untapped area of \emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence, in this study, we investigate approaches for confidence elicitation that do not require model fine-tuning or access to proprietary information. We introduce three categories of methods: verbalize-based, consistency-based, and their hybrid methods for benchmarking, and evaluate their performance across five types of datasets and four widely-used LLMs. Our analysis of these methods uncovers several key insights: 1) LLMs often exhibit a high degree of overconfidence when 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21322;&#33258;&#21160;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#38024;&#23545;IT&#39046;&#22495;&#30340;&#31616;&#21382;&#36827;&#34892;&#20102;&#35843;&#25972;&#24182;&#25104;&#21151;&#35782;&#21035;&#20843;&#31181;&#19981;&#21516;&#30340;&#23454;&#20307;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.13062</link><description>&lt;p&gt;
&#31616;&#21382;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition in resumes. (arXiv:2306.13062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21322;&#33258;&#21160;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#38024;&#23545;IT&#39046;&#22495;&#30340;&#31616;&#21382;&#36827;&#34892;&#20102;&#35843;&#25972;&#24182;&#25104;&#21151;&#35782;&#21035;&#20843;&#31181;&#19981;&#21516;&#30340;&#23454;&#20307;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#34987;&#29992;&#20110;&#20174;&#21508;&#31181;&#25991;&#26723;&#21644;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20363;&#22914;&#22995;&#21517;&#21644;&#26085;&#26399;&#12290;&#20174;&#31616;&#21382;&#20013;&#25552;&#21462;&#25945;&#32946;&#21644;&#24037;&#20316;&#32463;&#21382;&#20449;&#24687;&#20197;&#36827;&#34892;&#31579;&#36873;&#38750;&#24120;&#37325;&#35201;&#12290;&#32771;&#34385;&#21040;&#31616;&#21382;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#24517;&#39035;&#25163;&#21160;&#36755;&#20837;&#20844;&#21496;&#31995;&#32479;&#65292;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#23558;&#20026;&#20844;&#21496;&#33410;&#30465;&#26102;&#38388;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#37325;&#28857;&#20851;&#27880;IT&#39046;&#22495;&#30340;&#31616;&#21382;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21322;&#33258;&#21160;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;&#26631;&#27880;&#20102;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;IT&#30456;&#20851;&#39046;&#22495;&#21592;&#24037;&#30340;&#31616;&#21382;&#12290;&#20351;&#29992;&#26631;&#27880;&#25968;&#25454;&#65292;&#35843;&#25972;&#20102;&#20845;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#27969;&#34892;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#25152;&#24471;&#31995;&#32479;&#21487;&#20197;&#35782;&#21035;&#20843;&#31181;&#19981;&#21516;&#30340;&#23454;&#20307;&#31867;&#22411;&#65292;&#21253;&#25324;&#22478;&#24066;&#12289;&#26085;&#26399;&#12289;&#23398;&#20301;&#12289;&#27605;&#19994;&#35777;&#20027;&#20462;&#12289;&#32844;&#31216;&#12289;&#35821;&#35328;&#12289;&#22269;&#23478;&#21644;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition (NER) is used to extract information from various documents and texts such as names and dates. It is important to extract education and work experience information from resumes in order to filter them. Considering the fact that all information in a resume has to be entered to the companys system manually, automatizing this process will save time of the companies. In this study, a deep learning-based semi-automatic named entity recognition system has been implemented with a focus on resumes in the field of IT. Firstly, resumes of employees from five different IT related fields has been annotated. Six transformer based pre-trained models have been adapted to named entity recognition problem using the annotated data. These models have been selected among popular models in the natural language processing field. The obtained system can recognize eight different entity types which are city, date, degree, diploma major, job title, language, country and skill. Models u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CamChoice&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#65292;&#20026;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#25552;&#20379;&#20102;&#33258;&#21160;&#35780;&#20272;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.13047</link><description>&lt;p&gt;
CamChoice&#65306;&#19968;&#20221;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#20505;&#36873;&#31572;&#26696;&#20998;&#24067;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions. (arXiv:2306.13047v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CamChoice&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#65292;&#20026;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#25552;&#20379;&#20102;&#33258;&#21160;&#35780;&#20272;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#26159;&#29992;&#20110;&#34913;&#37327;&#20505;&#36873;&#20154;&#22312;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#33021;&#21147;&#30340;&#26222;&#36941;&#35780;&#20272;&#24418;&#24335;&#12290;&#25552;&#20986;&#30340;&#38382;&#39064;&#30340;&#36136;&#37327;&#23545;&#20110;&#27979;&#35797;&#35774;&#35745;&#20154;&#21592;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#27492;&#26032;&#25552;&#20986;&#30340;&#38382;&#39064;&#22312;&#37096;&#32626;&#21040;&#23454;&#38469;&#32771;&#35797;&#20043;&#21069;&#38656;&#35201;&#32463;&#36807;&#20960;&#20010;&#39044;&#27979;&#35797;&#35780;&#20272;&#38454;&#27573;&#12290;&#30446;&#21069;&#65292;&#36825;&#20010;&#36807;&#31243;&#26159;&#30456;&#24403;&#25163;&#21160;&#21270;&#30340;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38382;&#39064;&#24320;&#21457;&#21608;&#26399;&#30340;&#26102;&#38388;&#28382;&#21518;&#12290;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#23558;&#22823;&#22823;&#25552;&#39640;&#25928;&#29575;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#19981;&#21253;&#21547;&#36275;&#22815;&#30340;&#39044;&#27979;&#35797;&#20998;&#26512;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CamChoice&#65306;&#19968;&#20221;&#21253;&#21547;&#19981;&#21516;&#30446;&#26631;&#32423;&#21035;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#30340;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;RACE++&#19978;&#35757;&#32451;&#30340;&#33258;&#21160;&#31995;&#32479;&#21487;&#20197;&#23454;&#29616;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks. Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams. This process is currently quite manual, which can lead to time lags in the question development cycle. Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information. In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions. We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be lev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#26426;&#22120;&#32763;&#35793;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#20379;&#32508;&#21512;&#32508;&#36848;&#21644;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#36129;&#29486;&#19979;&#19968;&#20195;&#26041;&#27861;&#30340;&#24895;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.13041</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Evaluation Metrics for Machine Translation. (arXiv:2306.13041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#26426;&#22120;&#32763;&#35793;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#20379;&#32508;&#21512;&#32508;&#36848;&#21644;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#36129;&#29486;&#19979;&#19968;&#20195;&#26041;&#27861;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35789;&#27719;&#37325;&#21472;&#24230;&#37327;&#65288;&#22914;BLEU&#65289;&#19981;&#21516;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#25351;&#26631;&#65288;&#20363;&#22914;COMET&#25110;BERTScore&#65289;&#22522;&#20110;&#40657;&#30418;&#23376;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#65292;&#20294;&#26159;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36739;&#20302;&#36136;&#37327;&#30340;&#20256;&#32479;&#25351;&#26631;&#20173;&#28982;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20854;&#20013;&#19968;&#20010;&#28508;&#22312;&#21407;&#22240;&#26159;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#26356;&#36879;&#26126;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20419;&#36827;&#26032;&#30340;&#39640;&#36136;&#37327;&#25351;&#26631;&#30340;&#26356;&#24191;&#27867;&#25509;&#21463;&#65292;&#35299;&#37322;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#27010;&#24565;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#35299;&#37322;&#26426;&#22120;&#32763;&#35793;&#25351;&#26631;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#30446;&#26631;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#36817;&#25216;&#26415;&#30340;&#32508;&#21512;&#32508;&#36848;&#65292;&#23558;&#23427;&#20204;&#19982;&#25105;&#20204;&#30830;&#31435;&#30340;&#30446;&#26631;&#21644;&#23646;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;GPT4&#65289;&#30340;&#21487;&#35299;&#37322;&#25351;&#26631;&#30340;&#26368;&#26032;&#20808;&#36827;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19979;&#19968;&#20195;&#26041;&#27861;&#30340;&#24895;&#26223;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;e&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics for machine translation (for example, COMET or BERTScore) are based on black-box large language models. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent. To foster more widespread acceptance of novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties as well as key goals of explainable machine translation metrics and provide a comprehensive synthesis of recent techniques, relating them to our established goals and properties. In this context, we also discuss the latest state-of-the-art approaches to explainable metrics based on generative models such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation approaches, including natural language e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23457;&#26597;&#20102;&#29992;&#20110;&#20247;&#21253;&#20262;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Delphi&#22312;&#32654;&#22269;&#25919;&#27835;&#20105;&#35758;&#38382;&#39064;&#20013;&#30340;&#22238;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#19981;&#33391;&#65292;&#21576;&#29616;&#26174;&#33879;&#30340;&#25919;&#27835;&#20542;&#26012;&#12290;&#20316;&#32773;&#20174;&#25968;&#25454;&#22899;&#26435;&#20027;&#20041;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#20013;&#31435;&#24615;&#30340;&#38382;&#39064;&#65292;&#35752;&#35770;&#20102;&#20013;&#31435;&#24615;&#27010;&#24565;&#22914;&#20309;&#36716;&#31227;&#26435;&#21147;&#12289;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#26080;&#22768;&#30340;&#22768;&#38899;&#12290;</title><link>http://arxiv.org/abs/2306.13000</link><description>&lt;p&gt;
&#26080;&#25919;&#27835;&#26234;&#33021;&#65311;&#23457;&#26597;Delphi&#22312;&#32654;&#22269;&#26377;&#20105;&#35758;&#25919;&#27835;&#38382;&#39064;&#19978;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US. (arXiv:2306.13000v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23457;&#26597;&#20102;&#29992;&#20110;&#20247;&#21253;&#20262;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Delphi&#22312;&#32654;&#22269;&#25919;&#27835;&#20105;&#35758;&#38382;&#39064;&#20013;&#30340;&#22238;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#19981;&#33391;&#65292;&#21576;&#29616;&#26174;&#33879;&#30340;&#25919;&#27835;&#20542;&#26012;&#12290;&#20316;&#32773;&#20174;&#25968;&#25454;&#22899;&#26435;&#20027;&#20041;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#20013;&#31435;&#24615;&#30340;&#38382;&#39064;&#65292;&#35752;&#35770;&#20102;&#20013;&#31435;&#24615;&#27010;&#24565;&#22914;&#20309;&#36716;&#31227;&#26435;&#21147;&#12289;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#26080;&#22768;&#30340;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#34987;&#37096;&#32626;&#65292;&#26377;&#20851;&#23427;&#20204;&#25919;&#27835;&#20215;&#20540;&#30340;&#25285;&#24551;&#24050;&#25104;&#20026;&#21069;&#27839;&#38382;&#39064;&#65292;&#21508;&#20010;&#25919;&#27835;&#27966;&#21035;&#23545;&#20854;&#23384;&#22312;&#20559;&#35265;&#21644;&#32570;&#20047;&#20013;&#31435;&#24615;&#30340;&#25209;&#35780;&#32439;&#33267;&#27795;&#26469;&#12290;&#26412;&#25991;&#36890;&#36807;&#23457;&#35745;&#29992;&#20110;&#20247;&#21253;&#20262;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Delphi [arXiv:2110.07574]&#65292;&#20174;&#25919;&#27835;&#20105;&#35758;&#38382;&#39064;&#30340;&#35282;&#24230;&#65292;&#20998;&#26512;&#20102;Delphi&#19982;&#21508;&#20010;&#32654;&#22269;&#25919;&#27835;&#20998;&#32452;&#30340;&#19981;&#21516;&#22238;&#24212;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;Delphi&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#19981;&#33391;&#65292;&#21576;&#29616;&#26174;&#33879;&#30340;&#25919;&#27835;&#20542;&#26012;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#20316;&#32773;&#20174;&#25968;&#25454;&#22899;&#26435;&#20027;&#20041;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#20013;&#31435;&#24615;&#30340;&#38382;&#39064;&#65292;&#35752;&#35770;&#20102;&#20013;&#31435;&#24615;&#27010;&#24565;&#22914;&#20309;&#36716;&#31227;&#26435;&#21147;&#12289;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#26080;&#22768;&#30340;&#22768;&#38899;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#26395;&#20026;&#20851;&#20110;&#27169;&#22411;&#19982;&#20215;&#20540;&#30340;&#35268;&#33539;&#24615;&#38382;&#39064;&#24418;&#25104;&#26356;&#22810;&#26377;&#24605;&#32771;&#24615;&#30340;&#36777;&#35770;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative language models are deployed in ever-wider contexts, concerns about their political values have come to the forefront with critique from all parts of the political spectrum that the models are biased and lack neutrality. However, the question of what neutrality is and whether it is desirable remains underexplored. In this paper, I examine neutrality through an audit of Delphi [arXiv:2110.07574], a large language model designed for crowdsourced ethics. I analyse how Delphi responds to politically controversial questions compared to different US political subgroups. I find that Delphi is poorly calibrated with respect to confidence and exhibits a significant political skew. Based on these results, I examine the question of neutrality from a data-feminist lens, in terms of how notions of neutrality shift power and further marginalise unheard voices. These findings can hopefully contribute to a more reflexive debate about the normative questions of alignment and what role we 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20219;&#21153;&#65306;&#35821;&#38899;&#24773;&#24863;&#20998;&#27573;&#65288;SED&#65289;&#65292;&#26088;&#22312;&#21453;&#26144;&#35821;&#38899;&#24773;&#24863;&#30340;&#32454;&#31890;&#24230;&#29305;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#35821;&#38899;&#24773;&#24863;&#25968;&#25454;&#38598; ZED&#65292;&#24182;&#25552;&#20379;&#20102;&#31454;&#20105;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2306.12991</link><description>&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#20998;&#27573;&#65306;&#21738;&#31181;&#24773;&#24863;&#22312;&#20309;&#26102;&#20986;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Diarization: Which Emotion Appears When?. (arXiv:2306.12991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20219;&#21153;&#65306;&#35821;&#38899;&#24773;&#24863;&#20998;&#27573;&#65288;SED&#65289;&#65292;&#26088;&#22312;&#21453;&#26144;&#35821;&#38899;&#24773;&#24863;&#30340;&#32454;&#31890;&#24230;&#29305;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#35821;&#38899;&#24773;&#24863;&#25968;&#25454;&#38598; ZED&#65292;&#24182;&#25552;&#20379;&#20102;&#31454;&#20105;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#36890;&#24120;&#20381;&#36182;&#20110;&#35805;&#35821;&#27700;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35821;&#38899;&#20256;&#36798;&#30340;&#24773;&#24863;&#24212;&#34987;&#35270;&#20026;&#20855;&#26377;&#30830;&#23450;&#26102;&#38388;&#36793;&#30028;&#30340;&#31163;&#25955;&#35821;&#38899;&#20107;&#20214;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#35805;&#35821;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#21453;&#26144;&#35821;&#38899;&#24773;&#24863;&#30340;&#32454;&#31890;&#24230;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65306;&#35821;&#38899;&#24773;&#24863;&#20998;&#27573;&#65288;SED&#65289;&#12290;&#27491;&#22914;&#35828;&#35805;&#20154;&#20998;&#27573;&#22238;&#31572;&#8220;&#35841;&#20309;&#26102;&#35828;&#35805;&#65311;&#8221;&#30340;&#38382;&#39064;&#65292;&#35821;&#38899;&#24773;&#24863;&#20998;&#27573;&#22238;&#31572;&#8220;&#21738;&#31181;&#24773;&#24863;&#20309;&#26102;&#20986;&#29616;&#65311;&#8221;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#24615;&#33021;&#35780;&#20272;&#21644;&#20026;&#30740;&#31350;&#20154;&#21592;&#24314;&#31435;&#19968;&#20010;&#20849;&#21516;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Zaion &#24773;&#24863;&#25968;&#25454;&#38598;&#65288;ZED&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#35821;&#38899;&#24773;&#24863;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22312;&#30495;&#23454;&#29983;&#27963;&#26465;&#20214;&#19979;&#35760;&#24405;&#30340;&#38750;&#28436;&#20986;&#24773;&#24863;&#65292;&#20197;&#21450;&#35805;&#35821;&#20013;&#24773;&#24863;&#29255;&#27573;&#30340;&#25163;&#21160;&#27880;&#37322;&#36793;&#30028;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31454;&#20105;&#22522;&#32447;&#65292;&#24182;&#24320;&#28304;&#20102;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition (SER) typically relies on utterance-level solutions. However, emotions conveyed through speech should be considered as discrete speech events with definite temporal boundaries, rather than attributes of the entire utterance. To reflect the fine-grained nature of speech emotions, we propose a new task: Speech Emotion Diarization (SED). Just as Speaker Diarization answers the question of "Who speaks when?", Speech Emotion Diarization answers the question of "Which emotion appears when?". To facilitate the evaluation of the performance and establish a common benchmark for researchers, we introduce the Zaion Emotion Dataset (ZED), an openly accessible speech emotion dataset that includes non-acted emotions recorded in real-life conditions, along with manually-annotated boundaries of emotion segments within the utterance. We provide competitive baselines and open-source the code and the pre-trained models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32771;&#34385;&#20102;&#23545;&#35805;&#30340;&#29992;&#25143;&#21160;&#24577;&#21644;&#20844;&#20247;&#23545;&#35805;&#35328;&#35770;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#23545;&#35805;&#20559;&#31163;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12982</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#35805;&#20559;&#31163;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conversation Derailment Forecasting with Graph Convolutional Networks. (arXiv:2306.12982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32771;&#34385;&#20102;&#23545;&#35805;&#30340;&#29992;&#25143;&#21160;&#24577;&#21644;&#20844;&#20247;&#23545;&#35805;&#35328;&#35770;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#23545;&#35805;&#20559;&#31163;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#19978;&#23545;&#35805;&#23481;&#26131;&#34987;&#20559;&#31163;&#65292;&#34920;&#29616;&#20026;&#19981;&#23562;&#37325;&#35328;&#35770;&#12289;&#35328;&#35821;&#34384;&#24453;&#31561;&#24694;&#24847;&#20132;&#27969;&#27169;&#24335;&#12290;&#39044;&#27979;&#23545;&#35805;&#20559;&#31163;&#21487;&#20197;&#25552;&#21069;&#21457;&#29616;&#20559;&#31163;&#36857;&#35937;&#65292;&#23454;&#29616;&#23545;&#35805;&#30340;&#31215;&#26497;&#31649;&#29702;&#12290;&#30446;&#21069;&#65292;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#26159;&#20351;&#29992;&#23558;&#23545;&#35805;&#35270;&#20026;&#25991;&#26412;&#27969;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#23545;&#35805;&#29992;&#25143;&#21160;&#24577;&#21644;&#20844;&#20247;&#23545;&#35805;&#35328;&#35770;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#21160;&#24577;&#65292;&#24182;&#22312;CGA&#21644;CMV&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;1.5&#65285;&#21644;1.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online conversations are particularly susceptible to derailment, which can manifest itself in the form of toxic communication patterns like disrespectful comments or verbal abuse. Forecasting conversation derailment predicts signs of derailment in advance enabling proactive moderation of conversations. Current state-of-the-art approaches to address this problem rely on sequence models that treat dialogues as text streams. We propose a novel model based on a graph convolutional neural network that considers dialogue user dynamics and the influence of public perception on conversation utterances. Through empirical evaluation, we show that our model effectively captures conversation dynamics and outperforms the state-of-the-art models on the CGA and CMV benchmark datasets by 1.5\% and 1.7\%, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#30740;&#31350;&#20102;Twitter&#29992;&#25143;&#23545;ChatGPT&#30340;&#24577;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#24635;&#20307;&#24773;&#24863;&#26159;&#20013;&#24615;&#21040;&#31215;&#26497;&#30340;&#65292;&#26368;&#21463;&#20851;&#27880;&#30340;&#20027;&#39064;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#12289;&#25628;&#32034;&#24341;&#25806;&#12289;&#25945;&#32946;&#12289;&#20889;&#20316;&#21644;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.12951</link><description>&lt;p&gt;
&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#24314;&#27169;&#36319;&#36394;Twitter&#19978;ChatGPT&#30340;&#20844;&#20247;&#24577;&#24230;
&lt;/p&gt;
&lt;p&gt;
Tracking public attitudes toward ChatGPT on Twitter using sentiment analysis and topic modeling. (arXiv:2306.12951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#30740;&#31350;&#20102;Twitter&#29992;&#25143;&#23545;ChatGPT&#30340;&#24577;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#24635;&#20307;&#24773;&#24863;&#26159;&#20013;&#24615;&#21040;&#31215;&#26497;&#30340;&#65292;&#26368;&#21463;&#20851;&#27880;&#30340;&#20027;&#39064;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#12289;&#25628;&#32034;&#24341;&#25806;&#12289;&#25945;&#32946;&#12289;&#20889;&#20316;&#21644;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#29992;&#25143;&#22522;&#25968;&#22686;&#38271;&#26041;&#38754;&#21019;&#19979;&#20102;&#26032;&#35760;&#24405;&#12290;&#34429;&#28982;&#23427;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#33021;&#21147;&#65292;&#20294;&#23427;&#20063;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20844;&#20247;&#20851;&#27880;&#65292;&#28041;&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#24212;&#29992;&#20110;Twitter&#25968;&#25454;&#26469;&#35843;&#26597;&#20844;&#20247;&#23545;ChatGPT&#30340;&#24577;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#24635;&#20307;&#24773;&#24863;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#20013;&#24615;&#21040;&#31215;&#26497;&#30340;&#65292;&#36825;&#20063;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32844;&#19994;&#32676;&#20307;&#12290;&#22312;&#20247;&#22810;&#25552;&#21040;&#30340;&#20027;&#39064;&#20013;&#65292;&#26368;&#21463;&#20851;&#27880;&#30340;&#20027;&#39064;&#26159;&#20154;&#24037;&#26234;&#33021;&#12289;&#25628;&#32034;&#24341;&#25806;&#12289;&#25945;&#32946;&#12289;&#20889;&#20316;&#21644;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT sets a new record with the fastest-growing user base, as a chatbot powered by a large language model (LLM). While it demonstrates state-of-the-art capabilities in a variety of language-generating tasks, it also raises widespread public concerns regarding its societal impact. In this paper, we utilize natural language processing approaches to investigate the public attitudes towards ChatGPT by applying sentiment analysis and topic modeling techniques to Twitter data. Our result shows that the overall sentiment is largely neutral to positive, which also holds true across different occupation groups. Among a wide range of topics mentioned in tweets, the most popular topics are Artificial Intelligence, Search Engines, Education, Writing, and Question Answering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#25480;&#27880;&#24847;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#26469;&#28040;&#38500;&#31163;&#32676;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;transformer&#30340;&#21487;&#37327;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12929</link><description>&lt;p&gt;
&#21487;&#37327;&#21270;Transformer&#65306;&#36890;&#36807;&#24110;&#21161;&#27880;&#24847;&#21147;&#22836;&#8220;&#20160;&#20040;&#20063;&#19981;&#20570;&#8221;&#21435;&#38500;&#31163;&#32676;&#20540;
&lt;/p&gt;
&lt;p&gt;
Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#25480;&#27880;&#24847;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#26469;&#28040;&#38500;&#31163;&#32676;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;transformer&#30340;&#21487;&#37327;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;Transformer&#27169;&#22411;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26174;&#33879;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#30001;&#20110;&#20854;&#35268;&#27169;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#65292;&#20294;&#36825;&#26159;&#20197;&#26497;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#20026;&#20195;&#20215;&#30340;&#12290;&#37327;&#21270;&#26159;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#26102;&#38388;&#21644;&#23384;&#20648;&#22120;&#28040;&#32791;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;transformer&#27169;&#22411;&#24448;&#24448;&#23398;&#20064;&#21040;&#20854;&#28608;&#27963;&#20013;&#30340;&#24378;&#31163;&#32676;&#20540;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#37327;&#21270;&#12290;&#20026;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#31163;&#32676;&#20540;&#30340;&#23384;&#22312;&#38656;&#35201;&#23558;&#28608;&#27963;&#32622;&#20110;&#26356;&#39640;&#30340;&#27604;&#29305;&#23485;&#24230;&#25110;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#23383;&#26684;&#24335;&#65292;&#36827;&#34892;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#20854;&#20182;&#21464;&#36890;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24378;&#31163;&#32676;&#20540;&#19982;&#29305;&#23450;&#27880;&#24847;&#22836;&#34892;&#20026;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#22836;&#35797;&#22270;&#23398;&#20064;&#8220;&#26080;&#25805;&#20316;&#8221;&#25110;&#20165;&#20165;&#26159;&#37096;&#20998;&#27531;&#24046;&#26356;&#26032;&#12290;&#20026;&#20102;&#23454;&#29616;&#27880;&#24847;&#21147;&#22836;&#20013;&#38656;&#35201;&#30340;&#31934;&#30830;&#38646;&#20301;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#25945;&#25480;&#27880;&#24847;&#21147;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#30340;&#37327;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#29992;&#20302;&#31934;&#24230;&#37327;&#21270;&#29978;&#33267;&#26159;&#24378;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention 
&lt;/p&gt;</description></item><item><title>AudioPaLM&#26159;&#19968;&#27454;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#35821;&#38899;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32487;&#25215;&#20102;AudioLM&#30340;&#35821;&#38899;&#36523;&#20221;&#21644;&#35821;&#35843;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#20197;&#21450;PaLM-2&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#21487;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#31561;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.12925</link><description>&lt;p&gt;
AudioPaLM&#65306;&#19968;&#27454;&#33021;&#35828;&#20250;&#21548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AudioPaLM: A Large Language Model That Can Speak and Listen. (arXiv:2306.12925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12925
&lt;/p&gt;
&lt;p&gt;
AudioPaLM&#26159;&#19968;&#27454;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#35821;&#38899;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32487;&#25215;&#20102;AudioLM&#30340;&#35821;&#38899;&#36523;&#20221;&#21644;&#35821;&#35843;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#20197;&#21450;PaLM-2&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#21487;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;AudioPaLM&#12290;&#23427;&#23558;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;PaLM-2[Anil&#31561;&#20154;&#65292;2023]&#21644;&#22522;&#20110;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;AudioLM[Borsos&#31561;&#20154;&#65292;2022]&#32467;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#32467;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#65292;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#38899;&#32763;&#35793;&#31561;&#24212;&#29992;&#12290;AudioPaLM&#32487;&#25215;&#20102;&#20174;AudioLM&#20013;&#20445;&#30041;&#35821;&#38899;&#21457;&#38899;&#32773;&#36523;&#20221;&#21644;&#35821;&#35843;&#31561;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21482;&#23384;&#22312;&#20110;&#25991;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;PaLM-2&#20013;&#30340;&#35821;&#35328;&#30693;&#35782;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#21487;&#20197;&#25913;&#21892;&#35821;&#38899;&#22788;&#29702;&#65292;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#26356;&#22823;&#37327;&#30340;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#26469;&#21327;&#21161;&#35821;&#38899;&#20219;&#21153;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#31995;&#32479;&#65292;&#24182;&#19988;&#20855;&#26377;&#36827;&#34892;&#38646;-shot&#35328;&#35821;&#25991;&#26412;&#36716;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;&#20219;&#21153;&#65292;&#20351;&#29992;&#21382;&#21490;&#24187;&#24819;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#25688;&#35201;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;CLCTS&#35821;&#26009;&#24211;&#65292;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21450;&#20854;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#65307;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;ChatGPT&#22312;CLCTS&#20013;&#20316;&#20026;&#25688;&#35201;&#22120;&#21644;&#35780;&#20272;&#22120;&#30340;&#28508;&#21147;&#12290;&#26368;&#32456;&#21457;&#29616;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#20135;&#29983;&#20102;&#20013;&#31561;&#21040;&#24046;&#30340;&#25928;&#26524;&#65292;&#32780;ChatGPT&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20013;&#31561;&#21040;&#22909;&#30340;&#25688;&#35201;&#36136;&#37327;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12916</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;&#65306;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation. (arXiv:2306.12916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;&#20219;&#21153;&#65292;&#20351;&#29992;&#21382;&#21490;&#24187;&#24819;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#25688;&#35201;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;CLCTS&#35821;&#26009;&#24211;&#65292;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21450;&#20854;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#65307;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;ChatGPT&#22312;CLCTS&#20013;&#20316;&#20026;&#25688;&#35201;&#22120;&#21644;&#35780;&#20272;&#22120;&#30340;&#28508;&#21147;&#12290;&#26368;&#32456;&#21457;&#29616;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#20135;&#29983;&#20102;&#20013;&#31561;&#21040;&#24046;&#30340;&#25928;&#26524;&#65292;&#32780;ChatGPT&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20013;&#31561;&#21040;&#22909;&#30340;&#25688;&#35201;&#36136;&#37327;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25688;&#35201;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;(CLCTS)&#26159;&#19968;&#20010;&#28508;&#21147;&#24040;&#22823;&#20294;&#40092;&#26377;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#23427;&#26377;&#21487;&#33021;&#25552;&#39640;&#36328;&#25991;&#21270;&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#20449;&#24687;&#20849;&#20139;&#21644;&#29702;&#35299;&#12290;&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;CLCTS&#20219;&#21153;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21019;&#24314;&#12289;&#24314;&#27169;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;CLCTS&#35821;&#26009;&#24211;&#65292;&#21033;&#29992;&#21382;&#21490;&#24187;&#24819;&#25991;&#26412;&#21644;&#33521;&#35821;&#12289;&#24503;&#35821;&#32500;&#22522;&#30334;&#31185;&#25688;&#35201;&#65292;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#31471;&#21040;&#31471;&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#19981;&#21516;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#22312;CLCTS&#20013;&#20316;&#20026;&#25688;&#35201;&#22120;&#21644;&#35780;&#20272;&#22120;&#30340;&#28508;&#21147;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#20154;&#31867;&#12289;ChatGPT&#20197;&#21450;&#20960;&#20010;&#26368;&#36817;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#20135;&#29983;&#20102;&#20174;&#24046;&#21040;&#20013;&#31561;&#30340;&#25688;&#35201;&#36136;&#37327;&#65307;ChatGPT&#20316;&#20026;&#25688;&#35201;&#22120;(&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;)&#65292;&#25552;&#20379;&#20102;&#20013;&#31561;&#21040;&#22909;&#30340;&#25688;&#35201;&#36136;&#37327;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility, information sharing, and understanding. This paper comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We build the first CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in English and German, and examine the effectiveness of popular transformer end-to-end models with different intermediate task finetuning tasks. Additionally, we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report evaluations from humans, ChatGPT, and several recent automatic evaluation metrics where we find our intermediate task finetuned end-to-end models generate bad to moderate quality summaries; ChatGPT as a summarizer (without any finetuning) provides moderate to good qua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#38544;&#24335;&#35821;&#35328;&#20449;&#24687;&#24314;&#27169;&#30340;&#35821;&#38899;&#35821;&#35328;&#26085;&#24535;&#20998;&#31163;&#20219;&#21153;&#65292;&#20351;&#29992; x-vector &#26041;&#27861;&#36827;&#34892;&#20998;&#31163;&#26102;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20998;&#21035;&#20026; 6.78%/7.06% &#21644; 22.50%/60.38%&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340; wave2vec &#23884;&#20837;&#21521;&#37327;&#21487;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12913</link><description>&lt;p&gt;
&#38544;&#24335;&#35821;&#38899;&#35821;&#35328;&#26085;&#24535;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Implicit spoken language diarization. (arXiv:2306.12913v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#38544;&#24335;&#35821;&#35328;&#20449;&#24687;&#24314;&#27169;&#30340;&#35821;&#38899;&#35821;&#35328;&#26085;&#24535;&#20998;&#31163;&#20219;&#21153;&#65292;&#20351;&#29992; x-vector &#26041;&#27861;&#36827;&#34892;&#20998;&#31163;&#26102;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20998;&#21035;&#20026; 6.78%/7.06% &#21644; 22.50%/60.38%&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340; wave2vec &#23884;&#20837;&#21521;&#37327;&#21487;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#26085;&#24535;&#20998;&#31163;&#65288;LD&#65289;&#21450;&#30456;&#20851;&#20219;&#21153;&#20027;&#35201;&#20351;&#29992;&#38899;&#20301;&#27861;&#36827;&#34892;&#30740;&#31350;&#12290;&#38899;&#20301;&#27861;&#20027;&#35201;&#20351;&#29992;&#26174;&#24335;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#22240;&#27492;&#38656;&#35201;&#20013;&#38388;&#30340;&#38899;&#32032;&#24314;&#27169;&#21644;&#36716;&#24405;&#25968;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24314;&#27169;&#26102;&#38388;&#21160;&#24577;&#24615;&#30340;&#33021;&#21147;&#21487;&#33021;&#26377;&#21161;&#20110;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#21521;&#37327;&#38544;&#24335;&#22320;&#24314;&#27169;&#35821;&#35328;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken language diarization (LD) and related tasks are mostly explored using the phonotactic approach. Phonotactic approaches mostly use explicit way of language modeling, hence requiring intermediate phoneme modeling and transcribed data. Alternatively, the ability of deep learning approaches to model temporal dynamics may help for the implicit modeling of language information through deep embedding vectors. Hence this work initially explores the available speaker diarization frameworks that capture speaker information implicitly to perform LD tasks. The performance of the LD system on synthetic code-switch data using the end-to-end x-vector approach is 6.78% and 7.06%, and for practical data is 22.50% and 60.38%, in terms of diarization error rate and Jaccard error rate (JER), respectively. The performance degradation is due to the data imbalance and resolved to some extent by using pre-trained wave2vec embeddings that provide a relative improvement of 30.74% in terms of JER.
&lt;/p&gt;</description></item><item><title>xSIM++ &#26159;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#35780;&#20998;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#27604;&#25991;&#26412;&#25366;&#25496;&#34920;&#29616;&#30340;&#35780;&#20272;&#12290;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;(xSIM), xSIM++&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#24182;&#25552;&#39640;&#20102;&#19982;&#32763;&#35793;&#31995;&#32479;BLEU&#24471;&#20998;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12907</link><description>&lt;p&gt;
xSIM++&#65306;&#20302;&#36164;&#28304;&#35821;&#35328;&#27604;&#25991;&#26412;&#25366;&#25496;&#34920;&#29616;&#30340;&#25913;&#36827;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages. (arXiv:2306.12907v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12907
&lt;/p&gt;
&lt;p&gt;
xSIM++ &#26159;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#35780;&#20998;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#27604;&#25991;&#26412;&#25366;&#25496;&#34920;&#29616;&#30340;&#35780;&#20272;&#12290;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;(xSIM), xSIM++&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#24182;&#25552;&#39640;&#20102;&#19982;&#32763;&#35793;&#31995;&#32479;BLEU&#24471;&#20998;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#35780;&#20998;&#26469;&#35780;&#20272;&#22522;&#20110;&#22810;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#30456;&#20284;&#24615;&#30340;&#27604;&#25991;&#26412;&#25366;&#25496;&#65306;xSIM++&#12290;&#30456;&#36739;&#20110;xSIM&#65292;&#36825;&#20010;&#25913;&#36827;&#30340;&#20195;&#29702;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#22312;&#20219;&#20309;&#35780;&#20272;&#38598;&#20013;&#25193;&#23637;&#33521;&#35821;&#21477;&#23376;&#19982;&#21512;&#25104;&#30340;&#12289;&#38590;&#20197;&#21306;&#20998;&#30340;&#20363;&#23376;&#65292;&#36825;&#26356;&#25509;&#36817;&#20110;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#25366;&#25496;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#32452;&#20302;&#36164;&#28304;&#35821;&#35328;&#36816;&#34892;&#22823;&#37327;&#27604;&#25991;&#26412;&#25366;&#25496;&#23454;&#39564;&#65292;&#38543;&#21518;&#22312;&#25366;&#25496;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#26469;&#39564;&#35777;&#36825;&#20010;&#20195;&#29702;&#12290;&#19982;xSIM&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;xSIM++&#19982;&#35757;&#32451;&#20110;&#25366;&#25496;&#27604;&#25991;&#26412;&#30340;&#32763;&#35793;&#31995;&#32479;&#19979;&#28216;BLEU&#35780;&#20998;&#20043;&#38388;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#20026;&#20102;&#27604;&#25991;&#26412;&#25366;&#25496;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#20195;&#29702;&#65292;&#26080;&#38656;&#36816;&#34892;&#26114;&#36149;&#30340;&#27604;&#25991;&#26412;&#25366;&#25496;&#31649;&#36947;&#12290;xSIM++&#36824;&#25253;&#21578;&#20102;&#19981;&#21516;&#38169;&#35823;&#31867;&#22411;&#30340;&#24615;&#33021;&#65292;&#20026;&#27169;&#22411;&#24320;&#21457;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new proxy score for evaluating bitext mining based on similarity in a multilingual embedding space: xSIM++. In comparison to xSIM, this improved proxy leverages rule-based approaches to extend English sentences in any evaluation set with synthetic, hard-to-distinguish examples which more closely mirror the scenarios we encounter during large-scale mining. We validate this proxy by running a significant number of bitext mining experiments for a set of low-resource languages, and subsequently train NMT systems on the mined data. In comparison to xSIM, we show that xSIM++ is better correlated with the downstream BLEU scores of translation systems trained on mined bitexts, providing a reliable proxy of bitext mining performance without needing to run expensive bitext mining pipelines. xSIM++ also reports performance for different error types, offering more fine-grained feedback for model development.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#25910;&#38598;&#20102;&#32422;1.5&#30334;&#19975;&#26465;&#28085;&#30422;60&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25512;&#25991;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#25512;&#29305;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;&#20420;&#20044;&#20914;&#31361;&#30340;&#26032;&#38395;&#23186;&#20307;&#25253;&#36947;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#21487;&#20197;&#35782;&#21035;&#19982;&#35813;&#35805;&#39064;&#30456;&#20851;&#30340;&#20027;&#20307;&#12289;&#31435;&#22330;&#12289;&#27010;&#24565;&#21644;&#24773;&#24863;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2306.12886</link><description>&lt;p&gt;
&#20840;&#29699;&#21465;&#20107;&#30340;&#25581;&#31034;&#65306;&#19968;&#20221;&#22810;&#35821;&#31181;&#25512;&#29305;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20420;&#20044;&#20914;&#31361;&#30340;&#26032;&#38395;&#23186;&#20307;&#25253;&#36947;
&lt;/p&gt;
&lt;p&gt;
Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict. (arXiv:2306.12886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12886
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25910;&#38598;&#20102;&#32422;1.5&#30334;&#19975;&#26465;&#28085;&#30422;60&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25512;&#25991;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#25512;&#29305;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;&#20420;&#20044;&#20914;&#31361;&#30340;&#26032;&#38395;&#23186;&#20307;&#25253;&#36947;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#21487;&#20197;&#35782;&#21035;&#19982;&#35813;&#35805;&#39064;&#30456;&#20851;&#30340;&#20027;&#20307;&#12289;&#31435;&#22330;&#12289;&#27010;&#24565;&#21644;&#24773;&#24863;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20420;&#20044;&#20914;&#31361;&#19968;&#30452;&#37117;&#26159;&#20840;&#29699;&#23186;&#20307;&#23494;&#38598;&#25253;&#36947;&#30340;&#20027;&#39064;&#12290;&#20102;&#35299;&#36825;&#20010;&#35805;&#39064;&#32972;&#21518;&#30340;&#20840;&#29699;&#21465;&#20107;&#23545;&#20110;&#26088;&#22312;&#20174;&#22810;&#20010;&#23618;&#38754;&#33719;&#21462;&#27934;&#35265;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#24182;&#22788;&#29702;&#19990;&#30028;&#21508;&#22320;&#26032;&#38395;&#25110;&#23186;&#20307;&#20844;&#21496;&#21457;&#24067;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25512;&#25991;&#65292;&#37325;&#28857;&#20851;&#27880;&#36825;&#20010;&#35805;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;2022&#24180;2&#26376;&#33267;2023&#24180;5&#26376;&#30340;&#25512;&#25991;&#65292;&#20197;&#25910;&#38598;&#32422;1.5&#30334;&#19975;&#26465;&#20351;&#29992;60&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25512;&#25991;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#25512;&#25991;&#37117;&#38468;&#24102;&#26377;&#22788;&#29702;&#36807;&#30340;&#26631;&#31614;&#65292;&#20801;&#35768;&#23545;&#25552;&#21040;&#30340;&#20027;&#20307;&#12289;&#31435;&#22330;&#12289;&#27010;&#24565;&#21644;&#34920;&#36798;&#30340;&#24773;&#24863;&#36827;&#34892;&#35782;&#21035;&#12290;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20026;&#24076;&#26395;&#20174;&#19981;&#21516;&#26041;&#38754;&#35843;&#26597;&#20420;&#20044;&#20914;&#31361;&#30340;&#20840;&#29699;&#21465;&#20107;&#65292;&#20363;&#22914;&#35841;&#26159;&#20027;&#35201;&#30340;&#30456;&#20851;&#26041;&#12289;&#25345;&#20160;&#20040;&#24577;&#24230;&#12289;&#36825;&#20123;&#24577;&#24230;&#30340;&#26469;&#28304;&#22312;&#21738;&#37324;&#20197;&#21450;&#19981;&#21516;&#30340;&#27010;&#24565;&#22914;&#20309;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ongoing Russo-Ukrainian conflict has been a subject of intense media coverage worldwide. Understanding the global narrative surrounding this topic is crucial for researchers that aim to gain insights into its multifaceted dimensions. In this paper, we present a novel dataset that focuses on this topic by collecting and processing tweets posted by news or media companies on social media across the globe. We collected tweets from February 2022 to May 2023 to acquire approximately 1.5 million tweets in 60 different languages. Each tweet in the dataset is accompanied by processed tags, allowing for the identification of entities, stances, concepts, and sentiments expressed. The availability of the dataset serves as a valuable resource for researchers aiming to investigate the global narrative surrounding the ongoing conflict from various aspects such as who are the prominent entities involved, what stances are taken, where do these stances originate, and how are the different concepts 
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#35780;&#36848;NLP&#22312;&#30005;&#23376;&#30149;&#21382;&#20013;&#21457;&#29616;&#20855;&#26377;&#23454;&#29616;&#20020;&#24202;&#20915;&#31574;&#30340;&#28508;&#22312;&#26426;&#20250;&#65292;&#20294;&#20173;&#38656;&#20811;&#26381;&#25968;&#25454;&#30340;&#26631;&#20934;&#21270;&#21644;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#30340;&#38656;&#35201;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.12834</link><description>&lt;p&gt;
&#19982;&#21307;&#30103;&#20915;&#31574;&#30456;&#20851;&#30340;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing in Electronic Health Records in Relation to Healthcare Decision-making: A Systematic Review. (arXiv:2306.12834v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12834
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#35780;&#36848;NLP&#22312;&#30005;&#23376;&#30149;&#21382;&#20013;&#21457;&#29616;&#20855;&#26377;&#23454;&#29616;&#20020;&#24202;&#20915;&#31574;&#30340;&#28508;&#22312;&#26426;&#20250;&#65292;&#20294;&#20173;&#38656;&#20811;&#26381;&#25968;&#25454;&#30340;&#26631;&#20934;&#21270;&#21644;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#30340;&#38656;&#35201;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24191;&#27867;&#29992;&#20110;&#20174;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#20013;&#25552;&#21462;&#20020;&#24202;&#35265;&#35299;&#65292;&#20294;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#65292;&#33258;&#21160;&#21270;&#24037;&#20855;&#21644;&#20854;&#20182;&#25361;&#25112;&#20351;&#24471;&#20840;&#38754;&#21033;&#29992;NLP&#36827;&#34892;EHR&#21463;&#21040;&#38459;&#30861;&#12290;&#26412;&#25991;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;NLP&#25216;&#26415;&#65292;&#20197;&#20840;&#38754;&#20102;&#35299;&#36825;&#20010;&#39046;&#22495;&#30340;&#38480;&#21046;&#21644;&#26426;&#20250;&#12290; &#26041;&#27861;&#65306;&#22312;&#31579;&#36873;&#20102;11&#20010;&#25968;&#25454;&#24211;&#20013;&#30340;261&#31687;&#25991;&#31456;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;127&#31687;&#25991;&#31456;&#36827;&#34892;&#20840;&#25991;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#19971;&#31867;&#25991;&#31456;&#65306;1&#65289;&#21307;&#23398;&#31508;&#35760;&#20998;&#31867;&#65292;2&#65289;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#65292;3&#65289;&#25991;&#26412;&#25688;&#35201;&#65292;4&#65289;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#36801;&#31227;&#23398;&#20064;&#26550;&#26500;&#65292;5&#65289;&#20449;&#24687;&#25552;&#21462;&#65292;6&#65289;&#21307;&#30103;&#35821;&#35328;&#32763;&#35793;&#21644;7&#65289;&#20854;&#20182;NLP&#24212;&#29992;&#31243;&#24207;&#12290;&#36825;&#39033;&#30740;&#31350;&#36981;&#24490;&#20102;&#31995;&#32479;&#35780;&#36848;&#21644;Meta&#20998;&#26512;&#30340;&#25253;&#21578;&#39318;&#36873;&#39033;&#30446;&#65288;PRISMA&#65289;&#25351;&#21335;&#12290; &#32467;&#26524;&#21644;&#35752;&#35770;&#65306;EHR&#26159;&#25152;&#36873;&#25991;&#31456;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21307;&#30103;&#20915;&#31574;&#20219;&#21153;&#65288;&#22914;&#21307;&#23398;&#31508;&#35760;&#20998;&#31867;&#65292;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#21644;EHR&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#65289;&#20013;&#65292;NLP&#25216;&#26415;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#26631;&#20934;&#21270;&#25968;&#25454;&#21644;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#24314;&#35758;&#38656;&#35201;&#36827;&#19968;&#27493;&#21162;&#21147;&#25913;&#36827;NLP&#27169;&#22411;&#22312;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#24615;&#33021;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Natural Language Processing (NLP) is widely used to extract clinical insights from Electronic Health Records (EHRs). However, the lack of annotated data, automated tools, and other challenges hinder the full utilisation of NLP for EHRs. Various Machine Learning (ML), Deep Learning (DL) and NLP techniques are studied and compared to understand the limitations and opportunities in this space comprehensively.  Methodology: After screening 261 articles from 11 databases, we included 127 papers for full-text review covering seven categories of articles: 1) medical note classification, 2) clinical entity recognition, 3) text summarisation, 4) deep learning (DL) and transfer learning architecture, 5) information extraction, 6) Medical language translation and 7) other NLP applications. This study follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines.  Result and Discussion: EHR was the most commonly used data type among the selected art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;DSTC 11 Track 4&#20013;&#38024;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#25552;&#20379;&#32473;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#65292;&#24182;&#24635;&#32467;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#21450;&#20854;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12794</link><description>&lt;p&gt;
DSTC 11 Track 4&#20013;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4. (arXiv:2306.12794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;DSTC 11 Track 4&#20013;&#38024;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#25552;&#20379;&#32473;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#65292;&#24182;&#24635;&#32467;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#21450;&#20854;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#20986;&#29616;&#21644;&#24555;&#36895;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#24182;&#38543;&#20043;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#33258;&#21160;&#35780;&#20272;&#30340;&#21508;&#31181;&#25361;&#25112;&#12290;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#21160;&#35780;&#20272;&#20316;&#20026;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#24050;&#32463;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#19968;&#30452;&#22312;&#21162;&#21147;&#25552;&#39640;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#23581;&#35797;&#35780;&#20272;&#23427;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#32500;&#24230;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#23427;&#20204;&#30340;&#37325;&#28857;&#20027;&#35201;&#38598;&#20013;&#20110;&#33521;&#35821;&#35821;&#35328;&#19978;&#12290;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#20419;&#36827;&#20102;&#24320;&#21457;&#21487;&#38752;&#30340;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#12289;&#32500;&#24230;&#21644;&#35821;&#35328;&#20013;&#37117;&#33021;&#22815;&#20351;&#29992;&#12290;DSTC11&#20013;&#30340;&#36825;&#20010;&#36712;&#36947;&#26159;&#20419;&#36827;&#40065;&#26834;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#30340;&#25345;&#32493;&#21162;&#21147;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#20379;&#32473;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#36712;&#36947;&#30340;&#25552;&#20132;&#21644;&#32467;&#26524;&#32454;&#33410;&#12290;&#26412;&#25991;&#36824;&#24635;&#32467;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#21450;&#20854;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics' correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result det
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#23450;&#20041;&#20102;&#20174;&#19977;&#20010;&#26041;&#38754;&#34913;&#37327;OOD&#40065;&#26834;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#19982;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;OOD&#40065;&#26834;&#24615;&#36739;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#22330;&#26223;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#38024;&#23545;&#36896;&#25104;&#40065;&#26834;&#24615;&#36739;&#24369;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.12756</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;:&#22522;&#20110;&#36229;&#20986;&#20998;&#24067;&#35270;&#35282;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective. (arXiv:2306.12756v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#23450;&#20041;&#20102;&#20174;&#19977;&#20010;&#26041;&#38754;&#34913;&#37327;OOD&#40065;&#26834;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#19982;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;OOD&#40065;&#26834;&#24615;&#36739;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#22330;&#26223;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#38024;&#23545;&#36896;&#25104;&#40065;&#26834;&#24615;&#36739;&#24369;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#26631;&#35782;&#31526;&#26469;&#26816;&#32034;&#25991;&#26723;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#26469;&#24320;&#21457;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#21364;&#24471;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#24403;&#19968;&#20010;&#26032;&#30340;&#26816;&#32034;&#33539;&#24335;&#36827;&#20837;&#21040;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26102;&#65292;&#34913;&#37327;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21363;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#22914;&#20309;&#27867;&#21270;&#21040;&#26032;&#30340;&#20998;&#24067;&#20013;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#26816;&#32034;&#38382;&#39064;&#30340;&#19977;&#20010;&#26041;&#38754;&#23450;&#20041;OOD&#40065;&#26834;&#24615;&#65306;1&#65289;&#26597;&#35810;&#21464;&#21270;&#65307;2&#65289;&#26410;&#30693;&#30340;&#26597;&#35810;&#31867;&#22411;&#65307;3&#65289;&#26410;&#30693;&#20219;&#21153;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#20960;&#20010;&#20195;&#34920;&#24615;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#19982;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22312;OOD&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;OOD&#40065;&#26834;&#24615;&#27604;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;OOD&#22330;&#26223;&#20013;&#26356;&#26126;&#26174;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36896;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#40065;&#26834;&#24615;&#36739;&#24369;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23427;&#20204;OOD&#27867;&#21270;&#24615;&#33021;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, we have witnessed generative retrieval increasingly gaining attention in the information retrieval (IR) field, which retrieves documents by directly generating their identifiers. So far, much effort has been devoted to developing effective generative retrieval models. There has been less attention paid to the robustness perspective. When a new retrieval paradigm enters into the real-world application, it is also critical to measure the out-of-distribution (OOD) generalization, i.e., how would generative retrieval models generalize to new distributions. To answer this question, firstly, we define OOD robustness from three perspectives in retrieval problems: 1) The query variations; 2) The unforeseen query types; and 3) The unforeseen tasks. Based on this taxonomy, we conduct empirical studies to analyze the OOD robustness of several representative generative retrieval models against dense retrieval models. The empirical results indicate that the OOD robustness of generative re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; GEMEL &#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#65292;&#20165;&#35843;&#25972;&#20102;&#26497;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340; MEL &#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12725</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Generative Multimodal Entity Linking. (arXiv:2306.12725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; GEMEL &#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#65292;&#20165;&#35843;&#25972;&#20102;&#26497;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340; MEL &#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26159;&#23558;&#24102;&#26377;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#25552;&#21450;&#26144;&#23556;&#21040;&#30693;&#35782;&#24211;&#65288;&#20363;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#30340;&#24341;&#29992;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GEMEL &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#12290;&#25105;&#20204;&#20445;&#25345;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#21482;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#23618;&#20197;&#21551;&#29992;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#20026;&#20102;&#23558; LLMs &#36866;&#24212; MEL &#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992; LLMs &#30340;&#26032;&#20852;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#26816;&#32034;&#22810;&#27169;&#24577;&#23454;&#20363;&#20316;&#20026;&#31034;&#33539;&#26469;&#36827;&#34892;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#35843;&#25972;&#20102;&#22823;&#32422;0.3&#65285;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;GEMEL &#23601;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Entity Linking (MEL) is the task of mapping mentions with multimodal contexts to the referent entities from a knowledge base (e.g., Wikipedia). Prior MEL methods mainly focus on designing complex multimodal interaction mechanisms and require fine-tuning all model parameters, which can be prohibitively costly and difficult to scale in the era of Large Language Models (LLMs). In this work, we propose GEMEL, a simple yet effective Generative Multimodal Entity Linking method, which leverages the capabilities of LLMs from large-scale pre-training to directly generate target entity names. We keep the vision and language model frozen and only train a linear layer to enable cross-modality interactions. To adapt LLMs to the MEL task, we take advantage of the emerging in-context learning (ICL) capability of LLMs by retrieving multimodal instances as demonstrations. Extensive experiments show that with only ~0.3% of the model parameters fine-tuned, GEMEL achieves state-of-the-art resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#24191;&#21578;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#20174;&#22522;&#20110;&#27169;&#26495;&#30340;&#26041;&#27861;&#21040;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#24230;&#37327;&#20248;&#21270;&#12289;&#24544;&#23454;&#24230;&#12289;&#22810;&#26679;&#24615;&#12289;&#22810;&#27169;&#24577;&#20197;&#21450;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#31561;&#20851;&#38190;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.12719</link><description>&lt;p&gt;
&#24191;&#21578;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation for Advertising: A Survey. (arXiv:2306.12719v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#24191;&#21578;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#20174;&#22522;&#20110;&#27169;&#26495;&#30340;&#26041;&#27861;&#21040;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#24230;&#37327;&#20248;&#21270;&#12289;&#24544;&#23454;&#24230;&#12289;&#22810;&#26679;&#24615;&#12289;&#22810;&#27169;&#24577;&#20197;&#21450;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#31561;&#20851;&#38190;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#24110;&#21161;&#24191;&#21578;&#21830;&#22686;&#21152;&#22312;&#32447;&#24191;&#21578;&#25968;&#37327;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36235;&#21183;&#65292;&#20174;&#22522;&#20110;&#27169;&#26495;&#30340;&#26041;&#27861;&#21040;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#32508;&#36848;&#20013;&#25581;&#31034;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26041;&#21521;&#65292;&#21253;&#25324;&#24230;&#37327;&#20248;&#21270;&#12289;&#24544;&#23454;&#24230;&#12289;&#22810;&#26679;&#24615;&#12289;&#22810;&#27169;&#24577;&#20197;&#21450;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language generation methods have emerged as effective tools to help advertisers increase the number of online advertisements they produce. This survey entails a review of the research trends on this topic over the past decade, from template-based to extractive and abstractive approaches using neural networks. Additionally, key challenges and directions revealed through the survey, including metric optimization, faithfulness, diversity, multimodality, and the development of benchmark datasets, are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;60,000&#26465;&#27874;&#26031;&#35821;&#30340;&#31038;&#20132;&#24494;&#21338;&#21475;&#35821;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#20998;&#26512;&#31038;&#20132;&#24494;&#21338;&#20013;&#30340;&#21475;&#35821;&#25991;&#26412;&#24773;&#24863;&#12290;</title><link>http://arxiv.org/abs/2306.12679</link><description>&lt;p&gt;
&#26500;&#24314;&#27874;&#26031;&#31038;&#20132;&#24494;&#21338;&#21475;&#35821;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Constructing Colloquial Dataset for Persian Sentiment Analysis of Social Microblogs. (arXiv:2306.12679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;60,000&#26465;&#27874;&#26031;&#35821;&#30340;&#31038;&#20132;&#24494;&#21338;&#21475;&#35821;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#20998;&#26512;&#31038;&#20132;&#24494;&#21338;&#20013;&#30340;&#21475;&#35821;&#25991;&#26412;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;&#65306;&#24494;&#21338;&#32593;&#31449;&#24050;&#25104;&#20026;&#24773;&#24863;&#20998;&#26512;&#21644;&#35266;&#28857;&#25366;&#25496;&#30340;&#20016;&#23500;&#25968;&#25454;&#28304;&#12290;&#20294;&#30001;&#20110;&#24494;&#21338;&#24120;&#32570;&#23569;&#21477;&#27861;&#19968;&#33268;&#30340;&#26415;&#35821;&#21644;&#20195;&#34920;&#24615;&#65292;&#22240;&#27492;&#24773;&#24863;&#20998;&#31867;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#27874;&#26031;&#35821;&#35328;&#26377;&#20854;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#38656;&#35201;&#29420;&#29305;&#30340;&#27880;&#37322;&#25968;&#25454;&#21644;&#27169;&#22411;&#26469;&#23436;&#25104;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#21327;&#20316;&#29615;&#22659;&#21644;&#20869;&#37096;&#26469;&#28304;&#26041;&#24335;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ITRC-Opinion&#30340;&#29992;&#25143;&#24847;&#35265;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#20998;&#26512;&#31038;&#20132;&#24494;&#21338;&#20013;&#30340;&#21475;&#35821;&#25991;&#26412;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: Microblogging websites have massed rich data sources for sentiment analysis and opinion mining. In this regard, sentiment classification has frequently proven inefficient because microblog posts typically lack syntactically consistent terms and representatives since users on these social networks do not like to write lengthy statements. Also, there are some limitations to low-resource languages. The Persian language has exceptional characteristics and demands unique annotated data and models for the sentiment analysis task, which are distinctive from text features within the English dialect. Method: This paper first constructs a user opinion dataset called ITRC-Opinion by collaborative environment and insource way. Our dataset contains 60,000 informal and colloquial Persian texts from social microblogs such as Twitter and Instagram. Second, this study proposes a new deep convolutional neural network (CNN) model for more effective sentiment analysis of colloquial text in s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#23558;&#23569;&#37327;&#30417;&#30563;&#24335;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25968;&#25454;&#65292;&#24182;&#29992;&#27492;&#26041;&#27861;&#23545;&#36890;&#29992;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.12659</link><description>&lt;p&gt;
Instruct-FinGPT: &#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#26222;&#36866;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models. (arXiv:2306.12659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#23558;&#23569;&#37327;&#30417;&#30563;&#24335;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25968;&#25454;&#65292;&#24182;&#29992;&#27492;&#26041;&#27861;&#23545;&#36890;&#29992;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#21457;&#29616;&#37329;&#34701;&#25991;&#31456;&#12289;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#27934;&#23519;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22609;&#36896;&#25105;&#20204;&#23545;&#24066;&#22330;&#36208;&#21521;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#38754;&#20855;&#26377;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#20934;&#30830;&#35299;&#35835;&#25968;&#23383;&#20540;&#24182;&#25235;&#20303;&#37329;&#34701;&#32972;&#26223;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#37329;&#34701;&#24773;&#24863;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#23569;&#37327;&#30340;&#30417;&#30563;&#24335;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25968;&#25454;&#65292;&#24182;&#29992;&#27492;&#26041;&#27861;&#23545;&#36890;&#29992;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24335;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#65292;&#20197;&#21450;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#65292;&#22914;ChatGPT&#21644;LLaMAs&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#23383;&#29702;&#35299;&#21644;&#32972;&#26223;&#29702;&#35299;&#26159;&#20851;&#38190;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and fine-tuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#32597;&#35265;&#30149;&#34920;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#26080;&#38656;&#23545;&#22823;&#37327;&#35821;&#26009;&#36827;&#34892;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.12656</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37492;&#21035;&#21644;&#25552;&#21462;&#32597;&#35265;&#30149;&#34920;&#22411;
&lt;/p&gt;
&lt;p&gt;
Identifying and Extracting Rare Disease Phenotypes with Large Language Models. (arXiv:2306.12656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12656
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#32597;&#35265;&#30149;&#34920;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#26080;&#38656;&#23545;&#22823;&#37327;&#35821;&#26009;&#36827;&#34892;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32597;&#35265;&#30149;&#65288;RDs&#65289;&#22312;&#20840;&#29699;&#24433;&#21709;&#30528;3&#20159;&#20154;&#65292;&#27491;&#30830;&#30340;&#34920;&#22411;&#20998;&#31867;&#23545;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;RD&#34920;&#22411;&#36890;&#24120;&#23884;&#20837;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#65292;&#25163;&#21160;&#25552;&#21462;&#36807;&#31243;&#32791;&#26102;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20197;&#33258;&#21160;&#25552;&#21462;&#65292;&#20294;&#20854;&#35757;&#32451;&#20013;&#38656;&#35201;&#22823;&#37327;&#24102;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#12290; &#36817;&#26399;&#65292;&#25552;&#31034;&#23398;&#20064;&#20316;&#20026;&#21487;&#20197;&#23548;&#33268;&#26356;&#21487;&#25512;&#24191;&#32467;&#26524;&#30340;NLP&#33539;&#20363;&#20986;&#29616;&#20102;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#65288;&#38646;&#26679;&#26412;&#65289;&#25110;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;&#23569;&#26679;&#26412;&#65289;&#12290;&#23613;&#31649;&#23545;ChatGPT&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#36825;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#36981;&#24490;&#22797;&#26434;&#30340;&#20154;&#31867;&#25552;&#31034;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22238;&#22797;&#65292;&#20294;&#23578;&#26410;&#26377;&#20154;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30740;&#31350;&#20854;&#22312;RD NER&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#22411;&#25552;&#31034;&#20197;&#25552;&#21462;RD&#34920;&#22411;&#65292;&#24182;&#25454;&#25105;&#20204;&#25152;&#30693;&#26159;&#39318;&#27425;&#24314;&#31435;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rare diseases (RDs) are collectively common and affect 300 million people worldwide. Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually. While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training. Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot). Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings. To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.12619</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22914;&#26524;&#23558;CIL&#23450;&#24335;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#26631;&#31614;&#29983;&#25104;&#38382;&#39064;&#65292;&#21017;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;CF&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CIL&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#20102;&#35789;&#27719;&#34920;&#30340;&#31232;&#30095;&#24615;&#20197;&#20415;&#20110;&#29983;&#25104;&#65292;&#24182;&#20351;&#29992;&#26631;&#31614;&#35821;&#20041;&#21019;&#24314;&#20266;&#37325;&#25773;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VAG&#30340;&#24615;&#33021;&#27604;&#22522;&#32447;&#22823;&#24133;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31649;&#36947;&#26694;&#26550;&#65292;&#37319;&#29992;&#20998;&#23618;&#25628;&#32034;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#25968;&#25454;&#36873;&#25321;&#30340;&#39640;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26631;&#20934;&#21270;&#21644;&#28165;&#29702;&#20803;&#25968;&#25454;&#24182;&#38598;&#25104;&#19981;&#21516;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#19968;&#31181;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20043;&#38388;&#33258;&#21160;&#21270;&#25366;&#25496;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12596</link><description>&lt;p&gt;
TalkBank&#22810;&#25968;&#25454;&#24211;&#21033;&#29992;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Approach to exploiting Multiple Datasets from TalkBank. (arXiv:2306.12596v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31649;&#36947;&#26694;&#26550;&#65292;&#37319;&#29992;&#20998;&#23618;&#25628;&#32034;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#25968;&#25454;&#36873;&#25321;&#30340;&#39640;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26631;&#20934;&#21270;&#21644;&#28165;&#29702;&#20803;&#25968;&#25454;&#24182;&#38598;&#25104;&#19981;&#21516;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#19968;&#31181;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20043;&#38388;&#33258;&#21160;&#21270;&#25366;&#25496;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TalkBank&#26159;&#19968;&#20010;&#22312;&#32447;&#35821;&#35328;&#23398;&#30740;&#31350;&#25968;&#25454;&#20849;&#20139;&#25968;&#25454;&#24211;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;TalkBank API&#20855;&#26377;&#26377;&#38480;&#30340;&#25968;&#25454;&#36807;&#28388;&#21644;&#25209;&#22788;&#29702;&#21151;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31649;&#36947;&#26694;&#26550;&#65292;&#37319;&#29992;&#20998;&#23618;&#25628;&#32034;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#25968;&#25454;&#36873;&#25321;&#30340;&#39640;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23545;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#38656;&#35201;&#30340;&#30456;&#20851;&#25991;&#38598;&#36827;&#34892;&#24555;&#36895;&#30340;&#21021;&#27493;&#31579;&#36873;&#65292;&#28982;&#21518;&#26681;&#25454;&#20855;&#20307;&#26631;&#20934;&#36827;&#34892;&#30446;&#26631;&#25968;&#25454;&#30340;&#28145;&#20837;&#25628;&#32034;&#12290;&#30830;&#23450;&#30340;&#25991;&#20214;&#38543;&#21518;&#34987;&#32034;&#24341;&#65292;&#20026;&#26410;&#26469;&#30340;&#20998;&#26512;&#25552;&#20379;&#26356;&#23481;&#26131;&#30340;&#35775;&#38382;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#26631;&#20934;&#21270;&#21644;&#28165;&#29702;&#20803;&#25968;&#25454;&#65292;&#23558;&#20351;&#29992;&#35813;&#26694;&#26550;&#31574;&#21010;&#30340;&#19981;&#21516;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#25104;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20174;&#19968;&#20010;&#22823;&#22411;&#38598;&#25104;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#34429;&#28982;&#35813;&#26694;&#26550;&#26159;&#20026;TalkBank&#35774;&#35745;&#30340;&#65292;&#20294;&#20063;&#21487;&#20197;&#36866;&#29992;&#20110;&#22788;&#29702;&#20854;&#20182;&#24320;&#25918;&#31185;&#23398;&#24179;&#21488;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
TalkBank is an online database that facilitates the sharing of linguistics research data. However, the existing TalkBank's API has limited data filtering and batch processing capabilities. To overcome these limitations, this paper introduces a pipeline framework that employs a hierarchical search approach, enabling efficient complex data selection. This approach involves a quick preliminary screening of relevant corpora that a researcher may need, and then perform an in-depth search for target data based on specific criteria. The identified files are then indexed, providing easier access for future analysis. Furthermore, the paper demonstrates how data from different studies curated with the framework can be integrated by standardizing and cleaning metadata, allowing researchers to extract insights from a large, integrated dataset. While being designed for TalkBank, the framework can also be adapted to process data from other open-science platforms.
&lt;/p&gt;</description></item><item><title>ARIES&#26159;&#19968;&#20221;&#21253;&#21547;&#31185;&#23398;&#35770;&#25991;&#20462;&#35746;&#30340;&#35821;&#26009;&#24211;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#65292;&#21457;&#29616;&#20854;&#22312;&#23547;&#25214;&#23545;&#24212;&#30340;&#20462;&#35746;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#20462;&#35746;&#26102;&#36807;&#20998;&#36981;&#24490;&#21453;&#39304;&#30340;&#25514;&#36766;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#25972;&#20307;&#30340;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.12587</link><description>&lt;p&gt;
ARIES: &#19968;&#20221;&#21253;&#21547;&#31185;&#23398;&#35770;&#25991;&#20462;&#35746;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20123;&#20462;&#35746;&#26159;&#20316;&#20026;&#23545;&#21516;&#34892;&#35780;&#23457;&#30340;&#22238;&#24212;&#32780;&#36827;&#34892;&#30340;
&lt;/p&gt;
&lt;p&gt;
ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews. (arXiv:2306.12587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12587
&lt;/p&gt;
&lt;p&gt;
ARIES&#26159;&#19968;&#20221;&#21253;&#21547;&#31185;&#23398;&#35770;&#25991;&#20462;&#35746;&#30340;&#35821;&#26009;&#24211;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#65292;&#21457;&#29616;&#20854;&#22312;&#23547;&#25214;&#23545;&#24212;&#30340;&#20462;&#35746;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#20462;&#35746;&#26102;&#36807;&#20998;&#36981;&#24490;&#21453;&#39304;&#30340;&#25514;&#36766;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#25972;&#20307;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#21516;&#34892;&#21453;&#39304;&#20462;&#25913;&#31185;&#23398;&#35770;&#25991;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#28145;&#21402;&#30340;&#31185;&#23398;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#35782;&#21035;&#39640;&#32423;&#21453;&#39304;&#20013;&#30340;&#38544;&#21547;&#24847;&#20041;&#65292;&#24182;&#22312;&#20247;&#22810;&#21487;&#33021;&#30340;&#26041;&#24335;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#26041;&#24335;&#26469;&#26356;&#26032;&#25163;&#31295;&#12290;&#25105;&#20204;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;ARIES&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35780;&#35770;&#21450;&#20854;&#30456;&#24212;&#30340;&#35770;&#25991;&#20462;&#35746;&#65292;&#20197;&#20415;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#30340;&#20004;&#20010;&#29256;&#26412;&#65306;&#35780;&#35770;-&#20462;&#35746;&#23545;&#40784;&#21644;&#20462;&#35746;&#29983;&#25104;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-4&#12290;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#35780;&#35770;&#20197;&#38388;&#25509;&#26041;&#24335;&#34920;&#36848;&#25110;&#20462;&#35746;&#28041;&#21450;&#35780;&#35770;&#30340;&#20027;&#26088;&#32780;&#38750;&#31934;&#30830;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#23545;&#24212;&#20110;&#35780;&#35770;&#30340;&#20462;&#35746;&#12290;&#22312;&#29983;&#25104;&#20462;&#35746;&#26102;&#65292;GPT-4&#36890;&#24120;&#33021;&#22815;&#22312;&#34920;&#38754;&#19978;&#22788;&#29702;&#22909;&#35780;&#35770;&#65292;&#20294;&#23427;&#36807;&#20998;&#36981;&#24490;&#21453;&#39304;&#30340;&#25514;&#36766;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#25972;&#20307;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revising scientific papers based on peer feedback is a challenging task that requires not only deep scientific knowledge and reasoning, but also the ability to recognize the implicit requests in high-level feedback and to choose the best of many possible ways to update the manuscript in response. We introduce this task for large language models and release ARIES, a dataset of review comments and their corresponding paper edits, to enable training and evaluating models. We study two versions of the task: comment-edit alignment and edit generation, and evaluate several baselines, including GPT-4. We find that models struggle even to identify the edits that correspond to a comment, especially in cases where the comment is phrased in an indirect way or where the edit addresses the spirit of a comment but not the precise request. When tasked with generating edits, GPT-4 often succeeds in addressing comments on a surface level, but it rigidly follows the wording of the feedback rather than t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#21033;&#29992;&#27425;&#35201;&#30340;&#38899;&#38901;&#29305;&#24449;&#26469;&#25913;&#36827;&#24418;&#24577;&#23398;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20843;&#20010;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#36873;&#25321;&#35201;&#21253;&#25324;&#30340;&#38899;&#38901;&#29305;&#24449;&#21644;&#23427;&#20204;&#30340;&#25805;&#20316;&#26041;&#24335;&#23545;&#24615;&#33021;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#65292;&#32780;&#22312;&#35821;&#35328;&#23398;&#20013;&#36890;&#24120;&#19981;&#34987;&#32771;&#34385;&#30340;&#38899;&#38901;&#23646;&#24615;&#23545;&#21464;&#24418;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2306.12581</link><description>&lt;p&gt;
&#29992;&#38899;&#38901;&#29305;&#24449;&#36827;&#34892;&#24418;&#24577;&#23398;&#21464;&#24418;
&lt;/p&gt;
&lt;p&gt;
Morphological Inflection with Phonological Features. (arXiv:2306.12581v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#21033;&#29992;&#27425;&#35201;&#30340;&#38899;&#38901;&#29305;&#24449;&#26469;&#25913;&#36827;&#24418;&#24577;&#23398;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20843;&#20010;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#36873;&#25321;&#35201;&#21253;&#25324;&#30340;&#38899;&#38901;&#29305;&#24449;&#21644;&#23427;&#20204;&#30340;&#25805;&#20316;&#26041;&#24335;&#23545;&#24615;&#33021;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#65292;&#32780;&#22312;&#35821;&#35328;&#23398;&#20013;&#36890;&#24120;&#19981;&#34987;&#32771;&#34385;&#30340;&#38899;&#38901;&#23646;&#24615;&#23545;&#21464;&#24418;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#22823;&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#37325;&#26032;&#65289;&#21464;&#24418;&#21644;&#20998;&#26512;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20174;&#32780;&#22312;&#35299;&#20915;&#24418;&#24577;&#23398;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23588;&#20854;&#26159;&#24403;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#24456;&#23569;&#25110;&#25512;&#24191;&#21040;&#20043;&#21069;&#26410;&#35265;&#30340;&#35789;&#26465;&#26102;&#65292;&#36825;&#20123;&#24418;&#24577;&#23398;&#20219;&#21153;&#19981;&#33021;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#20102;&#12290;&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#24418;&#24577;&#27169;&#22411;&#33719;&#24471;&#30446;&#26631;&#24418;&#24577;&#23398;&#36807;&#31243;&#30340;&#27425;&#35201;&#35821;&#38899;&#29305;&#24449;&#30340;&#21508;&#31181;&#26041;&#24335;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65306;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#27169;&#22411;&#20445;&#30041;&#20026;&#26087;&#27169;&#22411;&#20294;&#25805;&#20316;&#25968;&#25454;&#20197;&#21253;&#21547;&#29305;&#24449;&#32780;&#19981;&#26159;&#23383;&#31526;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#25805;&#20316;&#27169;&#22411;&#20197;&#22312;&#26500;&#24314;&#38899;&#32032;&#34920;&#31034;&#26102;&#32771;&#34385;&#38899;&#38901;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#27973;&#34920;&#23383;&#24418;&#21040;&#38899;&#32032;&#26144;&#23556;&#30340;&#35821;&#35328;&#30340;&#35821;&#35328;&#29305;&#23450;&#35821;&#27861;&#20174;&#26631;&#20934;&#23383;&#24418;&#25968;&#25454;&#25910;&#38598;&#38899;&#32032;&#25968;&#25454;&#65292;&#24182;&#23545;&#20843;&#31181;&#35821;&#35328;&#23545;&#19978;&#30340;&#20004;&#20010;&#21464;&#24418;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#20960;&#20046;&#27599;&#20010;&#26696;&#20363;&#37117;&#33719;&#24471;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#36873;&#25321;&#35201;&#21253;&#25324;&#30340;&#38899;&#38901;&#29305;&#24449;&#20197;&#21450;&#23427;&#20204;&#30340;&#25805;&#20316;&#26041;&#24335;&#23545;&#24615;&#33021;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#65292;&#32780;&#22312;&#35821;&#35328;&#23398;&#20013;&#36890;&#24120;&#19981;&#32771;&#34385;&#30340;&#38899;&#38901;&#23646;&#24615;&#23545;&#21464;&#24418;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have brought great advances into solving morphological tasks, mostly due to powerful neural models applied to various tasks as (re)inflection and analysis. Yet, such morphological tasks cannot be considered solved, especially when little training data is available or when generalizing to previously unseen lemmas. This work explores effects on performance obtained through various ways in which morphological models get access to subcharacter phonological features that are the targets of morphological processes. We design two methods to achieve this goal: one that leaves models as is but manipulates the data to include features instead of characters, and another that manipulates models to take phonological features into account when building representations for phonemes. We elicit phonemic data from standard graphemic data using language-specific grammars for languages with shallow grapheme-to-phoneme mapping, and we experiment with two reinflection models over eight language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#30340;ASR&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#21487;&#20197;&#23545;ASR&#20551;&#35774;&#36827;&#34892;&#25490;&#21517;&#65292;&#24182;&#22312;&#36873;&#25321;&#28508;&#22312;&#38169;&#35823;&#30340;&#26679;&#26412;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.12577</link><description>&lt;p&gt;
NoRefER: &#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26080;&#21442;&#32771;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning. (arXiv:2306.12577v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#30340;ASR&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#21487;&#20197;&#23545;ASR&#20551;&#35774;&#36827;&#34892;&#25490;&#21517;&#65292;&#24182;&#22312;&#36873;&#25321;&#28508;&#22312;&#38169;&#35823;&#30340;&#26679;&#26412;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NoRefER&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#21442;&#32771;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;ASR&#35780;&#20272;&#25351;&#26631;&#38656;&#35201;&#26114;&#36149;&#30340;&#30495;&#23454;&#25991;&#26412;&#25104;&#32489;&#21333;&#12290;NoRefER&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#23402;&#29983;&#32593;&#32476;&#26550;&#26500;&#30340;&#23545;&#27604;&#23398;&#20064;&#35843;&#25972;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;ASR&#20551;&#35774;&#36827;&#34892;&#20004;&#20004;&#25490;&#21517;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#33258;&#30417;&#30563;&#30340;NoRefER&#21033;&#29992;&#23545;&#26469;&#33258;ASR&#30340;&#22810;&#20010;&#21387;&#32553;&#32423;&#21035;&#30340;&#20551;&#35774;&#20043;&#38388;&#30340;&#24050;&#30693;&#36136;&#37327;&#20851;&#31995;&#36827;&#34892;&#30340;&#23398;&#20064;&#65292;&#20197;&#25353;&#36136;&#37327;&#25490;&#24207;&#21333;&#20010;&#26679;&#26412;&#20869;&#30340;&#20551;&#35774;&#65292;&#36825;&#23545;&#20110;&#27169;&#22411;&#27604;&#36739;&#33267;&#20851;&#37325;&#35201;&#12290;&#21322;&#30417;&#30563;&#29256;&#26412;&#36824;&#20351;&#29992;&#24341;&#29992;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#20854;&#26679;&#26412;&#38388;&#36136;&#37327;&#25490;&#21517;&#65292;&#23545;&#20110;&#36873;&#25321;&#28508;&#22312;&#38169;&#35823;&#30340;&#26679;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NoRefER&#19982;&#22522;&#20110;&#21442;&#32771;&#30340;&#25351;&#26631;&#21450;&#20854;&#26679;&#26412;&#20869;&#25490;&#24207;&#39640;&#24230;&#30456;&#20851;&#65292;&#34920;&#26126;&#20855;&#26377;&#26080;&#21442;&#32771;ASR&#35780;&#20272;&#25110;A/B&#27979;&#35797;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces NoRefER, a novel referenceless quality metric for automatic speech recognition (ASR) systems. Traditional reference-based metrics for evaluating ASR systems require costly ground-truth transcripts. NoRefER overcomes this limitation by fine-tuning a multilingual language model for pair-wise ranking ASR hypotheses using contrastive learning with Siamese network architecture. The self-supervised NoRefER exploits the known quality relationships between hypotheses from multiple compression levels of an ASR for learning to rank intra-sample hypotheses by quality, which is essential for model comparisons. The semi-supervised version also uses a referenced dataset to improve its inter-sample quality ranking, which is crucial for selecting potentially erroneous samples. The results indicate that NoRefER correlates highly with reference-based metrics and their intra-sample ranks, indicating a high potential for referenceless ASR evaluation or a/b testing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21517;&#20026; NeuBAROCO &#30340;&#25968;&#25454;&#38598;&#65292;&#26816;&#39564;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#20449;&#24565;&#20559;&#35265;&#12289;&#36716;&#21270;&#38169;&#35823;&#21644;&#27675;&#22260;&#25928;&#24212;&#31561;&#38382;&#39064;&#26102;&#34920;&#29616;&#27424;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.12567</link><description>&lt;p&gt;
&#29992; NeuBAROCO &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#27573;&#35770;&#25512;&#29702;&#33021;&#21147;&#21644;&#20154;&#31867;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases. (arXiv:2306.12567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21517;&#20026; NeuBAROCO &#30340;&#25968;&#25454;&#38598;&#65292;&#26816;&#39564;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#20449;&#24565;&#20559;&#35265;&#12289;&#36716;&#21270;&#38169;&#35823;&#21644;&#27675;&#22260;&#25928;&#24212;&#31561;&#38382;&#39064;&#26102;&#34920;&#29616;&#27424;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#23384;&#22312;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#19977;&#27573;&#35770;&#25512;&#29702;&#65292;&#36825;&#26159;&#20154;&#20204;&#22312;&#25512;&#29702;&#35748;&#30693;&#31185;&#23398;&#20013;&#30740;&#31350;&#36807;&#30340;&#25512;&#29702;&#24418;&#24335;&#12290;&#20026;&#20102;&#26041;&#20415;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; NeuBAROCO &#30340;&#25968;&#25454;&#38598;&#65292;&#26368;&#21021;&#26159;&#20026;&#35780;&#20272;&#20154;&#31867;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#36923;&#36753;&#33021;&#21147;&#32780;&#35774;&#35745;&#30340;&#24515;&#29702;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#33521;&#25991;&#21644;&#26085;&#25991;&#30340;&#19977;&#27573;&#35770;&#25512;&#29702;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#35266;&#23519;&#21040;&#30340;&#19977;&#31181;&#20559;&#35265;&#31867;&#22411;&#65306;&#20449;&#24565;&#20559;&#35265;&#12289;&#36716;&#21270;&#38169;&#35823;&#21644;&#27675;&#22260;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#36825;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#26102;&#26356;&#23481;&#26131;&#20986;&#29616;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates whether current large language models exhibit biases in logical reasoning, similar to humans. Specifically, we focus on syllogistic reasoning, a well-studied form of inference in the cognitive science of human deduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO, originally designed for psychological experiments that assess human logical abilities in syllogistic reasoning. The dataset consists of syllogistic inferences in both English and Japanese. We examine three types of biases observed in human syllogistic reasoning: belief biases, conversion errors, and atmosphere effects. Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;SituatedGen&#65292;&#35201;&#27714;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#29983;&#25104;&#19968;&#23545;&#23545;&#27604;&#21477;&#23376;&#65292;&#20197;&#34701;&#20837;&#22320;&#29702;&#21644;&#26102;&#38388;&#32972;&#26223;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#20855;&#26377;&#24120;&#35782;&#21512;&#29702;&#24615;&#30340;&#21477;&#23376;&#30340;&#29983;&#25104;&#65292;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12552</link><description>&lt;p&gt;
SituatedGen: &#23558;&#22320;&#29702;&#21644;&#26102;&#38388;&#32972;&#26223;&#34701;&#20837;&#29983;&#25104;&#24335;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning. (arXiv:2306.12552v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;SituatedGen&#65292;&#35201;&#27714;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#29983;&#25104;&#19968;&#23545;&#23545;&#27604;&#21477;&#23376;&#65292;&#20197;&#34701;&#20837;&#22320;&#29702;&#21644;&#26102;&#38388;&#32972;&#26223;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#20855;&#26377;&#24120;&#35782;&#21512;&#29702;&#24615;&#30340;&#21477;&#23376;&#30340;&#29983;&#25104;&#65292;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29983;&#25104;&#24335;&#24120;&#35782;&#25512;&#29702;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#35201;&#27714;&#26426;&#22120;&#22312;&#32473;&#23450;&#19968;&#32452;&#20851;&#38190;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#24120;&#35782;&#21512;&#29702;&#24615;&#32452;&#21512;&#20986;&#19968;&#21477;&#36830;&#36143;&#30340;&#21477;&#23376;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#38024;&#23545;&#29983;&#25104;&#24335;&#24120;&#35782;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#20381;focus everyday scenarios&#65292;&#20294;&#26159;&#26426;&#22120;&#22312;&#29305;&#23450;&#30340;&#22320;&#29702;&#21644;&#26102;&#38388;&#32972;&#26223;&#19979;&#29702;&#35299;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;SituatedGen&#65292;&#35201;&#27714;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#29983;&#25104;&#19968;&#23545;&#23545;&#27604;&#21477;&#23376;&#65292;&#32473;&#23450;&#30340;&#20851;&#38190;&#35789;&#21253;&#25324;&#22320;&#29702;&#25110;&#26102;&#38388;&#23454;&#20307;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#20221;&#30456;&#24212;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;8,268&#23545;&#23545;&#27604;&#21477;&#23376;&#65292;&#36825;&#20123;&#21477;&#23376;&#24314;&#31435;&#22312;&#29616;&#26377;&#30340;&#20960;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#19978;&#65292;&#20154;&#24037;&#24037;&#20316;&#37327;&#26368;&#23567;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#24120;&#35782;&#21512;&#29702;&#24615;&#30340;&#21477;&#23376;&#65292;&#24182;&#19988;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, commonsense reasoning in text generation has attracted much attention. Generative commonsense reasoning is the task that requires machines, given a group of keywords, to compose a single coherent sentence with commonsense plausibility. While existing datasets targeting generative commonsense reasoning focus on everyday scenarios, it is unclear how well machines reason under specific geographical and temporal contexts. We formalize this challenging task as SituatedGen, where machines with commonsense should generate a pair of contrastive sentences given a group of keywords including geographical or temporal entities. We introduce a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor. Experiments show that state-of-the-art generative language models struggle to generate sentences with commonsense plausibility and still lag far behind human performance. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;RVL-CDIP&#22522;&#20934;&#30340;&#20960;&#20010;&#19981;&#33391;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#24314;&#26032;&#30340;&#25991;&#20214;&#20998;&#31867;&#22522;&#20934;&#30340;&#24314;&#35758;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2306.12550</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;RVL-CDIP&#36827;&#34892;&#25991;&#20214;&#20998;&#31867;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Evaluation of Document Classification using RVL-CDIP. (arXiv:2306.12550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;RVL-CDIP&#22522;&#20934;&#30340;&#20960;&#20010;&#19981;&#33391;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#24314;&#26032;&#30340;&#25991;&#20214;&#20998;&#31867;&#22522;&#20934;&#30340;&#24314;&#35758;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RVL-CDIP&#22522;&#20934;&#24050;&#24191;&#27867;&#29992;&#20110;&#34913;&#37327;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#20854;&#20351;&#29992;&#24191;&#27867;&#65292;&#20294;&#25105;&#20204;&#25581;&#31034;&#20102;RVL-CDIP&#22522;&#20934;&#30340;&#20960;&#20010;&#19981;&#33391;&#29305;&#24449;&#65292;&#21253;&#25324;&#65288;1&#65289;&#22823;&#37327;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#25105;&#20204;&#20272;&#35745;&#20026;8.1&#65285;&#65288;&#27599;&#20010;&#25991;&#26723;&#31867;&#21035;&#20171;&#20110;1.6&#65285;&#21040;16.9&#65285;&#19981;&#31561;&#65289;;&#65288;2&#65289;&#23384;&#22312;&#35768;&#22810;&#19981;&#26126;&#30830;&#25110;&#22810;&#26631;&#31614;&#25991;&#26723;;&#65288;3&#65289;&#27979;&#35797;&#21644;&#35757;&#32451;&#20998;&#35010;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#37325;&#21472;&#65292;&#36825;&#21487;&#33021;&#20250;&#22840;&#22823;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;;&#21644;&#65288;4&#65289;&#23384;&#22312;&#25935;&#24863;&#30340;&#21487;&#35782;&#21035;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#65292;&#22914;&#32654;&#22269;&#31038;&#20250;&#23433;&#20840;&#21495;&#30721;&#65288;SSN&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#20351;&#29992;RVL-CDIP&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#39118;&#38505;&#65292;&#22240;&#20026;&#20854;&#26377;&#38480;&#30340;&#33539;&#22260;&#65292;&#23384;&#22312;&#38169;&#35823;&#65288;&#29616;&#20195;&#20808;&#36827;&#27169;&#22411;&#29616;&#22312;&#21487;&#20197;&#23454;&#29616;&#25105;&#20204;&#20272;&#35745;&#30340;&#26631;&#31614;&#38169;&#35823;&#29575;&#20869;&#30340;&#20934;&#30830;&#24615;&#38169;&#35823;&#29575;&#65289;&#65292;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#19981;&#22815;&#29702;&#24819;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20513;&#23548;&#21019;&#24314;&#26032;&#30340;&#25991;&#20214;&#20998;&#31867;&#22522;&#20934;&#65292;&#24182;&#25552;&#20379;&#20854;&#26500;&#24314;&#21644;&#35780;&#20272;&#30340;&#24314;&#35758;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The RVL-CDIP benchmark is widely used for measuring performance on the task of document classification. Despite its widespread use, we reveal several undesirable characteristics of the RVL-CDIP benchmark. These include (1) substantial amounts of label noise, which we estimate to be 8.1% (ranging between 1.6% to 16.9% per document category); (2) presence of many ambiguous or multi-label documents; (3) a large overlap between test and train splits, which can inflate model performance metrics; and (4) presence of sensitive personally-identifiable information like US Social Security numbers (SSNs). We argue that there is a risk in using RVL-CDIP for benchmarking document classifiers, as its limited scope, presence of errors (state-of-the-art models now achieve accuracy error rates that are within our estimated label error rate), and lack of diversity make it less than ideal for benchmarking. We further advocate for the creation of a new document classification benchmark, and provide recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65288;DLN&#65289;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;&#30340;&#35821;&#35328;&#27169;&#22411;&#23618;&#65288;LLMs&#65289;&#65292;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#35757;&#32451;&#65292;&#20351;&#24471;DLN-2&#30340;&#24615;&#33021;&#29978;&#33267;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.12509</link><description>&lt;p&gt;
&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65306;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;LLM&#30340;&#25552;&#31034;&#23618;
&lt;/p&gt;
&lt;p&gt;
Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65288;DLN&#65289;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;&#30340;&#35821;&#35328;&#27169;&#22411;&#23618;&#65288;LLMs&#65289;&#65292;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#35757;&#32451;&#65292;&#20351;&#24471;DLN-2&#30340;&#24615;&#33021;&#29978;&#33267;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35270;&#20026;&#32593;&#32476;&#20013;&#30340;&#38543;&#26426;&#8220;&#35821;&#35328;&#23618;&#8221;&#65292;&#20854;&#20013;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#26159;&#27599;&#20010;&#23618;&#30340;&#33258;&#28982;&#35821;&#35328;&#8220;&#25552;&#31034;&#8221;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#36825;&#26679;&#30340;&#23618;&#21472;&#21152;&#22312;&#19968;&#36215;&#65292;&#23558;&#19968;&#20010;&#23618;&#30340;&#36755;&#20986;&#39304;&#36865;&#21040;&#19979;&#19968;&#20010;&#23618;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#22534;&#21472;&#30340;&#32467;&#26500;&#31216;&#20026;&#8220;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#8221;&#65288;DLN&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#26377;&#25928;&#22320;&#38024;&#23545;&#21333;&#23618;&#35821;&#35328;&#32593;&#32476;&#65288;DLN-1&#65289;&#25191;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#35757;&#32451;2&#23618;DLNs&#65288;DLN-2&#65289;&#65292;&#20854;&#20013;&#24517;&#39035;&#23398;&#20064;&#20004;&#20010;&#25552;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#31532;&#19968;&#23618;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#65292;&#38656;&#35201;&#36827;&#34892;&#36793;&#32536;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#25552;&#31034;&#35757;&#32451;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#12290;DLN-2&#27604;&#21333;&#23618;&#36798;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#21363;&#20351;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;LLM&#26356;&#23567;&#19988;&#26356;&#24369;&#65292;&#20063;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;DLN&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65306;https://github.com/microsoft/deep-language-networks&#12290;
&lt;/p&gt;
&lt;p&gt;
We view large language models (LLMs) as stochastic \emph{language layers} in a network, where the learnable parameters are the natural language \emph{prompts} at each layer. We stack two such layers, feeding the output of one layer to the next. We call the stacked architecture a \emph{Deep Language Network} (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs (DLN-2), where two prompts must be learnt. We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training. A DLN-2 reaches higher performance than a single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful. The DLN code is open source: https://github.com/microsoft/deep-language-networks .
&lt;/p&gt;</description></item><item><title>&#31038;&#20132;&#23186;&#20307;&#31639;&#27861;&#20250;&#20542;&#21521;&#20110;&#25512;&#24191;&#21253;&#25324;&#38169;&#35823;&#20449;&#24687;&#30340;&#26377;&#20105;&#35758;&#24086;&#23376;&#65292;&#22240;&#27492;&#23558;&#38169;&#35823;&#20449;&#24687;&#35270;&#20026;&#20449;&#24687;&#27745;&#26579;&#65292;&#36890;&#36807;&#23545;&#38169;&#35823;&#20449;&#24687;&#30340;Pigouv&#31246;&#36827;&#34892;&#32463;&#27982;&#28608;&#21169;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25511;&#21046;&#20854;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2306.12466</link><description>&lt;p&gt;
&#12298;&#38169;&#35823;&#20449;&#24687;&#20316;&#20026;&#20449;&#24687;&#27745;&#26579;&#12299;
&lt;/p&gt;
&lt;p&gt;
Misinformation as Information Pollution. (arXiv:2306.12466v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12466
&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#31639;&#27861;&#20250;&#20542;&#21521;&#20110;&#25512;&#24191;&#21253;&#25324;&#38169;&#35823;&#20449;&#24687;&#30340;&#26377;&#20105;&#35758;&#24086;&#23376;&#65292;&#22240;&#27492;&#23558;&#38169;&#35823;&#20449;&#24687;&#35270;&#20026;&#20449;&#24687;&#27745;&#26579;&#65292;&#36890;&#36807;&#23545;&#38169;&#35823;&#20449;&#24687;&#30340;Pigouv&#31246;&#36827;&#34892;&#32463;&#27982;&#28608;&#21169;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25511;&#21046;&#20854;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#21453;&#39304;&#31639;&#27861;&#34987;&#35774;&#35745;&#29992;&#20110;&#20248;&#21270;&#22312;&#32447;&#31038;&#20132;&#20114;&#21160;&#20197;&#26368;&#22823;&#21270;&#24191;&#21578;&#25910;&#30410;&#65292;&#22240;&#27492;&#26377;&#25512;&#24191;&#21253;&#25324;&#38169;&#35823;&#20449;&#24687;&#22312;&#20869;&#30340;&#26377;&#20105;&#35758;&#24086;&#23376;&#30340;&#20542;&#21521;&#12290;&#36890;&#36807;&#23558;&#38169;&#35823;&#20449;&#24687;&#35270;&#20026;&#20449;&#24687;&#27745;&#26579;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20854;&#19982;&#21453;&#20987;&#27745;&#26579;&#22914;&#30899;&#31246;&#31561;&#29615;&#22659;&#25919;&#31574;&#36827;&#34892;&#31867;&#27604;&#12290;&#31867;&#20284;&#20110;&#27745;&#26579;&#65292;&#23545;&#38169;&#35823;&#20449;&#24687;&#30340;Pigouv&#31246;&#25552;&#20379;&#20102;&#32463;&#27982;&#28608;&#21169;&#65292;&#20197;&#20351;&#31038;&#20132;&#23186;&#20307;&#20844;&#21496;&#26356;&#26377;&#25928;&#22320;&#25511;&#21046;&#38169;&#35823;&#20449;&#24687;&#30340;&#20256;&#25773;&#65292;&#20197;&#36991;&#20813;&#25110;&#20943;&#23569;&#20854;&#38169;&#35823;&#20449;&#24687;&#31246;&#65292;&#21516;&#26102;&#20445;&#30041;&#24179;&#21488;&#21709;&#24212;&#26576;&#20123;&#31243;&#24230;&#30340;&#33258;&#30001;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;Pigouv&#31246;&#30340;&#40479;&#30640;&#35270;&#35282;&#65292;&#24182;&#35752;&#35770;&#23454;&#26045;&#27492;&#31181;&#35838;&#31246;&#26041;&#26696;&#30340;&#20851;&#38190;&#38382;&#39064;&#21644;&#19979;&#19968;&#27493;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media feed algorithms are designed to optimize online social engagements for the purpose of maximizing advertising profits, and therefore have an incentive to promote controversial posts including misinformation. By thinking about misinformation as information pollution, we can draw parallels with environmental policy for countering pollution such as carbon taxes. Similar to pollution, a Pigouvian tax on misinformation provides economic incentives for social media companies to control the spread of misinformation more effectively to avoid or reduce their misinformation tax, while preserving some degree of freedom in platforms' response. In this paper, we highlight a bird's eye view of a Pigouvian misinformation tax and discuss the key questions and next steps for implementing such a taxing scheme.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#39062;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;DEPAC&#65292;&#35813;&#25968;&#25454;&#38598;&#26631;&#35760;&#20102;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#26631;&#20934;&#31579;&#26597;&#24037;&#20855;&#19978;&#30340;&#38376;&#27099;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#25163;&#24037;&#31579;&#36873;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20154;&#31867;&#35821;&#38899;&#20013;&#30340;&#31934;&#31070;&#30142;&#30149;&#36857;&#35937;&#12290;&#35813;&#30740;&#31350;&#20026;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#20449;&#24687;&#20016;&#23500;&#19988;&#24179;&#34913;&#30340;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2306.12443</link><description>&lt;p&gt;
DEPAC&#65306;&#19968;&#20221;&#38024;&#23545;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#26816;&#27979;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
DEPAC: a Corpus for Depression and Anxiety Detection from Speech. (arXiv:2306.12443v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#39062;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;DEPAC&#65292;&#35813;&#25968;&#25454;&#38598;&#26631;&#35760;&#20102;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#26631;&#20934;&#31579;&#26597;&#24037;&#20855;&#19978;&#30340;&#38376;&#27099;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#25163;&#24037;&#31579;&#36873;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20154;&#31867;&#35821;&#38899;&#20013;&#30340;&#31934;&#31070;&#30142;&#30149;&#36857;&#35937;&#12290;&#35813;&#30740;&#31350;&#20026;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#20449;&#24687;&#20016;&#23500;&#19988;&#24179;&#34913;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#22256;&#25200;&#65292;&#27604;&#22914;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#65292;&#23545;&#20840;&#29699;&#30142;&#30149;&#36127;&#25285;&#30340;&#36129;&#29486;&#26368;&#22823;&#12290;&#21463;&#21040;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26368;&#26032;&#25216;&#26415;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#38556;&#30861;&#30340;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#21487;&#20197;&#20026;&#21463;&#24433;&#21709;&#30340;&#20154;&#20204;&#20943;&#23569;&#30171;&#33510;&#12290;&#36825;&#31181;&#31995;&#32479;&#30340;&#24320;&#21457;&#38656;&#35201;&#20449;&#24687;&#20016;&#23500;&#19988;&#24179;&#34913;&#30340;&#35821;&#26009;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#39062;&#30340;&#24515;&#29702;&#22256;&#25200;&#20998;&#26512;&#38899;&#39057;&#25968;&#25454;&#38598;DEPAC&#65292;&#22522;&#20110;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#26631;&#20934;&#31579;&#26597;&#24037;&#20855;&#19978;&#30340;&#24050;&#24314;&#31435;&#38376;&#27099;&#36827;&#34892;&#26631;&#35760;&#12290;&#36825;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#21253;&#25324;&#27599;&#20010;&#20010;&#20307;&#30340;&#22810;&#20010;&#35821;&#38899;&#20219;&#21153;&#20197;&#21450;&#30456;&#20851;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#38598;&#65292;&#21253;&#25324;&#25163;&#24037;&#31579;&#36873;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#22312;&#20154;&#31867;&#35821;&#38899;&#20013;&#35782;&#21035;&#31934;&#31070;&#30142;&#30149;&#36857;&#35937;&#26041;&#38754;&#21457;&#25381;&#20102;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#22522;&#32447;&#30340;&#24615;&#33021;&#26469;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#38899;&#39057;&#35821;&#26009;&#24211;&#21644;&#29305;&#24449;&#38598;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#30340;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental distress like depression and anxiety contribute to the largest proportion of the global burden of diseases. Automated diagnosis systems of such disorders, empowered by recent innovations in Artificial Intelligence, can pave the way to reduce the sufferings of the affected individuals. Development of such systems requires information-rich and balanced corpora. In this work, we introduce a novel mental distress analysis audio dataset DEPAC, labeled based on established thresholds on depression and anxiety standard screening tools. This large dataset comprises multiple speech tasks per individual, as well as relevant demographic information. Alongside, we present a feature set consisting of hand-curated acoustic and linguistic features, which were found effective in identifying signs of mental illnesses in human speech. Finally, we justify the quality and effectiveness of our proposed audio corpus and feature set in predicting depression severity by comparing the performance of bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.11207</link><description>&lt;p&gt;
Quilt-1M: &#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#21152;&#36895;&#20351;&#24471;&#22312;&#32447;&#22270;&#20687;&#21644;&#25991;&#23383;&#25968;&#25454;&#22823;&#37327;&#28044;&#29616;&#65292;&#20294;&#21307;&#23398;&#39046;&#22495;&#65288;&#29305;&#21035;&#26159;&#30284;&#30151;&#32452;&#32455;&#23398;&#65289;&#31867;&#20284;&#30340;&#25968;&#25454;&#21364;&#24456;&#31232;&#23569;&#65292;&#36825;&#38459;&#30861;&#20102;&#21307;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#21033;&#29992;YouTube&#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#65292;&#20174;&#20013;&#36873;&#25321;&#20102; 1,087 &#23567;&#26102;&#30340;&#21307;&#23398;&#32452;&#32455;&#23398;&#35270;&#39057;&#65292;&#20197;&#27492;&#33258;&#21160;&#31579;&#36873;&#20986;&#20849;&#21253;&#21547; 768,826 &#20010;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#25991;&#23383;&#23545;&#30340; Quilt &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has halted comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of $768,826$ image and text pairs. Quilt was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine Quilt with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09896</link><description>&lt;p&gt;
&#25581;&#31192; GPT &#33258;&#25105;&#20462;&#22797;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#12290;&#33258;&#25105;&#20462;&#22797;&#8212;&#8212;&#21363;&#27169;&#22411;&#35843;&#35797;&#24182;&#20462;&#22797;&#33258;&#24049;&#30340;&#20195;&#30721;&#8212;&#8212;&#26368;&#36817;&#25104;&#20026;&#25552;&#39640;&#24615;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#25105;&#20462;&#22797;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26377;&#20154;&#20250;&#24819;&#30693;&#36947;&#65292;&#24403;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26102;&#65292;&#27169;&#22411;&#31350;&#31455;&#33021;&#21542;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#31181;&#32534;&#30721;&#25361;&#25112;&#32452;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#31574;&#30053; pass@t&#65292;&#35813;&#31574;&#30053;&#34913;&#37327;&#20102;&#20219;&#21153;&#36890;&#36807;&#29575;&#19982;&#20174;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#24635;&#26631;&#35760;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20165;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#33258;&#25105;&#20462;&#22797;&#34920;&#29616;&#30340;&#20960;&#20010;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36755;&#20837;&#22122;&#22768;&#36739;&#23569;&#19988;&#27169;&#22411;&#23545;&#21021;&#22987;&#36755;&#20986;&#19981;&#22826;&#33258;&#20449;&#30340;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#33258;&#25105;&#20462;&#22797;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#21453;&#39304;&#26469;&#22686;&#24378; GPT &#27169;&#22411;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#65292;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#21033;&#29992;GPT-4&#22686;&#24378;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#28040;&#38500;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2306.09525</link><description>&lt;p&gt;
&#21033;&#29992;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#35299;&#37322;&#27861;&#24459;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#21033;&#29992;GPT-4&#22686;&#24378;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#28040;&#38500;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#27861;&#24459;&#24320;&#25918;&#24615;&#26415;&#35821;&#30340;&#21547;&#20041;&#26159;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#20808;&#21069;&#27861;&#38498;&#26696;&#20363;&#20013;&#35813;&#26415;&#35821;&#30340;&#24212;&#29992;&#26159;&#35299;&#37322;&#20854;&#21547;&#20041;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;GPT-4&#29983;&#25104;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#12289;&#28165;&#26224;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;GPT-4&#34987;&#30452;&#25509;&#35201;&#27714;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#30340;&#22522;&#20934;&#35774;&#32622;&#30340;&#24615;&#33021;&#19982;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22312;&#22686;&#24378;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#34987;&#29992;&#26469;&#20026;&#27169;&#22411;&#25552;&#20379;&#30456;&#20851;&#32972;&#26223;&#65292;&#21363;&#26469;&#33258;&#26696;&#20363;&#27861;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#24212;&#29992;GPT-4&#20135;&#29983;&#30340;&#35299;&#37322;&#22312;&#34920;&#38754;&#19978;&#20284;&#20046;&#38750;&#24120;&#39640;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#35814;&#32454;&#20998;&#26512;&#25581;&#31034;&#20102;&#35299;&#37322;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#24615;&#33021;&#21487;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#24182;&#20284;&#20046;&#28040;&#38500;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#21457;&#26126;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting the meaning of legal open-textured terms is a key task of legal professionals. An important source for this interpretation is how the term was applied in previous court cases. In this paper, we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation. We compare the performance of a baseline setup, where GPT-4 is directly asked to explain a legal term, to an augmented approach, where a legal information retrieval module is used to provide relevant context to the model, in the form of sentences from case law. We found that the direct application of GPT-4 yields explanations that appear to be of very high quality on their surface. However, detailed analysis uncovered limitations in terms of the factual accuracy of the explanations. Further, we found that the augmentation leads to improved quality, and appears to eliminate the issue of hallucination, where models invent incorrect statements. These findings ope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09442</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#23454;&#29616;&#32418;&#38431;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#19982;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#27602;&#25110;&#19981;&#35802;&#23454;&#38472;&#36848;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#24037;&#20855;&#20197;&#35843;&#26597;&#26377;&#23475;&#36755;&#20986;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#34429;&#28982;&#36825;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#30340;&#26377;&#20215;&#20540;&#27493;&#39588;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38024;&#23545;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21482;&#26377;&#39044;&#20808;&#30693;&#36947;&#26377;&#23475;&#34892;&#20026;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#36339;&#36807;&#20102;&#32418;&#38431;&#34892;&#21160;&#30340;&#26680;&#24515;&#25361;&#25112;&#65306;&#24320;&#21457;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#30340;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#23384;&#22312;&#26102;&#65292;&#32418;&#38431;&#34892;&#21160;&#30340;&#36793;&#38469;&#20215;&#20540;&#26377;&#38480;&#65292;&#22240;&#20026;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#36755;&#20986;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20551;&#35774;&#23545;&#25163;&#20174;&#39640;&#32423;&#12289;&#25277;&#35937;&#30340;&#19981;&#33391;&#34892;&#20026;&#35268;&#33539;&#20986;&#21457;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#12290;&#32418;&#38431;&#24212;&#35813;&#22312;&#31934;&#21270;/&#25193;&#23637;&#27492;&#35268;&#33539;&#30340;&#21516;&#26102;&#23545;&#25239;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11554</link><description>&lt;p&gt;
ToolkenGPT&#65306;&#36890;&#36807;&#24037;&#20855;&#23884;&#20837;&#25193;&#20805;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#29992;&#24037;&#20855;&#28436;&#31034;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#26082;&#36153;&#26102;&#21448;&#21463;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#24037;&#20855;&#38598;&#12290;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#20363;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26159;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21482;&#20801;&#35768;&#28436;&#31034;&#20960;&#27425;&#65292;&#23548;&#33268;&#23545;&#24037;&#20855;&#30340;&#29702;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#27492;&#22806;&#65292;&#24403;&#26377;&#22823;&#37327;&#24037;&#20855;&#21487;&#20379;&#36873;&#25321;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#23436;&#20840;&#26080;&#27861;&#27491;&#24120;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\textbf{ToolkenGPT}$&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;$\underline{&#24037;&#20855;}$&#34920;&#31034;&#20026;&#19968;&#20010;$\underline{token}$&#65288;$\textit{toolken}$&#65289;&#65292;&#24182;&#20026;&#20854;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#65292;&#20351;&#24471;&#24037;&#20855;&#35843;&#29992;&#19982;&#29983;&#25104;&#24120;&#35268;&#21333;&#35789;&#26631;&#35760;&#30340;&#26041;&#24335;&#30456;&#21516;&#12290;&#19968;&#26086;&#35302;&#21457;&#20102;toolken&#65292;LLM&#34987;&#25552;&#31034;&#23436;&#25104;&#24037;&#20855;&#25191;&#34892;&#25152;&#38656;&#30340;&#21442;&#25968;&#12290;ToolkenGPT&#25552;&#20379;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;1&#65289;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#20197;&#25193;&#20805;LLM&#19982;&#22806;&#37096;&#24037;&#20855;&#30340;&#20132;&#20114;&#65292;2&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;3&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05480</link><description>&lt;p&gt;
&#25506;&#31350;&#23376;&#35789;&#20998;&#21106;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24819;&#30740;&#31350;&#35789;&#27573;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#65292;&#22312;&#33452;&#20848;&#35821;&#21644;&#20420;&#35821;&#20013;&#35757;&#32451;&#20102;GPT-2&#21644;BERT&#27169;&#22411;&#12290;&#20316;&#20026;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#20351;&#29992;BPE&#21644;Morfessor&#20998;&#21106;&#31639;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;StateMorph&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;early-stopping&#35757;&#32451;&#30340;seq2seq&#27169;&#22411;&#22312;token&#32423;&#21035;&#23384;&#22312;&#25311;&#21512;&#38382;&#39064;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;Token&#39057;&#29575;&#12289;&#35789;&#24615;&#21644;&#39044;&#27979;&#24046;&#24322;&#20197;&#21450;&#22806;&#37096;&#22240;&#32032;&#22914;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#39046;&#22495;&#12289;&#25968;&#25454;&#35268;&#27169;&#21644;&#39044;&#35757;&#32451;&#31561;&#12290;</title><link>http://arxiv.org/abs/2305.04493</link><description>&lt;p&gt;
Seq2seq&#27169;&#22411;&#30340;Token&#32423;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Token-Level Fitting Issues of Seq2seq Models. (arXiv:2305.04493v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04493
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;early-stopping&#35757;&#32451;&#30340;seq2seq&#27169;&#22411;&#22312;token&#32423;&#21035;&#23384;&#22312;&#25311;&#21512;&#38382;&#39064;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;Token&#39057;&#29575;&#12289;&#35789;&#24615;&#21644;&#39044;&#27979;&#24046;&#24322;&#20197;&#21450;&#22806;&#37096;&#22240;&#32032;&#22914;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#39046;&#22495;&#12289;&#25968;&#25454;&#35268;&#27169;&#21644;&#39044;&#35757;&#32451;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#27169;&#22411;&#24050;&#24191;&#27867;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;early-stopping&#35757;&#32451;&#30340;seq2seq&#27169;&#22411;&#22312;token&#32423;&#21035;&#23384;&#22312;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#34429;&#28982;&#35789;&#27719;&#34920;&#20013;&#30340;&#26576;&#20123;token&#34920;&#29616;&#20986;&#36807;&#25311;&#21512;&#65292;&#20294;&#24403;&#35757;&#32451;&#20572;&#27490;&#26102;&#65292;&#20854;&#20182;token&#21017;&#34920;&#29616;&#20986;&#27424;&#25311;&#21512;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312; fine-tune &#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#36825;&#31181;&#29616;&#35937;&#20063;&#24456;&#26222;&#36941;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;Token&#32423;&#21035;&#25311;&#21512;&#30340;&#19977;&#20010;&#20027;&#35201;&#22240;&#32032;&#65292;&#21253;&#25324;Token&#39057;&#29575;&#12289;&#35789;&#24615;&#21644;&#39044;&#27979;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#39046;&#22495;&#12289;&#25968;&#25454;&#35268;&#27169;&#21644;&#39044;&#35757;&#32451;&#31561;&#22806;&#37096;&#22240;&#32032;&#20063;&#21487;&#20197;&#24433;&#21709;Token&#30340;&#25311;&#21512;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence (seq2seq) models have been widely used for natural language processing, computer vision, and other deep learning tasks. We find that seq2seq models trained with early-stopping suffer from issues at the token level. In particular, while some tokens in the vocabulary demonstrate overfitting, others underfit when training is stopped. Experiments show that the phenomena are pervasive in different models, even in fine-tuned large pretrained-models. We identify three major factors that influence token-level fitting, which include token frequency, parts-of-speech, and prediction discrepancy. Further, we find that external factors such as language, model size, domain, data scale, and pretraining can also influence the fitting of tokens.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAPS&#30340;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#32763;&#35793;&#30340;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#20998;&#26512;&#28304;&#25991;&#26412;&#24182;&#25552;&#21462;&#20851;&#38190;&#35789;&#12289;&#20027;&#39064;&#21644;&#30456;&#20851;&#28436;&#31034;&#20197;&#25351;&#23548;&#32763;&#35793;&#36807;&#31243;&#12290;&#35813;&#26694;&#26550;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26126;&#26174;&#20248;&#20110;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#20026;&#24320;&#23637;&#20351;&#29992;LLM&#23454;&#29616;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.04118</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Exploring Human-Like Translation Strategy with Large Language Models. (arXiv:2305.04118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAPS&#30340;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#32763;&#35793;&#30340;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#20998;&#26512;&#28304;&#25991;&#26412;&#24182;&#25552;&#21462;&#20851;&#38190;&#35789;&#12289;&#20027;&#39064;&#21644;&#30456;&#20851;&#28436;&#31034;&#20197;&#25351;&#23548;&#32763;&#35793;&#36807;&#31243;&#12290;&#35813;&#26694;&#26550;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26126;&#26174;&#20248;&#20110;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#20026;&#24320;&#23637;&#20351;&#29992;LLM&#23454;&#29616;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#20102;&#25509;&#36817;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#27700;&#24179;&#12290;&#22312;&#20854;&#22810;&#31181;&#25216;&#33021;&#20013;&#65292;LLM&#30340;&#32763;&#35793;&#33021;&#21147;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#32763;&#35793;&#20165;&#20851;&#27880;&#28304;&#30446;&#26631;&#26144;&#23556;&#19981;&#21516;&#65292;&#22522;&#20110;LLM&#30340;&#32763;&#35793;&#21487;&#20197;&#28508;&#22312;&#22320;&#27169;&#20223;&#20154;&#31867;&#32763;&#35793;&#30340;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20250;&#37319;&#21462;&#35768;&#22810;&#20934;&#22791;&#27493;&#39588;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;MAPS&#26694;&#26550;&#65288;Multi-Aspect Prompting and Selection&#65289;&#25506;&#32034;&#36825;&#31181;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;LLM&#39318;&#20808;&#20998;&#26512;&#32473;&#23450;&#28304;&#25991;&#26412;&#24182;&#25552;&#21462;&#19977;&#20010;&#19982;&#32763;&#35793;&#30456;&#20851;&#30340;&#30693;&#35782;&#26041;&#38754;&#65306;&#20851;&#38190;&#35789;&#12289;&#20027;&#39064;&#21644;&#30456;&#20851;&#28436;&#31034;&#20197;&#25351;&#23548;&#32763;&#35793;&#36807;&#31243;&#12290;&#20026;&#20102;&#36807;&#28388;&#25481;&#22122;&#22768;&#21644;&#26080;&#29992;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#36873;&#25321;&#26426;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22810;&#20010;&#35821;&#35328;&#23545;&#21644;&#32763;&#35793;&#26041;&#21521;&#19978;&#26174;&#30528;&#20248;&#20110;&#22810;&#20010;&#24378;&#22522;&#32447;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#24320;&#23637;&#20351;&#29992;LLM&#23454;&#29616;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. In contrast to traditional machine translation that focuses solely on source-target mapping, LLM-based translation can potentially mimic the human translation process that takes many preparatory steps to ensure high-quality translation. This work aims to explore this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs to first analyze the given source text and extract three aspects of translation-related knowledge: keywords, topics and relevant demonstrations to guide the translation process. To filter out the noisy and unhelpful knowledge, we employ a selection mechanism based on quality estimation. Experiments sug
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;&#30340;&#21487;&#34892;&#24615;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#65292;&#19981;&#38656;&#22823;&#37327;&#24494;&#35843;&#25968;&#25454;&#65292;&#19988;&#26356;&#21152;&#20581;&#22766;&#12290;</title><link>http://arxiv.org/abs/2305.03423</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Using ChatGPT for Entity Matching. (arXiv:2305.03423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;&#30340;&#21487;&#34892;&#24615;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#65292;&#19981;&#38656;&#22823;&#37327;&#24494;&#35843;&#25968;&#25454;&#65292;&#19988;&#26356;&#21152;&#20581;&#22766;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#26159;&#21028;&#26029;&#20004;&#20010;&#23454;&#20307;&#25551;&#36848;&#26159;&#21542;&#25351;&#21521;&#21516;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23454;&#20307;&#21305;&#37197;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#24494;&#35843;&#35832;&#22914;BERT&#25110;RoBERTa&#20043;&#31867;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#12290;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;&#30340;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#65292;&#65288;i&#65289;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#24494;&#35843;&#25968;&#25454;&#25165;&#33021;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#65288;ii&#65289;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#23545;&#20110;&#20998;&#24067;&#22806;&#30340;&#23454;&#20307;&#19981;&#22826;&#20581;&#22766;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;&#65292;&#20316;&#20026;&#20256;&#32479;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#26356;&#20026;&#20581;&#22766;&#12289;&#25968;&#25454;&#39640;&#25928;&#30340;&#26367;&#20195;&#25216;&#26415;&#12290;&#25105;&#20204;&#20174;&#19977;&#20010;&#32500;&#24230;&#36827;&#34892;&#23454;&#39564;&#65306;&#65288;i&#65289;&#19968;&#33324;&#25552;&#31034;&#35774;&#35745;&#65292;&#65288;ii&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#25552;&#20379;&#26356;&#39640;&#32423;&#30340;&#21305;&#37197;&#30693;&#35782;&#12290;&#25105;&#20204;&#34920;&#26126; ChatGPT &#19982;&#32463;&#36807;&#24494;&#35843;&#30340; RoBERTa &#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#22312;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21305;&#37197;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#24179;&#22343;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#20026;83%&#30340;F1&#20540;&#65292;&#32780; RoBERTa &#38656;&#35201;2000&#20010;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#36798;&#21040;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Matching is the task of deciding if two entity descriptions refer to the same real-world entity. State-of-the-art entity matching methods often rely on fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks of using these models for entity matching are that (i) the models require significant amounts of fine-tuning data for reaching a good performance and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using ChatGPT for entity matching as a more robust, training data-efficient alternative to traditional Transformer models. We perform experiments along three dimensions: (i) general prompt design, (ii) in-context learning, and (iii) provision of higher-level matching knowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model, reaching an average zero-shot performance of 83% F1 on a challenging matching task on which RoBERTa requires 2000 training examples for reaching a similar
&lt;/p&gt;</description></item><item><title>SweCTRL-Mini&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#29790;&#20856;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#23427;&#29983;&#25104;&#30340;&#25991;&#26412;&#27969;&#27966;&#65292;&#23436;&#20840;&#24320;&#25918;&#19979;&#36733;&#12290;&#29983;&#25104;&#33021;&#21147;&#27604;&#36739;GPT-3&#12290;</title><link>http://arxiv.org/abs/2304.13994</link><description>&lt;p&gt;
SweCTRL-Mini&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25968;&#25454;&#36879;&#26126;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#30340;&#29790;&#20856;&#35821;&#35328;&#29256;
&lt;/p&gt;
&lt;p&gt;
SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13994
&lt;/p&gt;
&lt;p&gt;
SweCTRL-Mini&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#29790;&#20856;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#23427;&#29983;&#25104;&#30340;&#25991;&#26412;&#27969;&#27966;&#65292;&#23436;&#20840;&#24320;&#25918;&#19979;&#36733;&#12290;&#29983;&#25104;&#33021;&#21147;&#27604;&#36739;GPT-3&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SweCTRL-Mini&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29790;&#20856;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21333;&#20010;&#28040;&#36153;&#32423;GPU&#19978;&#30340;&#25512;&#29702;&#21644;fine-tuning&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#30001;Keskar&#12289;McCann&#12289;Varshney&#12289;Xiong&#21644;Socher&#65288;2019&#65289;&#24320;&#21457;&#30340;CTRL&#20307;&#31995;&#32467;&#26500;&#65292;&#36825;&#24847;&#21619;&#30528;SweCTRL-Mini&#27169;&#22411;&#30340;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#22312;&#29983;&#25104;&#25552;&#31034;&#20013;&#25554;&#20837;&#29305;&#27530;&#26631;&#35760;&#26469;&#25511;&#21046;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#27966;&#12290;SweCTRL-Mini&#22312;&#29790;&#20856;&#37096;&#20998;mC4&#35821;&#26009;&#24211;&#21644;&#19968;&#32452;&#29790;&#20856;&#23567;&#35828;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;(1)&#25152;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#25991;&#26412;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#35814;&#32454;&#35828;&#26126;&#65292;&#20197;&#20351;&#21487;&#20197;&#26816;&#26597;&#29305;&#23450;&#30701;&#35821;/&#26469;&#28304;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;;(2)&#20351;&#29992;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#36776;&#21035;&#24615;&#20219;&#21153;&#30340;&#27169;&#22411;&#35780;&#20272;&#65292;&#20351;&#29992;&#20154;&#24037;&#35009;&#21028;&#36827;&#34892;&#29983;&#25104;&#24615;&#20219;&#21153;&#30340;&#35780;&#20272;;&#25105;&#20204;&#36824;&#23558;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#19982;GPT-3&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;SweCTRL-Mini &#26159;&#23436;&#20840;&#24320;&#25918;&#30340;&#65292;&#21487;&#20379;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SweCTRL-Mini, a large Swedish language model that can be used for inference and fine-tuning on a single consumer-grade GPU. The model is based on the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019), which means that users of the SweCTRL-Mini model can control the genre of the generated text by inserting special tokens in the generation prompts. SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a set of Swedish novels. In this article, we provide (1) a detailed account of the utilized training data and text pre-processing steps, to the extent that it is possible to check whether a specific phrase/source was a part of the training data, and (2) an evaluation of the model on both discriminative tasks, using automatic evaluation methods, and generative tasks, using human referees. We also compare the generative capabilities of the model with those of GPT-3. SweCTRL-Mini is fully open and available for download.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.04091</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23454;&#29616;&#35270;&#35273;&#25277;&#35937;&#21644;&#25512;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#23616;&#38480;&#24212;&#29992;&#20013;&#24050;&#32463;&#36798;&#21040;&#20102;&#20154;&#31867;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#23637;&#29616;&#26356;&#24191;&#27867;&#21644;&#26356;&#28789;&#27963;&#30340;&#26234;&#33021;&#12290;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#26088;&#22312;&#35780;&#20272;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65288;DSL&#65289;&#65292;&#29992;&#20110;&#26292;&#21147;&#35299;&#20915;ARC&#20013;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;ARC&#38382;&#39064;&#12290;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Artificial Intelligence (AI) models have achieved human or even superhuman performance in narrowly defined applications, they still struggle to show signs of broader and more flexible intelligence. The Abstraction and Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how close AI systems are to human-like cognitive abilities. Most current approaches rely on carefully handcrafted domain-specific languages (DSLs), which are used to brute-force solutions to the tasks present in ARC. In this work, we propose a general framework for solving ARC based on natural language descriptions of the tasks. While not yet beating state-of-the-art DSL models on ARC, we demonstrate the immense potential of our approach hinted at by the ability to solve previously unsolved tasks.
&lt;/p&gt;</description></item><item><title>SemSup-XC&#26159;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26497;&#31471;&#20998;&#31867;&#30340;&#35821;&#20041;&#30417;&#30563;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#25910;&#38598;&#30340;&#35821;&#20041;&#31867;&#21035;&#25551;&#36848;&#21644;&#26032;&#39062;&#30340;&#28151;&#21512;&#21305;&#37197;&#27169;&#22359;&#30340;&#24110;&#21161;&#19979;&#65292;&#21487;&#20197;&#22312;&#27861;&#24459;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#32500;&#22522;&#30334;&#31185;&#31561;&#19977;&#20010;XC&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11309</link><description>&lt;p&gt;
SemSup-XC: &#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26497;&#31471;&#20998;&#31867;&#30340;&#35821;&#20041;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification. (arXiv:2301.11309v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11309
&lt;/p&gt;
&lt;p&gt;
SemSup-XC&#26159;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26497;&#31471;&#20998;&#31867;&#30340;&#35821;&#20041;&#30417;&#30563;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#25910;&#38598;&#30340;&#35821;&#20041;&#31867;&#21035;&#25551;&#36848;&#21644;&#26032;&#39062;&#30340;&#28151;&#21512;&#21305;&#37197;&#27169;&#22359;&#30340;&#24110;&#21161;&#19979;&#65292;&#21487;&#20197;&#22312;&#27861;&#24459;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#32500;&#22522;&#30334;&#31185;&#31561;&#19977;&#20010;XC&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#31471;&#20998;&#31867;&#26088;&#22312;&#39044;&#27979;&#22823;&#37327;&#31867;&#21035;&#65288;&#25968;&#21315;&#33267;&#25968;&#30334;&#19975;&#20010;&#65289;&#65292;&#20855;&#26377;&#26032;&#38395;&#25991;&#31456;&#20998;&#31867;&#21644;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#26631;&#35760;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;&#36825;&#39033;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#29256;&#26412;&#38656;&#35201;&#22312;&#26080;&#39069;&#22806;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#21040;&#26032;&#31867;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SemSup-XC&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33258;&#21160;&#25910;&#38598;&#30340;&#35821;&#20041;&#31867;&#21035;&#25551;&#36848;&#26469;&#34920;&#31034;&#31867;&#21035;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#28151;&#21512;&#21305;&#37197;&#27169;&#22359;&#20351;&#29992;&#35821;&#20041;&#21644;&#35789;&#27719;&#30456;&#20284;&#24615;&#23558;&#36755;&#20837;&#23454;&#20363;&#19982;&#31867;&#21035;&#25551;&#36848;&#36827;&#34892;&#21305;&#37197;&#65292;&#20351;&#24471;&#22312;&#27861;&#24459;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#32500;&#22522;&#30334;&#31185;&#31561;&#19977;&#20010;XC&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#32463;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#35757;&#32451;&#65292;SemSup-XC&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#22312;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#31934;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;12&#20010;&#30334;&#20998;&#28857;&#65292;&#23569;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#31934;&#24230;&#25552;&#39640;&#20102;10&#20010;&#30334;&#20998;&#28857;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extreme classification (XC) involves predicting over large numbers of classes (thousands to millions), with real-world applications like news article classification and e-commerce product tagging. The zero-shot version of this task requires generalization to novel classes without additional supervision. In this paper, we develop SemSup-XC, a model that achieves state-of-the-art zero-shot and few-shot performance on three XC datasets derived from legal, e-commerce, and Wikipedia data. To develop SemSup-XC, we use automatically collected semantic class descriptions to represent classes and facilitate generalization through a novel hybrid matching module that matches input instances to class descriptions using a combination of semantic and lexical similarity. Trained with contrastive learning, SemSup-XC significantly outperforms baselines and establishes state-of-the-art performance on all three datasets considered, gaining up to 12 precision points on zero-shot and more than 10 precision
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23398;&#25512;&#29702;&#39046;&#22495;&#30340;&#20851;&#38190;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2212.10535</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Learning for Mathematical Reasoning. (arXiv:2212.10535v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23398;&#25512;&#29702;&#39046;&#22495;&#30340;&#20851;&#38190;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#24212;&#29992;&#24191;&#27867;&#65292;&#21253;&#25324;&#31185;&#23398;&#12289;&#24037;&#31243;&#12289;&#37329;&#34701;&#21644;&#26085;&#24120;&#29983;&#27963;&#12290;&#21457;&#23637;&#33021;&#22815;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#21644;&#35777;&#26126;&#23450;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#22312;&#25968;&#23398;&#25512;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#20132;&#21449;&#39046;&#22495;&#30340;&#20851;&#38190;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20132;&#21644;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#32771;&#34385;&#22312;&#23545;&#35805;&#20013;&#65292;&#26469;&#20248;&#21270;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#20351;&#20854;&#22312;&#20219;&#21153;&#25928;&#29575;&#39640;&#30340;&#21516;&#26102;&#65292;&#20063;&#33021;&#20419;&#36827;&#29992;&#25143;&#20449;&#20219;&#65292;&#20174;&#32780;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.15359</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20132;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#20027;&#21160;&#23545;&#35805;&#20195;&#29702;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning. (arXiv:2211.15359v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20132;&#21644;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#32771;&#34385;&#22312;&#23545;&#35805;&#20013;&#65292;&#26469;&#20248;&#21270;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#20351;&#20854;&#22312;&#20219;&#21153;&#25928;&#29575;&#39640;&#30340;&#21516;&#26102;&#65292;&#20063;&#33021;&#20419;&#36827;&#29992;&#25143;&#20449;&#20219;&#65292;&#20174;&#32780;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;&#30340;&#19979;&#19968;&#20010;&#27493;&#39588;&#26159;&#20174;&#26049;&#35266;&#32773;&#30340;&#35282;&#33394;&#20013;&#35299;&#33073;&#20986;&#26469;&#65292;&#21464;&#24471;&#26356;&#21152;&#20027;&#21160;&#12290;&#26126;&#30830;&#23450;&#20041;&#30340;&#20027;&#21160;&#34892;&#20026;&#21487;&#20197;&#25913;&#21892;&#20154;&#26426;&#21512;&#20316;&#65292;&#22240;&#20026;&#20195;&#29702;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#31215;&#26497;&#30340;&#35282;&#33394;&#24182;&#35299;&#38500;&#20102;&#29992;&#25143;&#30340;&#36131;&#20219;&#12290;&#28982;&#32780;&#65292;&#20027;&#21160;&#24615;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#22240;&#20026;&#25191;&#34892;&#19981;&#24403;&#30340;&#39044;&#38450;&#24615;&#34892;&#21160;&#21487;&#33021;&#19981;&#20165;&#23545;&#20219;&#21153;&#32467;&#26524;&#20135;&#29983;&#30772;&#22351;&#24615;&#24433;&#21709;&#65292;&#32780;&#19988;&#36824;&#20250;&#23545;&#19982;&#29992;&#25143;&#30340;&#20851;&#31995;&#20135;&#29983;&#24433;&#21709;&#12290;&#20026;&#20102;&#35774;&#35745;&#21512;&#36866;&#30340;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31038;&#20132;&#21644;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#37117;&#32771;&#34385;&#22312;&#23545;&#35805;&#20013;&#12290;&#36825;&#37324;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20248;&#21270;&#20027;&#21160;&#34892;&#20026;&#65292;&#20351;&#20854;&#20219;&#21153;&#23548;&#21521;&#8212;&#8212;&#36825;&#24847;&#21619;&#30528;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#8212;&#8212;&#21516;&#26102;&#22312;&#20419;&#36827;&#29992;&#25143;&#20449;&#20219;&#26102;&#20063;&#20855;&#26377;&#31038;&#20132;&#25928;&#30410;&#12290;&#23558;&#36825;&#20004;&#20010;&#26041;&#38754;&#21253;&#21547;&#22312;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20027;&#21160;&#23545;&#35805;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#26356;&#21152;&#25104;&#21151;&#30340;&#20154;&#26426;&#20132;&#20114;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive. Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user. However, proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user. For designing adequate proactive dialog strategies, we propose a novel approach including both social as well as task-relevant features in the dialog. Here, the primary goal is to optimize proactive behavior so that it is task-oriented - this implies high task success and efficiency - while also being socially effective by fostering user trust. Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for more successful human-mach
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; SSC-Conformer &#30340;&#22359;&#24335;&#27169;&#22411;&#65292;&#21033;&#29992;&#20018;&#34892;&#37319;&#26679;&#22359;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#22359;&#38388;&#20132;&#20114;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#23558;&#22359;&#21367;&#31215;&#19982;&#22240;&#26524;&#21367;&#31215;&#30456;&#32467;&#21512;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340; CER &#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; SSC-Conformer &#22312; AISHELL-1 &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#27969;&#24335; E2E ASR &#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.11419</link><description>&lt;p&gt;
&#20018;&#34892;&#37319;&#26679;&#22359;&#24335;Conformer&#32593;&#32476;&#22312;&#27969;&#24335;&#31471;&#21040;&#31471;ASR&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR. (arXiv:2211.11419v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; SSC-Conformer &#30340;&#22359;&#24335;&#27169;&#22411;&#65292;&#21033;&#29992;&#20018;&#34892;&#37319;&#26679;&#22359;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#22359;&#38388;&#20132;&#20114;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#23558;&#22359;&#21367;&#31215;&#19982;&#22240;&#26524;&#21367;&#31215;&#30456;&#32467;&#21512;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340; CER &#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; SSC-Conformer &#22312; AISHELL-1 &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#27969;&#24335; E2E ASR &#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27969;&#24335;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035; (E2E ASR) &#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SSC-Conformer &#30340;&#20018;&#34892;&#37319;&#26679;&#22359;&#24335; Conformer &#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#20018;&#34892;&#37319;&#26679;&#22359;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046; (SSC-MHSA) &#26469;&#25552;&#39640;&#36328;&#22359;&#20132;&#20114;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#21033;&#29992;&#22359;&#21367;&#31215;&#26469;&#22686;&#21152;&#22359;&#32423;&#26410;&#26469;&#19978;&#19979;&#25991;&#65292;&#24182;&#23558;&#20854;&#19982;&#21367;&#31215;&#23618;&#30340;&#22240;&#26524;&#21367;&#31215;&#30456;&#32467;&#21512;&#20197;&#36827;&#19968;&#27493;&#38477;&#20302; CER&#12290;&#22312; AISHELL-1 &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; SSC-Conformer &#22312;&#26080;&#35821;&#35328;&#27169;&#22411;&#37325;&#25171;&#20998;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616; CER 5.33%&#65292;&#36798;&#21040;&#20102;&#27969;&#24335; E2E ASR &#30340;&#26368;&#26032;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#20351;&#29992;&#26356;&#22823;&#30340;&#25209;&#37327;&#36827;&#34892;&#35757;&#32451;&#24182;&#26356;&#39640;&#25928;&#22320;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an in-depth study on a Sequentially Sampled Chunk Conformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer first demonstrates the significant performance gains from using the sequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the Conformer encoder by allowing efficient cross-chunk interactions while keeping linear complexities. Furthermore, it explores taking advantage of chunked convolution to make use of the chunk-wise future context and integrates with casual convolution in the convolution layers to further reduce CER. We verify the proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results show that a state-of-the-art performance for streaming E2E ASR is achieved with CER 5.33% without LM rescoring. And, owing to its linear complexity, the SSC-Conformer can train with large batch sizes and infer more efficiently.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;VL-CheckList&#65292;&#20351;&#29992;&#29289;&#20307;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#19971;&#31181;&#27969;&#34892;&#30340;VLP&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#20998;&#26512;&#65292;&#25581;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2207.00221</link><description>&lt;p&gt;
VL-CheckList: &#20351;&#29992;&#29289;&#20307;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;VL-CheckList&#65292;&#20351;&#29992;&#29289;&#20307;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#19971;&#31181;&#27969;&#34892;&#30340;VLP&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#20998;&#26512;&#65292;&#25581;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#20419;&#36827;&#20102;&#35768;&#22810;&#36328;&#27169;&#24577;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#26159;&#36890;&#36807;&#27604;&#36739;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#30340;&#19979;&#28216;&#20219;&#21153;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#20379;&#24456;&#23569;&#20851;&#20110;&#27599;&#31181;VLP&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#30340;&#20449;&#24687;&#65292;&#26356;&#19981;&#29992;&#35828;&#20026;&#31038;&#21306;&#22312;&#26410;&#26469;&#22914;&#20309;&#25913;&#36827;&#31995;&#32479;&#25552;&#20379;&#35265;&#35299;&#20102;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27979;&#35797;CheckList&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VL-CheckList&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20102;&#35299;VLP&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;VLP&#27169;&#22411;&#30340;&#22270;&#20687;-&#25991;&#26412;&#33021;&#21147;&#20998;&#20026;&#19977;&#31867;&#65306;&#29289;&#20307;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#36827;&#19968;&#27493;&#20998;&#35299;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#36890;&#36807;&#35813;&#26694;&#26550;&#23545;&#19971;&#31181;&#26368;&#36817;&#27969;&#34892;&#30340;VLP&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#20998;&#26512;&#12290;&#32467;&#26524;&#36890;&#36807;&#25581;&#31034;&#27604;&#36739;&#27169;&#22411;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#24322;&#26469;&#30830;&#35748;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Pretraining (VLP) models have recently successfully facilitated many cross-modal downstream tasks. Most existing works evaluated their systems by comparing the fine-tuned downstream task performance. However, only average downstream task accuracy provides little information about the pros and cons of each VLP method, let alone provides insights on how the community can improve the systems in the future. Inspired by the CheckList for testing natural language processing, we exploit VL-CheckList, a novel framework to understand the capabilities of VLP models. The proposed method divides the image-texting ability of a VLP model into three categories: objects, attributes, and relations, and uses a novel taxonomy to further break down these three aspects. We conduct comprehensive studies to analyze seven recently popular VLP models via the proposed framework. Results confirm the effectiveness of the proposed method by revealing fine-grained differences among the compared mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;Less-Learn-Shortcut (LLS)&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#21333;&#35789;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#37327;&#21270;&#26377;&#20559;&#26679;&#26412;&#30340;&#26377;&#20559;&#31243;&#24230;&#65292;&#21152;&#24378;&#26080;&#20559;&#26679;&#26412;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.12593</link><description>&lt;p&gt;
&#12298;Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation&#12299; &#65288;arXiv:2205.12593v2 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation. (arXiv:2205.12593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;Less-Learn-Shortcut (LLS)&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#21333;&#35789;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#37327;&#21270;&#26377;&#20559;&#26679;&#26412;&#30340;&#26377;&#20559;&#31243;&#24230;&#65292;&#21152;&#24378;&#26080;&#20559;&#26679;&#26412;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23558;&#25968;&#25454;&#38598;&#30340;&#20559;&#32622;&#20316;&#20026;&#25463;&#24452;&#26469;&#20570;&#20986;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#29702;&#35299;&#20219;&#21153;&#65292;&#23548;&#33268;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#22833;&#36133;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#20559;&#32622;&#20998;&#24067;&#20013;&#23398;&#21040;&#30340;&#21333;&#35789;&#29305;&#24449;&#19982;&#26631;&#31614;&#38388;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#39640;&#24230;&#20849;&#29616;&#22312;&#26576;&#20010;&#29305;&#23450;&#26631;&#31614;&#19979;&#30340;&#21333;&#35789;&#23450;&#20041;&#20026;&#26377;&#20559;&#21333;&#35789;&#65292;&#23558;&#21253;&#21547;&#26377;&#20559;&#21333;&#35789;&#30340;&#26679;&#26412;&#23450;&#20041;&#20026;&#26377;&#20559;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#26377;&#20559;&#26679;&#26412;&#23545;&#20110;&#27169;&#22411;&#26469;&#35828;&#26356;&#23481;&#26131;&#23398;&#20064;&#65292;&#22312;&#39044;&#27979;&#26102;&#65292;&#26377;&#20559;&#21333;&#35789;&#22312;&#27169;&#22411;&#30340;&#39044;&#27979;&#20013;&#20316;&#20986;&#20102;&#26174;&#33879;&#30340;&#36129;&#29486;&#65292;&#27169;&#22411;&#24448;&#24448;&#36807;&#24230;&#20381;&#36182;&#21333;&#35789;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#26469;&#36827;&#34892;&#26631;&#31614;&#39044;&#27979;&#12290;&#20026;&#20102;&#32531;&#35299;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#36825;&#31181;&#25463;&#24452;&#65288;&#21363;&#38169;&#35823;&#30456;&#20851;&#24615;&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;Less-Learn-Shortcut&#65288;LLS&#65289;&#65306;&#25105;&#20204;&#30340;&#31574;&#30053;&#37327;&#21270;&#20102;&#26377;&#20559;&#26679;&#26412;&#30340;&#26377;&#20559;&#31243;&#24230;&#65292;&#21516;&#26102;&#21152;&#24378;&#20102;&#26080;&#20559;&#26679;&#26412;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has revealed that deep neural networks often take dataset biases as a shortcut to make decisions rather than understand tasks, leading to failures in real-world applications. In this study, we focus on the spurious correlation between word features and labels that models learn from the biased data distribution of training data. In particular, we define the word highly co-occurring with a specific label as biased word, and the example containing biased word as biased example. Our analysis shows that biased examples are easier for models to learn, while at the time of prediction, biased words make a significantly higher contribution to the models' predictions, and models tend to assign predicted labels over-relying on the spurious correlation between words and labels. To mitigate models' over-reliance on the shortcut (i.e. spurious correlation), we propose a training strategy Less-Learn-Shortcut (LLS): our strategy quantifies the biased degree of the biased examples and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;EmTract&#30340;&#24320;&#28304;&#24037;&#20855;&#65292;&#20854;&#22522;&#20110;&#39044;&#35843;&#25972;&#30340;NLP&#27169;&#22411;DistilBERT&#21644;4,861&#20010;&#26631;&#35760;&#65292;&#25552;&#21462;&#38754;&#21521;&#37329;&#34701;&#29615;&#22659;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#30340;&#24773;&#32490;&#65292;&#24182;&#22312;&#20154;&#24037;&#21644;chatGPT&#27880;&#37322;&#25968;&#25454;&#19978;&#20248;&#20110;&#31454;&#20105;&#30340;&#24320;&#28304;&#26368;&#20808;&#36827;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#26041;&#27861;&#20855;&#26377;&#37327;&#36523;&#23450;&#21046;&#30340;&#29305;&#28857;&#20197;&#21450;&#38024;&#23545;&#38750;&#26631;&#20934;&#30701;&#35821;&#12289;&#34920;&#24773;&#31526;&#21495;&#21644;&#34920;&#24773;&#31526;&#21495;&#36825;&#31867;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2112.03868</link><description>&lt;p&gt;
EmTract&#65306;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
EmTract: Extracting Emotions from Social Media. (arXiv:2112.03868v3 [q-fin.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;EmTract&#30340;&#24320;&#28304;&#24037;&#20855;&#65292;&#20854;&#22522;&#20110;&#39044;&#35843;&#25972;&#30340;NLP&#27169;&#22411;DistilBERT&#21644;4,861&#20010;&#26631;&#35760;&#65292;&#25552;&#21462;&#38754;&#21521;&#37329;&#34701;&#29615;&#22659;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#30340;&#24773;&#32490;&#65292;&#24182;&#22312;&#20154;&#24037;&#21644;chatGPT&#27880;&#37322;&#25968;&#25454;&#19978;&#20248;&#20110;&#31454;&#20105;&#30340;&#24320;&#28304;&#26368;&#20808;&#36827;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#26041;&#27861;&#20855;&#26377;&#37327;&#36523;&#23450;&#21046;&#30340;&#29305;&#28857;&#20197;&#21450;&#38024;&#23545;&#38750;&#26631;&#20934;&#30701;&#35821;&#12289;&#34920;&#24773;&#31526;&#21495;&#21644;&#34920;&#24773;&#31526;&#21495;&#36825;&#31867;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#65288;EmTract&#65289;&#65292;&#29992;&#20110;&#20174;&#38754;&#21521;&#37329;&#34701;&#29615;&#22659;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#25552;&#21462;&#24773;&#32490;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26631;&#27880;&#20102;&#19968;&#19975;&#26465;&#26469;&#33258;&#37329;&#34701;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;StockTwits&#30340;&#30701;&#28040;&#24687;&#65292;&#24182;&#23558;&#20854;&#19982;&#24320;&#28304;&#24773;&#32490;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35843;&#25972;&#22909;&#30340;NLP&#27169;&#22411;DistilBERT&#65292;&#36890;&#36807;&#21253;&#25324;4,861&#20010;&#26631;&#35760;&#65288;&#34920;&#24773;&#31526;&#21495;&#21644;&#34920;&#24773;&#31526;&#21495;&#65289;&#26469;&#22686;&#21152;&#20854;&#23884;&#20837;&#31354;&#38388;&#65292;&#28982;&#21518;&#39318;&#20808;&#22312;&#24320;&#28304;&#24773;&#24863;&#25968;&#25454;&#19978;&#23545;&#20854;&#36827;&#34892;&#25311;&#21512;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#31227;&#33267;&#25105;&#20204;&#26631;&#27880;&#30340;&#37329;&#34701;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20154;&#24037;&#21644;chatGPT&#27880;&#37322;&#25968;&#25454;&#19978;&#22343;&#20248;&#20110;&#31454;&#20105;&#30340;&#24320;&#28304;&#26368;&#20808;&#36827;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22914;Emotion English DistilRoBERTa-base&#12290;&#19982;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37329;&#34701;&#30740;&#31350;&#20013;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#38024;&#23545;&#37329;&#34701;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#36827;&#34892;&#20102;&#37327;&#36523;&#23450;&#21046;&#65307;&#20854;&#27425;&#65292;&#23427;&#21253;&#21547;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#22914;&#38750;&#26631;&#20934;&#30701;&#35821;&#12289;&#34920;&#24773;&#31526;&#21495;&#21644;&#34920;&#24773;&#31526;&#21495;&#65307;&#31532;&#19977;&#65292;&#23427;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an open-source tool (EmTract) that extracts emotions from social media text tailed for financial context. To do so, we annotate ten thousand short messages from a financial social media platform (StockTwits) and combine it with open-source emotion data. We then use a pre-tuned NLP model, DistilBERT, augment its embedding space by including 4,861 tokens (emojis and emoticons), and then fit it first on the open-source emotion data, then transfer it to our annotated financial social media data. Our model outperforms competing open-source state-of-the-art emotion classifiers, such as Emotion English DistilRoBERTa-base on both human and chatGPT annotated data. Compared to dictionary based methods, our methodology has three main advantages for research in finance. First, our model is tailored to financial social media text; second, it incorporates key aspects of social media data, such as non-standard phrases, emojis, and emoticons; and third, it operates by sequentially learning 
&lt;/p&gt;</description></item></channel></rss>