<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>GameEval&#26159;&#19968;&#31181;&#36890;&#36807;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#28216;&#25103;&#26469;&#35780;&#20272;LLM&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#21512;&#20316;&#25110;&#23545;&#25239;&#30446;&#26631;&#30340;&#29420;&#29305;&#28216;&#25103;&#65292;GameEval&#21487;&#20197;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10032</link><description>&lt;p&gt;
GameEval: &#22312;&#23545;&#35805;&#28216;&#25103;&#19978;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
GameEval: Evaluating LLMs on Conversational Games. (arXiv:2308.10032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10032
&lt;/p&gt;
&lt;p&gt;
GameEval&#26159;&#19968;&#31181;&#36890;&#36807;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#28216;&#25103;&#26469;&#35780;&#20272;LLM&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#21512;&#20316;&#25110;&#23545;&#25239;&#30446;&#26631;&#30340;&#29420;&#29305;&#28216;&#25103;&#65292;GameEval&#21487;&#20197;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#22522;&#20110;&#21442;&#32771;&#65292;&#35201;&#20040;&#22522;&#20110;&#20559;&#22909;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#20154;&#20026;&#24178;&#39044;&#25110;&#24341;&#20837;&#30001;&#35780;&#20272;&#27169;&#22411;&#24341;&#36215;&#30340;&#27979;&#35797;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GameEval&#65292;&#19968;&#31181;&#36890;&#36807;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#28216;&#25103;&#26469;&#35780;&#20272;LLM&#30340;&#26032;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;GameEval&#23558;LLM&#35270;&#20026;&#28216;&#25103;&#29609;&#23478;&#65292;&#24182;&#20026;&#20182;&#20204;&#20998;&#37197;&#20855;&#26377;&#29305;&#23450;&#30446;&#26631;&#30340;&#19981;&#21516;&#35282;&#33394;&#65292;&#36890;&#36807;&#21551;&#21160;&#21508;&#31181;&#24418;&#24335;&#30340;&#23545;&#35805;&#65288;&#21253;&#25324;&#35752;&#35770;&#12289;&#38382;&#31572;&#21644;&#25237;&#31080;&#65289;&#26469;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#20855;&#26377;&#21512;&#20316;&#25110;&#23545;&#25239;&#30446;&#26631;&#30340;&#29420;&#29305;&#28216;&#25103;&#65292;&#24182;&#38468;&#24102;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#23637;&#31034;&#36825;&#31181;&#26032;&#33539; Paradigm&#22914;&#20309;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GameEval&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#21508;&#31181;LLM&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#23545;&#23427;&#20204;&#25972;&#20307;&#34920;&#29616;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have presented challenges in evaluating those models. Existing evaluation methods are either reference-based or preference based, which inevitably need human intervention or introduce test bias caused by evaluator models. In this paper, we propose GameEval, a novel approach to evaluating LLMs through goal-driven conversational games, overcoming the limitations of previous methods. GameEval treats LLMs as game players and assigns them distinct roles with specific goals achieved by launching conversations of various forms, including discussion, question answering, and voting. We design three unique games with cooperative or adversarial objectives, accompanied by corresponding evaluation metrics, to show how this new paradigm comprehensively evaluates model performance.Through extensive experiments, we show that GameEval can effectively differentiate the capabilities of various LLMs, providing a comprehensive assessment of their integ
&lt;/p&gt;</description></item><item><title>ControlRetriever&#26159;&#19968;&#31181;&#26377;&#21442;&#25968;&#38548;&#31163;&#26550;&#26500;&#30340;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20511;&#21161;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#65292;&#33021;&#22815;&#25511;&#21046;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30452;&#25509;&#25191;&#34892;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#65292;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10025</link><description>&lt;p&gt;
ControlRetriever: &#21457;&#25381;&#25351;&#20196;&#30340;&#21147;&#37327;&#36827;&#34892;&#21487;&#25511;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
ControlRetriever: Harnessing the Power of Instructions for Controllable Retrieval. (arXiv:2308.10025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10025
&lt;/p&gt;
&lt;p&gt;
ControlRetriever&#26159;&#19968;&#31181;&#26377;&#21442;&#25968;&#38548;&#31163;&#26550;&#26500;&#30340;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20511;&#21161;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#65292;&#33021;&#22815;&#25511;&#21046;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30452;&#25509;&#25191;&#34892;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#65292;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32570;&#20047;&#19987;&#38376;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22312;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#20013;&#24448;&#24448;&#38590;&#20197;&#34920;&#29616;&#20986;&#33394;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#26816;&#32034;&#20219;&#21153;&#36890;&#24120;&#28041;&#21450;&#19981;&#21516;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ControlRetriever&#65292;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#25511;&#21046;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30452;&#25509;&#25191;&#34892;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#65292;&#21457;&#25381;&#33258;&#28982;&#35821;&#35328;&#20013;&#26126;&#30830;&#25551;&#36848;&#26816;&#32034;&#24847;&#22270;&#30340;&#25351;&#20196;&#30340;&#21147;&#37327;&#12290;&#20511;&#21161;&#24050;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35777;&#26126;&#24378;&#22823;&#30340;ControlNet&#30340;&#22522;&#30784;&#65292;ControlRetriever&#23558;&#19981;&#21516;&#30340;&#26816;&#32034;&#27169;&#22411;&#36171;&#20104;&#20102;&#21487;&#25511;&#24615;&#26816;&#32034;&#30340;&#26032;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#21463;&#21040;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#25351;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#24341;&#23548;&#30340;&#25351;&#20196;&#21512;&#25104;&#21644;&#36845;&#20195;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#24191;&#27867;&#29983;&#25104;&#30340;&#26816;&#32034;&#25968;&#25454;&#36845;&#20195;&#35843;&#25972;ControlRetriever&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that dense retrieval models, lacking dedicated training data, struggle to perform well across diverse retrieval tasks, as different retrieval tasks often entail distinct search intents. To address this challenge, in this work we introduce ControlRetriever, a generic and efficient approach with a parameter isolated architecture, capable of controlling dense retrieval models to directly perform varied retrieval tasks, harnessing the power of instructions that explicitly describe retrieval intents in natural language. Leveraging the foundation of ControlNet, which has proven powerful in text-to-image generation, ControlRetriever imbues different retrieval models with the new capacity of controllable retrieval, all while being guided by task-specific instructions. Furthermore, we propose a novel LLM guided Instruction Synthesizing and Iterative Training strategy, which iteratively tunes ControlRetriever based on extensive automatically-generated retrieval data wit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#39537;&#21160;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;HICL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#65283;hashtags&#39537;&#21160;BERT&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#20351;&#65283;Encoder&#33021;&#22815;&#25972;&#21512;&#20027;&#39064;&#30456;&#20851;&#35821;&#20041;&#20449;&#24687;&#65292;&#20016;&#23500;&#19978;&#19979;&#25991;&#24182;&#25552;&#21319;&#31038;&#20132;&#23186;&#20307;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09985</link><description>&lt;p&gt;
HICL: &#22522;&#20110;&#26631;&#31614;&#39537;&#21160;&#30340;&#31038;&#20132;&#23186;&#20307;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HICL: Hashtag-Driven In-Context Learning for Social Media Natural Language Understanding. (arXiv:2308.09985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#39537;&#21160;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;HICL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#65283;hashtags&#39537;&#21160;BERT&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#20351;&#65283;Encoder&#33021;&#22815;&#25972;&#21512;&#20027;&#39064;&#30456;&#20851;&#35821;&#20041;&#20449;&#24687;&#65292;&#20016;&#23500;&#19978;&#19979;&#25991;&#24182;&#25552;&#21319;&#31038;&#20132;&#23186;&#20307;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#23545;&#21508;&#31181;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLU&#27169;&#22411;&#22312;&#35821;&#20041;&#23398;&#20064;&#20013;&#36807;&#20110;&#20381;&#36182;&#19978;&#19979;&#25991;&#65292;&#23548;&#33268;&#22312;&#38754;&#23545;&#30701;&#19988;&#22024;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#26102;&#24615;&#33021;&#21463;&#25439;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#26377;&#38480;&#30340;&#31034;&#20363;&#36827;&#34892;&#26465;&#20214;&#25512;&#29702;&#20174;&#32780;&#20016;&#23500;&#19978;&#19979;&#25991;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26631;&#31614;&#39537;&#21160;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;HICL&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39044;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65283;Encoder&#65292;&#23427;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20351;&#24471;&#22522;&#20110;BERT&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#36890;&#36807;&#65283;hashtags&#65288;&#29992;&#25143;&#27880;&#37322;&#30340;&#20027;&#39064;&#26631;&#31614;&#65289;&#39537;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#65283;Encoder&#33021;&#22815;&#33719;&#21462;&#25972;&#21512;&#20102;&#20027;&#39064;&#30456;&#20851;&#35821;&#20041;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#26816;&#32034;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#24086;&#23376;&#20197;&#20016;&#23500;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#22122;&#22768;&#19978;&#19979;&#25991;&#22686;&#24378;&#31038;&#20132;&#23186;&#20307;NLU&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#23558;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#19982;&#28304;&#25991;&#26412;&#25972;&#21512;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#26631;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language understanding (NLU) is integral to various social media applications. However, existing NLU models rely heavily on context for semantic learning, resulting in compromised performance when faced with short and noisy social media content. To address this issue, we leverage in-context learning (ICL), wherein language models learn to make inferences by conditioning on a handful of demonstrations to enrich the context and propose a novel hashtag-driven in-context learning (HICL) framework. Concretely, we pre-train a model #Encoder, which employs #hashtags (user-annotated topic labels) to drive BERT-based pre-training through contrastive learning. Our objective here is to enable #Encoder to gain the ability to incorporate topic-related semantic information, which allows it to retrieve topic-related posts to enrich contexts and enhance social media NLU with noisy contexts. To further integrate the retrieved context with the source text, we employ a gradient-based method to id
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30693;&#35782;&#19978;&#30340;&#22522;&#20934;FinEval&#12290;&#36890;&#36807;&#22312;FinEval&#19978;&#35780;&#20272;&#20013;&#33521;&#25991;LLMs&#65292;&#32467;&#26524;&#26174;&#31034;&#21482;&#26377;GPT-4&#22312;&#19981;&#21516;&#25552;&#31034;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#25509;&#36817;70%&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#30693;&#35782;&#20013;&#30340;&#26174;&#33879;&#22686;&#38271;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.09975</link><description>&lt;p&gt;
FinEval&#65306;&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#37329;&#34701;&#39046;&#22495;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models. (arXiv:2308.09975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30693;&#35782;&#19978;&#30340;&#22522;&#20934;FinEval&#12290;&#36890;&#36807;&#22312;FinEval&#19978;&#35780;&#20272;&#20013;&#33521;&#25991;LLMs&#65292;&#32467;&#26524;&#26174;&#31034;&#21482;&#26377;GPT-4&#22312;&#19981;&#21516;&#25552;&#31034;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#25509;&#36817;70%&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#30693;&#35782;&#20013;&#30340;&#26174;&#33879;&#22686;&#38271;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#19987;&#19994;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FinEval&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;LLMs&#20013;&#30340;&#37329;&#34701;&#39046;&#22495;&#30693;&#35782;&#35774;&#35745;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;FinEval&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;&#37329;&#34701;&#12289;&#32463;&#27982;&#12289;&#20250;&#35745;&#21644;&#35777;&#20070;&#31561;34&#20010;&#23398;&#26415;&#31185;&#30446;&#30340;&#39640;&#36136;&#37327;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#38598;&#21512;&#65292;&#24635;&#35745;&#21253;&#21547;&#20102;4,661&#36947;&#39064;&#30446;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;FinEval&#20351;&#29992;&#20102;&#22810;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#20197;&#21450;&#20165;&#31572;&#26696;&#25552;&#31034;&#21644;&#24605;&#36335;&#38142;&#24335;&#25552;&#31034;&#12290;&#36890;&#36807;&#22312;FinEval&#19978;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLMs&#65292;&#32467;&#26524;&#26174;&#31034;&#21482;&#26377;GPT-4&#22312;&#19981;&#21516;&#30340;&#25552;&#31034;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#25509;&#36817;70%&#30340;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#30693;&#35782;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#22686;&#38271;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#37329;&#34701;&#39046;&#22495;&#30340;&#30693;&#35782;&#35780;&#20272;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated exceptional performance in various natural language processing tasks, yet their efficacy in more challenging and domain-specific tasks remains largely unexplored. This paper presents FinEval, a benchmark specifically designed for the financial domain knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice questions covering Finance, Economy, Accounting, and Certificate. It includes 4,661 questions spanning 34 different academic subjects. To ensure a comprehensive model performance evaluation, FinEval employs a range of prompt types, including zero-shot and few-shot prompts, as well as answer-only and chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs on FinEval, the results show that only GPT-4 achieved an accuracy close to 70% in different prompt settings, indicating significant growth potential for LLMs in the financial domain knowledge. Our work offers a more comprehensive financial kno
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20869;&#24515;&#29420;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65288;IMMO&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#28151;&#21512;&#34701;&#21512;&#21644;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09970</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20869;&#24515;&#29420;&#30333;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Tackling Vision Language Tasks Through Learning Inner Monologues. (arXiv:2308.09970v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09970
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20869;&#24515;&#29420;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65288;IMMO&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#28151;&#21512;&#34701;&#21512;&#21644;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#38656;&#35201;AI&#27169;&#22411;&#23545;&#35270;&#35273;&#21644;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#29702;&#35299;&#21644;&#25512;&#29702;&#12290;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#21147;&#37327;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#65288;1&#65289;LLM&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20043;&#38388;&#30340;&#28151;&#21512;&#34701;&#21512;&#65292;&#20854;&#20013;&#35270;&#35273;&#36755;&#20837;&#39318;&#20808;&#34987;VLM&#36716;&#21270;&#20026;&#35821;&#35328;&#25551;&#36848;&#65292;&#25104;&#20026;LLM&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#30340;&#36755;&#20837;&#65307;&#65288;2&#65289;&#35821;&#35328;&#31354;&#38388;&#20013;&#30340;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#65292;&#20854;&#20013;&#35270;&#35273;&#36755;&#20837;&#34987;&#32534;&#30721;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#30417;&#30563;&#24494;&#35843;&#23558;&#20854;&#25237;&#24433;&#21040;LLM&#30340;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20855;&#26377;&#36731;&#37327;&#32423;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#24456;&#38590;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#29305;&#24449;&#23545;&#40784;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20869;&#24515;&#29420;&#30333;&#22810;&#27169;&#24577;&#20248;&#21270;&#65288;IMMO&#65289;&#65292;&#36890;&#36807;&#27169;&#25311;&#24605;&#32500;&#36807;&#31243;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#20005;&#37325;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#21457;&#29616;few-shot&#24341;&#23548;&#23545;&#20110;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#20013;&#36716;&#21518;&#65292;&#36825;&#31181;&#24046;&#24322;&#28040;&#22833;&#20102;&#12290;&#22312;WebNLG 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;few-shot + &#32763;&#35793;&#31995;&#32479;&#30340;&#21464;&#20307;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#37117;&#20248;&#20110;&#31454;&#20105;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09957</link><description>&lt;p&gt;
&#20351;&#29992;GPT-3.5&#22312;&#36164;&#28304;&#20005;&#37325;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;:&#38656;&#35201;&#35895;&#27468;&#32763;&#35793;&#30340;&#19968;&#28857;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Data-to-text Generation for Severely Under-Resourced Languages with GPT-3.5: A Bit of Help Needed from Google Translate. (arXiv:2308.09957v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#20005;&#37325;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#21457;&#29616;few-shot&#24341;&#23548;&#23545;&#20110;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#20013;&#36716;&#21518;&#65292;&#36825;&#31181;&#24046;&#24322;&#28040;&#22833;&#20102;&#12290;&#22312;WebNLG 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;few-shot + &#32763;&#35793;&#31995;&#32479;&#30340;&#21464;&#20307;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#37117;&#20248;&#20110;&#31454;&#20105;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#65288;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT&#65289;&#22312;&#22788;&#29702;&#33521;&#35821;&#30456;&#20851;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20013;&#33521;&#35821;&#30340;&#27604;&#20363;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#22914;&#20309;&#24212;&#23545;&#37027;&#20123;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20005;&#37325;&#19981;&#36275;&#30340;&#35821;&#35328;&#65292;&#20855;&#20307;&#21253;&#25324;&#29233;&#23572;&#20848;&#35821;&#12289;&#39532;&#32819;&#20182;&#35821;&#12289;&#23041;&#23572;&#22763;&#35821;&#21644;&#24067;&#37324;&#22810;&#23612;&#35821;&#12290;&#25105;&#20204;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#36827;&#34892;&#20102;&#24341;&#23548;&#24335;&#24037;&#31243;&#38454;&#27573;&#30340;&#27979;&#35797;&#65292;&#20351;&#29992;&#20102;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#36755;&#20837;/&#36755;&#20986;&#23545;&#30340;&#21508;&#31181;&#24341;&#23548;&#31867;&#22411;&#21644;&#26684;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#20004;&#31181;&#22330;&#26223;&#20013;&#23545;&#20004;&#20010;&#26368;&#26377;&#24076;&#26395;&#30340;&#24341;&#23548;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65306;&#65288;i&#65289;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#65288;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65289;&#65292;&#65288;ii&#65289;&#29983;&#25104;&#33521;&#35821;&#21518;&#20877;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#30452;&#25509;&#29983;&#25104;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#26041;&#38754;&#65292;few-shot&#24341;&#23548;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#20013;&#36716;&#21518;&#65292;&#36825;&#31181;&#24046;&#24322;&#28040;&#22833;&#20102;&#12290;&#25105;&#20204;&#23558;few-shot + &#32763;&#35793;&#31995;&#32479;&#30340;&#21464;&#20307;&#25552;&#20132;&#21040;&#20102;WebNLG 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#65292;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#37117;&#27604;&#31454;&#20105;&#31995;&#32479;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs like GPT are great at tasks involving English which dominates in their training data. In this paper, we look at how they cope with tasks involving languages that are severely under-represented in their training data, in the context of data-to-text generation for Irish, Maltese, Welsh and Breton. During the prompt-engineering phase we tested a range of prompt types and formats on GPT-3.5 and~4 with a small sample of example input/output pairs. We then fully evaluated the two most promising prompts in two scenarios: (i) direct generation into the under-resourced language, and (ii) generation into English followed by translation into the under-resourced language. We find that few-shot prompting works better for direct generation into under-resourced languages, but that the difference disappears when pivoting via English. The few-shot + translation system variants were submitted to the WebNLG 2023 shared task where they outperformed competitor systems by substantial margins in all lan
&lt;/p&gt;</description></item><item><title>Eva-KELLM&#26159;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#21644;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#26469;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#25910;&#38598;&#25104;&#26412;&#39640;&#12289;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#22256;&#38590;&#12289;&#35780;&#20272;&#35270;&#35282;&#21463;&#38480;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09954</link><description>&lt;p&gt;
Eva-KELLM&#65306;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs. (arXiv:2308.09954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09954
&lt;/p&gt;
&lt;p&gt;
Eva-KELLM&#26159;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#21644;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#26469;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#25910;&#38598;&#25104;&#26412;&#39640;&#12289;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#22256;&#38590;&#12289;&#35780;&#20272;&#35270;&#35282;&#21463;&#38480;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30528;&#20016;&#23500;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30693;&#35782;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21487;&#33021;&#21464;&#24471;&#36807;&#26102;&#25110;&#19981;&#21512;&#36866;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#24182;&#35780;&#20272;&#20854;&#25928;&#26524;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20107;&#23454;&#19977;&#20803;&#32452;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#65292;&#36825;&#19981;&#20165;&#22312;&#25910;&#38598;&#19978;&#20135;&#29983;&#39640;&#25104;&#26412;&#65292;&#32780;&#19988;&#22312;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#26102;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30740;&#31350;&#22312;&#35780;&#20272;&#35270;&#35282;&#19978;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Eva-KELLM&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#39318;&#20808;&#35201;&#27714;LLM&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#65292;&#19982;&#20351;&#29992;&#20107;&#23454;&#19977;&#20803;&#32452;&#30456;&#27604;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26041;&#20415;&#12289;&#26356;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#20174;&#22810;&#20010;&#35282;&#24230;&#35780;&#20272;&#26356;&#26032;&#21518;&#30340;LLM&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) possess a wealth of knowledge encoded in their parameters. However, this knowledge may become outdated or unsuitable over time. As a result, there has been a growing interest in knowledge editing for LLMs and evaluating its effectiveness. Existing studies primarily focus on knowledge editing using factual triplets, which not only incur high costs for collection but also struggle to express complex facts. Furthermore, these studies are often limited in their evaluation perspectives. In this paper, we propose Eva-KELLM, a new benchmark for evaluating knowledge editing of LLMs. This benchmark includes an evaluation framework and a corresponding dataset. Under our framework, we first ask the LLM to perform knowledge editing using raw documents, which provides a more convenient and universal approach compared to using factual triplets. We then evaluate the updated LLM from multiple perspectives. In addition to assessing the effectiveness of knowledge editing and
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#23545;&#20020;&#24202;&#35843;&#26597;&#25968;&#25454;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#35780;&#20272;&#29305;&#24449;&#21517;&#31216;&#19982;&#30446;&#26631;&#21517;&#31216;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#36873;&#25321;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.09892</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#36827;&#34892;&#20020;&#24202;&#35843;&#26597;&#25968;&#25454;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection. (arXiv:2308.09892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09892
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#23545;&#20020;&#24202;&#35843;&#26597;&#25968;&#25454;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#35780;&#20272;&#29305;&#24449;&#21517;&#31216;&#19982;&#30446;&#26631;&#21517;&#31216;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#36873;&#25321;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#22823;&#37327;&#29305;&#24449;&#65292;&#32780;&#31034;&#20363;&#25968;&#37327;&#30456;&#23545;&#36739;&#20302;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35797;&#22270;&#20174;&#35843;&#26597;&#25968;&#25454;&#39044;&#27979;&#32467;&#26524;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#29305;&#24449;&#36873;&#25321;&#65292;&#23427;&#35797;&#22270;&#36873;&#25321;&#19968;&#20010;&#26368;&#20339;&#30340;&#29305;&#24449;&#23376;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#19968;&#20010;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#20449;&#24687;&#26469;&#28304;&#26159;&#29305;&#24449;&#30340;&#25991;&#26412;&#21517;&#31216;&#65292;&#36825;&#21487;&#33021;&#22312;&#35821;&#20041;&#19978;&#25351;&#31034;&#21738;&#20123;&#29305;&#24449;&#19982;&#30446;&#26631;&#32467;&#26524;&#30456;&#20851;&#12290;&#21487;&#20197;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35780;&#20272;&#29305;&#24449;&#21517;&#31216;&#21644;&#30446;&#26631;&#21517;&#31216;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#29983;&#25104;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20998;&#25968;&#65292;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#36873;&#25321;&#29305;&#24449;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;STS&#30452;&#25509;&#36873;&#25321;&#29305;&#24449;&#21644;&#22312;&#26368;&#23567;&#20887;&#20313;&#26368;&#22823;&#30456;&#20851;&#65288;mRMR&#65289;&#31639;&#27861;&#20013;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#21021;&#27493;&#35843;&#26597;&#30340;&#32467;&#26524;&#35780;&#20272;&#20102;STS&#20316;&#20026;&#29305;&#24449;&#36873;&#25321;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survey data can contain a high number of features while having a comparatively low quantity of examples. Machine learning models that attempt to predict outcomes from survey data under these conditions can overfit and result in poor generalizability. One remedy to this issue is feature selection, which attempts to select an optimal subset of features to learn upon. A relatively unexplored source of information in the feature selection process is the usage of textual names of features, which may be semantically indicative of which features are relevant to a target outcome. The relationships between feature names and target names can be evaluated using language models (LMs) to produce semantic textual similarity (STS) scores, which can then be used to select features. We examine the performance using STS to select features directly and in the minimal-redundancy-maximal-relevance (mRMR) algorithm. The performance of STS as a feature selection metric is evaluated against preliminary survey
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#65288;IBL&#65289;&#65292;&#23427;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#20195;&#30721;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#21040;&#25552;&#31034;&#20013;&#65292;&#36755;&#20986;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2308.09890</link><description>&lt;p&gt;
&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;: &#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Inductive-bias Learning: Generating Code Models with Large Language Model. (arXiv:2308.09890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09890
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#65288;IBL&#65289;&#65292;&#23427;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#20195;&#30721;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#21040;&#25552;&#31034;&#20013;&#65292;&#36755;&#20986;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22240;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26041;&#38754;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;ICL&#25216;&#26415;&#22312;&#19981;&#26356;&#26032;LLM&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#21040;&#25552;&#31034;&#20013;&#21363;&#21487;&#23454;&#29616;&#22522;&#20110;&#35268;&#21017;&#30340;&#39640;&#20934;&#30830;&#24615;&#25512;&#29702;&#12290;&#34429;&#28982;ICL&#26159;&#19968;&#20010;&#21457;&#23637;&#20013;&#30340;&#39046;&#22495;&#65292;&#36824;&#26377;&#35768;&#22810;&#26410;&#35299;&#31572;&#30340;&#38382;&#39064;&#65292;&#20294;LLMs&#26412;&#36523;&#20316;&#20026;&#25512;&#29702;&#27169;&#22411;&#20284;&#20046;&#23454;&#29616;&#20102;&#19981;&#38656;&#35201;&#26126;&#30830;&#25351;&#20986;"&#24402;&#32435;&#20559;&#24046;"&#30340;&#25512;&#29702;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20195;&#30721;&#29983;&#25104;&#20063;&#26159;LLMs&#30340;&#19968;&#39033;&#37325;&#35201;&#24212;&#29992;&#12290;&#20195;&#30721;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#22823;&#22823;&#25552;&#39640;&#65292;&#20351;&#24471;&#21363;&#20351;&#38750;&#24037;&#31243;&#24072;&#20063;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#29983;&#25104;&#25191;&#34892;&#25152;&#38656;&#20219;&#21153;&#30340;&#20195;&#30721;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#23398;&#20064;&#8221;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#65288;IBL&#65289;&#8221;&#65292;&#23427;&#32467;&#21512;&#20102;ICL&#21644;&#20195;&#30721;&#29983;&#25104;&#30340;&#25216;&#26415;&#12290;IBL&#30340;&#24605;&#24819;&#24456;&#30452;&#35266;&#12290;&#19982;ICL&#31867;&#20284;&#65292;IBL&#23558;&#35757;&#32451;&#25968;&#25454;&#36755;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#24182;&#36755;&#20986;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models(LLMs) have been attracting attention due to a ability called in-context learning(ICL). ICL, without updating the parameters of a LLM, it is possible to achieve highly accurate inference based on rules ``in the context'' by merely inputting a training data into the prompt. Although ICL is a developing field with many unanswered questions, LLMs themselves serves as a inference model, seemingly realizing inference without explicitly indicate ``inductive bias''. On the other hand, a code generation is also a highlighted application of LLMs. The accuracy of code generation has dramatically improved, enabling even non-engineers to generate code to perform the desired tasks by crafting appropriate prompts. In this paper, we propose a novel ``learning'' method called an ``Inductive-Bias Learning (IBL)'', which combines the techniques of ICL and code generation. An idea of IBL is straightforward. Like ICL, IBL inputs a training data into the prompt and outputs a code with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32763;&#35793;SQuAD 2.0&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#31181;&#35821;&#35328;&#30340;&#26368;&#22909;&#34920;&#29616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.09862</link><description>&lt;p&gt;
&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#29992;&#20110;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi. (arXiv:2308.09862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32763;&#35793;SQuAD 2.0&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#31181;&#35821;&#35328;&#30340;&#26368;&#22909;&#34920;&#29616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#24320;&#21457;&#20986;&#39640;&#24230;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#23545;&#25968;&#25454;&#26377;&#30528;&#26080;&#27490;&#22659;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#26469;&#35828;&#65292;&#26500;&#24314;&#33391;&#22909;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#37325;&#28857;&#26159;&#20026;&#20004;&#31181;&#36825;&#26679;&#30340;&#35821;&#35328;-&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;-&#24320;&#21457;&#19968;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#21360;&#22320;&#35821;&#26159;&#20840;&#29699;&#31532;&#19977;&#22823;&#20351;&#29992;&#20154;&#25968;&#26368;&#22810;&#30340;&#35821;&#35328;&#65292;&#25317;&#26377;3.45&#20159;&#35828;&#35805;&#32773;&#65292;&#32780;&#39532;&#25289;&#22320;&#35821;&#21017;&#26159;&#20840;&#29699;&#31532;11&#22823;&#20351;&#29992;&#20154;&#25968;&#26368;&#22810;&#30340;&#35821;&#35328;&#65292;&#25317;&#26377;8.32&#21315;&#19975;&#35828;&#35805;&#32773;&#65292;&#20294;&#36825;&#20004;&#31181;&#35821;&#35328;&#22312;&#26500;&#24314;&#39640;&#25928;&#30340;&#38382;&#31572;&#31995;&#32479;&#30340;&#36164;&#28304;&#19978;&#37117;&#38754;&#20020;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#23558;SQuAD 2.0&#25968;&#25454;&#38598;&#32763;&#35793;&#25104;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#36825;&#20004;&#31181;&#35821;&#35328;&#20013;&#26368;&#22823;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;28,000&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#26550;&#26500;&#19978;&#35780;&#20272;&#20102;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#22312;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in deep-learning have led to the development of highly sophisticated systems with an unquenchable appetite for data. On the other hand, building good deep-learning models for low-resource languages remains a challenging task. This paper focuses on developing a Question Answering dataset for two such languages- Hindi and Marathi. Despite Hindi being the 3rd most spoken language worldwide, with 345 million speakers, and Marathi being the 11th most spoken language globally, with 83.2 million speakers, both languages face limited resources for building efficient Question Answering systems. To tackle the challenge of data scarcity, we have developed a novel approach for translating the SQuAD 2.0 dataset into Hindi and Marathi. We release the largest Question-Answering dataset available for these languages, with each dataset containing 28,000 samples. We evaluate the dataset on various architectures and release the best-performing models for both Hindi and Marathi, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#27450;&#39575;&#27169;&#22411;&#26816;&#32034;&#21040;&#21021;&#22987;&#20505;&#36873;&#25991;&#26723;&#38598;&#33539;&#22260;&#20043;&#22806;&#30340;&#30446;&#26631;&#25991;&#26723;&#12290;</title><link>http://arxiv.org/abs/2308.09861</link><description>&lt;p&gt;
&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#38024;&#23545;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65306;&#19968;&#31181;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Black-box Adversarial Attacks against Dense Retrieval Models: A Multi-view Contrastive Learning Method. (arXiv:2308.09861v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#27450;&#39575;&#27169;&#22411;&#26816;&#32034;&#21040;&#21021;&#22987;&#20505;&#36873;&#25991;&#26723;&#38598;&#33539;&#22260;&#20043;&#22806;&#30340;&#30446;&#26631;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#65288;NRMs&#65289;&#21644;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#27169;&#22411;&#22312;&#25972;&#20307;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25913;&#36827;&#12290;&#38500;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20043;&#22806;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#40065;&#26834;&#24615;&#24050;&#34987;&#35777;&#26126;&#19981;&#36275;&#65292;&#23545;&#20110;&#26680;&#24515;&#26816;&#32034;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#36234;&#26469;&#36234;&#24341;&#36215;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#25152;&#24320;&#21457;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25915;&#20987;NRMs&#19978;&#65292;&#23545;DR&#27169;&#22411;&#30340;&#25239;&#24615;&#40092;&#26377;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#25239;&#26816;&#32034;&#25915;&#20987;&#65288;AREA&#65289;&#20219;&#21153;&#12290;AREA&#20219;&#21153;&#26088;&#22312;&#27450;&#39575;DR&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#21709;&#24212;&#26597;&#35810;&#26102;&#20174;&#26368;&#21021;&#26816;&#32034;&#20505;&#36873;&#25991;&#26723;&#38598;&#20043;&#22806;&#26816;&#32034;&#21040;&#30446;&#26631;&#25991;&#26723;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20915;&#31574;&#30340;&#40657;&#30418;&#23545;&#25239;&#35774;&#32622;&#65292;&#36825;&#22312;&#23454;&#38469;&#25628;&#32034;&#24341;&#25806;&#20013;&#26159;&#29616;&#23454;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;AREA&#20219;&#21153;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#29616;&#26377;&#30340;&#20026;NRMs&#35774;&#35745;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ranking models (NRMs) and dense retrieval (DR) models have given rise to substantial improvements in overall retrieval performance. In addition to their effectiveness, and motivated by the proven lack of robustness of deep learning-based approaches in other areas, there is growing interest in the robustness of deep learning-based approaches to the core retrieval problem. Adversarial attack methods that have so far been developed mainly focus on attacking NRMs, with very little attention being paid to the robustness of DR models. In this paper, we introduce the adversarial retrieval attack (AREA) task. The AREA task is meant to trick DR models into retrieving a target document that is outside the initial set of candidate documents retrieved by the DR model in response to a query. We consider the decision-based black-box adversarial setting, which is realistic in real-world search engines. To address the AREA task, we first employ existing adversarial attack methods designed for N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#36718;&#36777;&#35770;&#20013;&#30340;&#21512;&#29702;&#24605;&#32771;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#36923;&#36753;&#33021;&#21147;&#27979;&#37327;&#22522;&#20934;&#65288;LOGICOM&#65289;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#36923;&#36753;&#35884;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#23637;&#31034;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#21253;&#21547;&#26377;&#20105;&#35758;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.09853</link><description>&lt;p&gt;
LLM&#23545;&#36923;&#36753;&#35884;&#35823;&#30340;&#23481;&#26131;&#21463;&#21040;&#30340;&#31243;&#24230;&#26377;&#22810;&#22823;&#65311;
&lt;/p&gt;
&lt;p&gt;
How susceptible are LLMs to Logical Fallacies?. (arXiv:2308.09853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#36718;&#36777;&#35770;&#20013;&#30340;&#21512;&#29702;&#24605;&#32771;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#36923;&#36753;&#33021;&#21147;&#27979;&#37327;&#22522;&#20934;&#65288;LOGICOM&#65289;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#36923;&#36753;&#35884;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#23637;&#31034;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#21253;&#21547;&#26377;&#20105;&#35758;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#35884;&#35823;&#35770;&#35777;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#36718;&#36777;&#35770;&#20013;&#30340;&#21512;&#29702;&#24605;&#32771;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;LLMs&#23545;&#36923;&#36753;&#25512;&#29702;&#24615;&#33021;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36923;&#36753;&#33021;&#21147;&#27979;&#37327;&#22522;&#20934;&#65288;LOGICOM&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#23545;&#36923;&#36753;&#35884;&#35823;&#30340;&#40065;&#26834;&#24615;&#30340;&#35786;&#26029;&#22522;&#20934;&#12290;LOGICOM&#28041;&#21450;&#20004;&#20010;&#35282;&#33394;&#65306;&#35828;&#26381;&#32773;&#21644;&#36777;&#25163;&#65292;&#22312;&#19968;&#20010;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#19978;&#36827;&#34892;&#22810;&#36718;&#36777;&#35770;&#65292;&#35828;&#26381;&#32773;&#35797;&#22270;&#35828;&#26381;&#36777;&#25163;&#20854;&#20027;&#24352;&#30340;&#27491;&#30830;&#24615;&#12290;&#39318;&#20808;&#65292;LOGICOM&#35780;&#20272;LLMs&#36890;&#36807;&#25512;&#29702;&#25913;&#21464;&#20854;&#35266;&#28857;&#30340;&#28508;&#21147;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#27604;&#35828;&#26381;&#32773;&#20351;&#29992;&#36923;&#36753;&#35884;&#35823;&#21644;&#20351;&#29992;&#36923;&#36753;&#25512;&#29702;&#30340;&#24773;&#26223;&#65292;&#35780;&#20272;&#36777;&#25163;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#21253;&#21547;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#12289;&#20027;&#24352;&#21644;&#25903;&#25345;&#29702;&#30001;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the rational thinking capability of Large Language Models (LLMs) in multi-round argumentative debates by exploring the impact of fallacious arguments on their logical reasoning performance. More specifically, we present Logic Competence Measurement Benchmark (LOGICOM), a diagnostic benchmark to assess the robustness of LLMs against logical fallacies. LOGICOM involves two agents: a persuader and a debater engaging in a multi-round debate on a controversial topic, where the persuader tries to convince the debater of the correctness of its claim. First, LOGICOM assesses the potential of LLMs to change their opinions through reasoning. Then, it evaluates the debater's performance in logical reasoning by contrasting the scenario where the persuader employs logical fallacies against one where logical reasoning is used. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4 using a dataset containing controversial topics, claims, and reasons supporting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OASIS&#65292;&#19968;&#20010;&#29992;&#20110;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#21464;&#24418;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#20195;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24694;&#24847;&#20869;&#23481;&#30340;&#20256;&#25773;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09810</link><description>&lt;p&gt;
&#19968;&#24133;&#22270;&#20687;&#32988;&#36807;&#21315;&#35328;&#19975;&#35821;&#65306;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#21464;&#24418;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth a Thousand Toxic Words: A Metamorphic Testing Framework for Content Moderation Software. (arXiv:2308.09810v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OASIS&#65292;&#19968;&#20010;&#29992;&#20110;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#21464;&#24418;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#20195;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24694;&#24847;&#20869;&#23481;&#30340;&#20256;&#25773;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#25351;&#25968;&#22686;&#38271;&#20026;&#20154;&#31867;&#31038;&#20250;&#30340;&#27807;&#36890;&#21644;&#20869;&#23481;&#20256;&#25773;&#24102;&#26469;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24179;&#21488;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#28389;&#29992;&#26469;&#20256;&#25773;&#26377;&#23475;&#20869;&#23481;&#65292;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#12289;&#24694;&#24847;&#24191;&#21578;&#21644;&#33394;&#24773;&#20869;&#23481;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#36127;&#38754;&#21518;&#26524;&#65292;&#22914;&#23545;&#38738;&#23569;&#24180;&#24515;&#29702;&#20581;&#24247;&#30340;&#20260;&#23475;&#12290;&#23613;&#31649;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#23457;&#26680;&#26041;&#27861;&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#20294;&#24694;&#24847;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23558;&#25991;&#23383;&#23884;&#20837;&#22270;&#20687;&#20013;&#26469;&#35268;&#36991;&#23457;&#26680;&#65292;&#20363;&#22914;&#25991;&#23383;&#30340;&#25130;&#22270;&#65292;&#36890;&#24120;&#24102;&#26377;&#19968;&#20123;&#24178;&#25200;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#20195;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#23545;&#27492;&#31867;&#24694;&#24847;&#36755;&#20837;&#30340;&#24615;&#33021;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OASIS&#65292;&#19968;&#20010;&#29992;&#20110;&#20869;&#23481;&#23457;&#26680;&#36719;&#20214;&#30340;&#21464;&#24418;&#27979;&#35797;&#26694;&#26550;&#12290;OASIS&#37319;&#29992;&#20102;21&#20010;&#36716;&#25442;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#26159;&#20174;&#25105;&#20204;&#23545;&#26469;&#33258;4&#20010;&#27969;&#34892;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#65288;&#21253;&#25324;Twitter&#12289;Instagram&#65289;&#25910;&#38598;&#30340;5000&#20010;&#30495;&#23454;&#26377;&#23475;&#20869;&#23481;&#30340;&#35797;&#39564;&#30740;&#31350;&#24635;&#32467;&#20986;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media platforms has brought about a revolution in communication and content dissemination in human society. Nevertheless, these platforms are being increasingly misused to spread toxic content, including hate speech, malicious advertising, and pornography, leading to severe negative consequences such as harm to teenagers' mental health. Despite tremendous efforts in developing and deploying textual and image content moderation methods, malicious users can evade moderation by embedding texts into images, such as screenshots of the text, usually with some interference. We find that modern content moderation software's performance against such malicious inputs remains underexplored. In this work, we propose OASIS, a metamorphic testing framework for content moderation software. OASIS employs 21 transform rules summarized from our pilot study on 5,000 real-world toxic contents collected from 4 popular social media applications, including Twitter, Instagram,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VL-PET&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#31890;&#24230;&#25511;&#21046;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#20462;&#25913;&#36827;&#34892;&#26377;&#25928;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#22312;&#24615;&#33021;&#21644;&#21151;&#33021;&#24046;&#36317;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.09804</link><description>&lt;p&gt;
VL-PET&#65306;&#36890;&#36807;&#31890;&#24230;&#25511;&#21046;&#23454;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control. (arXiv:2308.09804v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VL-PET&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#31890;&#24230;&#25511;&#21046;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#20462;&#25913;&#36827;&#34892;&#26377;&#25928;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#22312;&#24615;&#33021;&#21644;&#21151;&#33021;&#24046;&#36317;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#27169;&#22411;&#35268;&#27169;&#36805;&#36895;&#22686;&#38271;&#65292;&#20840;&#38754;&#24494;&#35843;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#23384;&#20648;&#26041;&#38754;&#21464;&#24471;&#20195;&#20215;&#39640;&#26114;&#12290;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#65288;VL&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#25216;&#26415;&#65292;&#23558;&#27169;&#22359;&#21270;&#20462;&#25913;&#65288;&#20363;&#22914;Adapter&#21644;LoRA&#65289;&#38598;&#25104;&#21040;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;PLMs&#20013;&#12290;&#36890;&#36807;&#35843;&#25972;&#19968;&#23567;&#32452;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#24615;&#33021;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#30340;&#27169;&#22359;&#21270;&#20462;&#25913;&#21644;&#24573;&#35270;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#21151;&#33021;&#24046;&#36317;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#32780;&#29616;&#26377;&#30340;PET&#25216;&#26415;&#65288;&#20363;&#22914;VL-Adapter&#65289;&#24573;&#35270;&#20102;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-and-Language Parameter-Efficient Tuning&#65288;VL-PET&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31890;&#24230;&#25511;&#21046;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#20462;&#25913;&#36827;&#34892;&#26377;&#25928;&#25511;&#21046;&#12290;&#36890;&#36807;&#32771;&#34385;&#30001;&#36825;&#31181;&#26426;&#21046;&#29983;&#25104;&#30340;&#19981;&#21516;&#31890;&#24230;&#25511;&#21046;&#30697;&#38453;&#65292;&#21487;&#20197;&#23454;&#20363;&#21270;&#22810;&#31181;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;VL-PET&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the model size of pre-trained language models (PLMs) grows rapidly, full fine-tuning becomes prohibitively expensive for model training and storage. In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques perform on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09778</link><description>&lt;p&gt;
&#36861;&#27714;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#23454;&#38469;&#30340;&#35270;&#35273;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#35745;&#25968;&#12289;&#25351;&#28041;&#34920;&#36798;&#21644;&#19968;&#33324;&#30340;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65289;&#19978;&#30340;&#34920;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#65292;&#20154;&#20204;&#23581;&#35797;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;Liu, Emerson, and Collier 2022) &#25110;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20294;&#37117;&#34920;&#29616;&#20986;&#24615;&#33021;&#19981;&#20339;&#24182;&#19988;&#19982;&#20154;&#31867;&#24615;&#33021;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#26469;&#23545;&#31354;&#38388;&#20174;&#21477;&#36827;&#34892;&#25490;&#21517;&#24182;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#32467;&#21512;&#21644;&#22320;&#38754;&#21270;&#29289;&#20307;&#23545;&#24212;&#30340;&#21517;&#35789;&#30701;&#35821;&#21644;&#23427;&#20204;&#30340;&#20301;&#32622;&#30340;&#35777;&#25454;&#26469;&#35745;&#31639;&#31354;&#38388;&#20174;&#21477;&#30340;&#26368;&#32456;&#25490;&#21517;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#36873;&#25321;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;YORC&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20379;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#32467;&#26524;&#21644;&#26356;&#39640;&#23618;&#27425;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.09768</link><description>&lt;p&gt;
YORC&#65306;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
YORC: Yoruba Reading Comprehension dataset. (arXiv:2308.09768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#36873;&#25321;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;YORC&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20379;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#32467;&#26524;&#21644;&#26356;&#39640;&#23618;&#27425;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;YORC&#65306;&#19968;&#20010;&#22522;&#20110;&#32422;&#40065;&#24052;&#35821;&#39640;&#20013;&#38405;&#35835;&#29702;&#35299;&#32771;&#35797;&#30340;&#26032;&#30340;&#22810;&#39033;&#36873;&#25321;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24050;&#35757;&#32451;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#26469;&#25552;&#20379;&#22522;&#20934;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25552;&#20379;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we create YORC: a new multi-choice Yoruba Reading Comprehension dataset that is based on Yoruba high-school reading comprehension examination. We provide baseline results by performing cross-lingual transfer using existing English RACE dataset based on a pre-trained encoder-only model. Additionally, we provide results by prompting large language models (LLMs) like GPT-4.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09765</link><description>&lt;p&gt;
&#21463;&#20919;&#33853;: &#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#21453;&#24046;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#29289;&#20307;&#21521;&#37327;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#27969;&#34892;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#65288;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#23545;&#65292;&#24182;&#24573;&#30053;&#20102;&#20174;&#20013;&#25552;&#21462;&#23545;&#35937;&#30340;&#20998;&#24067;&#12290;&#20154;&#31867;&#23545;&#29289;&#20307;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#26174;&#33879;&#21462;&#20915;&#20110;&#23545;&#35937;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#25972;&#20307;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20102;&#20154;&#31867;&#24863;&#30693;&#30340;&#21453;&#24046;&#25928;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#20998;&#25968;&#37327;&#21270;&#20102;&#22312;&#20004;&#20010;&#20803;&#32032;&#20043;&#38388;&#25214;&#21040;&#32473;&#23450;&#30456;&#20284;&#24230;&#30340;&#24778;&#21916;&#65292;&#30456;&#23545;&#20110;&#25104;&#23545;&#30340;&#25972;&#20307;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#24230;&#37327;&#65292;&#36890;&#24120;&#21457;&#29616;&#19982;&#21407;&#22987;&#20313;&#24358;&#30456;&#20284;&#24230;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;10-15\%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#22312;&#20020;&#24202;&#20915;&#31574;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#35774;&#35745;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#24182;&#20197;&#39046;&#22495;&#30693;&#35782;&#20026;&#22522;&#30784;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#21307;&#30103;&#19987;&#23478;&#65292;&#25552;&#21462;&#20851;&#38190;&#35265;&#35299;&#24182;&#36741;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#19968;&#39046;&#22495;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#32467;&#21512;&#22312;&#21019;&#24314;&#26356;&#20855;&#27934;&#23519;&#21147;&#30340;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#30340;&#21160;&#24577;&#65292;&#24182;&#39564;&#35777;&#20102;ChatGPT&#22312;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.09731</link><description>&lt;p&gt;
ChatGPT-HealthPrompt. &#21033;&#29992;ChatGPT&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20013;&#21457;&#25381;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT. (arXiv:2308.09731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#22312;&#20020;&#24202;&#20915;&#31574;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#35774;&#35745;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#24182;&#20197;&#39046;&#22495;&#30693;&#35782;&#20026;&#22522;&#30784;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#21307;&#30103;&#19987;&#23478;&#65292;&#25552;&#21462;&#20851;&#38190;&#35265;&#35299;&#24182;&#36741;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#19968;&#39046;&#22495;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#32467;&#21512;&#22312;&#21019;&#24314;&#26356;&#20855;&#27934;&#23519;&#21147;&#30340;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#30340;&#21160;&#24577;&#65292;&#24182;&#39564;&#35777;&#20102;ChatGPT&#22312;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#20020;&#24202;&#20915;&#31574;&#65292;&#37325;&#28857;&#20851;&#27880;OpenAI&#30340;ChatGPT&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#20351;&#29992;&#65292;&#31574;&#30053;&#24615;&#22320;&#35774;&#35745;&#21253;&#25324;&#20219;&#21153;&#25551;&#36848;&#12289;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#19988;&#20851;&#38190;&#22320;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#20415;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#20174;&#39640;&#24615;&#33021;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#24471;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#25552;&#31034;&#35774;&#35745;&#20013;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#21307;&#30103;&#19987;&#23478;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20851;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20851;&#38190;&#35265;&#35299;&#65292;&#20197;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#12290;&#39046;&#22495;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#22312;&#21019;&#24314;&#26356;&#20855;&#27934;&#23519;&#21147;&#30340;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LLMs&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#30340;&#21160;&#24577;&#12290;&#36890;&#36807;&#27604;&#36739;OpenAI&#30340;ChatGPT&#19982;&#20256;&#32479;&#30340;supervised&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool.  Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#39640;&#25928;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#21152;&#36895;&#25512;&#29702;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#36136;&#37327;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2308.09723</link><description>&lt;p&gt;
FineQuant: &#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#26435;&#37325;&#37327;&#21270;&#20026;LLMs&#35299;&#38145;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs. (arXiv:2308.09723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#39640;&#25928;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#21152;&#36895;&#25512;&#29702;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#36136;&#37327;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#22823;&#37327;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#23545;&#20110;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#30001;&#20110;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#24102;&#23485;&#29942;&#39048;&#23548;&#33268;&#25512;&#29702;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20165;&#22522;&#20110;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;LLMs&#30340;&#20869;&#23384;&#28040;&#32791;&#24182;&#21152;&#36895;&#25512;&#29702;&#12290;&#20026;&#20102;&#30830;&#20445;&#36136;&#37327;&#38477;&#20302;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27169;&#22411;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#30340;Mixture-of-Experts&#65288;MoE&#65289;&#21644;&#23494;&#38598;&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#19982;LLMs&#37327;&#21270;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25214;&#21040;&#26435;&#37325;&#32454;&#31890;&#24230;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved state-of-the-art performance across various language tasks but pose challenges for practical deployment due to their substantial memory requirements. Furthermore, the latest generative models suffer from high inference costs caused by the memory bandwidth bottleneck in the auto-regressive decoding process. To address these issues, we propose an efficient weight-only quantization method that reduces memory consumption and accelerates inference for LLMs. To ensure minimal quality degradation, we introduce a simple and effective heuristic approach that utilizes only the model weights of a pre-trained model. This approach is applicable to both Mixture-of-Experts (MoE) and dense models without requiring additional fine-tuning. To demonstrate the effectiveness of our proposed method, we first analyze the challenges and issues associated with LLM quantization. Subsequently, we present our heuristic approach, which adaptively finds the granularity of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#30340;&#21487;&#20449;LSTM-Autoencoder&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09722</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#20449;&#30340;&#22522;&#20110;LSTM-Autoencoder&#32593;&#32476;&#30340;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#26041;&#27861;&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data. (arXiv:2308.09722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#30340;&#21487;&#20449;LSTM-Autoencoder&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#27450;&#20940;&#23545;&#20154;&#31867;&#29983;&#27963;&#26377;&#23475;&#12290;&#38543;&#30528;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#20167;&#24680;&#35328;&#35770;&#30340;&#25968;&#37327;&#20063;&#22312;&#22686;&#21152;&#12290;&#36825;&#20123;&#21487;&#24597;&#30340;&#20869;&#23481;&#21487;&#33021;&#23548;&#33268;&#25233;&#37057;&#21644;&#19982;&#33258;&#26432;&#26377;&#20851;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#30340;&#21487;&#20449;LSTM-Autoencoder&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#23637;&#31034;&#20102;&#19968;&#31181;&#35299;&#20915;&#25968;&#25454;&#21487;&#29992;&#24615;&#22256;&#38590;&#30340;&#21069;&#27839;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21360;&#22320;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#31561;&#20960;&#31181;&#35821;&#35328;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#38598;&#30340;&#21407;&#22240;&#65292;&#20173;&#28982;&#32570;&#20047;&#36275;&#22815;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#20256;&#32479;&#27169;&#22411;&#65288;&#21253;&#25324;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#65292;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;BiLSTM&#65289;&#65292;LSTM-Autoencoder&#65292;Word2vec&#65292;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#65288;BERT&#65289;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;2&#65288;GPT-2&#65289;&#27169;&#22411;&#65289;&#23545;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#24615;&#30340;&#20405;&#29359;&#35780;&#35770;&#35782;&#21035;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media cyberbullying has a detrimental effect on human life. As online social networking grows daily, the amount of hate speech also increases. Such terrible content can cause depression and actions related to suicide. This paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection on social media using synthetic data. We have demonstrated a cutting-edge method to address data availability difficulties by producing machine-translated data. However, several languages such as Hindi and Bangla still lack adequate investigations due to a lack of datasets. We carried out experimental identification of aggressive comments on Hindi, Bangla, and English datasets using the proposed model and traditional models, including Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models. We employed evaluation m
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2308.09720</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24847;&#24819;&#19981;&#21040;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09720
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#23637;&#31034;&#20986;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#65288;&#39044;&#27979;&#20154;&#31867;&#20070;&#20889;&#25991;&#26412;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#65289;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#30340;&#24615;&#36136;&#21450;&#20854;&#19982;&#20854;&#20182;&#24050;&#30693;&#38388;&#25509;&#36807;&#31243;&#30340;&#20851;&#31995;&#12290;&#25991;&#31456;&#20027;&#24352;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#30340;&#19968;&#20010;&#37325;&#35201;&#21103;&#20316;&#29992;&#26159;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#24320;&#21457;&#30340;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#21487;&#39044;&#27979;&#30340;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#31616;&#35201;&#35752;&#35770;&#20102;&#36825;&#20123;&#31995;&#32479;&#25152;&#33719;&#24471;&#30340;&#35748;&#30693;&#25216;&#33021;&#19982;&#20154;&#31867;&#35748;&#30693;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I discuss the nature of this indirect acquisition process and its relation to other known indirect processes. I argue that an important side effect of such indirect acquisition is the development of integrated abilities. I discuss the extent to which the abilities developed by large language models are predictable. Finally, I briefly discuss the relation between the cognitive skills acquired by these systems and human cognition.
&lt;/p&gt;</description></item><item><title>&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2308.09687</link><description>&lt;p&gt;
&#24819;&#27861;&#22270;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09687
&lt;/p&gt;
&lt;p&gt;
&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#27861;&#22270;&#65288;Graph of Thoughts&#65292;GoT&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;Chain-of-Thought&#25110;Tree of Thoughts&#65288;ToT&#65289;&#31561;&#33539;&#24335;&#12290;GoT&#30340;&#20851;&#38190;&#24605;&#24819;&#21644;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#23558;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#20854;&#20013;&#20449;&#24687;&#21333;&#20803;&#65288;"LLM&#24819;&#27861;"&#65289;&#26159;&#39030;&#28857;&#65292;&#36793;&#34920;&#31034;&#36825;&#20123;&#39030;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#23558;&#20219;&#24847;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#12289;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#25490;&#24207;&#20219;&#21153;&#19978;&#36136;&#37327;&#25552;&#39640;&#20102;62%&#65292;&#21516;&#26102;&#25104;&#26412;&#38477;&#20302;&#20102;&#36229;&#36807;31%&#12290;&#25105;&#20204;&#30830;&#20445;GoT&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#24320;&#21019;&#26032;&#30340;&#25552;&#31034;&#26041;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;&#24418;&#28151;&#21512;&#24605;&#32500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#36339;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09658</link><description>&lt;p&gt;
&#26641;&#24418;&#28151;&#21512;&#24605;&#32500;: &#23558;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#32467;&#21512;&#29992;&#20110;&#22810;&#36339;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning. (arXiv:2308.09658v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;&#24418;&#28151;&#21512;&#24605;&#32500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#36339;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31867;&#20284;&#20195;&#30721;&#30340;&#35745;&#21010;&#65292;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#35270;&#35273;&#25512;&#29702;&#65289;&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36235;&#21183;&#12290;&#36825;&#31181;&#34987;&#31216;&#20026;LLM-based planning&#30340;&#33539;&#24335;&#22312;&#38382;&#39064;&#35299;&#20915;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#31616;&#21333;&#38382;&#39064;&#30340;&#22522;&#26412;&#24773;&#26223;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#30452;&#25509;&#22238;&#31572;&#20986;&#26469;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#25512;&#29702;&#27493;&#39588;&#12290;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#30340;&#35745;&#21010;&#21046;&#35746;&#36824;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#22810;&#36339;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#21644;&#35745;&#21010;&#25628;&#32034;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#21464;&#24471;&#26174;&#33879;&#12290;&#30446;&#21069;&#30340;&#31639;&#27861;&#35201;&#20040;&#36890;&#36807;&#37319;&#29992;&#24555;&#36895;&#19968;&#27425;&#24615;&#29983;&#25104;&#26469;&#35299;&#20915;&#25928;&#29575;&#38382;&#39064;&#65292;&#35201;&#20040;&#37319;&#29992;&#22797;&#26434;&#30340;&#36845;&#20195;&#29983;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20294;&#20004;&#31181;&#26041;&#27861;&#37117;&#26080;&#27861;&#24179;&#34913;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#38656;&#27714;&#12290;&#21463;&#21040;&#20154;&#33041;&#20013;&#30340;&#21452;&#31995;&#32479;&#35748;&#30693;&#65288;&#24555;&#36895;&#24605;&#32771;&#21644;&#24930;&#36895;&#24605;&#32771;&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;&#24418;&#28151;&#21512;&#24605;&#32500;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There emerges a promising trend of using large language models (LLMs) to generate code-like plans for complex inference tasks such as visual reasoning. This paradigm, known as LLM-based planning, provides flexibility in problem solving and endows better interpretability. However, current research is mostly limited to basic scenarios of simple questions that can be straightforward answered in a few inference steps. Planning for the more challenging multi-hop visual reasoning tasks remains under-explored. Specifically, under multi-hop reasoning situations, the trade-off between accuracy and the complexity of plan-searching becomes prominent. The prevailing algorithms either address the efficiency issue by employing the fast one-stop generation or adopt a complex iterative generation method to improve accuracy. Both fail to balance the need for efficiency and performance. Drawing inspiration from the dual system of cognition in the human brain, the fast and the slow think processes, we pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09308</link><description>&lt;p&gt;
&#21487;&#24494;&#26816;&#32034;&#22686;&#24378;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#30340;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentiable Retrieval Augmentation via Generative Language Modeling for E-commerce Query Intent Classification. (arXiv:2308.09308v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#26816;&#32034;&#22120;&#21644;&#22806;&#37096;&#35821;&#26009;&#24211;&#26469;&#22686;&#24378;&#19979;&#28216;&#27169;&#22411;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#37096;&#20998;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20998;&#21035;&#25110;&#24322;&#27493;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#26469;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#19982;&#31471;&#21040;&#31471;&#32852;&#21512;&#35757;&#32451;&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Differentiable Retrieval Augmentation via Generative lANguage modeling&#65288;Dragan&#65289;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#37325;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#30340;&#19968;&#20010;&#26377;&#25361;&#25112;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#28040;&#34701;&#30740;&#31350;&#22343;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#19988;&#21512;&#29702;&#22320;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmentation, which enhances downstream models by a knowledge retriever and an external corpus instead of by merely increasing the number of model parameters, has been successfully applied to many natural language processing (NLP) tasks such as text classification, question answering and so on. However, existing methods that separately or asynchronously train the retriever and downstream model mainly due to the non-differentiability between the two parts, usually lead to degraded performance compared to end-to-end joint training. In this paper, we propose Differentiable Retrieval Augmentation via Generative lANguage modeling(Dragan), to address this problem by a novel differentiable reformulation. We demonstrate the effectiveness of our proposed method on a challenging NLP task in e-commerce search, namely query intent classification. Both the experimental results and ablation study show that the proposed method significantly and reasonably improves the state-of-the-art basel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.08998</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforced Self-Training (ReST) for Language Modeling. (arXiv:2308.08998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064; (RLHF)&#65292;&#21487;&#20197;&#36890;&#36807;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#38271;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064; (RL) &#26469;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784; LLM&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22686;&#24378;&#33258;&#23398;&#20064; (ReST)&#12290;&#32473;&#23450;&#21021;&#22987;&#30340;LLM&#31574;&#30053;&#65292;ReST&#36890;&#36807;&#20174;&#31574;&#30053;&#20013;&#29983;&#25104;&#26679;&#26412;&#26469;&#20135;&#29983;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;LLM&#31574;&#30053;&#12290;ReST&#27604;&#20856;&#22411;&#30340;&#22312;&#32447;RLHF&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#31163;&#32447;&#29983;&#25104;&#30340;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#25968;&#25454;&#12290;&#34429;&#28982;ReST&#26159;&#36866;&#29992;&#20110;&#25152;&#26377;&#29983;&#25104;&#23398;&#20064;&#35774;&#32622;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#21487;&#20197;&#20197;&#35745;&#31639;&#21644;&#37319;&#26679;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#22312;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#19978;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#20998;&#25104;&#23376;&#38598;&#24182;&#32452;&#21512;&#21508;&#23376;&#38598;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08780</link><description>&lt;p&gt;
&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Exploring Demonstration Ensembling for In-context Learning. (arXiv:2308.08780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#20998;&#25104;&#23376;&#38598;&#24182;&#32452;&#21512;&#21508;&#23376;&#38598;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#36890;&#36807;&#21521;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#31034;&#20363;&#26469;&#36827;&#34892;&#25805;&#20316;&#65292;&#21363;&#28436;&#31034;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#23558;&#28436;&#31034;&#19982;&#27979;&#35797;&#36755;&#20837;&#36830;&#25509;&#36215;&#26469;&#25552;&#31034;&#32473;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36830;&#25509;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#27599;&#20010;&#28436;&#31034;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#36129;&#29486;&#12290;&#24403;&#19968;&#20123;&#28436;&#31034;&#19982;&#27979;&#35797;&#31034;&#20363;&#26080;&#20851;&#26102;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#26576;&#20123;&#21464;&#25442;&#22120;&#27169;&#22411;&#23545;&#36755;&#20837;&#38271;&#24230;&#26377;&#38480;&#21046;&#65292;&#23558;&#35768;&#22810;&#31034;&#20363;&#25918;&#20837;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#20219;&#21153;&#26102;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28436;&#31034;&#38598;&#25104;&#65288;DENSE&#65289;&#20316;&#20026;&#31616;&#21333;&#36830;&#25509;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#27169;&#22411;&#20351;&#29992;&#28436;&#31034;&#30340;&#23376;&#38598;&#65288;&#21363;bucket&#65289;&#26469;&#39044;&#27979;&#36755;&#20986;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#23376;&#38598;&#24471;&#21040;&#30340;&#36755;&#20986;&#27010;&#29575;&#32452;&#21512;&#36215;&#26469;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-j&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) operates by showing language models (LMs) examples of input-output pairs for a given task, i.e., demonstrations. The standard approach for ICL is to prompt the LM with concatenated demonstrations followed by the test input. This approach suffers from some issues. First, concatenation offers almost no control over the contribution of each demo to the model prediction. This can be sub-optimal when some demonstrations are irrelevant to the test example. Second, due to the input length limit of some transformer models, it might be infeasible to fit many examples into the context, especially when dealing with long-input tasks. In this work, we explore Demonstration Ensembling (DENSE) as an alternative to simple concatenation. \model predicts outputs using subsets (i.e., buckets) of the demonstrations and then combines the output probabilities resulting from each subset to produce the final prediction. We study different ensembling methods using GPT-j and experiment
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#35777;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65292;&#24182;&#19988;ALPACA&#22312;&#20445;&#30041;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.08747</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. (arXiv:2308.08747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08747
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#35777;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65292;&#24182;&#19988;ALPACA&#22312;&#20445;&#30041;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#29616;&#35937;&#65292;&#24403;&#27169;&#22411;&#23398;&#20064;&#26032;&#20449;&#24687;&#26102;&#65292;&#23427;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#25506;&#31350;LLMs&#22312;&#25345;&#32493;&#24494;&#35843;&#20013;&#26159;&#21542;&#23384;&#22312;CF&#26159;&#24456;&#26377;&#24847;&#20041;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#39046;&#22495;&#30693;&#35782;&#12289;&#25512;&#29702;&#21644;&#38405;&#35835;&#29702;&#35299;&#30340;&#35282;&#24230;&#23545;LLMs&#30340;&#36951;&#24536;&#29616;&#35937;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;1b&#21040;7b&#30340;&#33539;&#22260;&#20869;&#65292;LLMs&#26222;&#36941;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#19988;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;mT0&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;BLOOMZ&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65288;&#22914;&#24615;&#21035;&#20559;&#35265;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;LLAMA&#30456;&#27604;&#65292;ALPACA&#22312;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information as it learns new information. As large language models (LLMs) have shown excellent performance, it is interesting to uncover whether CF exists in the continual fine-tuning of LLMs. In this study, we empirically evaluate the forgetting phenomenon in LLMs' knowledge, from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments demonstrate that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b. Furthermore, as the scale increases, the severity of forgetting also intensifies. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ suffers less forgetting and maintains more knowledge. We also observe that LLMs can mitigate language bias (e.g. gender bias) during continual fine-tuning. Moreover, we find that ALPACA can maintain more knowledge and capacity compared with LLAMA du
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#21477;&#38405;&#35835;&#22120;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20960;&#20046;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04566</link><description>&lt;p&gt;
&#21333;&#21477;&#38405;&#35835;&#22120;&#65306;&#35299;&#20915;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Single-Sentence Reader: A Novel Approach for Addressing Answer Position Bias. (arXiv:2308.04566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#21477;&#38405;&#35835;&#22120;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20960;&#20046;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#27169;&#22411;&#24448;&#24448;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#65288;&#20063;&#31216;&#20026;&#25968;&#25454;&#38598;&#20559;&#24046;&#25110;&#30740;&#31350;&#30028;&#30340;&#26631;&#27880;&#24037;&#20214;&#65289;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#22312;&#19981;&#23436;&#20840;&#29702;&#35299;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;MRC&#20219;&#21153;&#65292;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#20302;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#35757;&#32451;&#38382;&#39064;&#20013;&#26377;&#30456;&#24403;&#27604;&#20363;&#30340;&#31572;&#26696;&#20165;&#20301;&#20110;&#19978;&#19979;&#25991;&#30340;&#31532;&#19968;&#21477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;MRC&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#32467;&#26524;&#20960;&#20046;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21333;&#21477;&#38405;&#35835;&#22120;&#36935;&#21040;&#30340;&#20960;&#20010;&#25361;&#25112;&#21644;&#25552;&#20986;&#30340;&#24212;&#23545;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Reading Comprehension (MRC) models tend to take advantage of spurious correlations (also known as dataset bias or annotation artifacts in the research community). Consequently, these models may perform the MRC task without fully comprehending the given context and question, which is undesirable since it may result in low robustness against distribution shift. This paper delves into the concept of answer-position bias, where a significant percentage of training questions have answers located solely in the first sentence of the context. We propose a Single-Sentence Reader as a new approach for addressing answer position bias in MRC. We implement this approach using six different models and thoroughly analyze their performance. Remarkably, our proposed Single-Sentence Readers achieve results that nearly match those of models trained on conventional training sets, proving their effectiveness. Our study also discusses several challenges our Single-Sentence Readers encounter and prop
&lt;/p&gt;</description></item><item><title>PaniniQA&#26159;&#19968;&#20010;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#24739;&#32773;&#29702;&#35299;&#20182;&#20204;&#30340;&#20986;&#38498;&#25351;&#23548;&#65292;&#25552;&#20379;&#21450;&#26102;&#30340;&#21453;&#39304;&#20197;&#32416;&#27491;&#24739;&#32773;&#30340;&#35823;&#35299;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#20114;&#21160;&#25552;&#39640;&#24739;&#32773;&#23545;&#21307;&#30103;&#25351;&#23548;&#30340;&#25484;&#25569;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.03253</link><description>&lt;p&gt;
PaniniQA: &#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#31572;&#25552;&#21319;&#24739;&#32773;&#25945;&#32946;
&lt;/p&gt;
&lt;p&gt;
PaniniQA: Enhancing Patient Education Through Interactive Question Answering. (arXiv:2308.03253v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03253
&lt;/p&gt;
&lt;p&gt;
PaniniQA&#26159;&#19968;&#20010;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#24739;&#32773;&#29702;&#35299;&#20182;&#20204;&#30340;&#20986;&#38498;&#25351;&#23548;&#65292;&#25552;&#20379;&#21450;&#26102;&#30340;&#21453;&#39304;&#20197;&#32416;&#27491;&#24739;&#32773;&#30340;&#35823;&#35299;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#20114;&#21160;&#25552;&#39640;&#24739;&#32773;&#23545;&#21307;&#30103;&#25351;&#23548;&#30340;&#25484;&#25569;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#38376;&#25143;&#20801;&#35768;&#20986;&#38498;&#24739;&#32773;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#35775;&#38382;&#20182;&#20204;&#30340;&#20010;&#24615;&#21270;&#20986;&#38498;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24739;&#32773;&#24456;&#38590;&#29702;&#35299;&#25110;&#35760;&#20303;&#20182;&#20204;&#30340;&#20986;&#38498;&#25351;&#23548;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PaniniQA&#65292;&#19968;&#20010;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#24739;&#32773;&#29702;&#35299;&#20182;&#20204;&#30340;&#20986;&#38498;&#25351;&#23548;&#12290;PaniniQA&#39318;&#20808;&#20174;&#24739;&#32773;&#30340;&#20986;&#38498;&#25351;&#23548;&#20013;&#35782;&#21035;&#37325;&#35201;&#30340;&#20020;&#24202;&#20869;&#23481;&#65292;&#28982;&#21518;&#25552;&#20986;&#24739;&#32773;&#29305;&#23450;&#30340;&#25945;&#32946;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;PaniniQA&#36824;&#37197;&#22791;&#20102;&#31572;&#26696;&#39564;&#35777;&#21151;&#33021;&#65292;&#21487;&#20197;&#21450;&#26102;&#21453;&#39304;&#26469;&#32416;&#27491;&#24739;&#32773;&#30340;&#35823;&#35299;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PaniniQA&#33021;&#22815;&#36890;&#36807;&#26377;&#25928;&#30340;&#20114;&#21160;&#25552;&#39640;&#24739;&#32773;&#23545;&#21307;&#30103;&#25351;&#23548;&#30340;&#25484;&#25569;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patient portal allows discharged patients to access their personalized discharge instructions in electronic health records (EHRs). However, many patients have difficulty understanding or memorizing their discharge instructions. In this paper, we present PaniniQA, a patient-centric interactive question answering system designed to help patients understand their discharge instructions. PaniniQA first identifies important clinical content from patients' discharge instructions and then formulates patient-specific educational questions. In addition, PaniniQA is also equipped with answer verification functionality to provide timely feedback to correct patients' misunderstandings. Our comprehensive automatic and human evaluation results demonstrate our PaniniQA is capable of improving patients' mastery of their medical instructions through effective interactions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#65306;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#21307;&#30103;&#20256;&#36882;&#21644;&#21457;&#29616;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#24222;&#22823;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#19981;&#21487;&#25193;&#23637;&#30340;&#25163;&#21160;&#22788;&#29702;&#65292;&#35813;&#36807;&#31243;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#32959;&#30244;&#23398;&#20026;&#37325;&#28857;&#39046;&#22495;&#65292;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20010;&#27491;&#22312;&#32654;&#22269;&#19968;&#20010;&#22823;&#22411;&#21307;&#30103;&#32593;&#32476;&#36827;&#34892;&#27979;&#35797;&#37096;&#32626;&#30340;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#31995;&#32479;&#12290;&#21021;&#27493;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65306;&#20808;&#36827;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#21487;&#20197;&#31435;&#21363;&#36830;&#25509;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#30340;&#21512;&#26684;&#26465;&#20214;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#30340;&#21305;&#37197;&#36923;&#36753;&#65288;&#20363;&#22914;&#23884;&#22871;&#30340;AND/OR/NOT&#65289;&#12290;&#34429;&#28982;&#20173;&#19981;&#23436;&#32654;&#65292;LLM&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#24378;&#22522;&#20934;&#32447;&#65292;&#24182;&#21487;&#33021;&#20316;&#20026;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#36827;&#34892;&#20505;&#36873;&#24739;&#32773;-&#35797;&#39564;&#21010;&#20998;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#19968;&#20123;&#24212;&#29992;LLM&#36827;&#34892;&#31471;&#21040;&#31471;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#37325;&#35201;&#22686;&#38271;&#39046;&#22495;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#38480;&#21046;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35299;&#20915;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00946</link><description>&lt;p&gt;
&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Teaching Smaller Language Models To Generalise To Unseen Compositional Questions. (arXiv:2308.00946v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00946
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35299;&#20915;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#19968;&#20010;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#22238;&#31572;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#20986;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#26368;&#22810;93&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#22521;&#20859;&#22810;&#26679;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#20010;&#23494;&#38598;&#30340;&#26816;&#32034;&#31995;&#32479;&#65292;&#26088;&#22312;&#26816;&#32034;&#19968;&#32452;&#35777;&#25454;&#24615;&#30340;&#27573;&#33853;&#29255;&#27573;&#12290;&#22312;&#38382;&#31572;&#26041;&#38754;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#20040;&#36890;&#36807;&#38024;&#23545;&#38750;&#24120;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#23454;&#29616;&#38646;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#26377;&#26102;&#32467;&#21512;&#20449;&#24687;&#26816;&#32034;&#36827;&#34892;&#12290;&#25105;&#20204;&#20851;&#27880;&#36739;&#23569;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#21363;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#23545;&#20110;&#19981;&#23384;&#22312;&#36275;&#22815;&#20449;&#24687;&#26469;&#22238;&#31572;&#29305;&#23450;&#38382;&#39064;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#26816;&#32034;&#26102;&#65292;&#33021;&#21542;&#23454;&#29616;&#38646;&#26679;&#26412;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#20026;&#22810;&#26679;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65288;StrategyQA&#65292;CommonsenseQA&#65292;IIRC&#65292;DROP&#65292;Musique&#21644;ARC-DA&#65289;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We equip a smaller Language Model to generalise to answering challenging compositional questions that have not been seen in training. To do so we propose a combination of multitask supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities, and a dense retrieval system that aims to retrieve a set of evidential paragraph fragments. Recent progress in question-answering has been achieved either through prompting methods against very large pretrained Language Models in zero or few-shot fashion, or by fine-tuning smaller models, sometimes in conjunction with information retrieval. We focus on the less explored question of the extent to which zero-shot generalisation can be enabled in smaller models with retrieval against a corpus within which sufficient information to answer a particular question may not exist. We establish strong baselines in this setting for diverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and ARC-DA), and show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00158</link><description>&lt;p&gt;
&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#30340;&#23436;&#32654;&#36136;&#37327;&#27573;&#33853;&#65306;&#26159;&#21542;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#32534;&#36753;&#36317;&#31163;&#27169;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#65288;TQE&#65289;&#26159;&#23558;&#36755;&#20986;&#32763;&#35793;&#37096;&#32626;&#21040;&#20351;&#29992;&#20013;&#20043;&#21069;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290; TQE&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#30340;&#36136;&#37327;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#21442;&#32771;&#32763;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20026;TQE&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;Fine-Tune&#12290;&#25105;&#20204;&#20197;ChatGPT&#20026;&#20363;&#65292;&#23558;TQE&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;&#33521;&#24847;&#21644;&#33521;&#24503;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;ChatGPT&#30340;API Fine-Tuned&#21487;&#20197;&#22312;&#39044;&#27979;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#21363;&#26159;&#21542;&#38656;&#35201;&#32534;&#36753;&#32763;&#35793;&#65292;&#20294;&#32943;&#23450;&#26377;&#25913;&#36827;&#20934;&#30830;&#24615;&#30340;&#31354;&#38388;&#12290;&#33521;&#24847;&#21452;&#35821;&#25688;&#35201;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.00121</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#65306;AI&#20316;&#20026;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23433;&#20840;&#27979;&#35797;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28183;&#36879;&#27979;&#35797;&#26159;&#19968;&#39033;&#38656;&#35201;&#39640;&#27700;&#24179;&#19987;&#19994;&#30693;&#35782;&#30340;&#27963;&#21160;&#65292;&#24182;&#28041;&#21450;&#35768;&#22810;&#25163;&#21160;&#27979;&#35797;&#21644;&#20998;&#26512;&#27493;&#39588;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29992;&#20363;&#65306;&#29992;&#20110;&#23433;&#20840;&#27979;&#35797;&#20219;&#21153;&#30340;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#22312;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#20013;&#36827;&#34892;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#38381;&#29615;&#21453;&#39304;&#65292;&#23558;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20302;&#32423;&#25805;&#20316;&#19982;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#65288;&#36890;&#36807;SSH&#36830;&#25509;&#65289;&#30456;&#36830;&#65292;&#24182;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#34394;&#25311;&#26426;&#29366;&#24577;&#20197;&#23547;&#25214;&#28431;&#27934;&#65292;&#24182;&#25552;&#20379;&#20855;&#20307;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#36884;&#24452;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#36328;&#39046;&#22495;&#21644;&#36328;&#20154;&#21475;&#30340;&#25991;&#26412;&#20998;&#31867;&#21160;&#24577;&#65292;&#26500;&#24314;&#26356;&#36890;&#29992;&#30340;&#28389;&#29992;&#20998;&#31867;&#22120;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23569;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#23545;&#20110;&#27169;&#22411;&#30340;&#36890;&#29992;&#21270;&#21644;&#36866;&#24212;&#38750;&#24120;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.16811</link><description>&lt;p&gt;
DoDo&#23398;&#20064;: &#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#26816;&#27979;&#38024;&#23545;&#20844;&#20247;&#20154;&#29289;&#30340;&#28389;&#29992;&#30340;&#39046;&#22495;-&#20154;&#21475;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
DoDo Learning: DOmain-DemOgraphic Transfer in Language Models for Detecting Abuse Targeted at Public Figures. (arXiv:2307.16811v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#36328;&#39046;&#22495;&#21644;&#36328;&#20154;&#21475;&#30340;&#25991;&#26412;&#20998;&#31867;&#21160;&#24577;&#65292;&#26500;&#24314;&#26356;&#36890;&#29992;&#30340;&#28389;&#29992;&#20998;&#31867;&#22120;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23569;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#23545;&#20110;&#27169;&#22411;&#30340;&#36890;&#29992;&#21270;&#21644;&#36866;&#24212;&#38750;&#24120;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20247;&#20154;&#29289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#21463;&#21040;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#28389;&#29992;&#65292;&#36825;&#23545;&#20182;&#20204;&#22312;&#20844;&#20247;&#29983;&#27963;&#20013;&#30340;&#31215;&#26497;&#21442;&#19982;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#33258;&#21160;&#21270;&#31995;&#32479;&#21487;&#20197;&#22823;&#35268;&#27169;&#35782;&#21035;&#28389;&#29992;&#65292;&#20294;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#26082;&#26114;&#36149;&#21448;&#22797;&#26434;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#30340;&#39640;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#26159;&#21487;&#21462;&#30340;&#65292;&#21487;&#20197;&#22788;&#29702;&#22312;&#32447;&#28389;&#29992;&#30340;&#20849;&#20139;&#21644;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#25506;&#32034;&#20132;&#21449;&#32676;&#20307;&#25991;&#26412;&#20998;&#31867;&#30340;&#21160;&#24577;&#65292;&#20197;&#20102;&#35299;&#35757;&#32451;&#22312;&#19968;&#20010;&#39046;&#22495;&#25110;&#20154;&#21475;&#32479;&#35745;&#19978;&#30340;&#20998;&#31867;&#22120;&#22312;&#20854;&#20182;&#39046;&#22495;&#25110;&#20154;&#21475;&#32479;&#35745;&#19978;&#30340;&#36716;&#31227;&#24773;&#20917;&#65292;&#20174;&#32780;&#26500;&#24314;&#26356;&#36890;&#29992;&#30340;&#28389;&#29992;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#21019;&#26032;DODO&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;28,000&#20010;&#26631;&#35760;&#26465;&#30446;&#65292;&#22312;&#36328;&#39046;&#22495;&#65288;&#20307;&#32946;&#21644;&#25919;&#27835;&#65289;&#21644;&#36328;&#20154;&#21475;&#65288;&#22899;&#24615;&#21644;&#30007;&#24615;&#65289;&#30340;&#22235;&#20010;&#39046;&#22495;-&#20154;&#21475;&#23545;&#20013;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26469;&#20998;&#31867;&#38024;&#23545;&#20844;&#20247;&#20154;&#29289;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#65288;i&#65289;&#23569;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#23545;&#36890;&#29992;&#21270;&#21644;&#27169;&#22411;&#36866;&#24212;&#38750;&#24120;&#26377;&#30410;&#65307;&#65288;ii&#65289;&#27169;&#22411;&#30340;&#36716;&#31227;&#26356;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public figures receive a disproportionate amount of abuse on social media, impacting their active participation in public life. Automated systems can identify abuse at scale but labelling training data is expensive, complex and potentially harmful. So, it is desirable that systems are efficient and generalisable, handling both shared and specific aspects of online abuse. We explore the dynamics of cross-group text classification in order to understand how well classifiers trained on one domain or demographic can transfer to others, with a view to building more generalisable abuse classifiers. We fine-tune language models to classify tweets targeted at public figures across DOmains (sport and politics) and DemOgraphics (women and men) using our novel DODO dataset, containing 28,000 labelled entries, split equally across four domain-demographic pairs. We find that (i) small amounts of diverse data are hugely beneficial to generalisation and model adaptation; (ii) models transfer more eas
&lt;/p&gt;</description></item><item><title>RLCD&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#33976;&#39311;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#12290;&#22312;&#22810;&#20010;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;RLCD&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.12950</link><description>&lt;p&gt;
RLCD: &#22522;&#20110;&#23545;&#27604;&#33976;&#39311;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment. (arXiv:2307.12950v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12950
&lt;/p&gt;
&lt;p&gt;
RLCD&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#33976;&#39311;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#12290;&#22312;&#22810;&#20010;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;RLCD&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Reinforcement Learning from Contrast Distillation (RLCD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#38656;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21363;&#21487;&#20351;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#30340;&#23545;&#40784;&#12290;RLCD&#20351;&#29992;&#27169;&#25311;&#30340;&#20559;&#22909;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#23545;&#21253;&#21547;&#20102;&#39640;&#36136;&#37327;&#21644;&#20302;&#36136;&#37327;&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#20351;&#29992;&#23545;&#27604;&#30340;&#27491;&#36127;&#25552;&#31034;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#22522;&#30784;&#30340;&#26080;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;RLCD&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#23545;&#40784;&#20219;&#21153;&#65288;&#26080;&#23475;&#24615;&#12289;&#26377;&#29992;&#24615;&#21644;&#25925;&#20107;&#22823;&#32434;&#29983;&#25104;&#65289;&#20197;&#21450;7B&#21644;30B&#27169;&#22411;&#35268;&#27169;&#30340;&#20559;&#22909;&#25968;&#25454;&#27169;&#25311;&#19978;&#65292;&#37117;&#20248;&#20110;RLAIF (Bai&#31561;&#20154;&#65292;2022b)&#21644;&#19978;&#19979;&#25991;&#33976;&#39311; (Huang&#31561;&#20154;&#65292;2022) &#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Reinforcement Learning from Contrast Distillation (RLCD), a method for aligning language models to follow natural language principles without using human feedback. RLCD trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7B and 30B model scales for preference data simulation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;ACL Anthology&#20013;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25552;&#20379;&#20102;&#23545;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#21644;NLP&#39046;&#22495;&#30340;&#20998;&#31867;&#23398;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;NLP&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.10652</link><description>&lt;p&gt;
&#25506;&#35752;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;ACL Anthology&#20013;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25552;&#20379;&#20102;&#23545;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#21644;NLP&#39046;&#22495;&#30340;&#20998;&#31867;&#23398;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;NLP&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20316;&#20026;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#65292;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#20256;&#25773;&#21644;&#24191;&#27867;&#24212;&#29992;&#12290;&#37492;&#20110;&#35813;&#39046;&#22495;&#30740;&#31350;&#24037;&#20316;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#30740;&#31350;&#30028;&#24050;&#23545;&#25968;&#20010;&#19982;NLP&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20173;&#32570;&#23569;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23545;&#24050;&#24314;&#31435;&#30340;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#12289;&#35782;&#21035;&#36235;&#21183;&#24182;&#27010;&#25324;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;ACL Anthology&#20013;&#21253;&#21547;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#32467;&#26524;&#21576;&#29616;&#20102;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#65292;&#20026;NLP&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20998;&#31867;&#23398;&#65292;&#20998;&#26512;&#20102;NLP&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years. Given the increasing amount of research work in this area, several NLP-related approaches have been surveyed in the research community. However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent to this day. Contributing to closing this gap, we have systematically classified and analyzed research papers included in the ACL Anthology. As a result, we present a structured overview of the research landscape, provide a taxonomy of fields-of-study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09702</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;LLM&#24341;&#23548;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#35760;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#20960;&#20046;&#19981;&#22686;&#21152;&#20219;&#20309;&#24320;&#38144;&#65292;&#24182;&#20351;&#24471;&#24341;&#23548;&#29983;&#25104;&#22312;&#23454;&#38469;&#20013;&#21487;&#34892;&#12290;&#22312;&#24320;&#28304;Python&#24211;Outlines&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.07870</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#21270;&#35282;&#24230;&#30340;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07870
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#35748;&#20026;LLMs&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#21472;&#21152;&#12290;LLMs&#34920;&#29616;&#20986;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22522;&#20110;&#20135;&#29983;&#30340;&#35282;&#24230;&#32780;&#25913;&#21464;&#65288;&#19982;&#20154;&#31867;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#36890;&#24120;&#20855;&#26377;&#26356;&#19968;&#33268;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35282;&#24230;&#21487;&#25511;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#12289;VSM&#12289;IPIP&#65289;&#26469;&#30740;&#31350;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#35282;&#24230;&#32780;&#25913;&#21464;&#12290;&#36890;&#36807;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#31034;&#20013;&#65288;&#38544;&#24335;&#25110;&#26174;&#24335;&#65289;&#26263;&#31034;&#20102;&#26576;&#20123;&#20215;&#20540;&#35266;&#26102;&#65292;LLMs&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#26263;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.08107</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;AutoML&#65306;&#24403;&#21069;&#25361;&#25112;&#65292;&#26410;&#26469;&#26426;&#36935;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08107
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#29305;&#21035;&#26159;&#22312;NLP&#39046;&#22495;&#65292;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#32463;&#21382;&#20102;&#19968;&#31995;&#21015;&#31361;&#30772;&#12290;&#25105;&#20204;&#35774;&#24819;&#65292;&#20004;&#20010;&#39046;&#22495;&#36890;&#36807;&#32039;&#23494;&#30340;&#34701;&#21512;&#21487;&#20197;&#24444;&#27492;&#25512;&#21160;&#26497;&#38480;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#28508;&#21147;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#23427;&#20204;&#22914;&#20309;&#20114;&#30456;&#21463;&#30410;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#20174;&#19981;&#21516;&#35282;&#24230;&#22686;&#24378;LLMs&#30340;AutoML&#26041;&#27861;&#30340;&#26426;&#20250;&#20197;&#21450;&#21033;&#29992;AutoML&#36827;&#19968;&#27493;&#25913;&#36827;LLMs&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#21487;&#33021;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#12290;&#36890;&#36807;&#24378;&#35843;&#21487;&#24819;&#35937;&#30340;&#21327;&#21516;&#20316;&#29992;&#21644;&#39118;&#38505;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#22312;&#20132;&#21449;&#28857;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#35328;&#21464;&#21270;&#20013;&#30340;&#36873;&#25321;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#38752;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#21382;&#21490;&#35821;&#35328;&#21464;&#21270;&#30340;&#29305;&#23450;&#23454;&#20363;&#20013;&#30340;&#36873;&#25321;&#24378;&#24230;&#12290;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#27604;&#20197;&#21069;&#24212;&#29992;&#36807;&#30340;&#26041;&#27861;&#26356;&#21487;&#38752;&#12290;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#35821;&#38899;&#31616;&#21333;&#24615;&#20248;&#20808;&#20110;&#35821;&#27861;&#31616;&#21333;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#26816;&#27979;&#36873;&#25321;&#24378;&#24230;&#21464;&#21270;&#30340;&#26102;&#38388;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.15914</link><description>&lt;p&gt;
&#35821;&#35328;&#21464;&#21270;&#20013;&#36873;&#25321;&#26426;&#21046;&#30340;&#21487;&#38752;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Reliable identification of selection mechanisms in language change. (arXiv:2305.15914v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#35328;&#21464;&#21270;&#20013;&#30340;&#36873;&#25321;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#38752;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#21382;&#21490;&#35821;&#35328;&#21464;&#21270;&#30340;&#29305;&#23450;&#23454;&#20363;&#20013;&#30340;&#36873;&#25321;&#24378;&#24230;&#12290;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#27604;&#20197;&#21069;&#24212;&#29992;&#36807;&#30340;&#26041;&#27861;&#26356;&#21487;&#38752;&#12290;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#35821;&#38899;&#31616;&#21333;&#24615;&#20248;&#20808;&#20110;&#35821;&#27861;&#31616;&#21333;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#26816;&#27979;&#36873;&#25321;&#24378;&#24230;&#21464;&#21270;&#30340;&#26102;&#38388;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#21464;&#21270;&#26159;&#19968;&#31181;&#25991;&#21270;&#36827;&#21270;&#36807;&#31243;&#65292;&#20854;&#20013;&#35821;&#35328;&#21464;&#37327;&#30340;&#21464;&#24322;&#36890;&#36807;&#31867;&#20284;&#20110;&#31361;&#21464;&#12289;&#36873;&#25321;&#21644;&#36951;&#20256;&#28418;&#21464;&#30340;&#36807;&#31243;&#32780;&#39057;&#32321;&#21464;&#21270;&#12290;&#26412;&#25991;&#24212;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#19968;&#31181;&#26041;&#27861;&#26469;&#23545;&#35821;&#26009;&#24211;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#21382;&#21490;&#35821;&#35328;&#21464;&#21270;&#30340;&#29305;&#23450;&#23454;&#20363;&#20013;&#30340;&#36873;&#25321;&#24378;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#33521;&#35821;&#19981;&#35268;&#21017;&#21160;&#35789;&#30340;&#35821;&#22659;&#19979;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#27604;&#20197;&#21069;&#24212;&#29992;&#36807;&#30340;&#31867;&#20284;&#26041;&#27861;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#39033;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22312;&#35821;&#38899;&#31616;&#21333;&#24615;&#19982;&#35821;&#27861;&#31616;&#21333;&#24615;&#20914;&#31361;&#26102;&#65292;&#23545;&#35821;&#38899;&#31616;&#21333;&#24615;&#30340;&#20559;&#22909;&#20248;&#20808;&#20110;&#23545;&#35821;&#27861;&#31616;&#21333;&#24615;&#30340;&#20559;&#22909;&#12290;&#26368;&#21518;&#65292;&#38024;&#23545;&#35199;&#29677;&#29273;&#30340;&#25340;&#20889;&#25913;&#38761;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#26816;&#27979;&#36873;&#25321;&#24378;&#24230;&#21464;&#21270;&#30340;&#26102;&#38388;&#28857;&#65292;&#36825;&#26159;&#31038;&#20250;&#21160;&#26426;&#35821;&#35328;&#21464;&#21270;&#36890;&#24120;&#20855;&#26377;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#32467;&#26524;&#20849;&#21516;&#34920;&#26126;&#22914;&#20309;&#27979;&#35797;&#35821;&#35328;&#21464;&#21270;&#26426;&#21046;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language change is a cultural evolutionary process in which variants of linguistic variables change in frequency through processes analogous to mutation, selection and genetic drift. In this work, we apply a recently-introduced method to corpus data to quantify the strength of selection in specific instances of historical language change. We first demonstrate, in the context of English irregular verbs, that this method is more reliable and interpretable than similar methods that have previously been applied. We further extend this study to demonstrate that a bias towards phonological simplicity overrides that favouring grammatical simplicity when these are in conflict. Finally, with reference to Spanish spelling reforms, we show that the method can also detect points in time at which selection strengths change, a feature that is generically expected for socially-motivated language change. Together, these results indicate how hypotheses for mechanisms of language change can be tested qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#23545;&#35805;&#29983;&#25104;&#25991;&#26412;&#20013;&#38477;&#20302;&#35828;&#35805;&#32773;&#21517;&#31216;&#25935;&#24863;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#25935;&#24863;&#24230;&#24182;&#20840;&#38754;&#35780;&#20272;&#24050;&#30693;&#26041;&#27861;&#65292;&#24471;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#30340;&#33391;&#22909;&#34920;&#29616;&#65292;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.13833</link><description>&lt;p&gt;
&#20174;&#23545;&#35805;&#29983;&#25104;&#25991;&#26412;&#20013;&#38477;&#20302;&#35828;&#35805;&#32773;&#21517;&#31216;&#25935;&#24863;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reducing Sensitivity on Speaker Names for Text Generation from Dialogues. (arXiv:2305.13833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#23545;&#35805;&#29983;&#25104;&#25991;&#26412;&#20013;&#38477;&#20302;&#35828;&#35805;&#32773;&#21517;&#31216;&#25935;&#24863;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#25935;&#24863;&#24230;&#24182;&#20840;&#38754;&#35780;&#20272;&#24050;&#30693;&#26041;&#27861;&#65292;&#24471;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#30340;&#33391;&#22909;&#34920;&#29616;&#65292;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#22987;&#32456;&#20445;&#25345;&#35828;&#35805;&#32773;&#21517;&#31216;&#30340;&#19968;&#33268;&#24615;&#19981;&#24212;&#35813;&#24433;&#21709;&#21040;&#20854;&#21547;&#20041;&#20197;&#21450;&#23545;&#35805;&#29983;&#25104;&#30340;&#30456;&#24212;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#22788;&#29702;&#20219;&#21153;&#30340;&#20027;&#24178;&#24050;&#32463;&#26174;&#31034;&#20986;&#23545;&#24494;&#22937;&#20043;&#22788;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#20844;&#24179;&#12290;&#36807;&#21435;&#27809;&#26377;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#23545;&#35828;&#35805;&#32773;&#21517;&#31216;&#30340;&#25935;&#24863;&#24230;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#35768;&#22810;&#24050;&#30693;&#30340;&#20943;&#23569;&#35828;&#35805;&#32773;&#21517;&#31216;&#25935;&#24863;&#24230;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25105;&#20204;&#33258;&#24049;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25935;&#24863;&#24230;&#38477;&#20302;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changing speaker names consistently throughout a dialogue should not affect its meaning and corresponding outputs for text generation from dialogues. However, pre-trained language models, serving as the backbone for dialogue-processing tasks, have shown to be sensitive to nuances. This may result in unfairness in real-world applications. No comprehensive analysis of this problem has been done in the past. In this work, we propose to quantitatively measure a model's sensitivity on speaker names, and comprehensively evaluate a number of known methods for reducing speaker name sensitivity, including a novel approach of our own. Extensive experiments on multiple datasets provide a benchmark for this problem and show the favorable performance of our approach in sensitivity reduction and quality of generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09438</link><description>&lt;p&gt;
MPI-rical&#65306;&#22522;&#20110;Transformer&#30340;&#25968;&#25454;&#39537;&#21160;MPI&#20998;&#24067;&#24335;&#24182;&#34892;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#65292;&#23558;&#20018;&#34892;&#20195;&#30721;&#33258;&#21160;&#24182;&#34892;&#21270;&#20197;&#25903;&#25345;&#20849;&#20139;&#20869;&#23384;&#21644;&#20998;&#24067;&#24335;&#20869;&#23384;&#31995;&#32479;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#35768;&#22810;&#23581;&#35797;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#20849;&#20139;&#20869;&#23384;&#29615;&#22659;&#30340;&#24182;&#34892;&#20195;&#30721;&#65288;&#36890;&#24120;&#20351;&#29992;OpenMP&#65289;&#65292;&#20294;&#27809;&#26377;&#20219;&#20309;&#19968;&#39033;&#23581;&#35797;&#25104;&#21151;&#23558;&#20854;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#20869;&#23384;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MPI-rical&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;Transformer&#27169;&#22411;&#23545;&#22823;&#32422;25,000&#20010;&#20018;&#34892;&#20195;&#30721;&#29255;&#27573;&#21450;&#20854;&#23545;&#24212;&#30340;&#24182;&#34892;MPI&#20195;&#30721;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65288;MPICodeCorpus&#65289;&#30340;50,000&#22810;&#20010;&#20195;&#30721;&#29255;&#27573;&#20013;&#29983;&#25104;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#20195;&#30721;&#32763;&#35793;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#21046;&#23450;&#20004;&#20010;&#30740;&#31350;&#30446;&#26631;&#65306;&#20195;&#30721;&#34917;&#20840;&#65292;&#21363;&#22312;&#32473;&#23450;&#28304;&#20195;&#30721;&#20013;&#30340;&#26576;&#20010;&#20301;&#32622;&#65292;&#39044;&#27979;&#35813;&#20301;&#32622;&#30340;MPI&#20989;&#25968;&#65307;&#20195;&#30721;&#32763;&#35793;&#65292;&#21363;&#39044;&#27979;&#19968;&#20010;MPI&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic source-to-source parallelization of serial code for shared and distributed memory systems is a challenging task in high-performance computing. While many attempts were made to translate serial code into parallel code for a shared memory environment (usually using OpenMP), none has managed to do so for a distributed memory environment. In this paper, we propose a novel approach, called MPI-rical, for automated MPI code generation using a transformer-based model trained on approximately 25,000 serial code snippets and their corresponding parallelized MPI code out of more than 50,000 code snippets in our corpus (MPICodeCorpus). To evaluate the performance of the model, we first break down the serial code to MPI-based parallel code translation problem into two sub-problems and develop two research objectives: code completion defined as given a location in the source code, predict the MPI function for that location, and code translation defined as predicting an MPI function as wel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.00948</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20998;&#26512;LLM&#30340;&#29702;&#35770;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs. (arXiv:2305.00948v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#26368;&#36817;&#24050;&#32463;&#25552;&#39640;&#21040;&#20102;&#33021;&#22815;&#22312;&#35768;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#30340;&#24418;&#24335;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#20803;&#35821;&#35328;&#33021;&#21147;&#20998;&#26512;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;LLMs&#20027;&#35201;&#26159;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#30340;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65307;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#25913;&#36827;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;&#35821;&#35328;&#23398;&#20013;&#30340;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#27880;&#20110;&#24418;&#24335;&#35821;&#35328;&#23398;&#30340;&#19977;&#20010;&#23376;&#39046;&#22495;&#65306;&#21477;&#27861;&#12289;&#38899;&#38901;&#23398;&#21644;&#35821;&#20041;&#23398;&#65292;&#25506;&#31350;&#20102;GPT-4&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20803;&#35821;&#35328;&#20998;&#26512;&#30340;&#30740;&#31350;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#23454;&#39564;&#35774;&#35745;&#65292;&#25552;&#20379;&#20102;&#19968;&#33324;&#25351;&#23548;&#26041;&#38024;&#65292;&#35752;&#35770;&#20102;&#38480;&#21046;&#65292;&#24182;&#20026;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#36825;&#20010;&#30740;&#31350;&#36824;&#26377;&#21161;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#21644;&#29702;&#35770;&#27169;&#22411;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of large language models (LLMs) has recently improved to the point where the models can perform well on many language tasks. We show here that for the first time, the models can also generate coherent and valid formal analyses of linguistic data and illustrate the vast potential of large language models for analyses of their metalinguistic abilities. LLMs are primarily trained on language data in the form of text; analyzing and evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. In this paper, we probe into GPT-4's metalinguistic capabilities by focusing on three subfields of formal linguistics: syntax, phonology, and semantics. We outline a research program for metalinguistic analyses of large language models, propose experimental designs, provide general guidelines, discuss limitations, and offer future directions for this line of research. This line of inquiry als
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#25104;&#32489;&#26174;&#31034;&#20986;&#23427;&#22312;&#21307;&#23398;&#32771;&#35797;&#19978;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#21478;&#22806;&#65292;ChatGPT-4 &#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#39592;&#39612;&#21644;&#36719;&#32452;&#32455;&#20197;&#21450;&#22919;&#31185;&#26041;&#38754;&#26377;&#24453;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.11957</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT-4&#30340;ACR&#25918;&#23556;&#32959;&#30244;&#20869;&#31185;&#65288;TXIT&#65289;&#32771;&#35797;&#21644;Red Journal Gray Zone&#26696;&#20363;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;AI&#36741;&#21161;&#21307;&#23398;&#25945;&#32946;&#21644;&#25918;&#23556;&#32959;&#30244;&#27835;&#30103;&#20915;&#31574;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology. (arXiv:2304.11957v2 [physics.med-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#25104;&#32489;&#26174;&#31034;&#20986;&#23427;&#22312;&#21307;&#23398;&#32771;&#35797;&#19978;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#21478;&#22806;&#65292;ChatGPT-4 &#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#39592;&#39612;&#21644;&#36719;&#32452;&#32455;&#20197;&#21450;&#22919;&#31185;&#26041;&#38754;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#19978;&#30340;&#25945;&#32946;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#28508;&#21147;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#32654;&#22269;&#21307;&#23398;&#35768;&#21487;&#32771;&#35797;&#65288;USMLE&#65289;&#21644;MedQA&#32771;&#35797;&#31561;&#21307;&#23398;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#19987;&#19994;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#20102;&#31532;38&#23626;&#32654;&#22269;&#25918;&#23556;&#23398;&#38498;&#65288;ACR&#65289;&#25918;&#23556;&#32959;&#30244;&#20869;&#31185;&#65288;TXIT&#65289;&#32771;&#35797;&#21644;2022&#24180;&#30340;Red Journal Gray Zone&#26696;&#20363;&#12290;&#22522;&#20110;TXIT&#32771;&#35797;&#65292;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ACR&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#39592;&#39612;&#21644;&#36719;&#32452;&#32455;&#20197;&#21450;&#22919;&#31185;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#20020;&#24202;&#36335;&#24452;&#26041;&#38754;&#65292;ChatGPT-4&#22312;2022&#24180;&#30340;Red Journal Gray Zone&#26696;&#20363;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#20855;&#26377;70.65&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT-4&#65289;&#36827;&#34892;&#25918;&#23556;&#32959;&#30244;&#23398;AI&#36741;&#21161;&#21307;&#23398;&#25945;&#32946;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of large language models in medicine for education and decision making purposes has been demonstrated as they achieve decent scores on medical exams such as the United States Medical Licensing Exam (USMLE) and the MedQA exam. In this work, we evaluate the performance of ChatGPT-4 in the specialized field of radiation oncology using the 38th American College of Radiology (ACR) radiation oncology in-training (TXIT) exam and the 2022 red journal gray zone cases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of 63.65% and 74.57%, respectively, highlighting the advantage of the latest ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in radiation oncology are identified to some extent. Specifically, ChatGPT-4 demonstrates good knowledge of statistics, CNS &amp; eye, pediatrics, biology, and physics but has limitations in bone &amp; soft tissue and gynecology, as per the ACR knowledge domain. Regarding clinical care paths, ChatGPT-4 perf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.09991</link><description>&lt;p&gt;
&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#23457;&#35745;LLM&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#37096;&#32626;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#21644;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;&#20998;&#31867;&#36824;&#26159;&#29983;&#25104;&#65292;&#37117;&#34920;&#29616;&#20986;&#26377;&#20559;&#24046;&#21644;&#19981;&#36127;&#36131;&#20219;&#30340;&#34892;&#20026;&#65292;&#23545;&#20154;&#31867;&#36896;&#25104;&#20102;&#35268;&#27169;&#24615;&#30340;&#20260;&#23475;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#23457;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23457;&#35745;&#24037;&#20855;&#21033;&#29992;&#20154;&#21644;&#25110;AI&#26469;&#21457;&#29616;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#30340;&#25991;&#29486;&#65292;&#24182;&#37319;&#35775;&#20102;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#19987;&#23478;&#65292;&#20197;&#22686;&#24378;&#23457;&#35745;&#24037;&#20855;&#8220;AdaTest&#8221;&#65288;Ribeiro&#21644;Lundberg&#65292;2022&#65289;&#65292;&#35813;&#24037;&#20855;&#30001;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#12290;&#36890;&#36807;&#35774;&#35745;&#36807;&#31243;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24863;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#22312;&#21327;&#20316;&#23457;&#35745;&#20013;&#21033;&#29992;&#20154;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#22686;&#24378;&#24037;&#20855;AdaTest ++&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#20351;&#21442;&#19982;&#32773;&#36827;&#34892;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants audit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;(ZS-CIR)&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;CLIP&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20266;&#35789;&#26631;&#35760;&#24182;&#19982;&#30456;&#23545;&#26631;&#39064;&#38598;&#25104;&#65292;&#35299;&#20915;&#20102;&#19981;&#38656;&#35201;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#12290;&#24341;&#20837;&#20102;&#21517;&#20026;CIRCO&#30340;&#24320;&#25918;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#27599;&#20010;&#26597;&#35810;&#30340;&#22810;&#20010;&#30495;&#23454;&#31572;&#26696;&#30340;CIR&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEARLE&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15247</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#19982;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Composed Image Retrieval with Textual Inversion. (arXiv:2303.15247v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15247
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;(ZS-CIR)&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;CLIP&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20266;&#35789;&#26631;&#35760;&#24182;&#19982;&#30456;&#23545;&#26631;&#39064;&#38598;&#25104;&#65292;&#35299;&#20915;&#20102;&#19981;&#38656;&#35201;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#12290;&#24341;&#20837;&#20102;&#21517;&#20026;CIRCO&#30340;&#24320;&#25918;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#27599;&#20010;&#26597;&#35810;&#30340;&#22810;&#20010;&#30495;&#23454;&#31572;&#26696;&#30340;CIR&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEARLE&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;(CIR)&#26088;&#22312;&#26681;&#25454;&#30001;&#21442;&#32771;&#22270;&#20687;&#21644;&#25551;&#36848;&#20004;&#20010;&#22270;&#20687;&#20043;&#38388;&#24046;&#24322;&#30340;&#30456;&#23545;&#26631;&#39064;&#32452;&#25104;&#30340;&#26597;&#35810;&#26469;&#26816;&#32034;&#30446;&#26631;&#22270;&#20687;&#12290;&#26631;&#35760;CIR&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#39640;&#24037;&#20316;&#37327;&#21644;&#25104;&#26412;&#38459;&#30861;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#38646;&#26679;&#26412;CIR (ZS-CIR)&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;CIR&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#38646;&#26679;&#26412;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#19982;&#25991;&#26412;&#21453;&#36716;(SEARLE)&#65292;&#23427;&#23558;&#21442;&#32771;&#22270;&#20687;&#30340;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;CLIP&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20266;&#35789;&#26631;&#35760;&#65292;&#24182;&#23558;&#20854;&#19982;&#30456;&#23545;&#26631;&#39064;&#38598;&#25104;&#12290;&#20026;&#20102;&#25903;&#25345;ZS-CIR&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20849;&#21516;&#23545;&#35937;&#29615;&#22659;&#20013;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#8221;(CIRCO)&#30340;&#24320;&#25918;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#27599;&#20010;&#26597;&#35810;&#30340;&#22810;&#20010;&#30495;&#23454;&#31572;&#26696;&#30340;CIR&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SEARLE&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Composed Image Retrieval (CIR) aims to retrieve a target image based on a query composed of a reference image and a relative caption that describes the difference between the two images. The high effort and cost required for labeling datasets for CIR hamper the widespread usage of existing methods, as they rely on supervised learning. In this work, we propose a new task, Zero-Shot CIR (ZS-CIR), that aims to address CIR without requiring a labeled training dataset. Our approach, named zero-Shot composEd imAge Retrieval with textuaL invErsion (SEARLE), maps the visual features of the reference image into a pseudo-word token in CLIP token embedding space and integrates it with the relative caption. To support research on ZS-CIR, we introduce an open-domain benchmarking dataset named Composed Image Retrieval on Common Objects in context (CIRCO), which is the first dataset for CIR containing multiple ground truths for each query. The experiments show that SEARLE exhibits better performance 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;ICL-D3IE&#65292;&#36825;&#20010;&#26694;&#26550;&#20351;LLM&#22312;&#19981;&#21516;&#31867;&#22411;&#28436;&#31034;&#19979;&#30340;DIE&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#25913;&#36827;&#24615;&#33021;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#20301;&#32622;&#21644;&#26684;&#24335;&#26041;&#38754;&#30340;&#28436;&#31034;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.05063</link><description>&lt;p&gt;
ICL-D3IE&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;+&#22810;&#26679;&#23637;&#31034;&#26356;&#26032;&#65292;&#29992;&#20110;&#25991;&#26723;&#20449;&#24687;&#25277;&#21462;&#65288;arXiv:2303.05063v2 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction. (arXiv:2303.05063v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05063
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;ICL-D3IE&#65292;&#36825;&#20010;&#26694;&#26550;&#20351;LLM&#22312;&#19981;&#21516;&#31867;&#22411;&#28436;&#31034;&#19979;&#30340;DIE&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#25913;&#36827;&#24615;&#33021;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#20301;&#32622;&#21644;&#26684;&#24335;&#26041;&#38754;&#30340;&#28436;&#31034;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-3&#21644;ChatGPT&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#23588;&#20854;&#26159;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21363;&#22522;&#20110;&#23569;&#37327;&#28436;&#31034;&#31034;&#20363;&#36827;&#34892;&#25512;&#29702;&#12290;&#23613;&#31649;&#22312;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#20197;&#35780;&#20272;LLM&#22312;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#25991;&#26723;&#20449;&#24687;&#25277;&#21462;&#65288;DIE&#65289;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;LLM&#25191;&#34892;DIE&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65306;&#27169;&#24577;&#21644;&#20219;&#21153;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;ICL-D3IE&#65292;&#23427;&#20351;LLM&#33021;&#22815;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#28436;&#31034;&#31034;&#20363;&#25191;&#34892;DIE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#38590;&#20197;&#35757;&#32451;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#26368;&#22256;&#38590;&#21644;&#26368;&#19981;&#21516;&#30340;&#29255;&#27573;&#20316;&#20026;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#20415;&#21463;&#30410;&#20110;&#25152;&#26377;&#27979;&#35797;&#23454;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25551;&#36848;&#20851;&#31995;&#30340;&#28436;&#31034;&#31034;&#20363;&#65292;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#20301;&#32622;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26684;&#24335;&#21270;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#26041;&#20415;&#25552;&#21462;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21453;&#39304;&#26426;&#21046;&#65292;&#26356;&#26032;&#20102;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ICL-D3IE&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated remarkable results in various natural language processing (NLP) tasks with in-context learning, which involves inference based on a few demonstration examples. Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning. Applying LLMs to DIE poses two challenges: the modality and task gap. To this end, we propose a simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples. Specifically, we extract the most difficult and distinct segments from hard training documents as hard demonstrations for benefiting all test instances. We design demonstrations describing relationships that enable LLMs to understand positional relationships. We introduce formatting demonstrations for easy answer extraction. Additionally
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2302.09582</link><description>&lt;p&gt;
&#35821;&#35328;&#29305;&#23450;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#23545;&#24773;&#32490;&#25512;&#26029;&#30340;&#22240;&#26524;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#32490;&#31185;&#23398;&#20013;&#65292;&#22914;&#20309;&#29702;&#35299;&#35821;&#35328;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#20173;&#28982;&#26159;&#19968;&#20010;&#20105;&#35758;&#30340;&#35805;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#35843;&#26597;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#20351;&#29992;&#25552;&#31034;&#25216;&#26415;&#65292;&#21457;&#29616;&#20102;14&#20010;&#24773;&#32490;&#27010;&#24565;&#30340;&#23646;&#24615;&#30001;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#32676;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#23646;&#24615;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#19982;&#38543;&#26426;&#25805;&#32437;&#30456;&#27604;&#65292;&#22823;&#22810;&#25968;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#34920;&#29616;&#20986;&#29616;&#20102;&#19979;&#38477;&#12290;&#23646;&#24615;&#29305;&#23450;&#30340;&#34920;&#29616;&#19979;&#38477;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#30340;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#24378;&#35843;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;AraGPT-2&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;Euclidean&#12289;cosine&#12289;Jaccard&#21644;BLEU&#36317;&#31163;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2212.13939</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#25913;&#36827;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification. (arXiv:2212.13939v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;AraGPT-2&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;Euclidean&#12289;cosine&#12289;Jaccard&#21644;BLEU&#36317;&#31163;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#20805;&#36275;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20805;&#36275;&#24615;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#25506;&#32034;&#20102;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;DA&#36890;&#36807;&#23545;&#29616;&#26377;&#25968;&#25454;&#24212;&#29992;&#36716;&#25442;&#26469;&#29983;&#25104;&#26032;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#20174;&#32780;&#22686;&#21152;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#21464;&#21270;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#38463;&#25289;&#20271;&#35821;&#30340;DA&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#22914;&#37322;&#20041;&#25110;&#22522;&#20110;&#22122;&#22768;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38463;&#25289;&#20271;&#35821;DA&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#26368;&#36817;&#24378;&#22823;&#30340;&#24314;&#27169;&#25216;&#26415;AraGPT-2&#26469;&#36827;&#34892;&#22686;&#24378;&#36807;&#31243;&#12290;&#21033;&#29992;&#27431;&#27663;&#36317;&#31163;&#12289;&#20313;&#24358;&#36317;&#31163;&#12289;Jaccard&#36317;&#31163;&#21644;BLEU&#36317;&#31163;&#23545;&#29983;&#25104;&#30340;&#21477;&#23376;&#36827;&#34892;&#20102;&#19978;&#19979;&#25991;&#12289;&#35821;&#20041;&#12289;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#30340;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;AraBERT transformer&#23545;&#24773;&#24863;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of learning models heavily relies on the availability and adequacy of training data. To address the dataset adequacy issue, researchers have extensively explored data augmentation (DA) as a promising approach. DA generates new data instances through transformations applied to the available data, thereby increasing dataset size and variability. This approach has enhanced model performance and accuracy, particularly in addressing class imbalance problems in classification tasks. However, few studies have explored DA for the Arabic language, relying on traditional approaches such as paraphrasing or noising-based techniques. In this paper, we propose a new Arabic DA method that employs the recent powerful modeling technique, namely the AraGPT-2, for the augmentation process. The generated sentences are evaluated in terms of context, semantics, diversity, and novelty using the Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT transformer is used on sentime
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23569;&#26679;&#26412;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#29983;&#25104;&#22120;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#23454;&#29616;&#19982;&#23436;&#25972;&#24494;&#35843;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24615;&#33021;&#35201;&#27714;&#20184;&#20986;&#26174;&#33879;&#30340;&#31995;&#32479;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.00192</link><description>&lt;p&gt;
&#36808;&#21521;&#23454;&#29992;&#30340;&#23569;&#26679;&#26412;&#32852;&#37030;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Few-shot Federated NLP. (arXiv:2212.00192v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23569;&#26679;&#26412;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#29983;&#25104;&#22120;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#23454;&#29616;&#19982;&#23436;&#25972;&#24494;&#35843;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24615;&#33021;&#35201;&#27714;&#20184;&#20986;&#26174;&#33879;&#30340;&#31995;&#32479;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#20027;&#27969;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#23454;&#38469;&#19978;&#65292;&#31169;&#26377;&#25968;&#25454;&#36890;&#24120;&#20998;&#24067;&#22312;&#24322;&#26500;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#65292;&#24182;&#19988;&#21487;&#33021;&#34987;&#31105;&#27490;&#19978;&#20256;&#12290;&#27492;&#22806;&#65292;&#31934;&#24515;&#31574;&#21010;&#30340;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#24456;&#31232;&#32570;&#65292;&#36825;&#22686;&#21152;&#20102;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#27169;&#25311;&#20102;&#29616;&#23454;&#24773;&#20917;&#19979;&#31232;&#32570;&#26631;&#35760;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#20559;&#26012;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUG-FedPrompt&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26377;&#38480;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#65292;AUG-FedPrompt&#33021;&#22815;&#19982;&#23436;&#25972;&#24494;&#35843;&#30456;&#23218;&#32654;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31454;&#20105;&#24615;&#33021;&#26159;&#20197;&#26174;&#33879;&#30340;&#31995;&#32479;&#25104;&#26412;&#20026;&#20195;&#20215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained models have emerged as the predominant solution for natural language processing (NLP). Fine-tuning such pre-trained models for downstream tasks often requires a considerable amount of labeled private data. In practice, private data is often distributed across heterogeneous mobile devices and may be prohibited from being uploaded. Moreover, well-curated labeled data is often scarce, presenting an additional challenge. To address these challenges, we first introduce a data generator for federated few-shot learning tasks, which encompasses the quantity and skewness of scarce labeled data in a realistic setting. Subsequently, we propose AUG-FedPrompt, a prompt-based federated learning system that exploits abundant unlabeled data for data augmentation. Our experiments indicate that AUG-FedPrompt can perform on par with full-set fine-tuning with a limited amount of labeled data. However, such competitive performance comes at a significant system cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20165;&#20351;&#29992;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#27809;&#26377;&#23545;&#35270;&#35273;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22235;&#39033;&#20195;&#34920;&#24615;&#35270;&#35273;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09778</link><description>&lt;p&gt;
&#25105;&#31455;&#28982;&#27809;&#26377;&#22270;&#29255;&#20102;&#65281;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#23398;&#20064;&#35270;&#35273;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
I Can't Believe There's No Images! Learning Visual Tasks Using only Language Data. (arXiv:2211.09778v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20165;&#20351;&#29992;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#27809;&#26377;&#23545;&#35270;&#35273;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22235;&#39033;&#20195;&#34920;&#24615;&#35270;&#35273;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#25152;&#38656;&#30340;&#35768;&#22810;&#39640;&#32423;&#25216;&#33021;&#65292;&#20363;&#22914;&#35299;&#26512;&#38382;&#39064;&#12289;&#27604;&#36739;&#21644;&#23545;&#27604;&#35821;&#20041;&#20197;&#21450;&#32534;&#20889;&#25551;&#36848;&#65292;&#20063;&#21516;&#26679;&#36866;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#65292;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#33021;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#20123;&#25216;&#33021;&#65292;&#28982;&#21518;&#22312;&#27809;&#26377;&#23545;&#35270;&#35273;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#23427;&#20204;&#36716;&#31227;&#21040;&#35270;&#35273;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#21033;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#36341;&#20013;&#65292;&#23545;&#27604;&#27169;&#22411;&#20013;&#19981;&#21516;&#27169;&#24577;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#31995;&#32479;&#24615;&#24046;&#24322;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#32531;&#35299;&#27492;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#20165; &#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#22312;&#22235;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#19978;&#29983;&#25104;&#27169;&#22411;&#65306;&#22270;&#20687;&#23383;&#24149;&#12289;&#35270;&#35273;&#34164;&#21547;&#12289;&#35270;&#35273;&#38382;&#31572;&#21644;&#35270;&#35273;&#26032;&#38395;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#23545;&#26631;&#20934;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many high-level skills that are required for computer vision tasks, such as parsing questions, comparing and contrasting semantics, and writing descriptions, are also required in other domains such as natural language processing. In this paper, we ask whether it is possible to learn those skills from textual data and then transfer them to vision tasks without ever training on visual training data. Key to our approach is exploiting the joint embedding space of contrastively trained vision and language encoders. In practice, there can be systematic differences between embedding spaces for different modalities in contrastive models, and we analyze how these differences affect our approach and study strategies to mitigate this concern. We produce models using only text training data on four representative tasks: image captioning, visual entailment, visual question answering and visual news, and evaluate them on standard benchmarks using images. We find these models generally perform close 
&lt;/p&gt;</description></item><item><title>YATO&#26159;&#19968;&#20010;&#36731;&#37327;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#25991;&#26412;&#20998;&#26512;&#65292;&#21487;&#32452;&#21512;&#19981;&#21516;&#30340;&#29305;&#24449;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#26131;&#29992;&#24615;&#30340;&#20248;&#21183;&#65292;&#20419;&#36827;&#20102;&#20808;&#36827;NLP&#27169;&#22411;&#30340;&#22797;&#29616;&#21644;&#25913;&#36827;&#65292;&#20197;&#21450;&#36328;&#23398;&#31185;&#24212;&#29992;&#30340;&#25512;&#21160;&#12290;</title><link>http://arxiv.org/abs/2209.13877</link><description>&lt;p&gt;
YATO: &#21478;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#20998;&#26512;&#24320;&#28304;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
YATO: Yet Another deep learning based Text analysis Open toolkit. (arXiv:2209.13877v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13877
&lt;/p&gt;
&lt;p&gt;
YATO&#26159;&#19968;&#20010;&#36731;&#37327;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#25991;&#26412;&#20998;&#26512;&#65292;&#21487;&#32452;&#21512;&#19981;&#21516;&#30340;&#29305;&#24449;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#26131;&#29992;&#24615;&#30340;&#20248;&#21183;&#65292;&#20419;&#36827;&#20102;&#20808;&#36827;NLP&#27169;&#22411;&#30340;&#22797;&#29616;&#21644;&#25913;&#36827;&#65292;&#20197;&#21450;&#36328;&#23398;&#31185;&#24212;&#29992;&#30340;&#25512;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;YATO&#65292;&#19968;&#20010;&#24320;&#28304;&#12289;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#25991;&#26412;&#20998;&#26512;&#30340;&#24037;&#20855;&#21253;&#12290;&#19982;&#29616;&#26377;&#30340;&#37325;&#24230;&#24037;&#31243;&#21270;&#24037;&#20855;&#21253;&#21644;&#24179;&#21488;&#19981;&#21516;&#65292;YATO&#36731;&#37327;&#19988;&#29992;&#25143;&#21451;&#22909;&#65292;&#36866;&#29992;&#20110;&#36328;&#23398;&#31185;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;YATO&#35774;&#35745;&#25104;&#20998;&#23618;&#32467;&#26500;&#65292;&#25903;&#25345;&#33258;&#30001;&#32452;&#21512;&#19977;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#29305;&#24449;&#31867;&#22411;&#65292;&#21253;&#25324;1&#65289;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65292;RNN&#31561;&#65289;&#65307;2&#65289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#65292;RoBERTa&#65292;ELECTRA&#31561;&#65289;&#65307;&#21644;3&#65289;&#36890;&#36807;&#31616;&#21333;&#21487;&#37197;&#32622;&#30340;&#25991;&#20214;&#23454;&#29616;&#30340;&#29992;&#25143;&#23450;&#21046;&#21270;&#31070;&#32463;&#29305;&#24449;&#12290;&#30001;&#20110;&#28789;&#27963;&#24615;&#21644;&#26131;&#29992;&#24615;&#30340;&#20248;&#21183;&#65292;YATO&#21487;&#20197;&#20419;&#36827;&#24555;&#36895;&#22797;&#29616;&#21644;&#25913;&#36827;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#24182;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36328;&#23398;&#31185;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20195;&#30721;&#12289;&#31034;&#20363;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/jiesutd/YATO&#20844;&#24320;&#33719;&#21462;&#12290;&#36824;&#25552;&#20379;&#20102;&#28436;&#31034;&#35270;&#39057;https://youtu.be/tSjjf5BzfQg&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce YATO, an open-source, easy-to-use toolkit for text analysis with deep learning. Different from existing heavily engineered toolkits and platforms, YATO is lightweight and user-friendly for researchers from cross-disciplinary areas. Designed in a hierarchical structure, YATO supports free combinations of three types of widely used features including 1) traditional neural networks (CNN, RNN, etc.); 2) pre-trained language models (BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a simple configurable file. Benefiting from the advantages of flexibility and ease of use, YATO can facilitate fast reproduction and refinement of state-of-the-art NLP models, and promote the cross-disciplinary applications of NLP techniques. The code, examples, and documentation are publicly available at https://github.com/jiesutd/YATO. A demo video is also available at https://youtu.be/tSjjf5BzfQg.
&lt;/p&gt;</description></item><item><title>PreSTU&#26159;&#19968;&#31181;&#38024;&#23545;&#22330;&#26223;&#25991;&#26412;&#29702;&#35299;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;OCR&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22270;&#20687;&#20013;&#23884;&#20837;&#25991;&#26412;&#30340;&#35782;&#21035;&#21644;&#36830;&#25509;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#35270;&#35273;&#38382;&#31572;&#21644;&#22270;&#20687;&#23383;&#24149;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.05534</link><description>&lt;p&gt;
PreSTU: &#22330;&#26223;&#25991;&#26412;&#29702;&#35299;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PreSTU: Pre-Training for Scene-Text Understanding. (arXiv:2209.05534v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05534
&lt;/p&gt;
&lt;p&gt;
PreSTU&#26159;&#19968;&#31181;&#38024;&#23545;&#22330;&#26223;&#25991;&#26412;&#29702;&#35299;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;OCR&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22270;&#20687;&#20013;&#23884;&#20837;&#25991;&#26412;&#30340;&#35782;&#21035;&#21644;&#36830;&#25509;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#35270;&#35273;&#38382;&#31572;&#21644;&#22270;&#20687;&#23383;&#24149;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23545;&#20110;&#23884;&#20837;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#25991;&#26412;&#36827;&#34892;&#35782;&#21035;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#36890;&#24120;&#26159;&#32570;&#20047;&#30340;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#35270;&#35273;&#19982;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#20854;&#35757;&#32451;&#30446;&#26631;&#20013;&#24448;&#24448;&#27809;&#26377;&#21253;&#25324;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PreSTU&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#22330;&#26223;&#25991;&#26412;&#29702;&#35299;&#12290;PreSTU&#24341;&#20837;&#20102;OCR&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#40723;&#21169;&#27169;&#22411;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#24182;&#23558;&#20854;&#19982;&#22270;&#20687;&#30340;&#20854;&#20182;&#20869;&#23481;&#36830;&#25509;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;transformer&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#22823;&#35268;&#27169;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#23454;&#29616;&#20102;PreSTU&#65292;&#20854;&#20013;&#30340;&#22330;&#26223;&#25991;&#26412;&#26469;&#33258;&#29616;&#25104;&#30340;OCR&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20843;&#20010;&#35270;&#35273;&#38382;&#31572;&#21644;&#22235;&#20010;&#22270;&#20687;&#23383;&#24149;&#22522;&#20934;&#19978;&#36827;&#34892;&#23454;&#35777;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to recognize and reason about text embedded in visual inputs is often lacking in vision-and-language (V&amp;L) models, perhaps because V&amp;L pre-training methods have often failed to include such an ability in their training objective. In this paper, we propose PreSTU, a novel pre-training recipe dedicated to scene-text understanding (STU). PreSTU introduces OCR-aware pre-training objectives that encourage the model to recognize text from an image and connect it to the rest of the image content. We implement PreSTU using a simple transformer-based encoder-decoder architecture, combined with large-scale image-text datasets with scene text obtained from an off-the-shelf OCR system. We empirically demonstrate the effectiveness of this pre-training approach on eight visual question answering and four image captioning benchmarks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#20869;&#37096;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#24110;&#21161;&#26500;&#24314;&#26356;&#21487;&#20449;&#36182;&#30340;AI&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2207.13243</link><description>&lt;p&gt;
&#36208;&#21521;&#36879;&#26126;AI: &#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#30340;&#35299;&#37322;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#20869;&#37096;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#24110;&#21161;&#26500;&#24314;&#26356;&#21487;&#20449;&#36182;&#30340;AI&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#30340;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#35268;&#27169;&#21644;&#33021;&#21147;&#30340;&#22686;&#38271;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24456;&#38590;&#20998;&#26512;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#22312;&#19981;&#24443;&#24213;&#29702;&#35299;&#20854;&#24037;&#20316;&#21407;&#29702;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#20204;&#30340;&#25285;&#24551;&#12290;&#35299;&#37322;&#23427;&#20204;&#30340;&#26377;&#25928;&#24037;&#20855;&#23558;&#23545;&#26500;&#24314;&#26356;&#21487;&#20449;&#36182;&#30340;AI&#38750;&#24120;&#37325;&#35201;&#65292;&#36890;&#36807;&#24110;&#21161;&#35782;&#21035;&#38382;&#39064;&#12289;&#20462;&#22797;&#38169;&#35823;&#21644;&#22686;&#36827;&#22522;&#26412;&#29702;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;"&#20869;&#37096;"&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#23427;&#20204;&#19987;&#27880;&#20110;&#35299;&#37322;DNNs&#30340;&#20869;&#37096;&#32452;&#20214;&#65292;&#38750;&#24120;&#36866;&#21512;&#20110;&#24320;&#21457;&#26426;&#26800;&#29702;&#35299;&#12289;&#25351;&#23548;&#25163;&#21160;&#20462;&#25913;&#21644;&#36870;&#21521;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;DNN&#21487;&#35299;&#37322;&#24615;&#19978;&#65292;&#36805;&#36895;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#24471;&#23545;&#26041;&#27861;&#36827;&#34892;&#24443;&#24213;&#31995;&#32479;&#21270;&#30340;&#22256;&#38590;&#12290;&#22312;&#36825;&#31687;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;300&#22810;&#31687;&#20316;&#21697;&#65292;&#37325;&#28857;&#20851;&#27880;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#26041;&#27861;&#25353;&#32593;&#32476;&#30340;&#21738;&#20010;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions.  Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#35821;&#38899;&#23398;&#20449;&#24687;&#21644;&#23545;&#35937;&#20449;&#24687;&#65292;&#29992;&#20110;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#21457;&#29616;&#21333;&#35789;&#21644;&#38899;&#32032;&#65292;&#24182;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#27169;&#24577;&#30340;&#23545;&#35937;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#35789;&#21457;&#29616;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.06786</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21452;&#20851;&#33410;&#20998;&#26512;&#19982;&#20849;&#29616;&#32447;&#32034;&#30340;&#22810;&#27169;&#24577;&#21333;&#35789;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Multimodal Word Discovery based on Double Articulation Analysis with Co-occurrence cues. (arXiv:2201.06786v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#35821;&#38899;&#23398;&#20449;&#24687;&#21644;&#23545;&#35937;&#20449;&#24687;&#65292;&#29992;&#20110;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#21457;&#29616;&#21333;&#35789;&#21644;&#38899;&#32032;&#65292;&#24182;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#27169;&#24577;&#30340;&#23545;&#35937;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#35789;&#21457;&#29616;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23156;&#20799;&#22312;&#27809;&#26377;&#22826;&#22810;&#35821;&#35328;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22768;&#38899;&#20998;&#24067;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#20854;&#20182;&#24863;&#23448;&#21050;&#28608;&#30340;&#20849;&#29616;&#26469;&#33719;&#21462;&#20854;&#35821;&#35328;&#35789;&#27719;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#23398;&#20449;&#24687;&#21644;&#23545;&#35937;&#20449;&#24687;&#30340;&#20840;&#26032;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#35821;&#38899;&#21333;&#20803;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#33719;&#24471;&#21333;&#35789;&#21644;&#38899;&#32032;&#65292;&#24182;&#21516;&#26102;&#21033;&#29992;&#22522;&#20110;&#22810;&#31181;&#27169;&#24577;&#30340;&#23545;&#35937;&#20449;&#24687;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35302;&#35273;&#21644;&#21548;&#35273;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#21452;&#20851;&#33410;&#20998;&#26512;&#22120;&#65288;NPB-DAA&#65289;&#20174;&#35821;&#38899;&#23398;&#29305;&#24449;&#20013;&#21457;&#29616;&#38899;&#32032;&#21644;&#21333;&#35789;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#27169;&#24577;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;MLDA&#65289;&#23545;&#20174;&#23545;&#35937;&#20013;&#33719;&#21462;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#21333;&#35789;&#21457;&#29616;&#24615;&#33021;&#12290;&#34920;&#36798;&#20102;&#29305;&#24615;&#30340;&#21333;&#35789;&#21253;&#21547;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Human infants acquire their verbal lexicon with minimal prior knowledge of language based on the statistical properties of phonological distributions and the co-occurrence of other sensory stimuli. This study proposes a novel fully unsupervised learning method for discovering speech units using phonological information as a distributional cue and object information as a co-occurrence cue. The proposed method can acquire words and phonemes from speech signals using unsupervised learning and utilize object information based on multiple modalities-vision, tactile, and auditory-simultaneously. The proposed method is based on the nonparametric Bayesian double articulation analyzer (NPB-DAA) discovering phonemes and words from phonological features, and multimodal latent Dirichlet allocation (MLDA) categorizing multimodal information obtained from objects. In an experiment, the proposed method showed higher word discovery performance than baseline methods. Words that expressed the characteri
&lt;/p&gt;</description></item></channel></rss>