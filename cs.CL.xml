<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#20960;&#20309;&#21644;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20013;&#30340;&#21387;&#32553;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#35270;&#35282;&#39640;&#24230;&#30456;&#20851;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#35821;&#35328;&#25968;&#25454;&#30340;&#39640;&#21387;&#32553;&#39044;&#27979;&#20102;&#23545;&#35813;&#25968;&#25454;&#38598;&#30340;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.13620</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36830;&#25509;&#20449;&#24687;&#35770;&#21644;&#20960;&#20309;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Bridging Information-Theoretic and Geometric Compression in Language Models. (arXiv:2310.13620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#20960;&#20309;&#21644;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20013;&#30340;&#21387;&#32553;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#35270;&#35282;&#39640;&#24230;&#30456;&#20851;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#35821;&#35328;&#25968;&#25454;&#30340;&#39640;&#21387;&#32553;&#39044;&#27979;&#20102;&#23545;&#35813;&#25968;&#25454;&#38598;&#30340;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30495;&#23454;&#22320;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24517;&#39035;&#23558;&#22823;&#37327;&#30340;&#12289;&#28508;&#22312;&#26080;&#38480;&#30340;&#20449;&#24687;&#21387;&#32553;&#21040;&#30456;&#23545;&#36739;&#23569;&#30340;&#32500;&#24230;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20174;&#20960;&#20309;&#21644;&#20449;&#24687;&#35770;&#30340;&#20004;&#20010;&#35282;&#24230;&#20998;&#26512;&#65288;&#39044;&#35757;&#32451;&#30340;&#65289;LM&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#35270;&#35282;&#39640;&#24230;&#30456;&#20851;&#65292;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#20960;&#20309;&#32500;&#24230;&#21487;&#20197;&#39044;&#27979;&#23427;&#20204;&#22312;LM&#19979;&#30340;&#32534;&#30721;&#38271;&#24230;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#36827;&#19968;&#27493;&#65292;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#39640;&#21387;&#32553;&#39044;&#27979;&#20102;&#23545;&#35813;&#25968;&#25454;&#38598;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#30830;&#35748;&#20102;&#33021;&#22815;&#21387;&#32553;&#35821;&#35328;&#20449;&#24687;&#26159;&#25104;&#21151;LM&#24615;&#33021;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#26512;&#30340;&#23454;&#38469;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#39318;&#27425;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#22312;&#35821;&#35328;&#25968;&#25454;&#19978;&#30340;&#20869;&#22312;&#32500;&#24230;&#20272;&#35745;&#22120;&#65292;&#34920;&#26126;&#21482;&#26377;&#19968;&#37096;&#20998;&#23553;&#35013;&#20102;&#20449;&#24687;&#35770;&#21387;&#32553;&#12289;&#20960;&#20309;&#21387;&#32553;&#21644;&#36866;&#24212;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views are highly correlated, such that the intrinsic geometric dimension of linguistic data predicts their coding length under the LM. We then show that, in turn, high compression of a linguistic dataset predicts rapid adaptation to that dataset, confirming that being able to compress linguistic information is an important part of successful LM performance. As a practical byproduct of our analysis, we evaluate a battery of intrinsic dimension estimators for the first time on linguistic data, showing that only some encapsulate the relationship between information-theoretic compression, geometric compression, and ease-of-adaptation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#20687;&#21465;&#36848;&#23545;&#36827;&#34892;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#20849;&#25351;&#28040;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20849;&#25351;&#28040;&#35299;&#21644;&#21465;&#20107;&#22522;&#30784;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13619</link><description>&lt;p&gt;
&#22270;&#20687;&#21465;&#36848;&#20013;&#30340;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#20849;&#25351;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised multimodal coreference resolution in image narrations. (arXiv:2310.13619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#20687;&#21465;&#36848;&#23545;&#36827;&#34892;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#20849;&#25351;&#28040;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20849;&#25351;&#28040;&#35299;&#21644;&#21465;&#20107;&#22522;&#30784;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#20849;&#25351;&#28040;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#19982;&#38271;&#31687;&#21465;&#36848;&#25991;&#26412;&#37197;&#23545;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#23384;&#22312;&#32454;&#31890;&#24230;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#65292;&#21465;&#36848;&#35821;&#35328;&#20013;&#22266;&#26377;&#30340;&#27495;&#20041;&#20197;&#21450;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#27880;&#35757;&#32451;&#38598;&#31561;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#21465;&#36848;&#23545;&#26469;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#35299;&#20915;&#20849;&#25351;&#28040;&#35299;&#21644;&#21465;&#20107;&#22522;&#30784;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36328;&#27169;&#24577;&#26694;&#26550;&#20013;&#32467;&#21512;&#20102;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20849;&#25351;&#28040;&#35299;&#21644;&#21465;&#20107;&#22522;&#30784;&#20219;&#21153;&#19978;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#19978;&#22343;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study multimodal coreference resolution, specifically where a longer descriptive text, i.e., a narration is paired with an image. This poses significant challenges due to fine-grained image-text alignment, inherent ambiguity present in narrative language, and unavailability of large annotated training sets. To tackle these challenges, we present a data efficient semi-supervised approach that utilizes image-narration pairs to resolve coreferences and narrative grounding in a multimodal context. Our approach incorporates losses for both labeled and unlabeled data within a cross-modal framework. Our evaluation shows that the proposed approach outperforms strong baselines both quantitatively and qualitatively, for the tasks of coreference resolution and narrative grounding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24110;&#21161;&#23398;&#29983;&#23398;&#20064;&#25968;&#23398;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20135;&#29983;&#38169;&#35823;&#30340;&#25512;&#29702;&#36807;&#31243;&#12289;&#35823;&#35299;&#38382;&#39064;&#30340;&#21547;&#20041;&#20197;&#21450;&#38590;&#20197;&#29702;&#35299;&#38382;&#39064;&#30340;&#29702;&#30001;&#12290;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13615</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20419;&#36827;&#25968;&#23398;&#23398;&#20064;&#30340;&#19977;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning. (arXiv:2310.13615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24110;&#21161;&#23398;&#29983;&#23398;&#20064;&#25968;&#23398;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20135;&#29983;&#38169;&#35823;&#30340;&#25512;&#29702;&#36807;&#31243;&#12289;&#35823;&#35299;&#38382;&#39064;&#30340;&#21547;&#20041;&#20197;&#21450;&#38590;&#20197;&#29702;&#35299;&#38382;&#39064;&#30340;&#29702;&#30001;&#12290;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#22240;&#27492;&#30740;&#31350;&#20102;&#23558;&#20854;&#24212;&#29992;&#20110;&#25945;&#32946;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#25506;&#35752;LLM&#22312;&#24110;&#21161;&#23398;&#29983;&#23398;&#20064;&#25968;&#23398;&#26041;&#38754;&#30340;&#25945;&#32946;&#33021;&#21147;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;LLM&#25552;&#20379;&#20010;&#24615;&#21270;&#21453;&#39304;&#26469;&#22686;&#24378;&#23398;&#29983;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#38500;&#20102;&#20135;&#29983;&#38169;&#35823;&#30340;&#25512;&#29702;&#36807;&#31243;&#22806;&#65292;LLM&#36824;&#21487;&#33021;&#35823;&#35299;&#38382;&#39064;&#30340;&#21547;&#20041;&#65292;&#24182;&#22312;&#23581;&#35797;&#32416;&#27491;&#23398;&#29983;&#31572;&#26696;&#26102;&#38590;&#20197;&#29702;&#35299;&#32473;&#23450;&#38382;&#39064;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this position paper, we discuss the challenges associated with employing LLMs to enhance students' mathematical problem-solving skills by providing adaptive feedback. Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers. Three research questions are formulated.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#65292;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#21644;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.13613</link><description>&lt;p&gt;
Hunayn&#65306;&#36229;&#36234;&#23383;&#38754;&#24847;&#20041;&#30340;&#32763;&#35793;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;
Hunayn: Elevating Translation Beyond the Literal. (arXiv:2310.13613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#65292;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#21644;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#12290;&#21033;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#65288;MarianMT&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#12289;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#19982;&#35895;&#27468;&#32763;&#35793;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#23450;&#24615;&#35780;&#20272;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#22312;&#20351;&#29992;Fusha&#25968;&#25454;&#38598;&#30340;&#33521;&#38463;&#32763;&#35793;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project introduces an advanced English-to-Arabic translator surpassing conventional tools. Leveraging the Helsinki transformer (MarianMT), our approach involves fine-tuning on a self-scraped, purely literary Arabic dataset. Evaluations against Google Translate show consistent outperformance in qualitative assessments. Notably, it excels in cultural sensitivity and context accuracy. This research underscores the Helsinki transformer's superiority for English-to-Arabic translation using a Fusha dataset.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#25105;&#24402;&#22240;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#32479;&#19968;&#20004;&#38454;&#27573;&#26694;&#26550;&#65288;SADM&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#35299;&#37322;&#26469;&#24314;&#31435;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#20915;&#31574;&#35299;&#37322;&#32852;&#31995;&#65292;&#24182;&#22312;ERASER&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13610</link><description>&lt;p&gt;
&#20351;&#24744;&#30340;&#20915;&#31574;&#26377;&#35828;&#26381;&#21147;&#65281;&#19968;&#20010;&#32479;&#19968;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65306;&#33258;&#25105;&#24402;&#22240;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making. (arXiv:2310.13610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13610
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#25105;&#24402;&#22240;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#32479;&#19968;&#20004;&#38454;&#27573;&#26694;&#26550;&#65288;SADM&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#35299;&#37322;&#26469;&#24314;&#31435;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#20915;&#31574;&#35299;&#37322;&#32852;&#31995;&#65292;&#24182;&#22312;ERASER&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#34892;&#20026;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#25903;&#25345;&#27169;&#22411;&#20915;&#31574;&#30340;&#35777;&#25454;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26694;&#26550;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35299;&#37322;&#30340;&#21516;&#26102;&#21462;&#24471;&#20102;&#39640;&#20219;&#21153;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24573;&#30053;&#20102;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#27169;&#22411;&#20915;&#31574;&#20043;&#38388;&#30340;&#19981;&#21487;&#38752;&#32852;&#31995;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#27169;&#22411;&#21487;&#33021;&#22312;&#24402;&#22240;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#27491;&#30830;&#30340;&#20915;&#31574;&#65292;&#25110;&#32773;&#22312;&#24402;&#22240;&#27491;&#30830;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#31967;&#31957;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#33258;&#25105;&#24402;&#22240;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#32479;&#19968;&#20004;&#38454;&#27573;&#26694;&#26550;&#65288;SADM&#65289;&#12290;&#36890;&#36807;&#23545;ERASER&#22522;&#20934;&#27979;&#35797;&#30340;&#20116;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#33021;&#24314;&#31435;&#29983;&#25104;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#20043;&#38388;&#26356;&#21487;&#38752;&#30340;&#32852;&#31995;&#65292;&#32780;&#19988;&#36824;&#33021;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining black-box model behavior with natural language has achieved impressive results in various NLP tasks. Recent research has explored the utilization of subsequences from the input text as a rationale, providing users with evidence to support the model decision. Although existing frameworks excel in generating high-quality rationales while achieving high task performance, they neglect to account for the unreliable link between the generated rationale and model decision. In simpler terms, a model may make correct decisions while attributing wrong rationales, or make poor decisions while attributing correct rationales. To mitigate this issue, we propose a unified two-stage framework known as Self-Attribution and Decision-Making (SADM). Through extensive experiments on five reasoning datasets from the ERASER benchmark, we demonstrate that our framework not only establishes a more reliable link between the generated rationale and model decision but also achieves competitive results 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MULTITuDE&#65292;&#19968;&#20010;&#38024;&#23545;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26816;&#27979;&#22120;&#22312;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#29983;&#25104;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#19979;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.13606</link><description>&lt;p&gt;
MULTITuDE: &#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark. (arXiv:2310.13606v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13606
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MULTITuDE&#65292;&#19968;&#20010;&#38024;&#23545;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26816;&#27979;&#22120;&#22312;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#29983;&#25104;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#19979;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#20110;&#26368;&#36817;&#30340;LLMs&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#25991;&#26412;&#33021;&#21147;&#20197;&#21450;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#30740;&#31350;&#19981;&#36275;&#12290;&#36825;&#20063;&#21453;&#26144;&#22312;&#29616;&#26377;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;&#32570;&#20047;&#20854;&#20182;&#35821;&#35328;&#30495;&#23454;&#25991;&#26412;&#65292;&#20027;&#35201;&#28085;&#30422;&#36739;&#26087;&#30340;&#29983;&#25104;&#22120;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MULTITuDE&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;11&#31181;&#35821;&#35328;&#65288;&#38463;&#25289;&#20271;&#35821;&#65292;&#21152;&#27888;&#32599;&#23612;&#20122;&#35821;&#65292;&#25463;&#20811;&#35821;&#65292;&#24503;&#35821;&#65292;&#33521;&#35821;&#65292;&#35199;&#29677;&#29273;&#35821;&#65292;&#33655;&#20848;&#35821;&#65292;&#33889;&#33796;&#29273;&#35821;&#65292;&#20420;&#35821;&#65292;&#20044;&#20811;&#20848;&#35821;&#21644;&#20013;&#25991;&#65289;&#29983;&#25104;&#30340;74,081&#20010;&#30495;&#23454;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#30001;8&#20010;&#22810;&#35821;&#35328;LLMs&#29983;&#25104;&#12290;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38646;&#26679;&#26412;&#65288;&#32479;&#35745;&#21644;&#40657;&#30418;&#65289;&#21644;&#24494;&#35843;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#22810;&#35821;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;1&#65289;&#36825;&#20123;&#26816;&#27979;&#22120;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#65288;&#35821;&#35328;&#19978;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#35821;&#35328;&#65289;&#21644;&#26410;&#35265;&#36807;&#30340;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;2&#65289;&#24403;&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26816;&#27979;&#22120;&#26159;&#21542;&#33021;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of 74,081 authentic and machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare the performance of zero-shot (statistical and black-box) and fine-tuned detectors. Considering the multilinguality, we evaluate 1) how these detectors generalize to unseen languages (linguistically similar as well as dissimilar) and unseen LLMs and 2) whether the detectors improve their performance when trained on multiple lang
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MarineGPT&#30340;&#28023;&#27915;&#39046;&#22495;&#19987;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#25935;&#24863;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#31185;&#23398;&#30340;&#26041;&#24335;&#22238;&#24212;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.13596</link><description>&lt;p&gt;
MarineGPT: &#21521;&#20844;&#20247;&#25581;&#31034;&#28023;&#27915;&#30340;&#31192;&#23494;
&lt;/p&gt;
&lt;p&gt;
MarineGPT: Unlocking Secrets of Ocean to the Public. (arXiv:2310.13596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MarineGPT&#30340;&#28023;&#27915;&#39046;&#22495;&#19987;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#25935;&#24863;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#31185;&#23398;&#30340;&#26041;&#24335;&#22238;&#24212;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#34987;&#35777;&#26126;&#26159;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#21487;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#32852;&#21512;&#35821;&#20041;&#31354;&#38388;&#65288;&#22914;&#35270;&#35273;-&#25991;&#26412;&#31354;&#38388;&#65289;&#65292;&#20351;LLM&#20855;&#22791;&#24863;&#30693;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;LLM&#21644;MLLM&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#19987;&#19994;&#30693;&#35782;&#30340;&#39046;&#22495;&#29305;&#23450;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28023;&#27915;&#39046;&#22495;&#65292;&#23545;LLM&#21644;MLLM&#30340;&#25506;&#32034;&#23578;&#19981;&#20805;&#20998;&#12290;&#19982;&#36890;&#29992;&#30446;&#30340;&#30340;MLLM&#19981;&#21516;&#65292;&#28023;&#27915;&#29305;&#23450;MLLM&#38656;&#35201;&#20135;&#29983;&#26356;&#21152;&#25935;&#24863;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#20855;&#26377;&#31185;&#23398;&#24615;&#30340;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#36890;&#36807;&#22823;&#37327;&#36890;&#29992;&#22521;&#35757;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#30340;MLLM&#22312;&#29702;&#35299;&#39046;&#22495;&#29305;&#23450;&#24847;&#22270;&#24182;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#28385;&#24847;&#30340;&#22238;&#24212;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT/GPT-4, have proven to be powerful tools in promoting the user experience as an AI assistant. The continuous works are proposing multi-modal large language models (MLLM), empowering LLMs with the ability to sense multiple modality inputs through constructing a joint semantic space (e.g. visual-text space). Though significant success was achieved in LLMs and MLLMs, exploring LLMs and MLLMs in domain-specific applications that required domain-specific knowledge and expertise has been less conducted, especially for \textbf{marine domain}. Different from general-purpose MLLMs, the marine-specific MLLM is required to yield much more \textbf{sensitive}, \textbf{informative}, and \textbf{scientific} responses. In this work, we demonstrate that the existing MLLMs optimized on huge amounts of readily available general-purpose training data show a minimal ability to understand domain-specific intents and then generate informative and satisfactory resp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21046;&#21442;&#32771;&#25991;&#29486;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#36848;&#30495;&#23454;&#21442;&#32771;&#25991;&#29486;&#65292;&#23450;&#21046;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#22312;&#19981;&#21516;&#24310;&#36831;&#19979;&#35757;&#32451;SiMT&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#24378;&#21046;&#39044;&#27979;&#24182;&#20445;&#25345;&#39640;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.13588</link><description>&lt;p&gt;
&#20351;&#29992;&#23450;&#21046;&#30340;&#21442;&#32771;&#25991;&#29486;&#30340;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Machine Translation with Tailored Reference. (arXiv:2310.13588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21046;&#21442;&#32771;&#25991;&#29486;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#36848;&#30495;&#23454;&#21442;&#32771;&#25991;&#29486;&#65292;&#23450;&#21046;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#22312;&#19981;&#21516;&#24310;&#36831;&#19979;&#35757;&#32451;SiMT&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#24378;&#21046;&#39044;&#27979;&#24182;&#20445;&#25345;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#22312;&#35835;&#21462;&#25972;&#20010;&#28304;&#21477;&#23376;&#30340;&#21516;&#26102;&#29983;&#25104;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SiMT&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#30456;&#21516;&#30340;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#24573;&#30053;&#20102;&#22312;&#19981;&#21516;&#24310;&#36831;&#19979;&#21487;&#29992;&#30340;&#28304;&#20449;&#24687;&#30340;&#25968;&#37327;&#19981;&#21516;&#12290;&#20351;&#29992;&#20302;&#24310;&#36831;&#19979;&#30340;&#30495;&#23454;&#21442;&#32771;&#25991;&#29486;&#21487;&#33021;&#24341;&#20837;&#24378;&#21046;&#39044;&#27979;&#65292;&#32780;&#20351;&#29992;&#19982;&#28304;&#35789;&#39034;&#24207;&#19968;&#33268;&#30340;&#21442;&#32771;&#25991;&#29486;&#22312;&#39640;&#24310;&#36831;&#19979;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#26159;&#20351;&#29992;&#36866;&#24403;&#30340;&#21442;&#32771;&#25991;&#29486;&#26469;&#35757;&#32451;SiMT&#27169;&#22411;&#65292;&#26082;&#36991;&#20813;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24378;&#21046;&#39044;&#27979;&#65292;&#21448;&#20445;&#25345;&#39640;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#36848;&#30495;&#23454;&#21442;&#32771;&#25991;&#29486;&#65292;&#20026;&#19981;&#21516;&#24310;&#36831;&#36827;&#34892;&#35757;&#32451;&#30340;SiMT&#27169;&#22411;&#25552;&#20379;&#23450;&#21046;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30001;&#24378;&#21270;&#23398;&#20064;&#24341;&#21457;&#30340;&#25913;&#32534;&#22120;&#26469;&#20462;&#25913;&#30495;&#23454;&#21442;&#32771;&#25991;&#29486;&#65292;&#20197;&#24471;&#21040;&#23450;&#21046;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;SiMT&#27169;&#22411;&#26159;&#20351;&#29992;&#23450;&#21046;&#30340;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19982;&#23450;&#21046;&#22120;&#20849;&#21516;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous machine translation (SiMT) generates translation while reading the whole source sentence. However, existing SiMT models are typically trained using the same reference disregarding the varying amounts of available source information at different latency. Training the model with ground-truth at low latency may introduce forced anticipations, whereas utilizing reference consistent with the source word order at high latency results in performance degradation. Consequently, it is crucial to train the SiMT model with appropriate reference that avoids forced anticipations during training while maintaining high quality. In this paper, we propose a novel method that provides tailored reference for the SiMT models trained at different latency by rephrasing the ground-truth. Specifically, we introduce the tailor, induced by reinforcement learning, to modify ground-truth to the tailored reference. The SiMT model is trained with the tailored reference and jointly optimized with the tai
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23376;&#26641;&#24863;&#30693;&#30340;&#21333;&#35789;&#37325;&#25490;&#24207;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#33021;&#21147;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#35821;&#35328;&#29305;&#23450;&#35268;&#21017;&#12289;POS&#26631;&#31614;&#32423;&#21035;&#25110;&#21482;&#38024;&#23545;&#20027;&#21477;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.13583</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#26641;&#24863;&#30693;&#30340;&#21333;&#35789;&#37325;&#25490;&#24207;&#25552;&#21319;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering. (arXiv:2310.13583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13583
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23376;&#26641;&#24863;&#30693;&#30340;&#21333;&#35789;&#37325;&#25490;&#24207;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#33021;&#21147;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#35821;&#35328;&#29305;&#23450;&#35268;&#21017;&#12289;POS&#26631;&#31614;&#32423;&#21035;&#25110;&#21482;&#38024;&#23545;&#20027;&#21477;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;XLM-R&#21644;mT5&#65289;&#30340;&#33021;&#21147;&#26377;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22686;&#38271;&#65292;&#20294;&#24050;&#32463;&#21457;&#29616;&#23427;&#20204;&#22312;&#22788;&#29702;&#35821;&#35328;&#24046;&#24322;&#36739;&#22823;&#30340;&#35821;&#35328;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#19968;&#20010;&#38556;&#30861;&#26159;&#21333;&#35789;&#39034;&#24207;&#27169;&#24335;&#30340;&#21487;&#21464;&#24615;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#28304;&#31471;&#25110;&#30446;&#26631;&#31471;&#30340;&#21333;&#35789;&#37325;&#25490;&#24207;&#26469;&#20943;&#36731;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#37325;&#25490;&#24207;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#29305;&#23450;&#35821;&#35328;&#30340;&#35268;&#21017;&#65292;&#24037;&#20316;&#22312;POS&#26631;&#31614;&#30340;&#32423;&#21035;&#19978;&#65292;&#25110;&#32773;&#21482;&#38024;&#23545;&#20027;&#21477;&#65292;&#32780;&#20445;&#25345;&#20174;&#23646;&#20174;&#21477;&#19981;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#22823;&#30340;&#37325;&#25490;&#24207;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#36890;&#29992;&#20381;&#36182;&#20851;&#31995;&#26469;&#23450;&#20041;&#65292;&#33021;&#22815;&#21033;&#29992;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#20197;&#21477;&#27861;&#19978;&#19979;&#25991;&#20026;&#26465;&#20214;&#30340;&#32454;&#31890;&#24230;&#21333;&#35789;&#39034;&#24207;&#27169;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#21477;&#27861;&#26641;&#30340;&#25152;&#26377;&#23618;&#32423;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via sourceor target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on language-specific rules, work on the level of POS tags, or only target the main clause, leaving subordinate clauses intact. To address these limitations, we present a new powerful reordering method, defined in terms of Universal Dependencies, that is able to learn fine-grained word-order patterns conditioned on the syntactic context from a small amount of annotated data and can be applied at all levels of the syntactic tree. We conduct experiments on a diverse set of tasks and show that our method consiste
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#30340;&#38382;&#39064;&#20998;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22359;&#21270;&#26597;&#35810;&#35745;&#21010;&#35821;&#35328;&#65288;QPL&#65289;&#65292;&#36890;&#36807;&#23558;SQL&#26597;&#35810;&#20998;&#35299;&#25104;&#31616;&#21333;&#21644;&#35268;&#21017;&#30340;&#23376;&#26597;&#35810;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;QPL&#36827;&#34892;&#35299;&#26512;&#23545;&#20110;&#35821;&#20041;&#31561;&#25928;&#30340;&#26597;&#35810;&#27604;&#20351;&#29992;SQL&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.13575</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#20013;&#30340;&#38382;&#39064;&#21644;SQL&#30340;&#35821;&#20041;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Semantic Decomposition of Question and SQL for Text-to-SQL Parsing. (arXiv:2310.13575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13575
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#30340;&#38382;&#39064;&#20998;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22359;&#21270;&#26597;&#35810;&#35745;&#21010;&#35821;&#35328;&#65288;QPL&#65289;&#65292;&#36890;&#36807;&#23558;SQL&#26597;&#35810;&#20998;&#35299;&#25104;&#31616;&#21333;&#21644;&#35268;&#21017;&#30340;&#23376;&#26597;&#35810;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;QPL&#36827;&#34892;&#35299;&#26512;&#23545;&#20110;&#35821;&#20041;&#31561;&#25928;&#30340;&#26597;&#35810;&#27604;&#20351;&#29992;SQL&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#22312;&#36328;&#39046;&#22495;&#21644;&#22797;&#26434;&#26597;&#35810;&#19978;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#38382;&#39064;&#20998;&#35299;&#31574;&#30053;&#26469;&#22686;&#24378;&#23545;&#22797;&#26434;SQL&#26597;&#35810;&#30340;&#35299;&#26512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#36935;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#65306;&#65288;1&#65289;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#32570;&#20047;&#38382;&#39064;&#20998;&#35299;&#65307;&#65288;2&#65289;&#30001;&#20110;SQL&#30340;&#21477;&#27861;&#22797;&#26434;&#24615;&#65292;&#22823;&#22810;&#25968;&#22797;&#26434;&#26597;&#35810;&#19981;&#33021;&#34987;&#35299;&#24320;&#25104;&#21487;&#20197;&#34987;&#37325;&#26032;&#32452;&#21512;&#30340;&#23376;&#26597;&#35810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22359;&#21270;&#26597;&#35810;&#35745;&#21010;&#35821;&#35328;&#65288;QPL&#65289;&#65292;&#23558;SQL&#26597;&#35810;&#31995;&#32479;&#22320;&#20998;&#35299;&#20026;&#31616;&#21333;&#21644;&#35268;&#21017;&#30340;&#23376;&#26597;&#35810;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;SQL&#26381;&#21153;&#22120;&#26597;&#35810;&#20248;&#21270;&#35745;&#21010;&#24320;&#21457;&#20102;&#19968;&#20010;&#20174;SQL&#21040;QPL&#30340;&#36716;&#25442;&#22120;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;QPL&#31243;&#24207;&#22686;&#24378;&#20102;Spider&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QPL&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#26377;&#30410;&#20110;&#29616;&#26377;&#30340;&#35821;&#20041;&#35299;&#26512;&#26550;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#35821;&#20041;&#31561;&#25928;&#30340;&#26597;&#35810;&#65292;&#35757;&#32451;&#25991;&#26412;&#21040;QPL&#35299;&#26512;&#22120;&#27604;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL semantic parsing faces challenges in generalizing to cross-domain and complex queries. Recent research has employed a question decomposition strategy to enhance the parsing of complex SQL queries. However, this strategy encounters two major obstacles: (1) existing datasets lack question decomposition; (2) due to the syntactic complexity of SQL, most complex queries cannot be disentangled into sub-queries that can be readily recomposed. To address these challenges, we propose a new modular Query Plan Language (QPL) that systematically decomposes SQL queries into simple and regular sub-queries. We develop a translator from SQL to QPL by leveraging analysis of SQL server query optimization plans, and we augment the Spider dataset with QPL programs. Experimental results demonstrate that the modular nature of QPL benefits existing semantic-parsing architectures, and training text-to-QPL parsers is more effective than text-to-SQL parsing for semantically equivalent queries. The Q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#29983;&#25104;&#36830;&#36143;&#30340;&#24605;&#32500;&#38142;&#26465;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#23427;&#19982;&#30495;&#23454;&#35821;&#35328;&#26469;&#28304;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.13571</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20309;&#33021;&#29983;&#25104;&#27491;&#30830;&#30340;&#24605;&#32500;&#38142;&#26465;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Can Large Language Models Generate Correct Chain-of-Thoughts?. (arXiv:2310.13571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#29983;&#25104;&#36830;&#36143;&#30340;&#24605;&#32500;&#38142;&#26465;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#23427;&#19982;&#30495;&#23454;&#35821;&#35328;&#26469;&#28304;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#25512;&#21160;&#23545;&#24605;&#32500;&#38142;&#26465;&#24341;&#21457;&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#35825;&#23548;LLM&#29983;&#25104;&#36830;&#36143;&#30340;&#24605;&#32500;&#38142;&#26465;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#20004;&#32423;&#20998;&#23618;&#22270;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#35828;&#26381;&#21147;&#30340;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#65292;&#29992;&#20110;&#34913;&#37327;LLM&#29983;&#25104;&#30340;&#24605;&#32500;&#38142;&#26465;&#19982;&#30495;&#23454;&#35821;&#35328;&#26469;&#28304;&#30340;&#24605;&#32500;&#38142;&#26465;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;LLM&#33021;&#22815;&#20135;&#29983;&#27491;&#30830;&#30340;&#24605;&#32500;&#24207;&#21015;&#65288;&#21487;&#33021;&#65289;&#35299;&#37322;&#20102;&#22312;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#20013;&#24615;&#33021;&#25552;&#21319;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the capabilities of large language models (LLMs), specifically focusing on advancing the theoretical comprehension of chain-of-thought prompting. We investigate how LLMs can be effectively induced to generate a coherent chain of thoughts. To achieve this, we introduce a two-level hierarchical graphical model tailored for natural language generation. Within this framework, we establish a compelling geometrical convergence rate that gauges the likelihood of an LLM-generated chain of thoughts compared to those originating from the true language. Our findings provide a theoretical justification for the ability of LLMs to produce the correct sequence of thoughts (potentially) explaining performance gains in tasks demanding reasoning skills.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#30693;&#35782;&#39537;&#21160;&#21709;&#24212;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23545;&#35805;&#36718;&#27425;&#20013;&#35780;&#20272;&#30693;&#35782;&#22270;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#23545;&#35805;&#30456;&#20851;&#24615;&#65292;&#22312;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13566</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#21644;&#30456;&#20851;&#24615;&#35780;&#20998;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Neural Response Generation Using Logical Reasoning and Relevance Scoring. (arXiv:2310.13566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13566
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#30693;&#35782;&#39537;&#21160;&#21709;&#24212;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23545;&#35805;&#36718;&#27425;&#20013;&#35780;&#20272;&#30693;&#35782;&#22270;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#23545;&#35805;&#30456;&#20851;&#24615;&#65292;&#22312;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#26500;&#36896;&#21709;&#24212;&#36890;&#24120;&#20381;&#36182;&#20110;&#24403;&#21069;&#23545;&#35805;&#29366;&#24577;&#25110;&#22806;&#37096;&#25968;&#25454;&#24211;&#31561;&#20449;&#24687;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#30693;&#35782;&#39537;&#21160;&#21709;&#24212;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22260;&#32469;&#30528;&#34920;&#31034;&#24403;&#21069;&#23545;&#35805;&#29366;&#24577;&#21644;&#32972;&#26223;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#26469;&#36827;&#34892;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#25512;&#26029;&#20986;&#36923;&#36753;&#25512;&#23548;&#30340;&#20107;&#23454;&#65292;&#20016;&#23500;&#30693;&#35782;&#22270;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#23545;&#35805;&#36718;&#27425;&#20013;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#26469;&#35780;&#20998;&#25193;&#23637;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#23545;&#35805;&#30456;&#20851;&#24615;&#12290;&#26368;&#21518;&#65292;&#23558;&#30456;&#20851;&#24615;&#24471;&#20998;&#26368;&#39640;&#30340;&#20803;&#32032;&#36716;&#25442;&#25104;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#65292;&#24182;&#25972;&#21512;&#21040;&#29992;&#20110;&#29983;&#25104;&#31995;&#32479;&#21709;&#24212;&#30340;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#30340;&#25552;&#31034;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;KVRET&#21644;GraphWOZ&#65289;&#19978;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing responses in task-oriented dialogue systems typically relies on information sources such the current dialogue state or external databases. This paper presents a novel approach to knowledge-grounded response generation that combines retrieval-augmented language models with logical reasoning. The approach revolves around a knowledge graph representing the current dialogue state and background information, and proceeds in three steps. The knowledge graph is first enriched with logically derived facts inferred using probabilistic logical programming. A neural model is then employed at each turn to score the conversational relevance of each node and edge of this extended graph. Finally, the elements with highest relevance scores are converted to a natural language form, and are integrated into the prompt for the neural conversational model employed to generate the system response.  We investigate the benefits of the proposed approach on two datasets (KVRET and GraphWOZ) along w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#31070;&#32463;&#32531;&#23384;&#25216;&#26415;&#65292;&#20351;&#29992;&#36793;&#30028;&#25277;&#26679;&#21644;&#22996;&#21592;&#20250;&#26597;&#35810;&#20316;&#20026;&#20915;&#31574;&#31574;&#30053;&#65292;&#20248;&#21270;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.13561</link><description>&lt;p&gt;
&#32531;&#23384;&#19982;&#31934;&#28860;&#65306;&#20248;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cache &amp; Distil: Optimising API Calls to Large Language Models. (arXiv:2310.13561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#31070;&#32463;&#32531;&#23384;&#25216;&#26415;&#65292;&#20351;&#29992;&#36793;&#30028;&#25277;&#26679;&#21644;&#22996;&#21592;&#20250;&#26597;&#35810;&#20316;&#20026;&#20915;&#31574;&#31574;&#30053;&#65292;&#20248;&#21270;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#37096;&#32626;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#24448;&#24448;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;API&#35843;&#29992;&#26469;&#28385;&#36275;&#29992;&#25143;&#26597;&#35810;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#35843;&#29992;&#30340;&#39057;&#29575;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;--&#23398;&#29983;&#27169;&#22411;--&#19981;&#26029;&#22320;&#22312;LLM&#30340;&#21709;&#24212;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20010;&#23398;&#29983;&#27169;&#22411;&#36880;&#28176;&#29420;&#31435;&#22788;&#29702;&#36234;&#26469;&#36234;&#22810;&#30340;&#29992;&#25143;&#35831;&#27714;&#65292;&#24182;&#36880;&#27493;&#25552;&#39640;&#20854;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#31070;&#32463;&#32531;&#23384;&#12290;&#31070;&#32463;&#32531;&#23384;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#19968;&#20010;&#20915;&#31574;&#31574;&#30053;&#65292;&#29992;&#20110;&#20915;&#23450;&#21738;&#20123;&#35831;&#27714;&#24212;&#30001;&#23398;&#29983;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#65292;&#21738;&#20123;&#35831;&#27714;&#24212;&#37325;&#23450;&#21521;&#32473;LLM&#65292;&#20174;&#32780;&#24110;&#21161;&#23398;&#29983;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#32463;&#20856;&#30340;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#36873;&#25321;&#20934;&#21017;&#20316;&#20026;&#35813;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36793;&#30028;&#25277;&#26679;&#21644;&#22996;&#21592;&#20250;&#26597;&#35810;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39044;&#31639;&#19979;&#24102;&#26469;&#19968;&#33268;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale deployment of generative AI tools often depends on costly API calls to a Large Language Model (LLM) to fulfil user queries. To curtail the frequency of these calls, one can employ a smaller language model -- a student -- which is continuously trained on the responses of the LLM. This student gradually gains proficiency in independently handling an increasing number of user requests, a process we term neural caching. The crucial element in neural caching is a policy that decides which requests should be processed by the student alone and which should be redirected to the LLM, subsequently aiding the student's learning. In this study, we focus on classification tasks, and we consider a range of classic active learning-based selection criteria as the policy. Our experiments suggest that Margin Sampling and Query by Committee bring consistent benefits across tasks and budgets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#21551;&#31034;&#30340;&#24605;&#32500;&#38142;&#65288;SP-CoT&#65289;&#26694;&#26550;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#25512;&#29702;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;CoTs&#65292;&#25193;&#23637;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13552</link><description>&lt;p&gt;
&#33258;&#25105;&#21551;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning. (arXiv:2310.13552v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#21551;&#31034;&#30340;&#24605;&#32500;&#38142;&#65288;SP-CoT&#65289;&#26694;&#26550;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#25512;&#29702;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;CoTs&#65292;&#25193;&#23637;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#20013;&#65292;&#22823;&#22810;&#25968;&#38382;&#39064;&#38656;&#35201;&#23545;&#24120;&#35782;&#36827;&#34892;&#21333;&#36339;&#25512;&#29702;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25193;&#23637;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#27491;&#24335;&#24341;&#20837;&#20102;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#25512;&#29702;&#65288;ODMR&#65289;&#65292;&#36890;&#36807;&#22312;&#24320;&#25918;&#39046;&#22495;&#35774;&#32622;&#20013;&#22238;&#31572;&#22810;&#36339;&#38382;&#39064;&#24182;&#25552;&#20379;&#26126;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26080;&#38656;&#22806;&#37096;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#22312;&#20419;&#36827;ODQA&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#36890;&#36807;&#25163;&#21160;&#25110;&#33258;&#21160;&#21270;&#33539;&#20363;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#32570;&#20047;&#36136;&#37327;&#20445;&#35777;&#65292;&#32780;&#25163;&#21160;&#26041;&#27861;&#21463;&#21040;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#21551;&#31034;&#30340;&#24605;&#32500;&#38142;&#65288;SP-CoT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;LLMs&#24182;&#20026;LLMs&#22823;&#35268;&#27169;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;CoTs&#12290;
&lt;/p&gt;
&lt;p&gt;
In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently, large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts the reasoning capability of LLMs to a greater extent with manual or automated paradigms. However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs. In this paper, we propose Self-prompted Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#37197;&#22791;&#19978;&#19979;&#25991;&#20449;&#24687;&#21518;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24615;&#23384;&#22312;&#19968;&#23450;&#24046;&#24322;&#65292;&#22240;&#27492;&#22312;&#20351;&#29992;&#20013;&#38656;&#35201;&#35880;&#24910;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2310.13549</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#21361;&#38505;&#19982;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Perils &amp; Promises of Fact-checking with Large Language Models. (arXiv:2310.13549v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#37197;&#22791;&#19978;&#19979;&#25991;&#20449;&#24687;&#21518;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24615;&#23384;&#22312;&#19968;&#23450;&#24046;&#24322;&#65292;&#22240;&#27492;&#22312;&#20351;&#29992;&#20013;&#38656;&#35201;&#35880;&#24910;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20107;&#23454;&#26816;&#26597;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39564;&#35777;&#35770;&#26029;&#65292;&#22312;&#34394;&#20551;&#20449;&#24687;&#36229;&#20986;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#34987;&#20449;&#20219;&#65292;&#21487;&#20197;&#39564;&#35777;&#20449;&#24687;&#12289;&#25776;&#20889;&#23398;&#26415;&#35770;&#25991;&#12289;&#27861;&#24459;&#35785;&#35772;&#21644;&#26032;&#38395;&#25991;&#31456;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#21306;&#20998;&#30495;&#23454;&#19982;&#34394;&#20551;&#20197;&#21450;&#39564;&#35777;&#20854;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20154;&#25552;&#20986;&#26597;&#35810;&#12289;&#26816;&#32034;&#19978;&#19979;&#25991;&#25968;&#25454;&#21644;&#20570;&#20986;&#20915;&#31574;&#26469;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#20351;&#29992;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20195;&#29702;&#20154;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#24182;&#24341;&#29992;&#26816;&#32034;&#21040;&#30340;&#30456;&#20851;&#26469;&#28304;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#37197;&#22791;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#26102;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#22686;&#24378;&#12290;GPT-4&#20248;&#20110;GPT-3&#65292;&#20294;&#20934;&#30830;&#24615;&#22240;&#26597;&#35810;&#35821;&#35328;&#21644;&#35770;&#26029;&#30495;&#23454;&#24615;&#32780;&#24322;&#12290;&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26816;&#26597;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20934;&#30830;&#24615;&#19981;&#19968;&#33268;&#65292;&#24517;&#39035;&#35880;&#24910;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#20419;&#36827;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper compr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.13548</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12301;&#26159;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;RLHF&#21487;&#33021;&#20250;&#40723;&#21169;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#20449;&#24565;&#30456;&#31526;&#30340;&#22238;&#31572;&#26469;&#20195;&#26367;&#30495;&#23454;&#22238;&#31572;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#35844;&#23194;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#35757;&#32451;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#20197;&#21450;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#26159;&#21542;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;AI&#21161;&#25163;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#30001;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#35844;&#23194;&#34892;&#20026;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;&#26159;&#21542;&#39537;&#21160;&#20102;RLHF&#27169;&#22411;&#30340;&#36825;&#31181;&#24191;&#27867;&#34892;&#20026;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#22238;&#31572;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#30456;&#31526;&#26102;&#65292;&#23427;&#26356;&#26377;&#21487;&#33021;&#34987;&#36873;&#20013;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#20559;&#22909;&#27169;&#22411;&#65288;PMs&#65289;&#23558;&#26377;&#35828;&#26381;&#21147;&#30340;&#35844;&#23194;&#22238;&#31572;&#19982;&#27491;&#30830;&#22238;&#31572;&#30456;&#27604;&#65292;&#26377;&#26102;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#22320;&#36873;&#25321;&#20102;&#35844;&#23194;&#22238;&#31572;&#12290;&#20248;&#21270;&#27169;&#22411;&#36755;&#20986;&#20197;&#28385;&#36275;PMs&#26377;&#26102;&#20063;&#20250;&#22312;&#30495;&#23454;&#24615;&#21644;&#35844;&#23194;&#34892;&#20026;&#20043;&#38388;&#20570;&#20986;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;&#20250;&#21463;&#21040;&#31995;&#32479;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#25968;&#38169;&#35823;&#39044;&#27979;&#65292;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#34920;&#29616;&#20063;&#20250;&#21463;&#21040;&#30772;&#22351;&#65292;&#24674;&#22797;&#26102;&#38388;&#32531;&#24930;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#26657;&#20934;&#38169;&#35823;&#23545;&#29992;&#25143;&#20449;&#20219;&#26377;&#19981;&#21516;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13544</link><description>&lt;p&gt;
&#29992;&#25143;&#22312;&#19981;&#30830;&#23450;&#24773;&#20917;&#19979;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;&#30340;&#21382;&#26102;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Diachronic Perspective on User Trust in AI under Uncertainty. (arXiv:2310.13544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13544
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;&#20250;&#21463;&#21040;&#31995;&#32479;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#25968;&#38169;&#35823;&#39044;&#27979;&#65292;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#34920;&#29616;&#20063;&#20250;&#21463;&#21040;&#30772;&#22351;&#65292;&#24674;&#22797;&#26102;&#38388;&#32531;&#24930;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#26657;&#20934;&#38169;&#35823;&#23545;&#29992;&#25143;&#20449;&#20219;&#26377;&#19981;&#21516;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#20013;&#65292;&#29992;&#25143;&#22522;&#20110;&#20854;&#21487;&#38752;&#24615;&#21644;&#20915;&#31574;&#21576;&#29616;&#26041;&#24335;&#24314;&#31435;&#20102;&#23545;AI&#31995;&#32479;&#30340;&#24515;&#29702;&#27169;&#22411;&#65292;&#20363;&#22914;&#31995;&#32479;&#32622;&#20449;&#24230;&#30340;&#23637;&#31034;&#21644;&#36755;&#20986;&#30340;&#35299;&#37322;&#12290;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#24448;&#24448;&#26410;&#32463;&#26657;&#20934;&#65292;&#23548;&#33268;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#30772;&#22351;&#20102;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#20026;&#20102;&#24314;&#31435;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#29992;&#25143;&#20449;&#20219;&#26159;&#22914;&#20309;&#24418;&#25104;&#30340;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#28508;&#22312;&#30772;&#22351;&#20449;&#20219;&#30340;&#20107;&#20214;&#21518;&#37325;&#26032;&#33719;&#24471;&#20449;&#20219;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21338;&#24328;&#28216;&#25103;&#30740;&#31350;&#20102;&#29992;&#25143;&#20449;&#20219;&#22312;&#38754;&#23545;&#36825;&#20123;&#30772;&#22351;&#20449;&#20219;&#20107;&#20214;&#26102;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#20960;&#20010;&#38169;&#35823;&#23454;&#20363;&#21644;&#19981;&#20934;&#30830;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#20063;&#20250;&#30772;&#22351;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#34920;&#29616;&#65292;&#24182;&#19988;&#24674;&#22797;&#36807;&#31243;&#38750;&#24120;&#32531;&#24930;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20449;&#20219;&#30340;&#38477;&#32423;&#20250;&#38477;&#20302;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#19981;&#21516;&#31867;&#22411;&#30340;&#26657;&#20934;&#38169;&#35823;&#65292;&#19981;&#33258;&#20449;&#20294;&#27491;&#30830;&#21644;&#33258;&#20449;&#20294;&#38169;&#35823;&#65292;&#20250;&#23545;&#29992;&#25143;&#20449;&#20219;&#20135;&#29983;&#19981;&#21516;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a human-AI collaboration, users build a mental model of the AI system based on its reliability and how it presents its decision, e.g. its presentation of system confidence and an explanation of the output. Modern NLP systems are often uncalibrated, resulting in confidently incorrect predictions that undermine user trust. In order to build trustworthy AI, we must understand how user trust is developed and how it can be regained after potential trust-eroding events. We study the evolution of user trust in response to these trust-eroding events using a betting game. We find that even a few incorrect instances with inaccurate confidence estimates damage user trust and performance, with very slow recovery. We also show that this degradation in trust reduces the success of human-AI collaboration and that different types of miscalibration -- unconfidently correct and confidently incorrect -- have different negative effects on user trust. Our findings highlight the importance of calibration
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#21463;&#25511;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#21644;&#25991;&#26412;&#25688;&#35201;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13526</link><description>&lt;p&gt;
&#21463;&#25511;&#38543;&#26426;&#24615;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Controlled Randomness Improves the Performance of Transformer Models. (arXiv:2310.13526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#21463;&#25511;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#21644;&#25991;&#26412;&#25688;&#35201;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#26469;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#27492;&#30456;&#21453;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#37327;&#24448;&#24448;&#36828;&#36828;&#19981;&#21450;&#19978;&#36848;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#25511;&#38543;&#26426;&#24615;&#65292;&#21363;&#22122;&#22768;&#65292;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#32034;&#30446;&#26631;&#22122;&#22768;&#20197;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#28155;&#21152;&#36825;&#26679;&#30340;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#30340;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#32852;&#21512;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#65292;&#20197;&#21450;&#25991;&#26412;&#25688;&#35201;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the pre-training step of natural language models, the main objective is to learn a general representation of the pre-training dataset, usually requiring large amounts of textual data to capture the complexity and diversity of natural language. Contrasting this, in most cases, the size of the data available to solve the specific downstream task is often dwarfed by the aforementioned pre-training dataset, especially in domains where data is scarce. We introduce controlled randomness, i.e. noise, into the training process to improve fine-tuning language models and explore the performance of targeted noise in addition to the parameters of these models. We find that adding such noise can improve the performance in our two downstream tasks of joint named entity recognition and relation extraction and text summarization.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#28436;&#31034;&#30340;&#26041;&#24335;&#65292;&#25945;&#23548;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#33258;&#25105;&#25552;&#21319;&#33021;&#21147;&#65292;&#20943;&#23567;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#19982;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.13522</link><description>&lt;p&gt;
&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20114;&#21160;&#28436;&#31034;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Teaching Language Models to Self-Improve through Interactive Demonstrations. (arXiv:2310.13522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13522
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#28436;&#31034;&#30340;&#26041;&#24335;&#65292;&#25945;&#23548;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#33258;&#25105;&#25552;&#21319;&#33021;&#21147;&#65292;&#20943;&#23567;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#19982;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#25552;&#31034;&#20854;&#20998;&#26512;&#21644;&#20462;&#35746;&#33258;&#24049;&#30340;&#36755;&#20986;&#26469;&#23454;&#29616;&#33258;&#25105;&#25552;&#21319;&#30340;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#22312;&#30740;&#31350;&#20013;&#24341;&#36215;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26159;&#32570;&#22833;&#19988;&#38590;&#20197;&#23398;&#20064;&#30340;&#65292;&#20174;&#32780;&#25193;&#22823;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#19982;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#19988;&#36895;&#24230;&#26356;&#24555;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#20943;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TriPosT&#65292;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#36171;&#20104;&#36739;&#23567;&#30340;&#27169;&#22411;&#36825;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;LLaMA-7b&#22312;&#25968;&#23398;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;7.13%&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#19982;LLM&#36827;&#34892;&#20114;&#21160;&#20197;&#25910;&#38598;&#21453;&#39304;&#21644;&#25913;&#36827;&#33258;&#36523;&#29983;&#25104;&#30340;&#32467;&#26524;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#25773;&#36825;&#19968;&#32463;&#39564;&#26469;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#23398;&#21644;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;&#24182;&#32416;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#20013;&#36827;&#34892;&#20114;&#21160;&#23398;&#20064;&#30340;&#32463;&#39564;&#23545;&#20110;&#23567;&#22411;&#27169;&#22411;&#30340;&#25552;&#21319;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-improving ability of large language models (LLMs), enabled by prompting them to analyze and revise their own outputs, has garnered significant interest in recent research. However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more cost-effective and faster ones. To reduce this gap, we introduce TriPosT, a training algorithm that endows smaller models with such self-improvement ability, and show that our approach can improve a LLaMA-7b's performance on math and reasoning tasks by up to 7.13%. In contrast to prior work, we achieve this by using the smaller model to interact with LLMs to collect feedback and improvements on its own generations. We then replay this experience to train the small model. Our experiments on four math and reasoning datasets show that the interactive experience of learning from and correcting its own mistakes is crucial for small models to improve 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SpanEx&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#20154;&#31867;&#26631;&#35760;&#33539;&#22260;&#20132;&#20114;&#35299;&#37322;&#30340;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#35299;&#37322;&#26041;&#27861;&#24448;&#24448;&#21482;&#20851;&#27880;&#30456;&#37051;&#26631;&#35760;&#25110;&#20803;&#32452;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#32570;&#20047;&#25429;&#25417;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20013;&#24517;&#35201;&#20132;&#20114;&#30340;&#27880;&#37322;&#12290;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20316;&#32773;&#21457;&#29616;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#26816;&#27979;&#30340;&#20803;&#24314;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13506</link><description>&lt;p&gt;
&#35299;&#37322;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Explaining Interactions Between Text Spans. (arXiv:2310.13506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SpanEx&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#20154;&#31867;&#26631;&#35760;&#33539;&#22260;&#20132;&#20114;&#35299;&#37322;&#30340;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#35299;&#37322;&#26041;&#27861;&#24448;&#24448;&#21482;&#20851;&#27880;&#30456;&#37051;&#26631;&#35760;&#25110;&#20803;&#32452;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#32570;&#20047;&#25429;&#25417;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20013;&#24517;&#35201;&#20132;&#20114;&#30340;&#27880;&#37322;&#12290;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20316;&#32773;&#21457;&#29616;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#26816;&#27979;&#30340;&#20803;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#26469;&#33258;&#36755;&#20837;&#30340;&#19981;&#21516;&#37096;&#20998;&#30340;&#26631;&#35760;&#33539;&#22260;&#36827;&#34892;&#25512;&#29702;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#20107;&#23454;&#26680;&#26597;&#65288;FC&#65289;&#12289;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#25110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#20142;&#30340;&#35299;&#37322;&#20027;&#35201;&#38598;&#20013;&#22312;&#35782;&#21035;&#20010;&#21035;&#37325;&#35201;&#26631;&#35760;&#25110;&#20165;&#22312;&#30456;&#37051;&#26631;&#35760;&#25110;&#26631;&#35760;&#20803;&#32452;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#32570;&#20047;&#25429;&#25417;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20013;&#24517;&#35201;&#20132;&#20114;&#30340;&#27880;&#37322;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SpanEx&#65292;&#19968;&#20010;&#20851;&#20110;NLI&#21644;FC&#30340;&#20154;&#31867;&#26631;&#35760;&#33539;&#22260;&#20132;&#20114;&#35299;&#37322;&#30340;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36755;&#20837;&#30340;&#19981;&#21516;&#37096;&#20998;&#20043;&#38388;&#20351;&#29992;&#30340;&#36830;&#25509;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#36827;&#34892;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#20803;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important tokens or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process w.r.t. the necessary interactions for informed decision-making in such tasks. To bridge this gap, we introduce SpanEx, a multi-annotator dataset of human span interaction explanations for two NLU tasks: NLI and FC. We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes. Finally, we present a novel community detection based unsupervised met
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13505</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#25913;&#20889;&#29983;&#25104;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#30340;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#38382;&#31572;&#65288;ConvQA&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#40644;&#37329;QA&#23545;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20165;&#38480;&#20110;&#22312;&#30456;&#24212;&#25968;&#25454;&#38598;&#20013;&#35265;&#21040;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#35780;&#20272;&#20165;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;REIGN&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20960;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#30340;&#23398;&#20064;&#35774;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;ConvQA&#27169;&#22411;&#24341;&#23548;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#21482;&#25552;&#20379;&#37027;&#20123;&#26377;&#21161;&#20110;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#30340;&#25913;&#20889;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#22522;&#20934;&#19978;&#35757;&#32451;&#20027;&#35201;&#27169;&#22411;&#32452;&#20214;&#24182;&#23558;&#20854;&#38646;-shot&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#21644;&#37325;&#26032;&#37197;&#32622;&#21021;&#22987;&#30340;&#25913;&#20889;&#12289;&#27979;&#35797;&#35821;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#20102;&#27169;&#25311;&#27604;&#20363;&#22312;&#21019;&#36896;&#21147;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#35789;&#23884;&#20837;&#21487;&#20197;&#26356;&#22909;&#22320;&#22522;&#20110;&#27169;&#25311;&#27604;&#20363;&#25552;&#20986;&#26032;&#30340;&#21160;&#29289;&#12290;</title><link>http://arxiv.org/abs/2310.13500</link><description>&lt;p&gt;
&#27169;&#25311;&#27604;&#20363;&#19982;&#21019;&#36896;&#21147;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Analogical Proportions and Creativity: A Preliminary Study. (arXiv:2310.13500v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#20102;&#27169;&#25311;&#27604;&#20363;&#22312;&#21019;&#36896;&#21147;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#35789;&#23884;&#20837;&#21487;&#20197;&#26356;&#22909;&#22320;&#22522;&#20110;&#27169;&#25311;&#27604;&#20363;&#25552;&#20986;&#26032;&#30340;&#21160;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#27604;&#20363;&#26159;&#24418;&#24335;&#20026;&#8220;$a$&#23545;$b$&#22914;&#21516;$c$&#23545;$d$&#8221;&#30340;&#38472;&#36848;&#65292;&#34920;&#36798;&#20102;&#20803;&#32032;&#23545;$(a,b)$&#21644;&#23545;$(c,d)$&#30340;&#27604;&#36739;&#24471;&#20986;&#30456;&#20284;&#32467;&#26524;&#12290;&#27169;&#25311;&#27604;&#20363;&#26159;&#21019;&#36896;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#32473;&#23450;3&#20010;&#19981;&#21516;&#30340;&#39033;&#30446;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26681;&#25454;&#19968;&#23450;&#26465;&#20214;&#35745;&#31639;&#20986;&#19982;&#20043;&#21069;&#30340;&#39033;&#30446;&#19981;&#21516;&#30340;&#31532;4&#20010;&#39033;&#30446;$d$&#30340;&#34920;&#31034;&#65292;&#20351;&#20854;&#19982;&#20043;&#24418;&#25104;&#27169;&#25311;&#27604;&#20363;&#12290;&#22312;&#20171;&#32461;&#27169;&#25311;&#27604;&#20363;&#21450;&#20854;&#29305;&#24615;&#21518;&#65292;&#35770;&#25991;&#25253;&#21578;&#20102;&#20351;&#29992;&#21160;&#29289;&#25551;&#36848;&#21644;&#31867;&#21035;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35797;&#22270;&#20174;&#29616;&#26377;&#21160;&#29289;&#20013;&#8220;&#21019;&#36896;&#8221;&#26032;&#30340;&#21160;&#29289;&#65292;&#22914;&#40493;&#22068;&#20861;&#31561;&#32597;&#35265;&#21160;&#29289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#65292;&#20351;&#29992;&#35789;&#23884;&#20837;&#21644;&#24067;&#23572;&#29305;&#24449;&#26469;&#22522;&#20110;&#27169;&#25311;&#27604;&#20363;&#25552;&#20986;&#26032;&#30340;&#21160;&#29289;&#65292;&#32467;&#26524;&#34920;&#26126;&#35789;&#23884;&#20837;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical proportions are statements of the form "$a$ is to $b$ as $c$ is to $d$", which expresses that the comparisons of the elements in pair $(a, b)$ and in pair $(c, d)$ yield similar results. Analogical proportions are creative in the sense that given 3 distinct items, the representation of a 4th item $d$, distinct from the previous items, which forms an analogical proportion with them can be calculated, provided certain conditions are met. After providing an introduction to analogical proportions and their properties, the paper reports the results of an experiment made with a database of animal descriptions and their class, where we try to "create" new animals from existing ones, retrieving rare animals such as platypus. We perform a series of experiments using word embeddings as well as Boolean features in order to propose novel animals based on analogical proportions, showing that word embeddings obtain better results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DistillCSE&#26694;&#26550;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#21319;&#21477;&#23376;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#24179;&#22343;&#25945;&#24072;&#27169;&#22411;&#30340;&#26631;&#31614;&#65292;&#25913;&#36827;&#20102;&#26631;&#20934;&#30693;&#35782;&#33976;&#39311;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13499</link><description>&lt;p&gt;
DistillCSE: &#33976;&#39311;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
DistillCSE: Distilled Contrastive Learning for Sentence Embeddings. (arXiv:2310.13499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DistillCSE&#26694;&#26550;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#21319;&#21477;&#23376;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#24179;&#22343;&#25945;&#24072;&#27169;&#22411;&#30340;&#26631;&#31614;&#65292;&#25913;&#36827;&#20102;&#26631;&#20934;&#30693;&#35782;&#33976;&#39311;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DistillCSE&#26694;&#26550;&#65292;&#23427;&#22312;&#33258;&#25105;&#35757;&#32451;&#33539;&#24335;&#19979;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#24182;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#12290;DistillCSE&#30340;&#28508;&#22312;&#20248;&#21183;&#26159;&#20854;&#33258;&#25105;&#22686;&#24378;&#29305;&#24615;&#65306;&#36890;&#36807;&#20351;&#29992;&#22522;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#21487;&#20197;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23398;&#20064;&#21040;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20005;&#37325;&#36807;&#25311;&#21512;&#65292;&#36890;&#36807;&#26631;&#20934;&#30340;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#30340;&#26222;&#36890;DistillCSE&#21482;&#33021;&#21462;&#24471;&#36793;&#38469;&#30340;&#25913;&#36827;&#12290;&#36827;&#19968;&#27493;&#30340;&#23450;&#37327;&#20998;&#26512;&#26174;&#31034;&#65292;&#26631;&#20934;&#30693;&#35782;&#33976;&#39311;&#30001;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26412;&#36136;&#65292;&#23548;&#33268;&#20102;&#25945;&#24072;&#27169;&#22411;&#30340;&#26631;&#31614;&#30340;&#30456;&#23545;&#22823;&#30340;&#26041;&#24046;&#12290;&#20026;&#20102;&#32531;&#35299;&#39640;&#26041;&#24046;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30693;&#35782;&#33976;&#39311;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;Group-P&#38543;&#26426;&#31574;&#30053;&#20316;&#20026;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#20174;&#22810;&#20010;&#25945;&#24072;&#32452;&#20214;&#24179;&#22343;&#26631;&#31614;&#30340;&#26041;&#27861;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the DistillCSE framework, which performs contrastive learning under the self-training paradigm with knowledge distillation. The potential advantage of DistillCSE is its self-enhancing feature: using a base model to provide additional supervision signals, a stronger model may be learned through knowledge distillation. However, the vanilla DistillCSE through the standard implementation of knowledge distillation only achieves marginal improvements due to severe overfitting. The further quantitative analyses demonstrate the reason that the standard knowledge distillation exhibits a relatively large variance of the teacher model's logits due to the essence of contrastive learning. To mitigate the issue induced by high variance, this paper accordingly proposed two simple yet effective solutions for knowledge distillation: a Group-P shuffling strategy as an implicit regularization and the averaging logits from multiple teacher components. Experiments on standard benchmarks
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#23548;&#33268;&#39044;&#27979;&#19981;&#31283;&#23450;&#21644;&#19981;&#19968;&#33268;&#30340;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#19981;&#21516;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#30830;&#23450;&#20102;&#23545;&#39044;&#27979;&#24433;&#21709;&#26368;&#22823;&#12289;&#20855;&#26377;&#20132;&#20114;&#24615;&#25110;&#31283;&#23450;&#24615;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.13486</link><description>&lt;p&gt;
&#30041;&#24847;&#25351;&#20196;&#65306;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#19968;&#33268;&#24615;&#21644;&#20132;&#20114;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning. (arXiv:2310.13486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13486
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#23548;&#33268;&#39044;&#27979;&#19981;&#31283;&#23450;&#21644;&#19981;&#19968;&#33268;&#30340;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#19981;&#21516;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#30830;&#23450;&#20102;&#23545;&#39044;&#27979;&#24433;&#21709;&#26368;&#22823;&#12289;&#20855;&#26377;&#20132;&#20114;&#24615;&#25110;&#31283;&#23450;&#24615;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23547;&#25214;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#21040;&#20219;&#21153;&#30340;&#26368;&#20339;&#26041;&#24335;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#19982;&#20808;&#21069;&#19968;&#20195;&#30340;&#20219;&#21153;&#35843;&#25972;&#27169;&#22411;&#65288;TT&#65289;&#31867;&#20284;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36866;&#24212;&#20219;&#21153;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#35774;&#32622;&#19979;&#26159;&#20581;&#22766;&#30340;&#65292;&#20294;&#22312;&#20854;&#20182;&#35774;&#32622;&#19979;&#19981;&#26159;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#23548;&#33268;&#32447;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39044;&#27979;&#19981;&#31283;&#23450;&#21644;&#19981;&#19968;&#33268;&#30340;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36755;&#20837;&#20998;&#24067;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#20266;&#30456;&#20851;&#24615;&#22312;&#25552;&#31034;&#27169;&#22411;&#20013;&#21482;&#26159;&#19968;&#20010;&#27425;&#35201;&#38382;&#39064;&#65292;&#32780;&#23545;&#20110;TT&#27169;&#22411;&#26469;&#35828;&#26159;&#24050;&#30693;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#25552;&#31034;&#35774;&#32622;&#20013;&#24433;&#21709;&#39044;&#27979;&#30340;&#19981;&#21516;&#22240;&#32032;&#30340;&#31995;&#32479;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#21407;&#22987;&#21644;&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;LLMs&#19978;&#27979;&#35797;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#22240;&#32032;&#32452;&#21512;&#65292;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23637;&#31034;&#21738;&#20123;&#22240;&#32032;&#26368;&#20855;&#24433;&#21709;&#21147;&#12289;&#20132;&#20114;&#24615;&#25110;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#21738;&#20123;&#22240;&#32032;&#21487;&#20197;&#22312;&#19981;&#21152;&#39044;&#38450;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#65292;&#21738;&#20123;&#22240;&#32032;&#38656;&#35201;&#39044;&#38450;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of task-tuned models (TT), models that are adapted to tasks via in-context-learning (ICL) are robust in some setups but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels -- a known issue in TT models -- form only a minor problem for prompted models. Then, we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instruction-tuned (IT) LLMs of different scale and statistically analyse the results to show which factors are the most influential, interactive or stable. Our results show which factors can be used without precautions and whi
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#22122;&#22768;&#36755;&#20837;&#65292;&#36890;&#36807;&#20174;MTNT&#25968;&#25454;&#38598;&#20013;&#28165;&#29702;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#29983;&#25104;&#20102;C-MTNT&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2310.13469</link><description>&lt;p&gt;
&#35753;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#24744;&#30340;&#26377;&#22122;&#38899;&#30340;&#32763;&#35793;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Ask Language Model to Clean Your Noisy Translation Data. (arXiv:2310.13469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13469
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#22122;&#22768;&#36755;&#20837;&#65292;&#36890;&#36807;&#20174;MTNT&#25968;&#25454;&#38598;&#20013;&#28165;&#29702;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#29983;&#25104;&#20102;C-MTNT&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#22122;&#22768;&#36755;&#20837;&#30340;&#33030;&#24369;&#24615;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20174;&#22122;&#22768;&#36755;&#20837;&#20013;&#29983;&#25104;&#24178;&#20928;&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;MTNT&#25968;&#25454;&#38598;&#34987;&#24191;&#27867;&#29992;&#20316;&#35780;&#20272;NMT&#27169;&#22411;&#23545;&#22122;&#22768;&#36755;&#20837;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28304;&#35821;&#21477;&#21644;&#30446;&#26631;&#35821;&#21477;&#20013;&#37117;&#23384;&#22312;&#22122;&#22768;&#65292;&#20854;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28165;&#29702;MTNT&#20013;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#22122;&#22768;&#35780;&#20272;&#30340;&#22522;&#20934;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#21435;&#22122;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#32771;&#34385;&#35821;&#20041;&#21547;&#20041;&#30340;&#21516;&#26102;&#21024;&#38500;&#34920;&#24773;&#31526;&#21495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#25913;&#20442;&#35821;&#12289;&#26415;&#35821;&#21644;&#31895;&#21475;&#12290;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;C-MTNT&#65292;&#22122;&#22768;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset \cite{MTNT} is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this limitation, we focus on cleaning the noise from the target sentences in MTNT, making it more suitable as a benchmark for noise evaluation. Leveraging the capabilities of large language models (LLMs), we observe their impressive abilities in noise removal. For example, they can remove emojis while considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities. The resulting datasets, called C-MTNT, exhibit significantly less noise 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36866;&#37197;&#22120;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#35757;&#32451;&#21442;&#25968;&#30340;&#21516;&#26102;&#21305;&#37197;&#20256;&#32479;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#24335;&#65292;&#24182;&#28040;&#38500;&#20102;&#21518;&#22788;&#29702;&#25110;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.13448</link><description>&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning. (arXiv:2310.13448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36866;&#37197;&#22120;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#35757;&#32451;&#21442;&#25968;&#30340;&#21516;&#26102;&#21305;&#37197;&#20256;&#32479;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#24335;&#65292;&#24182;&#28040;&#38500;&#20102;&#21518;&#22788;&#29702;&#25110;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#26377;&#30528;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23384;&#22312;&#38382;&#39064;&#65306;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#39640;&#24230;&#20381;&#36182;&#20110;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#36873;&#25321;&#65292;&#24182;&#19988;&#30001;&#20110;&#36807;&#24230;&#29983;&#25104;&#32780;&#32463;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#21518;&#22788;&#29702;&#12290;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#65292;&#22914;&#22312;&#32763;&#35793;&#25351;&#20196;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#35745;&#31639;&#36127;&#33655;&#24456;&#22823;&#65292;&#24182;&#21487;&#33021;&#21066;&#24369;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#23548;&#33268;&#36807;&#24230;&#19987;&#38376;&#21270;&#12290;&#26412;&#25991;&#25552;&#20379;&#23545;&#27492;&#38382;&#39064;&#30340;&#26356;&#35814;&#32454;&#20171;&#32461;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20351;&#29992;LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#24494;&#35843;&#19982;&#20256;&#32479;&#24494;&#35843;&#30340;&#24615;&#33021;&#21305;&#37197;&#65292;&#21516;&#26102;&#23558;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;50&#20493;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#20248;&#20110;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#24335;&#65292;&#24182;&#28040;&#38500;&#20102;&#21518;&#22788;&#29702;&#25110;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#36890;&#24120;&#20250;&#38477;&#20302;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#33719;&#24471;&#20004;&#32773;&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#36866;&#37197;&#22120;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are a promising avenue for machine translation (MT). However, current LLM-based MT systems are brittle: their effectiveness highly depends on the choice of few-shot examples and they often require extra post-processing due to overgeneration. Alternatives such as finetuning on translation instructions are computationally expensive and may weaken in-context learning capabilities, due to overspecialization. In this paper, we provide a closer look at this problem. We start by showing that adapter-based finetuning with LoRA matches the performance of traditional finetuning while reducing the number of training parameters by a factor of 50. This method also outperforms few-shot prompting and eliminates the need for post-processing or in-context examples. However, we show that finetuning generally degrades few-shot performance, hindering adaptation capabilities. Finally, to obtain the best of both worlds, we propose a simple approach that incorporates few-shot exa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.13447</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation. (arXiv:2310.13447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#65292;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20851;&#38190;&#22312;&#20110;&#24314;&#31435;&#19968;&#20010;&#33391;&#22909;&#30340;&#23545;&#40784;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#20041;&#34920;&#24449;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#24403;&#21069;&#22522;&#20110;&#20687;&#32032;&#25110;&#22359;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#25552;&#21462;&#22797;&#26434;&#22330;&#26223;&#36793;&#30028;&#26041;&#38754;&#23384;&#22312;&#31354;&#38388;&#35821;&#20041;&#36830;&#36143;&#24615;&#19981;&#36275;&#21644;&#23545;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#23558;&#36229;&#20687;&#32032;&#20316;&#20026;&#21487;&#23398;&#20064;&#22270;&#20687;&#25968;&#25454;&#30340;&#32508;&#21512;&#32039;&#20945;&#34920;&#24449;&#65292;&#36890;&#36807;&#23545;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#36827;&#34892;&#32858;&#31867;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#12290;&#20026;&#20102;&#25366;&#25496;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#12290;&#23427;&#23558;&#25972;&#20010;&#22270;&#20687;&#35299;&#26512;&#20026;&#32454;&#21040;&#31895;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25972;&#20010;&#22270;&#20687;&#30340;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the multimodal field, the key to integrating vision and language lies in establishing a good alignment strategy. Recently, benefiting from the success of self-supervised learning, significant progress has been made in multimodal semantic representation based on pre-trained models for vision and language. However, there is still room for improvement in visual semantic representation. The lack of spatial semantic coherence and vulnerability to noise makes it challenging for current pixel or patch-based methods to accurately extract complex scene boundaries. To this end, this paper develops superpixel as a comprehensive compact representation of learnable image data, which effectively reduces the number of visual primitives for subsequent processing by clustering perceptually similar pixels. To mine more precise topological relations, we propose a Multiscale Difference Graph Convolutional Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical structure of cons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#35821;&#35328;&#31867;&#22411;&#25968;&#25454;&#24211;&#30340;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#12290;&#21457;&#29616;&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#31867;&#22411;&#25968;&#25454;&#24211;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#25345;&#32493;&#30340;&#35821;&#35328;&#31867;&#22411;&#29305;&#24449;&#35266;&#28857;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35748;&#20026;&#36825;&#26679;&#30340;&#35266;&#28857;&#22312;&#26410;&#26469;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#31232;&#32570;&#22330;&#26223;&#19979;&#30340;&#35821;&#35328;&#24314;&#27169;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.13440</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#35821;&#35328;&#31867;&#22411;&#25968;&#25454;&#24211;&#30340;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
The Past, Present, and Future of Typological Databases in NLP. (arXiv:2310.13440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#35821;&#35328;&#31867;&#22411;&#25968;&#25454;&#24211;&#30340;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#12290;&#21457;&#29616;&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#31867;&#22411;&#25968;&#25454;&#24211;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#25345;&#32493;&#30340;&#35821;&#35328;&#31867;&#22411;&#29305;&#24449;&#35266;&#28857;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35748;&#20026;&#36825;&#26679;&#30340;&#35266;&#28857;&#22312;&#26410;&#26469;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#31232;&#32570;&#22330;&#26223;&#19979;&#30340;&#35821;&#35328;&#24314;&#27169;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#31867;&#22411;&#20449;&#24687;&#22312;NLP&#27169;&#22411;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#31867;&#22411;&#25968;&#25454;&#24211;&#65292;&#23588;&#20854;&#26159;WALS&#21644;Grambank&#65292;&#22312;&#24444;&#27492;&#20043;&#38388;&#20197;&#21450;&#19982;&#20854;&#20182;&#35821;&#35328;&#31867;&#22411;&#20449;&#24687;&#26469;&#28304;&#65288;&#22914;&#35821;&#35328;&#35821;&#27861;&#65289;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#19968;&#20123;&#19981;&#19968;&#33268;&#24615;&#28304;&#20110;&#32534;&#30721;&#38169;&#35823;&#25110;&#35821;&#35328;&#21464;&#24322;&#65292;&#20294;&#35768;&#22810;&#20998;&#27495;&#26159;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#24211;&#30340;&#31163;&#25955;&#20998;&#31867;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#25506;&#35752;&#35821;&#35328;&#31867;&#22411;&#25968;&#25454;&#24211;&#21644;&#36164;&#28304;&#38388;&#30340;&#20998;&#27495;&#21450;&#20854;&#22312;NLP&#20013;&#30340;&#20351;&#29992;&#65292;&#38416;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#26679;&#30340;&#24037;&#20316;&#30340;&#26410;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35770;&#28857;&#65292;&#21363;&#25345;&#32493;&#30340;&#35821;&#35328;&#31867;&#22411;&#29305;&#24449;&#35266;&#28857;&#26126;&#26174;&#26377;&#30410;&#65292;&#21628;&#24212;&#20102;&#35821;&#35328;&#23398;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#31867;&#22411;&#35266;&#28857;&#22312;&#26410;&#26469;&#20855;&#26377;&#37325;&#22823;&#28508;&#21147;&#65292;&#21253;&#25324;&#22312;&#36164;&#28304;&#31232;&#32570;&#22330;&#26223;&#19979;&#30340;&#35821;&#35328;&#24314;&#27169;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typological information has the potential to be beneficial in the development of NLP models, particularly for low-resource languages. Unfortunately, current large-scale typological databases, notably WALS and Grambank, are inconsistent both with each other and with other sources of typological information, such as linguistic grammars. Some of these inconsistencies stem from coding errors or linguistic variation, but many of the disagreements are due to the discrete categorical nature of these databases. We shed light on this issue by systematically exploring disagreements across typological databases and resources, and their uses in NLP, covering the past and present. We next investigate the future of such work, offering an argument that a continuous view of typological features is clearly beneficial, echoing recommendations from linguistics. We propose that such a view of typology has significant potential in the future, including in language modeling in low-resource scenarios.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#26865;&#20004;&#21487;&#38382;&#39064;&#20013;&#30340;&#33258;&#19968;&#33268;&#24615;&#23384;&#22312;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#29305;&#21035;&#35757;&#32451;&#20063;&#33021;&#22312;&#31283;&#20581;&#24615;&#26816;&#26597;&#20013;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13439</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#26865;&#20004;&#21487;&#24773;&#20917;&#19979;&#30340;&#33258;&#19968;&#33268;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-Consistency of Large Language Models under Ambiguity. (arXiv:2310.13439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13439
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#26865;&#20004;&#21487;&#38382;&#39064;&#20013;&#30340;&#33258;&#19968;&#33268;&#24615;&#23384;&#22312;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#29305;&#21035;&#35757;&#32451;&#20063;&#33021;&#22312;&#31283;&#20581;&#24615;&#26816;&#26597;&#20013;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#35201;&#19968;&#33268;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#19979;&#32473;&#20986;&#19981;&#19968;&#33268;&#30340;&#31572;&#26696;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#20363;&#22914;&#38382;&#31572;&#12289;&#35299;&#37322;&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#19968;&#33268;&#24615;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;&#23384;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#27491;&#30830;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;OpenAI&#27169;&#22411;&#22871;&#20214;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#34892;&#20026;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#27169;&#26865;&#20004;&#21487;&#30340;&#25972;&#25968;&#24207;&#21015;&#34917;&#20840;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#24179;&#22343;&#19968;&#33268;&#24615;&#33539;&#22260;&#20174;67&#65285;&#21040;82&#65285;&#19981;&#31561;&#65292;&#36828;&#36828;&#39640;&#20110;&#19968;&#20010;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22914;&#26524;&#26159;&#38543;&#26426;&#30340;&#35805;&#25152;&#33021;&#39044;&#27979;&#30340;&#27700;&#24179;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#33021;&#21147;&#30340;&#25552;&#21319;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#31283;&#20581;&#24615;&#26816;&#26597;&#20013;&#37117;&#20542;&#21521;&#20110;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#25552;&#31034;&#35828;&#35805;&#32773;&#21464;&#21270;&#21644;&#24207;&#21015;&#38271;&#24230;&#21464;&#21270;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#27809;&#26377;&#29305;&#21035;&#35757;&#32451;&#20063;&#33021;&#20135;&#29983;&#30340;&#26032;&#33021;&#21147;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#21028;&#26029;&#33258;&#36523;&#19968;&#33268;&#24615;&#26102;&#32570;&#23569;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency, e.g., question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67\% to 82\%, far higher than would be predicted if a model's consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consiste
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#22330;&#20250;&#35805;&#25968;&#25454;&#38598;Conversation Chronicles&#65292;&#29992;&#20110;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35774;&#32622;&#65292;&#24182;&#19988;&#34701;&#20837;&#20102;&#26102;&#38388;&#38388;&#38548;&#21644;&#32454;&#31890;&#24230;&#30340;&#21457;&#35328;&#32773;&#20851;&#31995;&#12290;&#36825;&#22635;&#34917;&#20102;&#29616;&#26377;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#30740;&#31350;&#22312;&#22810;&#22330;&#20250;&#35805;&#19978;&#19979;&#25991;&#29702;&#35299;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.13420</link><description>&lt;p&gt;
&#20250;&#35805;&#32534;&#24180;&#21490;&#65306;&#23454;&#29616;&#22810;&#22330;&#20250;&#35805;&#20013;&#30340;&#20016;&#23500;&#26102;&#38388;&#21644;&#20851;&#31995;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations. (arXiv:2310.13420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#22330;&#20250;&#35805;&#25968;&#25454;&#38598;Conversation Chronicles&#65292;&#29992;&#20110;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35774;&#32622;&#65292;&#24182;&#19988;&#34701;&#20837;&#20102;&#26102;&#38388;&#38388;&#38548;&#21644;&#32454;&#31890;&#24230;&#30340;&#21457;&#35328;&#32773;&#20851;&#31995;&#12290;&#36825;&#22635;&#34917;&#20102;&#29616;&#26377;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#30740;&#31350;&#22312;&#22810;&#22330;&#20250;&#35805;&#19978;&#19979;&#25991;&#29702;&#35299;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#24320;&#25918;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#20027;&#35201;&#38480;&#21046;&#26159;&#20854;&#23545;&#30701;&#26399;&#21333;&#27425;&#23545;&#35805;&#30340;&#21333;&#19968;&#20851;&#27880;&#65292;&#24573;&#35270;&#20102;&#22312;&#36827;&#34892;&#20013;&#30340;&#23545;&#35805;&#20043;&#21069;&#22810;&#20010;&#36830;&#32493;&#30340;&#20250;&#35805;&#20013;&#29702;&#35299;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#28508;&#22312;&#38656;&#27714;&#12290;&#22312;&#22810;&#22330;&#23545;&#35805;&#29615;&#22659;&#20013;&#32452;&#25104;&#19978;&#19979;&#25991;&#30340;&#20803;&#32032;&#20013;&#65292;&#20250;&#35805;&#20043;&#38388;&#30340;&#26102;&#38388;&#38388;&#38548;&#21644;&#21457;&#35328;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#23588;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#24037;&#20316;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#36825;&#20123;&#23545;&#35805;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;1M&#22810;&#22330;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;Conversation Chronicles&#65292;&#29992;&#20110;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35774;&#32622;&#65292;&#24182;&#21253;&#21547;&#20102;&#26102;&#38388;&#38388;&#38548;&#21644;&#32454;&#31890;&#24230;&#30340;&#21457;&#35328;&#32773;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#25968;&#25454;&#65292;&#36861;&#38543;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of natural language processing, open-domain chatbots have emerged as an important research topic. However, a major limitation of existing open-domain chatbot research is its singular focus on short single-session dialogue, neglecting the potential need for understanding contextual information in multiple consecutive sessions that precede an ongoing dialogue. Among the elements that compose the context in multi-session conversation settings, the time intervals between sessions and the relationships between speakers would be particularly important. Despite their importance, current research efforts have not sufficiently addressed these dialogical components. In this paper, we introduce a new 1M multi-session dialogue dataset, called Conversation Chronicles, for implementing a long-term conversation setup in which time intervals and fine-grained speaker relationships are incorporated. Following recent works, we exploit a large language model to produce the data. The extensive
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;&#20851;&#31995;&#35268;&#21017;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;RUN-GNN&#65289;&#65292;&#20197;&#35299;&#20915;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#24120;&#24573;&#35270;&#30340;&#20851;&#31995;&#32452;&#21512;&#30340;&#39034;&#24207;&#24615;&#21644;&#28382;&#21518;&#30340;&#23454;&#20307;&#20449;&#24687;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13411</link><description>&lt;p&gt;
&#25552;&#21319;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#20013;&#20851;&#31995;&#35268;&#21017;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhancing Relational Rules for Knowledge Graph Link Prediction. (arXiv:2310.13411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13411
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;&#20851;&#31995;&#35268;&#21017;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;RUN-GNN&#65289;&#65292;&#20197;&#35299;&#20915;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#24120;&#24573;&#35270;&#30340;&#20851;&#31995;&#32452;&#21512;&#30340;&#39034;&#24207;&#24615;&#21644;&#28382;&#21518;&#30340;&#23454;&#20307;&#20449;&#24687;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#26368;&#36817;&#30340;GNN&#21464;&#20307;&#31216;&#20026;&#28176;&#36827;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PRGNN&#65289;&#65292;&#21033;&#29992;&#20851;&#31995;&#35268;&#21017;&#25512;&#26029;&#20851;&#31995;&#26377;&#32570;&#22833;&#30340;&#26377;&#21521;&#22270;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;PRGNN&#36827;&#34892;&#25512;&#29702;&#26102;&#65292;&#24120;&#24120;&#24573;&#35270;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#23646;&#24615;&#65306;&#65288;1&#65289;&#20851;&#31995;&#32452;&#21512;&#30340;&#39034;&#24207;&#24615;&#65292;&#19981;&#21516;&#20851;&#31995;&#30340;&#32452;&#21512;&#39034;&#24207;&#24433;&#21709;&#30528;&#20851;&#31995;&#35268;&#21017;&#30340;&#35821;&#20041;&#65307;&#65288;2&#65289;&#28382;&#21518;&#30340;&#23454;&#20307;&#20449;&#24687;&#20256;&#25773;&#65292;&#25152;&#38656;&#20449;&#24687;&#30340;&#20256;&#36755;&#36895;&#24230;&#33853;&#21518;&#20110;&#26032;&#23454;&#20307;&#30340;&#20986;&#29616;&#36895;&#24230;&#12290;&#24573;&#35270;&#36825;&#20123;&#23646;&#24615;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#20851;&#31995;&#35268;&#21017;&#23398;&#20064;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;&#20851;&#31995;&#35268;&#21017;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;RUN-GNN&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RUN-GNN &#20351;&#29992;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#34701;&#21512;&#38376;&#21333;&#20803;&#26469;&#24314;&#27169;&#39034;&#24207;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown promising performance for knowledge graph reasoning. A recent variant of GNN called progressive relational graph neural network (PRGNN), utilizes relational rules to infer missing knowledge in relational digraphs and achieves notable results. However, during reasoning with PRGNN, two important properties are often overlooked: (1) the sequentiality of relation composition, where the order of combining different relations affects the semantics of the relational rules, and (2) the lagged entity information propagation, where the transmission speed of required information lags behind the appearance speed of new entities. Ignoring these properties leads to incorrect relational rule learning and decreased reasoning accuracy. To address these issues, we propose a novel knowledge graph reasoning approach, the Relational rUle eNhanced Graph Neural Network (RUN-GNN). Specifically, RUN-GNN employs a query related fusion gate unit to model the sequentiality 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#23545;&#40784;&#21644;&#22810;&#23545;&#22810;&#34164;&#28085;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20250;&#35805;&#24335;&#26426;&#22120;&#38405;&#35835;&#65292;&#36890;&#36807;&#23545;&#40784;&#25991;&#26723;&#21644;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#26681;&#25454;&#21069;&#26399;&#38382;&#36807;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20013;&#38388;&#20915;&#31574;&#21644;&#21518;&#32493;&#38382;&#39064;&#29983;&#25104;&#30340;&#20248;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;CMR&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.13409</link><description>&lt;p&gt;
&#26174;&#24335;&#23545;&#40784;&#21644;&#22810;&#23545;&#22810;&#34164;&#28085;&#25512;&#29702;&#65306;&#29992;&#20110;&#20250;&#35805;&#24335;&#26426;&#22120;&#38405;&#35835;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explicit Alignment and Many-to-many Entailment Based Reasoning for Conversational Machine Reading. (arXiv:2310.13409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#23545;&#40784;&#21644;&#22810;&#23545;&#22810;&#34164;&#28085;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20250;&#35805;&#24335;&#26426;&#22120;&#38405;&#35835;&#65292;&#36890;&#36807;&#23545;&#40784;&#25991;&#26723;&#21644;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#26681;&#25454;&#21069;&#26399;&#38382;&#36807;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20013;&#38388;&#20915;&#31574;&#21644;&#21518;&#32493;&#38382;&#39064;&#29983;&#25104;&#30340;&#20248;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;CMR&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#26426;&#22120;&#38405;&#35835;&#65288;CMR&#65289;&#35201;&#27714;&#22522;&#20110;&#32473;&#23450;&#30340;&#25991;&#26723;&#65292;&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#20132;&#20114;&#22238;&#31572;&#29992;&#25143;&#30340;&#21021;&#22987;&#38382;&#39064;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22312;&#25991;&#26723;&#21644;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#23545;&#40784;&#26041;&#38754;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#20013;&#38388;&#20915;&#31574;&#21644;&#21518;&#32493;&#38382;&#39064;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#27700;&#32447;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#65288;1&#65289;&#20197;&#26174;&#24335;&#26041;&#24335;&#23545;&#40784;&#19978;&#36848;&#20004;&#20010;&#26041;&#38754;&#65292;&#65288;2&#65289;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22810;&#23545;&#22810;&#34164;&#28085;&#25512;&#29702;&#27169;&#22359;&#36827;&#34892;&#20915;&#31574;&#65292;&#65288;3&#65289;&#22522;&#20110;&#25991;&#26723;&#21644;&#20808;&#21069;&#38382;&#36807;&#30340;&#38382;&#39064;&#30452;&#25509;&#29983;&#25104;&#21518;&#32493;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24494;&#20934;&#30830;&#24230;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#24182;&#22312;CMR&#22522;&#20934;&#25968;&#25454;&#38598;ShARC&#30340;&#20844;&#24320;&#25490;&#34892;&#27036;&#19978;&#21517;&#21015;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational Machine Reading (CMR) requires answering a user's initial question through multi-turn dialogue interactions based on a given document. Although there exist many effective methods, they largely neglected the alignment between the document and the user-provided information, which significantly affects the intermediate decision-making and subsequent follow-up question generation. To address this issue, we propose a pipeline framework that (1) aligns the aforementioned two sides in an explicit way, (2)makes decisions using a lightweight many-to-many entailment reasoning module, and (3) directly generates follow-up questions based on the document and previously asked questions. Our proposed method achieves state-of-the-art in micro-accuracy and ranks the first place on the public leaderboard of the CMR benchmark dataset ShARC.
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#32447;&#25104;&#26412;&#24863;&#30693;&#30340;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#65292;&#36890;&#36807;&#32531;&#23384;&#24182;&#21033;&#29992;&#20808;&#21069;&#30340;LLM&#21709;&#24212;&#26469;&#35757;&#32451;&#26412;&#22320;&#24265;&#20215;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.13395</link><description>&lt;p&gt;
&#22914;&#26524;&#21487;&#20197;&#30340;&#35805;&#23601;&#32473;&#25105;&#32531;&#23384;&#36215;&#26469;&#65306;&#19968;&#31181;&#22312;&#32447;&#25104;&#26412;&#24863;&#30693;&#30340;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#23569;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#29992;&#27425;&#25968;
&lt;/p&gt;
&lt;p&gt;
Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models. (arXiv:2310.13395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13395
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#32447;&#25104;&#26412;&#24863;&#30693;&#30340;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#65292;&#36890;&#36807;&#32531;&#23384;&#24182;&#21033;&#29992;&#20808;&#21069;&#30340;LLM&#21709;&#24212;&#26469;&#35757;&#32451;&#26412;&#22320;&#24265;&#20215;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#25552;&#31034;&#25928;&#26524;&#26174;&#33879;&#12290;&#22240;&#27492;&#65292;&#26080;&#27861;&#25215;&#25285;&#21019;&#24314;&#22823;&#35268;&#27169;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#33258;&#24049;&#30340;LLM&#25104;&#26412;&#30340;&#20013;&#23567;&#22411;&#20225;&#19994; (SMEs) &#36234;&#26469;&#36234;&#22810;&#22320;&#36716;&#21521;&#20801;&#35768;&#23427;&#20204;&#20174;LLM&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#31532;&#19977;&#26041;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26381;&#21153;&#30446;&#21069;&#38656;&#35201;&#25353;&#35843;&#29992;&#25903;&#20184;&#36153;&#29992;&#65292;&#36825;&#25104;&#20026;&#37325;&#35201;&#30340;&#36816;&#33829;&#25104;&#26412;&#65288;OpEx&#65289;&#12290;&#27492;&#22806;&#65292;&#23458;&#25143;&#30340;&#36755;&#20837;&#36890;&#24120;&#38543;&#26102;&#38388;&#30456;&#20284;&#65292;SMEs&#26368;&#32456;&#20197;&#38750;&#24120;&#30456;&#20284;&#30340;&#23454;&#20363;&#25552;&#31034;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#32531;&#23384;&#20808;&#21069;&#30340;LLM&#21709;&#24212;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;SME&#31471;&#19978;&#30340;&#26412;&#22320;&#24265;&#20215;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;LLM&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20915;&#23450;&#20309;&#26102;&#30456;&#20449;&#26412;&#22320;&#27169;&#22411;&#25110;&#35843;&#29992;LLM&#30340;&#20934;&#21017;&#65292;&#20197;&#21450;&#35843;&#25972;&#20934;&#21017;&#21644;&#34913;&#37327;&#24615;&#33021;&#19982;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#26041;&#27861;&#35770;&#12290;&#20026;&#20102;&#23454;&#39564;&#30446;&#30340;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31034;&#20363;&#26469;&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models (LLMs) performs impressively in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets, but also the cost of pretraining their own LLMs, are increasingly turning to third-party services that allow them to prompt LLMs. However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are often very similar over time, hence SMEs end-up prompting LLMs with very similar instances. We propose a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side. The framework includes criteria for deciding when to trust the local model or call the LLM, and a methodology to tune the criteria and measure the tradeoff between performance and cost. For experimental purposes, we instantiate our framework with
&lt;/p&gt;</description></item><item><title>POSQA&#26159;&#19968;&#20010;&#29992;&#26469;&#25506;&#32034;LLMs&#22312;&#20855;&#36523;&#29702;&#35299;&#26041;&#38754;&#30340;&#29289;&#20307;&#22823;&#23567;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;LLMs&#22312;&#38646;-shot&#24773;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#25512;&#21160;&#25216;&#26415;&#21644;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#26497;&#38480;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#20854;&#29616;&#23454;&#19990;&#30028;&#29702;&#35299;&#30340;&#26469;&#28304;&#21644;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.13394</link><description>&lt;p&gt;
POSQA&#65306;&#20351;&#29992;&#23610;&#23544;&#27604;&#36739;&#25506;&#32034;LLMs&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
POSQA: Probe the World Models of LLMs with Size Comparisons. (arXiv:2310.13394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13394
&lt;/p&gt;
&lt;p&gt;
POSQA&#26159;&#19968;&#20010;&#29992;&#26469;&#25506;&#32034;LLMs&#22312;&#20855;&#36523;&#29702;&#35299;&#26041;&#38754;&#30340;&#29289;&#20307;&#22823;&#23567;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;LLMs&#22312;&#38646;-shot&#24773;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#25512;&#21160;&#25216;&#26415;&#21644;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#26497;&#38480;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#20854;&#29616;&#23454;&#19990;&#30028;&#29702;&#35299;&#30340;&#26469;&#28304;&#21644;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#36523;&#21270;&#35821;&#35328;&#29702;&#35299;&#24378;&#35843;&#35821;&#35328;&#29702;&#35299;&#19981;&#20165;&#20165;&#26159;&#22823;&#33041;&#20013;&#30340;&#24515;&#29702;&#22788;&#29702;&#38382;&#39064;&#65292;&#36824;&#28041;&#21450;&#19982;&#29289;&#29702;&#21644;&#31038;&#20250;&#29615;&#22659;&#30340;&#20114;&#21160;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#23427;&#20204;&#24050;&#32463;&#26222;&#36941;&#23384;&#22312;&#20110;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#36234;&#26469;&#36234;&#26377;&#24517;&#35201;&#39564;&#35777;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#21463;&#35748;&#30693;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;POSQA&#65306;&#19968;&#20010;&#24102;&#26377;&#31616;&#21333;&#23610;&#23544;&#27604;&#36739;&#38382;&#39064;&#30340;&#29289;&#20307;&#22823;&#23567;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#26816;&#39564;&#26368;&#26032;LLMs&#30340;&#20855;&#36523;&#29702;&#35299;&#30340;&#26497;&#31471;&#24615;&#24182;&#20998;&#26512;&#20854;&#28508;&#22312;&#26426;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#26159;&#20170;&#22825;&#30340;&#26368;&#22823;LLMs&#22312;&#38646;-shot&#24773;&#26223;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21644;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#25512;&#21160;&#20102;&#23427;&#20204;&#30340;&#26497;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#29616;&#23454;&#19990;&#30028;&#29702;&#35299;&#20027;&#35201;&#26159;&#26469;&#33258;&#19978;&#19979;&#25991;&#20449;&#24687;&#36824;&#26159;&#20869;&#37096;&#26435;&#37325;&#65292;&#24182;&#20998;&#26512;&#20102;&#25552;&#31034;&#23545;&#20854;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied language comprehension emphasizes that language understanding is not solely a matter of mental processing in the brain but also involves interactions with the physical and social environment. With the explosive growth of Large Language Models (LLMs) and their already ubiquitous presence in our daily lives, it is becoming increasingly necessary to verify their real-world understanding. Inspired by cognitive theories, we propose POSQA: a Physical Object Size Question Answering dataset with simple size comparison questions to examine the extremity and analyze the potential mechanisms of the embodied comprehension of the latest LLMs.  We show that even the largest LLMs today perform poorly under the zero-shot setting. We then push their limits with advanced prompting techniques and external knowledge augmentation. Furthermore, we investigate whether their real-world comprehension primarily derives from contextual information or internal weights and analyse the impact of prompt for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#25490;&#21517;&#21644;&#19978;&#19979;&#25991;&#25490;&#21517;&#26469;&#22686;&#21152;&#29983;&#25104;&#26356;&#22909;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13385</link><description>&lt;p&gt;
Tuna: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Tuna: Instruction Tuning using Feedback from Large Language Models. (arXiv:2310.13385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#25490;&#21517;&#21644;&#19978;&#19979;&#25991;&#25490;&#21517;&#26469;&#22686;&#21152;&#29983;&#25104;&#26356;&#22909;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Instruct-GPT&#21644;GPT-4&#65289;&#30340;&#30452;&#25509;&#36755;&#20986;&#65292;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMA&#65289;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#20165;&#30475;&#21040;&#27599;&#20010;&#25351;&#20196;&#30340;&#19968;&#20010;&#21709;&#24212;&#65292;&#32570;&#20047;&#21487;&#33021;&#26356;&#22909;&#30340;&#21709;&#24212;&#30340;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#8220;&#27010;&#29575;&#25490;&#21517;&#8221;&#21644;&#8220;&#19978;&#19979;&#25991;&#25490;&#21517;&#8221;&#26041;&#27861;&#26469;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#21152;&#29983;&#25104;&#26356;&#22909;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#12290;&#27010;&#29575;&#25490;&#21517;&#20351;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#32487;&#25215;&#20102;&#26469;&#33258;&#25945;&#24072;LLM&#30340;&#39640;&#36136;&#37327;&#21644;&#20302;&#36136;&#37327;&#21709;&#24212;&#30340;&#30456;&#23545;&#25490;&#21517;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23398;&#20064;&#19978;&#19979;&#25991;&#25490;&#21517;&#21487;&#20197;&#35753;&#27169;&#22411;&#20351;&#29992;&#26356;&#24378;&#22823;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#26469;&#36827;&#19968;&#27493;&#20248;&#21270;&#33258;&#24049;&#30340;&#21709;&#24212;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27010;&#29575;&#25490;&#21517;&#21644;&#19978;&#19979;&#25991;&#25490;&#21517;&#25353;&#39034;&#24207;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences. However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. In this paper, we propose finetuning an instruction-tuned LLM using our novel \textit{probabilistic ranking} and \textit{contextual ranking} approaches to increase the likelihood of generating better responses. Probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher LLM. On the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger LLMs. Furthermore, we apply probabilistic ranking and contextual ranking sequentially to the in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#21407;&#22411;&#20266;&#26631;&#35760;&#65288;APP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#23569;&#26679;&#26412;OOD&#26816;&#27979;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21407;&#22411;OOD&#26816;&#27979;&#26694;&#26550;&#65288;ProtoOOD&#65289;&#65292;&#29992;&#20110;&#20174;&#26377;&#38480;&#30340;IND&#25968;&#25454;&#20013;&#36827;&#34892;&#20302;&#36164;&#28304;OOD&#26816;&#27979;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#20266;&#26631;&#35760;&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;OOD&#21644;IND&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;OOD&#26816;&#27979;&#20013;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13380</link><description>&lt;p&gt;
APP&#65306;&#33258;&#36866;&#24212;&#21407;&#22411;&#20266;&#26631;&#35760;&#29992;&#20110;&#23569;&#26679;&#26412;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection. (arXiv:2310.13380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#21407;&#22411;&#20266;&#26631;&#35760;&#65288;APP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#23569;&#26679;&#26412;OOD&#26816;&#27979;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21407;&#22411;OOD&#26816;&#27979;&#26694;&#26550;&#65288;ProtoOOD&#65289;&#65292;&#29992;&#20110;&#20174;&#26377;&#38480;&#30340;IND&#25968;&#25454;&#20013;&#36827;&#34892;&#20302;&#36164;&#28304;OOD&#26816;&#27979;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#20266;&#26631;&#35760;&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;OOD&#21644;IND&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;OOD&#26816;&#27979;&#20013;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26816;&#27979;&#29992;&#25143;&#26597;&#35810;&#20013;&#30340;&#22495;&#22806;&#24847;&#22270;&#65288;OOD&#65289;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20043;&#21069;&#30340;OOD&#26816;&#27979;&#30740;&#31350;&#36890;&#24120;&#22522;&#20110;&#23384;&#22312;&#22823;&#37327;&#26631;&#35760;&#30340;&#20869;&#37096;&#24847;&#22270;&#65288;IND&#65289;&#12290;&#26412;&#25991;&#20851;&#27880;&#26356;&#23454;&#38469;&#30340;&#23569;&#26679;&#26412;OOD&#22330;&#26223;&#65292;&#20854;&#20013;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#30340;IND&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#21152;&#26631;&#31614;&#30340;&#28151;&#21512;&#25968;&#25454;&#65292;&#21487;&#33021;&#23646;&#20110;IND&#25110;OOD&#12290;&#36825;&#31181;&#26032;&#30340;&#22330;&#26223;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20351;&#29992;&#26377;&#38480;&#30340;IND&#25968;&#25454;&#23398;&#20064;&#26377;&#21306;&#20998;&#21147;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#26410;&#21152;&#26631;&#31614;&#30340;&#28151;&#21512;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21407;&#22411;&#20266;&#26631;&#35760;&#65288;APP&#65289;&#26041;&#27861;&#29992;&#20110;&#23569;&#26679;&#26412;OOD&#26816;&#27979;&#65292;&#21253;&#25324;&#19968;&#20010;&#21407;&#22411;OOD&#26816;&#27979;&#26694;&#26550;&#65288;ProtoOOD&#65289;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;IND&#25968;&#25454;&#20013;&#20419;&#36827;&#20302;&#36164;&#28304;OOD&#26816;&#27979;&#65292;&#20197;&#21450;&#19968;&#31181;&#33258;&#36866;&#24212;&#20266;&#26631;&#35760;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;OOD&#21644;IND&#26631;&#31614;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#23569;&#26679;&#26412;OOD&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out-of-domain (OOD) intents from user queries is essential for a task-oriented dialogue system. Previous OOD detection studies generally work on the assumption that plenty of labeled IND intents exist. In this paper, we focus on a more practical few-shot OOD setting where there are only a few labeled IND data and massive unlabeled mixed data that may belong to IND or OOD. The new scenario carries two key challenges: learning discriminative representations using limited IND data and leveraging unlabeled mixed data. Therefore, we propose an adaptive prototypical pseudo-labeling (APP) method for few-shot OOD detection, including a prototypical OOD detection framework (ProtoOOD) to facilitate low-resource OOD detection using limited IND data, and an adaptive pseudo-labeling method to produce high-quality pseudo OOD\&amp;IND labels. Extensive experiments and analysis demonstrate the effectiveness of our method for few-shot OOD detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#20114;&#21160;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#36890;&#36807;&#23398;&#20064;&#31526;&#21495;&#35821;&#35328;&#26469;&#35782;&#21035;&#26426;&#22120;&#20154;&#30340;&#20869;&#37096;&#31283;&#24577;&#38656;&#27714;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#24046;&#24322;&#32467;&#26524;&#35757;&#32451;&#21327;&#35758;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;&#20154;&#31867;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#36827;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#35821;&#35328;&#20064;&#24471;&#12290;</title><link>http://arxiv.org/abs/2310.13377</link><description>&lt;p&gt;
&#20855;&#26377;&#24773;&#24863;&#22522;&#30784;&#35821;&#35328;&#20064;&#24471;&#21644;&#24046;&#24322;&#32467;&#26524;&#35757;&#32451;&#30340;&#20154;&#26426;&#20114;&#21160;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Human-Robot Mutual Learning System with Affect-Grounded Language Acquisition and Differential Outcomes Training. (arXiv:2310.13377v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#20114;&#21160;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#36890;&#36807;&#23398;&#20064;&#31526;&#21495;&#35821;&#35328;&#26469;&#35782;&#21035;&#26426;&#22120;&#20154;&#30340;&#20869;&#37096;&#31283;&#24577;&#38656;&#27714;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#24046;&#24322;&#32467;&#26524;&#35757;&#32451;&#21327;&#35758;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;&#20154;&#31867;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#36827;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#35821;&#35328;&#20064;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#26426;&#20114;&#21160;&#35774;&#32622;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#23398;&#20064;&#31526;&#21495;&#35821;&#35328;&#20197;&#35782;&#21035;&#26426;&#22120;&#20154;&#30340;&#20869;&#31283;&#24577;&#38656;&#27714;&#12290;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#23398;&#20064;&#20351;&#29992;&#21644;&#21709;&#24212;&#20256;&#36798;&#20869;&#31283;&#24577;&#38656;&#27714;&#21644;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#30340;&#21050;&#28608;&#30340;&#30456;&#21516;&#35821;&#35328;&#31526;&#21495;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#24046;&#24322;&#32467;&#26524;&#35757;&#32451;&#65288;DOT&#65289;&#21327;&#35758;&#65292;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#27491;&#30830;&#21050;&#28608;&#65288;&#22914;&#39292;&#24178;&#65289;&#26102;&#25552;&#20379;&#29305;&#23450;&#20110;&#20854;&#20869;&#37096;&#38656;&#27714;&#65288;&#22914;&#8220;&#39269;&#39295;&#8221;&#65289;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#21457;&#29616;DOT&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#36827;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#35821;&#35328;&#20064;&#24471;&#12290;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#20154;&#20855;&#26377;&#19982;&#22788;&#20110;&#35821;&#35328;&#8220;&#21695;&#21568;&#23398;&#35821;&#8221;&#38454;&#27573;&#30340;&#20154;&#31867;&#23156;&#20799;&#31867;&#20284;&#30340;&#35789;&#27719;&#37327;&#12290;&#26426;&#22120;&#20154;&#30340;&#36719;&#20214;&#26550;&#26500;&#22522;&#20110;&#24773;&#24863;&#22522;&#30784;&#35821;&#35328;&#20064;&#24471;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20132;&#20114;&#23558;&#35789;&#27719;&#19982;&#20869;&#37096;&#38656;&#27714;&#65288;&#39269;&#39295;&#12289;&#21475;&#28212;&#12289;&#22909;&#22855;&#65289;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel human-robot interaction setup for robot and human learning of symbolic language for identifying robot homeostatic needs. The robot and human learn to use and respond to the same language symbols that convey homeostatic needs and the stimuli that satisfy the homeostatic needs, respectively. We adopted a differential outcomes training (DOT) protocol whereby the robot provides feedback specific (differential) to its internal needs (e.g. `hunger') when satisfied by the correct stimulus (e.g. cookie). We found evidence that DOT can enhance the human's learning efficiency, which in turn enables more efficient robot language acquisition. The robot used in the study has a vocabulary similar to that of a human infant in the linguistic ``babbling'' phase. The robot software architecture is built upon a model for affect-grounded language acquisition where the robot associates vocabulary with internal needs (hunger, thirst, curiosity) through interactions with the human
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#30340;&#34892;&#20026;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35786;&#26029;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36890;&#29992;&#38169;&#35823;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#29992;&#20363;&#21450;&#20854;&#20266;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#20154;&#24037;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.13362</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#20026;&#27979;&#35797;&#23454;&#29616;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#36890;&#29992;&#38169;&#35823;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Towards General Error Diagnosis via Behavioral Testing in Machine Translation. (arXiv:2310.13362v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#30340;&#34892;&#20026;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35786;&#26029;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36890;&#29992;&#38169;&#35823;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#29992;&#20363;&#21450;&#20854;&#20266;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#20154;&#24037;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#27979;&#35797;&#20026;&#35786;&#26029;&#35821;&#35328;&#38169;&#35823;&#21644;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#37325;&#35201;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#23558;&#34892;&#20026;&#27979;&#35797;&#24212;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#20154;&#21147;&#26469;&#21046;&#20316;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#22312;&#26032;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#19978;&#30340;&#32763;&#35793;&#36136;&#37327;&#30340;&#21442;&#32771;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#32763;&#35793;&#34892;&#20026;&#27979;&#35797;&#24037;&#20316;&#36890;&#36807;&#22312;&#27809;&#26377;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#26469;&#32469;&#24320;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#38480;&#21046;&#20102;&#23545;&#29305;&#23450;&#31867;&#22411;&#38169;&#35823;&#30340;&#35786;&#26029;&#65292;&#27604;&#22914;&#21333;&#20010;&#25968;&#23383;&#25110;&#36135;&#24065;&#35789;&#30340;&#38169;&#35823;&#32763;&#35793;&#12290;&#20026;&#20102;&#35786;&#26029;&#36890;&#29992;&#38169;&#35823;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#30340;&#34892;&#20026;&#27979;&#35797;&#65288;BTPGBT&#65289;&#26694;&#26550;&#26469;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#34892;&#20026;&#27979;&#35797;&#12290;BTPGBT&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#65288;BTPG&#65289;&#26041;&#27861;&#65292;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#29992;&#20363;&#21450;&#20854;&#20266;&#21442;&#32771;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35786;&#26029;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36890;&#29992;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral testing offers a crucial means of diagnosing linguistic errors and assessing capabilities of NLP models. However, applying behavioral testing to machine translation (MT) systems is challenging as it generally requires human efforts to craft references for evaluating the translation quality of such systems on newly generated test cases. Existing works in behavioral testing of MT systems circumvent this by evaluating translation quality without references, but this restricts diagnosis to specific types of errors, such as incorrect translation of single numeric or currency words. In order to diagnose general errors, this paper proposes a new Bilingual Translation Pair Generation based Behavior Testing (BTPGBT) framework for conducting behavioral testing of MT systems. The core idea of BTPGBT is to employ a novel bilingual translation pair generation (BTPG) approach that automates the construction of high-quality test cases and their pseudoreferences. Experimental results on var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36755;&#20837;&#22270;&#20687;&#34920;&#31034;&#21644;&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#35843;&#25972;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#40511;&#27807;&#12290;</title><link>http://arxiv.org/abs/2310.13361</link><description>&lt;p&gt;
&#22635;&#34917;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#38388;&#30340;&#40511;&#27807;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation. (arXiv:2310.13361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36755;&#20837;&#22270;&#20687;&#34920;&#31034;&#21644;&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#35843;&#25972;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#40511;&#27807;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#21516;&#26102;&#23558;&#28304;&#21477;&#23376;&#21644;&#30456;&#20851;&#22270;&#20687;&#20316;&#20026;&#32763;&#35793;&#30340;&#36755;&#20837;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#36755;&#20837;&#21477;&#23376;&#27809;&#26377;&#37197;&#23545;&#30340;&#22270;&#20687;&#21487;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#21033;&#29992;&#24378;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#22270;&#20687;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#24448;&#24448;&#19982;&#30495;&#23454;&#22270;&#20687;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#30495;&#23454;&#22270;&#20687;&#65292;&#32780;&#22312;&#25512;&#29702;&#20013;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#21487;&#33021;&#24341;&#20837;&#20998;&#24067;&#20559;&#31227;&#65292;&#23548;&#33268;&#25512;&#29702;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#20998;&#21035;&#36755;&#20837;&#21040;MMT&#27169;&#22411;&#20013;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25509;&#36817;Transformer&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#22270;&#20687;&#34920;&#31034;&#21644;Transformer&#35299;&#30721;&#22120;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#26368;&#23567;&#21270;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32531;&#35299;&#20102;&#21512;&#25104;&#22270;&#20687;&#24341;&#20837;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs. Nevertheless, synthetic images generated by these models often follow different distributions compared to authentic images. Consequently, using authentic images for training and synthetic images for inference can introduce a distribution shift, resulting in performance degradation during inference. To tackle this challenge, in this paper, we feed synthetic and authentic images to the MMT model, respectively. Then we minimize the gap between the synthetic and authentic images by drawing close the input image representations of the Transformer Encoder and the output distributions of the Transformer Decoder. Therefore, we mitigate the distribution disparity introduced by the synthetic images
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#20998;&#26512;&#20102;&#23376;&#35789;&#20998;&#35789;&#30340;&#35748;&#30693;&#21512;&#29702;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#31181;&#19981;&#21516;&#31639;&#27861;&#22312;&#22810;&#31181;&#35821;&#35328;&#21644;&#35789;&#27719;&#22823;&#23567;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#21457;&#29616;UnigramLM&#31639;&#27861;&#30340;&#20998;&#35789;&#34892;&#20026;&#21644;&#27966;&#29983;&#24418;&#24577;&#32032;&#35206;&#30422;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.13348</link><description>&lt;p&gt;
&#20998;&#26512;&#23376;&#35789;&#20998;&#35789;&#30340;&#35748;&#30693;&#21512;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing Cognitive Plausibility of Subword Tokenization. (arXiv:2310.13348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#20998;&#26512;&#20102;&#23376;&#35789;&#20998;&#35789;&#30340;&#35748;&#30693;&#21512;&#29702;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#31181;&#19981;&#21516;&#31639;&#27861;&#22312;&#22810;&#31181;&#35821;&#35328;&#21644;&#35789;&#27719;&#22823;&#23567;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#21457;&#29616;UnigramLM&#31639;&#27861;&#30340;&#20998;&#35789;&#34892;&#20026;&#21644;&#27966;&#29983;&#24418;&#24577;&#32032;&#35206;&#30422;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#20998;&#35789;&#24050;&#32463;&#25104;&#20026;&#26631;&#20934;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#28982;&#32780;&#23545;&#20110;&#19981;&#21516;&#35821;&#35328;&#20013;&#23376;&#35789;&#35789;&#27719;&#36136;&#37327;&#30340;&#27604;&#36739;&#35780;&#20272;&#21364;&#38750;&#24120;&#31232;&#32570;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20998;&#35789;&#31639;&#27861;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25110;&#32773;&#24037;&#31243;&#26631;&#20934;&#22914;&#21387;&#32553;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#19987;&#27880;&#20110;&#23376;&#35789;&#20998;&#35789;&#30340;&#35748;&#30693;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;Tokenizer&#36755;&#20986;&#19982;&#20154;&#31867;&#22312;&#35789;&#27719;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#23376;&#35789;&#20998;&#35789;&#31639;&#27861;&#22312;&#20960;&#31181;&#35821;&#35328;&#21644;&#35789;&#27719;&#22823;&#23567;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;UnigramLM&#31639;&#27861;&#20135;&#29983;&#20102;&#26356;&#23569;&#35748;&#30693;&#21512;&#29702;&#30340;&#20998;&#35789;&#34892;&#20026;&#21644;&#26356;&#24046;&#30340;&#27966;&#29983;&#24418;&#24577;&#32032;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subword tokenization has become the de-facto standard for tokenization, although comparative evaluations of subword vocabulary quality across languages are scarce. Existing evaluation studies focus on the effect of a tokenization algorithm on the performance in downstream tasks, or on engineering criteria such as the compression rate. We present a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization. We analyze the correlation of the tokenizer output with the response time and accuracy of human performance on a lexical decision task. We compare three tokenization algorithms across several languages and vocabulary sizes. Our results indicate that the UnigramLM algorithm yields less cognitively plausible tokenization behavior and a worse coverage of derivational morphemes, in contrast with prior work.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#38754;&#20020;&#39046;&#22495;&#29305;&#24322;&#24615;&#12289;&#30693;&#35782;&#36951;&#24536;&#12289;&#30693;&#35782;&#22797;&#21046;&#12289;&#30693;&#35782;&#38169;&#35273;&#21644;&#30693;&#35782;&#26377;&#27602;&#31561;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#24314;&#35758;&#21253;&#25324;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#12289;&#24494;&#35843;&#27169;&#22411;&#65292;&#22686;&#24378;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13343</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21033;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs). (arXiv:2310.13343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13343
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#38754;&#20020;&#39046;&#22495;&#29305;&#24322;&#24615;&#12289;&#30693;&#35782;&#36951;&#24536;&#12289;&#30693;&#35782;&#22797;&#21046;&#12289;&#30693;&#35782;&#38169;&#35273;&#21644;&#30693;&#35782;&#26377;&#27602;&#31561;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#24314;&#35758;&#21253;&#25324;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#12289;&#24494;&#35843;&#27169;&#22411;&#65292;&#22686;&#24378;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT&#31995;&#21015;&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;&#26412;&#32508;&#36848;&#39318;&#20808;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#30340;&#38382;&#39064;&#65292;LLMs&#21487;&#33021;&#38590;&#20197;&#23545;&#19987;&#19994;&#39046;&#22495;&#30340;&#29305;&#23450;&#38382;&#39064;&#25552;&#20379;&#31934;&#30830;&#31572;&#26696;&#12290;&#30693;&#35782;&#36951;&#24536;&#30340;&#38382;&#39064;&#26159;&#36825;&#20123;LLMs&#21487;&#33021;&#38590;&#20197;&#24179;&#34913;&#26032;&#26087;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#30693;&#35782;&#22797;&#21046;&#29616;&#35937;&#25581;&#31034;&#20102;LLMs&#26377;&#26102;&#21487;&#33021;&#25552;&#20379;&#36807;&#24230;&#26426;&#26800;&#21270;&#30340;&#22238;&#31572;&#65292;&#32570;&#20047;&#28145;&#24230;&#21644;&#29420;&#21019;&#24615;&#12290;&#27492;&#22806;&#65292;&#30693;&#35782;&#38169;&#35273;&#25551;&#36848;&#20102;LLMs&#21487;&#33021;&#25552;&#20379;&#30475;&#20284;&#28145;&#21051;&#20294;&#23454;&#38469;&#19978;&#32932;&#27973;&#30340;&#31572;&#26696;&#30340;&#24773;&#20917;&#65292;&#32780;&#30693;&#35782;&#26377;&#27602;&#21017;&#20391;&#37325;&#20110;&#26377;&#23475;&#25110;&#26377;&#20559;&#35265;&#30340;&#20449;&#24687;&#36755;&#20986;&#12290;&#36825;&#20123;&#25361;&#25112;&#20984;&#26174;&#20102;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31639;&#27861;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24314;&#35758;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#65292;&#24494;&#35843;&#27169;&#22411;&#65292;&#22686;&#24378;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large language models (LLMs) like the GPT series, their widespread use across various application scenarios presents a myriad of challenges. This review initially explores the issue of domain specificity, where LLMs may struggle to provide precise answers to specialized questions within niche fields. The problem of knowledge forgetting arises as these LLMs might find it hard to balance old and new information. The knowledge repetition phenomenon reveals that sometimes LLMs might deliver overly mechanized responses, lacking depth and originality. Furthermore, knowledge illusion describes situations where LLMs might provide answers that seem insightful but are actually superficial, while knowledge toxicity focuses on harmful or biased information outputs. These challenges underscore problems in the training data and algorithmic design of LLMs. To address these issues, it's suggested to diversify training data, fine-tune models, enhance transparency and interpretab
&lt;/p&gt;</description></item><item><title>SUBSUMM&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#36879;&#35270;&#35266;&#28857;&#25688;&#35201;&#21270;&#30340;&#26377;&#30417;&#30563;&#25688;&#35201;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#35282;&#24230;&#36873;&#25321;&#35780;&#35770;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#26696;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#25552;&#20379;&#31934;&#28860;&#30340;&#22810;&#35282;&#24230;&#30340;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.13340</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#36879;&#35270;&#35266;&#28857;&#25688;&#35201;&#21270;&#19982;&#22810;&#26679;&#21270;&#35780;&#35770;&#23376;&#38598;
&lt;/p&gt;
&lt;p&gt;
Large-Scale and Multi-Perspective Opinion Summarization with Diverse Review Subsets. (arXiv:2310.13340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13340
&lt;/p&gt;
&lt;p&gt;
SUBSUMM&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#36879;&#35270;&#35266;&#28857;&#25688;&#35201;&#21270;&#30340;&#26377;&#30417;&#30563;&#25688;&#35201;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#35282;&#24230;&#36873;&#25321;&#35780;&#35770;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#26696;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#25552;&#20379;&#31934;&#28860;&#30340;&#22810;&#35282;&#24230;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35266;&#28857;&#25688;&#35201;&#21270;&#24076;&#26395;&#33021;&#22815;&#28040;&#21270;&#26356;&#22823;&#37327;&#30340;&#35780;&#35770;&#38598;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#19981;&#21516;&#35282;&#24230;&#30340;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22823;&#22810;&#22240;&#20026;&#32570;&#20047;&#20449;&#24687;&#36873;&#25321;&#35774;&#35745;&#32780;&#26080;&#27861;&#31934;&#28860;&#24191;&#27867;&#30340;&#35780;&#35770;&#21644;&#25552;&#20379;&#22810;&#35282;&#24230;&#35266;&#28857;&#30340;&#25688;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SUBSUMM&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#36879;&#35270;&#35266;&#28857;&#25688;&#35201;&#21270;&#30340;&#26377;&#30417;&#30563;&#25688;&#35201;&#26694;&#26550;&#12290;SUBSUMM&#21253;&#25324;&#19968;&#20010;&#35780;&#35770;&#37319;&#26679;&#31574;&#30053;&#38598;&#21644;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#26696;&#12290;&#36825;&#20123;&#37319;&#26679;&#31574;&#30053;&#32771;&#34385;&#20102;&#24773;&#24863;&#20542;&#21521;&#21644;&#23545;&#27604;&#20449;&#24687;&#20215;&#20540;&#65292;&#21487;&#20197;&#36873;&#25321;&#19981;&#21516;&#35282;&#24230;&#21644;&#36136;&#37327;&#27700;&#24179;&#30340;&#35780;&#35770;&#23376;&#38598;&#12290;&#38543;&#21518;&#65292;&#25688;&#35201;&#22120;&#20381;&#27425;&#20174;&#27425;&#20248;&#21644;&#26368;&#20248;&#23376;&#38598;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#22823;&#37327;&#30340;&#36755;&#20837;&#12290;&#22312;AmaSum&#21644;Rotten Tomatoes&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SUBSUMM&#33021;&#22815;&#29087;&#32451;&#29983;&#25104;&#20248;&#28857;&#12289;&#32570;&#28857;&#21644;&#32467;&#35770;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinion summarization is expected to digest larger review sets and provide summaries from different perspectives. However, most existing solutions are deficient in epitomizing extensive reviews and offering opinion summaries from various angles due to the lack of designs for information selection. To this end, we propose SUBSUMM, a supervised summarization framework for large-scale multi-perspective opinion summarization. SUBSUMM consists of a review sampling strategy set and a two-stage training scheme. The sampling strategies take sentiment orientation and contrastive information value into consideration, with which the review subsets from different perspectives and quality levels can be selected. Subsequently, the summarizer is encouraged to learn from the sub-optimal and optimal subsets successively in order to capitalize on the massive input. Experimental results on AmaSum and Rotten Tomatoes datasets demonstrate that SUBSUMM is adept at generating pros, cons, and verdict summarie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#65292;&#24182;&#24314;&#31435;&#20132;&#20114;&#24335;&#22810;&#36718;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#25512;&#29702;&#33021;&#21147;&#25552;&#21462;&#21040;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#20419;&#36827;&#20854;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.13332</link><description>&lt;p&gt;
&#27665;&#20027;&#21270;&#25512;&#29702;&#33021;&#21147;&#65306;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#21046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Democratizing Reasoning Ability: Tailored Learning from Large Language Model. (arXiv:2310.13332v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#65292;&#24182;&#24314;&#31435;&#20132;&#20114;&#24335;&#22810;&#36718;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#25512;&#29702;&#33021;&#21147;&#25552;&#21462;&#21040;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#20419;&#36827;&#20854;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#24040;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#29305;&#24615;&#65292;&#20854;&#27665;&#20027;&#21270;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26368;&#36817;&#23545;&#24320;&#28304;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#25552;&#21462;&#30340;&#30740;&#31350;&#22312;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#25552;&#21462;&#21040;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#20419;&#36827;&#19987;&#23646;&#25512;&#29702;&#33021;&#21147;&#30340;&#27665;&#20027;&#21270;&#12290;&#19982;&#20165;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#27880;&#37322;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#30340;&#28508;&#21147;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#22810;&#36718;&#23398;&#20064;&#33539;&#24335;&#12290;&#36825;&#20010;&#33539;&#24335;&#20351;&#24471;&#23398;&#29983;&#33021;&#22815;&#23558;&#33258;&#24049;&#30340;&#19981;&#36275;&#26292;&#38706;&#32473;&#40657;&#31665;&#25945;&#24072;&#65292;&#28982;&#21518;&#25945;&#24072;&#21487;&#20197;&#25552;&#20379;&#23450;&#21046;&#30340;&#35757;&#32451;&#25968;&#25454;&#20316;&#20026;&#22238;&#25253;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21033;&#29992;&#25512;&#29702;&#28508;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#29305;&#23450;&#25512;&#29702;&#20219;&#21153;&#65292;&#36890;&#36807;&#19982;&#40657;&#31665;&#25945;&#24072;&#30340;&#20132;&#20114;&#26469;&#23450;&#21046;&#23398;&#29983;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24207;&#21015;&#21040;&#24207;&#21015;&#33539;&#24335;&#19979;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#33258;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#22686;&#24378;&#25968;&#25454;&#21644;&#24490;&#29615;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#25968;&#25454;&#65292;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13321</link><description>&lt;p&gt;
&#36229;&#36234;&#38590;&#26679;&#26412;&#65306;&#24490;&#29615;&#33258;&#22686;&#24378;&#30340;&#31283;&#20581;&#26377;&#25928;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Hard Samples: Robust and Effective Grammatical Error Correction with Cycle Self-Augmenting. (arXiv:2310.13321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24207;&#21015;&#21040;&#24207;&#21015;&#33539;&#24335;&#19979;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#33258;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#22686;&#24378;&#25968;&#25454;&#21644;&#24490;&#29615;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#25968;&#25454;&#65292;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24207;&#21015;&#21040;&#24207;&#21015;&#33539;&#24335;&#19979;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#32780;&#20165;&#20165;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#22312;&#39044;&#35757;&#32451;&#25110;&#21518;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;GEC&#27169;&#22411;&#23545;&#26576;&#20123;&#31867;&#22411;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#20960;&#20046;&#19981;&#20250;&#25439;&#22833;&#22826;&#22810;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26368;&#20808;&#36827;&#30340;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;GEC&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#24182;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#38750;&#24120;&#26377;&#25928;&#30340;&#24490;&#29615;&#33258;&#22686;&#24378;&#65288;CSA&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GEC&#27169;&#22411;&#33258;&#36523;&#22312;&#21518;&#35757;&#32451;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#22686;&#24378;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#24490;&#29615;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#20960;&#20010;&#39069;&#22806;&#30340;&#35757;&#32451;&#36718;&#27425;&#20316;&#20026;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;GEC&#27169;&#22411;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#27491;&#21017;&#21270;&#25968;&#25454;&#30340;&#28145;&#20837;&#35757;&#32451;&#21487;&#20197;&#38450;&#27490;GEC&#27169;&#22411;&#36807;&#24230;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed that grammatical error correction methods in the sequence-to-sequence paradigm are vulnerable to adversarial attack, and simply utilizing adversarial examples in the pre-training or post-training process can significantly enhance the robustness of GEC models to certain types of attack without suffering too much performance loss on clean data. In this paper, we further conduct a thorough robustness evaluation of cutting-edge GEC methods for four different types of adversarial attacks and propose a simple yet very effective Cycle Self-Augmenting (CSA) method accordingly. By leveraging the augmenting data from the GEC models themselves in the post-training process and introducing regularization data for cycle training, our proposed method can effectively improve the model robustness of well-trained GEC models with only a few more training epochs as an extra cost. More concretely, further training on the regularization data can prevent the GEC models from over-
&lt;/p&gt;</description></item><item><title>CoFFTEA&#26159;&#19968;&#31181;&#31895;&#32454;&#26694;&#26550;&#21644;&#30446;&#26631;&#32534;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21452;&#32534;&#30721;&#22120;&#26469;&#26377;&#25928;&#24314;&#27169;&#26694;&#26550;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#31895;&#21040;&#32454;&#30340;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#36880;&#28176;&#23398;&#20250;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.13316</link><description>&lt;p&gt;
&#31895;&#32454;&#21452;&#32534;&#30721;&#22120;&#26159;&#26356;&#22909;&#30340;&#24103;&#35782;&#21035;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Coarse-to-Fine Dual Encoders are Better Frame Identification Learners. (arXiv:2310.13316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13316
&lt;/p&gt;
&lt;p&gt;
CoFFTEA&#26159;&#19968;&#31181;&#31895;&#32454;&#26694;&#26550;&#21644;&#30446;&#26631;&#32534;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21452;&#32534;&#30721;&#22120;&#26469;&#26377;&#25928;&#24314;&#27169;&#26694;&#26550;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#31895;&#21040;&#32454;&#30340;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#36880;&#28176;&#23398;&#20250;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24103;&#35782;&#21035;&#26088;&#22312;&#25214;&#21040;&#19982;&#30446;&#26631;&#35789;&#22312;&#21477;&#23376;&#20013;&#30456;&#20851;&#30340;&#35821;&#20041;&#26694;&#26550;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#24314;&#27169;&#26694;&#26550;&#23450;&#20041;&#26469;&#34913;&#37327;&#30446;&#26631;&#21644;&#20505;&#36873;&#26694;&#26550;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#25110;&#21305;&#37197;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#35201;&#20040;&#32570;&#20047;&#23545;&#23450;&#20041;&#30340;&#20805;&#20998;&#34920;&#31034;&#23398;&#20064;&#65292;&#35201;&#20040;&#38754;&#20020;&#22312;&#36229;&#36807;1000&#20010;&#20505;&#36873;&#26694;&#26550;&#20013;&#26377;&#25928;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26694;&#26550;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24120;&#29992;&#30340;&#35789;&#20856;&#36807;&#28388;&#26041;&#27861;&#65288;lf&#65289;&#33719;&#21462;&#30446;&#26631;&#30340;&#20505;&#36873;&#26694;&#26550;&#26102;&#21487;&#33021;&#20250;&#24573;&#30053;&#35789;&#27719;&#34920;&#22806;&#30340;&#30446;&#26631;&#65292;&#24182;&#23548;&#33268;&#26694;&#26550;&#24314;&#27169;&#19981;&#20805;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoFFTEA&#65292;&#21363;&#31895;&#32454;&#26694;&#26550;&#21644;&#30446;&#26631;&#32534;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21452;&#32534;&#30721;&#22120;&#65292;CoFFTEA&#26377;&#25928;&#22320;&#24314;&#27169;&#20102;&#26694;&#26550;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#37319;&#29992;&#31895;&#21040;&#32454;&#30340;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#65292;CoFFTEA&#36880;&#28176;&#23398;&#20250;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frame identification aims to find semantic frames associated with target words in a sentence. Recent researches measure the similarity or matching score between targets and candidate frames by modeling frame definitions. However, they either lack sufficient representation learning of the definitions or face challenges in efficiently selecting the most suitable frame from over 1000 candidate frames. Moreover, commonly used lexicon filtering ($lf$) to obtain candidate frames for the target may ignore out-of-vocabulary targets and cause inadequate frame modeling. In this paper, we propose CoFFTEA, a $\underline{Co}$arse-to-$\underline{F}$ine $\underline{F}$rame and $\underline{T}$arget $\underline{E}$ncoders $\underline{A}$rchitecture. With contrastive learning and dual encoders, CoFFTEA efficiently and effectively models the alignment between frames and targets. By employing a coarse-to-fine curriculum learning procedure, CoFFTEA gradually learns to differentiate frames with varying degr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#38160;&#24230;&#24863;&#30693;&#37327;&#21270;&#65288;ZSAQ&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20248;&#21270;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#37327;&#21270;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13315</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#38160;&#24230;&#24863;&#30693;&#37327;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models. (arXiv:2310.13315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#38160;&#24230;&#24863;&#30693;&#37327;&#21270;&#65288;ZSAQ&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20248;&#21270;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#37327;&#21270;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#19968;&#31181;&#20943;&#23569;&#20869;&#23384;&#24320;&#38144;&#21644;&#21152;&#36895;&#25512;&#26029;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22330;&#26223;&#19979;&#12290;&#30001;&#20110;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#26080;&#27861;&#35775;&#38382;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#23545;&#38646;&#26679;&#26412;&#37327;&#21270;&#30340;&#38656;&#27714;&#36880;&#28176;&#22686;&#21152;&#12290;&#22823;&#37096;&#20998;&#26368;&#26032;&#30340;&#38646;&#26679;&#26412;&#37327;&#21270;&#26041;&#27861;&#20027;&#35201;&#36866;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#24573;&#35270;&#20102;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#21508;&#31181;PLM&#30340;&#38646;&#26679;&#26412;&#38160;&#24230;&#24863;&#30693;&#37327;&#21270;&#65288;ZSAQ&#65289;&#26694;&#26550;&#12290;&#35299;&#20915;ZSAQ&#30340;&#20851;&#38190;&#31639;&#27861;&#26159;SAM-SGA&#20248;&#21270;&#65292;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#26469;&#25552;&#39640;&#37327;&#21270;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#36825;&#20010;&#32467;&#26524;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#38750;&#20984;PL&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is a promising approach for reducing memory overhead and accelerating inference, especially in large pre-trained language model (PLM) scenarios. While having no access to original training data due to security and privacy concerns has emerged the demand for zero-shot quantization. Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to sub-optimal performance. Motivated by this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ) framework for the zero-shot quantization of various PLMs. The key algorithm in solving ZSAQ is the SAM-SGA optimization, which aims to improve the quantization accuracy and model generalization via optimizing a minimax problem. We theoretically prove the convergence rate for the minimax optimization problem and this result can be applied to other nonconvex-PL minimax optimization frameworks. Ext
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#37329;&#34701;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#26009;&#24211;&#22810;&#26679;&#24615;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#37329;&#34701;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#26032;&#27169;&#22411;FiLM&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13312</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#26009;&#24211;&#22810;&#26679;&#24615;&#23545;&#37329;&#34701;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models. (arXiv:2310.13312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#37329;&#34701;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#26009;&#24211;&#22810;&#26679;&#24615;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#37329;&#34701;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#26032;&#27169;&#22411;FiLM&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#12289;&#31185;&#23398;&#21644;&#20020;&#24202;&#31561;&#19987;&#19994;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#37329;&#34701;&#25968;&#25454;&#20998;&#26512;&#30340;&#37325;&#22823;&#32463;&#27982;&#24433;&#21709;&#65292;&#37329;&#34701;PLM&#20063;&#24471;&#21040;&#20102;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#37329;&#34701;PLMs&#22312;&#39044;&#35757;&#32451;&#20013;&#27809;&#26377;&#20805;&#20998;&#22810;&#26679;&#21270;&#30340;&#37329;&#34701;&#25968;&#25454;&#12290;&#36825;&#31181;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#23548;&#33268;&#20102;&#36739;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#36890;&#29992;&#30340;PLMs&#65292;&#21253;&#25324;BERT&#65292;&#24448;&#24448;&#20248;&#20110;&#37329;&#34701;PLMs&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#21508;&#31181;&#24191;&#27867;&#30340;&#37329;&#34701;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#65288;FiLM&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#25454;&#35777;&#23454;&#65292;FiLM&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#30340;&#37329;&#34701;PLMs&#65292;&#32780;&#19988;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;PLMs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#65292;&#21363;&#36825;&#31181;&#25913;&#36827;&#21363;&#20351;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#26009;&#32452;&#20063;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, various domain-specific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic impact of financial data analysis. However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in general-purpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks. To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets. Our experimental results confirm that FiLM outperforms not only existing financial PLMs but also general domain PLMs. Furthermore, we provide empirical evidence that this improvement can be achieved even for unseen corpus groups.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#26080;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#30340;&#23567;&#22411;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#22810;&#20010;&#31572;&#26696;&#24182;&#36827;&#34892;&#38598;&#25104;&#65292;&#20197;&#36798;&#21040;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13307</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Test-Time Self-Adaptive Small Language Models for Question Answering. (arXiv:2310.13307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#26080;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#30340;&#23567;&#22411;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#22810;&#20010;&#31572;&#26696;&#24182;&#36827;&#34892;&#38598;&#25104;&#65292;&#20197;&#36798;&#21040;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#32489;&#65292;&#20363;&#22914;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#33021;&#22815;&#35760;&#24518;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#24120;&#35782;&#65292;&#20294;&#30001;&#20110;&#33021;&#21147;&#26377;&#38480;&#65292;&#36716;&#31227;&#21644;&#36866;&#24212;&#30693;&#35782;&#21040;&#30446;&#26631;&#20219;&#21153;&#26102;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32570;&#23569;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#24102;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#24494;&#35843;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20294;&#25105;&#20204;&#20063;&#21487;&#20197;&#36890;&#36807;&#20165;&#20351;&#29992;&#26080;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#26469;&#36716;&#31227;&#20855;&#26377;&#26377;&#38480;&#30693;&#35782;&#30340;&#26356;&#23567;&#35821;&#35328;&#27169;&#22411;&#20063;&#26159;&#19968;&#20010;&#20540;&#24471;&#25506;&#31350;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#24182;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#26080;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#30340;&#36739;&#23567;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#38543;&#26426;&#29983;&#25104;&#22810;&#20010;&#31572;&#26696;&#65292;&#28982;&#21518;&#22312;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#26679;&#26412;&#30340;&#21516;&#26102;&#23545;&#23427;&#20204;&#36827;&#34892;&#38598;&#25104;&#65292;&#20197;&#20943;&#36731;&#19981;&#20934;&#30830;&#26631;&#31614;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#22312;&#22522;&#20934;QA&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness ac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SocialSense&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35825;&#23548;&#20449;&#24565;&#22686;&#24378;&#30340;&#22270;&#21644;&#22522;&#20110;&#22270;&#30340;&#20256;&#25773;&#26469;&#39044;&#27979;&#26032;&#38395;&#21457;&#24067;&#30340;&#21709;&#24212;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#27809;&#26377;&#29992;&#25143;&#26126;&#30830;&#20010;&#20154;&#36164;&#26009;&#25110;&#21382;&#21490;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#31038;&#20132;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.13297</link><description>&lt;p&gt;
&#35299;&#30721;&#27785;&#40664;&#30340;&#22823;&#22810;&#25968;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35825;&#23548;&#20449;&#24565;&#22686;&#24378;&#30340;&#31038;&#20132;&#22270;&#36827;&#34892;&#21709;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting. (arXiv:2310.13297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13297
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SocialSense&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35825;&#23548;&#20449;&#24565;&#22686;&#24378;&#30340;&#22270;&#21644;&#22522;&#20110;&#22270;&#30340;&#20256;&#25773;&#26469;&#39044;&#27979;&#26032;&#38395;&#21457;&#24067;&#30340;&#21709;&#24212;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#27809;&#26377;&#29992;&#25143;&#26126;&#30830;&#20010;&#20154;&#36164;&#26009;&#25110;&#21382;&#21490;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#31038;&#20132;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#23186;&#20307;&#30340;&#33258;&#21160;&#21709;&#24212;&#39044;&#27979;&#22312;&#24110;&#21161;&#20869;&#23481;&#29983;&#20135;&#32773;&#26377;&#25928;&#39044;&#27979;&#26032;&#38395;&#21457;&#24067;&#30340;&#24433;&#21709;&#24182;&#38450;&#27490;&#24847;&#22806;&#30340;&#36127;&#38754;&#32467;&#26524;&#65288;&#22914;&#31038;&#20250;&#20914;&#31361;&#21644;&#36947;&#24503;&#20260;&#23475;&#65289;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#39044;&#27979;&#21709;&#24212;&#65292;&#24517;&#39035;&#24320;&#21457;&#33021;&#22815;&#21033;&#29992;&#20010;&#20307;&#21608;&#22260;&#30340;&#31038;&#20132;&#21160;&#24577;&#21644;&#32972;&#26223;&#20449;&#24687;&#30340;&#25514;&#26045;&#65292;&#23588;&#20854;&#26159;&#22312;&#29992;&#25143;&#26126;&#30830;&#30340;&#20010;&#20154;&#36164;&#26009;&#25110;&#21382;&#21490;&#34892;&#20026;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#28508;&#22312;&#21442;&#19982;&#32773;&#65289;&#12290;&#27491;&#22914;&#20808;&#21069;&#30340;&#30740;&#31350;&#25152;&#31034;&#65292;97%&#30340;&#25512;&#25991;&#20165;&#30001;&#26368;&#27963;&#36291;&#30340;25%&#29992;&#25143;&#20135;&#29983;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23545;&#20110;&#22914;&#20309;&#22788;&#29702;&#21644;&#21033;&#29992;&#36825;&#20123;&#37325;&#35201;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#24335;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SocialSense&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#26377;&#31038;&#20132;&#32593;&#32476;&#20043;&#19978;&#35825;&#23548;&#20986;&#19968;&#20010;&#20197;&#20449;&#24565;&#20026;&#20013;&#24515;&#30340;&#22270;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#30340;&#20256;&#25773;&#26469;&#25429;&#25417;&#31038;&#20132;&#21160;&#24577;&#12290;&#25105;&#20204;&#20551;&#35774;&#35825;&#23548;&#20986;&#30340;&#22270;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#21457;&#29616;&#25688;&#35201;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#30340;&#39118;&#38505;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#20445;&#25252;&#25514;&#26045;&#21644;&#38544;&#31169;&#19982;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.13291</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#65306;&#20197;&#25688;&#35201;&#20219;&#21153;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks. (arXiv:2310.13291v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#21457;&#29616;&#25688;&#35201;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#30340;&#39118;&#38505;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#20445;&#25252;&#25514;&#26045;&#21644;&#38544;&#31169;&#19982;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#27844;&#38706;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#35843;&#26597;&#25104;&#21592;&#25512;&#26029;&#65288;MI&#65289;&#25915;&#20987;&#65306;&#36890;&#36807;&#32473;&#20986;&#19968;&#20010;&#26679;&#26412;&#21644;&#23545;&#27169;&#22411;API&#30340;&#40657;&#30418;&#35775;&#38382;&#65292;&#21487;&#20197;&#30830;&#23450;&#26679;&#26412;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#30456;&#20284;&#24615;&#21644;&#27169;&#22411;&#23545;&#25991;&#26723;&#20462;&#25913;&#30340;&#25269;&#25239;&#21147;&#20316;&#20026;&#28508;&#22312;&#30340;MI&#20449;&#21495;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25688;&#35201;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#30340;&#39118;&#38505;&#65292;&#29978;&#33267;&#22312;&#27809;&#26377;&#21442;&#32771;&#25688;&#35201;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35757;&#32451;&#25688;&#35201;&#27169;&#22411;&#20197;&#38450;&#27490;MI&#25915;&#20987;&#30340;&#20960;&#31181;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#35752;&#35770;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#22266;&#26377;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have revolutionized the field of NLP by achieving state-of-the-art performance on various tasks. However, there is a concern that these models may disclose information in the training data. In this study, we focus on the summarization task and investigate the membership inference (MI) attack: given a sample and black-box access to a model's API, it is possible to determine if the sample was part of the training data. We exploit text similarity and the model's resistance to document modifications as potential MI signals and evaluate their effectiveness on widely used datasets. Our results demonstrate that summarization models are at risk of exposing data membership, even in cases where the reference summary is not available. Furthermore, we discuss several safeguards for training summarization models to protect against MI attacks and discuss the inherent trade-off between privacy and utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#35299;&#35835;&#23545;&#20110;&#26159;&#21542;&#38382;&#39064;&#30340;&#38388;&#25509;&#22238;&#31572;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#20843;&#31181;&#35821;&#35328;&#20013;&#21457;&#24067;&#26032;&#30340;&#22522;&#20934;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#21518;&#65292;&#21333;&#35821;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#24494;&#35843;&#37117;&#33021;&#24102;&#26469;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.13290</link><description>&lt;p&gt;
&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#35299;&#35835;&#23545;&#20110;&#26159;&#21542;&#38382;&#39064;&#30340;&#38388;&#25509;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Interpreting Indirect Answers to Yes-No Questions in Multiple Languages. (arXiv:2310.13290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#35299;&#35835;&#23545;&#20110;&#26159;&#21542;&#38382;&#39064;&#30340;&#38388;&#25509;&#22238;&#31572;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#20843;&#31181;&#35821;&#35328;&#20013;&#21457;&#24067;&#26032;&#30340;&#22522;&#20934;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#21518;&#65292;&#21333;&#35821;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#24494;&#35843;&#37117;&#33021;&#24102;&#26469;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26159;&#19982;&#21542;&#30340;&#38382;&#39064;&#26399;&#26395;&#24471;&#21040;&#26159;&#25110;&#21542;&#30340;&#22238;&#31572;&#65292;&#20294;&#20154;&#20204;&#32463;&#24120;&#36339;&#36807;&#26497;&#24615;&#20851;&#38190;&#35789;&#12290;&#30456;&#21453;&#65292;&#20182;&#20204;&#29992;&#38271;&#31687;&#35299;&#37322;&#26469;&#22238;&#31572;&#65292;&#32780;&#36825;&#20123;&#35299;&#37322;&#38656;&#35201;&#36827;&#34892;&#35299;&#35835;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20843;&#31181;&#35821;&#35328;&#20013;&#21457;&#24067;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#30452;&#25509;&#22238;&#31572;&#65288;&#21363;&#24102;&#26377;&#26497;&#24615;&#20851;&#38190;&#35789;&#65289;&#23545;&#20110;&#35757;&#32451;&#27169;&#22411;&#35299;&#35835;&#38388;&#25509;&#22238;&#31572;&#65288;&#21363;&#19981;&#24102;&#26377;&#26497;&#24615;&#20851;&#38190;&#35789;&#65289;&#26159;&#26377;&#29992;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#21487;&#20197;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#33719;&#24471;&#24863;&#20852;&#36259;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21017;&#21333;&#35821;&#24494;&#35843;&#26159;&#26377;&#30410;&#30340;&#65288;5&#31181;&#35821;&#35328;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36328;&#35821;&#35328;&#24494;&#35843;&#24635;&#26159;&#26377;&#30410;&#30340;&#65288;8&#31181;&#35821;&#35328;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yes-no questions expect a yes or no for an answer, but people often skip polar keywords. Instead, they answer with long explanations that must be interpreted. In this paper, we focus on this challenging problem and release new benchmarks in eight languages. We present a distant supervision approach to collect training data. We also demonstrate that direct answers (i.e., with polar keywords) are useful to train models to interpret indirect answers (i.e., without polar keywords). Experimental results demonstrate that monolingual fine-tuning is beneficial if training data can be obtained via distant supervision for the language of interest (5 languages). Additionally, we show that cross-lingual fine-tuning is always beneficial (8 languages).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SALMONN&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;/&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#30452;&#25509;&#22788;&#29702;&#21644;&#29702;&#35299;&#26222;&#36890;&#38899;&#39057;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#35821;&#38899;&#21644;&#38899;&#39057;&#20219;&#21153;&#19978;&#21462;&#24471;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.13289</link><description>&lt;p&gt;
SALMONN&#65306;&#36808;&#21521;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#21548;&#35273;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SALMONN: Towards Generic Hearing Abilities for Large Language Models. (arXiv:2310.13289v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SALMONN&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;/&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#30452;&#25509;&#22788;&#29702;&#21644;&#29702;&#35299;&#26222;&#36890;&#38899;&#39057;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#35821;&#38899;&#21644;&#38899;&#39057;&#20219;&#21153;&#19978;&#21462;&#24471;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#21487;&#20197;&#35828;&#26159;&#29289;&#29702;&#19990;&#30028;&#20013;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#29702;&#30340;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#33021;&#21147;&#65292;&#23427;&#28041;&#21450;&#21040;&#23545;&#33267;&#23569;&#19977;&#31181;&#22768;&#38899;&#31867;&#22411;&#65288;&#35821;&#38899;&#12289;&#38899;&#39057;&#20107;&#20214;&#21644;&#38899;&#20048;&#65289;&#30340;&#26222;&#36890;&#21548;&#35273;&#20449;&#24687;&#30340;&#24863;&#30693;&#21644;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SALMONN&#65292;&#19968;&#31181;&#35821;&#38899;&#38899;&#39057;&#35821;&#35328;&#38899;&#20048;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#35821;&#38899;&#21644;&#38899;&#39057;&#32534;&#30721;&#22120;&#38598;&#25104;&#21040;&#21333;&#19968;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#36827;&#34892;&#26500;&#24314;&#12290;SALMONN&#20351;&#24471;LLM&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#21644;&#29702;&#35299;&#26222;&#36890;&#38899;&#39057;&#36755;&#20837;&#65292;&#22312;&#35757;&#32451;&#20013;&#22312;&#35768;&#22810;&#35821;&#38899;&#21644;&#38899;&#39057;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#65292;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#12289;&#22522;&#20110;&#21548;&#35273;&#20449;&#24687;&#30340;&#38382;&#39064;&#22238;&#31572;&#12289;&#24773;&#24863;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#39564;&#35777;&#20197;&#21450;&#38899;&#20048;&#21644;&#38899;&#39057;&#23383;&#24149;&#31561;&#12290;SALMONN&#36824;&#20855;&#26377;&#22312;&#35757;&#32451;&#20013;&#26410;&#35265;&#30340;&#21508;&#31181;&#26032;&#33021;&#21147;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#23545;&#26410;&#35757;&#32451;&#35821;&#35328;&#30340;&#35821;&#38899;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning \textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languag
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#36827;&#34892;&#30340;&#40065;&#26834;&#36328;&#27169;&#24577;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#20998;&#31163;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.13276</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#23454;&#29616;&#40065;&#26834;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution. (arXiv:2310.13276v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13276
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#36827;&#34892;&#30340;&#40065;&#26834;&#36328;&#27169;&#24577;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#20998;&#31163;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#37325;&#22823;&#36827;&#23637;&#20027;&#35201;&#26159;&#36890;&#36807;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#31361;&#30772;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#25968;&#25454;&#34920;&#31034;&#24448;&#24448;&#22312;&#26377;&#38480;&#30340;&#20984;&#38181;&#20869;&#32858;&#38598;&#65288;&#20316;&#20026;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65289;&#65292;&#36825;&#30001;&#20110;&#36825;&#20123;&#34920;&#31034;&#30340;&#19981;&#21487;&#20998;&#31163;&#24615;&#32780;&#38459;&#30861;&#20102;&#26816;&#32034;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22810;&#20010;&#36328;&#27169;&#24577;&#22522;&#20934;&#21644;&#26041;&#27861;&#32463;&#39564;&#35777;&#23454;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#30340;&#23384;&#22312;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;InvGC&#65292;&#23427;&#26159;&#19968;&#31181;&#21463;&#22270;&#21367;&#31215;&#21644;&#24179;&#22343;&#27744;&#21270;&#21551;&#21457;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;InvGC&#22312;&#25968;&#25454;&#38598;&#20013;&#23450;&#20041;&#22270;&#25299;&#25169;&#65292;&#28982;&#21518;&#24212;&#29992;&#22270;&#21367;&#31215;&#20197;&#19968;&#31181;&#20943;&#27861;&#30340;&#26041;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#22320;&#20998;&#31163;&#34920;&#31034;&#12290;&#20026;&#20102;&#25552;&#39640;InvGC&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32423;&#22270;&#25299;&#25169;&#65292;Lo
&lt;/p&gt;
&lt;p&gt;
Over recent decades, significant advancements in cross-modal retrieval are mainly driven by breakthroughs in visual and linguistic modeling. However, a recent study shows that multi-modal data representations tend to cluster within a limited convex cone (as representation degeneration problem), which hinders retrieval performance due to the inseparability of these representations. In our study, we first empirically validate the presence of the representation degeneration problem across multiple cross-modal benchmarks and methods. Next, to address it, we introduce a novel method, called InvGC, a post-processing technique inspired by graph convolution and average pooling. Specifically, InvGC defines the graph topology within the datasets and then applies graph convolution in a subtractive manner. This method effectively separates representations by increasing the distances between data points. To improve the efficiency and effectiveness of InvGC, we propose an advanced graph topology, Lo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#23545;&#27604;&#36328;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#21457;&#29616;&#22312;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25552;&#39640;&#20102;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#36136;&#37327;&#24182;&#25913;&#21892;&#20102;&#36328;&#27169;&#24577;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#32780;&#22312;&#38899;&#39057;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25928;&#26524;&#36739;&#23567;&#12290;&#21478;&#22806;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25913;&#21892;&#20102;&#25991;&#26412;&#31354;&#38388;&#30340;&#22343;&#21248;&#24615;&#65292;&#20294;&#38477;&#20302;&#20102;&#36328;&#27169;&#24577;&#23545;&#40784;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.13267</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#27604;&#36328;&#27169;&#24577;&#27169;&#22411;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Language Encoder of Contrastive Cross-modal Models. (arXiv:2310.13267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#23545;&#27604;&#36328;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#21457;&#29616;&#22312;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25552;&#39640;&#20102;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#36136;&#37327;&#24182;&#25913;&#21892;&#20102;&#36328;&#27169;&#24577;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#32780;&#22312;&#38899;&#39057;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25928;&#26524;&#36739;&#23567;&#12290;&#21478;&#22806;&#65292;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25913;&#21892;&#20102;&#25991;&#26412;&#31354;&#38388;&#30340;&#22343;&#21248;&#24615;&#65292;&#20294;&#38477;&#20302;&#20102;&#36328;&#27169;&#24577;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#36328;&#27169;&#24577;&#27169;&#22411;&#22914;CLIP&#21644;CLAP&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#21644;&#38899;&#39057;&#35821;&#35328;&#20219;&#21153;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#30740;&#31350;&#21644;&#25913;&#36827;&#36824;&#24456;&#26377;&#38480;&#65292;&#32780;&#35821;&#35328;&#32534;&#30721;&#22120;&#26159;&#23558;&#22270;&#20687;/&#38899;&#39057;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#32534;&#30721;&#25104;&#21521;&#37327;&#34920;&#31034;&#30340;&#26680;&#24515;&#32452;&#20214;&#12290;&#25105;&#20204;&#24191;&#27867;&#35780;&#20272;&#20102;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#30340;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#23545;&#35821;&#35328;&#32534;&#30721;&#22120;&#36136;&#37327;&#21644;&#36328;&#27169;&#24577;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#25552;&#39640;&#20102;&#35821;&#35328;&#32534;&#30721;&#22120;&#36136;&#37327;&#65292;&#24182;&#26377;&#21161;&#20110;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#25913;&#36827;&#20102;&#35832;&#22914;CyCLIP&#31561;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38899;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#30340;&#25928;&#30410;&#36739;&#23567;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#34920;&#31034;&#31354;&#38388;&#65292;&#20197;&#20102;&#35299;&#21477;&#23376;&#23884;&#20837;&#35757;&#32451;&#30340;&#20248;&#21183;&#65292;&#24182;&#21457;&#29616;&#23427;&#25913;&#21892;&#20102;&#25991;&#26412;&#31354;&#38388;&#30340;&#22343;&#21248;&#24615;&#65292;&#20294;&#20195;&#20215;&#26159;&#38477;&#20302;&#20102;&#36328;&#27169;&#24577;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive cross-modal models such as CLIP and CLAP aid various vision-language (VL) and audio-language (AL) tasks. However, there has been limited investigation of and improvement in their language encoder, which is the central component of encoding natural language descriptions of image/audio into vector representations. We extensively evaluate how unsupervised and supervised sentence embedding training affect language encoder quality and cross-modal task performance. In VL pretraining, we found that sentence embedding training language encoder quality and aids in cross-modal tasks, improving contrastive VL models such as CyCLIP. In contrast, AL pretraining benefits less from sentence embedding training, which may result from the limited amount of pretraining data. We analyze the representation spaces to understand the strengths of sentence embedding training, and find that it improves text-space uniformity, at the cost of decreased cross-modal alignment.
&lt;/p&gt;</description></item><item><title>MoqaGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#27169;&#24335;&#22810;&#27169;&#24577;&#24320;&#25918;&#22495;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.13265</link><description>&lt;p&gt;
MoqaGPT: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#27169;&#24335;&#22810;&#27169;&#24577;&#24320;&#25918;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model. (arXiv:2310.13265v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13265
&lt;/p&gt;
&lt;p&gt;
MoqaGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#27169;&#24335;&#22810;&#27169;&#24577;&#24320;&#25918;&#22495;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24320;&#25918;&#22495;&#38382;&#31572;&#36890;&#24120;&#38656;&#35201;&#20174;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#22914;&#22270;&#20687;&#12289;&#34920;&#26684;&#12289;&#27573;&#33853;&#31561;&#65289;&#30340;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#35777;&#25454;&#12290;&#21363;&#20351;&#26159;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#38646;&#23556;&#27169;&#24335;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MoqaGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#30452;&#35266;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#19968;&#31181;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#32469;&#36807;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25490;&#24207;&#65292;&#21487;&#20197;&#23481;&#32435;&#26032;&#30340;&#27169;&#24577;&#65292;&#24182;&#26080;&#32541;&#36716;&#25442;&#21040;&#20219;&#21153;&#30340;&#26032;&#27169;&#22411;&#12290;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;MoqaGPT&#20998;&#21035;&#20174;&#27599;&#20010;&#27169;&#24577;&#20013;&#26816;&#32034;&#21644;&#25552;&#21462;&#31572;&#26696;&#65292;&#28982;&#21518;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#36825;&#20123;&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20135;&#29983;&#26368;&#32456;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MMCoQA&#25968;&#25454;&#38598;&#19978;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#30456;&#27604;&#26377;&#30417;&#30563;&#22522;&#20934;&#32447;&#65292;F1&#25552;&#39640;&#20102;37.91&#20010;&#28857;&#65292;EM&#25552;&#39640;&#20102;34.07&#20010;&#28857;&#12290;&#22312;MultiModalQA&#25968;&#25454;&#38598;&#19978;&#65292;MoqaGPT&#36229;&#36234;&#20102;&#38646;&#23556;&#22522;&#20934;&#32447;&#65292;F1&#25552;&#39640;&#20102;9.5&#20010;&#28857;&#65292;EM&#25552;&#39640;&#20102;10.1&#20010;&#28857;&#65292;&#24182;&#26174;&#33879;&#32553;&#23567;&#20102;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal open-domain question answering typically requires evidence retrieval from databases across diverse modalities, such as images, tables, passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this task. To enable LLMs to tackle the task in a zero-shot manner, we introduce MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer strategy that bypasses intricate multi-modality ranking, our framework can accommodate new modalities and seamlessly transition to new models for the task. Built upon LLMs, MoqaGPT retrieves and extracts answers from each modality separately, then fuses this multi-modal information using LLMs to produce a final answer. Our methodology boosts performance on the MMCoQA dataset, improving F1 by +37.91 points and EM by +34.07 points over the supervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the zero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and significantly closes the gap wit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#30340;&#21477;&#27861;&#27169;&#26495;&#26816;&#32034;&#22120;&#65288;QSTR&#65289;&#65292;&#29992;&#20110;&#21477;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#12290;&#36890;&#36807;&#33719;&#21462;&#36136;&#37327;&#36739;&#39640;&#30340;&#24453;&#29983;&#25104;&#37322;&#20041;&#26469;&#26816;&#32034;&#27169;&#26495;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#26679;&#24615;&#27169;&#26495;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#37322;&#20041;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;QSTR&#21487;&#20197;&#26174;&#33879;&#36229;&#36234;&#29616;&#26377;&#30340;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13262</link><description>&lt;p&gt;
&#22522;&#20110;&#36136;&#37327;&#30340;&#21477;&#27861;&#27169;&#26495;&#26816;&#32034;&#22120;&#29992;&#20110;&#21477;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A Quality-based Syntactic Template Retriever for Syntactically-controlled Paraphrase Generation. (arXiv:2310.13262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#30340;&#21477;&#27861;&#27169;&#26495;&#26816;&#32034;&#22120;&#65288;QSTR&#65289;&#65292;&#29992;&#20110;&#21477;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#12290;&#36890;&#36807;&#33719;&#21462;&#36136;&#37327;&#36739;&#39640;&#30340;&#24453;&#29983;&#25104;&#37322;&#20041;&#26469;&#26816;&#32034;&#27169;&#26495;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#26679;&#24615;&#27169;&#26495;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#37322;&#20041;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;QSTR&#21487;&#20197;&#26174;&#33879;&#36229;&#36234;&#29616;&#26377;&#30340;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21477;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#27169;&#22411;&#22312;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#25110;&#31934;&#24515;&#36873;&#25321;&#30340;&#21477;&#27861;&#27169;&#26495;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36825;&#20123;&#21477;&#27861;&#27169;&#26495;&#30340;&#22256;&#38590;&#23454;&#38469;&#19978;&#38459;&#30861;&#20102;&#21477;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#26114;&#36149;&#30340;&#25104;&#26412;&#20351;&#24471;&#20026;&#27599;&#20010;&#28304;&#21477;&#23376;&#25163;&#21160;&#35774;&#35745;&#21512;&#36866;&#30340;&#27169;&#26495;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#20854;&#27425;&#65292;&#24403;&#21069;&#21551;&#21457;&#24335;&#26041;&#27861;&#33258;&#21160;&#26816;&#32034;&#30340;&#27169;&#26495;&#36890;&#24120;&#23545;&#20110;&#29983;&#25104;&#21512;&#26684;&#30340;&#37322;&#20041;&#32780;&#35328;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#25670;&#33073;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36136;&#37327;&#30340;&#21477;&#27861;&#27169;&#26495;&#26816;&#32034;&#22120;&#65288;QSTR&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#24453;&#29983;&#25104;&#37322;&#20041;&#30340;&#36136;&#37327;&#26469;&#26816;&#32034;&#27169;&#26495;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#20010;&#28304;&#21477;&#23376;&#38656;&#35201;&#22810;&#20010;&#37322;&#20041;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#26679;&#24615;&#27169;&#26495;&#25628;&#32034;&#65288;DTS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#37322;&#20041;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#32780;&#19981;&#25439;&#23475;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;QSTR&#21487;&#20197;&#26174;&#33879;&#36229;&#36234;&#29616;&#26377;&#30340;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing syntactically-controlled paraphrase generation (SPG) models perform promisingly with human-annotated or well-chosen syntactic templates. However, the difficulty of obtaining such templates actually hinders the practical application of SPG models. For one thing, the prohibitive cost makes it unfeasible to manually design decent templates for every source sentence. For another, the templates automatically retrieved by current heuristic methods are usually unreliable for SPG models to generate qualified paraphrases. To escape this dilemma, we propose a novel Quality-based Syntactic Template Retriever (QSTR) to retrieve templates based on the quality of the to-be-generated paraphrases. Furthermore, for situations requiring multiple paraphrases for each source sentence, we design a Diverse Templates Search (DTS) algorithm, which can enhance the diversity between paraphrases without sacrificing quality. Experiments demonstrate that QSTR can significantly surpass existing retrieval m
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#30340;&#35821;&#35328;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13257</link><description>&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#35270;&#35273;&#23450;&#20301;&#26377;&#21161;&#20110;&#23398;&#20064;&#21333;&#35789;&#30340;&#21547;&#20041;
&lt;/p&gt;
&lt;p&gt;
Visual Grounding Helps Learn Word Meanings in Low-Data Regimes. (arXiv:2310.13257v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13257
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#30340;&#35821;&#35328;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26159;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#21477;&#23376;&#20135;&#29983;&#21644;&#29702;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20854;&#20869;&#37096;&#34920;&#36798;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#30340;&#35821;&#35328;&#34920;&#36798;&#38750;&#24120;&#21563;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21462;&#24471;&#36825;&#20123;&#32467;&#26524;&#65292;LM&#24517;&#39035;&#20197;&#19982;&#20154;&#31867;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#27604;&#20799;&#31461;&#22312;&#21457;&#32946;&#36807;&#31243;&#20013;&#25509;&#25910;&#21040;&#30340;&#35821;&#35328;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#24863;&#30693;&#12289;&#34892;&#21160;&#25110;&#31038;&#20132;&#34892;&#20026;&#30340;&#22522;&#30784;&#12290;&#22914;&#26524;&#29992;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#20381;&#38752;&#24863;&#30693;&#30340;&#30417;&#30563;&#65292;&#27169;&#22411;&#30340;&#35821;&#35328;&#23398;&#20064;&#26159;&#21542;&#26356;&#25509;&#36817;&#20154;&#31867;&#65311;&#25105;&#20204;&#22312;&#21333;&#35789;&#23398;&#20064;&#36825;&#19968;&#35821;&#35328;&#20064;&#24471;&#30340;&#20851;&#38190;&#23376;&#20219;&#21153;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;LM&#26550;&#26500;&#65292;&#24182;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#36741;&#21161;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#21477;&#27861;&#31867;&#21035;&#12289;&#35789;&#27719;&#20851;&#31995;&#12289;&#35821;&#20041;&#23398;&#31561;&#26041;&#38754;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural language models (LMs) are powerful tools for modeling human sentence production and comprehension, and their internal representations are remarkably well-aligned with representations of language in the human brain. But to achieve these results, LMs must be trained in distinctly un-human-like ways -- requiring orders of magnitude more language data than children receive during development, and without any of the accompanying grounding in perception, action, or social behavior. Do models trained more naturalistically -- with grounded supervision -- exhibit more human-like language learning? We investigate this question in the context of word learning, a key sub-task in language acquisition. We train a diverse set of LM architectures, with and without auxiliary supervision from image captioning tasks, on datasets of varying scales. We then evaluate these models on a broad set of benchmarks characterizing models' learning of syntactic categories, lexical relations, semantic f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;DistilBERT&#27169;&#22411;, &#32508;&#21512;&#24212;&#29992;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102; Unix &#21629;&#20196;&#34892;&#20250;&#35805;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#25429;&#25417;&#21629;&#20196;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#65292;&#23454;&#29616;&#23545;&#20250;&#35805;&#20013;&#19982;&#27491;&#24120;&#34892;&#20026;&#20559;&#31163;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#20225;&#19994;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20984;&#26174;&#20102;&#21033;&#29992; Transformer &#35299;&#20915;&#35745;&#31639;&#26426;&#23433;&#20840;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13247</link><description>&lt;p&gt;
&#22522;&#20110;DistilBERT&#30340;&#21629;&#20196;&#34892;&#20250;&#35805;&#24322;&#24120;&#26816;&#27979;&#65306;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection of Command Shell Sessions based on DistilBERT: Unsupervised and Supervised Approaches. (arXiv:2310.13247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;DistilBERT&#27169;&#22411;, &#32508;&#21512;&#24212;&#29992;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102; Unix &#21629;&#20196;&#34892;&#20250;&#35805;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#25429;&#25417;&#21629;&#20196;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#65292;&#23454;&#29616;&#23545;&#20250;&#35805;&#20013;&#19982;&#27491;&#24120;&#34892;&#20026;&#20559;&#31163;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#20225;&#19994;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20984;&#26174;&#20102;&#21033;&#29992; Transformer &#35299;&#20915;&#35745;&#31639;&#26426;&#23433;&#20840;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#20196;&#34892;&#20250;&#35805;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#23433;&#20840;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#22312;&#35299;&#20915;&#22797;&#26434;&#23433;&#20840;&#25361;&#25112;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;DistilBERT&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;Unix&#21629;&#20196;&#34892;&#20250;&#35805;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#25968;&#25454;&#26631;&#35760;&#12290;&#26080;&#30417;&#30563;&#26041;&#27861;&#25429;&#25417;Unix&#21629;&#20196;&#34892;&#21629;&#20196;&#30340;&#24213;&#23618;&#32467;&#26500;&#21644;&#35821;&#27861;&#65292;&#23454;&#29616;&#23545;&#20250;&#35805;&#19982;&#27491;&#24120;&#34892;&#20026;&#30340;&#20559;&#24046;&#30340;&#26816;&#27979;&#12290;&#22312;&#20174;&#29983;&#20135;&#31995;&#32479;&#25910;&#38598;&#30340;&#22823;&#35268;&#27169;&#20225;&#19994;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26816;&#27979;Unix&#21629;&#20196;&#34892;&#20250;&#35805;&#20013;&#24322;&#24120;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#20984;&#26174;&#20102;&#21033;&#29992;Transformer&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#35299;&#20915;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#23433;&#20840;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in command shell sessions is a critical aspect of computer security. Recent advances in deep learning and natural language processing, particularly transformer-based models, have shown great promise for addressing complex security challenges. In this paper, we implement a comprehensive approach to detect anomalies in Unix shell sessions using a pretrained DistilBERT model, leveraging both unsupervised and supervised learning techniques to identify anomalous activity while minimizing data labeling. The unsupervised method captures the underlying structure and syntax of Unix shell commands, enabling the detection of session deviations from normal behavior. Experiments on a large-scale enterprise dataset collected from production systems demonstrate the effectiveness of our approach in detecting anomalous behavior in Unix shell sessions. This work highlights the potential of leveraging recent advances in transformers to address important computer security challenges.
&lt;/p&gt;</description></item><item><title>&#26368;&#26032;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;&#30340;&#25490;&#21517;&#33021;&#21147;&#65292;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#30417;&#30563;&#25351;&#23548;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#22815;&#36827;&#34892;&#26377;&#25928;&#30340;&#25490;&#24207;&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25490;&#24207;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;&#19982;&#38646;-shot&#26816;&#32034;&#22120;&#38598;&#25104;&#65292;&#19981;&#20165;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#22312;&#23569;-shot&#22330;&#26223;&#20013;&#20063;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13243</link><description>&lt;p&gt;
&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#29992;&#20110;&#25991;&#26723;&#25490;&#24207;&#30340;&#38646;-shot&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking. (arXiv:2310.13243v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13243
&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;&#30340;&#25490;&#21517;&#33021;&#21147;&#65292;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#30417;&#30563;&#25351;&#23548;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#22815;&#36827;&#34892;&#26377;&#25928;&#30340;&#25490;&#24207;&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25490;&#24207;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;&#19982;&#38646;-shot&#26816;&#32034;&#22120;&#38598;&#25104;&#65292;&#19981;&#20165;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#22312;&#23569;-shot&#22330;&#26223;&#20013;&#20063;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#20013;&#65292;&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;&#65288;QLMs&#65289;&#26681;&#25454;&#29983;&#25104;&#26597;&#35810;&#30340;&#27010;&#29575;&#26469;&#23545;&#25991;&#26723;&#36827;&#34892;&#25490;&#24207;&#12290;&#26368;&#36817;&#65292;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#26377;&#25928;&#30340;QLMs&#65292;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#25490;&#24207;&#33021;&#21147;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#26368;&#36817;LLMs&#30340;&#30495;&#23454;&#38646;-shot&#25490;&#24207;&#25928;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#20165;&#22312;&#26080;&#32467;&#26500;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#27809;&#26377;&#36827;&#34892;&#30417;&#30563;&#25351;&#23548;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#36825;&#20123;LLMs&#30340;&#24378;&#22823;&#38646;-shot&#25490;&#24207;&#33021;&#21147;&#65292;&#21516;&#26102;&#24378;&#35843;&#38500;&#38750;&#24494;&#35843;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#38382;&#31572;&#29983;&#25104;&#20219;&#21153;&#65292;&#21542;&#21017;&#39069;&#22806;&#30340;&#25351;&#23548;&#24494;&#35843;&#21487;&#33021;&#20250;&#38477;&#20302;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20808;&#36827;&#25490;&#24207;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#22522;&#20110;LLM&#30340;QLMs&#19982;&#28151;&#21512;&#38646;-shot&#26816;&#32034;&#22120;&#38598;&#25104;&#65292;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#21644;&#23569;-shot&#22330;&#26223;&#20013;&#30340;&#20986;&#33394;&#25928;&#26524;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#20844;&#24320;&#22312;https://github.com/ielab/llm-qlm&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21095;&#26412;&#20013;&#35282;&#33394;&#30340;&#29702;&#35299;&#65292;&#20174;&#35282;&#33394;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#20854;&#20010;&#24615;&#21644;&#36523;&#20221;&#65292;&#24182;&#22312;&#22810;&#20010;&#35282;&#33394;&#29702;&#35299;&#23376;&#20219;&#21153;&#19978;&#36890;&#36807;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.13231</link><description>&lt;p&gt;
&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#21095;&#26412;&#30340;&#35282;&#33394;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Multi-level Contrastive Learning for Script-based Character Understanding. (arXiv:2310.13231v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21095;&#26412;&#20013;&#35282;&#33394;&#30340;&#29702;&#35299;&#65292;&#20174;&#35282;&#33394;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#20854;&#20010;&#24615;&#21644;&#36523;&#20221;&#65292;&#24182;&#22312;&#22810;&#20010;&#35282;&#33394;&#29702;&#35299;&#23376;&#20219;&#21153;&#19978;&#36890;&#36807;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21095;&#26412;&#20013;&#30340;&#35282;&#33394;&#29702;&#35299;&#22330;&#26223;&#65292;&#26088;&#22312;&#20174;&#20182;&#20204;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#35282;&#33394;&#30340;&#20010;&#24615;&#21644;&#36523;&#20221;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#36825;&#19968;&#22330;&#26223;&#20013;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#25429;&#25417;&#35282;&#33394;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;SpanBERT&#65292;Longformer&#65292;BigBird&#21644;ChatGPT-3.5&#65289;&#36827;&#34892;&#20102;&#19977;&#20010;&#35282;&#33394;&#29702;&#35299;&#23376;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#25361;&#25112;&#21644;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#35282;&#33394;&#29702;&#35299;&#22330;&#26223;&#30340;&#32447;&#32034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#22312;github&#19978;&#24320;&#28304;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#32593;&#22336;&#20026;https://github.com/David-Li0406/Script-based-Character-Understanding&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters' personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters' global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong pre-trained language models, including SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate that our method improves the performances by a considerable margin. Through further in-depth analysis, we show the effectiveness of our method in addressing the challenges and provide more hints on the scenario of character understanding. We will open-source our work on github at https://github.com/David-Li0406/Script-based-Character-Understanding.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#35328;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#27969;&#34892;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#25903;&#25345;&#21644;&#24573;&#35270;&#30340;&#35821;&#35328;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#35821;&#35328;&#30340;&#23398;&#20064;&#34920;&#31034;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13228</link><description>&lt;p&gt;
&#36234;&#23569;&#36234;&#22909;&#65311;&#25506;&#31350;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
The Less the Merrier? Investigating Language Representation in Multilingual Models. (arXiv:2310.13228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13228
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#35328;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#27969;&#34892;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#25903;&#25345;&#21644;&#24573;&#35270;&#30340;&#35821;&#35328;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#35821;&#35328;&#30340;&#23398;&#20064;&#34920;&#31034;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#22810;&#31181;&#35821;&#35328;&#21512;&#24182;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#65292;&#24182;&#21033;&#29992;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#22810;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#35821;&#35328;&#37117;&#24471;&#21040;&#20102;&#21516;&#26679;&#30340;&#25903;&#25345;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#27969;&#34892;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#25903;&#25345;&#21738;&#20123;&#35821;&#35328;&#65292;&#21738;&#20123;&#35821;&#35328;&#34987;&#24573;&#35270;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#21253;&#21547;&#30340;&#35821;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#35821;&#31995;&#21644;&#26041;&#35328;&#26469;&#35266;&#23519;&#27169;&#22411;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#23581;&#35797;&#29702;&#35299;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#32452;&#20013;&#23545;&#20110;&#65288;1&#65289;&#24050;&#30693;&#35821;&#35328;&#21644;&#65288;2&#65289;&#26410;&#30693;&#35821;&#35328;&#30340;&#23398;&#20064;&#34920;&#31034;&#22914;&#20309;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27979;&#35797;&#24182;&#20998;&#26512;&#20102;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20197;&#31038;&#21306;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#8212;&#8212;
&lt;/p&gt;
&lt;p&gt;
Multilingual Language Models offer a way to incorporate multiple languages in one model and utilize cross-language transfer learning to improve performance for different Natural Language Processing (NLP) tasks. Despite progress in multilingual models, not all languages are supported as well, particularly in low-resource settings. In this work, we investigate the linguistic representation of different languages in multilingual models. We start by asking the question which languages are supported in popular multilingual models and which languages are left behind. Then, for included languages, we look at models' learned representations based on language family and dialect and try to understand how models' learned representations for~(1) seen and~(2) unseen languages vary across different language groups. In addition, we test and analyze performance on downstream tasks such as text generation and Named Entity Recognition. We observe from our experiments that community-centered models -- mo
&lt;/p&gt;</description></item><item><title>ToolChain*&#26159;&#19968;&#31181;&#25552;&#20379;&#20102;&#39640;&#25928;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20915;&#31574;&#21644;&#35268;&#21010;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13227</link><description>&lt;p&gt;
ToolChain*: &#20351;&#29992;A*&#25628;&#32034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search. (arXiv:2310.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13227
&lt;/p&gt;
&lt;p&gt;
ToolChain*&#26159;&#19968;&#31181;&#25552;&#20379;&#20102;&#39640;&#25928;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20915;&#31574;&#21644;&#35268;&#21010;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#26102;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#20915;&#31574;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#21487;&#20197;&#19982;&#21508;&#31181;&#24037;&#20855;&#65288;&#20363;&#22914;&#21151;&#33021;&#24615;API&#65289;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#29983;&#25104;&#25191;&#34892;&#19968;&#31995;&#21015;API&#20989;&#25968;&#35843;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#35745;&#21010;&#12290;&#20505;&#36873;API&#20989;&#25968;&#35843;&#29992;&#30340;&#22810;&#26679;&#24615;&#26174;&#33879;&#25193;&#23637;&#20102;&#21160;&#20316;&#31354;&#38388;&#65292;&#21152;&#22823;&#20102;&#23545;&#39640;&#25928;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#22312;&#24222;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#20013;&#38754;&#20020;&#21333;&#21521;&#25506;&#32034;&#22256;&#38590;&#65292;&#38519;&#20837;&#23616;&#37096;&#20248;&#21270;&#35299;&#65292;&#35201;&#20040;&#36973;&#21463;&#31351;&#23613;&#22320;&#36941;&#21382;&#25152;&#26377;&#28508;&#22312;&#21160;&#20316;&#25152;&#23548;&#33268;&#30340;&#20302;&#25928;&#23548;&#33322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolChain*&#65292;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;LLM&#20195;&#29702;&#35268;&#21010;&#31639;&#27861;&#12290;&#23427;&#23558;&#25972;&#20010;&#21160;&#20316;&#31354;&#38388;&#26500;&#24314;&#20026;&#19968;&#20010;&#20915;&#31574;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#35299;&#20915;&#26041;&#26696;&#35745;&#21010;&#20013;&#28041;&#21450;&#30340;&#21487;&#33021;&#30340;API&#20989;&#25968;&#35843;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#25216;&#26415;&#20197;&#25552;&#39640;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#24773;&#32490;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#31934;&#35843;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#31934;&#35843;&#65292;&#24182;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#38646;&#30693;&#35782;&#34920;&#29616;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.13226</link><description>&lt;p&gt;
&#25552;&#21319;&#38646;&#30693;&#35782;&#21152;&#23494;&#24773;&#32490;&#20998;&#26512;&#30340;&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering. (arXiv:2310.13226v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#25216;&#26415;&#20197;&#25552;&#39640;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#24773;&#32490;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#31934;&#35843;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#31934;&#35843;&#65292;&#24182;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#38646;&#30693;&#35782;&#34920;&#29616;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#24050;&#32463;&#25913;&#21464;&#20102;&#37329;&#34701;&#39046;&#22495;&#65292;&#30001;&#20110;&#21435;&#20013;&#24515;&#21270;&#21644;&#36879;&#26126;&#30340;&#29305;&#28857;&#65292;&#21152;&#23494;&#36135;&#24065;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25509;&#21463;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#34920;&#36798;&#30340;&#24773;&#32490;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#30340;&#35752;&#35770;&#21644;&#24066;&#22330;&#21464;&#21160;&#65292;&#24773;&#32490;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#20026;&#20102;&#25552;&#39640;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#24773;&#32490;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#35843;&#25216;&#26415;&#12290;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21033;&#29992;&#26377;&#30417;&#30563;&#31934;&#35843;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#31934;&#35843;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31934;&#35843;&#21518;&#65292;&#24179;&#22343;&#38646;&#30693;&#35782;&#34920;&#29616;&#25552;&#21319;&#20102;40%&#65292;&#20984;&#26174;&#20102;&#36825;&#19968;&#25216;&#26415;&#22312;&#20248;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#23545;&#19981;&#21516;&#35268;&#27169;&#27169;&#22411;&#30340;&#25351;&#20196;&#31934;&#35843;&#30340;&#24433;&#21709;&#20063;&#34987;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain technology has revolutionized the financial landscape, with cryptocurrencies gaining widespread adoption for their decentralized and transparent nature. As the sentiment expressed on social media platforms can significantly influence cryptocurrency discussions and market movements, sentiment analysis has emerged as a crucial tool for understanding public opinion and predicting market trends. Motivated by the aim to enhance sentiment analysis accuracy in the cryptocurrency domain, this paper investigates fine-tuning techniques on large language models. This paper also investigates the efficacy of supervised fine-tuning and instruction-based fine-tuning on large language models for unseen tasks. Experimental results demonstrate a significant average zero-shot performance gain of 40% after fine-tuning, highlighting the potential of this technique in optimizing pre-trained language model efficiency. Additionally, the impact of instruction tuning on models of varying scales is ex
&lt;/p&gt;</description></item><item><title>MultiCoNER v2&#26159;&#19968;&#20010;&#22823;&#22411;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#21644;&#21547;&#22122;&#38899;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#23427;&#35299;&#20915;&#20102;&#32454;&#31890;&#24230;&#31867;&#21035;&#22788;&#29702;&#21644;&#22122;&#38899;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#32454;&#31890;&#24230;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#23454;&#20307;&#25439;&#22351;&#23545;&#24615;&#33021;&#24433;&#21709;&#26356;&#22823;&#12290;</title><link>http://arxiv.org/abs/2310.13213</link><description>&lt;p&gt;
MultiCoNER v2: &#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#21644;&#21547;&#22122;&#38899;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition. (arXiv:2310.13213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13213
&lt;/p&gt;
&lt;p&gt;
MultiCoNER v2&#26159;&#19968;&#20010;&#22823;&#22411;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#21644;&#21547;&#22122;&#38899;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#23427;&#35299;&#20915;&#20102;&#32454;&#31890;&#24230;&#31867;&#21035;&#22788;&#29702;&#21644;&#22122;&#38899;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#32454;&#31890;&#24230;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#23454;&#20307;&#25439;&#22351;&#23545;&#24615;&#33021;&#24433;&#21709;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MULTICONER V2&#65292;&#36825;&#26159;&#19968;&#20010;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;12&#31181;&#35821;&#35328;&#20013;&#30340;33&#20010;&#23454;&#20307;&#31867;&#21035;&#65292;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#22659;&#30340;&#35774;&#32622;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#35299;&#20915;NER&#20013;&#30340;&#20197;&#19979;&#23454;&#38469;&#25361;&#25112;&#65306;(i) &#26377;&#25928;&#22788;&#29702;&#21253;&#25324;&#30005;&#24433;&#26631;&#39064;&#31561;&#22797;&#26434;&#23454;&#20307;&#30340;&#32454;&#31890;&#24230;&#31867;&#21035;&#65292;&#20197;&#21450;(ii) &#30001;&#20110;&#25171;&#23383;&#38169;&#35823;&#25110;OCR&#38169;&#35823;&#32780;&#20135;&#29983;&#30340;&#22122;&#38899;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#32500;&#22522;&#30334;&#31185;&#21644;&#32500;&#22522;&#25968;&#25454;&#31561;&#20844;&#24320;&#36164;&#28304;&#20013;&#32534;&#35793;&#32780;&#25104;&#65292;&#24182;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;&#22522;&#20110;XLM-RoBERTa&#22522;&#20934;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;MULTICONER V2&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65306;(i)&#32454;&#31890;&#24230;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#30340;&#23439;F1&#24471;&#20998;&#36739;&#20302;&#65292;&#20026;0.63&#65307;&#20197;&#21450;(ii)&#25439;&#22351;&#31574;&#30053;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#65292;&#23454;&#20307;&#25439;&#22351;&#23548;&#33268;&#30340;&#24615;&#33021;&#27604;&#38750;&#23454;&#20307;&#25439;&#22351;&#20302;9%&#12290;&#36825;&#31361;&#26174;&#20102;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#23454;&#20307;&#22122;&#38899;&#30456;&#27604;&#20110;&#20854;&#20182;&#22122;&#38899;&#23545;&#24615;&#33021;&#30340;&#26356;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MULTICONER V2, a dataset for fine-grained Named Entity Recognition covering 33 entity classes across 12 languages, in both monolingual and multilingual settings. This dataset aims to tackle the following practical challenges in NER: (i) effective handling of fine-grained classes that include complex entities like movie titles, and (ii) performance degradation due to noise generated from typing mistakes or OCR errors. The dataset is compiled from open resources like Wikipedia and Wikidata, and is publicly available. Evaluation based on the XLM-RoBERTa baseline highlights the unique challenges posed by MULTICONER V2: (i) the fine-grained taxonomy is challenging, where the scores are low with macro-F1=0.63 (across all languages), and (ii) the corruption strategy significantly impairs performance, with entity corruption resulting in 9% lower performance relative to non-entity corruptions across all languages. This highlights the greater impact of entity noise in contrast to cont
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#36873;&#25321;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#25552;&#31034;&#20013;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13206</link><description>&lt;p&gt;
ChatGPT&#30340;&#21021;&#21360;&#35937;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Primacy Effect of ChatGPT. (arXiv:2310.13206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13206
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#36873;&#25321;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#25552;&#31034;&#20013;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;ChatGPT&#19978;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#30340;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#36873;&#25321;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;i&#65289;ChatGPT&#30340;&#20915;&#31574;&#23545;&#25552;&#31034;&#20013;&#26631;&#31614;&#30340;&#39034;&#24207;&#25935;&#24863;&#65307;ii&#65289;ChatGPT&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#33021;&#20026;&#26500;&#24314;&#26356;&#21487;&#38752;&#30340;&#22522;&#20110;ChatGPT&#30340;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#39069;&#22806;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherits humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25193;&#23637;&#34920;&#26684;&#25968;&#25454;&#20013;&#21015;&#21517;&#31216;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#32553;&#20889;&#21015;&#21517;&#31216;&#23545;&#25968;&#25454;&#25628;&#32034;&#12289;&#35775;&#38382;&#21644;&#29702;&#35299;&#20219;&#21153;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20316;&#32773;&#25552;&#20986;&#30340;NameGuess&#26041;&#27861;&#36890;&#36807;&#23545;&#34920;&#26684;&#20869;&#23481;&#21644;&#21015;&#22836;&#21517;&#31216;&#36827;&#34892;&#35843;&#25972;&#65292;&#24471;&#21040;&#20102;&#19982;&#20154;&#31867;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13196</link><description>&lt;p&gt;
NameGuess&#65306;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#21015;&#21517;&#31216;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
NameGuess: Column Name Expansion for Tabular Data. (arXiv:2310.13196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13196
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25193;&#23637;&#34920;&#26684;&#25968;&#25454;&#20013;&#21015;&#21517;&#31216;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#32553;&#20889;&#21015;&#21517;&#31216;&#23545;&#25968;&#25454;&#25628;&#32034;&#12289;&#35775;&#38382;&#21644;&#29702;&#35299;&#20219;&#21153;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20316;&#32773;&#25552;&#20986;&#30340;NameGuess&#26041;&#27861;&#36890;&#36807;&#23545;&#34920;&#26684;&#20869;&#23481;&#21644;&#21015;&#22836;&#21517;&#31216;&#36827;&#34892;&#35843;&#25972;&#65292;&#24471;&#21040;&#20102;&#19982;&#20154;&#31867;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#21253;&#25324;&#25968;&#25454;&#24211;&#34892;&#19994;&#12290;&#22312;&#22788;&#29702;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#26222;&#36941;&#20351;&#29992;&#32553;&#20889;&#30340;&#21015;&#21517;&#31216;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#21508;&#31181;&#25968;&#25454;&#25628;&#32034;&#12289;&#35775;&#38382;&#21644;&#29702;&#35299;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21517;&#20026;NameGuess&#65292;&#23558;&#25193;&#23637;&#21015;&#21517;&#31216;&#65288;&#29992;&#20110;&#25968;&#25454;&#24211;&#27169;&#24335;&#65289;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#21046;&#20316;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;384K&#20010;&#32553;&#20889;-&#25193;&#23637;&#21015;&#23545;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#35780;&#20272;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#34920;&#26684;&#30340;9.2K&#20010;&#20363;&#23376;&#12290;&#20026;&#20102;&#24212;&#23545;NameGuess&#20013;&#30340;&#22810;&#20041;&#21644;&#27495;&#20041;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#34920;&#26684;&#20869;&#23481;&#21644;&#21015;&#22836;&#21517;&#31216;&#36827;&#34892;&#35843;&#25972;&#26469;&#22686;&#24378;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#65288;&#20855;&#26377;2.7B&#20010;&#21442;&#25968;&#65289;&#65292;&#20854;&#24615;&#33021;&#19982;&#20154;&#31867;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65288;o
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact performance on various data search, access, and understanding tasks. To address this issue, we introduce a new task, called NameGuess, to expand column names (used in database schema) as a natural language generation problem. We create a training dataset of 384K abbreviated-expanded column pairs using a new data fabrication method and a human-annotated evaluation benchmark that includes 9.2K examples from real-world tables. To tackle the complexities associated with polysemy and ambiguity in NameGuess, we enhance auto-regressive language models by conditioning on table content and column header names -- yielding a fine-tuned model (with 2.7B parameters) that matches human performance. Furthermore, we conduct a comprehensive analysis (o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.13191</link><description>&lt;p&gt;
&#26397;&#30528;&#40065;&#26834;&#21098;&#26525;&#65306;&#19968;&#31181;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#30446;&#26631;&#36817;&#26399;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#20934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36824;&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#25345;&#32493;&#22686;&#21152;&#27169;&#22411;&#31232;&#30095;&#24615;&#26102;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#12290;&#38543;&#30528;&#20154;&#20204;&#27493;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36825;&#20123;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#19982;&#20854;&#28085;&#30422;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#31243;&#24230;&#25104;&#27491;&#27604;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#20197;&#24544;&#23454;&#22320;&#22797;&#21046;&#23494;&#38598;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27599;&#19968;&#23618;&#30340;&#37325;&#26500;&#35823;&#24046;&#19981;&#20165;&#28304;&#33258;&#33258;&#36523;&#65292;&#36824;&#21253;&#25324;&#21069;&#38754;&#23618;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#28982;&#21518;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#30699;&#27491;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance bet
&lt;/p&gt;</description></item><item><title>SCALE&#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#38271;&#25991;&#26412;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#22359;&#21270;&#31574;&#30053;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#35299;&#37322;&#20915;&#31574;&#24182;&#36229;&#36807;&#31454;&#20105;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.13189</link><description>&lt;p&gt;
&#38271;&#25991;&#26723;&#20013;&#24555;&#36895;&#20934;&#30830;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fast and Accurate Factual Inconsistency Detection Over Long Documents. (arXiv:2310.13189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13189
&lt;/p&gt;
&lt;p&gt;
SCALE&#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#38271;&#25991;&#26412;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#22359;&#21270;&#31574;&#30053;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#35299;&#37322;&#20915;&#31574;&#24182;&#36229;&#36807;&#31454;&#20105;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#27169;&#22411;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#24403;&#21069;&#26041;&#27861;&#38590;&#20197;&#26377;&#25928;&#35299;&#20915;&#30340;&#36739;&#38271;&#36755;&#20837;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SCALE&#65288;&#29992;&#20110;&#22823;&#35268;&#27169;&#19981;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#28304;&#22359;&#21270;&#26041;&#27861;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22359;&#21270;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SCALE&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22823;&#30340;&#25991;&#26412;&#22359;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#26465;&#20214;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22359;&#21270;&#26426;&#21046;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#36890;&#36807;&#30456;&#20851;&#30340;&#28304;&#35821;&#21477;&#26816;&#32034;&#26469;&#35299;&#37322;SCALE&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;SCALE&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#21644;&#25105;&#20204;&#26500;&#24314;&#30340;&#26032;&#30340;&#38271;&#24418;&#24335;&#23545;&#35805;&#25968;&#25454;&#38598;ScreenEval&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;SCALE&#36229;&#36807;&#20102;&#31454;&#20105;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI) based model that uses large text chunks to condition over long texts. This approach achieves state-of-the-art performance in factual inconsistency detection for diverse tasks and long inputs. Additionally, we leverage the chunking mechanism and employ a novel algorithm to explain SCALE's decisions through relevant source sentence retrieval. Our evaluations reveal that SCALE outperforms existing methods on both standard benchmarks and a new long-form dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses competitive systems 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#22411;&#20462;&#21098;&#30340;&#38543;&#26426;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#38543;&#26426;&#20462;&#21098;&#25513;&#30721;&#65292;&#24182;&#32467;&#21512;&#26377;&#25928;&#30340;&#36873;&#25321;&#35268;&#21017;&#36873;&#21462;&#26368;&#20248;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#22312;&#20843;&#20010;GLUE&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13183</link><description>&lt;p&gt;
&#25171;&#30772;&#30830;&#23450;&#24615;&#38480;&#21046;&#65306;&#38543;&#26426;&#20462;&#21098;&#25513;&#30721;&#30340;&#29983;&#25104;&#21644;&#36873;&#21462;
&lt;/p&gt;
&lt;p&gt;
Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection. (arXiv:2310.13183v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#22411;&#20462;&#21098;&#30340;&#38543;&#26426;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#38543;&#26426;&#20462;&#21098;&#25513;&#30721;&#65292;&#24182;&#32467;&#21512;&#26377;&#25928;&#30340;&#36873;&#25321;&#35268;&#21017;&#36873;&#21462;&#26368;&#20248;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#22312;&#20843;&#20010;GLUE&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30456;&#21516;&#27169;&#22411;&#23610;&#23544;&#32422;&#26463;&#19979;&#65292;&#22823;&#19988;&#31232;&#30095;&#30340;&#27169;&#22411;&#24448;&#24448;&#27604;&#23567;&#19988;&#23494;&#38598;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36890;&#36807;&#20462;&#21098;&#26469;&#31227;&#38500;&#20887;&#20313;&#30340;&#31070;&#32463;&#20803;&#25110;&#26435;&#37325;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#20462;&#21098;&#65292;&#20854;&#24615;&#33021;&#20165;&#20381;&#36182;&#20110;&#21333;&#19968;&#30340;&#20462;&#21098;&#20934;&#21017;&#65292;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#20462;&#21098;&#31574;&#30053;&#65292;&#39318;&#20808;&#20197;&#35774;&#35745;&#22909;&#30340;&#38543;&#26426;&#26041;&#24335;&#29983;&#25104;&#22810;&#20010;&#20462;&#21098;&#25513;&#30721;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#25513;&#30721;&#36873;&#25321;&#35268;&#21017;&#65292;&#20174;&#20505;&#36873;&#25513;&#30721;&#27744;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#25513;&#30721;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26089;&#26399;&#25513;&#30721;&#35780;&#20272;&#31574;&#30053;&#65292;&#20943;&#36731;&#20102;&#35757;&#32451;&#22810;&#20010;&#25513;&#30721;&#25152;&#24102;&#26469;&#30340;&#24320;&#38144;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;GLUE&#30340;&#20843;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#22312;&#39640;&#31232;&#30095;&#31243;&#24230;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely acknowledged that large and sparse models have higher accuracy than small and dense models under the same model size constraints. This motivates us to train a large model and then remove its redundant neurons or weights by pruning. Most existing works pruned the networks in a deterministic way, the performance of which solely depends on a single pruning criterion and thus lacks variety. Instead, in this paper, we propose a model pruning strategy that first generates several pruning masks in a designed random way. Subsequently, along with an effective mask-selection rule, the optimal mask is chosen from the pool of mask candidates. To further enhance efficiency, we introduce an early mask evaluation strategy, mitigating the overhead associated with training multiple masks. Our extensive experiments demonstrate that this approach achieves state-of-the-art performance across eight datasets from GLUE, particularly excelling at high levels of sparsity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20020;&#24202;&#39046;&#22495;&#38382;&#31572;&#27169;&#22411;&#27979;&#35797;&#24179;&#21488;CLIFT&#65292;&#20854;&#20013;&#21253;&#25324;7.5k&#20010;&#39640;&#36136;&#37327;&#30340;&#38382;&#31572;&#26679;&#26412;&#12290;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#27979;&#35797;&#38598;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25552;&#39640;&#20020;&#24202;&#39046;&#22495;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36861;&#36394;&#35813;&#26041;&#21521;&#36827;&#23637;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2310.13146</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#38382;&#31572;&#27169;&#22411;&#20013;&#20998;&#26512;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#30340;CLIFT
&lt;/p&gt;
&lt;p&gt;
CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain. (arXiv:2310.13146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20020;&#24202;&#39046;&#22495;&#38382;&#31572;&#27169;&#22411;&#27979;&#35797;&#24179;&#21488;CLIFT&#65292;&#20854;&#20013;&#21253;&#25324;7.5k&#20010;&#39640;&#36136;&#37327;&#30340;&#38382;&#31572;&#26679;&#26412;&#12290;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#27979;&#35797;&#38598;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25552;&#39640;&#20020;&#24202;&#39046;&#22495;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36861;&#36394;&#35813;&#26041;&#21521;&#36827;&#23637;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#24179;&#21488;CLIFT&#65288;&#20020;&#24202;&#20998;&#24067;&#20559;&#31227;&#65289;&#65292;&#29992;&#20110;&#20020;&#24202;&#39046;&#22495;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27979;&#35797;&#24179;&#21488;&#21253;&#25324;7.5k&#20010;&#39640;&#36136;&#37327;&#30340;&#38382;&#31572;&#26679;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#19988;&#21487;&#38752;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#22312;&#35813;&#27979;&#35797;&#24179;&#21488;&#19979;&#35780;&#20272;&#20102;&#20960;&#20010;QA&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#27979;&#35797;&#38598;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#25552;&#39640;&#20020;&#24202;&#39046;&#22495;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#21644;&#28508;&#21147;&#12290;&#35813;&#27979;&#35797;&#24179;&#21488;&#20026;&#36861;&#36394;&#35813;&#26041;&#21521;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;&#23427;&#36824;&#24378;&#35843;&#20102;&#37319;&#29992;&#32771;&#34385;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;&#36890;&#36807;&#28155;&#21152;&#26356;&#22810;&#26679;&#26412;&#21644;&#27169;&#22411;&#32467;&#26524;&#26469;&#25193;&#23637;&#35821;&#26009;&#24211;&#12290;&#23436;&#25972;&#30340;&#35770;&#25991;&#21644;&#26356;&#26032;&#30340;&#22522;&#20934;&#21487;&#20197;&#22312;github.com/openlifescience-ai/clift&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new testbed CLIFT (Clinical Shift) for the clinical domain Question-answering task. The testbed includes 7.5k high-quality question answering samples to provide a diverse and reliable benchmark. We performed a comprehensive experimental study and evaluated several QA deep-learning models under the proposed testbed. Despite impressive results on the original test set, the performance degrades when applied to new test sets, which shows the distribution shift. Our findings emphasize the need for and the potential for increasing the robustness of clinical domain models under distributional shifts. The testbed offers one way to track progress in that direction. It also highlights the necessity of adopting evaluation metrics that consider robustness to natural distribution shifts. We plan to expand the corpus by adding more samples and model results. The full paper and the updated benchmark are available at github.com/openlifescience-ai/clift
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30740;&#31350;LLMs&#20316;&#20026;&#21307;&#30103;&#38382;&#39064;&#30340;&#22810;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13132</link><description>&lt;p&gt;
&#19981;&#22952;&#29992;&#33521;&#25991;&#38382;&#25105;&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21307;&#30103;&#38382;&#39064;&#36328;&#35821;&#35328;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Ask Me in English Instead: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries. (arXiv:2310.13132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30740;&#31350;LLMs&#20316;&#20026;&#21307;&#30103;&#38382;&#39064;&#30340;&#22810;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#25913;&#21464;&#19968;&#33324;&#20844;&#20247;&#33719;&#21462;&#21644;&#28040;&#36153;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#23427;&#20204;&#22312;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24433;&#21709;&#23588;&#20026;&#26174;&#33879;&#65292;&#26222;&#36890;&#20154;&#26085;&#24120;&#26597;&#35810;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;LLMs&#20316;&#20026;&#23545;&#35805;&#20195;&#29702;&#12290;&#34429;&#28982;LLMs&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#20851;&#27880;&#22312;&#36825;&#20123;&#39640;&#39118;&#38505;&#39046;&#22495;&#20173;&#28982;&#24456;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#24320;&#21457;&#36807;&#20110;&#38598;&#20013;&#22312;&#33521;&#25991;&#19978;&#12290;&#36825;&#20123;LLMs&#22312;&#38750;&#33521;&#25991;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#19981;&#26126;&#30830;&#65292;&#36825;&#26159;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#20844;&#24179;&#24615;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30740;&#31350;LLMs&#20316;&#20026;&#22810;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#22312;&#21307;&#30103;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#24471;&#20986;&#30340;&#26694;&#26550;XlingEval&#65292;&#37325;&#28857;&#20851;&#27880;&#23545;LLMs&#22238;&#31572;&#33258;&#28982;&#20154;&#25776;&#20889;&#30340;&#19982;&#20581;&#24247;&#30456;&#20851;&#38382;&#39064;&#30340;&#35780;&#20272;&#30340;&#19977;&#20010;&#22522;&#26412;&#26631;&#20934;&#65306;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems.This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: co
&lt;/p&gt;</description></item><item><title>Auto-Instruct&#26159;&#19968;&#31181;&#33258;&#21160;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25351;&#20196;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20135;&#29983;&#22810;&#26679;&#30340;&#20505;&#36873;&#25351;&#20196;&#65292;&#24182;&#20351;&#29992;&#35780;&#20998;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#25490;&#24207;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#20196;&#21644;&#29616;&#26377;LLM&#29983;&#25104;&#25351;&#20196;&#22522;&#32447;&#30340;&#24615;&#33021;&#65292;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13127</link><description>&lt;p&gt;
Auto-Instruct: &#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25351;&#20196;&#29983;&#25104;&#19982;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models. (arXiv:2310.13127v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13127
&lt;/p&gt;
&lt;p&gt;
Auto-Instruct&#26159;&#19968;&#31181;&#33258;&#21160;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25351;&#20196;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20135;&#29983;&#22810;&#26679;&#30340;&#20505;&#36873;&#25351;&#20196;&#65292;&#24182;&#20351;&#29992;&#35780;&#20998;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#25490;&#24207;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#20196;&#21644;&#29616;&#26377;LLM&#29983;&#25104;&#25351;&#20196;&#22522;&#32447;&#30340;&#24615;&#33021;&#65292;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25351;&#20196;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#32780;&#20026;&#27599;&#20010;&#20219;&#21153;&#25163;&#21160;&#32534;&#20889;&#26377;&#25928;&#30340;&#25351;&#20196;&#26159;&#19968;&#39033;&#32321;&#29712;&#32780;&#20027;&#35266;&#30340;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Auto-Instruct&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#25552;&#39640;&#25552;&#20379;&#32473;LLMs&#30340;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#30340;&#20869;&#22312;&#29983;&#25104;&#33021;&#21147;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#22810;&#26679;&#30340;&#20505;&#36873;&#25351;&#20196;&#65292;&#28982;&#21518;&#20351;&#29992;&#22312;&#22810;&#31181;575&#20010;&#29616;&#26377;NLP&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#35780;&#20998;&#27169;&#22411;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#24207;&#12290;&#22312;118&#20010;&#39046;&#22495;&#22806;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;Auto-Instruct&#36229;&#36234;&#20102;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#20196;&#21644;&#29616;&#26377;&#30340;LLM&#29983;&#25104;&#30340;&#25351;&#20196;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#20351;&#20351;&#29992;&#20854;&#20182;&#26410;&#21253;&#25324;&#22312;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;LLMs&#20063;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can perform a wide range of tasks by following natural language instructions, without the necessity of task-specific fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process. In this paper, we introduce Auto-Instruct, a novel method to automatically improve the quality of instructions provided to LLMs. Our method leverages the inherent generative ability of LLMs to produce diverse candidate instructions for a given task, and then ranks them using a scoring model trained on a variety of 575 existing NLP tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both human-written instructions and existing baselines of LLM-generated instructions. Furthermore, our method exhibits notable generalizability even with other LLMs that are not incorporated into its training process.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20505;&#36873;&#31572;&#26696;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#24494;&#30340;&#36974;&#32617;-&#37325;&#26500;&#27169;&#22411;(DMR)&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#20505;&#36873;&#31572;&#26696;&#65292;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#36873;&#25321;&#26174;&#33879;&#30340;&#20449;&#24687;&#26631;&#35760;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19982;&#21463;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2310.13106</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#30340;&#27169;&#22411;&#23454;&#29616;&#26080;&#30417;&#30563;&#20505;&#36873;&#31572;&#26696;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model. (arXiv:2310.13106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13106
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20505;&#36873;&#31572;&#26696;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#24494;&#30340;&#36974;&#32617;-&#37325;&#26500;&#27169;&#22411;(DMR)&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#20505;&#36873;&#31572;&#26696;&#65292;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#36873;&#25321;&#26174;&#33879;&#30340;&#20449;&#24687;&#26631;&#35760;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19982;&#21463;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#29983;&#25104;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#12289;&#26377;&#30528;&#24191;&#27867;&#24212;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#21512;&#26684;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#22823;&#22810;&#25968;&#38382;&#39064;&#29983;&#25104;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20505;&#36873;&#31572;&#26696;&#25552;&#21462;&#26041;&#27861;&#20381;&#36182;&#20110;&#35821;&#35328;&#35268;&#21017;&#25110;&#27880;&#37322;&#36807;&#30340;&#25968;&#25454;&#65292;&#38754;&#20020;&#37096;&#20998;&#27880;&#37322;&#38382;&#39064;&#21644;&#27867;&#21270;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#20505;&#36873;&#31572;&#26696;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#36974;&#32617;-&#37325;&#26500;&#27169;&#22411;(DMR)&#21033;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#26469;&#36873;&#25321;&#26174;&#33879;&#30340;&#20449;&#24687;&#26631;&#35760;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#20004;&#20010;&#32463;&#36807;&#35814;&#23613;&#27880;&#37322;&#30340;&#31572;&#26696;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#19968;&#32452;&#20840;&#38754;&#30340;&#21463;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20505;&#36873;&#31572;&#26696;&#25552;&#21462;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;DMR&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#26126;&#23427;&#22312;&#26080;&#30417;&#30563;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19982;&#21463;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question generation is a widely used data augmentation approach with extensive applications, and extracting qualified candidate answers from context passages is a critical step for most question generation systems. However, existing methods for candidate answer extraction are reliant on linguistic rules or annotated data that face the partial annotation issue and challenges in generalization. To overcome these limitations, we propose a novel unsupervised candidate answer extraction approach that leverages the inherent structure of context passages through a Differentiable Masker-Reconstructor (DMR) Model with the enforcement of self-consistency for picking up salient information tokens. We curated two datasets with exhaustively-annotated answers and benchmark a comprehensive set of supervised and unsupervised candidate answer extraction methods. We demonstrate the effectiveness of the DMR model by showing its performance is superior among unsupervised methods and comparable to supervis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21477;&#32423;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#31215;&#26497;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#26469;&#25913;&#21464;&#40657;&#30418;&#26377;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#22343;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.13099</link><description>&lt;p&gt;
&#19981;&#20882;&#29359;&#65292;&#20271;&#29305; - &#25105;&#21482;&#20398;&#36785;&#20154;&#31867;&#65281;&#22810;&#20010;&#25910;&#20214;&#20154;&#21477;&#32423;&#25915;&#20987;&#26377;&#27602;&#24615;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network. (arXiv:2310.13099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21477;&#32423;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#31215;&#26497;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#26469;&#25913;&#21464;&#40657;&#30418;&#26377;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#22343;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21477;&#32423;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#40657;&#30418;&#26377;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#20167;&#24680;&#20449;&#24687;&#26411;&#23614;&#28155;&#21152;&#19968;&#20123;&#31215;&#26497;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#65292;&#25105;&#20204;&#33021;&#22815;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#26377;&#27602;&#24615;&#26816;&#27979;&#31995;&#32479;&#30340;&#26816;&#26597;&#12290;&#35813;&#26041;&#27861;&#22312;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;&#19971;&#31181;&#35821;&#35328;&#19978;&#34987;&#35777;&#26126;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#23545;&#25239;&#19978;&#36848;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. This approach is shown to be working on seven languages from three different language families. We also describe the defence mechanism against the aforementioned attack and discuss its limitations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21040;&#27861;&#24459;&#23454;&#20307;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#23454;&#20307;&#31867;&#22411;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;Llama2&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.13092</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#65292;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21040;&#27861;&#24459;&#23454;&#20307;&#31867;&#22411;?
&lt;/p&gt;
&lt;p&gt;
Do Language Models Learn about Legal Entity Types during Pretraining?. (arXiv:2310.13092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21040;&#27861;&#24459;&#23454;&#20307;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#23454;&#20307;&#31867;&#22411;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;Llama2&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#33719;&#21462;&#22810;&#26679;&#21270;&#35821;&#35328;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#28508;&#22312;&#22320;&#25104;&#20026;&#19979;&#28216;&#20219;&#21153;&#20013;&#26377;&#20215;&#20540;&#30340;&#38468;&#24102;&#30417;&#30563;&#28304;&#12290;&#28982;&#32780;&#65292;&#22312;&#26816;&#32034;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#27861;&#24459;&#30693;&#35782;&#26041;&#38754;&#65292;&#30456;&#20851;&#30740;&#31350;&#36824;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#25506;&#32034;&#23454;&#20307;&#31867;&#22411;&#20219;&#21153;&#65292;&#20316;&#20026;&#35780;&#20272;&#27861;&#24459;&#30693;&#35782;&#20316;&#20026;&#25991;&#26412;&#29702;&#35299;&#30340;&#22522;&#26412;&#26041;&#38754;&#20197;&#21450;&#22810;&#20010;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#22522;&#30784;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#21644;&#38271;&#24230;&#30340;&#23454;&#20307;&#65288;&#21253;&#25324;&#36890;&#29992;&#23454;&#20307;&#21644;&#39046;&#22495;&#29305;&#23450;&#23454;&#20307;&#65289;&#12289;&#35821;&#20041;&#25110;&#35821;&#27861;&#20449;&#21495;&#20197;&#21450;&#19981;&#21516;&#30340;LM&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65288;&#36890;&#29992;&#21644;&#27861;&#24459;&#23548;&#21521;&#65289;&#21644;&#26550;&#26500;&#65288;&#22522;&#20110;BERT&#30340;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#30340;Llama2&#65289;&#65292;&#24182;&#20351;&#29992;&#22635;&#31354;&#21477;&#21644;&#22522;&#20110;&#38382;&#31572;&#30340;&#27169;&#26495;&#26469;&#28548;&#28165;&#36825;&#20123;&#33719;&#21462;&#30340;&#32447;&#32034;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Llama2&#24615;&#33021;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have proven their ability to acquire diverse linguistic knowledge during the pretraining phase, potentially serving as a valuable source of incidental supervision for downstream tasks. However, there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. We propose to explore the task of Entity Typing, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications. Through systematic evaluation and analysis and two types of prompting (cloze sentences and QA-based templates) and to clarify the nature of these acquired cues, we compare diverse types and lengths of entities both general and domain-specific entities, semantics or syntax signals, and different LM pretraining corpus (generic and legal-oriented) and architectures (encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2 perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#24120;&#35782;&#20449;&#24687;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#25581;&#31034;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.13080</link><description>&lt;p&gt;
&#20174;&#22810;&#35821;&#35328;&#22797;&#26434;&#24615;&#21040;&#24773;&#24863;&#28165;&#26224;&#24230;&#65306;&#21033;&#29992;&#24120;&#35782;&#25581;&#31034;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues. (arXiv:2310.13080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#24120;&#35782;&#20449;&#24687;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#25581;&#31034;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#26159;&#20154;&#31867;&#20132;&#27969;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#25512;&#21160;&#30528;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;&#65288;ERC&#65289;&#30340;NLP&#30740;&#31350;&#12290;&#23613;&#31649;&#22312;&#35782;&#21035;&#21333;&#35821;&#23545;&#35805;&#20013;&#20010;&#20307;&#28436;&#35762;&#32773;&#30340;&#24773;&#24863;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#22312;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#29702;&#35299;&#24773;&#24863;&#21160;&#24577;&#30456;&#23545;&#36739;&#23569;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#23545;&#30721;&#28151;&#21512;&#23545;&#35805;&#36827;&#34892;ERC&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#24773;&#24863;&#26234;&#33021;&#21253;&#21547;&#23545;&#19990;&#30028;&#24615;&#30693;&#35782;&#30340;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#24120;&#35782;&#20449;&#24687;&#19982;&#23545;&#35805;&#19978;&#19979;&#25991;&#30456;&#32467;&#21512;&#65292;&#20419;&#36827;&#23545;&#24773;&#24863;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#27969;&#31243;&#65292;&#20174;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#19982;&#30721;&#28151;&#21512;&#36755;&#20837;&#30456;&#20851;&#30340;&#24120;&#35782;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#34701;&#21512;&#25216;&#26415;&#65292;&#23558;&#33719;&#21462;&#30340;&#24120;&#35782;&#20449;&#24687;&#19982;&#23545;&#35805;&#34920;&#31034;&#26080;&#32541;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence encompasses a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we devise an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained fr
&lt;/p&gt;</description></item><item><title>GARI&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#22312;&#30456;&#23545;&#21516;&#26500;&#24615;&#20219;&#21153;&#20013;&#32467;&#21512;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#30446;&#26631;&#21644;&#22810;&#20010;&#21516;&#26500;&#24615;&#25439;&#22833;&#65292;&#32771;&#34385;&#20102;&#35821;&#20041;&#30456;&#20851;&#21333;&#35789;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.13068</link><description>&lt;p&gt;
GARI: &#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#38463;&#25289;&#20271;&#35789;&#23884;&#20837;&#30456;&#23545;&#21516;&#26500;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GARI: Graph Attention for Relative Isomorphism of Arabic Word Embeddings. (arXiv:2310.13068v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13068
&lt;/p&gt;
&lt;p&gt;
GARI&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#22312;&#30456;&#23545;&#21516;&#26500;&#24615;&#20219;&#21153;&#20013;&#32467;&#21512;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#30446;&#26631;&#21644;&#22810;&#20010;&#21516;&#26500;&#24615;&#25439;&#22833;&#65292;&#32771;&#34385;&#20102;&#35821;&#20041;&#30456;&#20851;&#21333;&#35789;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#35821;&#35789;&#27719;&#24402;&#32435;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#23427;&#20381;&#36182;&#20110;&#21508;&#20010;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#30456;&#23545;&#21516;&#26500;&#24615;&#12290;&#29616;&#26377;&#30340;&#23581;&#35797;&#25511;&#21046;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#30456;&#23545;&#21516;&#26500;&#24615;&#26410;&#33021;&#22312;&#27169;&#22411;&#35757;&#32451;&#30446;&#26631;&#20013;&#32771;&#34385;&#35821;&#20041;&#30456;&#20851;&#21333;&#35789;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GARI&#65292;&#23427;&#23558;&#20998;&#24067;&#24335;&#35757;&#32451;&#30446;&#26631;&#19982;&#30001;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#24341;&#23548;&#30340;&#22810;&#20010;&#21516;&#26500;&#24615;&#25439;&#22833;&#30456;&#32467;&#21512;&#12290;GARI&#32771;&#34385;&#20102;&#35789;&#27719;&#30340;&#35821;&#20041;&#21464;&#21270;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#30456;&#23545;&#21516;&#26500;&#24615;&#30340;&#23450;&#20041;&#12290;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;GARI&#30456;&#23545;&#20110;&#29616;&#26377;&#30740;&#31350;&#25552;&#39640;&#20102;&#24179;&#22343;P@1&#30340;&#30456;&#23545;&#20998;&#25968;&#65292;&#20998;&#21035;&#20026;&#39046;&#22495;&#20869;&#35774;&#32622;&#19979;&#30340;40.95%&#21644;&#39046;&#22495;&#19981;&#21305;&#37197;&#35774;&#32622;&#19979;&#30340;76.80%&#12290;&#25105;&#20204;&#22312;https://github.com/asif6827/GARI&#19978;&#21457;&#24067;&#20102;GARI&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilingual Lexical Induction (BLI) is a core challenge in NLP, it relies on the relative isomorphism of individual embedding spaces. Existing attempts aimed at controlling the relative isomorphism of different embedding spaces fail to incorporate the impact of semantically related words in the model training objective. To address this, we propose GARI that combines the distributional training objectives with multiple isomorphism losses guided by the graph attention network. GARI considers the impact of semantical variations of words in order to define the relative isomorphism of the embedding spaces. Experimental evaluation using the Arabic language data set shows that GARI outperforms the existing research by improving the average P@1 by a relative score of up to 40.95% and 76.80% for in-domain and domain mismatch settings respectively. We release the codes for GARI at https://github.com/asif6827/GARI.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#30340;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37325;&#20889;&#38463;&#25289;&#20271;&#35821;&#29992;&#25143;&#26597;&#35810;&#20197;&#25913;&#21892;&#25628;&#32034;&#24341;&#25806;&#30340;&#26816;&#32034;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13031</link><description>&lt;p&gt;
&#23558;&#26597;&#35810;&#37325;&#20889;&#37325;&#26032;&#23450;&#20041;&#20026;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#30340;&#29992;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Use Case: Reformulating Query Rewriting as a Statistical Machine Translation Problem. (arXiv:2310.13031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#30340;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37325;&#20889;&#38463;&#25289;&#20271;&#35821;&#29992;&#25143;&#26597;&#35810;&#20197;&#25913;&#21892;&#25628;&#32034;&#24341;&#25806;&#30340;&#26816;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#26816;&#32034;&#30456;&#20851;&#30340;&#32593;&#32476;&#20869;&#23481;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#25361;&#25112;&#65292;&#25628;&#32034;&#24341;&#25806;&#26377;&#19968;&#20010;&#27169;&#22359;&#26469;&#37325;&#20889;&#29992;&#25143;&#26597;&#35810;&#12290;&#22240;&#27492;&#65292;&#29616;&#20195;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#21033;&#29992;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#19968;&#20123;&#32479;&#35745;&#21644;&#31070;&#32463;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;NLP&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35821;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26597;&#35810;&#37325;&#20889;&#27969;&#31243;&#65292;&#23398;&#20064;&#22914;&#20309;&#37325;&#20889;&#38463;&#25289;&#20271;&#35821;&#29992;&#25143;&#25628;&#32034;&#26597;&#35810;&#12290;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#21019;&#24314;&#29992;&#25143;&#26597;&#35810;&#21644;&#32593;&#39029;&#26631;&#39064;&#20043;&#38388;&#26144;&#23556;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important challenges for modern search engines is to retrieve relevant web content based on user queries. In order to achieve this challenge, search engines have a module to rewrite user queries. That is why modern web search engines utilize some statistical and neural models used in the natural language processing domain. Statistical machine translation is a well-known NLP method among them. The paper proposes a query rewriting pipeline based on a monolingual machine translation model that learns to rewrite Arabic user search queries. This paper also describes preprocessing steps to create a mapping between user queries and web page titles.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#32455;&#21322;&#32467;&#26500;&#21270;&#30340;&#20250;&#35758;&#25968;&#25454;&#24182;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#33719;&#21462;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#26102;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.13028</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable Academic Conference Question Answering: A Study Based on Large Language Model. (arXiv:2310.13028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#32455;&#21322;&#32467;&#26500;&#21270;&#30340;&#20250;&#35758;&#25968;&#25454;&#24182;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#33719;&#21462;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#26102;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#23398;&#26415;&#20250;&#35758;&#19978;&#30340;&#30740;&#31350;&#22823;&#37327;&#22686;&#21152;&#65292;&#20419;&#36827;&#20102;&#20840;&#29699;&#23398;&#26415;&#20132;&#27969;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#20010;&#38454;&#27573;&#37117;&#25345;&#32493;&#23547;&#27714;&#20851;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#12290;&#36825;&#31181;&#25968;&#25454;&#29190;&#21457;&#38656;&#35201;&#19968;&#20010;&#26234;&#33021;&#30340;&#38382;&#31572;&#31995;&#32479;&#26469;&#39640;&#25928;&#35299;&#20915;&#30740;&#31350;&#20154;&#21592;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#23545;&#26368;&#26032;&#36827;&#23637;&#30340;&#20102;&#35299;&#12290;&#20250;&#35758;&#20449;&#24687;&#36890;&#24120;&#22312;&#23448;&#26041;&#32593;&#31449;&#19978;&#21457;&#24067;&#65292;&#20197;&#21322;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#32452;&#32455;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ConferenceQA&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;7&#20010;&#19981;&#21516;&#23398;&#26415;&#20250;&#35758;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#26631;&#27880;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#25163;&#21160;&#21644;&#33258;&#21160;&#26041;&#27861;&#30340;&#32452;&#21512;&#65292;&#20197;&#21322;&#32467;&#26500;&#21270;&#30340;JSON&#26684;&#24335;&#32452;&#32455;&#23398;&#26415;&#20250;&#35758;&#25968;&#25454;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#20250;&#35758;&#27880;&#37322;&#20102;&#36817;100&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#27599;&#20010;&#23545;&#24212;&#23545;&#24212;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#32500;&#24230;&#20998;&#31867;&#12290;&#20026;&#20102;&#30830;&#20445;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25163;&#21160;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of computer science has led to a proliferation of research presented at academic conferences, fostering global scholarly communication. Researchers consistently seek accurate, current information about these events at all stages. This data surge necessitates an intelligent question-answering system to efficiently address researchers' queries and ensure awareness of the latest advancements. The information of conferences is usually published on their official website, organized in a semi-structured way with a lot of text. To address this need, we have developed the ConferenceQA dataset for 7 diverse academic conferences with human annotations. Firstly, we employ a combination of manual and automated methods to organize academic conference data in a semi-structured JSON format. Subsequently, we annotate nearly 100 question-answer pairs for each conference. Each pair is classified into four different dimensions. To ensure the reliability of the data, we manually annotate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22810;&#26631;&#31614;&#20844;&#24335;&#25913;&#20026;&#24130;&#38598;&#22810;&#31867;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#37117;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;&#26041;&#27861;&#65292;&#24182;&#28040;&#38500;&#20102;&#22810;&#26631;&#31614;&#20844;&#24335;&#20013;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.13025</link><description>&lt;p&gt;
&#20351;&#29992;&#24130;&#38598;&#22810;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#31070;&#32463;&#35828;&#35805;&#20154;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Powerset multi-class cross entropy loss for neural speaker diarization. (arXiv:2310.13025v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22810;&#26631;&#31614;&#20844;&#24335;&#25913;&#20026;&#24130;&#38598;&#22810;&#31867;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#37117;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;&#26041;&#27861;&#65292;&#24182;&#28040;&#38500;&#20102;&#22810;&#26631;&#31614;&#20844;&#24335;&#20013;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;2019&#24180;&#25552;&#20986;&#20197;&#26469;&#65292;&#20840;&#31471;&#21040;&#31471;&#31070;&#32463;&#35828;&#35805;&#20154;&#20998;&#31163;&#65288;EEND&#65289;&#19968;&#30452;&#23558;&#35828;&#35805;&#20154;&#20998;&#31163;&#35270;&#20026;&#19968;&#20010;&#36880;&#24103;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20102;&#25490;&#21015;&#19981;&#21464;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#23613;&#31649;EEND&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#36864;&#22238;&#21040;&#20102;&#30740;&#31350;&#65288;&#26412;&#22320;&#65289;&#26377;&#30417;&#30563;EEND&#35828;&#35805;&#20154;&#20998;&#31163;&#19982;&#65288;&#20840;&#23616;&#65289;&#26080;&#30417;&#30563;&#32858;&#31867;&#30340;&#21487;&#33021;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28151;&#21512;&#26041;&#27861;&#24182;&#27809;&#26377;&#23545;&#21407;&#22987;&#30340;&#22810;&#26631;&#31614;&#20844;&#24335;&#20135;&#29983;&#30097;&#38382;&#12290;&#25105;&#20204;&#25552;&#20986;&#20174;&#22810;&#26631;&#31614;&#65288;&#20801;&#35768;&#21516;&#26102;&#20986;&#29616;&#20004;&#20010;&#35828;&#35805;&#20154;&#65289;&#36716;&#25442;&#20026;&#24130;&#38598;&#22810;&#31867;&#20998;&#31867;&#65288;&#20026;&#37325;&#21472;&#35828;&#35805;&#20154;&#23545;&#20998;&#37197;&#19987;&#38376;&#30340;&#31867;&#21035;&#65289;&#12290;&#36890;&#36807;&#22312;9&#20010;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#20844;&#24335;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65288;&#20027;&#35201;&#26159;&#22312;&#37325;&#21472;&#35821;&#38899;&#19978;&#65289;&#24182;&#19988;&#23545;&#39046;&#22495;&#19981;&#21305;&#37197;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#22810;&#26631;&#31614;&#20844;&#24335;&#20013;&#20851;&#38190;&#30340;&#26816;&#27979;&#38408;&#20540;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its introduction in 2019, the whole end-to-end neural diarization (EEND) line of work has been addressing speaker diarization as a frame-wise multi-label classification problem with permutation-invariant training. Despite EEND showing great promise, a few recent works took a step back and studied the possible combination of (local) supervised EEND diarization with (global) unsupervised clustering. Yet, these hybrid contributions did not question the original multi-label formulation. We propose to switch from multi-label (where any two speakers can be active at the same time) to powerset multi-class classification (where dedicated classes are assigned to pairs of overlapping speakers). Through extensive experiments on 9 different benchmarks, we show that this formulation leads to significantly better performance (mostly on overlapping speech) and robustness to domain mismatch, while eliminating the detection threshold hyperparameter, critical for the multi-label formulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#26174;&#33879;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#24182;&#20419;&#36827;&#20102;&#30693;&#35782;&#30340;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2310.13024</link><description>&lt;p&gt;
&#26397;&#21521;&#38543;&#26102;&#24494;&#35843;&#65306;&#20351;&#29992;&#36229;&#32593;&#32476;&#25552;&#31034;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompt. (arXiv:2310.13024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#26174;&#33879;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#24182;&#20419;&#36827;&#20102;&#30693;&#35782;&#30340;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#19990;&#30028;&#20013;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#21464;&#24471;&#36843;&#20999;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#26399;&#26395;&#19981;&#20165;&#22312;&#39044;&#35757;&#32451;&#39046;&#22495;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#20855;&#26377;&#26356;&#22823;&#30340;&#23481;&#37327;&#65292;&#32780;&#19988;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#19981;&#20250;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#29616;&#26377;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#38543;&#26102;&#24494;&#35843;&#25928;&#26524;&#65292;&#24471;&#20986;&#20102;&#23545;&#26410;&#30693;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#33021;&#19979;&#38477;&#30340;&#32467;&#35770;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#35757;&#32451;&#36229;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#12290;&#19968;&#33268;&#24615;&#25439;&#22833;&#26368;&#22823;&#31243;&#24230;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#26032;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#21017;&#20445;&#25252;&#20102;&#20026;&#27599;&#20010;&#39046;&#22495;&#29983;&#25104;&#30340;&#38544;&#34255;&#29366;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#36229;&#32593;&#32476;&#29983;&#25104;&#30340;&#25552;&#31034;&#22312;&#24494;&#35843;&#26102;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#24182;&#20419;&#36827;&#20102;&#30693;&#35782;&#30340;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual pre-training has been urgent for adapting a pre-trained model to a multitude of domains and tasks in the fast-evolving world. In practice, a continually pre-trained model is expected to demonstrate not only greater capacity when fine-tuned on pre-trained domains but also a non-decreasing performance on unseen ones. In this work, we first investigate such anytime fine-tuning effectiveness of existing continual pre-training approaches, concluding with unanimously decreased performance on unseen domains. To this end, we propose a prompt-guided continual pre-training method, where we train a hypernetwork to generate domain-specific prompts by both agreement and disagreement losses. The agreement loss maximally preserves the generalization of a pre-trained model to new domains, and the disagreement one guards the exclusiveness of the generated hidden states for each domain. Remarkably, prompts by the hypernetwork alleviate the domain identity when fine-tuning and promote knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13023</link><description>&lt;p&gt;
GraphGPT: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
GraphGPT: Graph Instruction Tuning for Large Language Models. (arXiv:2310.13023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36882;&#24402;&#20449;&#24687;&#20132;&#25442;&#21644;&#32858;&#21512;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35299;&#22270;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#22270;&#23884;&#20837;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#26631;&#31614;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#25552;&#21319;&#22270;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;LLM&#65292;&#21363;&#20351;&#27809;&#26377;&#26469;&#33258;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#20219;&#20309;&#20449;&#24687;&#65292;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23558;LLM&#19982;&#22270;&#32467;&#26500;&#30693;&#35782;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#65288;UPET&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;Monte Carlo dropout&#26469;&#36827;&#34892;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13022</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding. (arXiv:2310.13022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#65288;UPET&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;Monte Carlo dropout&#26469;&#36827;&#34892;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36890;&#24120;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#35757;&#32451;&#20316;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#20013;&#30340;&#19968;&#31181;&#20027;&#35201;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22826;&#22810;&#30340;&#22122;&#22768;&#26631;&#31614;&#20250;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19988;&#33258;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#22810;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#22914;&#26524;&#26356;&#26032;PLM&#30340;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#20250;&#26356;&#21152;&#26114;&#36149;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UPET&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#33945;&#29305;&#21345;&#32599;&#65288;MC&#65289;dropout&#24341;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#20197;&#36827;&#34892;&#25945;&#24072;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#28982;&#21518;&#26681;&#25454;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#26469;&#31934;&#30830;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios. To remedy this dilemma, we study self-training as one of the predominant semi-supervised learning (SSL) approaches, which utilizes large-scale unlabeled data to generate synthetic examples. However, too many noisy labels will hurt the model performance, and the self-training procedure requires multiple training iterations making it more expensive if all the model parameters of the PLM are updated. This paper presents UPET, a novel Uncertainty-aware Parameter-Efficient self-Training framework to effectively and efficiently address the labeled data scarcity issue. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the teacher model and then judiciously select reliable pseudo-labeled examples based on confidence and certainty. During the stude
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26469;&#25913;&#36827;ALiBi&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19978;&#28216;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#25688;&#35201;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.13017</link><description>&lt;p&gt;
&#20301;&#32622;&#25554;&#20540;&#25913;&#36827;&#20102;ALiBi&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Position Interpolation Improves ALiBi Extrapolation. (arXiv:2310.13017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26469;&#25913;&#36827;ALiBi&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19978;&#28216;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#25688;&#35201;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26377;&#21161;&#20110;&#20351;&#29992;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#26356;&#38271;&#30340;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#22806;&#25512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26469;&#25193;&#23637;&#20351;&#29992;&#24102;&#26377;&#32447;&#24615;&#20559;&#24046;&#30340;&#27880;&#24847;&#21147;&#65288;ALiBi&#65289;&#30340;&#27169;&#22411;&#30340;&#22806;&#25512;&#33539;&#22260;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20301;&#32622;&#25554;&#20540;&#26174;&#33879;&#25913;&#21892;&#20102;&#19978;&#28216;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#25688;&#35201;&#21644;&#26816;&#32034;&#20219;&#21153;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear position interpolation helps pre-trained models using rotary position embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using linear position interpolation to extend the extrapolation range of models using Attention with Linear Biases (ALiBi). We find position interpolation significantly improves extrapolation capability on upstream language modelling and downstream summarization and retrieval tasks.
&lt;/p&gt;</description></item><item><title>Audio-AdapterFusion&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20219;&#21153;ID&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21333;&#20219;&#21153;&#36866;&#37197;&#22120;&#23454;&#29616;&#38750;&#30772;&#22351;&#24615;&#21644;&#21442;&#25968;&#39640;&#25928;&#65292;&#19988;&#22312;&#22810;&#20010;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.13015</link><description>&lt;p&gt;
Audio-AdapterFusion&#65306;&#19968;&#31181;&#26080;&#20219;&#21153;ID&#30340;&#39640;&#25928;&#21644;&#38750;&#30772;&#22351;&#24615;&#22810;&#20219;&#21153;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive Multi-task Speech Recognition. (arXiv:2310.13015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13015
&lt;/p&gt;
&lt;p&gt;
Audio-AdapterFusion&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20219;&#21153;ID&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21333;&#20219;&#21153;&#36866;&#37197;&#22120;&#23454;&#29616;&#38750;&#30772;&#22351;&#24615;&#21644;&#21442;&#25968;&#39640;&#25928;&#65292;&#19988;&#22312;&#22810;&#20010;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Adapter&#26159;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39640;&#25928;&#21487;&#32452;&#21512;&#26367;&#20195;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#23558;&#22823;&#22411;ASR&#27169;&#22411;&#25193;&#23637;&#21040;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#65292;&#24120;&#24120;&#38656;&#35201;&#22312;&#36755;&#20837;&#19978;&#28155;&#21152;&#20219;&#21153;ID&#20197;&#23558;&#20854;&#36335;&#30001;&#21040;&#29305;&#23450;&#20219;&#21153;&#30340;&#21333;&#20219;&#21153;&#36866;&#37197;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#22312;&#25512;&#26029;&#26399;&#38388;&#20219;&#21153;ID&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#22810;&#20219;&#21153;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#26080;&#20219;&#21153;ID&#26041;&#27861;&#26469;&#32467;&#21512;&#22810;&#20219;&#21153;ASR&#20013;&#30340;&#21333;&#20219;&#21153;&#36866;&#37197;&#22120;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#29992;&#20110;&#35757;&#32451;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;4&#20010;&#19981;&#21516;ASR&#20219;&#21153;&#30340;10&#20010;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#30772;&#22351;&#24615;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#12290;&#22312;&#20165;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#30340;17%&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#23436;&#20840;&#24494;&#35843;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;8%&#30340;&#24179;&#22343;WER&#25913;&#36827;&#65292;&#24182;&#19982;&#20219;&#21153;ID&#36866;&#37197;&#22120;&#36335;&#30001;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapters are an efficient, composable alternative to full fine-tuning of pre-trained models and help scale the deployment of large ASR models to many tasks. In practice, a task ID is commonly prepended to the input during inference to route to single-task adapters for the specified task. However, one major limitation of this approach is that the task ID may not be known during inference, rendering it unsuitable for most multi-task settings. To address this, we propose three novel task-ID-free methods to combine single-task adapters in multi-task ASR and investigate two learning algorithms for training. We evaluate our methods on 10 test sets from 4 diverse ASR tasks and show that our methods are non-destructive and parameter-efficient. While only updating 17% of the model parameters, our methods can achieve an 8% mean WER improvement relative to full fine-tuning and are on-par with task-ID adapter routing.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21442;&#19982;Metaculus&#24179;&#21488;&#20030;&#21150;&#30340;&#39044;&#27979;&#31454;&#36187;&#65292;&#23454;&#35777;&#27979;&#35797;&#20102;OpenAI&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#30340;&#27010;&#29575;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#39044;&#27979;&#30456;&#27604;&#26126;&#26174;&#19981;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2310.13014</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65306;&#26469;&#33258;&#29616;&#23454;&#39044;&#27979;&#31454;&#36187;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament. (arXiv:2310.13014v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21442;&#19982;Metaculus&#24179;&#21488;&#20030;&#21150;&#30340;&#39044;&#27979;&#31454;&#36187;&#65292;&#23454;&#35777;&#27979;&#35797;&#20102;OpenAI&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#30340;&#27010;&#29575;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#39044;&#27979;&#30456;&#27604;&#26126;&#26174;&#19981;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#23558;&#26159;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20851;&#20110;&#26410;&#26469;&#20107;&#20214;&#27010;&#29575;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#20026;&#20102;&#32463;&#39564;&#24615;&#22320;&#27979;&#35797;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;OpenAI&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#32435;&#20837;&#20102;Metaculus&#24179;&#21488;&#20030;&#21150;&#30340;&#20026;&#26399;&#19977;&#20010;&#26376;&#30340;&#39044;&#27979;&#31454;&#36187;&#12290;&#36825;&#22330;&#20174;2023&#24180;7&#26376;&#21040;10&#26376;&#36827;&#34892;&#30340;&#31454;&#36187;&#21560;&#24341;&#20102;843&#21517;&#21442;&#19982;&#32773;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#12289;&#32654;&#22269;&#25919;&#27835;&#12289;&#30149;&#27602;&#29190;&#21457;&#21644;&#20044;&#20811;&#20848;&#20914;&#31361;&#22312;&#20869;&#30340;&#21508;&#31181;&#20027;&#39064;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#20108;&#36827;&#21046;&#39044;&#27979;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20154;&#32676;&#20013;&#20301;&#25968;&#39044;&#27979;&#30456;&#27604;&#65292;GPT-4&#30340;&#27010;&#29575;&#39044;&#27979;&#26126;&#26174;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4&#30340;&#39044;&#27979;&#19982;&#23558;&#27599;&#20010;&#38382;&#39064;&#30340;&#27010;&#29575;&#20998;&#37197;&#20026;50%&#30340;&#26080;&#20449;&#24687;&#39044;&#27979;&#31574;&#30053;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#37322;&#65292;&#21363;GPT-4&#21487;&#33021;&#26377;&#20542;&#21521;&#24615;&#22320;&#39044;&#27979;&#27010;&#29575;&#20026;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the future would be an important milestone in the capabilities of artificial intelligence. However, research on the ability of large language models to provide probabilistic predictions about future events remains nascent. To empirically test this ability, we enrolled OpenAI's state-of-the-art large language model, GPT-4, in a three-month forecasting tournament hosted on the Metaculus platform. The tournament, running from July to October 2023, attracted 843 participants and covered diverse topics including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict. Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts. We find that GPT-4's forecasts did not significantly differ from the no-information forecasting strategy of assigning a 50% probability to every question. We explore a potential explanation, that GPT-4 might be predisposed to predict probabilities clo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#20551;&#35774;&#21015;&#34920;&#26469;&#36827;&#34892;&#20195;&#30721;&#20132;&#26367;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13013</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#20132;&#26367;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Generative error correction for code-switching speech recognition using large language models. (arXiv:2310.13013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#20551;&#35774;&#21015;&#34920;&#26469;&#36827;&#34892;&#20195;&#30721;&#20132;&#26367;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#20132;&#26367;&#35821;&#38899;&#35782;&#21035;&#26159;&#25351;&#22312;&#21516;&#19968;&#21477;&#23376;&#20013;&#28151;&#21512;&#20351;&#29992;&#20004;&#31181;&#25110;&#26356;&#22810;&#35821;&#35328;&#30340;&#29616;&#35937;&#12290;&#23613;&#31649;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20195;&#30721;&#20132;&#26367;&#35821;&#38899;&#35782;&#21035;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21407;&#22240;&#26159;&#35813;&#29616;&#35937;&#30340;&#35821;&#27861;&#32467;&#26500;&#22797;&#26434;&#20197;&#21450;&#29305;&#23450;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;ASR&#29983;&#25104;&#30340;&#20551;&#35774;&#21015;&#34920;&#26469;&#35299;&#20915;&#20195;&#30721;&#20132;&#26367;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22810;&#20010;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#36827;&#34892;N-best&#20551;&#35774;&#29983;&#25104;&#65292;&#26088;&#22312;&#22686;&#21152;&#20551;&#35774;&#38598;&#20013;&#30340;&#22810;&#26679;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#23398;&#20064;&#20551;&#35774;&#21040;&#36716;&#24405;&#30340;&#26144;&#23556;&#65292;&#36890;&#36807;&#28155;&#21152;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#12290;&#36825;&#31181;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;&#65288;GER&#65289;&#26041;&#27861;&#26681;&#25454;&#20854;&#19987;&#19994;&#30340;&#35821;&#35328;&#30693;&#35782;&#21644;N-best&#20551;&#35774;&#30452;&#25509;&#39044;&#27979;&#20934;&#30830;&#30340;&#36716;&#24405;&#65292;&#20174;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#24335;&#36716;&#21464;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-switching (CS) speech refers to the phenomenon of mixing two or more languages within the same sentence. Despite the recent advances in automatic speech recognition (ASR), CS-ASR is still a challenging task ought to the grammatical structure complexity of the phenomenon and the data scarcity of specific training corpus. In this work, we propose to leverage large language models (LLMs) and lists of hypotheses generated by an ASR to address the CS problem. Specifically, we first employ multiple well-trained ASR models for N-best hypotheses generation, with the aim of increasing the diverse and informative elements in the set of hypotheses. Next, we utilize the LLMs to learn the hypotheses-to-transcription (H2T) mapping by adding a trainable low-rank adapter. Such a generative error correction (GER) method directly predicts the accurate transcription according to its expert linguistic knowledge and N-best hypotheses, resulting in a paradigm shift from the traditional language model r
&lt;/p&gt;</description></item><item><title>H2O&#25512;&#20986;&#20102;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;h2oGPT&#21644;H2O LLM Studio&#12290;&#36825;&#19968;&#24320;&#28304;&#39033;&#30446;&#25552;&#20379;&#20102;&#20840;&#38754;&#24320;&#25918;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#24110;&#21161;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#20449;&#36182;&#21644;&#21487;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2310.13012</link><description>&lt;p&gt;
H2O&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
H2O Open Ecosystem for State-of-the-art Large Language Models. (arXiv:2310.13012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13012
&lt;/p&gt;
&lt;p&gt;
H2O&#25512;&#20986;&#20102;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;h2oGPT&#21644;H2O LLM Studio&#12290;&#36825;&#19968;&#24320;&#28304;&#39033;&#30446;&#25552;&#20379;&#20102;&#20840;&#38754;&#24320;&#25918;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#24110;&#21161;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#20449;&#36182;&#21644;&#21487;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#39033;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#37325;&#22823;&#39118;&#38505;&#65292;&#20363;&#22914;&#23384;&#22312;&#20559;&#35265;&#12289;&#31169;&#26377;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#26377;&#23475;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#24320;&#25918;&#12289;&#36879;&#26126;&#21644;&#23433;&#20840;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#24320;&#28304;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#27979;&#35797;LLMs&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#25512;&#21160;&#23545;&#23553;&#38381;&#28304;&#26041;&#27861;&#30340;&#24320;&#25918;&#24335;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;h2oGPT&#65292;&#21363;&#20174;70&#20159;&#21040;700&#20159;&#21442;&#25968;&#30340;&#19968;&#31995;&#21015;&#31934;&#32454;&#35843;&#25972;&#30340;LLMs&#12290;&#25105;&#20204;&#36824;&#25512;&#20986;&#20102;H2O LLM Studio&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#21644;&#26080;&#20195;&#30721;GUI&#65292;&#19987;&#20026;&#20351;&#29992;&#26368;&#26032;&#30340;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;LLMs&#30340;&#39640;&#25928;&#31934;&#32454;&#35843;&#25972;&#12289;&#35780;&#20272;&#21644;&#37096;&#32626;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#22312;&#23436;&#20840;&#33258;&#30001;&#30340;Apache 2.0&#35768;&#21487;&#35777;&#19979;&#25480;&#26435;&#20351;&#29992;&#12290;&#25105;&#20204;&#30456;&#20449;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#24182;&#20351;&#20854;&#26356;&#21487;&#35775;&#38382;&#21644;&#21487;&#20449;&#36182;&#12290;&#28436;&#31034;&#32593;&#22336;&#20026;&#65306;https://gpt.h2o.ai/
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. The demo is available at: https://gpt.h2o.ai/
&lt;/p&gt;</description></item><item><title>&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#35299;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#24182;&#26681;&#25454;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#26631;&#37327;&#35780;&#20998;&#65292;&#24471;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13011</link><description>&lt;p&gt;
&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compositional preference models for aligning LMs. (arXiv:2310.13011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13011
&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#35299;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#24182;&#26681;&#25454;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#26631;&#37327;&#35780;&#20998;&#65292;&#24471;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#23558;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#21450;&#23545;&#20559;&#22909;&#25968;&#25454;&#38598;&#36807;&#25311;&#21512;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#20998;&#35299;&#20026;&#22810;&#20010;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20174;&#19968;&#20010;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#36825;&#20123;&#29305;&#24449;&#30340;&#26631;&#37327;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#32858;&#21512;&#36825;&#20123;&#35780;&#20998;&#12290;CPMs&#20801;&#35768;&#25511;&#21046;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#20351;&#29992;&#21738;&#20123;&#23646;&#24615;&#26469;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#22522;&#30784;&#30340;&#29305;&#24449;&#26500;&#24314;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CPMs&#19981;&#20165;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#27604;&#26631;&#20934;&#20559;&#22909;&#27169;&#22411;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;CPMs&#33719;&#24471;&#30340;&#26368;&#20339;n&#20010;&#26679;&#26412;&#27604;&#20351;&#29992;&#26631;&#20934;PMs&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;LoBaSS&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#30340;&#33021;&#21147;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#27169;&#22411;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13008</link><description>&lt;p&gt;
LoBaSS&#65306;&#22312;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#20013;&#27979;&#37327;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoBaSS: Gauging Learnability in Supervised Fine-tuning Data. (arXiv:2310.13008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;LoBaSS&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#30340;&#33021;&#21147;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#27169;&#22411;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;&#20808;&#20915;&#26465;&#20214;&#23545;&#40784;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#24494;&#35843;&#25968;&#25454;&#30340;&#36873;&#25321;&#28145;&#21051;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20256;&#32479;&#19978;&#20197;&#25968;&#25454;&#36136;&#37327;&#21644;&#20998;&#24067;&#20026;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SFT&#25968;&#25454;&#36873;&#25321;&#30340;&#19968;&#20010;&#26032;&#32500;&#24230;&#65306;&#21487;&#23398;&#20064;&#24615;&#12290;&#36825;&#20010;&#26032;&#32500;&#24230;&#30340;&#21160;&#26426;&#26159;&#30001;LLM&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#33719;&#24471;&#30340;&#33021;&#21147;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#65292;&#36866;&#21512;&#19968;&#20010;&#27169;&#22411;&#30340;SFT&#25968;&#25454;&#21487;&#33021;&#19981;&#36866;&#21512;&#21478;&#19968;&#20010;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#33021;&#21147;&#36825;&#20010;&#26415;&#35821;&#26469;&#23450;&#20041;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#23398;&#20064;&#30340;&#36866;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25439;&#22833;&#30340;SFT&#25968;&#25454;&#36873;&#25321;&#65288;LoBaSS&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;SFT&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#22266;&#26377;&#30340;&#27169;&#22411;&#33021;&#21147;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large Language Models (LLMs) to specific task prerequisites. The selection of fine-tuning data profoundly influences the model's performance, whose principle is traditionally grounded in data quality and distribution. In this paper, we introduce a new dimension in SFT data selection: learnability. This new dimension is motivated by the intuition that SFT unlocks capabilities acquired by a LLM during the pretraining phase. Given that different pretrained models have disparate capabilities, the SFT data appropriate for one may not suit another. Thus, we introduce the term learnability to define the suitability of data for effective learning by the model. We present the Loss Based SFT Data Selection (LoBaSS) method, utilizing data learnability as the principal criterion for the selection SFT data. This method provides a nuanced approach, allowing the alignment of data selection with inherent model capabilities, ensuring op
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22320;&#29702;&#25968;&#25454;&#30340;&#29702;&#35299;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32508;&#21512;&#22320;&#29702;&#30693;&#35782;&#38656;&#35201;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.13002</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#22320;&#29702;&#30693;&#35782;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Geospatially Knowledgeable?. (arXiv:2310.13002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22320;&#29702;&#25968;&#25454;&#30340;&#29702;&#35299;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32508;&#21512;&#22320;&#29702;&#30693;&#35782;&#38656;&#35201;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#23545;&#22320;&#29702;&#25968;&#25454;&#30340;&#29702;&#35299;&#31243;&#24230;&#20197;&#21450;&#22312;&#25512;&#21160;&#26126;&#26234;&#30340;&#22320;&#29702;&#20915;&#31574;&#26041;&#38754;&#30340;&#33021;&#21147;&#20102;&#35299;&#29978;&#23569;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#39044;&#35757;&#32451;LLM&#20013;&#32534;&#30721;&#30340;&#22320;&#29702;&#30693;&#35782;&#12289;&#24847;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20851;&#27880;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;i&#65289;&#25506;&#27979;LLM&#30340;&#22320;&#29702;&#22352;&#26631;&#65292;&#20197;&#35780;&#20272;&#22320;&#29702;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#20351;&#29992;&#22320;&#29702;&#21644;&#38750;&#22320;&#29702;&#20171;&#35789;&#26469;&#35780;&#20272;&#20854;&#22320;&#29702;&#24847;&#35782;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#21033;&#29992;&#22810;&#32500;&#23610;&#24230;&#65288;MDS&#65289;&#23454;&#39564;&#35780;&#20272;&#27169;&#22411;&#30340;&#22320;&#29702;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#26681;&#25454;&#25552;&#31034;&#30830;&#23450;&#22478;&#24066;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#65292;&#20174;&#25991;&#26412;&#20449;&#24687;&#20013;&#32508;&#21512;&#22320;&#29702;&#30693;&#35782;&#19981;&#20165;&#38656;&#35201;&#26356;&#22823;&#30340;&#65292;&#32780;&#19988;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#30740;&#31350;&#20026;&#25105;&#20204;&#23545;LLM&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance of Large Language Models (LLM) for various natural language processing tasks, little is known about their comprehension of geographic data and related ability to facilitate informed geospatial decision-making. This paper investigates the extent of geospatial knowledge, awareness, and reasoning abilities encoded within such pretrained LLMs. With a focus on autoregressive language models, we devise experimental approaches related to (i) probing LLMs for geo-coordinates to assess geospatial knowledge, (ii) using geospatial and non-geospatial prepositions to gauge their geospatial awareness, and (iii) utilizing a multidimensional scaling (MDS) experiment to assess the models' geospatial reasoning capabilities and to determine locations of cities based on prompting. Our results confirm that it does not only take larger, but also more sophisticated LLMs to synthesize geospatial knowledge from textual information. As such, this research contributes to unders
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#24573;&#35270;&#20851;&#31995;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#20851;&#31995;&#22270;&#12289;&#21152;&#26435;&#22788;&#29702;&#21644;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25991;&#26723;&#20013;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22359;&#38598;&#25104;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13000</link><description>&lt;p&gt;
&#20851;&#31995;&#30456;&#20851;&#24615;&#22686;&#24378;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Document-Level Relation Extraction with Relation Correlation Enhancement. (arXiv:2310.13000v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#24573;&#35270;&#20851;&#31995;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#20851;&#31995;&#22270;&#12289;&#21152;&#26435;&#22788;&#29702;&#21644;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25991;&#26723;&#20013;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22359;&#38598;&#25104;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#26159;&#19968;&#39033;&#33268;&#21147;&#20110;&#35782;&#21035;&#25991;&#26723;&#20869;&#23454;&#20307;&#38388;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#24448;&#24448;&#24573;&#35270;&#20102;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#32570;&#20047;&#23545;&#20851;&#31995;&#30456;&#20851;&#24615;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24182;&#26377;&#25928;&#25429;&#25417;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#20851;&#31995;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#22270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26126;&#30830;&#21033;&#29992;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#20851;&#31995;&#22270;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#20851;&#31995;&#30693;&#35782;&#23548;&#20986;&#30340;&#32479;&#35745;&#20849;&#29616;&#20449;&#24687;&#26469;&#24314;&#27169;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#21019;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#20851;&#31995;&#30456;&#20851;&#24615;&#30697;&#38453;&#65292;&#20197;&#24341;&#23548;&#20851;&#31995;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#32858;&#21512;&#20851;&#31995;&#23884;&#20837;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#20316;&#20026;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#20851;&#31995;&#25277;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) is a task that focuses on identifying relations between entities within a document. However, existing DocRE models often overlook the correlation between relations and lack a quantitative analysis of relation correlations. To address this limitation and effectively capture relation correlations in DocRE, we propose a relation graph method, which aims to explicitly exploit the interdependency among relations. Firstly, we construct a relation graph that models relation correlations using statistical co-occurrence information derived from prior relation knowledge. Secondly, we employ a re-weighting scheme to create an effective relation correlation matrix to guide the propagation of relation information. Furthermore, we leverage graph attention networks to aggregate relation embeddings. Importantly, our method can be seamlessly integrated as a plug-and-play module into existing models. Experimental results demonstrate that our approach can enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22686;&#24378;&#21307;&#30103;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLM&#19981;&#20165;&#31616;&#21270;&#20102;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26657;&#20934;&#36807;&#31243;&#65292;&#32780;&#19988;&#22312;&#19982;&#20154;&#24037;&#26631;&#27880;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#31934;&#30830;&#21305;&#37197;&#29575;&#36229;&#36807;90%&#12290;</title><link>http://arxiv.org/abs/2310.12989</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20581;&#24247;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#65306;&#19968;&#39033;FHIR&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Health Data Interoperability with Large Language Models: A FHIR Study. (arXiv:2310.12989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22686;&#24378;&#21307;&#30103;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLM&#19981;&#20165;&#31616;&#21270;&#20102;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26657;&#20934;&#36807;&#31243;&#65292;&#32780;&#19988;&#22312;&#19982;&#20154;&#24037;&#26631;&#27880;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#31934;&#30830;&#21305;&#37197;&#29575;&#36229;&#36807;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22686;&#24378;&#21307;&#30103;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#23558;&#20020;&#24202;&#25991;&#26412;&#36716;&#25442;&#20026;&#23545;&#24212;&#30340;FHIR&#36164;&#28304;&#12290;&#25105;&#20204;&#22312;3671&#20010;&#20020;&#24202;&#25991;&#26412;&#29255;&#27573;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;LLM&#19981;&#20165;&#31616;&#21270;&#20102;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26657;&#20934;&#36807;&#31243;&#65292;&#32780;&#19988;&#22312;&#19982;&#20154;&#24037;&#26631;&#27880;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#20934;&#30830;&#21305;&#37197;&#29575;&#36229;&#36807;90%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the ability of the large language model (LLM) to enhance healthcare data interoperability. We leveraged the LLM to convert clinical texts into their corresponding FHIR resources. Our experiments, conducted on 3,671 snippets of clinical text, demonstrated that the LLM not only streamlines the multi-step natural language processing and human calibration processes but also achieves an exceptional accuracy rate of over 90% in exact matches when compared to human annotations.
&lt;/p&gt;</description></item><item><title>GestureGPT&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#35299;&#35835;&#25163;&#21183;&#25551;&#36848;&#24182;&#26681;&#25454;&#20132;&#20114;&#29615;&#22659;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#24847;&#22270;&#23545;&#25509;&#21040;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;</title><link>http://arxiv.org/abs/2310.12821</link><description>&lt;p&gt;
GestureGPT: &#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#19982;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents. (arXiv:2310.12821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12821
&lt;/p&gt;
&lt;p&gt;
GestureGPT&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#35299;&#35835;&#25163;&#21183;&#25551;&#36848;&#24182;&#26681;&#25454;&#20132;&#20114;&#29615;&#22659;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#24847;&#22270;&#23545;&#25509;&#21040;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#39044;&#23450;&#20041;&#38598;&#21512;&#20013;&#30340;&#25163;&#21183;&#65292;&#26410;&#33021;&#23558;&#36825;&#20123;&#25163;&#21183;&#19982;&#20132;&#20114;&#24335;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#20803;&#32032;&#25110;&#31995;&#32479;&#21151;&#33021;&#30456;&#36830;&#25509;&#65288;&#20363;&#22914;&#65292;&#23558;&#8220;&#31446;&#36215;&#22823;&#25287;&#25351;&#8221;&#25163;&#21183;&#19982;&#8220;&#21916;&#27426;&#8221;&#25353;&#38062;&#20851;&#32852;&#36215;&#26469;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GestureGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25163;&#21183;&#25551;&#36848;&#26681;&#25454;&#25163;&#21183;&#35270;&#39057;&#20013;&#30340;&#25163;&#37096;&#20851;&#38190;&#28857;&#22352;&#26631;&#36827;&#34892;&#24418;&#24335;&#21270;&#65292;&#24182;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;&#21452;&#20195;&#29702;&#23545;&#35805;&#31995;&#32479;&#20013;&#12290;&#19968;&#20010;&#25163;&#21183;&#20195;&#29702;&#35299;&#35835;&#36825;&#20123;&#25551;&#36848;&#65292;&#24182;&#35810;&#38382;&#26377;&#20851;&#20132;&#20114;&#29615;&#22659;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#30028;&#38754;&#12289;&#21382;&#21490;&#35760;&#24405;&#12289;&#20957;&#35270;&#25968;&#25454;&#65289;&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#20195;&#29702;&#36127;&#36131;&#32452;&#32455;&#24182;&#25552;&#20379;&#36825;&#20123;&#20449;&#24687;&#12290;&#32463;&#36807;&#36845;&#20195;&#30340;&#20132;&#27969;&#65292;&#25163;&#21183;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#23545;&#25509;&#21040;&#19968;&#20010;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#30340;&#31532;&#19968;&#35270;&#35282;&#21644;&#31532;&#19977;&#35270;&#35282;&#25163;&#21183;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25163;&#21183;&#25551;&#36848;&#27169;&#22359;&#65292;&#24182;&#22312;&#35270;&#39057;&#27969;&#21644;&#26234;&#33021;&#23478;&#23621;&#29289;&#32852;&#32593;&#25511;&#21046;&#30340;&#20004;&#20010;&#30495;&#23454;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#25972;&#20010;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current gesture recognition systems primarily focus on identifying gestures within a predefined set, leaving a gap in connecting these gestures to interactive GUI elements or system functions (e.g., linking a 'thumb-up' gesture to a 'like' button). We introduce GestureGPT, a novel zero-shot gesture understanding and grounding framework leveraging large language models (LLMs). Gesture descriptions are formulated based on hand landmark coordinates from gesture videos and fed into our dual-agent dialogue system. A gesture agent deciphers these descriptions and queries about the interaction context (e.g., interface, history, gaze data), which a context agent organizes and provides. Following iterative exchanges, the gesture agent discerns user intent, grounding it to an interactive function. We validated the gesture description module using public first-view and third-view gesture datasets and tested the whole system in two real-world settings: video streaming and smart home IoT control. T
&lt;/p&gt;</description></item><item><title>ICU&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35270;&#35273;&#19982;&#35821;&#35328;&#24314;&#27169;&#20013;&#35821;&#35328;&#38556;&#30861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#29702;&#35299;&#20004;&#20010;&#38454;&#27573;&#65292;&#23558;&#22810;&#35821;&#35328;&#22788;&#29702;&#36127;&#25285;&#36716;&#31227;&#21040;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ICU&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12531</link><description>&lt;p&gt;
ICU&#65306;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#29702;&#35299;&#26469;&#20811;&#26381;&#35270;&#35273;&#19982;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding. (arXiv:2310.12531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12531
&lt;/p&gt;
&lt;p&gt;
ICU&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35270;&#35273;&#19982;&#35821;&#35328;&#24314;&#27169;&#20013;&#35821;&#35328;&#38556;&#30861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#29702;&#35299;&#20004;&#20010;&#38454;&#27573;&#65292;&#23558;&#22810;&#35821;&#35328;&#22788;&#29702;&#36127;&#25285;&#36716;&#31227;&#21040;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ICU&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35270;&#35273;&#19982;&#35821;&#35328;(V&amp;L)&#30740;&#31350;&#26088;&#22312;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#30340;&#22810;&#35821;&#35328;&#23383;&#24149;&#31232;&#32570;&#19968;&#30452;&#20197;&#26469;&#19968;&#30452;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ICU&#65288;Image Caption Understanding&#65289;&#65292;&#23558;V&amp;L&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#19968;&#20010;V&amp;L&#27169;&#22411;&#20197;&#33521;&#25991;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#28982;&#21518;&#19968;&#20010;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;mLM&#65289;&#20197;&#23383;&#24149;&#20316;&#20026;&#26367;&#20195;&#25991;&#26412;&#36827;&#34892;&#36328;&#35821;&#35328;&#35821;&#35328;&#29702;&#35299;&#12290;&#36825;&#31181;&#26041;&#24335;&#20943;&#36731;&#20102;V&amp;L&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#22788;&#29702;&#36127;&#25285;&#65292;&#23558;&#20854;&#36716;&#31227;&#21040;&#20102;mLM&#19978;&#12290;&#30001;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#25968;&#25454;&#30456;&#23545;&#20016;&#23500;&#21644;&#36136;&#37327;&#36739;&#39640;&#65292;ICU&#21487;&#20197;&#24110;&#21161;&#20811;&#26381;V&amp;L&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;&#22312;IGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#28041;&#21450;9&#31181;&#35821;&#35328;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ICU&#21487;&#20197;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#23454;&#29616;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#22312;&#20854;&#20313;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most multilingual vision-and-language (V&amp;L) research aims to accomplish multilingual and multimodal capabilities within one model. However, the scarcity of multilingual captions for images has hindered the development. To overcome this obstacle, we propose ICU, Image Caption Understanding, which divides a V&amp;L task into two stages: a V&amp;L model performs image captioning in English, and a multilingual language model (mLM), in turn, takes the caption as the alt text and performs crosslingual language understanding. The burden of multilingual processing is lifted off V&amp;L model and placed on mLM. Since the multilingual text data is relatively of higher abundance and quality, ICU can facilitate the conquering of language barriers for V&amp;L models. In experiments on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can achieve new state-of-the-art results for five languages, and comparable results for the rest.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#29983;&#21644;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#30340;&#20934;&#30830;&#20998;&#31867;&#19978;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#65292;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#25110;&#20854;&#20182;&#25216;&#26415;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#21307;&#29983;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.12489</link><description>&lt;p&gt;
MedAI&#23545;&#35805;&#35821;&#26009;&#24211;&#65288;MEDIC&#65289;&#65306;&#38646;&#26679;&#26412;&#20998;&#31867;&#21307;&#29983;&#19982;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations. (arXiv:2310.12489v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#29983;&#21644;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#30340;&#20934;&#30830;&#20998;&#31867;&#19978;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#65292;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#25110;&#20854;&#20182;&#25216;&#26415;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#21307;&#29983;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#20998;&#31867;&#20351;&#24471;&#21487;&#20197;&#23558;&#25991;&#26412;&#20998;&#31867;&#21040;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#31867;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#20998;&#31867;&#26469;&#33258;&#21307;&#29983;&#21644;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#26816;&#27979;&#25991;&#26412;&#26159;&#26469;&#33258;&#20154;&#31867;&#36824;&#26159;AI&#27169;&#22411;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#21307;&#29983;&#23545;&#20110;&#24739;&#32773;&#20581;&#24247;&#21672;&#35810;&#30340;&#22238;&#31572;&#65292;&#24182;&#23545;&#21516;&#26679;&#30340;&#38382;&#39064;/&#22238;&#31572;&#25552;&#38382;&#20102;AI&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#65292;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#25110;&#20854;&#20182;&#25216;&#26415;&#20197;&#23454;&#29616;&#23545;&#21307;&#29983;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20934;&#30830;&#20998;&#31867;&#12290;&#20316;&#20026;&#22522;&#32447;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20165;&#20381;&#38752;&#38646;&#26679;&#26412;&#20998;&#31867;&#22312;&#21307;&#30103;&#20998;&#31867;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot classification has enabled the classification of text into classes that were not seen during training. In this paper, we investigate the effectiveness of pre-trained language models to accurately classify responses from Doctors and AI in health consultations through zero-shot learning. Our study aims to determine whether these models can effectively detect if a text originates from human or AI models without specific corpus training. For our experiments, we collected responses from doctors to patient inquiries about their health and posed the same question/response to AI models. Our findings revealed that while pre-trained language models demonstrate a strong understanding of language generally, they may require specific corpus training or other techniques to achieve accurate classification of doctor- and AI-generated text in healthcare consultations. As a baseline approach, this study shows the limitations of relying solely on zero-shot classification in medical classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#22686;&#24378;&#19968;&#33268;&#24615;&#24314;&#27169;&#30340;&#26041;&#24335;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#24341;&#20837;&#20027;&#39064;&#24863;&#30693;&#30340;&#21477;&#23376;&#32467;&#26500;&#39044;&#27979;&#21644;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#24615;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#22312;&#25429;&#25417;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#21010;&#20998;&#20043;&#38388;&#30340;&#28145;&#23618;&#20851;&#31995;&#26041;&#38754;&#26377;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.11772</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#19968;&#33268;&#24615;&#24314;&#27169;&#26469;&#25913;&#36827;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling. (arXiv:2310.11772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#22686;&#24378;&#19968;&#33268;&#24615;&#24314;&#27169;&#30340;&#26041;&#24335;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#24341;&#20837;&#20027;&#39064;&#24863;&#30693;&#30340;&#21477;&#23376;&#32467;&#26500;&#39044;&#27979;&#21644;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#24615;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#22312;&#25429;&#25417;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#21010;&#20998;&#20043;&#38388;&#30340;&#28145;&#23618;&#20851;&#31995;&#26041;&#38754;&#26377;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#21010;&#20998;&#23545;&#20110;&#33719;&#21462;&#32467;&#26500;&#21270;&#30340;&#38271;&#25991;&#26723;&#21644;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#31561;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#33021;&#22815;&#33258;&#21160;&#20174;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#20013;&#25506;&#32034;&#20027;&#39064;&#36716;&#21464;&#30340;&#32447;&#32034;&#65292;&#26368;&#36817;&#30340;&#26377;&#30417;&#30563;&#31070;&#32463;&#27169;&#22411;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#30340;&#21457;&#23637;&#65292;&#20294;&#23545;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#21010;&#20998;&#20043;&#38388;&#26356;&#28145;&#23618;&#27425;&#30340;&#20851;&#31995;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22686;&#24378;&#20102;&#26377;&#30417;&#30563;&#27169;&#22411;&#20174;&#32467;&#26500;&#21644;&#30456;&#20284;&#24615;&#20004;&#20010;&#26041;&#38754;&#25429;&#25417;&#19968;&#33268;&#24615;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20027;&#39064;&#21010;&#20998;&#24615;&#33021;&#65292;&#21253;&#25324;&#20027;&#39064;&#24863;&#30693;&#30340;&#21477;&#23376;&#32467;&#26500;&#39044;&#27979;&#65288;TSSP&#65289;&#21644;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#24615;&#23398;&#20064;&#65288;CSSL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;TSSP&#20219;&#21153;&#65292;&#36890;&#36807;&#23398;&#20064;&#26080;&#24207;&#25991;&#26723;&#20013;&#30456;&#37051;&#21477;&#23376;&#30340;&#21407;&#22987;&#20851;&#31995;&#65292;&#24378;&#21046;&#27169;&#22411;&#29702;&#35299;&#32467;&#26500;&#20449;&#24687;&#65292;&#35813;&#26080;&#24207;&#25991;&#26723;&#30001;&#21516;&#26102;&#30772;&#22351;&#20027;&#39064;&#21644;
&lt;/p&gt;
&lt;p&gt;
Topic segmentation is critical for obtaining structured long documents and improving downstream tasks like information retrieval. Due to its ability of automatically exploring clues of topic shift from a large amount of labeled data, recent supervised neural models have greatly promoted the development of long document topic segmentation, but leaving the deeper relationship of semantic coherence and topic segmentation underexplored. Therefore, this paper enhances the supervised model's ability to capture coherence from both structure and similarity perspectives to further improve the topic segmentation performance, including the Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to force the model to comprehend structural information by learning the original relations of adjacent sentences in a disarrayed document, which is constructed by jointly disrupting the original document at the topic and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#22833;&#35821;&#31867;&#22411;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.11710</link><description>&lt;p&gt;
&#23398;&#20064;&#21516;&#26102;&#35821;&#35328;&#25163;&#21183;&#29992;&#20110;&#22810;&#27169;&#24577;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Co-Speech Gesture for Multimodal Aphasia Type Detection. (arXiv:2310.11710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11710
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#22833;&#35821;&#31867;&#22411;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#35821;&#26159;&#19968;&#31181;&#30001;&#33041;&#25439;&#20260;&#24341;&#36215;&#30340;&#35821;&#35328;&#38556;&#30861;&#65292;&#38656;&#35201;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#30340;&#22833;&#35821;&#31867;&#22411;&#65292;&#22914;Broca&#22833;&#35821;&#21644;Wernicke&#22833;&#35821;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#22833;&#35821;&#30340;&#26041;&#27861;&#65292;&#20154;&#20204;&#24182;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#20998;&#26512;&#21516;&#26102;&#35821;&#35328;&#25163;&#21183;&#23545;&#20110;&#21306;&#20998;&#22833;&#35821;&#31867;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#35821;&#38899;&#21644;&#30456;&#24212;&#30340;&#25163;&#21183;&#27169;&#24335;&#36827;&#34892;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;&#12290;&#36890;&#36807;&#23398;&#20064;&#27599;&#31181;&#22833;&#35821;&#31867;&#22411;&#30340;&#35821;&#38899;&#21644;&#25163;&#21183;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#23545;&#25163;&#21183;&#20449;&#24687;&#25935;&#24863;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#30340;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25163;&#21183;&#29305;&#24449;&#20248;&#20110;&#22768;&#23398;&#29305;&#24449;&#65292;&#31361;&#26174;&#20102;&#25163;&#21183;&#34920;&#36798;&#22312;&#26816;&#27979;&#22833;&#35821;&#31867;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca's and Wernicke's aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each aphasia type, our model can generate textual representations sensitive to gesture information, leading to accurate aphasia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2\%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types. We pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#36890;&#29992;&#21644;&#29305;&#23450;&#39046;&#22495;&#20013;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#26159;&#22312;&#25152;&#26377;&#39046;&#22495;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.11638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Systematic Assessment of Factual Knowledge in Large Language Models. (arXiv:2310.11638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#36890;&#29992;&#21644;&#29305;&#23450;&#39046;&#22495;&#20013;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#26159;&#22312;&#25152;&#26377;&#39046;&#22495;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38382;&#31572;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#28085;&#30422;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#29992;&#39046;&#22495;&#65292;&#36825;&#21487;&#33021;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#37325;&#21472;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20174;&#32473;&#23450;KG&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#39044;&#26399;&#31572;&#26696;&#65292;&#28982;&#21518;&#35780;&#20272;LLMs&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#36890;&#29992;&#39046;&#22495;&#21644;&#29305;&#23450;&#39046;&#22495;&#20013;&#31995;&#32479;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#19982;KGs&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;ChatGPT&#22312;&#25152;&#26377;&#39046;&#22495;&#20013;&#22987;&#32456;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;LLMs&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#25351;&#20196;&#24494;&#35843;&#12289;&#39046;&#22495;&#21644;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.
&lt;/p&gt;</description></item><item><title>IDMO&#39033;&#30446;&#26088;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#65292;&#20854;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#26032;&#22411;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#12289;&#35780;&#20272;GPT-4&#31561;&#12290;</title><link>http://arxiv.org/abs/2310.11097</link><description>&lt;p&gt;
&#29992;&#20110;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#23454;&#39564;&#65306;IDMO&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11097
&lt;/p&gt;
&lt;p&gt;
IDMO&#39033;&#30446;&#26088;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#65292;&#20854;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#26032;&#22411;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#12289;&#35780;&#20272;GPT-4&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22823;&#21033;&#25968;&#23383;&#23186;&#20307;&#35266;&#23519;&#39033;&#30446;&#65288;IDMO&#65289;&#26159;&#27431;&#27954;&#19968;&#39033;&#20513;&#35758;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#12290;&#26412;&#25253;&#21578;&#27010;&#36848;&#20102;Rai-CRITS&#22312;&#35813;&#39033;&#30446;&#20013;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#21019;&#24314;&#29992;&#20110;&#27979;&#35797;&#25216;&#26415;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#65288;ii&#65289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#31867;Pagella Politica&#30340;&#35009;&#20915;&#20197;&#20415;&#20110;&#26356;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#65288;iii&#65289;&#21019;&#24314;&#33258;&#21160;&#27169;&#22411;&#65292;&#23545;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#25991;&#26412;&#34164;&#21547;&#20855;&#26377;&#24322;&#24120;&#31934;&#24230;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#65288;iv&#65289;&#20351;&#29992;GPT-4&#35780;&#20272;&#25991;&#26412;&#34164;&#21547;&#65292; &#65288;v&#65289;&#22312;&#22269;&#23478;&#27963;&#21160;&#20013;&#24320;&#23637;&#25552;&#39640;&#23545;&#20551;&#26032;&#38395;&#24847;&#35782;&#30340;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Italian Digital Media Observatory (IDMO) project, part of a European initiative, focuses on countering disinformation and fake news. This report outlines contributions from Rai-CRITS to the project, including: (i) the creation of novel datasets for testing technologies (ii) development of an automatic model for categorizing Pagella Politica verdicts to facilitate broader analysis (iii) creation of an automatic model for recognizing textual entailment with exceptional accuracy on the FEVER dataset (iv) assessment using GPT-4 to identify textual entailmen (v) a game to raise awareness about fake news at national events.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2310.09886</link><description>&lt;p&gt;
&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#65288;LSG&#65289;&#26159;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35753;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#65292;&#20197;&#19981;&#26029;&#23398;&#20064;&#26032;&#30340;&#29983;&#25104;&#27169;&#24335;&#24182;&#36991;&#20813;&#36951;&#24536;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;LSG&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#32500;&#25345;&#26087;&#30693;&#35782;&#65292;&#32780;&#23545;&#36328;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#20851;&#27880;&#36739;&#23569;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#33719;&#21462;&#30340;&#31867;&#20284;&#20219;&#21153;&#30340;&#30693;&#35782;&#26356;&#22909;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#65288;DMEA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#30830;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#20419;&#36827;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23398;&#20064;&#36807;&#31243;&#24456;&#23481;&#26131;&#20559;&#21521;&#20110;&#24403;&#21069;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the lea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#22312;&#26410;&#35265;&#25554;&#27133;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09135</link><description>&lt;p&gt;
HierarchicalContrast: &#19968;&#31181;&#29992;&#20110;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling. (arXiv:2310.09135v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#22312;&#26410;&#35265;&#25554;&#27133;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#22330;&#26223;&#20013;&#65292;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#22312;&#21033;&#29992;&#28304;&#39046;&#22495;&#30693;&#35782;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#32570;&#23569;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#20854;&#24615;&#33021;&#24448;&#24448;&#19981;&#29702;&#24819;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#26041;&#27861;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#23427;&#20204;&#21482;&#33021;&#22312;&#24050;&#35265;&#25554;&#27133;&#19978;&#26377;&#25928;&#22320;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#65292;&#23545;&#26410;&#35265;&#25554;&#27133;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#23884;&#20837;&#30340;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#38388;&#38548;&#21644;&#20869;&#37096;&#26631;&#35760;&#20998;&#24067;&#30340;&#36317;&#31163;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#12290;&#36825;&#40723;&#21169;HiCL&#22312;&#35757;&#32451;&#38454;&#27573;&#27867;&#21270;&#21040;&#26410;&#35265;&#30340;&#25554;&#27133;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#23545;&#26631;&#31614;&#36827;&#34892;&#20844;&#27491;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
In task-oriented dialogue scenarios, cross-domain zero-shot slot filling plays a vital role in leveraging source domain knowledge to learn a model with high generalization ability in unknown target domain where annotated data is unavailable. However, the existing state-of-the-art zero-shot slot filling methods have limited generalization ability in target domain, they only show effective knowledge transfer on seen slots and perform poorly on unseen slots. To alleviate this issue, we present a novel Hierarchical Contrastive Learning Framework (HiCL) for zero-shot slot filling. Specifically, we propose a coarseto fine-grained contrastive learning based on Gaussian-distributed embedding to learn the generalized deep semantic relations between utterance-tokens, by optimizing inter- and intra-token distribution distance. This encourages HiCL to generalize to the slot types unseen at training phase. Furthermore, we present a new iterative label set semantics inference method to unbiasedly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26816;&#32034;-&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#26469;&#35757;&#32451;&#26377;&#24863;&#30693;&#21147;&#30340;&#26816;&#32034;&#22120;&#65292;&#24182;&#32467;&#21512;&#21508;&#31181;&#20803;&#30693;&#35782;&#26469;&#25351;&#23548;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30693;&#35782;&#30340;&#21033;&#29992;&#25928;&#29575;&#21644;&#29983;&#25104;&#22238;&#24212;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.08877</link><description>&lt;p&gt;
&#29992;&#20110;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26816;&#32034;-&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System. (arXiv:2310.08877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26816;&#32034;-&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#26469;&#35757;&#32451;&#26377;&#24863;&#30693;&#21147;&#30340;&#26816;&#32034;&#22120;&#65292;&#24182;&#32467;&#21512;&#21508;&#31181;&#20803;&#30693;&#35782;&#26469;&#25351;&#23548;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30693;&#35782;&#30340;&#21033;&#29992;&#25928;&#29575;&#21644;&#29983;&#25104;&#22238;&#24212;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#19968;&#20010;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#20174;&#22823;&#35268;&#27169;&#30340;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#30693;&#35782;&#23545;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20415;&#26377;&#25928;&#22788;&#29702;&#26412;&#22320;&#21270;&#21644;&#19987;&#19994;&#21270;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;T5&#21644;ChatGPT&#65289;&#22312;&#29983;&#25104;&#22238;&#24212;&#26102;&#36890;&#24120;&#24456;&#38590;&#21306;&#20998;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#24211;&#35760;&#24405;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#24322;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#22238;&#24212;&#36136;&#37327;&#19981;&#29702;&#24819;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#26469;&#35757;&#32451;&#19968;&#20010;&#26377;&#24863;&#30693;&#21147;&#30340;&#26816;&#32034;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22238;&#24212;&#29983;&#25104;&#30340;&#20449;&#21495;&#36827;&#34892;&#30417;&#30563;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#21040;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#65292;&#36824;&#32467;&#21512;&#20102;&#21508;&#31181;&#20803;&#30693;&#35782;&#26469;&#25351;&#23548;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30693;&#35782;&#30340;&#21033;&#29992;&#25928;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;T5&#21644;ChatGPT&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#19977;&#20010;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#32467;&#21512;&#20803;&#30693;&#35782;&#26102;&#65292;&#22238;&#24212;&#29983;&#25104;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#35813;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing an efficient retriever to retrieve knowledge from a large-scale knowledge base (KB) is critical for task-oriented dialogue systems to effectively handle localized and specialized tasks. However, widely used generative models such as T5 and ChatGPT often struggle to differentiate subtle differences among the retrieved KB records when generating responses, resulting in suboptimal quality of generated responses. In this paper, we propose the application of maximal marginal likelihood to train a perceptive retriever by utilizing signals from response generation for supervision. In addition, our approach goes beyond considering solely retrieved entities and incorporates various meta knowledge to guide the generator, thus improving the utilization of knowledge. We evaluate our approach on three task-oriented dialogue datasets using T5 and ChatGPT as the backbone models. The results demonstrate that when combined with meta knowledge, the response generator can effectively leverage 
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#35821;&#35328;&#20195;&#29702;&#26426;&#21046;&#65292;&#23427;&#19981;&#38656;&#35201;&#19987;&#23478;&#31034;&#36394;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#21644;&#32467;&#26500;&#21270;&#24605;&#32771;&#31649;&#29702;&#26469;&#23398;&#20064;&#21644;&#25913;&#21892;&#35745;&#31639;&#26426;&#19978;&#30340;&#25511;&#21046;&#65292;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08740</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#26426;&#25511;&#21046;&#30340;&#38646;&#26679;&#26412;&#35821;&#35328;&#20195;&#29702;&#26426;&#21046;&#21450;&#20854;&#32467;&#26500;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
A Zero-Shot Language Agent for Computer Control with Structured Reflection. (arXiv:2310.08740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#35821;&#35328;&#20195;&#29702;&#26426;&#21046;&#65292;&#23427;&#19981;&#38656;&#35201;&#19987;&#23478;&#31034;&#36394;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#21644;&#32467;&#26500;&#21270;&#24605;&#32771;&#31649;&#29702;&#26469;&#23398;&#20064;&#21644;&#25913;&#21892;&#35745;&#31639;&#26426;&#19978;&#30340;&#25511;&#21046;&#65292;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35745;&#21010;&#21644;&#25191;&#34892;&#39640;&#32423;&#30446;&#26631;&#26041;&#38754;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#22914;&#22312;&#27963;&#21160;&#30340;&#35745;&#31639;&#26426;&#29615;&#22659;&#65288;&#20363;&#22914;MiniWoB ++&#65289;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#35201;&#27714;&#27169;&#22411;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#25110;&#23569;/&#22810;&#26679;&#26412;&#25552;&#31034;&#20174;&#20219;&#21153;&#30340;&#36319;&#36394;&#31034;&#20363;&#20013;&#23398;&#20064;&#12290;&#22312;&#27809;&#26377;&#36825;&#20123;&#36319;&#36394;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20195;&#29702;&#26426;&#21046;&#22914;&#20309;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#24182;&#25913;&#21892;&#22312;&#35745;&#31639;&#26426;&#19978;&#30340;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#19968;&#20010;&#20195;&#29702;&#26426;&#26500;&#25191;&#34892;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#38646;&#26679;&#26412;&#20195;&#29702;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#19981;&#38656;&#35201;&#32473;&#23450;&#30340;&#19987;&#23478;&#31034;&#36394;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#26426;&#21046;&#23545;&#20110;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#19978;&#30340;&#21487;&#25191;&#34892;&#34892;&#21160;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#21644;&#32467;&#26500;&#21270;&#24605;&#32771;&#31649;&#29702;&#26469;&#35782;&#21035;&#21644;&#23398;&#20064;&#38169;&#35823;&#65292;&#20174;&#32780;&#36880;&#27493;&#25512;&#36827;&#20219;&#21153;&#12290;&#22312;MiniWoB ++&#30340;&#31616;&#21333;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#20195;&#29702;&#26426;&#21046;&#24448;&#24448;&#32988;&#36807;&#26368;&#36817;&#30340;SoTA&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#21453;&#24605;&#20195;&#29702;&#26426;&#21046;&#19982;&#20808;&#21069;&#30340;&#20195;&#29702;&#26426;&#21046;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25910;&#38598;&#30495;&#23454;&#25968;&#25454;&#38598;riSum&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21442;&#32771;&#26041;&#27861;&#65292;&#20197;&#34913;&#37327;&#36825;&#31181;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#38656;&#35201;&#39640;&#36136;&#37327;&#25688;&#35201;&#30340;&#21442;&#32771;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.08394</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#22909;&#30340;&#25351;&#20196;&#36981;&#24490;&#35780;&#20272;&#65306;&#25688;&#35201;&#20013;&#30340;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization. (arXiv:2310.08394v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25910;&#38598;&#30495;&#23454;&#25968;&#25454;&#38598;riSum&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21442;&#32771;&#26041;&#27861;&#65292;&#20197;&#34913;&#37327;&#36825;&#31181;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#38656;&#35201;&#39640;&#36136;&#37327;&#25688;&#35201;&#30340;&#21442;&#32771;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;&#20309;&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#19978;&#26377;&#25152;&#22686;&#38271;&#65292;&#20294;&#26159;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27491;&#30830;&#24615;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20803;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#34913;&#37327;LLMs&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#22312;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#20010;&#26032;&#30340;&#30701;&#25991;&#24418;&#24335;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;riSum&#65292;&#20854;&#20013;&#21253;&#21547;300&#20010;&#25991;&#26723;&#25351;&#20196;&#23545;&#65292;&#27599;&#20010;&#23545;&#24212;3&#20010;&#31572;&#26696;&#12290;&#25152;&#26377;900&#20010;&#31572;&#26696;&#30001;3&#21517;&#20154;&#31867;&#26631;&#27880;&#21592;&#36827;&#34892;&#35780;&#20998;&#12290;&#21033;&#29992;riSum&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35780;&#20272;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#26032;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#24050;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#19982;&#38656;&#35201;&#39640;&#36136;&#37327;&#25688;&#35201;&#30340;&#26114;&#36149;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing 300 document-instruction pairs with 3 answers each. All 900 answers are rated by 3 human annotators. Using riSum, we analyze the agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on par with costly reference-based metrics that require high-quality summaries.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#24182;&#22312;&#23545;&#35805;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08130</link><description>&lt;p&gt;
&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#23454;&#29616;&#32454;&#31890;&#24230;&#23545;&#35805;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Conversational Decoding via Isotropic and Proximal Search. (arXiv:2310.08130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#24182;&#22312;&#23545;&#35805;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#37319;&#29992;&#36890;&#29992;&#30340;&#25991;&#26412;&#35299;&#30721;&#26041;&#27861;&#26469;&#36827;&#34892;&#23545;&#35805;&#22238;&#24212;&#29983;&#25104;&#12290;&#34429;&#28982;&#37319;&#29992;&#20102;&#23545;&#35805;&#29305;&#23450;&#30340;&#32534;&#30721;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#30340;&#22238;&#24212;&#36136;&#37327;&#65292;&#20294;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#21040;wu2023learning&#30340;&#21551;&#21457;&#65292;&#35748;&#20026;&#22909;&#30340;&#23545;&#35805;&#29305;&#24449;&#31354;&#38388;&#24212;&#36981;&#24490;&#23616;&#37096;&#24615;&#21644;&#21508;&#21521;&#21516;&#24615;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#37327;&#21644;&#21306;&#20998;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#35805;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by \citet{wu2023learning} that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed \textit{isotropic and proximal search (IPS)}. Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#30340;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598; arXiv2Formal&#12290;</title><link>http://arxiv.org/abs/2310.07957</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#24418;&#24335;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Approach Towards Autoformalization. (arXiv:2310.07957v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#30340;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598; arXiv2Formal&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#25968;&#23398;&#35777;&#26126;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#26426;&#30340;&#36741;&#21161;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;&#33258;&#21160;&#24418;&#24335;&#21270;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#23398;&#33258;&#21160;&#36716;&#21270;&#20026;&#21487;&#20197;&#30001;&#31243;&#24207;&#39564;&#35777;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#23545;&#20110;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#39640;&#32423;&#25968;&#23398;&#26469;&#35828;&#12290;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#25968;&#23398;&#38656;&#35201;&#22823;&#37327;&#30340;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#26131;&#20110;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65306;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#65288;&#21253;&#21547;&#26410;&#38142;&#25509;&#30340;&#23450;&#20041;&#21644;&#23450;&#29702;&#30340;&#24418;&#24335;&#21270;&#65289;&#12289;&#23454;&#20307;&#38142;&#25509;&#65288;&#38142;&#25509;&#21040;&#27491;&#30830;&#30340;&#23450;&#29702;&#21644;&#23450;&#20041;&#65289;&#20197;&#21450;&#35843;&#25972;&#31867;&#22411;&#20197;&#36890;&#36807;&#31867;&#22411;&#26816;&#26597;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;arXiv2Formal&#65292;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;arXiv.org&#30340;&#35770;&#25991;&#20013;&#25277;&#21462;&#30340;50&#20010;&#23450;&#29702;&#22312;Lean&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#36827;&#34892;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#27426;&#36814;&#20219;&#20309;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying mathematical proofs is difficult, but can be automated with the assistance of a computer. Autoformalization is the task of automatically translating natural language mathematics into a formal language that can be verified by a program. This is a challenging task, and especially for higher-level mathematics found in research papers. Research paper mathematics requires large amounts of background and context. In this paper, we propose an avenue towards tackling autoformalization for research-level mathematics, by breaking the task into easier and more approachable subtasks: unlinked formalization (formalization with unlinked definitions and theorems), entity linking (linking to the proper theorems and definitions), and finally adjusting types so it passes the type checker. In addition, we present arXiv2Formal, a benchmark dataset for unlinked formalization consisting of 50 theorems formalized for the Lean theorem prover sampled from papers on arXiv.org. We welcome any contribut
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#26080;&#20851;&#30340;&#30693;&#35782;&#36873;&#25321;&#26041;&#27861;GATE&#65292;&#23558;&#30693;&#35782;&#36873;&#25321;&#25918;&#32622;&#22312;&#29983;&#25104;&#20043;&#21069;&#65292;&#21487;&#20197;&#20943;&#23569;&#21518;&#32493;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#30340;&#36127;&#25285;&#65292;&#24182;&#20026;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#37327;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.07659</link><description>&lt;p&gt;
&#20248;&#20808;&#36873;&#25321;&#30693;&#35782;&#65306;&#38754;&#21521;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#29983;&#25104;&#22120;&#26080;&#20851;&#30340;&#30693;&#35782;&#39044;&#36873;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue. (arXiv:2310.07659v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#26080;&#20851;&#30340;&#30693;&#35782;&#36873;&#25321;&#26041;&#27861;GATE&#65292;&#23558;&#30693;&#35782;&#36873;&#25321;&#25918;&#32622;&#22312;&#29983;&#25104;&#20043;&#21069;&#65292;&#21487;&#20197;&#20943;&#23569;&#21518;&#32493;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#30340;&#36127;&#25285;&#65292;&#24182;&#20026;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#37327;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#20934;&#30830;&#30340;&#30693;&#35782;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#26469;&#32452;&#32455;&#29616;&#26377;&#30340;&#25991;&#29486;&#65292;&#21363;&#23558;&#30693;&#35782;&#36873;&#25321;&#19982;&#29983;&#25104;&#22120;&#32806;&#21512;&#65292;&#24182;&#25918;&#32622;&#22312;&#29983;&#25104;&#20043;&#21069;&#21644;&#20043;&#21518;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#31532;&#19977;&#20010;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#30740;&#31350;&#31867;&#21035;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#25552;&#21069;&#20934;&#30830;&#36873;&#25321;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;&#21518;&#32493;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12289;&#35843;&#25972;&#21644;&#35299;&#37322;&#36127;&#25285;&#65292;&#29305;&#21035;&#26159;LLMs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#26080;&#20851;&#30340;&#30693;&#35782;&#36873;&#25321;&#26041;&#27861;GATE&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#30693;&#35782;&#32467;&#26500;&#21644;&#21487;&#21464;&#30340;&#30693;&#35782;&#35201;&#27714;&#20013;&#36873;&#25321;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#30693;&#35782;&#26469;&#20026;&#21518;&#32493;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#20934;&#22791;&#30693;&#35782;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;GATE&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#34920;&#26126;&#22312;&#29983;&#25104;&#20043;&#21069;&#36827;&#34892;&#30693;&#35782;&#36873;&#25321;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20419;&#20351;LLMs&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate knowledge selection is critical in knowledge-grounded dialogue systems. Towards a closer look at it, we offer a novel perspective to organize existing literature, i.e., knowledge selection coupled with, after, and before generation. We focus on the third under-explored category of study, which can not only select knowledge accurately in advance, but has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs. We propose GATE, a generator-agnostic knowledge selection method, to prepare knowledge for subsequent response generation models by selecting context-related knowledge among different knowledge structures and variable knowledge requirements. Experimental results demonstrate the superiority of GATE, and indicate that knowledge selection before generation is a lightweight yet effective way to facilitate LLMs (e.g., ChatGPT) to generate more informative responses.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20026;&#23450;&#21046;&#33021;&#26356;&#22909;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#30340;LLM&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.07328</link><description>&lt;p&gt;
&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Instruction-tuning Large Language Models in Chinese. (arXiv:2310.07328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20026;&#23450;&#21046;&#33021;&#26356;&#22909;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#30340;LLM&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#25104;&#21151;&#39564;&#35777;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#20013;&#30340;&#28508;&#21147;&#12290;&#38543;&#21518;&#65292;LLM&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#24320;&#28304;&#31038;&#21306;&#23545;&#25351;&#23548;&#35843;&#25972;&#30340;&#20852;&#36259;&#65292;&#36825;&#34987;&#35748;&#20026;&#21487;&#20197;&#21152;&#24555;ChatGPT&#30340;&#22797;&#21046;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20013;&#25991;&#30340;LLM&#25351;&#23548;&#35843;&#25972;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#20013;&#25991;LLM&#30340;&#25351;&#23548;&#35843;&#25972;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#19968;&#26412;&#25552;&#20379;&#26377;&#20215;&#20540;&#21457;&#29616;&#30340;&#28921;&#39274;&#20070;&#26469;&#26377;&#25928;&#22320;&#23450;&#21046;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#36825;&#19977;&#20010;&#23545;&#25351;&#23548;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#24605;&#32500;&#38142;&#25968;&#25454;&#21644;&#20154;&#31867;&#20215;&#20540;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can
&lt;/p&gt;</description></item><item><title>SEER&#26159;&#19968;&#31181;&#38024;&#23545;&#19978;&#19979;&#25991;&#28151;&#21512;&#38382;&#31572;&#30340;&#32972;&#21253;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32972;&#21253;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20195;&#34920;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.06675</link><description>&lt;p&gt;
SEER&#65306;&#19968;&#31181;&#38024;&#23545;&#19978;&#19979;&#25991;&#28151;&#21512;&#38382;&#31572;&#30340;&#32972;&#21253;&#26041;&#27861;&#36827;&#34892;&#26679;&#26412;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA. (arXiv:2310.06675v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06675
&lt;/p&gt;
&lt;p&gt;
SEER&#26159;&#19968;&#31181;&#38024;&#23545;&#19978;&#19979;&#25991;&#28151;&#21512;&#38382;&#31572;&#30340;&#32972;&#21253;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32972;&#21253;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20195;&#34920;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19978;&#19979;&#25991;&#30340;&#38382;&#31572;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20197;&#21508;&#31181;&#26041;&#24335;&#32467;&#21512;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21644;&#32467;&#26500;&#21270;&#34920;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#36827;&#27493;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#19968;&#23567;&#32452;&#25903;&#25345;&#31034;&#20363;&#36827;&#34892;&#39044;&#27979;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25903;&#25345;&#31034;&#20363;&#30340;&#36873;&#25321;&#36807;&#31243;&#65292;&#29305;&#21035;&#26159;&#22312;&#28151;&#21512;&#38382;&#31572;&#30340;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#21040;&#25512;&#29702;&#38142;&#30340;&#22810;&#26679;&#24615;&#21644;&#28151;&#21512;&#19978;&#19979;&#25991;&#30340;&#35268;&#27169;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#28151;&#21512;&#25512;&#29702;&#30340;&#31034;&#20363;&#36873;&#25321;&#65288;SEER&#65289;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#26082;&#20855;&#26377;&#20195;&#34920;&#24615;&#21448;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;SEER&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#23558;&#31034;&#20363;&#36873;&#25321;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32972;&#21253;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;&#32972;&#21253;&#26694;&#26550;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#23558;&#22810;&#26679;&#24615;&#32422;&#26463;&#32435;&#20837;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over hybrid contexts is a complex task, which requires the combination of information extracted from unstructured texts and structured tables in various ways. Recently, In-Context Learning demonstrated significant performance advances for reasoning tasks. In this paradigm, a large language model performs predictions based on a small set of supporting exemplars. The performance of In-Context Learning depends heavily on the selection procedure of the supporting exemplars, particularly in the case of HybridQA, where considering the diversity of reasoning chains and the large size of the hybrid contexts becomes crucial. In this work, we present Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse. The key novelty of SEER is that it formulates exemplar selection as a Knapsack Integer Linear Program. The Knapsack framework provides the flexibility to incorporate diversity constraints tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;Span-Trigger-based Contextual Pooling(STCP)&#21644;Role-based Latent Information Guidance (RLIG)&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#20013;&#24573;&#30053;&#30340;&#38750;&#35770;&#35777;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#20197;&#21450;&#35770;&#35777;&#35282;&#33394;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#65292;&#20197;&#21450;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#24182;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05991</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#25552;&#21319;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance. (arXiv:2310.05991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;Span-Trigger-based Contextual Pooling(STCP)&#21644;Role-based Latent Information Guidance (RLIG)&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#20013;&#24573;&#30053;&#30340;&#38750;&#35770;&#35777;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#20197;&#21450;&#35770;&#35777;&#35282;&#33394;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#65292;&#20197;&#21450;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#24182;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21477;&#23376;&#32423;&#20107;&#20214;&#35770;&#35777;&#30456;&#27604;&#65292;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#38754;&#20020;&#30528;&#38271;&#36755;&#20837;&#21644;&#36328;&#21477;&#23376;&#25512;&#29702;&#30340;&#26032;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#25429;&#25417;&#27599;&#20010;&#20107;&#20214;&#20013;&#20505;&#36873;&#35770;&#35777;&#19982;&#20107;&#20214;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;a&#65289;&#38750;&#35770;&#35777;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#65307;b&#65289;&#35770;&#35777;&#35282;&#33394;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#65288;&#22522;&#20110;&#36328;&#24230;&#35302;&#21457;&#22120;&#30340;&#19978;&#19979;&#25991;&#27719;&#32858;&#21644;&#28508;&#22312;&#35282;&#33394;&#24341;&#23548;&#65289;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#22522;&#20110;&#36328;&#24230;&#35302;&#21457;&#22120;&#30340;&#19978;&#19979;&#25991;&#27719;&#32858;&#65288;STCP&#65289;&#26681;&#25454;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#35282;&#33394;&#30340;&#28508;&#22312;&#20449;&#24687;&#24341;&#23548;&#65288;RLIG&#65289;&#27169;&#22359;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#65292;&#36890;&#36807;&#35282;&#33394;&#20132;&#20114;&#32534;&#30721;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#20197;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;&#20505;&#36873;&#35770;&#35777;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial points: a) non-argument contextual clue information; b) the relevance among argument roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling and latent Role Guidance) model, which contains two novel and effective modules for the above problem. The Span-Trigger-based Contextual Pooling(STCP) adaptively selects and aggregates the information of non-argument clue words based on the context attention weights of specific argument-trigger pairs from pre-trained model. The Role-based Latent Information Guidance (RLIG) module constructs latent role representations, makes them interact through role-interactive encoding to capture semantic relevance, and merges them into candidate ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20027;&#24352;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#33021;&#22815;&#39564;&#35777;&#22797;&#26434;&#30340;&#20027;&#24352;&#24182;&#29983;&#25104;&#35299;&#37322;&#65292;&#23545;&#21327;&#21161;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20027;&#24352;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models. (arXiv:2310.05253v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20027;&#24352;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#33021;&#22815;&#39564;&#35777;&#22797;&#26434;&#30340;&#20027;&#24352;&#24182;&#29983;&#25104;&#35299;&#37322;&#65292;&#23545;&#21327;&#21161;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#24352;&#39564;&#35777;&#22312;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20027;&#24352;&#39564;&#35777;&#24037;&#20316;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#26377;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#65292;&#37027;&#23601;&#26159;&#22914;&#20309;&#22312;&#19981;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20027;&#24352;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#25552;&#20379;&#20840;&#38754;&#35299;&#37322;&#20197;&#35777;&#26126;&#20854;&#20915;&#31574;&#24182;&#21327;&#21161;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#20027;&#24352;&#39564;&#35777;&#21644;&#29983;&#25104;&#35299;&#37322;&#30340;&#19968;&#38454;&#36923;&#36753;&#25351;&#23548;&#30340;&#30693;&#35782;&#22522;&#30784;&#30340;&#25512;&#29702;&#26041;&#27861;&#65288;FOLK&#65289;&#12290;FOLK&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23558;&#20027;&#24352;&#36716;&#21270;&#20026;&#19968;&#38454;&#36923;&#36753;&#23376;&#21477;&#65292;&#24182;&#29983;&#25104;&#19968;&#32452;&#35859;&#35789;&#65292;&#27599;&#20010;&#35859;&#35789;&#23545;&#24212;&#19968;&#20010;&#38656;&#35201;&#39564;&#35777;&#30340;&#23376;&#20027;&#24352;&#12290;&#28982;&#21518;&#65292;FOLK&#36890;&#36807;&#19968;&#32452;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#23545;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#25351;&#23548;&#30340;&#25512;&#29702;&#65292;&#20174;&#32780;&#36827;&#34892;&#30495;&#23454;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02954</link><description>&lt;p&gt;
DQ-LoRe: &#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20302;&#31209;&#36817;&#20284;&#21452;&#37325;&#26597;&#35810;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#30340;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#24341;&#23548;LLMs&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#21033;&#29992;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#33539;&#24335;&#20013;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#26368;&#20027;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#36873;&#25321;&#31034;&#20363;&#26469;&#20419;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#65288;DQ-LoRe&#65289;&#26469;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;&#21452;&#37325;&#26597;&#35810;&#39318;&#20808;&#26597;&#35810;LLM&#20197;&#33719;&#21462;LLM&#29983;&#25104;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;CoT&#65292;&#28982;&#21518;&#36890;&#36807;&#38382;&#39064;&#21644;&#30693;&#35782;&#26597;&#35810;&#26816;&#32034;&#22120;&#20197;&#33719;&#21462;&#26368;&#32456;&#30340;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#31532;&#20108;&#20010;&#26597;&#35810;&#65292;LoRe&#21033;&#29992;&#38477;&#32500;&#25216;&#26415;&#26469;&#25913;&#36827;&#31034;&#20363;&#36873;&#25321;&#65292;&#30830;&#20445;&#19982;&#36755;&#20837;&#38382;&#39064;&#30340;&#30693;&#35782;&#23494;&#20999;&#23545;&#40784;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;DQ-LoRe&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive ex
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02357</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#27602;&#24615;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02357
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#19981;&#28165;&#12290;&#35895;&#27468;&#26071;&#19979;&#30340;&#22242;&#38431;Jigsaw&#26159;&#35813;&#39046;&#22495;&#30340;&#39046;&#23548;&#32773;&#20043;&#19968;&#65292;&#20182;&#20204;&#20351;&#29992;Dixon&#31561;&#20154;&#32473;&#20986;&#30340;&#27602;&#24615;&#23450;&#20041;&#65306;&#8220;&#31895;&#40065;&#12289;&#19981;&#23562;&#37325;&#25110;&#19981;&#21512;&#29702;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#35753;&#26576;&#20154;&#31163;&#24320;&#35752;&#35770;&#8221;&#12290;&#20154;&#20204;&#21487;&#20197;&#31435;&#21363;&#30475;&#21040;&#36825;&#20010;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32473;&#20986;&#27602;&#24615;&#30340;&#23450;&#37327;&#24230;&#37327;&#65292;&#32780;&#19988;&#28041;&#21450;&#39640;&#24230;&#20027;&#35266;&#30340;&#25991;&#21270;&#26415;&#35821;&#12290;&#23613;&#31649;&#23384;&#22312;&#27169;&#31946;&#21644;&#32570;&#38519;&#65292;&#20294;&#36825;&#20010;&#23450;&#20041;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.12273</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#22522;&#20110;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#25913;&#36827;VTE&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12273
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#38745;&#33033;&#34880;&#26643;&#26643;&#22622;&#65288;VTE&#65289;&#65292;&#21253;&#25324;&#28145;&#38745;&#33033;&#34880;&#26643;&#65288;DVT&#65289;&#21644;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#65292;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#24050;&#32463;&#22312;&#20174;&#22238;&#39038;&#24615;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;VTE&#20107;&#20214;&#25110;&#24110;&#21161;&#20020;&#24202;&#19987;&#23478;&#35782;&#21035;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;VTE&#20107;&#20214;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#26377;&#38480;&#30340;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22797;&#26434;&#24615;&#21644;&#24322;&#36136;&#24615;&#20197;&#21450;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#26377;&#25928;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;NLP&#27169;&#22411;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DL&#26041;&#27861;&#30340;&#26032;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#12289;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;NLP&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#25552;&#39640;&#38750;&#32467;&#26500;&#21270;&#65288;&#33258;&#30001;&#25991;&#26412;&#65289;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;VTE&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid and accurate identification of Venous thromboembolism (VTE), a severe cardiovascular condition including deep vein thrombosis (DVT) and pulmonary embolism (PE), is important for effective treatment. Leveraging Natural Language Processing (NLP) on radiology reports, automated methods have shown promising advancements in identifying VTE events from retrospective data cohorts or aiding clinical experts in identifying VTE events from radiology reports. However, effectively training Deep Learning (DL) and the NLP models is challenging due to limited labeled medical text data, the complexity and heterogeneity of radiology reports, and data imbalance. This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier, to improve the accuracy of VTE identification in unstructured (free-text) radiology reports. Our experimental results demonstrate the model's efficacy, achievi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11895</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20998;&#31867;&#22312;&#35821;&#38899;&#21644;&#22768;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#35757;&#32451;&#25968;&#25454;&#65288;&#36991;&#20813;&#36807;&#25311;&#21512;&#65289;&#24182;&#20351;&#20854;&#33021;&#22815;&#33391;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio classification plays a crucial role in speech and sound processing tasks with a wide range of applications. There still remains a challenge of striking the right balance between fitting the model to the training data (avoiding overfitting) and enabling it to generalise well to a new domain. Leveraging the transferability of contrastive learning, we introduce Audio Contrastive-based Fine-tuning (AudioConFit), an efficient approach characterised by robust generalisability. Empirical experiments on a variety of audio classification tasks demonstrate the effectiveness and robustness of our approach, which achieves state-of-the-art results in various settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20154;&#21475;&#23398;&#39046;&#22495;&#30340;Agent Based Models (ABMs)&#30340;&#25968;&#23398;&#35268;&#33539;&#30340;&#21512;&#36866;&#24418;&#24335;&#26415;&#35821;&#65292;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#19982;O.D.D.&#21327;&#35758;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#21046;&#36807;&#31243;&#20013;&#30340;&#27495;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.13081</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#21475;&#29305;&#24449;&#30340;&#22266;&#23450;&#27493;&#38271;&#21333;&#26102;&#38047;&#27169;&#25311;&#30340;Agent-Based&#27169;&#22411;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#26415;&#35821;
&lt;/p&gt;
&lt;p&gt;
Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations. (arXiv:2308.13081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20154;&#21475;&#23398;&#39046;&#22495;&#30340;Agent Based Models (ABMs)&#30340;&#25968;&#23398;&#35268;&#33539;&#30340;&#21512;&#36866;&#24418;&#24335;&#26415;&#35821;&#65292;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#19982;O.D.D.&#21327;&#35758;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#21046;&#36807;&#31243;&#20013;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20154;&#21475;&#23398;&#39046;&#22495;&#30340;Agent Based Models (ABMs)&#30340;&#25968;&#23398;&#35268;&#33539;&#30340;&#21512;&#36866;&#24418;&#24335;&#26415;&#35821;&#12290;&#30446;&#26631;ABMs&#30340;&#27169;&#25311;&#36981;&#24490;&#22266;&#23450;&#27493;&#38271;&#21333;&#26102;&#38047;&#27169;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#26415;&#35821;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#30340;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#35268;&#33539;&#21644;&#36873;&#25321;&#24615;&#22320;&#35760;&#24405;&#19968;&#32452;&#37325;&#35201;&#30340;&#65288;&#20154;&#21475;&#65289;ABMs&#12290;&#28982;&#32780;&#65292;&#21487;&#20197;&#24819;&#35937;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#36825;&#31181;&#26415;&#35821;&#21487;&#33021;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#25991;&#26723;&#21644;&#36890;&#20449;O.D.D.&#21327;&#35758;[Grimm&#21644;et al.&#65292;2020&#65292;Amouroux&#31561;&#65292;2010]&#21512;&#24182;&#65292;&#20197;&#20943;&#23569;&#35768;&#22810;&#27169;&#22411;&#24314;&#27169;&#32773;&#30340;&#28304;&#28304;&#19981;&#26029;&#20135;&#29983;&#30340;&#27495;&#20041;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#22797;&#21046;&#12290;&#24050;&#32463;&#20986;&#29256;&#30340;&#20154;&#21475;&#27169;&#22411;&#25991;&#26723;&#65292;&#21333;&#20146;&#27169;&#22411;&#30340;&#22823;&#22823;&#31616;&#21270;&#29256;&#26412;[Gostoli&#21644;Silverman&#65292;2020]&#20316;&#20026;&#24418;&#24335;&#26415;&#35821;&#30340;&#31034;&#20363;&#65292;&#21333;&#29420;&#21457;&#24067;&#22312;[Elsheikh&#65292;2023b]&#20013;&#12290;&#35813;&#27169;&#22411;&#24050;&#34987;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document presents adequate formal terminology for the mathematical specification of a subset of Agent Based Models (ABMs) in the field of Demography. The simulation of the targeted ABMs follows a fixed-step single-clocked pattern. The proposed terminology further improves the model understanding and can act as a stand-alone methodology for the specification and optionally the documentation of a significant set of (demographic) ABMs. Nevertheless, it is imaginable the this terminology probably with further extensions can be merged with the largely-informal widely-used model documentation and communication O.D.D. protocol [Grimm and et al., 2020, Amouroux et al., 2010] to reduce many sources of ambiguity, hindering model replications by other modelers. A published demographic model documentation, largely simplified version of the Lone Parent Model [Gostoli and Silverman, 2020] is separately published in [Elsheikh, 2023b] as illustration for the formal terminology. The model was impl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#33521;&#22269;&#20154;&#21475;&#27169;&#22411;MiniDemographicABM.jl&#65292;&#20197;&#25506;&#32034;&#21644;&#21033;&#29992;Agents.jl&#22312;&#20154;&#21475;&#24314;&#27169;&#24212;&#29992;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.16548</link><description>&lt;p&gt;
MiniDemographicABM.jl&#35268;&#33539;: &#19968;&#20010;&#31616;&#21270;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#33521;&#22269;&#20154;&#21475;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Specification of MiniDemographicABM.jl: A simplified agent-based demographic model of the UK. (arXiv:2307.16548v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#33521;&#22269;&#20154;&#21475;&#27169;&#22411;MiniDemographicABM.jl&#65292;&#20197;&#25506;&#32034;&#21644;&#21033;&#29992;Agents.jl&#22312;&#20154;&#21475;&#24314;&#27169;&#24212;&#29992;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#38750;&#26657;&#20934;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#33521;&#22269;&#20154;&#21475;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;[Gostolil&#21644;Silverman 2020]&#20013;Lone Parent Model&#30340;&#22823;&#24133;&#31616;&#21270;&#29256;&#26412;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#21021;&#22987;&#20154;&#21475;&#20013;&#30340;&#20010;&#20307;&#32463;&#21382;&#20102;&#32769;&#21270;&#12289;&#27515;&#20129;&#12289;&#29983;&#32946;&#12289;&#31163;&#23130;&#21644;&#32467;&#23130;&#31561;&#36807;&#31243;&#65292;&#27169;&#22411;&#20351;&#29992;&#20102;[Elsheikh 2023a]&#20013;&#20171;&#32461;&#30340;&#24418;&#24335;&#26415;&#35821;&#12290;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25506;&#32034;&#21644;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;Agents.jl Julia&#21253;&#22312;&#20154;&#21475;&#27169;&#25311;&#24212;&#29992;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#23454;&#29616;&#36890;&#36807;Julia&#21253;MiniDemographicABM.jl[Elsheikh 2023b]&#25552;&#20379;&#12290;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#27169;&#25311;&#22266;&#23450;&#27493;&#38271;&#25353;&#23567;&#26102;&#12289;&#22825;&#12289;&#21608;&#12289;&#26376;&#29978;&#33267;&#20219;&#24847;&#29992;&#25143;&#23450;&#20041;&#30340;&#26102;&#38047;&#36895;&#29575;&#36827;&#34892;&#29305;&#23450;&#30340;&#27169;&#25311;&#12290;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#20854;&#20182;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26694;&#26550;&#20013;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#36827;&#19968;&#27493;&#35299;&#37322;...
&lt;/p&gt;
&lt;p&gt;
This documentation specifies a simplified non-calibrated demographic agent-based model of the UK, a largely simplified version of the Lone Parent Model presented in [Gostolil and Silverman 2020]. In the presented model, individuals of an initial population are subject to ageing, deaths, births, divorces and marriages throughout a simplified map of towns of the UK. The specification employs the formal terminology presented in [Elsheikh 2023a]. The main purpose of the model is to explore and exploit capabilities of the state-of-the-art Agents.jl Julia package [Datseris2022] in the context of demographic modeling applications. Implementation is provided via the Julia package MiniDemographicABM.jl [Elsheikh 2023b]. A specific simulation is progressed with a user-defined simulation fixed step size on a hourly, daily, weekly, monthly basis or even an arbitrary user-defined clock rate. The model can serve for comparative studies if implemented in other agent-based modelling frameworks and pro
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11224</link><description>&lt;p&gt;
Jina Embeddings:&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11224
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#30001;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#32452;&#25104;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#25991;&#26412;&#36755;&#20837;&#36716;&#21270;&#20026;&#25968;&#20540;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#22312;&#23494;&#38598;&#26816;&#32034;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20174;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#21644;&#19977;&#20803;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#25454;&#28165;&#29702;&#22312;&#25968;&#25454;&#38598;&#20934;&#22791;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#26368;&#21518;&#21033;&#29992;Massive Textual Embedding Benchmark&#65288;MTEB&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20219;&#21153;&#65306;&#35821;&#38899;&#24773;&#24863;&#20998;&#27573;&#65288;SED&#65289;&#65292;&#26088;&#22312;&#21453;&#26144;&#35821;&#38899;&#24773;&#24863;&#30340;&#32454;&#31890;&#24230;&#29305;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#35821;&#38899;&#24773;&#24863;&#25968;&#25454;&#38598; ZED&#65292;&#24182;&#25552;&#20379;&#20102;&#31454;&#20105;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2306.12991</link><description>&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#20998;&#27573;&#65306;&#21738;&#31181;&#24773;&#24863;&#22312;&#20309;&#26102;&#20986;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Diarization: Which Emotion Appears When?. (arXiv:2306.12991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20219;&#21153;&#65306;&#35821;&#38899;&#24773;&#24863;&#20998;&#27573;&#65288;SED&#65289;&#65292;&#26088;&#22312;&#21453;&#26144;&#35821;&#38899;&#24773;&#24863;&#30340;&#32454;&#31890;&#24230;&#29305;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#35821;&#38899;&#24773;&#24863;&#25968;&#25454;&#38598; ZED&#65292;&#24182;&#25552;&#20379;&#20102;&#31454;&#20105;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#36890;&#24120;&#20381;&#36182;&#20110;&#35805;&#35821;&#27700;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35821;&#38899;&#20256;&#36798;&#30340;&#24773;&#24863;&#24212;&#34987;&#35270;&#20026;&#20855;&#26377;&#30830;&#23450;&#26102;&#38388;&#36793;&#30028;&#30340;&#31163;&#25955;&#35821;&#38899;&#20107;&#20214;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#35805;&#35821;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#21453;&#26144;&#35821;&#38899;&#24773;&#24863;&#30340;&#32454;&#31890;&#24230;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65306;&#35821;&#38899;&#24773;&#24863;&#20998;&#27573;&#65288;SED&#65289;&#12290;&#27491;&#22914;&#35828;&#35805;&#20154;&#20998;&#27573;&#22238;&#31572;&#8220;&#35841;&#20309;&#26102;&#35828;&#35805;&#65311;&#8221;&#30340;&#38382;&#39064;&#65292;&#35821;&#38899;&#24773;&#24863;&#20998;&#27573;&#22238;&#31572;&#8220;&#21738;&#31181;&#24773;&#24863;&#20309;&#26102;&#20986;&#29616;&#65311;&#8221;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#24615;&#33021;&#35780;&#20272;&#21644;&#20026;&#30740;&#31350;&#20154;&#21592;&#24314;&#31435;&#19968;&#20010;&#20849;&#21516;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Zaion &#24773;&#24863;&#25968;&#25454;&#38598;&#65288;ZED&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#35821;&#38899;&#24773;&#24863;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22312;&#30495;&#23454;&#29983;&#27963;&#26465;&#20214;&#19979;&#35760;&#24405;&#30340;&#38750;&#28436;&#20986;&#24773;&#24863;&#65292;&#20197;&#21450;&#35805;&#35821;&#20013;&#24773;&#24863;&#29255;&#27573;&#30340;&#25163;&#21160;&#27880;&#37322;&#36793;&#30028;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31454;&#20105;&#22522;&#32447;&#65292;&#24182;&#24320;&#28304;&#20102;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition (SER) typically relies on utterance-level solutions. However, emotions conveyed through speech should be considered as discrete speech events with definite temporal boundaries, rather than attributes of the entire utterance. To reflect the fine-grained nature of speech emotions, we propose a new task: Speech Emotion Diarization (SED). Just as Speaker Diarization answers the question of "Who speaks when?", Speech Emotion Diarization answers the question of "Which emotion appears when?". To facilitate the evaluation of the performance and establish a common benchmark for researchers, we introduce the Zaion Emotion Dataset (ZED), an openly accessible speech emotion dataset that includes non-acted emotions recorded in real-life conditions, along with manually-annotated boundaries of emotion segments within the utterance. We provide competitive baselines and open-source the code and the pre-trained models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09927</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;Transformer&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;Transformers&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#30701;&#35821;&#24207;&#21015;&#30340;&#25552;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#21046;&#23450;&#30456;&#20851;&#30340;&#27599;&#20010;&#20196;&#29260;&#21644;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#24207;&#21015;&#23884;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#36825;&#20351;&#24471;Transformer&#34920;&#29616;&#24471;&#20687;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#35757;&#32451;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#27169;&#20223;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SynGen&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#27861;&#20998;&#26512;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#30340;&#23545;&#40784;&#26469;&#35299;&#20915;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#20307;&#21644;&#35270;&#35273;&#23646;&#24615;&#20043;&#38388;&#38169;&#35823;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08877</link><description>&lt;p&gt;
&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#32465;&#23450;&#65306;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#23545;&#40784;&#22686;&#24378;&#23646;&#24615;&#23545;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment. (arXiv:2306.08877v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SynGen&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#27861;&#20998;&#26512;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#30340;&#23545;&#40784;&#26469;&#35299;&#20915;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#20307;&#21644;&#35270;&#35273;&#23646;&#24615;&#20043;&#38388;&#38169;&#35823;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24120;&#24120;&#22312;&#23454;&#20307;&#21644;&#20854;&#35270;&#35273;&#23646;&#24615;&#20043;&#38388;&#29983;&#25104;&#38169;&#35823;&#30340;&#20851;&#32852;&#12290;&#36825;&#21453;&#26144;&#20102;&#22312;&#25552;&#31034;&#20013;&#30340;&#23454;&#20307;&#21644;&#20462;&#39280;&#31526;&#30340;&#35821;&#35328;&#32465;&#23450;&#20197;&#21450;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30456;&#24212;&#20803;&#32032;&#30340;&#35270;&#35273;&#32465;&#23450;&#20043;&#38388;&#30340;&#26144;&#23556;&#21463;&#21040;&#25439;&#23475;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#31867;&#20284;&#8220;&#19968;&#20010;&#31881;&#33394;&#30340;&#21521;&#26085;&#33909;&#21644;&#19968;&#20010;&#40644;&#33394;&#30340;&#28779;&#28872;&#40479;&#8221;&#30340;&#26597;&#35810;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#20135;&#29983;&#19968;&#24352;&#40644;&#33394;&#30340;&#21521;&#26085;&#33909;&#21644;&#19968;&#21482;&#31881;&#33394;&#30340;&#28779;&#28872;&#40479;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SynGen&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#23545;&#25552;&#31034;&#36827;&#34892;&#21477;&#27861;&#20998;&#26512;&#20197;&#35782;&#21035;&#23454;&#20307;&#21644;&#23427;&#20204;&#30340;&#20462;&#39280;&#31526;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#26032;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#40723;&#21169;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#19982;&#35821;&#35328;&#32465;&#23450;&#30340;&#35821;&#27861;&#19968;&#33268;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#40723;&#21169;&#23454;&#20307;&#21644;&#20854;&#20462;&#39280;&#31526;&#30340;&#27880;&#24847;&#21147;&#22270;&#20043;&#38388;&#26377;&#22823;&#37327;&#30340;&#37325;&#21472;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#23454;&#20307;&#21644;&#20462;&#39280;&#31526;&#35789;&#20043;&#38388;&#30340;&#37325;&#21472;&#24456;&#23567;&#12290;&#25439;&#22833;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#12290;&#23545;&#19977;&#20010;&#25552;&#31034;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#20154;&#24037;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models often generate incorrect associations between entities and their visual attributes. This reflects an impaired mapping between linguistic binding of entities and modifiers in the prompt and visual binding of the corresponding elements in the generated image. As one notable example, a query like "a pink sunflower and a yellow flamingo" may incorrectly produce an image of a yellow sunflower and a pink flamingo. To remedy this issue, we propose SynGen, an approach which first syntactically analyses the prompt to identify entities and their modifiers, and then uses a novel loss function that encourages the cross-attention maps to agree with the linguistic binding reflected by the syntax. Specifically, we encourage large overlap between attention maps of entities and their modifiers, and small overlap with other entities and modifier words. The loss is optimized during inference, without retraining or fine-tuning the model. Human evaluation on three d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#22312;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;BERT&#30340;&#20998;&#31867;&#24615;&#33021;&#19979;&#38477;&#12290;&#25552;&#20986;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16337</link><description>&lt;p&gt;
&#22788;&#29702;BERT&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Handling Realistic Label Noise in BERT Text Classification. (arXiv:2305.16337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#22312;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;BERT&#30340;&#20998;&#31867;&#24615;&#33021;&#19979;&#38477;&#12290;&#25552;&#20986;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#30001;&#20110;&#24265;&#20215;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#65288;&#22914;&#32593;&#32476;&#29228;&#21462;&#25110;&#20247;&#21253;&#65289;&#23548;&#33268;&#30340;&#35757;&#32451;&#26631;&#31614;&#20013;&#30340;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#23545;&#30417;&#30563;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#23475;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#25269;&#28040;&#26377;&#30417;&#30563;&#20998;&#31867;&#20013;&#38543;&#26426;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;BERT&#24050;&#32463;&#23545;&#39640;&#27604;&#29575;&#30340;&#38543;&#26426;&#27880;&#20837;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#30340;&#26631;&#31614;&#22122;&#22768;&#24182;&#19981;&#26159;&#38543;&#26426;&#30340;&#65292;&#32780;&#26159;&#32463;&#24120;&#19982;&#36755;&#20837;&#29305;&#24449;&#25110;&#20854;&#20182;&#27880;&#37322;&#32773;&#29305;&#23450;&#22240;&#32032;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;BERT&#22312;&#38754;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#65306;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#31867;&#22411;&#22122;&#22768;&#30340;&#23384;&#22312;&#26174;&#33879;&#38477;&#20302;&#20102;BERT&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labels noise refers to errors in training labels caused by cheap data annotation methods, such as web scraping or crowd-sourcing, which can be detrimental to the performance of supervised classifiers. Several methods have been proposed to counteract the effect of random label noise in supervised classification, and some studies have shown that BERT is already robust against high rates of randomly injected label noise. However, real label noise is not random; rather, it is often correlated with input features or other annotator-specific factors. In this paper, we evaluate BERT in the presence of two types of realistic label noise: feature-dependent label noise, and synthetic label noise from annotator disagreements. We show that the presence of these types of noise significantly degrades BERT classification performance. To improve robustness, we evaluate different types of ensembles and noise-cleaning methods and compare their effectiveness against label noise across different datasets.
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#22240;&#20026;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#33021;&#20250;&#23548;&#33268;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;</title><link>http://arxiv.org/abs/2305.15425</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Language Model Tokenizers Introduce Unfairness Between Languages. (arXiv:2305.15425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15425
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#22240;&#20026;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#33021;&#20250;&#23548;&#33268;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#65292;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#20026;&#27492;&#36827;&#34892;&#36807;&#35757;&#32451;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#36755;&#20986;&#36136;&#37327;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20998;&#35789;&#38454;&#27573;&#20986;&#29616;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#22788;&#29702;&#24046;&#24322;&#65292;&#29978;&#33267;&#22312;&#27169;&#22411;&#34987;&#35843;&#29992;&#20043;&#21069;&#23601;&#24050;&#32463;&#20986;&#29616;&#20102;&#12290;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#20197;&#26377;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#24046;&#24322;&#21487;&#39640;&#36798;15&#20493;&#12290;&#36825;&#20123;&#24046;&#24322;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;17&#31181;&#20998;&#35789;&#22120;&#20013;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#20351;&#23427;&#20204;&#26159;&#26377;&#24847;&#20026;&#22810;&#35821;&#35328;&#25903;&#25345;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#26576;&#20123;&#35821;&#35328;&#23545;&#30340;&#23383;&#31526;&#32423;&#21644;&#23383;&#33410;&#32423;&#27169;&#22411;&#20063;&#26174;&#31034;&#20986;4&#20493;&#20197;&#19978;&#30340;&#32534;&#30721;&#38271;&#24230;&#24046;&#24322;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent language models have shown impressive multilingual performance, even when not explicitly trained for it. Despite this, concerns have been raised about the quality of their outputs across different languages. In this paper, we show how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked. The same text translated into different languages can have drastically different tokenization lengths, with differences up to 15 times in some cases. These disparities persist across the 17 tokenizers we evaluate, even if they are intentionally trained for multilingual support. Character-level and byte-level models also exhibit over 4 times the difference in the encoding length for some language pairs. This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided as context to the m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2305.15269</link><description>&lt;p&gt;
&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples. (arXiv:2305.15269v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35777;&#26126;&#31354;&#38388;&#30340;&#24222;&#22823;&#65292;&#20219;&#20309;&#20855;&#26377;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#25512;&#29702;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#32473;&#23450;&#25512;&#29702;&#38142;&#26465;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26576;&#20123;&#25277;&#35937;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#26159;&#22312;&#20351;&#29992;&#33707;&#24503;&#26031;&#22374;&#26031;&#25110;&#29305;&#23450;&#22823;&#23567;&#30340;&#35777;&#26126;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#19988;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#20998;&#24067;&#30456;&#21516;&#12290;&#20026;&#20102;&#34913;&#37327;LLM&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#24191;&#27867;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#25512;&#29702;&#26356;&#22797;&#26434;&#35777;&#26126;&#30340;&#33021;&#21147;&#65292;&#26041;&#27861;&#21253;&#25324;&#28145;&#24230;&#27867;&#21270;&#12289;&#23485;&#24230;&#27867;&#21270;&#21644;&#32452;&#21512;&#27867;&#21270;&#12290;&#20026;&#20102;&#20415;&#20110;&#31995;&#32479;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#21644;&#21487;&#32534;&#31243;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#28436;&#32462;&#35268;&#21017;&#21644;&#35777;&#26126;&#22797;&#26434;&#24615;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#21644;&#35757;&#32451;&#30446;&#26631;&#30340;LLMs&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#33021;&#22815;&#25512;&#24191;&#21040;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to c
&lt;/p&gt;</description></item><item><title>SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15255</link><description>&lt;p&gt;
&#24102;&#26377;&#35821;&#38899;&#30340;LM&#65306;&#36229;&#36234;&#35821;&#38899;&#20196;&#29260;&#30340;&#21475;&#35821;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15255
&lt;/p&gt;
&lt;p&gt;
SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SPECTRON&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#25191;&#34892;&#35821;&#38899;&#24310;&#32493;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#25972;&#20010;&#31995;&#32479;&#37117;&#22312;&#39057;&#35889;&#22270;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#22312;&#39057;&#35889;&#22270;&#39046;&#22495;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30456;&#23545;&#20110;&#20351;&#29992;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#30340;&#29616;&#26377;&#32423;&#32852;&#26041;&#27861;&#31616;&#21270;&#20102;&#25105;&#20204;&#30340;&#35821;&#38899;&#24310;&#32493;&#31995;&#32479;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#33719;&#24471;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32593;&#31449;https://michelleramanovich.github.io/spectron/spectron&#19978;&#21487;&#20197;&#25214;&#21040;&#38899;&#39057;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SPECTRON, a novel approach to adapting pre-trained language models (LMs) to perform speech continuation. By leveraging pre-trained speech encoders, our model generates both text and speech outputs with the entire system being trained end-to-end operating directly on spectrograms. Training the entire model in the spectrogram domain simplifies our speech continuation system versus existing cascade methods which use discrete speech representations. We further show our method surpasses existing spoken language models both in semantic content and speaker preservation while also benefiting from the knowledge transferred from pre-existing models. Audio samples can be found in our website https://michelleramanovich.github.io/spectron/spectron
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26694;&#26550;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26426;&#21046;&#35299;&#37322;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#20256;&#36755;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#19968;&#32452;MLP&#27169;&#22359;&#36827;&#34892;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.15054</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis. (arXiv:2305.15054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26694;&#26550;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26426;&#21046;&#35299;&#37322;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#20256;&#36755;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#19968;&#32452;MLP&#27169;&#22359;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#21644;&#23384;&#20648;&#19982;&#31639;&#26415;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#29702;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#21152;&#28145;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#36825;&#19968;&#26041;&#38754;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26694;&#26550;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#38382;&#39064;&#19978;&#36827;&#34892;&#26426;&#21046;&#35299;&#37322;&#12290;&#36890;&#36807;&#23545;&#29305;&#23450;&#27169;&#22411;&#32452;&#20214;&#30340;&#28608;&#27963;&#36827;&#34892;&#24178;&#39044;&#24182;&#27979;&#37327;&#39044;&#27979;&#27010;&#29575;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#29305;&#23450;&#39044;&#27979;&#25152;&#36127;&#36131;&#30340;&#21442;&#25968;&#23376;&#38598;&#12290;&#36825;&#20026;&#25105;&#20204;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#19982;&#31639;&#26415;&#30456;&#20851;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#23558;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20449;&#24687;&#20174;&#20013;&#38388;&#23618;&#20256;&#36755;&#21040;&#26368;&#32456;&#30340;&#20196;&#29260;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#32452;MLP&#27169;&#22359;&#22788;&#29702;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, 
&lt;/p&gt;</description></item><item><title>CAR&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#24120;&#35782;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#27010;&#24565;&#22686;&#24378;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#24120;&#35782;&#38382;&#39064;&#12290;&#23427;&#21033;&#29992;&#27010;&#24565;&#21270;&#35821;&#20041;&#22270;&#25193;&#23637;&#20102;&#38382;&#39064;&#30340;&#35821;&#20041;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#24120;&#35782;&#38382;&#39064;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14869</link><description>&lt;p&gt;
CAR: &#29992;&#20110;&#38646;&#26679;&#26412;&#24120;&#35782;&#38382;&#31572;&#30340;&#27010;&#24565;&#22686;&#24378;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering. (arXiv:2305.14869v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14869
&lt;/p&gt;
&lt;p&gt;
CAR&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#24120;&#35782;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#27010;&#24565;&#22686;&#24378;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#24120;&#35782;&#38382;&#39064;&#12290;&#23427;&#21033;&#29992;&#27010;&#24565;&#21270;&#35821;&#20041;&#22270;&#25193;&#23637;&#20102;&#38382;&#39064;&#30340;&#35821;&#20041;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#24120;&#35782;&#38382;&#39064;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#24120;&#35782;&#38382;&#31572;&#30340;&#20219;&#21153;&#35780;&#20272;&#27169;&#22411;&#22312;&#33021;&#22815;&#25512;&#29702;&#36229;&#20986;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#19968;&#33324;&#24773;&#26223;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#21512;&#25104;&#30340;&#20174;&#24120;&#35782;&#30693;&#35782;&#24211;&#65288;CSKBs&#65289;&#26500;&#24314;&#30340;QA&#23545;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;&#20174;CSKBs&#20013;&#38543;&#26426;&#37319;&#26679;&#26469;&#26500;&#36896;&#36127;&#20363;&#65288;&#24178;&#25200;&#39033;&#65289;&#30340;&#20851;&#38190;&#35789;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#29942;&#39048;&#65306;CSKB&#30340;&#26412;&#36136;&#19981;&#23436;&#25972;&#38480;&#21046;&#20102;&#21512;&#25104;QA&#23545;&#30340;&#35821;&#20041;&#35206;&#30422;&#33539;&#22260;&#65292;&#32570;&#20047;&#20154;&#24037;&#27880;&#37322;&#20351;&#24471;&#37319;&#26679;&#30340;&#36127;&#20363;&#21487;&#33021;&#26080;&#20449;&#24687;&#24615;&#19988;&#30456;&#20114;&#30683;&#30462;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#24565;&#22686;&#24378;&#25512;&#29702;&#22120;&#65288;CAR&#65289;&#65292;&#19968;&#20010;&#20805;&#20998;&#21033;&#29992;&#27010;&#24565;&#21270;&#33021;&#21147;&#30340;&#38646;&#26679;&#26412;&#24120;&#35782;&#38382;&#31572;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CAR&#25552;&#21462;&#20102;&#24120;&#35782;&#30693;&#35782;&#19977;&#20803;&#32452;&#65292;&#24182;&#36890;&#36807;&#27010;&#24565;&#21270;&#35821;&#20041;&#22270;&#29983;&#25104;QA&#23545;&#65292;&#20197;&#25193;&#23637;&#20174;CSKBs&#20013;&#36873;&#25321;&#24178;&#25200;&#39033;&#30340;&#33539;&#22260;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#23545;&#24120;&#35782;&#38382;&#39064;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of zero-shot commonsense question answering evaluates models on their capacity to reason about general scenarios beyond those presented in specific datasets. Existing approaches for tackling this task leverage external knowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model on synthetic QA pairs constructed from CSKBs. In these approaches, negative examples (distractors) are formulated by randomly sampling from CSKBs using fairly primitive keyword constraints. However, two bottlenecks limit these approaches: the inherent incompleteness of CSKBs limits the semantic coverage of synthetic QA pairs, and the lack of human annotations makes the sampled negative examples potentially uninformative and contradictory. To tackle these limitations above, we propose Conceptualization-Augmented Reasoner (CAR), a zero-shot commonsense question-answering framework that fully leverages the power of conceptualization. Specifically, CAR abstracts a commonsense knowledge tripl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14600</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23398;&#20064;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#30456;&#20132;&#26631;&#31614;&#38598;&#20043;&#38388;&#30340;&#20860;&#23481;&#32467;&#26500;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#20855;&#20307;&#22320;&#65292;&#26631;&#35760;&#20855;&#26377;&#20004;&#20010;&#35282;&#33394;&#24207;&#21015;&#30340;&#21477;&#23376;&#65306;VerbNet&#21442;&#25968;&#21644;PropBank&#21442;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#36328;&#20219;&#21153;&#20132;&#20114;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#20173;&#28982;&#26159;&#20998;&#21035;&#35299;&#30721;&#30340;&#65292;&#23384;&#22312;&#29983;&#25104;&#32467;&#26500;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#24207;&#21015; (&#22312;&#20687;SEMLINK&#30340;&#35789;&#20856;&#20013;)&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#32622;&#65292;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#12290;&#36890;&#36807;&#36825;&#20010;&#35774;&#32622;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;SEMLINK&#32422;&#26463;&#19981;&#26029;&#25552;&#39640;&#24635;F1&#20540;&#12290;&#36890;&#36807;&#29305;&#27530;&#30340;&#36755;&#20837;&#26500;&#36896;&#65292;&#25105;&#20204;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#20197;&#36229;&#36807;99%&#30340;&#20934;&#30830;&#24615;&#20174;PropBank&#21442;&#25968;&#20013;&#25512;&#26029;&#20986;VerbNet&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;co
&lt;/p&gt;
&lt;p&gt;
This paper addresses the question of how to efficiently learn from disjoint, compatible label sequences. We argue that the compatible structures between disjoint label sets help model learning and inference. We verify this hypothesis on the task of semantic role labeling (SRL), specifically, tagging a sentence with two role sequences: VerbNet arguments and PropBank arguments. Prior work has shown that cross-task interaction improves performance. However, the two tasks are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like SEMLINK). To eliminate this issue, we first propose a simple and effective setup that jointly handles VerbNet and PropBank labels as one sequence. With this setup, we show that enforcing SEMLINK constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from PropBank arguments with over 99% accuracy. We also propose a co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26597;&#35810;&#37325;&#20889;-&#26816;&#32034;-&#38405;&#35835;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#20851;&#27880;&#25628;&#32034;&#26597;&#35810;&#26412;&#36523;&#30340;&#36866;&#24212;&#24615;&#65292;&#37319;&#29992;&#21487;&#35757;&#32451;&#30340;&#37325;&#20889;&#22120;&#26469;&#26356;&#22909;&#22320;&#23545;&#40784;&#26597;&#35810;&#19982;&#20923;&#32467;&#27169;&#22359;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.14283</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Query Rewriting for Retrieval-Augmented Large Language Models. (arXiv:2305.14283v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14283
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26597;&#35810;&#37325;&#20889;-&#26816;&#32034;-&#38405;&#35835;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#20851;&#27880;&#25628;&#32034;&#26597;&#35810;&#26412;&#36523;&#30340;&#36866;&#24212;&#24615;&#65292;&#37319;&#29992;&#21487;&#35757;&#32451;&#30340;&#37325;&#20889;&#22120;&#26469;&#26356;&#22909;&#22320;&#23545;&#40784;&#26597;&#35810;&#19982;&#20923;&#32467;&#27169;&#22359;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#32034;&#21518;&#38405;&#35835;&#27969;&#31243;&#20013;&#20805;&#24403;&#24378;&#22823;&#30340;&#40657;&#30418;&#35835;&#32773;&#65292;&#22312;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#20174;&#26597;&#35810;&#37325;&#20889;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#37325;&#26032;&#20889;&#20837;-&#26816;&#32034;-&#38405;&#35835;&#65292;&#32780;&#19981;&#26159;&#20043;&#21069;&#30340;&#26816;&#32034;&#21518;&#38405;&#35835;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;LLMs&#12290;&#19982;&#20197;&#24448;&#20391;&#37325;&#20110;&#35843;&#25972;&#26816;&#32034;&#22120;&#25110;&#38405;&#35835;&#22120;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20851;&#27880;&#30340;&#26159;&#25628;&#32034;&#26597;&#35810;&#26412;&#36523;&#30340;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#36755;&#20837;&#25991;&#26412;&#19982;&#26816;&#32034;&#25152;&#38656;&#30693;&#35782;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#23384;&#22312;&#24046;&#36317;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#31034;LLM&#29983;&#25104;&#26597;&#35810;&#65292;&#28982;&#21518;&#20351;&#29992;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#20351;&#26597;&#35810;&#19982;&#20923;&#32467;&#27169;&#22359;&#23545;&#40784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#26041;&#26696;&#12290;&#37319;&#29992;&#19968;&#20010;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#35757;&#32451;&#30340;&#37325;&#20889;&#22120;&#65292;&#20197;&#36866;&#24212;&#40657;&#30418;LLM&#38405;&#35835;&#22120;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;LLM&#38405;&#35835;&#22120;&#30340;&#21453;&#39304;&#35757;&#32451;&#37325;&#20889;&#22120;&#12290;&#22312;Do&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on do
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#37325;&#26032;&#26500;&#26550;&#20026;&#32534;&#31243;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#33021;&#22815;&#35299;&#20915;&#26102;&#25928;&#24615;&#38382;&#39064;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2305.14221</link><description>&lt;p&gt;
&#23558;&#38382;&#39064;&#22238;&#31572;&#20316;&#20026;&#35299;&#20915;&#26102;&#25928;&#38382;&#39064;&#30340;&#32534;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Question Answering as Programming for Solving Time-Sensitive Questions. (arXiv:2305.14221v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#37325;&#26032;&#26500;&#26550;&#20026;&#32534;&#31243;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#33021;&#22815;&#35299;&#20915;&#26102;&#25928;&#24615;&#38382;&#39064;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#22312;&#20154;&#31867;&#26085;&#24120;&#29983;&#27963;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#25105;&#20204;&#23545;&#19990;&#30028;&#30693;&#35782;&#30340;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#30340;&#21160;&#24577;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#29305;&#24615;&#65292;&#24403;&#38382;&#39064;&#30340;&#26102;&#38388;&#38480;&#21046;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#31572;&#26696;&#21487;&#33021;&#23436;&#20840;&#19981;&#21516;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26174;&#31034;&#20986;&#22312;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#30340;&#26174;&#33879;&#26234;&#33021;&#65292;&#32780;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#19978;&#36848;&#38382;&#39064;&#20173;&#28982;&#23545;&#29616;&#26377;LLM&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;LLM&#26080;&#27861;&#22522;&#20110;&#34920;&#38754;&#32423;&#25991;&#26412;&#35821;&#20041;&#36827;&#34892;&#20005;&#26684;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#19981;&#26159;&#35201;&#27714;LLM&#30452;&#25509;&#22238;&#31572;&#38382;&#39064;&#65292;&#32780;&#26159;&#23558;&#8220;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#8221;&#37325;&#26032;&#26500;&#26550;&#20026;&#8220;&#32534;&#31243;&#20219;&#21153;&#8221;&#65288;QAaP&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;LLM&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#25105;&#20204;&#21162;&#21147; harness LLM models to craft programs that can solve time-sensitive questions.
&lt;/p&gt;
&lt;p&gt;
Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the $\textbf{Q}$uestion $\textbf{A}$nswering task $\textbf{a}$s $\textbf{P}$rogramming ($\textbf{QAaP}$). Concretely, by leveraging modern LLMs' superior capability in understanding both natural language and programming language, we endeavor to harne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#40657;&#30418;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13785</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;&#40657;&#30418;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation. (arXiv:2305.13785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#40657;&#30418;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25110;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22914; GPT-3 &#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#25512;&#21160;&#20102;&#26368;&#36817;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#21162;&#21147;&#12290;&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20248;&#21270;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382; LLM &#30340;&#26799;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#40657;&#30418;&#27169;&#22411;&#35270;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#20351;&#29992;&#22686;&#24378;&#30340;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#25968;&#25454;&#22686;&#24378;&#26159;&#36890;&#36807;&#22312;&#19968;&#20010;&#27604;&#40657;&#30418;&#27169;&#22411;&#21442;&#25968;&#35268;&#27169;&#23567;&#24471;&#22810;&#30340;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26469;&#23436;&#25104;&#30340;&#12290;&#36890;&#36807;&#23545;&#20843;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026; BT-Classifier&#65289;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#40657;&#30418;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#24182;&#19982;&#20381;&#36182;&#20110;&#20840;&#27169;&#22411;&#35843;&#25972;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training or finetuning large-scale language models (LLMs) such as GPT-3 requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One practical area of research is to treat these models as black boxes and interact with them through their inference APIs. In this paper, we investigate how to optimize few-shot text classification without accessing the gradients of the LLMs. To achieve this, we treat the black-box model as a feature extractor and train a classifier with the augmented text data. Data augmentation is performed using prompt-based finetuning on an auxiliary language model with a much smaller parameter size than the black-box model. Through extensive experiments on eight text classification datasets, we show that our approach, dubbed BT-Classifier, significantly outperforms state-of-the-art black-box few-shot learners and performs on par with methods that rely on full-model tuning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21483;PromptClass&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#22686;&#24378;&#23398;&#20064;&#65292;&#29983;&#25104;&#22122;&#22768;&#40065;&#26834;&#24615;&#26356;&#24378;&#30340;&#20266;&#26631;&#31614;&#21644;&#33258;&#25105;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.13723</link><description>&lt;p&gt;
PromptClass: &#21033;&#29992;&#25552;&#31034;&#22686;&#24378;&#22122;&#22768;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#65292;&#36827;&#34892;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
PromptClass: Weakly-Supervised Text Classification with Prompting Enhanced Noise-Robust Self-Training. (arXiv:2305.13723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13723
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21483;PromptClass&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#22686;&#24378;&#23398;&#20064;&#65292;&#29983;&#25104;&#22122;&#22768;&#40065;&#26834;&#24615;&#26356;&#24378;&#30340;&#20266;&#26631;&#31614;&#21644;&#33258;&#25105;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#27599;&#20010;&#30446;&#26631;&#31867;&#21035;&#30340;&#26631;&#31614;&#21517;&#20316;&#20026;&#21807;&#19968;&#30340;&#30417;&#30563;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20110;&#23436;&#20840;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#27169;&#22411;&#33021;&#22823;&#22823;&#20943;&#23569;&#20154;&#31867;&#27880;&#37322;&#30340;&#24037;&#20316;&#37327;&#65292;&#22240;&#27492;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#26631;&#31614;&#21517;&#26469;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21516;&#19968;&#21333;&#35789;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#20250;&#26377;&#19981;&#21516;&#30340;&#21547;&#20041;&#65292;&#22240;&#27492;&#20165;&#20165;&#20351;&#29992;&#26631;&#31614;&#21517;&#36827;&#34892;&#21305;&#37197;&#20250;&#23548;&#33268;&#38750;&#24120;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#22312;&#20266;&#26631;&#31614;&#29983;&#25104;&#38454;&#27573;&#20135;&#29983;&#30340;&#38169;&#35823;&#23558;&#30452;&#25509;&#20256;&#25773;&#21040;&#20998;&#31867;&#22120;&#35757;&#32451;&#38454;&#27573;&#65292;&#26080;&#27861;&#34987;&#32416;&#27491;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;PromptClass&#65292;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;(1)&#20266;&#26631;&#31614;&#33719;&#21462;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Recently proposed weakly-supervised text classification settings train a classifier using the label name of each target class as the only supervision. Such weakly-supervised settings have been gaining increasing attention since they can largely reduce human annotation efforts compared to fully-supervised and semi-supervised settings. Most existing methods follow the strategy that first uses the label names as static features to generate pseudo labels, which are then used for classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) words can have different meanings in different contexts, so using label names for context-free matching can induce very noisy pseudo labels; and (2) the errors made in the pseudo label generation stage will directly propagate to the classifier training stage without a chance of being corrected. In this paper, we propose a new method, PromptClass, consisting of two modules: (1) a pseudo label acquisition module
&lt;/p&gt;</description></item><item><title>GDP-Zero&#26159;&#19968;&#31181;&#20351;&#29992;Open-Loop MCTS&#36827;&#34892;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#20808;&#39564;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;&#29992;&#25143;&#27169;&#25311;&#22120;&#21644;&#31995;&#32479;&#27169;&#22411;&#65292;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#20248;&#20110;ChatGPT&#12290;</title><link>http://arxiv.org/abs/2305.13660</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;Monte-Carlo&#26641;&#25628;&#32034;&#29992;&#20110;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning. (arXiv:2305.13660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13660
&lt;/p&gt;
&lt;p&gt;
GDP-Zero&#26159;&#19968;&#31181;&#20351;&#29992;Open-Loop MCTS&#36827;&#34892;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#20808;&#39564;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;&#29992;&#25143;&#27169;&#25311;&#22120;&#21644;&#31995;&#32479;&#27169;&#22411;&#65292;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#20248;&#20110;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#35268;&#21010;&#36890;&#24120;&#38656;&#35201;&#27169;&#25311;&#26410;&#26469;&#30340;&#23545;&#35805;&#20132;&#20114;&#24182;&#20272;&#35745;&#20219;&#21153;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26041;&#27861;&#32771;&#34385;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#25191;&#34892;&#21069;&#30651;&#25628;&#32034;&#31639;&#27861;&#65292;&#20363;&#22914;A *&#25628;&#32034;&#21644;Monte Carlo Tree Search&#65288;MCTS&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35757;&#32451;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#24403;&#38754;&#20020;&#22024;&#26434;&#30340;&#27880;&#37322;&#25110;&#20302;&#36164;&#28304;&#35774;&#32622;&#26102;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;GDP-Zero&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;Open-Loop MCTS&#36827;&#34892;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;GDP-Zero&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26641;&#25628;&#32034;&#26399;&#38388;&#20805;&#24403;&#31574;&#30053;&#20808;&#39564;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;&#29992;&#25143;&#27169;&#25311;&#22120;&#21644;&#31995;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;PersuasionForGood&#19978;&#35780;&#20272;&#20102;GDP-Zero&#65292;&#24182;&#21457;&#29616;&#20854;&#21709;&#24212;&#27604;ChatGPT&#26356;&#21463;&#27426;&#36814;&#65292;&#36798;&#21040;&#20102;59.32&#65285;&#65292;&#22312;&#20132;&#20114;&#35780;&#20272;&#26399;&#38388;&#27604;ChatGPT&#26356;&#26377;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A* search and Monte Carlo Tree Search (MCTS). However, this training often require abundant annotated data, which creates challenges when faced with noisy annotations or low-resource settings. We introduce GDP-Zero, an approach using Open-Loop MCTS to perform goal-oriented dialogue policy planning without any model training. GDP-Zero prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search. We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that its responses are preferred over ChatGPT up to 59.32% of the time, and are rated more persuasive than ChatGPT during interactive evaluations.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#33539;&#20363;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#23558;&#35270;&#35273;&#30693;&#35782;&#26126;&#30830;&#20998;&#31867;&#20026;&#26356;&#32454;&#31890;&#24230;&#26469;&#22686;&#24378;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20174;&#20114;&#32852;&#32593;&#25110;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#35270;&#35273;&#20449;&#24687;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ReSee&#26694;&#26550;&#65292;&#21487;&#23558;&#35270;&#35273;&#34920;&#31034;&#28155;&#21152;&#21040;&#21407;&#22987;&#23545;&#35805;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.13602</link><description>&lt;p&gt;
ReSee&#65306;&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#36890;&#36807;&#35270;&#35273;&#30693;&#35782;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue. (arXiv:2305.13602v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#33539;&#20363;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#23558;&#35270;&#35273;&#30693;&#35782;&#26126;&#30830;&#20998;&#31867;&#20026;&#26356;&#32454;&#31890;&#24230;&#26469;&#22686;&#24378;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20174;&#20114;&#32852;&#32593;&#25110;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#35270;&#35273;&#20449;&#24687;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ReSee&#26694;&#26550;&#65292;&#21487;&#23558;&#35270;&#35273;&#34920;&#31034;&#28155;&#21152;&#21040;&#21407;&#22987;&#23545;&#35805;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35270;&#35273;&#30693;&#35782;&#19982;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#30456;&#32467;&#21512;&#25104;&#20026;&#19968;&#31181;&#27169;&#20223;&#20154;&#31867;&#24605;&#32771;&#12289;&#24819;&#35937;&#21644;&#20132;&#27969;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#25110;&#32773;&#21463;&#21040;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#36136;&#37327;&#30340;&#38480;&#21046;&#65292;&#25110;&#32773;&#21463;&#21040;&#35270;&#35273;&#30693;&#35782;&#27010;&#24565;&#30340;&#31895;&#31961;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#26032;&#33539;&#20363;&#21450;&#20854;&#30456;&#20851;&#25968;&#25454;&#38598;&#65288;ReSee-WoW&#12289;ReSee-DD&#65289;&#12290;&#25105;&#20204;&#25552;&#35758;&#23558;&#35270;&#35273;&#30693;&#35782;&#26126;&#30830;&#20998;&#20026;&#26356;&#32454;&#31890;&#24230;&#65288;&#8220;&#36716;&#21521;&#32423;&#8221;&#21644;&#8220;&#23454;&#20307;&#32423;&#8221;&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#35270;&#35273;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#20174;&#20114;&#32852;&#32593;&#25110;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#23427;&#20204;&#12290;&#20026;&#20102;&#23637;&#31034;&#25552;&#20379;&#30340;&#35270;&#35273;&#30693;&#35782;&#30340;&#20248;&#36234;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;ReSee&#65292;&#36890;&#36807;&#27169;&#24577;&#36830;&#25509;&#23558;&#35270;&#35273;&#34920;&#31034;&#28155;&#21152;&#21040;&#21407;&#22987;&#23545;&#35805;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating visual knowledge into text-only dialogue systems has become a potential direction to imitate the way humans think, imagine, and communicate. However, existing multimodal dialogue systems are either confined by the scale and quality of available datasets or the coarse concept of visual knowledge. To address these issues, we provide a new paradigm of constructing multimodal dialogues as well as two datasets extended from text-only dialogues under such paradigm (ReSee-WoW, ReSee-DD). We propose to explicitly split the visual knowledge into finer granularity (``turn-level'' and ``entity-level''). To further boost the accuracy and diversity of augmented visual information, we retrieve them from the Internet or a large image dataset. To demonstrate the superiority and universality of the provided visual knowledge, we propose a simple but effective framework ReSee to add visual representation into vanilla dialogue models by modality concatenations. We also conduct extensive expe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;BioDEX&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#36164;&#28304;&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21462;&#65292;&#21487;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#25913;&#36827;&#33647;&#29289;&#23433;&#20840;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.13395</link><description>&lt;p&gt;
BioDEX&#65306;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#33647;&#29289;&#30417;&#27979;&#30340;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
BioDEX: Large-Scale Biomedical Adverse Drug Event Extraction for Real-World Pharmacovigilance. (arXiv:2305.13395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;BioDEX&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#36164;&#28304;&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21462;&#65292;&#21487;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#25913;&#36827;&#33647;&#29289;&#23433;&#20840;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#20934;&#30830;&#22320;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;(Adverse Drug Events, ADE)&#23545;&#20844;&#20247;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#28041;&#21450;&#32531;&#24930;&#21644;&#26114;&#36149;&#30340;&#20154;&#24037;&#21171;&#21160;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#26469;&#25913;&#36827;&#33647;&#29289;&#23433;&#20840;&#30417;&#27979;(&#33647;&#29289;&#30417;&#31649;&#23398;, PV)&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;BioDEX&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#36164;&#28304;&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21462;&#65292;&#22522;&#20110;&#32654;&#22269;&#33647;&#29289;&#23433;&#20840;&#25253;&#21578;&#30340;&#21382;&#21490;&#36755;&#20986;&#12290;BioDEX&#21253;&#25324;65k&#20010;&#25688;&#35201;&#21644;19k&#20010;&#20840;&#25991;&#29983;&#29289;&#21307;&#23398;&#35770;&#25991;&#65292;&#20197;&#21450;&#30001;&#21307;&#23398;&#19987;&#23478;&#21019;&#24314;&#30340;256k&#20010;&#30456;&#20851;&#30340;&#25991;&#26723;&#32423;&#23433;&#20840;&#25253;&#21578;&#12290;&#36825;&#20123;&#25253;&#21578;&#30340;&#26680;&#24515;&#29305;&#24449;&#21253;&#25324;&#24739;&#32773;&#30340;&#20307;&#37325;&#12289;&#24180;&#40836;&#21644;&#29983;&#29289;&#24615;&#21035;&#65292;&#24739;&#32773;&#26381;&#29992;&#30340;&#19968;&#32452;&#33647;&#29289;&#12289;&#33647;&#29289;&#21058;&#37327;&#12289;&#32463;&#21382;&#30340;&#21453;&#24212;&#20197;&#21450;&#21453;&#24212;&#26159;&#21542;&#21361;&#21450;&#29983;&#21629;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26681;&#25454;&#36215;&#22987;&#35770;&#25991;&#39044;&#27979;&#25253;&#21578;&#30340;&#26680;&#24515;&#20449;&#24687;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20272;&#35745;&#20154;&#31867;&#30340;&#34920;&#29616;&#20026;72.0% F1&#65292;&#32780;&#25105;&#20204;&#26368;&#22909;&#30340;m.....
&lt;/p&gt;
&lt;p&gt;
Timely and accurate extraction of Adverse Drug Events (ADE) from biomedical literature is paramount for public safety, but involves slow and costly manual labor. We set out to improve drug safety monitoring (pharmacovigilance, PV) through the use of Natural Language Processing (NLP). We introduce BioDEX, a large-scale resource for Biomedical adverse Drug Event Extraction, rooted in the historical output of drug safety reporting in the U.S. BioDEX consists of 65k abstracts and 19k full-text biomedical papers with 256k associated document-level safety reports created by medical experts. The core features of these reports include the reported weight, age, and biological sex of a patient, a set of drugs taken by the patient, the drug dosages, the reactions experienced, and whether the reaction was life threatening. In this work, we consider the task of predicting the core information of the report given its originating paper. We estimate human performance to be 72.0% F1, whereas our best m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20316;&#20026;&#27880;&#37322;&#22120;&#20197;&#20415;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#35299;&#37322;&#20998;&#26512;&#65292;&#21457;&#29616;ChatGPT&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#35821;&#20041;&#26356;&#20016;&#23500;&#30340;&#27880;&#37322;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;GPT&#27880;&#37322;&#30340;&#35299;&#37322;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.13386</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#21487;&#20197;&#20419;&#36827;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs facilitate interpretation of pre-trained language models?. (arXiv:2305.13386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13386
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20316;&#20026;&#27880;&#37322;&#22120;&#20197;&#20415;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#35299;&#37322;&#20998;&#26512;&#65292;&#21457;&#29616;ChatGPT&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#35821;&#20041;&#26356;&#20016;&#23500;&#30340;&#27880;&#37322;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;GPT&#27880;&#37322;&#30340;&#35299;&#37322;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#24102;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#25110;&#20154;&#22312;&#29615;&#36335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21487;&#20280;&#32553;&#24615;&#21644;&#35299;&#37322;&#33539;&#22260;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20316;&#20026;&#27880;&#37322;&#22120;&#65292;&#20197;&#20415;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#35299;&#37322;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#34920;&#31034;&#19978;&#24212;&#29992;&#20998;&#23618;&#32858;&#31867;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27010;&#24565;&#65292;&#28982;&#21518;&#20351;&#29992;GPT&#27880;&#37322;&#23545;&#36825;&#20123;&#27010;&#24565;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#20154;&#24037;&#27880;&#37322;&#30340;&#27010;&#24565;&#30456;&#27604;&#65292;ChatGPT&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#35821;&#20041;&#26356;&#20016;&#23500;&#30340;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;GPT&#27880;&#37322;&#30340;&#35299;&#37322;&#20998;&#26512;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#65306;&#25506;&#38024;&#26694;&#26550;&#21644;&#31070;&#32463;&#20803;&#35299;&#37322;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#27010;&#24565;&#32593;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Work done to uncover the knowledge encoded within pre-trained language models, rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying hierarchical clustering over contextualized representations and then annotate these concepts using GPT annotations. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing framework and neuron interpretation. To facilitate further exploration and experimentation in this field, we have made available a substantial ConceptNet dataset compris
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#35782;&#21035;&#35821;&#20041;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#19977;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#21644;&#21477;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#31283;&#20581;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.13303</link><description>&lt;p&gt;
&#12298;&#20851;&#20110;&#30456;&#20851;&#25991;&#26723;&#20013;&#35821;&#20041;&#24046;&#24322;&#30340;&#26080;&#30417;&#30563;&#35782;&#21035;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Unsupervised Recognition of Semantic Differences in Related Documents. (arXiv:2305.13303v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#35782;&#21035;&#35821;&#20041;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#19977;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#21644;&#21477;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#31283;&#20581;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31361;&#20986;&#23548;&#33268;&#20004;&#20010;&#25991;&#26723;&#20043;&#38388;&#35821;&#20041;&#24046;&#24322;&#30340;&#21333;&#35789;&#21487;&#33021;&#23545;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#26377;&#29992;&#12290;&#25105;&#20204;&#23558;&#35821;&#20041;&#24046;&#24322;&#35782;&#21035;&#65288;RSD&#65289;&#23450;&#20041;&#20026;&#19968;&#20010;&#26631;&#35760;&#32423;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#19977;&#20010;&#20381;&#36182;&#20110;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#22522;&#26412;&#30340;&#33521;&#35821;&#21477;&#23376;&#24320;&#22987;&#65292;&#24182;&#36880;&#28176;&#36716;&#21521;&#26356;&#22797;&#26434;&#30340;&#36328;&#35821;&#35328;&#25991;&#26723;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#21644;&#21477;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#31283;&#20581;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#21487;&#20197;&#22312;https://github.com/ZurichNLP/recognizing-semantic-differences&#25214;&#21040;&#37325;&#29616;&#25105;&#20204;&#23454;&#39564;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically highlighting words that cause semantic differences between two documents could be useful for a wide range of applications. We formulate recognizing semantic differences (RSD) as a token-level regression task and study three unsupervised approaches that rely on a masked language model. To assess the approaches, we begin with basic English sentences and gradually move to more complex, cross-lingual document pairs. Our results show that an approach based on word alignment and sentence-level contrastive learning has a robust correlation to gold labels. However, all unsupervised approaches still leave a large margin of improvement. Code to reproduce our experiments is available at https://github.com/ZurichNLP/recognizing-semantic-differences
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22788;&#29702;dropout&#22122;&#22768;&#21644;&#35299;&#20915;&#29305;&#24449;&#30772;&#22351;&#38382;&#39064;&#20004;&#20010;&#35282;&#24230;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#29992;&#19988;&#26377;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.13192</link><description>&lt;p&gt;
SimCSE++&#65306;&#20174;&#20004;&#20010;&#35282;&#24230;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives. (arXiv:2305.13192v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22788;&#29702;dropout&#22122;&#22768;&#21644;&#35299;&#20915;&#29305;&#24449;&#30772;&#22351;&#38382;&#39064;&#20004;&#20010;&#35282;&#24230;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#29992;&#19988;&#26377;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20004;&#20010;&#35282;&#24230;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;&#65306;&#22788;&#29702;dropout&#22122;&#22768;&#21644;&#35299;&#20915;&#29305;&#24449;&#30772;&#22351;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#31532;&#19968;&#20010;&#35282;&#24230;&#65292;&#25105;&#20204;&#21457;&#29616;&#26469;&#33258;&#36127;&#26679;&#26412;&#30340;dropout&#22122;&#22768;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#24403;&#21069;&#35299;&#20915;&#26041;&#27861;&#20013;&#29305;&#24449;&#30772;&#22351;&#30340;&#25490;&#21517;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;-wise&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20004;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#26631;&#20934;&#22522;&#20934;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20004;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#30456;&#23545;&#20110;&#20197;BERT base&#37197;&#32622;&#30340;&#24378;&#22522;&#32447;SimCSE&#65292;&#21487;&#20197;&#33719;&#24471;1.8&#20010;&#28857;&#30340;&#22686;&#30410;&#12290;&#27492;&#22806;&#65292;&#23558;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#24378;&#23545;&#27604;&#23398;&#20064;&#22522;&#32447;DiffCSE&#65292;&#21487;&#20197;&#33719;&#24471;1.4&#20010;&#28857;&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper improves contrastive learning for sentence embeddings from two perspectives: handling dropout noise and addressing feature corruption. Specifically, for the first perspective, we identify that the dropout noise from negative pairs affects the model's performance. Therefore, we propose a simple yet effective method to deal with such type of noise. Secondly, we pinpoint the rank bottleneck of current solutions to feature corruption and propose a dimension-wise contrastive learning objective to address this issue. Both proposed methods are generic and can be applied to any contrastive learning based models for sentence embeddings. Experimental results on standard benchmarks demonstrate that combining both proposed methods leads to a gain of 1.8 points compared to the strong baseline SimCSE configured with BERT base. Furthermore, applying the proposed method to DiffCSE, another strong contrastive learning based baseline, results in a gain of 1.4 points.
&lt;/p&gt;</description></item><item><title>SCITAB&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#21644;&#30456;&#20851;&#30340;&#31185;&#23398;&#34920;&#26684;&#65292;&#35201;&#27714;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;SCITAB&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#27495;&#20041;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;&#38500;&#20102;GPT-4&#20043;&#22806;&#65292;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#25552;&#31034;&#25216;&#26415;&#22914;&#24605;&#32500;&#38142;&#23545;&#20110;&#22312;SCITAB&#19978;&#25552;&#21319;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13186</link><description>&lt;p&gt;
SCITAB: &#19968;&#20010;&#23545;&#31185;&#23398;&#34920;&#26684;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables. (arXiv:2305.13186v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13186
&lt;/p&gt;
&lt;p&gt;
SCITAB&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#21644;&#30456;&#20851;&#30340;&#31185;&#23398;&#34920;&#26684;&#65292;&#35201;&#27714;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;SCITAB&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#27495;&#20041;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;&#38500;&#20102;GPT-4&#20043;&#22806;&#65292;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#25552;&#31034;&#25216;&#26415;&#22914;&#24605;&#32500;&#38142;&#23545;&#20110;&#22312;SCITAB&#19978;&#25552;&#21319;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#31185;&#23398;&#20107;&#23454;&#26680;&#26597;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#19968;&#20123;&#19981;&#36275;&#65292;&#20363;&#22914;&#26469;&#33258;&#20247;&#21253;&#23457;&#26597;&#30340;&#20559;&#35265;&#21644;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#35777;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SCITAB&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#65292;&#36825;&#20123;&#20107;&#23454;1&#65289;&#26469;&#28304;&#20110;&#30495;&#23454;&#30340;&#31185;&#23398;&#20986;&#29256;&#29289;&#65292;2&#65289;&#38656;&#35201;&#32452;&#21512;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#12290;&#36825;&#20123;&#20107;&#23454;&#19982;&#21253;&#21547;&#35777;&#25454;&#30340;&#31185;&#23398;&#34920;&#26684;&#36827;&#34892;&#37197;&#23545;&#65292;&#24182;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;SCITAB&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#34920;&#26684;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38500;&#20102;GPT-4&#22806;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#27969;&#34892;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#23545;SCITAB&#30340;&#24615;&#33021;&#25552;&#21319;&#19981;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SCITAB&#25552;&#20986;&#30340;&#20960;&#20010;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#30340;&#27495;&#20041;&#24615;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#36866;&#21512;&#20316;&#20026;&#25277;&#35937;&#25688;&#35201;&#35780;&#20272;&#22120;&#30340;&#20154;&#31867;&#26367;&#20195;&#21697;&#65292;&#30001;&#20110;&#23384;&#22312;&#38480;&#21046;&#65292;&#23427;&#20204;&#22312;&#35780;&#20272;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.13091</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#20154;&#31867;&#32423;&#21035;&#30340;&#25277;&#35937;&#25688;&#35201;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization. (arXiv:2305.13091v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13091
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#36866;&#21512;&#20316;&#20026;&#25277;&#35937;&#25688;&#35201;&#35780;&#20272;&#22120;&#30340;&#20154;&#31867;&#26367;&#20195;&#21697;&#65292;&#30001;&#20110;&#23384;&#22312;&#38480;&#21046;&#65292;&#23427;&#20204;&#22312;&#35780;&#20272;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#30528;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#24320;&#22987;&#20351;&#29992;LLMs&#12290;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;LLMs&#30340;&#39046;&#22495;&#26159;&#20316;&#20026;&#22797;&#26434;&#29983;&#25104;&#20219;&#21153;&#30340;&#26367;&#20195;&#35780;&#20272;&#24230;&#37327;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#35780;&#23457;&#21592;&#26469;&#34917;&#20805;&#21508;&#31181;&#35780;&#20272;&#32500;&#24230;&#65288;&#22914;&#27969;&#30021;&#24615;&#21644;&#19968;&#33268;&#24615;&#65289;&#30340;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;LLMs&#20316;&#20026;&#25277;&#35937;&#25688;&#35201;&#35780;&#20272;&#22120;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;ChatGPT&#21644;GPT-4&#22312;&#24120;&#29992;&#30340;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#30001;&#20110;&#37325;&#35201;&#38480;&#21046;&#65292;&#23427;&#20204;&#23578;&#26410;&#20934;&#22791;&#22909;&#20195;&#26367;&#20154;&#24037;&#35780;&#20272;&#12290;&#21363;LLM&#35780;&#20272;&#22120;&#23545;&#27599;&#20010;&#20505;&#36873;&#31995;&#32479;&#30340;&#35780;&#20998;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#19982;&#35780;&#20272;&#32500;&#24230;&#30456;&#20851;&#12290;&#23427;&#20204;&#20063;&#38590;&#20197;&#27604;&#36739;&#24615;&#33021;&#25509;&#36817;&#30340;&#20505;&#36873;&#31995;&#32479;&#65292;&#24182;&#22312;&#33719;&#21462;&#36739;&#39640;&#36136;&#37327;&#25688;&#35201;&#26102;&#21464;&#24471;&#26356;&#21152;&#19981;&#21487;&#38752;&#65292;&#19982;&#20256;&#32479;&#24230;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13062</link><description>&lt;p&gt;
GPT4Table&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#21527;&#65311;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#36234;&#26469;&#36234;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#30340;&#29702;&#35299;&#31243;&#24230;&#36824;&#26377;&#24456;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#22320;&#26041;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#24207;&#21015;&#21270;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;LLMs&#26159;&#21542;&#30495;&#27491;&#33021;&#22815;&#29702;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;LLMs&#30340;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65288;SUC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19971;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21333;&#20803;&#26684;&#26597;&#25214;&#12289;&#34892;&#26816;&#32034;&#21644;&#22823;&#23567;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#22240;&#22810;&#31181;&#36755;&#20837;&#36873;&#25321;&#32780;&#24322;&#65292;&#21253;&#25324;&#34920;&#26684;&#36755;&#20837;&#26684;&#24335;&#12289;&#20869;&#23481;&#39034;&#24207;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#20998;&#21306;&#26631;&#35760;&#31561;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;ChatGPT&#26367;&#20195;&#20247;&#21253;&#29983;&#25104;&#24847;&#22270;&#20998;&#31867;&#30340;&#25913;&#20889;&#25351;&#20196;&#30340;&#21487;&#34892;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#29983;&#25104;&#30340;&#25913;&#20889;&#25351;&#20196;&#26356;&#21152;&#22810;&#26679;&#21270;&#65292;&#24182;&#19988;&#25152;&#24471;&#27169;&#22411;&#20855;&#26377;&#33267;&#23569;&#30456;&#21516;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12947</link><description>&lt;p&gt;
ChatGPT&#21462;&#20195;&#20247;&#21253;&#29983;&#25104;&#24847;&#22270;&#20998;&#31867;&#30340;&#25913;&#20889;&#25351;&#20196;&#65306;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness. (arXiv:2305.12947v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;ChatGPT&#26367;&#20195;&#20247;&#21253;&#29983;&#25104;&#24847;&#22270;&#20998;&#31867;&#30340;&#25913;&#20889;&#25351;&#20196;&#30340;&#21487;&#34892;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#29983;&#25104;&#30340;&#25913;&#20889;&#25351;&#20196;&#26356;&#21152;&#22810;&#26679;&#21270;&#65292;&#24182;&#19988;&#25152;&#24471;&#27169;&#22411;&#20855;&#26377;&#33267;&#23569;&#30456;&#21516;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#23427;&#23545;&#20247;&#21253;&#30340;&#24433;&#21709;&#20250;&#26159;&#20160;&#20040;&#65311;&#20256;&#32479;&#19978;&#65292;&#20247;&#21253;&#34987;&#29992;&#20110;&#33719;&#21462;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#28041;&#21450;&#25991;&#26412;&#29983;&#25104;&#12289;&#20462;&#25913;&#25110;&#35780;&#20272;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#20854;&#20013;&#19968;&#20123;&#20219;&#21153;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#27169;&#22411;&#26377;&#21487;&#33021;&#26367;&#20195;&#20154;&#24037;&#24037;&#20316;&#32773;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#36866;&#29992;&#20110;&#29983;&#25104;&#24847;&#22270;&#20998;&#31867;&#30340;&#25913;&#20889;&#25351;&#20196;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#29616;&#26377;&#20247;&#21253;&#30740;&#31350;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65288;&#30456;&#20284;&#30340;&#35268;&#27169;&#12289;&#25552;&#31034;&#20449;&#24687;&#21644;&#31181;&#23376;&#25968;&#25454;&#65289;&#65292;&#20351;&#29992;ChatGPT&#21644;Falcon-40B&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#29983;&#25104;&#30340;&#25913;&#20889;&#25351;&#20196;&#26356;&#22810;&#26679;&#21270;&#65292;&#24182;&#19988;&#20135;&#29983;&#30340;&#27169;&#22411;&#33267;&#23569;&#20855;&#26377;&#21516;&#26679;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of generative large language models (LLMs) raises the question: what will be its impact on crowdsourcing? Traditionally, crowdsourcing has been used for acquiring solutions to a wide variety of human-intelligence tasks, including ones involving text generation, modification or evaluation. For some of these tasks, models like ChatGPT can potentially substitute human workers. In this study, we investigate whether this is the case for the task of paraphrase generation for intent classification. We apply data collection methodology of an existing crowdsourcing study (similar scale, prompts and seed data) using ChatGPT and Falcon-40B. We show that ChatGPT-created paraphrases are more diverse and lead to at least as robust models.
&lt;/p&gt;</description></item><item><title>DisCo&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#24494;&#35843;&#30001;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#65292;&#37319;&#29992;&#21327;&#21516;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#30340;&#30693;&#35782;&#20849;&#20139;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DisCo&#30456;&#23545;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#12290;</title><link>http://arxiv.org/abs/2305.12074</link><description>&lt;p&gt;
DisCo: &#20351;&#29992;&#33976;&#39311;&#32858;&#21512;&#21327;&#21516;&#35757;&#32451;&#21322;&#30417;&#30563;&#25991;&#26412;&#25366;&#25496;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining. (arXiv:2305.12074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12074
&lt;/p&gt;
&lt;p&gt;
DisCo&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#24494;&#35843;&#30001;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#65292;&#37319;&#29992;&#21327;&#21516;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#30340;&#30693;&#35782;&#20849;&#20139;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DisCo&#30456;&#23545;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25991;&#26412;&#25366;&#25496;&#27169;&#22411;&#26159;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24494;&#35843;&#22823;&#22411;&#28145;&#24230;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26500;&#24314;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#26102;&#65292;&#20854;&#20013;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#20445;&#25345;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DisCo&#65292;&#36825;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#24494;&#35843;&#30001;&#22823;&#22411;PLM&#29983;&#25104;&#30340;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#38431;&#21015;&#65292;&#35813;&#38431;&#21015;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#20849;&#20139;&#31934;&#21326;&#30693;&#35782;&#20197;&#20419;&#36827;&#20854;SSL&#26377;&#25928;&#24615;&#30340;&#33976;&#39311;&#23398;&#29983;&#38431;&#21015;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;DisCo&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#33976;&#39311;&#31574;&#30053;&#21644;&#21508;&#31181;&#36755;&#20837;&#22686;&#24378;&#20135;&#29983;&#30340;&#27169;&#22411;&#35270;&#22270;&#21644;&#25968;&#25454;&#35270;&#22270;&#19979;&#20419;&#36827;&#23398;&#29983;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#26469;&#20248;&#21270;&#22810;&#20010;&#23567;&#23398;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#38024;&#23545;&#21322;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#21644;&#25552;&#21462;&#24335;&#24635;&#32467;&#20219;&#21153;&#23545;DisCo&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DisCo&#21487;&#20197;&#20135;&#29983;&#27604;&#21407;&#22987;&#27169;&#22411;&#23567;7.6&#20493;&#21644;&#27604;&#24050;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many text mining models are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge is maintaining performance when we use a lightweight model with limited labeled samples. We present DisCo, a semi-supervised learning (SSL) framework for fine-tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key insight is to share complementary knowledge among distilled student cohorts to promote their SSL effectiveness. DisCo employs a novel co-training technique to optimize multiple small student models by promoting knowledge sharing among students under diversified views: model views produced by different distillation strategies and data views produced by various input augmentations. We evaluate DisCo on both semi-supervised text classification and extractive summarization tasks. Experimental results show that DisCo can produce student models that are 7.6 times smaller and 4.8 t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#25226;&#30418;&#23884;&#20837;&#27010;&#24565;&#24341;&#20837;&#21040;&#26497;&#22352;&#26631;&#20013;&#65292;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#65292;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#65292;&#23454;&#29616;&#32858;&#31867;&#12290;&#22312;&#23454;&#20307;&#38142;&#25509;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2305.12027</link><description>&lt;p&gt;
&#26497;&#22320;&#40493;&#23376;&#30340;&#21457;&#29616;&#20043;&#26053;&#65306;&#20351;&#29992;&#40493;&#24335;&#36776;&#26512;&#21644;&#26497;&#22352;&#26631;&#30418;&#23884;&#20837;&#22686;&#24378;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings. (arXiv:2305.12027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#25226;&#30418;&#23884;&#20837;&#27010;&#24565;&#24341;&#20837;&#21040;&#26497;&#22352;&#26631;&#20013;&#65292;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#65292;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#65292;&#23454;&#29616;&#32858;&#31867;&#12290;&#22312;&#23454;&#20307;&#38142;&#25509;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#26816;&#32034;&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#26159;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#39640;&#25928;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#19981;&#22914;&#29983;&#25104;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#32467;&#26500;&#25935;&#24863;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#21463;&#32534;&#31243;&#35821;&#35328;&#20013;&#40493;&#24335;&#36776;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#35758;&#26681;&#25454;&#23454;&#20307;&#22312;&#30693;&#35782;&#22270;&#20013;&#30340;&#20851;&#31995;&#23450;&#20041;&#23454;&#20307;&#30340;&#31867;&#22411;&#12290;&#28982;&#21518;&#65292;&#23558;&#30418;&#23884;&#20837;&#30340;&#27010;&#24565;&#31227;&#26893;&#21040;&#29699;&#24418;&#26497;&#22352;&#26631;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#26469;&#20248;&#21270;&#27169;&#22411;&#20197;&#32858;&#31867;&#23454;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#23454;&#20307;&#28040;&#27495;&#22522;&#20934;&#27979;&#35797;&#19978;&#35774;&#32622;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#23427;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29305;&#21035;&#19981;&#36866;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#19981;&#21487;&#34892;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking methods based on dense retrieval are an efficient and widely used solution in large-scale applications, but they fall short of the performance of generative models, as they are sensitive to the structure of the embedding space. In order to address this issue, this paper introduces DUCK, an approach to infusing structural information in the space of entity representations, using prior knowledge of entity types. Inspired by duck typing in programming languages, we propose to define the type of an entity based on the relations that it has with other entities in a knowledge graph. Then, porting the concept of box embeddings to spherical polar coordinates, we propose to represent relations as boxes on the hypersphere. We optimize the model to cluster entities of similar type by placing them inside the boxes corresponding to their relations. Our experiments show that our method sets new state-of-the-art results on standard entity-disambiguation benchmarks, it improves the perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11707</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#20250;&#26159;&#20160;&#20040;&#65311;&#35780;&#20272;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#20219;&#20309;&#36755;&#20837;&#65292;&#23384;&#22312;&#22810;&#20010;&#21487;&#34892;&#30340;&#20132;&#38469;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#22810;&#31181;&#26041;&#24335;&#23558;&#20219;&#20309;&#30446;&#26631;&#29992;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#25110;&#36827;&#34892;&#29983;&#20135;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#20154;&#31867;&#29983;&#20135;&#22312;&#22235;&#20010;NLG&#20219;&#21153;&#20013;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#21464;&#24322;&#31243;&#24230;&#65292;&#24182;&#23558;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#19982;&#19981;&#30830;&#23450;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#29983;&#25104;&#31995;&#32479;&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;&#35299;&#30721;&#31639;&#27861;&#25152;&#24418;&#25104;&#30340;&#36755;&#20986;&#23383;&#31526;&#20018;&#31354;&#38388;&#65292;&#20197;&#25506;&#31350;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#29983;&#25104;&#22120;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;NLG&#27169;&#22411;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#23545;&#29983;&#25104;&#22120;&#36827;&#34892;&#25506;&#27979;&#65292;&#25552;&#20379;&#20102;&#29702;&#35299;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#25152;&#24517;&#38656;&#30340;&#35814;&#32454;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system's predicted probability distribution and decoding algorithm to probe its uncertainty. For each test input, we measure the generator's calibration to human production variability. Following this instance-level approach, we analyse NLG models and decoding strategies, demonstrating that probing a generator with multiple samples and, when possible, multiple references, provides the level of detail necessary to gain understanding of a model's representation of uncertainty.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#21518;&#38376;&#27745;&#26579;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#36731;&#25991;&#26412;&#29305;&#24449;&#21644;&#20998;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#26469;&#38450;&#24481;&#25915;&#20987;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25152;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#25554;&#20837;&#24335;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2305.11596</link><description>&lt;p&gt;
&#36879;&#36807;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#35270;&#35282;&#32531;&#35299;&#21518;&#38376;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. (arXiv:2305.11596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#21518;&#38376;&#27745;&#26579;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#36731;&#25991;&#26412;&#29305;&#24449;&#21644;&#20998;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#26469;&#38450;&#24481;&#25915;&#20987;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25152;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#25554;&#20837;&#24335;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;NLP&#27169;&#22411;&#36890;&#24120;&#22312;&#22823;&#22411;&#19981;&#21487;&#20449;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#25552;&#39640;&#20102;&#24694;&#24847;&#23545;&#25163;&#30772;&#22351;&#27169;&#22411;&#34892;&#20026;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21518;&#38376;&#27745;&#26579;&#25915;&#20987;&#23637;&#31034;&#20102;&#31616;&#21333;&#25991;&#26412;&#29305;&#24449;&#21644;&#20998;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#20316;&#20026;&#38450;&#24481;&#25163;&#27573;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24694;&#24847;&#35302;&#21457;&#22120;&#19982;&#20854;&#30446;&#26631;&#26631;&#31614;&#39640;&#24230;&#30456;&#20851;&#65292;&#22240;&#27492;&#36825;&#20123;&#30456;&#20851;&#24615;&#19982;&#33391;&#24615;&#29305;&#24449;&#24471;&#20998;&#30456;&#27604;&#26497;&#26131;&#21306;&#20998;&#65292;&#24182;&#21487;&#29992;&#20110;&#36807;&#28388;&#21487;&#33021;&#26377;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#19982;&#20960;&#31181;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#25152;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#22312;&#25554;&#20837;&#24335;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern NLP models are often trained over large untrusted datasets, raising the potential for a malicious adversary to compromise model behaviour. For instance, backdoors can be implanted through crafting training instances with a specific textual trigger and a target label. This paper posits that backdoor poisoning attacks exhibit spurious correlation between simple text features and classification labels, and accordingly, proposes methods for mitigating spurious correlation as means of defence. Our empirical study reveals that the malicious triggers are highly correlated to their target labels; therefore such correlations are extremely distinguishable compared to those scores of benign features, and can be used to filter out potentially problematic instances. Compared with several existing defences, our defence method significantly reduces attack success rates across backdoor attacks, and in the case of insertion based attacks, our method provides a near-perfect defence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#65292;&#23588;&#20854;&#26159;&#19981;&#38656;&#35201;&#20219;&#20309;&#26174;&#24335;&#27169;&#22359;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#31867;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#38544;&#24335;&#26377;&#25928;&#22320;&#32534;&#30721;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.10613</link><description>&lt;p&gt;
&#19981;&#20381;&#38752;&#20808;&#39564;&#30693;&#35782;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning. (arXiv:2305.10613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#65292;&#23588;&#20854;&#26159;&#19981;&#38656;&#35201;&#20219;&#20309;&#26174;&#24335;&#27169;&#22359;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#31867;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#38544;&#24335;&#26377;&#25928;&#22320;&#32534;&#30721;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#39044;&#27979;&#26159;&#19968;&#20010;&#25361;&#25112;&#27169;&#22411;&#20351;&#29992;&#36807;&#21435;&#30340;&#30693;&#35782;&#26469;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;LLMs&#22312;TKG&#39044;&#27979;&#20013;&#21487;&#20197;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#25110;&#25429;&#25417;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#26174;&#24335;&#27169;&#22359;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#30456;&#20851;&#21382;&#21490;&#20107;&#23454;&#36716;&#25442;&#20026;&#25552;&#31034;&#24182;&#20351;&#29992;&#20196;&#29260;&#27010;&#29575;&#29983;&#25104;&#25490;&#21517;&#39044;&#27979;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#30340;&#24615;&#33021;&#19982;&#20026;TKG&#39044;&#27979;&#31934;&#24515;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;TKG&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#23637;&#31034;&#20102;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#27604;&#36739;&#20102;&#20934;&#22791;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26367;&#20195;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#19982;&#33879;&#21517;&#30340;TKG&#26041;&#27861;&#21644;&#31616;&#21333;&#30340;&#39057;&#29575;&#21644;&#26368;&#36817;&#24615;&#22522;&#32447;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#25152;&#39044;&#27979;&#30340;&#20107;&#23454;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30830;&#23454;&#21487;&#20197;&#38544;&#24335;&#26377;&#25928;&#22320;&#32534;&#30721;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#36827;&#34892;TKG&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30693;&#35782;&#25110;&#39046;&#22495;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we apply large language models (LLMs) to these benchmarks using in-context learning (ICL). We investigate whether and to what extent LLMs can be used for TKG forecasting, especially without any fine-tuning or explicit modules for capturing structural and temporal information. For our experiments, we present a framework that converts relevant historical facts into prompts and generates ranked predictions using token probabilities. Surprisingly, we observe that LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully designed and trained for TKG forecasting. Our extensive evaluation presents performances across several models and datasets with different characteristics, compares alternative heuristics for preparing contextual information, and contrasts to prominent TKG methods and simple frequency and recency baselines. We als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35814;&#32454;&#38416;&#36848;&#35270;&#20026;&#38544;&#21547;&#38382;&#39064;&#30340;&#26126;&#30830;&#22238;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;ElabQUD&#23545;&#20316;&#32773;&#38416;&#36848;&#20449;&#24687;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.10387</link><description>&lt;p&gt;
&#20316;&#20026;&#38544;&#21547;&#35752;&#35770;&#38382;&#39064;&#30340;&#35814;&#32454;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Elaborative Simplification as Implicit Questions Under Discussion. (arXiv:2305.10387v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35814;&#32454;&#38416;&#36848;&#35270;&#20026;&#38544;&#21547;&#38382;&#39064;&#30340;&#26126;&#30830;&#22238;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;ElabQUD&#23545;&#20316;&#32773;&#38416;&#36848;&#20449;&#24687;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#19979;&#20174;&#22797;&#26434;&#21477;&#23376;&#21040;&#31616;&#21270;&#21477;&#23376;&#30340;&#21333;&#35821;&#32763;&#35793;&#24037;&#20316;&#65292;&#26377;&#21161;&#20110;&#20351;&#25991;&#26412;&#26356;&#26131;&#20110;&#35753;&#20799;&#31461;&#21644;&#26032;&#20852;&#21452;&#35821;&#32773;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35266;&#28857;&#24573;&#30053;&#20102;&#35814;&#32454;&#31616;&#21270;&#30340;&#24773;&#20917;&#65292;&#21363;&#22312;&#31616;&#21270;&#25991;&#26412;&#20013;&#28155;&#21152;&#26032;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#35814;&#32454;&#31616;&#21270;&#35270;&#20026;&#35752;&#35770;&#38382;&#39064;&#26694;&#26550;&#65288;QUD&#65289;&#30340;&#19968;&#37096;&#20998;&#65292;&#23558;&#35814;&#32454;&#38416;&#36848;&#30340;&#20449;&#24687;&#35270;&#20026;&#23545;&#38544;&#21547;&#38382;&#39064;&#30340;&#26126;&#30830;&#22238;&#31572;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#30740;&#31350;&#20316;&#32773;&#38416;&#36848;&#21738;&#20123;&#20449;&#24687;&#12289;&#22914;&#20309;&#38416;&#36848;&#20197;&#21450;&#38416;&#36848;&#23558;&#22914;&#20309;&#36866;&#24212;&#35805;&#35821;&#32972;&#26223;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ElabQUD&#65292;&#20854;&#20013;&#21253;&#25324;1.3K&#30340;&#35814;&#32454;&#38416;&#36848;&#21644;&#38544;&#21547;&#30340;QUD&#65292;&#20197;&#30740;&#31350;&#36825;&#20123;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated text simplification, a technique useful for making text more accessible to people such as children and emergent bilinguals, is often thought of as a monolingual translation task from complex sentences to simplified sentences using encoder-decoder models. This view fails to account for elaborative simplification, where new information is added into the simplified text. This paper proposes to view elaborative simplification through the lens of the Question Under Discussion (QUD) framework, providing a robust way to investigate what writers elaborate upon, how they elaborate, and how elaborations fit into the discourse context by viewing elaborations as explicit answers to implicit questions. We introduce ElabQUD, consisting of 1.3K elaborations accompanied with implicit QUDs, to study these phenomena. We show that explicitly modeling QUD (via question generation) not only provides essential understanding of elaborative simplification and how the elaborations connect with the re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PMIndiaSum&#65292;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#30340;&#26631;&#39064;&#25688;&#35201;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08828</link><description>&lt;p&gt;
PMIndiaSum&#65306;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26631;&#39064;&#25688;&#35201;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India. (arXiv:2305.08828v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PMIndiaSum&#65292;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#30340;&#26631;&#39064;&#25688;&#35201;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PMIndiaSum&#65292;&#19968;&#20010;&#19987;&#27880;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#25688;&#35201;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#25552;&#20379;&#20102;&#22235;&#20010;&#35821;&#35328;&#23478;&#26063;&#12289;14&#31181;&#35821;&#35328;&#20197;&#21450;&#36804;&#20170;&#26368;&#22823;&#30340;196&#31181;&#35821;&#35328;&#37197;&#23545;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#22522;&#30784;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26500;&#24314;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#25968;&#25454;&#37319;&#38598;&#12289;&#22788;&#29702;&#21644;&#36136;&#37327;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#12289;&#25552;&#31034;&#20197;&#21450;&#32763;&#35793;&#21644;&#25688;&#35201;&#21457;&#24067;&#20102;&#21333;&#35821;&#12289;&#36328;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#25688;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#22312;&#21360;&#24230;&#35821;&#35328;&#20043;&#38388;&#30340;&#25688;&#35201;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#65292;&#21487;&#20197;&#33258;&#30001;&#20462;&#25913;&#21644;&#37325;&#26032;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces PMIndiaSum, a multilingual and massively parallel summarization corpus focused on languages in India. Our corpus provides a training and testing ground for four language families, 14 languages, and the largest to date with 196 language pairs. We detail our construction workflow including data acquisition, processing, and quality assurance. Furthermore, we publish benchmarks for monolingual, cross-lingual, and multilingual summarization by fine-tuning, prompting, as well as translate-and-summarize. Experimental results confirm the crucial role of our data in aiding summarization between Indian languages. Our dataset is publicly available and can be freely modified and re-distributed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#65292;&#22312;&#22810;&#39046;&#22495;&#30340;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#65292;&#22312;&#22810;&#39046;&#22495;&#30340;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#25277;&#35937;&#21333;&#25991;&#26723;&#25688;&#35201;&#65288;SDS&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#22909;&#22788;&#21487;&#33021;&#19981;&#20250;&#36731;&#26131;&#25193;&#23637;&#21040;&#22810;&#25991;&#26723;&#25688;&#35201;&#65288;MDS&#65289;&#65292;&#22240;&#20026;&#25991;&#26723;&#20043;&#38388;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#35774;&#35745;&#26032;&#30340;&#26550;&#26500;&#25110;&#26032;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;MDS&#65292;&#35201;&#20040;&#23558;PLM&#24212;&#29992;&#20110;MDS&#65292;&#20294;&#26410;&#32771;&#34385;&#21040;&#22797;&#26434;&#30340;&#25991;&#26723;&#20132;&#20114;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#19978;&#24378;&#21046;&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#23547;&#27714;&#26356;&#22909;&#22320;&#21033;&#29992;PLM&#20419;&#36827;MDS&#20219;&#21153;&#30340;&#22810;&#25991;&#26723;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#35774;&#35745;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#35206;&#30422;&#21508;&#31181;&#39046;&#22495;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#37117;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#25913;&#36827;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;MDS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have accomplished impressive achievements in abstractive single-document summarization (SDS). However, such benefits may not be readily extended to muti-document summarization (MDS), where the interactions among documents are more complex. Previous works either design new architectures or new pre-training objectives for MDS, or apply PLMs to MDS without considering the complex document interactions. While the former does not make full use of previous pre-training efforts and may not generalize well across multiple domains, the latter cannot fully attend to the intricate relationships unique to MDS tasks. In this paper, we enforce hierarchy on both the encoder and decoder and seek to make better use of a PLM to facilitate multi-document interactions for the MDS task. We test our design on 10 MDS datasets across a wide range of domains. Extensive experiments show that our proposed method can achieve consistent improvements on all these datasets, outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06677</link><description>&lt;p&gt;
INGENIOUS&#65306;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#30528;&#29305;&#28857;&#26159;&#22312;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#26032;&#33021;&#21147;&#26041;&#38754;&#38543;&#30528;&#27169;&#22411;&#23481;&#37327;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780; achieved. &#28982;&#32780;&#65292;&#24517;&#39035;&#35748;&#35782;&#21040;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#36807;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12289;&#36807;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26377;&#23475;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#26159;&#21542;&#21487;&#33021;&#20165;&#20351;&#29992;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#26469;&#35757;&#32451; PTLM&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#20854;&#19979;&#28216;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
A salient characteristic of large pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; WikiWeb2M &#30340; 2M &#20010;&#22810;&#27169;&#24577;&#32593;&#39029;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#35813;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#29983;&#25104;&#20219;&#21153;&#24182;&#39564;&#35777;&#20102;&#25104;&#21151;&#24615;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Prefix Global &#30340;&#26032;&#39062;&#27880;&#24847;&#26426;&#21046;&#65292;&#20063;&#23545;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20026;&#22810;&#27169;&#24577;&#32593;&#39029;&#29702;&#35299;&#20219;&#21153;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#21644;&#23454;&#39564;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.03668</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#23618;&#22810;&#27169;&#24577;&#32593;&#39029;&#29983;&#25104;&#20219;&#21153;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding. (arXiv:2305.03668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; WikiWeb2M &#30340; 2M &#20010;&#22810;&#27169;&#24577;&#32593;&#39029;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#35813;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#29983;&#25104;&#20219;&#21153;&#24182;&#39564;&#35777;&#20102;&#25104;&#21151;&#24615;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Prefix Global &#30340;&#26032;&#39062;&#27880;&#24847;&#26426;&#21046;&#65292;&#20063;&#23545;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20026;&#22810;&#27169;&#24577;&#32593;&#39029;&#29702;&#35299;&#20219;&#21153;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#21644;&#23454;&#39564;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#39029;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#35270;&#35273;-&#35821;&#35328;&#21644;&#32431;&#35821;&#35328;&#20219;&#21153;&#36164;&#28304;&#65292;&#20294;&#21482;&#26377;&#22270;&#20687;-&#26631;&#39064;&#23545;&#12289;&#38271;&#25991;&#26412;&#25991;&#31456;&#25110;&#21407;&#22987;HTML&#31561;&#37096;&#20998;&#32452;&#25104;&#30340;&#32593;&#39029;&#24471;&#20197;&#20445;&#23384;&#65292;&#27809;&#26377;&#19968;&#20010;&#21253;&#21547;&#20840;&#37096;&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;&#22240;&#27492;&#65292;&#32593;&#39029;&#20219;&#21153;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#32467;&#26500;&#21450;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#19968;&#30452;&#21463;&#21040;&#20851;&#27880;&#30340;&#36739;&#23569;&#12290;&#20026;&#20102;&#30740;&#31350;&#22810;&#27169;&#24577;&#32593;&#39029;&#29702;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; Wikipedia Webpage &#22871;&#20214; (WikiWeb2M) &#65292;&#21253;&#21547; 2M &#20010;&#39029;&#38754;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#29983;&#25104;&#20219;&#21153;&#19978;&#39564;&#35777;&#20854;&#23454;&#29992;&#24615;: &#39029;&#38754;&#25551;&#36848;&#29983;&#25104;&#12289;&#31456;&#33410;&#25688;&#35201;&#21644;&#29615;&#22659;&#22270;&#20687;&#23383;&#24149;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#27880;&#24847;&#26426;&#21046; Prefix Global&#65292;&#23427;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#20869;&#23481;&#20316;&#20026;&#20840;&#23616;&#26631;&#35760;&#65292;&#20197;&#20415;&#20110;&#20851;&#27880;&#32593;&#39029;&#30340;&#20854;&#20313;&#37096;&#20998;&#20197;&#33719;&#21462;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#20351;&#29992;&#39029;&#38754;&#32467;&#26500;&#26469;&#20998;&#31163;&#36825;&#20123;&#26631;&#35760;&#65292;&#23427;&#27604;&#20840;&#27880;&#24847;&#21147;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;WikiWeb2M &#30340;&#26032;&#27880;&#37322;&#25913;&#36827;&#20102;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data left underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage suite (WikiWeb2M) of 2M pages. We verify its utility on three generative tasks: page description generation, section summarization, and contextual image captioning. We design a novel attention mechanism Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. By using page structure to separate such tokens, it performs better than full attention with lower computational complexity. Experiments show that the new annotations from WikiWeb2M improve task performance compared to data from prior work. We also include ablations on se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#29983;&#25104;&#26032;&#38395;&#25991;&#31456;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#25688;&#35201;&#20559;&#35265;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.02321</link><description>&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#29305;&#24449;&#20998;&#26512;&#65306;&#20197;&#29305;&#26391;&#26222;&#21644;&#25308;&#30331;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterizing Political Bias in Automatic Summaries: A Case Study of Trump and Biden. (arXiv:2305.02321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#29983;&#25104;&#26032;&#38395;&#25991;&#31456;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#25688;&#35201;&#20559;&#35265;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;NLP&#31995;&#32479;&#21487;&#33021;&#23545;&#31038;&#20250;&#20559;&#35265;&#36827;&#34892;&#32534;&#30721;&#65307;&#28982;&#32780;&#65292;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#30340;&#25919;&#27835;&#20559;&#35265;&#20173;&#30456;&#23545;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23454;&#20307;&#26367;&#25442;&#26041;&#27861;&#30740;&#31350;&#20102;&#26032;&#38395;&#25991;&#31456;&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#23478;&#25551;&#32472;&#12290;&#25105;&#20204;&#22522;&#20110;&#25919;&#27835;&#23454;&#20307;&#21644;&#35789;&#27719;&#36164;&#28304;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35780;&#20272;&#25277;&#21462;&#24335;&#21644;&#25277;&#35937;&#24335;&#25688;&#35201;&#27169;&#22411;&#20013;&#26377;&#20851;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#21644;&#20052;&#183;&#25308;&#30331;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#19968;&#33268;&#30340;&#24046;&#24322;&#65292;&#20363;&#22914;&#22312;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#24403;&#23454;&#20307;&#22312;&#28304;&#25991;&#31456;&#20013;&#37325;&#28857;&#20986;&#29616;&#26102;&#65292;&#36825;&#20123;&#25688;&#35201;&#24046;&#24322;&#26368;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21270;&#29305;&#24449;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#26410;&#26469;&#30740;&#31350;&#25688;&#35201;&#20559;&#35265;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing literature has shown that powerful NLP systems may encode social biases; however, the political bias of summarization models remains relatively unknown. In this work, we use an entity replacement method to investigate the portrayal of politicians in automatically generated summaries of news articles. We develop a computational framework based on political entities and lexical resources, and use it to assess biases about Donald Trump and Joe Biden in both extractive and abstractive summarization models. We find consistent differences, such as stronger associations of a collective US government (i.e., administration) with Biden than with Trump. These summary dissimilarities are most prominent when the entity is heavily featured in the source article. Our systematic characterization provides a framework for future studies of bias in summarization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;AmbiEnt&#22522;&#20934;&#25968;&#25454;&#38598;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#26469;&#35780;&#20272;GPT-4&#21644;&#20854;&#20182;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#27495;&#20041;&#30340;&#35782;&#21035;&#21644;&#20998;&#31163;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36824;&#26080;&#27861;&#20934;&#30830;&#22788;&#29702;&#27495;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.14399</link><description>&lt;p&gt;
&#25105;&#20204;&#25285;&#24515;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24314;&#27169;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
We're Afraid Language Models Aren't Modeling Ambiguity. (arXiv:2304.14399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;AmbiEnt&#22522;&#20934;&#25968;&#25454;&#38598;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#26469;&#35780;&#20272;GPT-4&#21644;&#20854;&#20182;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#27495;&#20041;&#30340;&#35782;&#21035;&#21644;&#20998;&#31163;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36824;&#26080;&#27861;&#20934;&#30830;&#22788;&#29702;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27495;&#20041;&#26159;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#31649;&#29702;&#27495;&#20041;&#26159;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#20801;&#35768;&#25105;&#20204;&#20316;&#20026;&#27807;&#36890;&#32773;&#39044;&#26399;&#21040;&#35823;&#35299;&#65292;&#24182;&#20316;&#20026;&#21548;&#20247;&#20462;&#27491;&#25105;&#20204;&#30340;&#35299;&#37322;&#12290;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#23545;&#35805;&#30028;&#38754;&#21644;&#20889;&#20316;&#21161;&#25163;&#65292;&#22788;&#29702;&#21547;&#31946;&#19981;&#28165;&#30340;&#35821;&#35328;&#23545;&#23427;&#20204;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#21478;&#19968;&#21477;&#23376;&#30340;&#34164;&#21547;&#20851;&#31995;&#26469;&#25551;&#36848;&#21477;&#23376;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;1,645&#20010;&#19981;&#21516;&#31867;&#22411;&#27495;&#20041;&#30340;&#35821;&#35328;&#23398;&#27880;&#37322;&#31034;&#20363;&#30340;&#22522;&#20934;&#25968;&#25454;AmbiEnt&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;AmbiEnt&#30340;&#27979;&#35797;&#65292;&#21576;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#27495;&#20041;&#21644;&#20998;&#31163;&#21487;&#33021;&#21547;&#20041;&#30340;&#31532;&#19968;&#27425;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#20219;&#21153;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21253;&#25324;&#26368;&#36817;&#21457;&#24067;&#30340;GPT-4&#12290;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;GPT-4&#20135;&#29983;&#30340;&#28040;&#27495;&#34987;&#35748;&#20026;&#20165;&#26377;32%&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#28040;&#27495;&#30340;&#27491;&#30830;&#29575;&#20026;90%&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38416;&#26126;&#25105;&#20204;&#30740;&#31350;&#30340;&#20215;&#20540;&#65292;
&lt;/p&gt;
&lt;p&gt;
Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models (LMs) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for the recent GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value 
&lt;/p&gt;</description></item><item><title>DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.14108</link><description>&lt;p&gt;
DataComp&#65306;&#23547;&#25214;&#19979;&#19968;&#20195;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14108
&lt;/p&gt;
&lt;p&gt;
DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22312;&#36817;&#26399;&#30340;&#31361;&#30772;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#27604;&#22914;CLIP&#12289;Stable Diffusion&#21644;GPT-4&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25968;&#25454;&#38598;&#24456;&#23569;&#24471;&#21040;&#19982;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31639;&#27861;&#21516;&#31561;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataComp&#65292;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#35757;&#32451;&#20195;&#30721;&#26159;&#22266;&#23450;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Common Crawl&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#20854;&#20013;&#21253;&#21547;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#12290;&#21442;&#21152;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#65292;&#24182;&#36890;&#36807;&#36816;&#34892;&#25105;&#20204;&#26631;&#20934;&#21270;&#30340;CLIP&#35757;&#32451;&#20195;&#30721;&#24182;&#22312;38&#20010;&#19979;&#28216;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#22810;&#20010;&#35268;&#27169;&#65292;&#22235;&#20010;&#20505;&#36873;&#27744;&#22823;&#23567;&#21644;&#30456;&#24212;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#28085;&#30422;&#20102;&#20174;12.8M&#21040;12.8B&#20010;&#26679;&#26412;&#12290;&#36825;&#31181;&#22810;&#35268;&#27169;&#35774;&#35745;&#26377;&#21161;&#20110;&#30740;&#31350;&#35268;&#27169;&#36235;&#21183;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#36873;&#25321;&#20313;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Outlier Suppression+&#26694;&#26550;&#30340;&#36890;&#36947;&#32423;&#31227;&#20301;&#21644;&#32553;&#25918;&#25805;&#20316;&#65292;&#20998;&#26512;&#24471;&#21040;&#26368;&#20248;&#31227;&#20301;&#21644;&#32553;&#25918;&#20540;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#37327;&#21270;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#19981;&#23545;&#31216;&#31163;&#32676;&#20540;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#28014;&#28857;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09145</link><description>&lt;p&gt;
Outlier Suppression+&#65306;&#36890;&#36807;&#31561;&#25928;&#21644;&#26368;&#20248;&#31227;&#20301;&#21644;&#32553;&#25918;&#26469;&#20934;&#30830;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. (arXiv:2304.09145v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Outlier Suppression+&#26694;&#26550;&#30340;&#36890;&#36947;&#32423;&#31227;&#20301;&#21644;&#32553;&#25918;&#25805;&#20316;&#65292;&#20998;&#26512;&#24471;&#21040;&#26368;&#20248;&#31227;&#20301;&#21644;&#32553;&#25918;&#20540;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#37327;&#21270;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#19981;&#23545;&#31216;&#31163;&#32676;&#20540;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#28014;&#28857;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#38754;&#20020;&#30528;&#23384;&#22312;&#25439;&#23475;&#24615;&#31163;&#32676;&#20540;&#30340;&#37325;&#35201;&#38590;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31163;&#32676;&#20540;&#26159;&#19981;&#23545;&#31216;&#30340;&#24182;&#19988;&#38598;&#20013;&#22312;&#29305;&#23450;&#36890;&#36947;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Outlier Suppression+&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36947;&#32423;&#21035;&#30340;&#31227;&#20301;&#21644;&#32553;&#25918;&#25805;&#20316;&#26469;&#28040;&#38500;&#19981;&#23545;&#31216;&#34920;&#31034;&#24182;&#32553;&#23567;&#26377;&#38382;&#39064;&#30340;&#36890;&#36947;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#25805;&#20316;&#21487;&#20197;&#26080;&#32541;&#22320;&#36801;&#31227;&#33267;&#21518;&#32493;&#27169;&#22359;&#32780;&#20445;&#25345;&#31561;&#25928;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37327;&#21270;&#20998;&#26512;&#20102;&#26368;&#20248;&#30340;&#31227;&#20301;&#21644;&#32553;&#25918;&#20540;&#65292;&#32771;&#34385;&#21040;&#19979;&#19968;&#23618;&#26435;&#37325;&#30340;&#19981;&#23545;&#31216;&#29305;&#24615;&#21644;&#37327;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#38745;&#24577;&#21644;&#26631;&#20934;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#35774;&#32622;&#19979;&#36896;&#25104;&#26368;&#23567;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#30340;&#20840;&#38754;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23567;&#22411;&#27169;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#22914;GPT-2&#26041;&#38754;&#23454;&#29616;&#20102;&#25509;&#36817;&#28014;&#28857;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are asymmetric and concentrated in specific channels. To address this issue, we propose the Outlier Suppression+ framework. First, we introduce channel-wise shifting and scaling operations to eliminate asymmetric presentation and scale down problematic channels. We demonstrate that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we quantitatively analyze the optimal values for shifting and scaling, taking into account both the asymmetric property and quantization errors of weights in the next layer. Our lightweight framework can incur minimal performance degradation under static and standard post-training quantization settings. Comprehensive results across various tasks and models reveal that our approach achieves near-floating-point performance on both small models
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#34917;&#20805;&#25945;&#26448;&#30340;&#26377;&#36259;&#35270;&#35273;&#25903;&#25345;&#65292;&#20197;&#20419;&#36827;&#23398;&#29983;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35299;&#20915;&#29616;&#26377;&#25945;&#26448;&#20013;&#32570;&#20047;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.08931</link><description>&lt;p&gt;
&#21033;&#29992;&#32593;&#32476;&#22270;&#29255;&#25552;&#21319;&#25945;&#26448;&#30340;&#21487;&#35270;&#21270;&#25928;&#26524;&#20197;&#20419;&#36827;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Textbooks with Visuals from the Web for Improved Learning. (arXiv:2304.08931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#34917;&#20805;&#25945;&#26448;&#30340;&#26377;&#36259;&#35270;&#35273;&#25903;&#25345;&#65292;&#20197;&#20419;&#36827;&#23398;&#29983;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35299;&#20915;&#29616;&#26377;&#25945;&#26448;&#20013;&#32570;&#20047;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#26448;&#26159;&#21521;&#23398;&#29983;&#20256;&#36798;&#20248;&#36136;&#25945;&#32946;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#35299;&#37322;&#24615;&#25110;&#20855;&#35937;&#21270;&#30340;&#21487;&#35270;&#21270;&#20869;&#23481;&#22312;&#30693;&#35782;&#30340;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20256;&#36882;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#21457;&#23637;&#20013;&#22269;&#23478;&#65292;&#35768;&#22810;&#25945;&#31185;&#20070;&#21697;&#36136;&#36739;&#20302;&#19988;&#32570;&#20047;&#26377;&#36259;&#30340;&#35270;&#35273;&#25903;&#25345;&#20197;&#21161;&#21147;&#20110;&#23398;&#29983;&#30340;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#33258;&#21160;&#20351;&#29992;&#26469;&#33258;&#32593;&#32476;&#30340;&#22270;&#20687;&#20026;&#25945;&#26448;&#22686;&#28155;&#22270;&#29255;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#20813;&#36153;&#22312;&#32447;&#20986;&#29256;&#21830;&#20043;&#19968;&#30340;&#30005;&#23376;&#25945;&#26448;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#23494;&#30340;&#25968;&#25454;&#38598;&#20998;&#26512;&#65292;&#24182;&#21033;&#29992;&#25152;&#24471;&#30340;&#20998;&#26512;&#32467;&#26524;&#28608;&#21457;&#20102;&#19968;&#39033;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#32034;&#24182;&#36866;&#24403;&#20998;&#37197;&#32593;&#32476;&#22270;&#29255;&#21040;&#25945;&#26448;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#19968;&#39033;&#26032;&#39062;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20247;&#21253;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#65306;(1)&#34429;&#28982;&#21407;&#22987;&#25945;&#26448;&#22270;&#20687;&#35780;&#32423;&#26356;&#39640;&#65292;&#20294;&#33258;&#21160;&#20998;&#37197;&#22270;&#20687;&#24182;&#19981;&#30456;&#36317;&#22826;&#36828;&#65292;&#24182;&#19988;(2)&#36873;&#25321;&#30340;&#22270;&#20687;&#26469;&#28304;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textbooks are the primary vehicle for delivering quality education to students. It has been shown that explanatory or illustrative visuals play a key role in the retention, comprehension and the general transfer of knowledge. However, many textbooks, especially in the developing world, are low quality and lack interesting visuals to support student learning. In this paper, we investigate the effectiveness of vision-language models to automatically enhance textbooks with images from the web. Specifically, we collect a dataset of e-textbooks from one of the largest free online publishers in the world. We rigorously analyse the dataset, and use the resulting analysis to motivate a task that involves retrieving and appropriately assigning web images to textbooks, which we frame as a novel optimization problem. Through a crowd-sourced evaluation, we verify that (1) while the original textbook images are rated higher, automatically assigned ones are not far behind, and (2) the choice of the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#26657;&#20934;&#30340;&#32622;&#20449;&#20998;&#25968;&#65292;&#24179;&#34913;&#35299;&#26512;&#20219;&#21153;&#20013;&#30340;&#25104;&#26412;&#12289;&#26631;&#27880;&#21592;&#36127;&#25285;&#12289;&#20934;&#30830;&#24615;&#12289;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#31561;&#22810;&#20010;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;DidYouMean&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.16857</link><description>&lt;p&gt;
&#8220;&#20320;&#25351;&#30340;&#26159;...&#65311;&#8221;&#65306;&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#32622;&#20449;&#24230;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Did You Mean...? Confidence-based Trade-offs in Semantic Parsing. (arXiv:2303.16857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#26657;&#20934;&#30340;&#32622;&#20449;&#20998;&#25968;&#65292;&#24179;&#34913;&#35299;&#26512;&#20219;&#21153;&#20013;&#30340;&#25104;&#26412;&#12289;&#26631;&#27880;&#21592;&#36127;&#25285;&#12289;&#20934;&#30830;&#24615;&#12289;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#31561;&#22810;&#20010;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;DidYouMean&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#20010;&#26657;&#20934;&#22909;&#30340;&#27169;&#22411;&#26469;&#24179;&#34913;&#20219;&#21153;&#23548;&#21521;&#35299;&#26512;&#20013;&#30340;&#24120;&#35265;&#26435;&#34913;&#12290;&#22312;&#19968;&#20010;&#27169;&#25311;&#30340;&#26631;&#27880;&#21592;&#20132;&#20114;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26657;&#20934;&#30340;&#32622;&#20449;&#20998;&#25968;&#22914;&#20309;&#24179;&#34913;&#25104;&#26412;&#21644;&#26631;&#27880;&#21592;&#36127;&#25285;&#65292;&#29992;&#36739;&#23569;&#30340;&#20132;&#20114;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#22914;&#20309;&#24110;&#21161;&#20248;&#21270;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#38408;&#20540;&#30340;&#35299;&#26512;&#38169;&#35823;&#25968;&#37327;&#22823;&#24133;&#20943;&#23569;&#30340;&#31995;&#32479;DidYouMean&#65292;&#28982;&#32780;&#36825;&#20063;&#29306;&#29298;&#20102;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We illustrate how a calibrated model can help balance common trade-offs in task-oriented parsing. In a simulated annotator-in-the-loop experiment, we show that well-calibrated confidence scores allow us to balance cost with annotator load, improving accuracy with a small number of interactions. We then examine how confidence scores can help optimize the trade-off between usability and safety. We show that confidence-based thresholding can substantially reduce the number of incorrect low-confidence programs executed; however, this comes at a cost to usability. We propose the DidYouMean system which better balances usability and safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#26469;&#20248;&#21270;ChatGPT&#22312;&#22797;&#26434;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#28201;&#24230;&#35774;&#32622;&#21644;&#20219;&#21153;&#20449;&#24687;&#23545;ChatGPT&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.13780</link><description>&lt;p&gt;
&#20248;&#21270;ChatGPT&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Making the Most of ChatGPT for Machine Translation. (arXiv:2303.13780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#26469;&#20248;&#21270;ChatGPT&#22312;&#22797;&#26434;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#28201;&#24230;&#35774;&#32622;&#21644;&#20219;&#21153;&#20449;&#24687;&#23545;ChatGPT&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#21487;&#20197;&#36798;&#21040;&#21830;&#19994;&#31995;&#32479;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#65288;&#20363;&#22914;&#20302;&#36164;&#28304;&#21644;&#36828;&#31243;&#35821;&#35328;&#23545;&#32763;&#35793;&#65289;&#33853;&#21518;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;ChatGPT&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#28201;&#24230;&#12289;&#20219;&#21153;&#20449;&#24687;&#21644;&#39046;&#22495;&#20449;&#24687;&#31561;&#20960;&#20010;&#26041;&#38754;&#65292;&#36827;&#19968;&#27493;&#25366;&#25496;ChatGPT&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#24182;&#30456;&#24212;&#22320;&#25552;&#20986;&#20004;&#31181;&#65288;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#65289;&#25552;&#31034;&#65306;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65288;TSP&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#65288;DSP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information further improves ChatGPT's performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT's generalization ability and improve i
&lt;/p&gt;</description></item><item><title>RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.12570</link><description>&lt;p&gt;
RepoCoder&#65306;&#36890;&#36807;&#36845;&#20195;&#26816;&#32034;&#21644;&#29983;&#25104;&#23454;&#29616;&#30340;&#20195;&#30721;&#23384;&#20648;&#24211;&#32423;&#21035;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12570
&lt;/p&gt;
&lt;p&gt;
RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#20219;&#21153;&#26159;&#22522;&#20110;&#20195;&#30721;&#24211;&#26356;&#24191;&#38420;&#19978;&#19979;&#25991;&#20013;&#32487;&#32493;&#32534;&#20889;&#26410;&#23436;&#25104;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#20294;&#26159;&#23545;&#20110;&#33258;&#21160;&#23436;&#25104;&#24037;&#20855;&#32780;&#35328;&#65292;&#24456;&#38590;&#21033;&#29992;&#25955;&#24067;&#22312;&#19981;&#21516;&#25991;&#20214;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RepoCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#20102;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#65292;&#20174;&#32780;&#20801;&#35768;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#23618;&#38754;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;RepoCoder &#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26816;&#32034;-&#29983;&#25104;&#27169;&#22411;&#65292;&#24357;&#21512;&#20102;&#26816;&#32034;&#19978;&#19979;&#25991;&#21644;&#39044;&#26399;&#23436;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;RepoEval&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26368;&#26032;&#21644;&#39640;&#36136;&#37327;&#30495;&#23454;&#19990;&#30028;&#30340;&#20195;&#30721;&#24211;&#65292;&#28085;&#30422;&#20102;&#34892;&#12289;API &#35843;&#29992;&#21644;&#20989;&#25968;&#20307;&#23436;&#25104;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion sce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CHiLL&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#29983;&#25104;&#29305;&#24449;&#65292;&#20351;&#21307;&#29983;&#33021;&#22815;&#29992;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21046;&#20316;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12343</link><description>&lt;p&gt;
CHiLL: &#38646;-shot&#23450;&#21046;&#21270;&#12289;&#21487;&#35299;&#37322;&#30340;&#21307;&#30103;&#35760;&#24405;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models. (arXiv:2302.12343v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CHiLL&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#29983;&#25104;&#29305;&#24449;&#65292;&#20351;&#21307;&#29983;&#33021;&#22815;&#29992;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21046;&#20316;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CHiLL (Crafting High-Level Latents)&#65292;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21270;&#32447;&#24615;&#27169;&#22411;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;CHiLL&#36890;&#36807;&#19987;&#23478;&#21046;&#20316;&#30340;&#26597;&#35810;&#25351;&#23548;LLMs&#29983;&#25104;&#35299;&#37322;&#24615;&#29305;&#24449;&#65292;&#29992;&#20110;&#20174;&#20581;&#24247;&#35760;&#24405;&#20013;&#35757;&#32451;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#29983;&#25104;&#29305;&#24449;&#21487;&#20197;&#20351;&#21307;&#29983;&#21033;&#29992;&#20182;&#20204;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#21046;&#20316;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#20174;&#21407;&#22987;&#30005;&#23376;&#30149;&#21382;&#20013;&#25552;&#21462;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#21463;&#21040;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#20294;&#25105;&#20204;&#20351;&#29992;MIMIC-III&#21644;MIMIC-CXR&#25968;&#25454;&#21644;&#26631;&#20934;&#30340;&#39044;&#27979;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;30&#22825;&#20877;&#20837;&#38498;&#65289;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#33258;&#21160;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#20351;&#29992;&#21442;&#32771;&#29305;&#24449;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#27604;&#20351;&#29992;&#8220;&#35789;&#34955;&#8221;&#29305;&#24449;&#30340;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35777;&#23454;&#20102;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26435;&#37325;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR. We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate this approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using "Bag-of-Words" features. We verify that learned feature weig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24418;&#24577;&#20449;&#24687;&#22312;&#20845;&#31181;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23545;&#35789;&#24418;&#36824;&#21407;&#22120;&#36827;&#34892;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.00407</link><description>&lt;p&gt;
&#20851;&#20110;&#24418;&#24577;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Morphological Information for Contextual Lemmatization. (arXiv:2302.00407v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24418;&#24577;&#20449;&#24687;&#22312;&#20845;&#31181;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23545;&#35789;&#24418;&#36824;&#21407;&#22120;&#36827;&#34892;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#24418;&#36824;&#21407;&#26159;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#23427;&#21253;&#25324;&#20174;&#32473;&#23450;&#30340;&#23624;&#25240;&#35789;&#29983;&#25104;&#20854;&#35268;&#33539;&#24418;&#24335;&#25110;&#35789;&#24418;&#36824;&#21407;&#12290;&#35789;&#24418;&#36824;&#21407;&#26159;&#31616;&#21270;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#23545;&#20110;&#39640;&#24230;&#23624;&#25240;&#35821;&#35328;&#23588;&#20026;&#37325;&#35201;&#12290;&#34429;&#28982;&#26681;&#25454;&#23624;&#25240;&#35789;&#33719;&#24471;&#35789;&#24418;&#36824;&#21407;&#24418;&#24335;&#30340;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#20854;&#24418;&#24577;&#21477;&#27861;&#31867;&#21035;&#26469;&#35299;&#37322;&#65292;&#20294;&#22312;&#35757;&#32451;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#26102;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#24120;&#35265;&#20570;&#27861;&#65292;&#32780;&#26080;&#35270;&#19979;&#28216;&#32489;&#25928;&#26159;&#21542;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#20845;&#31181;&#35821;&#35328;&#20013;&#32771;&#23519;&#24418;&#24577;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#36825;&#20845;&#31181;&#35821;&#35328;&#30340;&#24418;&#24577;&#22797;&#26434;&#24615;&#21508;&#19981;&#30456;&#21516;&#65292;&#21253;&#25324;&#24052;&#26031;&#20811;&#35821;&#12289;&#22303;&#32819;&#20854;&#35821;&#12289;&#20420;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#12290;&#27492;&#22806;&#65292;&#19982;&#32477;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#36824;&#22312;&#39046;&#22495;&#22806;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35789;&#24418;&#36824;&#21407;&#22120;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lemmatization is a natural language processing (NLP) task which consists of producing, from a given inflected word, its canonical form or lemma. Lemmatization is one of the basic tasks that facilitate downstream NLP applications, and is of particular importance for high-inflected languages. Given that the process to obtain a lemma from an inflected word can be explained by looking at its morphosyntactic category, including fine-grained morphosyntactic information to train contextual lemmatizers has become common practice, without considering whether that is the optimum in terms of downstream performance. In order to address this issue, in this paper we empirically investigate the role of morphological information to develop contextual lemmatizers in six languages within a varied spectrum of morphological complexity: Basque, Turkish, Russian, Czech, Spanish and English. Furthermore, and unlike the vast majority of previous work, we also evaluate lemmatizers in out-of-domain settings, wh
&lt;/p&gt;</description></item><item><title>Logic Mill&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#24320;&#25918;&#35775;&#38382;&#30340;&#30693;&#35782;&#23548;&#33322;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#35782;&#21035;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#65292;&#24182;&#21487;&#29992;&#20110;&#31185;&#23398;&#20986;&#29256;&#29289;&#21644;&#19987;&#21033;&#25991;&#20214;&#12290;&#36825;&#20010;&#31995;&#32479;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#21487;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#21644;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2301.00200</link><description>&lt;p&gt;
Logic Mill -- &#19968;&#20010;&#30693;&#35782;&#23548;&#33322;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Logic Mill -- A Knowledge Navigation System. (arXiv:2301.00200v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00200
&lt;/p&gt;
&lt;p&gt;
Logic Mill&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#24320;&#25918;&#35775;&#38382;&#30340;&#30693;&#35782;&#23548;&#33322;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#35782;&#21035;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#65292;&#24182;&#21487;&#29992;&#20110;&#31185;&#23398;&#20986;&#29256;&#29289;&#21644;&#19987;&#21033;&#25991;&#20214;&#12290;&#36825;&#20010;&#31995;&#32479;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#21487;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#21644;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Logic Mill&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#24320;&#25918;&#35775;&#38382;&#30340;&#36719;&#20214;&#31995;&#32479;&#65292;&#21487;&#22312;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#26009;&#24211;&#25110;&#22810;&#39046;&#22495;&#35821;&#26009;&#24211;&#20013;&#35782;&#21035;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#12290;&#23427;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#29983;&#25104;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#12290;&#30446;&#21069;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36825;&#20123;&#25991;&#26723;&#34920;&#31034;&#12290;&#35813;&#31995;&#32479;&#19987;&#27880;&#20110;&#31185;&#23398;&#20986;&#29256;&#29289;&#21644;&#19987;&#21033;&#25991;&#20214;&#65292;&#21253;&#21547;&#36229;&#36807;2&#20159;&#20221;&#25991;&#26723;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#25110;Web&#30028;&#38754;&#36731;&#26494;&#35775;&#38382;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#25345;&#32493;&#26356;&#26032;&#65292;&#24182;&#21487;&#20197;&#25193;&#23637;&#21040;&#26469;&#33258;&#20854;&#20182;&#39046;&#22495;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#31995;&#32479;&#26159;&#31038;&#20250;&#31185;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#24212;&#29992;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logic Mill is a scalable and openly accessible software system that identifies semantically similar documents within either one domain-specific corpus or multi-domain corpora. It uses advanced Natural Language Processing (NLP) techniques to generate numerical representations of documents. Currently it leverages a large pre-trained language model to generate these document representations. The system focuses on scientific publications and patent documents and contains more than 200 million documents. It is easily accessible via a simple Application Programming Interface (API) or via a web interface. Moreover, it is continuously being updated and can be extended to text corpora from other domains. We see this system as a general-purpose tool for future research applications in the social sciences and other domains.
&lt;/p&gt;</description></item><item><title>CoCo&#26159;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#22686;&#24378;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.10341</link><description>&lt;p&gt;
CoCo: &#22312;&#25968;&#25454;&#26377;&#38480;&#24773;&#20917;&#19979;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#19968;&#33268;&#24615;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning. (arXiv:2212.10341v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10341
&lt;/p&gt;
&lt;p&gt;
CoCo&#26159;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#22686;&#24378;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65288;MGT&#65289;&#26816;&#27979;&#26159;&#19968;&#39033;&#23558;MGT&#19982;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#65288;HWT&#65289;&#21306;&#20998;&#24320;&#30340;&#20219;&#21153;&#65292;&#22312;&#38450;&#27490;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#34987;&#28389;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36825;&#20123;&#27169;&#22411;&#26368;&#36817;&#22312;&#27169;&#20223;&#20154;&#31867;&#20889;&#20316;&#39118;&#26684;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#26368;&#26032;&#25552;&#20986;&#30340;&#26816;&#27979;&#22120;&#36890;&#24120;&#23558;&#31895;&#31961;&#30340;&#25991;&#26412;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;&#21040;&#25991;&#26412;&#30340;&#35821;&#35328;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20063;&#32570;&#20047;&#22788;&#29702;&#20302;&#36164;&#28304;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32771;&#34385;&#21040;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#30340;&#24222;&#22823;&#37327;&#65292;&#20302;&#36164;&#28304;&#38382;&#39064;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#20250;&#32463;&#24120;&#21457;&#29983;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;CoCo&#65292;&#29992;&#20110;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#26816;&#27979;&#21487;&#33021;&#30340;MGT&#12290;&#20026;&#20102;&#21033;&#29992;&#35821;&#35328;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#19968;&#33268;&#24615;&#20449;&#24687;&#20197;&#22270;&#24418;&#30340;&#24418;&#24335;&#32534;&#30721;&#21040;&#25991;&#26412;&#34920;&#31034;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#20302;&#25968;&#25454;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#38450;&#27490;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-Generated Text (MGT) detection, a task that discriminates MGT from Human-Written Text (HWT), plays a crucial role in preventing misuse of text generative models, which excel in mimicking human writing style recently. Latest proposed detectors usually take coarse text sequences as input and fine-tune pretrained models with standard cross-entropy loss. However, these methods fail to consider the linguistic structure of texts. Moreover, they lack the ability to handle the low-resource problem which could often happen in practice considering the enormous amount of textual data online. In this paper, we present a coherence-based contrastive learning model named CoCo to detect the possible MGT under low-resource scenario. To exploit the linguistic feature, we encode coherence information in form of graph into text representation. To tackle the challenges of low data resource, we employ a contrastive learning framework and propose an improved contrastive loss for preventing performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;KRLS&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20851;&#38190;&#35789;&#24378;&#21270;&#23398;&#20064;&#21644;&#31934;&#32454;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24110;&#21161;&#27169;&#22411;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#29983;&#25104;&#20851;&#38190;&#35789;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;MultiWoZ&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.16773</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#35789;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#31471;&#21040;&#31471;&#21709;&#24212;&#29983;&#25104;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning. (arXiv:2211.16773v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;KRLS&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20851;&#38190;&#35789;&#24378;&#21270;&#23398;&#20064;&#21644;&#31934;&#32454;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24110;&#21161;&#27169;&#22411;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#29983;&#25104;&#20851;&#38190;&#35789;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;MultiWoZ&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#20013;&#65292;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#19988;&#25104;&#21151;&#30340;&#31995;&#32479;&#21709;&#24212;&#38656;&#35201;&#21253;&#21547;&#20851;&#38190;&#20449;&#24687;&#65292;&#20363;&#22914;&#37202;&#24215;&#30340;&#30005;&#35805;&#21495;&#30721;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#27491;&#30830;&#29983;&#25104;&#20851;&#38190;&#25968;&#37327;&#65292;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#21363;&#20851;&#38190;&#35789;&#24378;&#21270;&#23398;&#20064;&#19982;&#19979;&#19968;&#20010;&#21333;&#35789;&#37319;&#26679;&#65288;KRLS&#65289;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#36991;&#20813;&#20102;&#32791;&#26102;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#37319;&#29992;&#20102;&#32454;&#31890;&#24230;&#30340;&#36880;&#20196;&#29260;&#22870;&#21169;&#20989;&#25968;&#26469;&#24110;&#21161;&#27169;&#22411;&#26356;&#21152;&#24378;&#20581;&#22320;&#23398;&#20064;&#20851;&#38190;&#35789;&#29983;&#25104;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;KRLS&#31639;&#27861;&#21487;&#20197;&#22312;MultiWoZ&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#20449;&#24687;&#12289;&#25104;&#21151;&#21644;&#32508;&#21512;&#20998;&#25968;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In task-oriented dialogs, an informative and successful system response needs to include key information such as the phone number of a hotel. Therefore, we hypothesize that a model can achieve better overall performance by focusing on correctly generating key quantities. In this paper, we propose a new training algorithm, Keywords Reinforcement Learning with Next-word Sampling (KRLS), that utilizes Reinforcement Learning but avoids the time-consuming auto-regressive generation, and a fine-grained per-token reward function to help the model learn keywords generation more robustly. Empirical results show that the KRLS algorithm can achieve state-of-the-art performance on the inform, success, and combined score on the MultiWoZ benchmark dataset.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;Attention-FFN Adapter&#65292;&#29992;&#20110;&#26500;&#24314;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30456;&#23545;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#24179;&#22343;&#25552;&#39640;&#20102;0.6%&#33267;2%&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#26377;&#25928;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.11363</link><description>&lt;p&gt;
AF Adapter: &#36830;&#32493;&#39044;&#35757;&#32451;&#29992;&#20110;&#26500;&#24314;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model. (arXiv:2211.11363v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11363
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;Attention-FFN Adapter&#65292;&#29992;&#20110;&#26500;&#24314;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30456;&#23545;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#24179;&#22343;&#25552;&#39640;&#20102;0.6%&#33267;2%&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#26377;&#25928;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#39044;&#35757;&#32451;&#26159;&#20174;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#29305;&#23450;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#23613;&#31649;&#39640;&#25928;&#65292;&#20294;&#36830;&#32493;&#39044;&#35757;&#32451;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;BERT-based&#27169;&#22411;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;Attention-FFN Adapter&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#22312;&#27599;&#20010;&#33258;&#27880;&#24847;&#23618;&#21644;&#21069;&#39304;&#32593;&#32476;&#20013;&#24341;&#20837;&#23569;&#37327;&#30340;&#27880;&#24847;&#22836;&#21644;&#38544;&#34255;&#21333;&#20803;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;RoBERTa&#30340;&#38024;&#23545;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#65292;&#21517;&#20026;AF Adapter&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#32422;17%&#65292;AF Adapter&#30456;&#23545;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;&#20102;0.6%&#33267;2%&#12290;&#36827;&#19968;&#27493;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual pretraining is a popular way of building a domain-specific pretrained language model from a general-domain language model. In spite of its high efficiency, continual pretraining suffers from catastrophic forgetting, which may harm the model's performance in downstream tasks. To alleviate the issue, in this paper, we propose a continual pretraining method for the BERT-based model, named Attention-FFN Adapter. Its main idea is to introduce a small number of attention heads and hidden units inside each self-attention layer and feed-forward network. Furthermore, we train a domain-specific language model named AF Adapter based RoBERTa for the Chinese biomedical domain. In experiments, models are applied to downstream tasks for evaluation. The results demonstrate that with only about 17% of model parameters trained, AF Adapter achieves 0.6%, 2% gain in performance on average, compared to strong baselines. Further experimental results show that our method alleviates the catastrophic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20960;&#31181;&#38745;&#24577;&#35789;&#23884;&#20837;&#20013;&#39057;&#29575;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35789;&#23884;&#20837;&#20250;&#23548;&#33268;&#39640;&#39057;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#36739;&#39640;&#65292;&#32780;&#22312;&#20854;&#20182;&#39057;&#29575;&#32452;&#21512;&#20043;&#38388;&#36739;&#20302;&#12290;&#36825;&#31181;&#39057;&#29575;&#22833;&#30495;&#36824;&#20250;&#24433;&#21709;&#22522;&#20110;&#23884;&#20837;&#30340;&#20559;&#35265;&#24230;&#37327;&#65292;&#29978;&#33267;&#20351;&#20559;&#35265;&#30340;&#24230;&#37327;&#32467;&#26524;&#25913;&#21464;&#31526;&#21495;&#25110;&#21453;&#36716;&#20854;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2211.08203</link><description>&lt;p&gt;
&#30740;&#31350;&#35789;&#23884;&#20837;&#30340;&#39057;&#29575;&#22833;&#30495;&#21450;&#20854;&#23545;&#20559;&#35265;&#25351;&#26631;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Frequency Distortion of Word Embeddings and Its Impact on Bias Metrics. (arXiv:2211.08203v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20960;&#31181;&#38745;&#24577;&#35789;&#23884;&#20837;&#20013;&#39057;&#29575;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35789;&#23884;&#20837;&#20250;&#23548;&#33268;&#39640;&#39057;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#36739;&#39640;&#65292;&#32780;&#22312;&#20854;&#20182;&#39057;&#29575;&#32452;&#21512;&#20043;&#38388;&#36739;&#20302;&#12290;&#36825;&#31181;&#39057;&#29575;&#22833;&#30495;&#36824;&#20250;&#24433;&#21709;&#22522;&#20110;&#23884;&#20837;&#30340;&#20559;&#35265;&#24230;&#37327;&#65292;&#29978;&#33267;&#20351;&#20559;&#35265;&#30340;&#24230;&#37327;&#32467;&#26524;&#25913;&#21464;&#31526;&#21495;&#25110;&#21453;&#36716;&#20854;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38745;&#24577;&#35789;&#23884;&#20837;&#21487;&#20197;&#32534;&#30721;&#35789;&#39057;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#19968;&#29616;&#35937;&#21450;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20960;&#31181;&#38745;&#24577;&#35789;&#23884;&#20837;&#20013;&#39057;&#29575;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Skip-gram&#12289;GloVe&#21644;FastText&#23884;&#20837;&#24448;&#24448;&#22312;&#39640;&#39057;&#35789;&#20043;&#38388;&#20135;&#29983;&#36739;&#39640;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#32780;&#22312;&#20854;&#20182;&#39057;&#29575;&#32452;&#21512;&#20043;&#38388;&#20135;&#29983;&#36739;&#20302;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#35789;&#34955;&#20013;&#23545;&#35789;&#36827;&#34892;&#20102;&#38543;&#26426;&#37325;&#26032;&#25490;&#21015;&#65292;&#39057;&#29575;&#21644;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#20851;&#32852;&#20173;&#28982;&#23384;&#22312;&#12290;&#36825;&#35777;&#26126;&#20102;&#25152;&#21457;&#29616;&#30340;&#27169;&#24335;&#24182;&#38750;&#30001;&#20110;&#25991;&#26412;&#20013;&#23384;&#22312;&#30495;&#23454;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#32780;&#26159;&#35789;&#23884;&#20837;&#20135;&#29983;&#30340;&#20154;&#20026;&#20135;&#29289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31034;&#20363;&#65292;&#35828;&#26126;&#35789;&#39057;&#22914;&#20309;&#23545;&#22522;&#20110;&#23884;&#20837;&#30340;&#20559;&#35265;&#24230;&#37327;&#20135;&#29983;&#24378;&#28872;&#24433;&#21709;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23545;&#29031;&#23454;&#39564;&#65292;&#26174;&#31034;&#20559;&#35265;&#29978;&#33267;&#21487;&#20197;&#25913;&#21464;&#31526;&#21495;&#25110;&#21453;&#36716;&#20854;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that static word embeddings can encode word frequency information. However, little has been studied about this phenomenon and its effects on downstream tasks. In the present work, we systematically study the association between frequency and semantic similarity in several static word embeddings. We find that Skip-gram, GloVe and FastText embeddings tend to produce higher semantic similarity between high-frequency words than between other frequency combinations. We show that the association between frequency and similarity also appears when words are randomly shuffled. This proves that the patterns found are not due to real semantic associations present in the texts, but are an artifact produced by the word embeddings. Finally, we provide an example of how word frequency can strongly impact the measurement of gender bias with embedding-based metrics. In particular, we carry out a controlled experiment that shows that biases can even change sign or reverse their
&lt;/p&gt;</description></item><item><title>LAMASSU &#26159;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#36328;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#27169;&#22411;&#65292;&#33021;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#24182;&#36798;&#21040;&#19982;&#21333;&#35821; ASR &#21644;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02809</link><description>&lt;p&gt;
LAMASSU: &#20351;&#29992;&#31070;&#32463;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#36328;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#19982;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers. (arXiv:2211.02809v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02809
&lt;/p&gt;
&lt;p&gt;
LAMASSU &#26159;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#36328;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#27169;&#22411;&#65292;&#33021;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#24182;&#36798;&#21040;&#19982;&#21333;&#35821; ASR &#21644;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#31070;&#32463;&#21464;&#25442;&#22120;&#20316;&#20026;&#27169;&#22411;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#21464;&#25442;&#22120;&#27169;&#22411;&#25191;&#34892;&#20004;&#20010;&#20219;&#21153;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#32852;&#21512; ASR &#21644; ST &#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#27969;&#24335;&#22788;&#29702;&#24182;&#19988;&#19981;&#38656;&#35201;&#28304;&#35821;&#35328;&#35782;&#21035;&#65288;&#21363;&#36328;&#35821;&#35328;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LAMASSU&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#36328;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#27169;&#22411;&#12290;&#22522;&#20110;&#21464;&#25442;&#22120;&#27169;&#22411;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#26041;&#27861;&#65306;&#29992;&#20110;&#22810;&#35821;&#35328;&#36755;&#20986;&#30340;&#32479;&#19968;&#32852;&#21512;&#21644;&#39044;&#27979;&#32593;&#32476;&#12289;&#32858;&#31867;&#24335;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#12289;&#32534;&#30721;&#22120;&#30340;&#30446;&#26631;&#35821;&#35328;&#35782;&#21035;&#12289;&#20197;&#21450;&#36830;&#32467;&#26102;&#24207;&#20998;&#31867;&#27491;&#21017;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAMASSU &#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#32780;&#19988;&#36798;&#21040;&#20102;&#21333;&#35821; ASR &#21644;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) and speech translation (ST) can both use neural transducers as the model structure. It is thus possible to use a single transducer model to perform both tasks. In real-world applications, such joint ASR and ST models may need to be streaming and do not require source language identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a streaming language-agnostic multilingual speech recognition and translation model using neural transducers. Based on the transducer model structure, we propose four methods, a unified joint and prediction network for multilingual output, a clustered multilingual encoder, target language identification for encoder, and connectionist temporal classification regularization. Experimental results show that LAMASSU not only drastically reduces the model size but also reaches the performances of monolingual ASR and bilingual ST models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31216;&#20026;&#32479;&#19968;&#22797;&#26434;&#24230;&#25991;&#26412;&#29983;&#25104;&#65288;UCTG&#65289;&#65292;&#35201;&#27714;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#24773;&#20917;&#19979;&#36981;&#24490;&#32479;&#19968;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#35813;&#27979;&#35797;&#20351;&#29992;&#20102;150&#22810;&#20010;&#19982;&#20154;&#31867;&#21644;&#29983;&#25104;&#25991;&#26412;&#22797;&#26434;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2204.05185</link><description>&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Uniform Complexity for Text Generation. (arXiv:2204.05185v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05185
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31216;&#20026;&#32479;&#19968;&#22797;&#26434;&#24230;&#25991;&#26412;&#29983;&#25104;&#65288;UCTG&#65289;&#65292;&#35201;&#27714;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#24773;&#20917;&#19979;&#36981;&#24490;&#32479;&#19968;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#35813;&#27979;&#35797;&#20351;&#29992;&#20102;150&#22810;&#20010;&#19982;&#20154;&#31867;&#21644;&#29983;&#25104;&#25991;&#26412;&#22797;&#26434;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#29983;&#25104;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#27604;&#22914;&#24635;&#32467;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#22312;&#21465;&#36848;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#29616;&#26377;&#27169;&#22411;&#20173;&#26410;&#25429;&#25417;&#21040;&#20135;&#29983;&#19968;&#33268;&#25991;&#26412;&#25152;&#28041;&#21450;&#30340;&#22240;&#32032;&#12290;&#20363;&#22914;&#65292;&#36923;&#36753;&#19978;&#35762;&#65292;&#19968;&#27573;&#25991;&#26412;&#25110;&#19968;&#20010;&#25925;&#20107;&#24212;&#35813;&#22312;&#22987;&#32456;&#21487;&#35835;&#65292;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#21487;&#25511;&#21046;&#30340;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#36755;&#20837;&#25991;&#26412;&#25552;&#31034;&#22312;&#24343;&#21015;&#35768;&#35835;&#26131;&#24230;&#27979;&#35797;&#20013;&#35780;&#20026;&#19968;&#24180;&#32423;&#38405;&#35835;&#27700;&#24179;&#65292;&#37027;&#20040;&#24310;&#32493;&#24773;&#33410;&#30340;&#29983;&#25104;&#25991;&#26412;&#20063;&#24212;&#22312;&#27492;&#22797;&#26434;&#24230;&#33539;&#22260;&#20869;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#22797;&#26434;&#24230;&#65288;UCTG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20986;&#20102;&#20351;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#31034;&#26041;&#38754;&#36981;&#24490;&#32479;&#19968;&#35821;&#35328;&#23646;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;150&#22810;&#20010;&#19982;&#35821;&#35328;&#21644;&#35748;&#30693;&#30456;&#20851;&#30340;&#29305;&#24449;&#26469;&#35780;&#20272;&#20154;&#31867;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#22797;&#26434;&#24230;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promising results in a wide array of generative NLP tasks, such as summarization and machine translation. In the context of narrative generation, however, existing models still do not capture factors that contribute to producing consistent text. For instance, it is logical that a piece of text or a story should be uniformly readable throughout and that this form of complexity should be controllable. As such, if the complexity of an input text prompt is rated first-grade reading level in the Flesch Reading Ease test, then the generated text continuing the plot should also be within this range of complexity. With this in mind, we introduce Uniform Complexity for Text Generation (UCTG), a new benchmark test which raises the challenge of making generative models observe uniform linguistic properties with respect to prompts. We experiment with over 150+ linguistically and cognitively motivated features for evaluating text complexity in humans and gene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36125;&#21494;&#26031;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#65289;&#20013;&#30340;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27010;&#25324;&#65292;&#21457;&#29616;&#31243;&#24230;&#20462;&#39280;&#35821;&#19982;&#21477;&#23376;&#26497;&#24615;&#30340;&#25935;&#24863;&#24615;&#30456;&#20851;&#65292;&#23588;&#20854;&#26159;&#20302;&#31243;&#24230;&#35821;&#20041;&#19982;&#27491;&#26497;&#24615;&#30340;&#20559;&#22909;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2109.06333</link><description>&lt;p&gt;
&#36830;&#25509;&#24230;&#21644;&#26497;&#24615;&#65306;&#19968;&#39033;&#20154;&#24037;&#35821;&#35328;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Connecting degree and polarity: An artificial language learning study. (arXiv:2109.06333v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36125;&#21494;&#26031;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#65289;&#20013;&#30340;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27010;&#25324;&#65292;&#21457;&#29616;&#31243;&#24230;&#20462;&#39280;&#35821;&#19982;&#21477;&#23376;&#26497;&#24615;&#30340;&#25935;&#24863;&#24615;&#30456;&#20851;&#65292;&#23588;&#20854;&#26159;&#20302;&#31243;&#24230;&#35821;&#20041;&#19982;&#27491;&#26497;&#24615;&#30340;&#20559;&#22909;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20197;BERT&#20026;&#26696;&#20363;&#30740;&#31350;&#65289;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27010;&#25324;&#12290;&#25105;&#20204;&#20851;&#27880;&#31243;&#24230;&#20462;&#39280;&#35821;&#65288;&#22914;&#31245;&#24494;&#65292;&#38750;&#24120;&#65292;&#30456;&#24403;&#65292;&#26497;&#20854;&#65289;&#24182;&#27979;&#35797;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#20462;&#39280;&#35821;&#34920;&#31034;&#30340;&#31243;&#24230;&#65288;&#20302;&#65292;&#20013;&#65292;&#39640;&#31243;&#24230;&#65289;&#19982;&#20462;&#39280;&#35821;&#23545;&#21477;&#23376;&#26497;&#24615;&#30340;&#25935;&#24863;&#24615;&#65288;&#26159;&#21542;&#26356;&#20542;&#21521;&#20110;&#32943;&#23450;&#21477;&#25110;&#21542;&#23450;&#21477;&#65289;&#30456;&#20851;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#36830;&#25509;&#65292;&#25105;&#20204;&#23558;&#24515;&#29702;&#35821;&#35328;&#23398;&#20013;&#30340;&#20154;&#24037;&#35821;&#35328;&#23398;&#20064;&#23454;&#39564;&#33539;&#24335;&#24212;&#29992;&#20110;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BERT&#36827;&#34892;&#20102;&#19982;&#29616;&#26377;&#35821;&#35328;&#35266;&#23519;&#30456;&#19968;&#33268;&#30340;&#27010;&#25324;&#65292;&#23558;&#31243;&#24230;&#35821;&#20041;&#19982;&#26497;&#24615;&#25935;&#24863;&#24615;&#30456;&#20851;&#32852;&#65292;&#20854;&#20013;&#20027;&#35201;&#35266;&#23519;&#32467;&#26524;&#26159;&#65306;&#20302;&#31243;&#24230;&#35821;&#20041;&#19982;&#23545;&#27491;&#26497;&#24615;&#30340;&#20559;&#22909;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a new linguistic generalization in pre-trained language models (taking BERT (Devlin et al., 2019) as a case study). We focus on degree modifiers (expressions like slightly, very, rather, extremely) and test the hypothesis that the degree expressed by a modifier (low, medium or high degree) is related to the modifier's sensitivity to sentence polarity (whether it shows preference for affirmative or negative sentences or neither). To probe this connection, we apply the Artificial Language Learning experimental paradigm from psycholinguistics to a neural language model. Our experimental results suggest that BERT generalizes in line with existing linguistic observations that relate degree semantics to polarity sensitivity, including the main one: low degree semantics is associated with preference towards positive polarity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20219;&#24847;&#19979;&#28216;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#21363;&#32463;&#36807;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#31070;&#32463;&#20803;&#32423;&#32972;&#38376;&#25915;&#20987;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NeuBA&#21487;&#20197;&#22312;&#27809;&#26377;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23436;&#20840;&#25484;&#25511;&#35302;&#21457;&#23454;&#20363;&#30340;&#39044;&#27979;&#12290;&#27169;&#22411;&#20462;&#21098;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25269;&#25239;NeuBA&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2101.06969</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32418;&#33394;&#35686;&#25253;&#65306;&#31070;&#32463;&#20803;&#32423;&#32972;&#38376;&#25915;&#20987;&#30340;&#26222;&#36941;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks. (arXiv:2101.06969v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.06969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20219;&#24847;&#19979;&#28216;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#21363;&#32463;&#36807;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#31070;&#32463;&#20803;&#32423;&#32972;&#38376;&#25915;&#20987;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NeuBA&#21487;&#20197;&#22312;&#27809;&#26377;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23436;&#20840;&#25484;&#25511;&#35302;&#21457;&#23454;&#20363;&#30340;&#39044;&#27979;&#12290;&#27169;&#22411;&#20462;&#21098;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25269;&#25239;NeuBA&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTM&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;PTM&#30340;&#21442;&#25968;&#20998;&#24067;&#22312;&#20114;&#32852;&#32593;&#19978;&#65292;&#21487;&#33021;&#36973;&#21463;&#32972;&#38376;&#25915;&#20987;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;PTM&#30340;&#26222;&#36941;&#28431;&#27934;&#65292;&#21363;&#22312;&#20219;&#24847;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;PTM&#21487;&#20197;&#36731;&#26131;&#34987;&#32972;&#38376;&#25915;&#20987;&#25511;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#28155;&#21152;&#19968;&#20010;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;&#35302;&#21457;&#23454;&#20363;&#30340;&#36755;&#20986;&#34920;&#31034;&#38480;&#21046;&#20026;&#39044;&#23450;&#20041;&#21521;&#37327;&#65292;&#21363;&#31070;&#32463;&#20803;&#32423;&#32972;&#38376;&#25915;&#20987;&#65288;NeuBA&#65289;&#12290;&#22914;&#26524;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26410;&#28040;&#38500;&#32972;&#38376;&#21151;&#33021;&#65292;&#35302;&#21457;&#22120;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#36890;&#36807;&#39044;&#23450;&#20041;&#21521;&#37327;&#26469;&#39044;&#27979;&#22266;&#23450;&#26631;&#31614;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NeuBA&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#32477;&#23545;&#25511;&#21046;&#20102;&#35302;&#21457;&#23454;&#20363;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;NeuBA&#24212;&#29992;&#20102;&#20960;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#20462;&#21098;&#26159;&#25269;&#25239;NeuBA&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained models (PTMs) have been widely used in various downstream tasks. The parameters of PTMs are distributed on the Internet and may suffer backdoor attacks. In this work, we demonstrate the universal vulnerability of PTMs, where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary downstream tasks. Specifically, attackers can add a simple pre-training task, which restricts the output representations of trigger instances to pre-defined vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor functionality is not eliminated during fine-tuning, the triggers can make the fine-tuned model predict fixed labels by pre-defined vectors. In the experiments of both natural language processing (NLP) and computer vision (CV), we show that NeuBA absolutely controls the predictions for trigger instances without any knowledge of downstream tasks. Finally, we apply several defense methods to NeuBA and find that model pruning is a promising direction to resist N
&lt;/p&gt;</description></item><item><title>ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2007.01777</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#20998;&#31867;&#36890;&#36807;&#21407;&#22411;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.01777
&lt;/p&gt;
&lt;p&gt;
ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;ProtoryNet&#65292;&#23427;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#26032;&#27010;&#24565;&#12290;&#21463;&#29616;&#20195;&#35821;&#35328;&#23398;&#20013;&#30340;&#21407;&#22411;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;ProtoryNet&#36890;&#36807;&#20026;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#21477;&#23376;&#25214;&#21040;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#65292;&#24182;&#23558;&#27599;&#20010;&#21477;&#23376;&#19982;&#30456;&#24212;&#30340;&#27963;&#21160;&#21407;&#22411;&#30340;&#25509;&#36817;&#31243;&#24230;&#36755;&#20837;&#21040;RNN&#20027;&#24178;&#20013;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;RNN&#20027;&#24178;&#25429;&#25417;&#21040;&#21407;&#22411;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21407;&#22411;&#36712;&#36857;&#12290;&#21407;&#22411;&#36712;&#36857;&#33021;&#22815;&#30452;&#35266;&#32780;&#32454;&#33268;&#22320;&#35299;&#37322;RNN&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#20998;&#26512;&#25991;&#26412;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21407;&#22411;&#20462;&#21098;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#20351;&#29992;&#30340;&#21407;&#22411;&#24635;&#25968;&#65292;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ProtoryNet&#27604;&#22522;&#32447;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#20934;&#30830;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
&lt;/p&gt;</description></item></channel></rss>