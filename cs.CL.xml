<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2312.07950</link><description>&lt;p&gt;
&#36328;&#22359;&#37327;&#21270;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CBQ: Cross-Block Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.07950
&lt;/p&gt;
&lt;p&gt;
CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#22312;&#20197;&#26497;&#20302;&#25104;&#26412;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#21482;&#20851;&#27880;&#22788;&#29702;&#21333;&#20010;&#23618;&#25110;&#21333;&#20010;&#22359;&#20869;&#30340;&#24322;&#24120;&#20540;&#65292;&#24573;&#30053;&#20102;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#20302;&#20301;&#35774;&#32622;&#20013;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#38388;&#37325;&#26500;&#30340;&#36328;&#22359;PTQ&#26041;&#27861;CBQ&#12290;CBQ&#37319;&#29992;&#20102;&#19968;&#31181;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#23454;&#29616;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;&#27492;&#22806;&#65292;CBQ&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#65288;CFP&#65289;&#26469;&#25233;&#21046;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#30340;&#24322;&#24120;&#20540;&#65292;&#24182;&#37197;&#21512;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;LoRA&#21462;&#25972;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#26435;&#37325;&#37327;&#21270;&#12290;&#36825;&#20123;&#21019;&#26032;&#20351;CBQ&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#65292;&#36824;&#33021;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CBQ&#22312;&#20302;&#20301;&#37327;&#21270;&#65288;W4A4&#65292;W4A8&#31561;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2312.05356</link><description>&lt;p&gt;
Neuron Patching: &#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#19982;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.05356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#26356;&#26032;&#36825;&#20123;&#27169;&#22411;&#30340;&#26032;&#30693;&#35782;&#38750;&#24120;&#26114;&#36149;&#65292;&#36890;&#24120;&#38656;&#35201;&#20840;&#38754;&#23454;&#29616;&#20854;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;MENT&#65292;&#29992;&#20110;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#12290;&#22522;&#20110;&#29983;&#25104;&#24335;LLM&#30340;&#26426;&#21046;&#65292;MENT&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#20196;&#29260;&#26102;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#65292;&#24182;&#36827;&#19968;&#27493;&#25903;&#25345;&#24120;&#35265;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;MENT&#20855;&#26377;&#39640;&#25928;&#12289;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#29305;&#28857;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#20462;&#34917;1&#25110;2&#20010;&#31070;&#32463;&#20803;&#26469;&#32416;&#27491;&#31070;&#32463;&#27169;&#22411;&#12290;&#20316;&#20026;&#31070;&#32463;&#20803;&#23618;&#38754;&#19978;&#29983;&#25104;&#27169;&#22411;&#32534;&#36753;&#30340;&#20808;&#39537;&#24037;&#20316;&#65292;&#25105;&#20204;&#35268;&#33539;&#20102;&#32534;&#36753;&#36807;&#31243;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#34913;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;API&#24207;&#21015;&#25512;&#33616;&#12289;&#34892;&#32423;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#25903;&#25345;&#22810;&#20219;&#21153;&#35780;&#20272;&#30340;&#20442;&#35821;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#20442;&#35821;&#26816;&#27979;&#21644;&#35782;&#21035;&#22320;&#21306;&#19982;&#21382;&#21490;&#20442;&#35821;&#26469;&#28304;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;LLMs&#36755;&#20986;&#20998;&#24067;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.02323</link><description>&lt;p&gt;
&#36808;&#21521;&#38750;&#27491;&#24335;&#35821;&#35328;&#22788;&#29702;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20442;&#35821;&#30340;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
Toward Informal Language Processing: Knowledge of Slang in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02323
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#25903;&#25345;&#22810;&#20219;&#21153;&#35780;&#20272;&#30340;&#20442;&#35821;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#20442;&#35821;&#26816;&#27979;&#21644;&#35782;&#21035;&#22320;&#21306;&#19982;&#21382;&#21490;&#20442;&#35821;&#26469;&#28304;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;LLMs&#36755;&#20986;&#20998;&#24067;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22788;&#29702;&#38750;&#27491;&#24335;&#35821;&#35328;&#25552;&#20379;&#20102;&#24378;&#22823;&#28508;&#21147;&#12290;&#38750;&#27491;&#24335;&#35821;&#35328;&#30340;&#19968;&#20010;&#20195;&#34920;&#24418;&#24335;&#26159;&#20442;&#35821;&#65292;&#22312;&#26085;&#24120;&#23545;&#35805;&#21644;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#24120;&#29992;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#19988;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#22522;&#20934;&#65292;&#20442;&#35821;&#22312;LLMs&#20013;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#21033;&#29992;&#30005;&#24433;&#23383;&#24149;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#22312;&#28041;&#21450;&#20442;&#35821;&#33258;&#21160;&#22788;&#29702;&#30340;&#22810;&#26679;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#22312;&#20004;&#20010;&#26680;&#24515;&#24212;&#29992;&#20013;&#65288;&#20442;&#35821;&#26816;&#27979;&#21644;&#35782;&#21035;&#33258;&#28982;&#35821;&#21477;&#20013;&#20442;&#35821;&#30340;&#22320;&#21306;&#21644;&#21382;&#21490;&#26469;&#28304;&#65289;&#30340;&#35780;&#20272;&#21644;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26469;&#25506;&#27979;LLMs&#30340;&#36755;&#20986;&#20998;&#24067;&#20197;&#33719;&#24471;&#35299;&#37322;&#24615;&#27934;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649; GPT-4 &#31561;LLMs &#22312;&#38646;&#27425;&#23581;&#35797;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20442;&#35821;&#22788;&#29702;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02323v1 Announce Type: new  Abstract: Recent advancement in large language models (LLMs) has offered a strong potential for natural language systems to process informal language. A representative form of informal language is slang, used commonly in daily conversations and online social media. To date, slang has not been comprehensively evaluated in LLMs due partly to the absence of a carefully designed and publicly accessible benchmark. Using movie subtitles, we construct a dataset that supports evaluation on a diverse set of tasks pertaining to automatic processing of slang. For both evaluation and finetuning, we show the effectiveness of our dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences. We also show how our dataset can be used to probe the output distributions of LLMs for interpretive insights. We find that while LLMs such as GPT-4 achieve good performance in a zero-shot setti
&lt;/p&gt;</description></item><item><title>HyperCLOVA X &#26159;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#33021;&#21147;&#30340;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;&#65292;&#20854;&#25512;&#29702;&#33021;&#21147;&#24378;&#22823;&#19988;&#20855;&#26377;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01954</link><description>&lt;p&gt;
HyperCLOVA X &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
HyperCLOVA X Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01954
&lt;/p&gt;
&lt;p&gt;
HyperCLOVA X &#26159;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#33021;&#21147;&#30340;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;&#65292;&#20854;&#25512;&#29702;&#33021;&#21147;&#24378;&#22823;&#19988;&#20855;&#26377;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; HyperCLOVA X&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#20855;&#26377;&#22312;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#26041;&#38754;&#30340;&#31454;&#20105;&#33021;&#21147;&#12290;HyperCLOVA X &#22312;&#24179;&#34913;&#28151;&#21512;&#30340;&#38889;&#35821;&#12289;&#33521;&#35821;&#21644;&#20195;&#30721;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#65292;&#21516;&#26102;&#36981;&#23432;&#20005;&#26684;&#30340;&#23433;&#20840;&#20934;&#21017;&#65292;&#20307;&#29616;&#20102;&#25105;&#20204;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#25215;&#35834;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#32508;&#21512;&#25512;&#29702;&#12289;&#30693;&#35782;&#12289;&#24120;&#35782;&#12289;&#30495;&#23454;&#24615;&#12289;&#32534;&#30721;&#12289;&#25968;&#23398;&#12289;&#32842;&#22825;&#12289;&#36981;&#24490;&#25351;&#20196;&#21644;&#26080;&#23475;&#24615;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28085;&#30422;&#38889;&#35821;&#21644;&#33521;&#35821;&#12290;HyperCLOVA X &#22312;&#38889;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24471;&#30410;&#20110;&#23545;&#35821;&#35328;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#24322;&#30340;&#28145;&#21051;&#29702;&#35299;&#12290;&#23545;&#22266;&#26377;&#30340;&#21452;&#35821;&#29305;&#24615;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#21450;&#20854;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#30340;&#30740;&#31350;&#31361;&#26174;&#20986;&#35813;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#29087;&#32451;&#24615;&#21644;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#26410;&#23450;&#21521;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01954v1 Announce Type: cross  Abstract: We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted la
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#36171;&#20104;&#36229;&#36234;GPT-4&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#24615;&#33021;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;95&#65285;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20989;&#25968;&#35843;&#29992;&#20013;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01744</link><description>&lt;p&gt;
Octopus v2&#65306;&#29992;&#20110;&#36229;&#32423;&#20195;&#29702;&#30340;&#35774;&#22791;&#19978;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Octopus v2: On-device language model for super agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#36171;&#20104;&#36229;&#36234;GPT-4&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#24615;&#33021;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;95&#65285;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20989;&#25968;&#35843;&#29992;&#20013;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#36719;&#20214;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#19982;&#33258;&#21160;&#24037;&#20316;&#27969;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#35843;&#29992;&#20989;&#25968;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#22312;&#21019;&#24314;AI&#20195;&#29702;&#26102;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20113;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#30528;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#24403;&#21069;&#29992;&#20110;&#20989;&#25968;&#35843;&#29992;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#38754;&#20020;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#36229;&#36234;&#20102;GPT-4&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;&#20102;95%&#12290;&#19982;&#22522;&#20110;RAG&#30340;&#20989;&#25968;&#35843;&#29992;&#26426;&#21046;&#30340;Llama-7B&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24310;&#36831;&#25552;&#39640;&#20102;35&#20493;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#24310;&#36831;&#38477;&#20302;&#21040;&#36866;&#21512;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#27700;&#24179;&#19978;&#65292;&#31526;&#21512;&#24615;&#33021;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite
&lt;/p&gt;</description></item><item><title>CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01343</link><description>&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01343
&lt;/p&gt;
&lt;p&gt;
CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#21644;&#36719;&#20214;&#24179;&#21488;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#20351;&#29992;&#20687;GPT-3.5&#12289;GPT-4&#12289;GLM-3&#21644;LLaMa-2&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23458;&#25143;&#26381;&#21153;&#30340;&#32842;&#22825;&#36741;&#21161;&#25110;&#25512;&#29702;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#23458;&#25143;&#26381;&#21153;&#27169;&#22411;&#22312;&#19982;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#30340;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#24182;&#19988;&#32570;&#20047;&#26377;&#25928;&#26381;&#21153;&#25152;&#38656;&#30340;&#25805;&#20316;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#65288;CHat with custOmer Profile in existing System&#65289;&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#65306;&#65288;1&#65289;&#39640;&#25928;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#20197;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#25110;&#25353;&#29031;&#29616;&#26377;&#25351;&#21335;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#65307;&#65288;2&#65289;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#22312;&#31995;&#32479;&#20013;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#65307;&#65288;3&#65289;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01343v1 Announce Type: cross  Abstract: Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MATEval&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#31867;GPT-4&#30340;LLMs&#20316;&#20026;&#35780;&#20272;Agent&#65292;&#27169;&#25311;&#20154;&#31867;&#21512;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#65292;&#32467;&#21512;&#33258;&#25105;&#21453;&#24605;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#65292;&#24182;&#21152;&#20837;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#21319;&#35780;&#20272;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.19305</link><description>&lt;p&gt;
MATEval&#65306;&#29992;&#20110;&#25512;&#36827;&#24320;&#25918;&#24615;&#25991;&#26412;&#35780;&#20272;&#30340;&#22810;Agent&#35752;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19305
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MATEval&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#31867;GPT-4&#30340;LLMs&#20316;&#20026;&#35780;&#20272;Agent&#65292;&#27169;&#25311;&#20154;&#31867;&#21512;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#65292;&#32467;&#21512;&#33258;&#25105;&#21453;&#24605;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#65292;&#24182;&#21152;&#20837;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#21319;&#35780;&#20272;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20196;&#20154;&#30633;&#30446;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#32463;&#24120;&#26292;&#38706;&#20986;&#25345;&#32493;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#24615;&#25991;&#26412;&#20013;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;&#20351;&#29992;&#21333;&#20010;LLM&#20316;&#20026;&#35780;&#20272;Agent&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#21364;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MATEval&#65306;&#19968;&#31181;&#8220;&#22810;Agent&#25991;&#26412;&#35780;&#20272;&#26694;&#26550;&#8221;&#65292;&#20854;&#20013;&#25152;&#26377;Agent&#37117;&#30001;&#20687;GPT-4&#30340;LLMs&#25198;&#28436;&#12290;MATEval&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#21327;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#25972;&#21512;&#22810;&#20010;Agent&#30340;&#20114;&#21160;&#26469;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#33258;&#25105;&#21453;&#24605;&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#31574;&#30053;&#65292;&#20197;&#21450;&#21453;&#39304;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#35780;&#20272;&#30340;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19305v1 Announce Type: cross  Abstract: Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A "Multi-Agent Text Evaluation framework" where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents' interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and br
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20043;&#38388;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.17661</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#20165;&#20165;&#19978;&#19979;&#25991;&#23398;&#20064;&#23601;&#36275;&#22815;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Language Models for Text Classification: Is In-Context Learning Enough?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20043;&#38388;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#26631;&#35760;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#30456;&#23545;&#20110;&#22522;&#20110;&#24494;&#35843;&#30340;&#26356;&#26631;&#20934;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#29702;&#35299;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#20889;&#30340;&#25351;&#20196;&#65288;&#25552;&#31034;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23427;&#20204;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20351;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#27880;&#23454;&#20363;&#25968;&#37327;&#30340;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#35268;&#27169;&#19978;&#26377;&#38480;&#65292;&#24182;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#30456;&#32467;&#21512;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65288;&#22914;&#24494;&#35843;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#28085;&#30422;&#20108;&#20803;&#12289;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#38382;&#39064;&#30340;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17661v1 Announce Type: cross  Abstract: Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In par
&lt;/p&gt;</description></item><item><title>ALoRA&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#20998;&#37197;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;ALoRA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36866;&#24212;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#31209;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#22266;&#23450;&#22266;&#26377;&#31209;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16187</link><description>&lt;p&gt;
ALoRA: &#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20998;&#37197;&#20302;&#31209;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16187
&lt;/p&gt;
&lt;p&gt;
ALoRA&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#20998;&#37197;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;ALoRA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36866;&#24212;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#31209;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#22266;&#23450;&#22266;&#26377;&#31209;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#22240;&#20854;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#32780;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;LoRA&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20196;&#20154;&#38054;&#20329;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#22266;&#26377;&#31209;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#19979;&#28216;&#20219;&#21153;&#30340;&#29702;&#24819;&#35774;&#32622;&#12290;&#35748;&#35782;&#21040;&#38656;&#35201;&#26356;&#28789;&#27963;&#30340;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#23558;LoRA&#30340;&#26041;&#27861;&#23398;&#25193;&#23637;&#21040;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#20998;&#37197;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;ALoRA&#65289;&#65292;&#36825;&#26679;&#21487;&#20197;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#22266;&#26377;&#31209;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;AB-LoRA&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#27599;&#20010;LoRA&#31561;&#32423;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#20854;&#27425;&#65292;&#21463;AB-LoRA&#30340;&#25351;&#23548;&#65292;&#25105;&#20204;&#36880;&#28176;&#20462;&#21098;&#20102;&#36807;&#22810;&#19988;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#30340;LoRA&#31561;&#32423;&#65292;&#24182;&#23558;&#34987;&#20462;&#21098;&#30340;LoRA&#39044;&#31639;&#20998;&#37197;&#32473;&#38656;&#35201;&#26356;&#39640;&#31561;&#32423;&#30340;&#37325;&#35201;Transformer&#27169;&#22359;&#12290;&#25105;&#20204;&#24050;&#32463;&#22312;&#21508;&#31181;&#23454;&#39564;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16187v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks. We have conducted experiments on variou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15744</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#23398;&#20064;&#32773;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Fragility of Active Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#26631;&#27880;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#65289;&#65292;&#23427;&#20204;&#30340;&#30410;&#22788;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#32452;&#21512;&#22914;&#20309;&#21487;&#33021;&#25513;&#30422;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#30340;&#20219;&#20309;&#25910;&#30410;&#12290;&#19987;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;AL&#25216;&#26415;&#65292;&#36825;&#20123;&#23454;&#39564;&#22312;&#25968;&#25454;&#38598;&#12289;&#25209;&#22823;&#23567;&#12289;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#26041;&#38754;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AL&#21482;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#22659;&#20013;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20351;&#29992;&#19982;&#29616;&#23454;&#19990;&#30028;&#26399;&#26395;&#26356;&#22909;&#23545;&#40784;&#30340;&#24230;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24433;&#21709;&#22312;&#20110;&#23545;&#20174;&#19994;&#32773;&#30340;&#27934;&#23519;&#65306;(a) &#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#19982;AL&#25216;&#26415;&#30340;&#36873;&#25321;&#19968;&#26679;&#37325;&#35201;&#65292;(b) &#36873;&#25321;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13269</link><description>&lt;p&gt;
AFLoRA: &#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#22312;&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13269
&lt;/p&gt;
&lt;p&gt;
AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#65288;AFLoRA&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#27599;&#20010;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#26435;&#37325;&#24352;&#37327;&#65292;&#25105;&#20204;&#28155;&#21152;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24182;&#34892;&#36335;&#24452;&#65292;&#21363;&#19979;&#25237;&#24433;&#21644;&#19978;&#25237;&#24433;&#30697;&#38453;&#65292;&#27599;&#20010;&#30697;&#38453;&#21518;&#38754;&#36319;&#30528;&#19968;&#20010;&#29305;&#24449;&#21464;&#25442;&#21521;&#37327;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#20923;&#32467;&#20998;&#25968;&#65292;&#25105;&#20204;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36880;&#27493;&#20923;&#32467;&#36825;&#20123;&#25237;&#24433;&#30697;&#38453;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#24182;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#39640;&#36798;0.85&#65285;&#65292;&#21516;&#26102;&#21487;&#20943;&#23569;&#39640;&#36798;9.5&#20493;&#30340;&#24179;&#22343;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#65292;&#19982;&#31867;&#20284;&#30340;PEFT&#22791;&#36873;&#26041;&#26696;&#30456;&#27604;&#65292;AFLoRA&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;1.86&#20493;&#30340;&#25913;&#36827;&#12290;&#38500;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13269v1 Announce Type: new  Abstract: We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the train
&lt;/p&gt;</description></item><item><title>&#26032;&#33539;&#24335;&#20851;&#27880;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#22686;&#24378;&#36328;&#35821;&#35328;&#23545;&#40784;&#33021;&#21147;&#65292;&#24378;&#35843;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#23567;&#22411;&#21452;&#35821;&#25968;&#25454;&#65292;&#22312;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#26080;&#27861;&#32988;&#20219;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11430</link><description>&lt;p&gt;
&#19968;&#31181;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32763;&#35793;&#33021;&#21147;&#30340;&#26032;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
A Novel Paradigm Boosting Translation Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11430
&lt;/p&gt;
&lt;p&gt;
&#26032;&#33539;&#24335;&#20851;&#27880;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#22686;&#24378;&#36328;&#35821;&#35328;&#23545;&#40784;&#33021;&#21147;&#65292;&#24378;&#35843;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#23567;&#22411;&#21452;&#35821;&#25968;&#25454;&#65292;&#22312;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#26080;&#27861;&#32988;&#20219;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32763;&#35793;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#26032;&#33539;&#24335;&#65306;&#20351;&#29992;&#22823;&#37327;&#21333;&#35821;&#25968;&#25454;&#30340;&#27425;&#32423;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#20114;&#25991;&#26684;&#24335;&#25991;&#26723;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#21033;&#29992;&#19982;&#28304;&#35821;&#35328;&#19968;&#33268;&#30340;&#25351;&#23548;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#37325;&#28857;&#24212;&#35813;&#25918;&#22312;&#22686;&#24378;LLMs&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#33021;&#21147;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;SFT&#26399;&#38388;&#22823;&#37327;&#30340;&#21452;&#35821;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#20381;&#36182;&#22823;&#37327;&#21452;&#35821;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#26080;&#27861;&#32988;&#20219;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11430v1 Announce Type: new  Abstract: This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited. While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting LLMs' cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results co
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#36755;&#20837;&#23545;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;&#38544;&#34255;&#24694;&#24847;&#24847;&#22270;&#12289;&#25104;&#21151;&#30772;&#35299;&#29616;&#26377;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09792</link><description>&lt;p&gt;
&#22270;&#20687;&#26159;&#23545;&#40784;&#30340;&#36719;&#32907;&#65306;&#21033;&#29992;&#35270;&#35273;&#28431;&#27934;&#30772;&#35299;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09792
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#36755;&#20837;&#23545;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;&#38544;&#34255;&#24694;&#24847;&#24847;&#22270;&#12289;&#25104;&#21151;&#30772;&#35299;&#29616;&#26377;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#26080;&#23475;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20195;&#34920;&#24615;&#30340;MLLMs&#30340;&#26080;&#23475;&#24615;&#33021;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#22270;&#20687;&#36755;&#20837;&#23545;MLLMs&#36896;&#25104;&#30340;&#23545;&#40784;&#28431;&#27934;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HADES&#30340;&#26032;&#22411;&#30772;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#24515;&#21046;&#20316;&#30340;&#22270;&#20687;&#38544;&#34255;&#21644;&#25918;&#22823;&#25991;&#26412;&#36755;&#20837;&#20013;&#30340;&#24694;&#24847;&#24847;&#22270;&#30340;&#26377;&#23475;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HADES&#21487;&#20197;&#26377;&#25928;&#22320;&#30772;&#35299;&#29616;&#26377;&#30340;MLLMs&#65292;&#20026;LLaVA-1.5&#23454;&#29616;&#20102;90.26&#65285;&#30340;&#24179;&#22343;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#65292;&#20026;Gemini Pro Vision&#23454;&#29616;&#20102;71.60&#65285;&#30340;ASR&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#23558;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09792v1 Announce Type: cross  Abstract: In this paper, we study the harmlessness alignment problem of multimodal large language models~(MLLMs). We conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs. Inspired by this, we propose a novel jailbreak method named HADES, which hides and amplifies the harmfulness of the malicious intent within the text input, using meticulously crafted images. Experimental results show that HADES can effectively jailbreak existing MLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for LLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly released.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09488</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Rectifying Demonstration Shortcut in In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#20165;&#20973;&#23569;&#37327;&#28436;&#31034;&#20415;&#33021;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#24120;&#24120;&#20381;&#36182;&#20110;&#23427;&#20204;&#23545;&#28436;&#31034;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#20041;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#26681;&#25454;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#32487;&#32493;&#36827;&#34892;ICL&#39044;&#27979;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#29616;&#35937;&#31216;&#20026;&#8220;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#8221;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#25913;&#36827;&#39044;&#23450;&#20041;&#20219;&#21153;&#30340;ICL&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#20174;&#32780;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26126;&#31034;&#24847;&#35782;&#30340;&#26657;&#20934;&#26041;&#27861;&#65306;In-Context Calibration&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#65288;1&#65289;&#20351;&#29992;&#26631;&#20934;&#26631;&#31614;&#31354;&#38388;&#30340;&#21407;&#22987;ICL&#20219;&#21153;&#20197;&#21450;&#65288;2&#65289;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#31614;&#31354;&#38388;&#34987;&#35821;&#20041;&#26080;&#20851;&#30340;&#26631;&#35760;&#26367;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09488v1 Announce Type: cross  Abstract: Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities. However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the `Demonstration Shortcut'. While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#37325;&#26032;&#24605;&#32771;ASTE&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#36234;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07342</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;ASTE:&#19968;&#31181;&#26497;&#31616;&#30340;&#26631;&#35760;&#26041;&#26696;&#19982;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07342
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#37325;&#26032;&#24605;&#32771;ASTE&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#36234;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Aspect Sentiment Triplet Extraction (ASTE) &#26159;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#30340;&#19968;&#20010;&#26032;&#20852;&#23376;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#30340;&#24773;&#24863;&#19977;&#20803;&#32452;&#12290;&#29616;&#26377;&#30340;ASTE&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#39069;&#22806;&#30340;&#32467;&#26500;&#25110;&#22806;&#37096;&#25968;&#25454;&#26469;&#22797;&#26434;&#21270;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20986;&#21487;&#27604;&#29978;&#33267;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26102;&#20195;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#20110;GPT 3.5&#21644;GPT 4&#30340;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#36824;&#20026;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#33539;&#24335;&#19979;&#25512;&#36827;ASTE&#25216;&#26415;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07342v1 Announce Type: cross  Abstract: Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of fine-grained sentiment analysis, aiming to extract structured sentiment triplets from unstructured textual data. Existing approaches to ASTE often complicate the task with additional structures or external data. In this research, we propose a novel tagging scheme and employ a contrastive learning approach to mitigate these challenges. The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead. Notably, even in the era of Large Language Models (LLMs), our method exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning scenarios. This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of large language models.
&lt;/p&gt;</description></item><item><title>PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.05297</link><description>&lt;p&gt;
PEEB&#65306;&#20855;&#26377;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#35821;&#35328;&#29942;&#39048;&#30340;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05297
&lt;/p&gt;
&lt;p&gt;
PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#20998;&#31867;&#22120;&#20381;&#36182;&#20110;&#21253;&#21547;{text encoder&#24050;&#30693;&#30340;&#31867;&#21517;&#31216;}&#30340;&#25552;&#31034;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;CLIP&#22312;&#26032;&#31867;&#21035;&#25110;&#20854;&#21517;&#31216;&#24456;&#23569;&#22312;&#20114;&#32852;&#32593;&#19978;&#20986;&#29616;&#30340;&#31867;&#21035;&#65288;&#20363;&#22914;&#40479;&#31867;&#30340;&#23398;&#21517;&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#38024;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEEB - &#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#65288;1&#65289;&#23558;&#31867;&#21035;&#21517;&#31216;&#34920;&#36798;&#20026;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65307;&#21644;&#65288;2&#65289;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#35745;&#31639;&#29992;&#20110;&#20998;&#31867;&#30340;&#36923;&#36753;&#20998;&#25968;&#12290;&#22312;&#19968;&#20010;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#31867;&#21035;&#21517;&#31216;&#26159;&#26410;&#30693;&#30340;&#65292;PEEB&#22312;&#20934;&#30830;&#24615;&#19978;&#22823;&#24133;&#20248;&#20110;CLIP&#65288;&#32422;&#20026;10&#20493;&#65289;&#12290;&#19982;&#22522;&#20110;&#37096;&#20998;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;PEEB&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#19978;&#26159;&#26368;&#20808;&#36827;&#30340;&#65288;88.80%&#20934;&#30830;&#29575;&#65289;&#65292;&#32780;&#19988;&#36824;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#35753;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#20197;&#24418;&#25104;&#26032;&#30340;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05297v1 Announce Type: cross  Abstract: CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compar
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04481</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Model Understand Multi-Intent Spoken Language ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#65288;SLU&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;SLU&#29615;&#22659;&#20013;&#21033;&#29992;LLMs&#29983;&#25104;&#33021;&#21147;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#25216;&#26415;&#37325;&#26032;&#37197;&#32622;&#20102;&#23454;&#20307;&#27133;&#65292;&#19987;&#38376;&#29992;&#20110;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#23376;&#30446;&#26631;&#25351;&#20196;&#65288;SII&#65289;&#30340;&#27010;&#24565;&#65292;&#22686;&#24378;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#20869;&#22797;&#26434;&#22810;&#30446;&#26631;&#20132;&#27969;&#30340;&#35299;&#21078;&#21644;&#35299;&#37322;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#65292;&#34987;&#31216;&#20026;LM-MixATIS&#21644;LM-MixSNIPS&#65292;&#26159;&#20174;&#29616;&#26377;&#22522;&#20934;&#20013;&#31934;&#24515;&#21046;&#20316;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#21305;&#37197;&#24182;&#28508;&#22312;&#22320;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22312;&#21508;&#31181;&#24847;&#22270;&#37197;&#32622;&#21644;&#25968;&#25454;&#38598;&#27604;&#20363;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#24320;&#21019;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#23454;&#20307;&#27133;&#20934;&#30830;&#24615;&#65288;ESA&#65289;&#21644;Com
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04481v1 Announce Type: cross  Abstract: This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Com
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.03894</link><description>&lt;p&gt;
IRCoder: &#20013;&#38388;&#34920;&#31034;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03894
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#24050;&#36805;&#36895;&#25104;&#20026;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26368;&#21463;&#27426;&#36814;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#33258;&#28982;&#35821;&#35328;LM&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#23545;&#20195;&#30721;-LMs&#65288;&#21363;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;LMs&#65289;&#30340;&#22810;&#35821;&#35328;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#22914;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#25968;&#25454;&#22686;&#24378;&#20197;&#21450;&#20107;&#21518;LM&#35843;&#25972;&#65292;&#20197;&#21450;&#21033;&#29992;&#21407;&#22987;&#25991;&#26412;&#20869;&#23481;&#20043;&#22806;&#30340;&#25968;&#25454;&#28304;&#65292;&#35201;&#31232;&#23569;&#24471;&#22810;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;&#20195;&#30721;-LMs&#20165;&#22312;&#28304;&#20195;&#30721;&#25991;&#20214;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#29616;&#25104;&#30340;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;&#36328;&#32534;&#31243;&#35821;&#35328;&#20849;&#20139;&#65289;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#24182;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#21069;&#26223;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;SLTrans&#65292;&#19968;&#20010;&#30001;&#36817;400&#19975;&#20010;&#33258;&#21253;&#21547;&#28304;&#20195;&#30721;&#25991;&#20214;&#32452;&#25104;&#30340;&#24182;&#34892;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 Announce Type: new  Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled wi
&lt;/p&gt;</description></item><item><title>&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.02181</link><description>&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#26159;&#25152;&#26377;LLMs&#30340;&#23618;&#37117;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Not all Layers of LLMs are Necessary during Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02181
&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#38454;&#27573;&#38750;&#24120;&#26114;&#36149;&#12290;&#29702;&#24819;&#30340;LLMs&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#20854;&#33021;&#21147;&#65288;&#20363;&#22914;&#27867;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65289;&#12290;&#26412;&#25991;&#23581;&#35797;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#31616;&#21333;&#23454;&#20363;&#20351;&#29992;&#27973;&#23618;&#65292;&#24182;&#20026;&#38590;&#20197;&#22788;&#29702;&#30340;&#23454;&#20363;&#20351;&#29992;&#28145;&#23618;&#21527;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#36328;&#20219;&#21153;&#28608;&#27963;&#30340;&#23618;&#26469;&#25351;&#20986;&#24182;&#38750;&#25152;&#26377;&#23618;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#37117;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;AdaInfer&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#25512;&#29702;&#32456;&#27490;&#26102;&#21051;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;AdaInfer&#19981;&#25913;&#21464;LLMs&#21442;&#25968;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#30693;&#21517;LLMs&#65288;&#21363;Llama2&#31995;&#21015;&#21644;OPT&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;AdaInfer&#33410;&#30465;&#20102;&#24179;&#22343;14.8%&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#29978;&#33267;&#22312;&#24773;&#24863;&#26041;&#38754;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02181v1 Announce Type: cross  Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20351;&#25968;&#23383;&#24187;&#35273;&#24471;&#21040;&#26174;&#33879;&#25913;&#21892;</title><link>https://arxiv.org/abs/2403.01373</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#65306;&#19968;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20351;&#25968;&#23383;&#24187;&#35273;&#24471;&#21040;&#26174;&#33879;&#25913;&#21892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#20869;&#23481;&#30456;&#20851;&#25361;&#25112;&#26041;&#38754;&#30340;&#26174;&#33879;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#21508;&#31181;&#24187;&#35273;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#24187;&#35273;&#65292;&#31216;&#20026;&#25968;&#23383;&#24187;&#35273;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#26410;&#33021;&#20934;&#30830;&#35782;&#21035;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;&#25968;&#37327;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#25968;&#23383;&#24187;&#35273;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#22312;&#20027;&#27969;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#20013;&#30340;&#26126;&#26174;&#26222;&#36941;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#30456;&#20851;&#35270;&#35282;&#28145;&#20837;&#20998;&#26512;&#20102;&#25968;&#23383;&#24187;&#35273;&#65292;&#32771;&#23519;&#20102;&#20869;&#22312;&#21644;&#22806;&#22312;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26159;&#25968;&#23383;&#24187;&#35273;&#30340;&#19968;&#20010;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#20316;&#20026;&#20943;&#36731;&#27492;&#31867;&#24187;&#35273;&#30340;&#25163;&#27573;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;8%&#30340;&#24179;&#22343;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01373v1 Announce Type: new  Abstract: Large vision language models have demonstrated remarkable efficacy in addressing challenges related to both textual and visual content. Nevertheless, these models are susceptible to various hallucinations. In this paper, we focus on a new form of hallucination, specifically termed as number hallucination, which denotes instances where models fail to accurately identify the quantity of objects in an image. We establish a dataset and employ evaluation metrics to assess number hallucination, revealing a pronounced prevalence of this issue across mainstream large vision language models (LVLMs). Additionally, we delve into a thorough analysis of number hallucination, examining inner and outer inconsistency problem from two related perspectives. We assert that this inconsistency is one cause of number hallucination and propose a consistency training method as a means to alleviate such hallucination, which achieves an average improvement of 8\%
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.19404</link><description>&lt;p&gt;
&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#29992;&#20110;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Entity-Aware Multimodal Alignment Framework for News Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19404
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#26159;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#35201;&#27714;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#23383;&#24149;&#65292;&#20854;&#20013;&#21253;&#21547;&#26032;&#38395;&#22270;&#20687;&#21644;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#12290;&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#24120;&#35265;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#23450;&#19979;&#29983;&#25104;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#21363;&#20351;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#65292;&#23427;&#20204;&#22788;&#29702;&#23454;&#20307;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#23454;&#20307;&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#24863;&#30693;&#23545;&#40784;&#20219;&#21153;&#21644;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#20197;&#23545;&#40784;&#27169;&#22411;&#24182;&#29983;&#25104;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GoodNews&#25968;&#25454;&#38598;&#19978;&#23558;CIDEr&#20998;&#25968;&#25552;&#39640;&#21040;86.29&#65288;&#20174;72.33&#65289;&#65292;&#22312;NYTimes800k&#25968;&#25454;&#38598;&#19978;&#23558;&#20854;&#25552;&#39640;&#21040;85.61&#65288;&#20174;70.83&#65289;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19404v1 Announce Type: cross  Abstract: News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -&gt; 86.29) on GoodNews dataset and (70.83 -&gt; 85.61) on NYTimes800k dataset.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26088;&#22312;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24110;&#21161;&#27668;&#20505;&#27963;&#21160;&#20154;&#22763;&#35782;&#21035;&#21644;&#24212;&#23545;&#20167;&#24680;&#35328;&#35770;&#65292;&#22242;&#38431;&#35780;&#20272;&#20102;&#22810;&#31181;&#27169;&#22411;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#35782;&#21035;&#21644;&#31435;&#22330;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17014</link><description>&lt;p&gt;
Z-AGI&#23454;&#39564;&#23460;&#21442;&#19982;ClimateActivism 2024&#65306;&#31038;&#20132;&#23186;&#20307;&#19978;&#31435;&#22330;&#21644;&#20167;&#24680;&#20107;&#20214;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17014
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26088;&#22312;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24110;&#21161;&#27668;&#20505;&#27963;&#21160;&#20154;&#22763;&#35782;&#21035;&#21644;&#24212;&#23545;&#20167;&#24680;&#35328;&#35770;&#65292;&#22242;&#38431;&#35780;&#20272;&#20102;&#22810;&#31181;&#27169;&#22411;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#35782;&#21035;&#21644;&#31435;&#22330;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#39046;&#22495;&#65292;&#20016;&#23500;&#30340;&#25968;&#25454;&#26159;&#27934;&#23519;&#31038;&#20250;&#12289;&#25919;&#27835;&#21644;&#32463;&#27982;&#26684;&#23616;&#22797;&#26434;&#24615;&#30340;&#20851;&#38190;&#20449;&#24687;&#26469;&#28304;&#12290;&#38024;&#23545;&#23545;&#20107;&#20214;&#39640;&#36136;&#37327;&#20449;&#24687;&#30340;&#26085;&#30410;&#22686;&#38271;&#38656;&#27714;&#20197;&#21450;&#25171;&#20987;&#20167;&#24680;&#35328;&#35770;&#30340;&#24517;&#35201;&#24615;&#65292;&#26412;&#30740;&#31350;&#21457;&#36215;&#20102;&#22312;CASE 2024&#20030;&#21150;&#30340;Climate Activism&#31435;&#22330;&#21644;&#20167;&#24680;&#20107;&#20214;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#30340;&#24314;&#31435;&#12290;&#32858;&#28966;&#27668;&#20505;&#27963;&#21160;&#20154;&#22763;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24212;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20174;Twitter&#25512;&#25991;&#20013;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#36890;&#36807;&#20998;&#26512;&#19977;&#20010;&#23376;&#20219;&#21153;-&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65288;&#23376;&#20219;&#21153;A&#65289;&#12289;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#35782;&#21035;&#65288;&#23376;&#20219;&#21153;B&#65289;&#21644;&#31435;&#22330;&#26816;&#27979;&#65288;&#23376;&#20219;&#21153;C&#65289;- Z-AGI&#23454;&#39564;&#23460;&#22242;&#38431;&#35780;&#20272;&#20102;&#22522;&#20110;Tf-Idf&#30340;&#21508;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;LSTM&#12289;Xgboost&#21644;LGBM&#12290;&#32467;&#26524;&#26174;&#31034;&#20986;&#26377;&#36259;&#30340;&#21464;&#21270;&#65292;Catboost&#22312;&#23376;&#20219;&#21153;B&#65288;F1&#65306;0.5604&#65289;&#21644;&#23376;&#20219;&#21153;C&#65288;F1&#65306;0.7081&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;LGBM&#25104;&#20026;&#23376;&#20219;&#21153;A&#65288;F1&#65306;0.8684&#65289;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17014v1 Announce Type: new  Abstract: In the digital realm, rich data serves as a crucial source of insights into the complexities of social, political, and economic landscapes. Addressing the growing need for high-quality information on events and the imperative to combat hate speech, this research led to the establishment of the Shared Task on Climate Activism Stance and Hate Event Detection at CASE 2024. Focused on climate activists contending with hate speech on social media, our study contributes to hate speech identification from tweets. Analyzing three sub-tasks - Hate Speech Detection (Sub-task A), Targets of Hate Speech Identification (Sub-task B), and Stance Detection (Sub-task C) - Team Z-AGI Labs evaluated various models, including LSTM, Xgboost, and LGBM based on Tf-Idf. Results unveiled intriguing variations, with Catboost excelling in Subtask-B (F1: 0.5604) and Subtask-C (F1: 0.7081), while LGBM emerged as the top-performing model for Subtask-A (F1: 0.8684). T
&lt;/p&gt;</description></item><item><title>Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14320</link><description>&lt;p&gt;
Triad: &#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14320
&lt;/p&gt;
&lt;p&gt;
Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#31572;&#30693;&#35782;&#24211;&#20013;&#38382;&#39064;&#30340;&#36816;&#29992;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#23454;&#29616;KBQA&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#20197;&#20219;&#21153;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triad&#65292;&#19968;&#20010;&#21033;&#29992;&#20855;&#26377;&#19977;&#20010;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#36827;&#34892;KBQA&#20219;&#21153;&#12290;&#20195;&#29702;&#34987;&#20998;&#37197;&#19977;&#20010;&#35282;&#33394;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;KBQA&#23376;&#20219;&#21153;&#65306;&#20316;&#20026;&#25484;&#25569;&#21508;&#31181;&#23376;&#20219;&#21153;&#30340;&#36890;&#25165;&#65292;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#32773;&#30340;&#20915;&#31574;&#32773;&#65292;&#20197;&#21450;&#20316;&#20026;&#22238;&#31572;&#24102;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#30340;&#39038;&#38382;&#12290;&#25105;&#20204;&#30340;KBQA&#26694;&#26550;&#22312;&#22235;&#20010;&#38454;&#27573;&#20013;&#25191;&#34892;&#65292;&#28041;&#21450;&#20195;&#29702;&#30340;&#22810;&#37325;&#35282;&#33394;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#65292;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12038</link><description>&lt;p&gt;
Self-AMPLIFY&#65306;&#36890;&#36807;&#33258;&#25105;&#20107;&#21518;&#35299;&#37322;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#65292;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;Self-AMPLIFY&#26159;&#19968;&#20010;3&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#26679;&#26412;&#12289;&#29983;&#25104;&#29702;&#30001;&#21644;&#26500;&#24314;&#26368;&#32456;&#25552;&#31034;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;SLMs&#21644;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Self-AMPLIFY&#30340;&#24615;&#33021;&#65306;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;Self-AMPLIFY&#22312;&#19982;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;Self-AMPLIFY&#26159;&#31532;&#19968;&#20010;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;SLMs&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#35299;&#37322;&#24182;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12038v1 Announce Type: new  Abstract: Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance. However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a full
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06025</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#27010;&#29575;&#25512;&#29702;&#36827;&#34892;&#23454;&#39564;&#19982;&#20462;&#35746;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;&#35813;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#65292;&#21363;&#20351;&#35268;&#21017;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#23398;&#20064;&#32773;&#20063;&#20250;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#27169;&#31946;&#27010;&#29575;&#35268;&#21017;&#65292;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#26681;&#25454;&#36817;&#20284;&#36125;&#21494;&#26031;&#21407;&#21017;&#22312;&#27599;&#27425;&#23454;&#39564;&#21518;&#22312;&#32447;&#26356;&#26032;&#33258;&#24049;&#30340;&#20551;&#35774;&#12290;&#22312;&#21516;&#19968;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#26681;&#25454;&#20449;&#24687;&#35770;&#20934;&#21017;&#24314;&#31435;&#20102;&#23454;&#39564;&#35774;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#21407;&#21017;&#30340;&#32452;&#21512;&#8212;&#8212;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#8212;&#8212;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#21435;&#25481;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#32452;&#20214;&#37117;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.01643</link><description>&lt;p&gt;
L-TUNING&#65306;&#29992;&#20110;LLMs&#20013;&#30340;&#25552;&#31034;&#21644;&#21069;&#32512;&#30340;&#21516;&#27493;&#26631;&#31614;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20219;&#24847;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#24182;&#19988;&#36890;&#29992;&#26631;&#35760;&#22312;&#21508;&#31181;&#31867;&#21035;&#26631;&#31614;&#20013;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;L-Tuning&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#35774;&#35745;&#30340;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;L-Tuning&#19987;&#27880;&#20110;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;LLM&#22788;&#29702;&#30340;&#26631;&#31614;&#26631;&#35760;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#21033;&#29992;&#20854;&#39044;&#20808;&#23384;&#22312;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#25552;&#39640;&#20102;&#24494;&#35843;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36824;&#20419;&#36827;&#20102;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#19981;&#21516;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;L-Tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#23567;&#22411;&#32039;&#20945;LLMs&#21644;&#38646;&#23556;&#20987;&#36739;&#22823;LLMs&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#23567;&#22411;LLMs&#26080;&#27861;&#36229;&#36234;&#36739;&#22823;&#30340;&#38646;&#23556;&#20987;LLMs&#65292;&#20294;FLAN-T5&#26159;&#19968;&#20010;&#20363;&#22806;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#35768;&#22810;&#38646;&#23556;&#20987;LLMs&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.00841</link><description>&lt;p&gt;
&#23567;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20250;&#35758;&#25688;&#35201;&#20013;&#26159;&#21542;&#33021;&#22815;&#36229;&#36234;&#20854;&#20307;&#37327;&#65311;
&lt;/p&gt;
&lt;p&gt;
Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#23567;&#22411;&#32039;&#20945;LLMs&#21644;&#38646;&#23556;&#20987;&#36739;&#22823;LLMs&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#23567;&#22411;LLMs&#26080;&#27861;&#36229;&#36234;&#36739;&#22823;&#30340;&#38646;&#23556;&#20987;LLMs&#65292;&#20294;FLAN-T5&#26159;&#19968;&#20010;&#20363;&#22806;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#35768;&#22810;&#38646;&#23556;&#20987;LLMs&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27809;&#26377;&#26126;&#30830;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#37096;&#32626;LLMs&#24182;&#38750;&#26131;&#20107;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#32463;&#36807;&#24494;&#35843;&#30340;&#23567;&#22411;&#32039;&#20945;LLMs&#65288;&#22914;FLAN-T5&#12289;TinyLLaMA&#12289;LiteLLaMA&#65289;&#19982;&#38646;&#23556;&#20987;&#36739;&#22823;LLMs&#65288;&#22914;LLaMA-2&#12289;GPT-3.5&#12289;PaLM-2&#65289;&#30340;&#24615;&#33021;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#35299;&#20915;&#21033;&#29992;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#24040;&#22823;&#25104;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22810;&#25968;&#23567;&#22411;LLMs&#65292;&#21363;&#20351;&#32463;&#36807;&#24494;&#35843;&#65292;&#20063;&#26080;&#27861;&#22312;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;&#20013;&#36229;&#36234;&#36739;&#22823;&#30340;&#38646;&#23556;&#20987;LLMs&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#20363;&#22806;&#26159;FLAN-T5&#65288;780M&#20010;&#21442;&#25968;&#65289;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#35768;&#22810;&#38646;&#23556;&#20987;LLMs&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22312;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#19978;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00658</link><description>&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36712;&#36857;&#21644;&#21512;&#25104;&#22870;&#21169;&#26469;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22312;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#19978;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#36880;&#27493;&#21512;&#29702;&#21270;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#21512;&#29702;&#21270;&#30340;&#21487;&#38752;&#24615;&#21644;&#24544;&#23454;&#24615;&#65292;&#27491;&#22312;&#36827;&#34892;&#22823;&#37327;&#24037;&#20316;&#12290;&#26377;&#20123;&#26041;&#27861;&#23558;&#25512;&#29702;&#24314;&#27169;&#20026;&#35268;&#21010;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21017;&#19987;&#27880;&#20110;&#27880;&#37322;&#30340;&#36807;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35268;&#21010;&#30340;&#25628;&#32034;&#36807;&#31243;&#24448;&#24448;&#30001;&#20110;&#39057;&#32321;&#35780;&#20272;&#20013;&#38388;&#25512;&#29702;&#29366;&#24577;&#21644;&#24191;&#27867;&#30340;&#25506;&#32034;&#31354;&#38388;&#32780;&#23548;&#33268;&#39640;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30417;&#30563;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;LLM&#35757;&#32451;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#36712;&#36857;&#30452;&#25509;&#26681;&#25454;&#21512;&#25104;&#30340;&#36807;&#31243;&#22870;&#21169;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26032;&#30340;PILOT&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26696;&#20363;&#27861;&#39044;&#27979;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30456;&#20851;&#26696;&#20363;&#26816;&#32034;&#21644;&#26102;&#38388;&#27169;&#24335;&#22788;&#29702;&#20004;&#20010;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2401.15770</link><description>&lt;p&gt;
PILOT: &#20351;&#29992;&#27861;&#24459;&#26696;&#20363;&#39044;&#27979;&#26696;&#20363;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
PILOT: Legal Case Outcome Prediction with Case Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15770
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26032;&#30340;PILOT&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26696;&#20363;&#27861;&#39044;&#27979;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30456;&#20851;&#26696;&#20363;&#26816;&#32034;&#21644;&#26102;&#38388;&#27169;&#24335;&#22788;&#29702;&#20004;&#20010;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#39044;&#27979;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#21069;&#26223;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#27665;&#20107;&#27861;&#24459;&#26696;&#20363;&#32780;&#38750;&#26696;&#20363;&#27861;&#31995;&#32479;&#19978;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#20351;&#29992;&#26696;&#20363;&#27861;&#36827;&#34892;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#39044;&#27979;&#26102;&#30340;&#20004;&#20010;&#29420;&#29305;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#35782;&#21035;&#20316;&#20026;&#27861;&#23448;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#22522;&#26412;&#35777;&#25454;&#30340;&#30456;&#20851;&#20808;&#20363;&#26696;&#20363;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#27425;&#65292;&#26377;&#24517;&#35201;&#32771;&#34385;&#27861;&#24459;&#21407;&#21017;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#26089;&#26399;&#26696;&#20363;&#21487;&#33021;&#36981;&#24490;&#19981;&#21516;&#30340;&#27861;&#24459;&#32972;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#65288;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#39044;&#27979;&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#29992;&#20110;&#30456;&#20851;&#26696;&#20363;&#26816;&#32034;&#21644;&#26102;&#38388;&#27169;&#24335;&#22788;&#29702;&#30340;&#20004;&#20010;&#27169;&#22359;&#12290;&#20026;&#20102;&#34913;&#37327;&#29616;&#26377;&#27861;&#24459;&#26696;&#20363;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20174;&#22823;&#35268;&#27169;&#26696;&#20363;&#27861;&#25968;&#25454;&#24211;&#20013;&#31574;&#21010;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20934;&#30830;&#35782;&#21035;&#20808;&#20363;&#26696;&#20363;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#32531;&#35299;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15770v2 Announce Type: replace  Abstract: Machine learning shows promise in predicting the outcome of legal cases, but most research has concentrated on civil law cases rather than case law systems. We identified two unique challenges in making legal case outcome predictions with case law. First, it is crucial to identify relevant precedent cases that serve as fundamental evidence for judges during decision-making. Second, it is necessary to consider the evolution of legal principles over time, as early cases may adhere to different legal contexts. In this paper, we proposed a new framework named PILOT (PredictIng Legal case OuTcome) for case outcome prediction. It comprises two modules for relevant case retrieval and temporal pattern handling, respectively. To benchmark the performance of existing legal case outcome prediction models, we curated a dataset from a large-scale case law database. We demonstrate the importance of accurately identifying precedent cases and mitiga
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20195;&#29702;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;&#35299;&#30721;&#26102;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#23567;&#22411;&#35843;&#25972;&#21518;&#30340;LM&#30340;&#39044;&#27979;&#19982;&#26410;&#35843;&#25972;LM&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;LM&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#36164;&#28304;&#33410;&#32422;&#21644;&#20445;&#30041;&#26356;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2401.08565</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tuning Language Models by Proxy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08565
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20195;&#29702;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;&#35299;&#30721;&#26102;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#23567;&#22411;&#35843;&#25972;&#21518;&#30340;LM&#30340;&#39044;&#27979;&#19982;&#26410;&#35843;&#25972;LM&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;LM&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#36164;&#28304;&#33410;&#32422;&#21644;&#20445;&#30041;&#26356;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#19968;&#33324;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22987;&#32456;&#21463;&#30410;&#20110;&#36827;&#19968;&#27493;&#35843;&#25972;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#28040;&#32791;&#36164;&#28304;&#65292;&#25110;&#32773;&#22312;&#27169;&#22411;&#26435;&#37325;&#26159;&#31169;&#26377;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20195;&#29702;&#35843;&#25972;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35299;&#30721;&#26102;&#31639;&#27861;&#65292;&#23427;&#22312;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36816;&#34892;&#65292;&#20197;&#23454;&#29616;&#19982;&#30452;&#25509;&#35843;&#25972;&#30456;&#21516;&#30340;&#30446;&#30340;&#65292;&#20294;&#21482;&#35775;&#38382;&#20854;&#22312;&#36755;&#20986;&#35789;&#27719;&#19978;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#20854;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35843;&#25972;&#20102;&#19968;&#20010;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#32463;&#36807;&#35843;&#25972;&#21644;&#26410;&#32463;&#35843;&#25972;&#30340;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#24212;&#29992;&#20110;&#23558;&#26356;&#22823;&#30340;&#26410;&#35843;&#25972;&#27169;&#22411;&#30340;&#21407;&#22987;&#39044;&#27979;&#36716;&#31227;&#21040;&#35843;&#25972;&#26041;&#21521;&#65292;&#21516;&#26102;&#20445;&#30041;&#36739;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#24403;&#25105;&#20204;&#20351;&#29992;&#20165;&#20026;7B&#22823;&#23567;&#30340;&#20195;&#29702;&#23545;Llama2-70B&#24212;&#29992;&#20195;&#29702;&#35843;&#25972;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20851;&#38381;88% Llama2-70B &#19982;&#20854;&#30495;&#27491;&#35843;&#25972;&#36807;&#30340;&#32842;&#22825;&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08565v2 Announce Type: replace  Abstract: Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the same end as direct tuning, but by accessing only its predictions over the output vocabulary, not its parameters. Our method tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the larger untuned model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when 
&lt;/p&gt;</description></item><item><title>MAPO&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#23545;&#40784;&#20316;&#20026;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.06838</link><description>&lt;p&gt;
MAPO&#65306;&#36890;&#36807;&#22810;&#35821;&#35328;&#23545;&#40784;&#20316;&#20026;&#20559;&#22909;&#20248;&#21270;&#25512;&#36827;&#22810;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06838
&lt;/p&gt;
&lt;p&gt;
MAPO&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#23545;&#40784;&#20316;&#20026;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25512;&#29702;&#33021;&#21147;&#34987;&#35748;&#20026;&#19982;&#35821;&#35328;&#26080;&#20851;&#65292;&#20294;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#30340;&#25512;&#29702;&#33021;&#21147;&#19981;&#19968;&#33268;&#65292;&#20363;&#22914;&#65292;&#23545;&#20027;&#23548;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#36825;&#26159;&#30001;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#22686;&#24378;&#38750;&#20027;&#23548;&#35821;&#35328;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35821;&#35328;&#23545;&#40784;&#20316;&#20026;&#20559;&#22909;&#20248;&#21270;&#65288;MAPO&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#19982;&#20027;&#23548;&#35821;&#35328;&#23545;&#40784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#32763;&#35793;&#27169;&#22411;&#26469;&#20445;&#35777;&#38750;&#20027;&#23548;&#35821;&#35328;&#21644;&#20027;&#23548;&#35821;&#35328;&#20043;&#38388;&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#65292;&#36825;&#34987;&#37319;&#32435;&#20026;&#20248;&#21270;&#30340;&#20559;&#22909;&#65292;&#20363;&#22914;&#65292;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#25110;&#20020;&#36817;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MAPO&#22312;&#25152;&#26377;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65288;MSVAMP +16.2&#65285;&#65292;MGSM +6.1&#65285;&#65289;&#31283;&#23450;&#22320;&#23454;&#29616;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06838v2 Announce Type: replace  Abstract: Though reasoning abilities are considered language-agnostic, existing LLMs exhibit inconsistent reasoning abilities across different languages, e.g., reasoning in the dominant language like English is superior to other languages due to the imbalance of multilingual training data. To enhance reasoning abilities in non-dominant languages, we propose a Multilingual-Alignment-as-Preference Optimization framework (MAPO), aiming to align the reasoning processes in other languages with the dominant language. Specifically, we harness an off-the-shelf translation model for the consistency between answers in non-dominant and dominant languages, which we adopt as the preference for optimization, e.g., Direct Preference Optimization (DPO) or Proximal Policy Optimization (PPO). Experiments show that MAPO stably achieves significant improvements in the multilingual reasoning of various models on all three benchmarks (MSVAMP +16.2%, MGSM +6.1%, and
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#25913;&#21892;&#30422;&#20857;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#30456;&#20851;&#35821;&#35328;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12289;&#20248;&#21270;&#20849;&#20139;&#35789;&#27719;&#21644;&#26631;&#35760;&#20998;&#21106;&#26041;&#27861;&#12289;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20197;&#21450;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#65292;&#20854;&#20013;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#30456;&#20851;&#24615;&#30340;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30456;&#27604;&#26631;&#20934;&#21452;&#35821;&#27169;&#22411;&#26377;4 BLEU&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2311.14530</link><description>&lt;p&gt;
&#29992;&#20110;&#30422;&#20857;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Machine Translation for Ge'ez Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#25913;&#21892;&#30422;&#20857;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#30456;&#20851;&#35821;&#35328;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12289;&#20248;&#21270;&#20849;&#20139;&#35789;&#27719;&#21644;&#26631;&#35760;&#20998;&#21106;&#26041;&#27861;&#12289;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20197;&#21450;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#65292;&#20854;&#20013;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#30456;&#20851;&#24615;&#30340;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30456;&#27604;&#26631;&#20934;&#21452;&#35821;&#27169;&#22411;&#26377;4 BLEU&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14530v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#26426;&#22120;&#32763;&#35793;(MT)&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;&#30422;&#20857;&#35821;&#65292;&#19968;&#31181;&#21476;&#32769;&#30340;&#35821;&#35328;&#65292;&#19981;&#20877;&#26159;&#20219;&#20309;&#31038;&#21306;&#30340;&#27597;&#35821;&#65292;&#38754;&#20020;&#35832;&#22914;&#35789;&#27719;&#22806;&#29983;&#12289;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#32570;&#20047;&#36275;&#22815;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25913;&#21892;&#30422;&#20857;&#35821;MT&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#30456;&#20851;&#35821;&#35328;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12289;&#20248;&#21270;&#20849;&#20139;&#35789;&#27719;&#21644;&#26631;&#35760;&#20998;&#21106;&#26041;&#27861;&#12289;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23569;&#26679;&#26412;&#32763;&#35793;&#37197;&#21512;&#27169;&#31946;&#21305;&#37197;&#12290;&#25105;&#20204;&#22522;&#20110;&#35821;&#35328;&#30456;&#20851;&#24615;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#27169;&#22411;&#65292;&#20351;&#26631;&#20934;&#21452;&#35821;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;&#32422;4 BLEU&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#24494;&#35843;NLLB-200&#27169;&#22411;&#65292;&#36825;&#26159;&#24403;&#20170;&#21487;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#32763;&#35793;&#27169;&#22411;&#20043;&#19968;&#65292;&#20294;&#21457;&#29616;&#21482;&#26377;4k&#35757;&#32451;&#26679;&#26412;&#30340;&#30422;&#20857;&#35821;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14530v2 Announce Type: replace  Abstract: Machine translation (MT) for low-resource languages such as Ge'ez, an ancient language that is no longer the native language of any community, faces challenges such as out-of-vocabulary words, domain mismatches, and lack of sufficient labeled training data. In this work, we explore various methods to improve Ge'ez MT, including transfer-learning from related languages, optimizing shared vocabulary and token segmentation approaches, finetuning large pre-trained models, and using large language models (LLMs) for few-shot translation with fuzzy matches. We develop a multilingual neural machine translation (MNMT) model based on languages relatedness, which brings an average performance improvement of about 4 BLEU compared to standard bilingual models. We also attempt to finetune the NLLB-200 model, one of the most advanced translation models available today, but find that it performs poorly with only 4k training samples for Ge'ez. Furthe
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#27861;&#25552;&#20379;&#27604;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2311.07484</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#27979;&#37327;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Psychometric Predictive Power of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07484
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#27861;&#25552;&#20379;&#27604;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07484v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;:&#25351;&#20196;&#35843;&#25972;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21709;&#24212;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#23613;&#31649;&#22312;&#20154;-LLM&#23545;&#40784;&#26041;&#38754;&#36827;&#34892;&#20102;&#21162;&#21147;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#24182;&#19981;&#24635;&#26159;&#20351;LLMs&#20174;&#35748;&#30693;&#24314;&#27169;&#30340;&#35282;&#24230;&#30475;&#36215;&#26469;&#26356;&#20687;&#20154;&#31867;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#30001;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#20272;&#35745;&#30340;&#19979;&#19968;&#20010;&#35789;&#27010;&#29575;&#24448;&#24448;&#27604;&#22522;&#30784;LLM&#20272;&#35745;&#30340;&#27010;&#29575;&#26356;&#31967;&#31957;&#65292;&#26080;&#27861;&#27169;&#25311;&#20154;&#31867;&#38405;&#35835;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#27169;&#25311;&#20154;&#31867;&#38405;&#35835;&#34892;&#20026;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#26144;&#29305;&#23450;&#35821;&#35328;&#20551;&#35774;&#30340;&#25552;&#31034;&#21487;&#20197;&#25552;&#39640;PPP&#65292;&#20294;&#20173;&#19981;&#21450;&#23567;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;PPP&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20986;LLMs&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#27604;&#22522;&#30784;LLMs&#30452;&#25509;&#27010;&#29575;&#27979;&#37327;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32431;&#31929;&#30340;&#19979;&#19968;&#20010;&#35789;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07484v2 Announce Type: replace-cross  Abstract: Instruction tuning aligns the response of large language models (LLMs) with human preferences. Despite such efforts in human--LLM alignment, we report that, interestingly, instruction tuning does not always make LLMs human-like from a cognitive modeling perspective. More specifically, next-word probabilities estimated by instruction-tuned LLMs are often worse at simulating human reading behavior than those estimated by base LLMs. In addition, we explore prompting methodologies in simulating human reading behavior with LLMs. Our results show that prompts reflecting a particular linguistic hypothesis improve PPP but are still inferior to PPP from small base models. These findings highlight that recent advancements in LLMs, i.e., instruction tuning and prompting, do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling. In other words, our experiments highlight that pure next-word pro
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.06899</link><description>&lt;p&gt;
&#28779;&#28976;: &#35780;&#20272;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#22865;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Flames: Benchmarking Value Alignment of Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06899
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#22320;&#21306;&#30340;&#24191;&#27867;&#24212;&#29992;&#24378;&#35843;&#20102;&#35780;&#20272;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#30340;&#36843;&#20999;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#26377;&#25928;&#22320;&#25581;&#31034;LLMs&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#23613;&#31649;&#35768;&#22810;&#27169;&#22411;&#22312;&#36825;&#20123;&#35780;&#20272;&#20013;&#24471;&#20998;&#24456;&#39640;&#65292;&#19988;&#8220;&#21517;&#21015;&#21069;&#33541;&#8221;&#65292;&#20294;&#22312;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#28145;&#23618;&#22865;&#21512;&#24615;&#21644;&#23454;&#29616;&#30495;&#27491;&#26080;&#23475;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#65292;&#20197;&#21450;&#19968;&#20010;&#25972;&#21512;&#20102;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#22914;&#21644;&#35856;&#30340;&#29420;&#29305;&#36947;&#24503;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#21253;&#21547;&#22797;&#26434;&#24773;&#22659;&#21644;&#22823;&#22810;&#24102;&#26377;&#38544;&#21547;&#24694;&#24847;&#30340;&#30772;&#35299;&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#36890;&#36807;&#23545;17&#20010;&#20027;&#27969;LLMs&#36827;&#34892;&#25552;&#31034;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27169;&#22411;&#30340;&#22238;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#26159;&#20943;&#23569;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#24187;&#35273;&#30340;&#20851;&#38190;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#26356;&#39640;&#27700;&#24179;&#30340;&#30830;&#23450;&#24615;&#23545;&#24212;&#26356;&#20302;&#27700;&#24179;&#30340;&#24187;&#35273;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#26041;&#27861;</title><link>https://arxiv.org/abs/2310.18794</link><description>&lt;p&gt;
&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#38477;&#20302;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18794
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#26159;&#20943;&#23569;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#24187;&#35273;&#30340;&#20851;&#38190;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#26356;&#39640;&#27700;&#24179;&#30340;&#30830;&#23450;&#24615;&#23545;&#24212;&#26356;&#20302;&#27700;&#24179;&#30340;&#24187;&#35273;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#20316;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#34394;&#26500;&#24773;&#20917;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#24187;&#35273;&#27700;&#24179;&#19982;&#20004;&#31181;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65306;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#21709;&#24212;&#20013;&#20004;&#31181;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#25552;&#39640;&#19982;&#34394;&#26500;&#27700;&#24179;&#30340;&#38477;&#20302;&#30456;&#20851;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#65288;CRR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35299;&#30721;&#26102;&#30340;&#24187;&#35273;&#32531;&#35299;&#26041;&#27861;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#23545;&#21709;&#24212;&#20505;&#36873;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#36755;&#20986;&#20855;&#26377;&#26368;&#39640;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#31572;&#26696;&#12290;&#19982;&#25105;&#20204;&#20851;&#20110;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#30340;&#23450;&#20041;&#30456;&#19968;&#33268;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;CRR&#26041;&#27861;&#65306;&#27010;&#29575;CRR&#65288;P-CRR&#65289;&#21644;&#35821;&#20041;CRR&#65288;S-CRR&#65289;&#12290;P-CRR&#20351;&#29992;&#25972;&#20010;&#24207;&#21015;&#30340;&#31639;&#26415;&#24179;&#22343;&#23545;&#21508;&#20010;&#21333;&#29420;&#25277;&#26679;&#30340;&#27169;&#22411;&#21709;&#24212;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18794v2 Announce Type: replace-cross  Abstract: In this work, we propose sequence-level certainty as a common theme over hallucination in Knowledge Grounded Dialogue Generation (KGDG). We explore the correlation between the level of hallucination and two types of sequence-level certainty: probabilistic certainty and semantic certainty. Empirical results reveal that a higher level of both types of sequence-level certainty in model responses is correlated with a lower level of hallucination. We further propose Certainty-based Response Ranking (CRR), a decoding-time hallucination mitigation method that ranks response candidates based on their sequence-level certainty and outputs the answer with the highest certainty level. Aligning with our definitions of sequence-level certainty, we design 2 types of CRR approaches: Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR). P-CRR ranks individually sampled model responses using the arithmetic mean log-probability of the entire sequen
&lt;/p&gt;</description></item><item><title>SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.18376</link><description>&lt;p&gt;
SQLformer&#65306;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18376
&lt;/p&gt;
&lt;p&gt;
SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#36825;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#36825;&#39033;&#25216;&#26415;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20351;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#25552;&#21462;&#27665;&#20027;&#21270;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#20027;&#35201;&#38556;&#30861;&#21253;&#25324;&#39046;&#22495;&#27867;&#21270;&#65292;&#21363;&#36866;&#24212;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25968;&#25454;&#24211;&#65292;&#24182;&#19988;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19982;&#30456;&#24212;&#30340;SQL&#26597;&#35810;&#23545;&#40784;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SQLformer&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#25191;&#34892;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;Transformer&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#39044;&#27979;SQL&#26597;&#35810;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23618;&#20013;&#32467;&#21512;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#26159;&#30001;&#25968;&#25454;&#24211;&#34920;&#21644;&#21015;&#36873;&#25321;&#24341;&#23548;&#30340;&#65292;&#26377;&#21161;&#20110;&#35299;&#30721;&#22120;&#20197;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#35268;&#33539;&#39034;&#24207;&#29983;&#25104;SQL&#26597;&#35810;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35828;&#26126;&#20102;&#29616;&#38454;&#27573;&#30340;&#25216;&#26415;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been growing interest in text-to-SQL translation, which is the task of converting natural language questions into executable SQL queries. This technology is important for its potential to democratize data extraction from databases. However, some of its key hurdles include domain generalisation, which is the ability to adapt to previously unseen databases, and alignment of natural language questions with the corresponding SQL queries. To overcome these challenges, we introduce SQLformer, a novel Transformer architecture specifically crafted to perform text-to-SQL translation tasks. Our model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive way, incorporating structural inductive bias in the encoder and decoder layers. This bias, guided by database table and column selection, aids the decoder in generating SQL query ASTs represented as graphs in a Breadth-First Search canonical order. Comprehensive experiments illustrate the state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31354;&#38388;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#22312;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#26102;&#30340;&#34920;&#29616;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20855;&#26377;&#25429;&#25417;&#31354;&#38388;&#32467;&#26500;&#38544;&#21547;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2310.14540</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Spatial Understanding of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31354;&#38388;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#22312;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#26102;&#30340;&#34920;&#29616;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20855;&#26377;&#25429;&#25417;&#31354;&#38388;&#32467;&#26500;&#38544;&#21547;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#30475;&#21040;&#25991;&#26412;&#65292;&#20294;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#34920;&#31034;&#38544;&#21547;&#22320;&#25429;&#25417;&#20102;&#22320;&#38754;&#27010;&#24565;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#34920;&#31034;&#23545;&#19968;&#31181;&#29305;&#21035;&#26174;&#33879;&#30340;&#22522;&#30784;&#30693;&#35782; -- &#31354;&#38388;&#20851;&#31995;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;LLMs&#65292;&#29305;&#21035;&#26159;GPT-3.5-turbo&#12289;GPT-4 &#21644; Llama2 &#31995;&#21015;&#27169;&#22411;&#65292;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#20219;&#21153;&#25581;&#31034;&#20102;LLM&#22312;&#19981;&#21516;&#31354;&#38388;&#32467;&#26500; (&#21253;&#25324;&#27491;&#26041;&#24418;&#12289;&#20845;&#36793;&#24418;&#21644;&#19977;&#35282;&#24418;&#26684;&#12289;&#29615;&#21644;&#26641;) &#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#22312;&#24191;&#27867;&#30340;&#38169;&#35823;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#38169;&#35823;&#21453;&#26144;&#20102;&#31354;&#38388;&#21644;&#38750;&#31354;&#38388;&#22240;&#32032;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#20284;&#20046;&#38544;&#21547;&#22320;&#25429;&#25417;&#20102;&#31354;&#38388;&#32467;&#26500;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#20294;&#20173;&#26377;&#25552;&#21319;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14540v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge -- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room f
&lt;/p&gt;</description></item><item><title>LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2309.13788</link><description>&lt;p&gt;
&#33021;&#22815;&#26816;&#27979;&#21040;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLM-Generated Misinformation Be Detected?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13788
&lt;/p&gt;
&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;LLMs&#65288;&#22914;ChatGPT&#65289;&#21487;&#33021;&#34987;&#21033;&#29992;&#26469;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#32473;&#22312;&#32447;&#23433;&#20840;&#21644;&#20844;&#20247;&#20449;&#20219;&#24102;&#26469;&#20102;&#20005;&#37325;&#20851;&#20999;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#20250;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#36896;&#25104;&#26356;&#22823;&#21361;&#23475;?&#25105;&#20204;&#25552;&#20986;&#20174;&#26816;&#27979;&#38590;&#24230;&#30340;&#35282;&#24230;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21033;&#29992;LLMs&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#28508;&#22312;&#30495;&#23454;&#19990;&#30028;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#30456;&#27604;&#65292;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#23545;&#20154;&#31867;&#21644;&#26816;&#27979;&#22120;&#26469;&#35828;&#26356;&#38590;&#26816;&#27979;&#65292;&#36825;&#34920;&#26126;&#23427;&#21487;&#33021;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#28508;&#22312;&#22320;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25237;&#24433;&#26041;&#27861;CLaP&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#32763;&#35793;&#26631;&#31614;&#65292;&#30830;&#20445;&#26356;&#20934;&#30830;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#36328;&#35821;&#35328;&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2309.08943</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#32467;&#26500;&#25552;&#21462;&#30340;&#24773;&#22659;&#26631;&#31614;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Contextual Label Projection for Cross-Lingual Structure Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25237;&#24433;&#26041;&#27861;CLaP&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#32763;&#35793;&#26631;&#31614;&#65292;&#30830;&#20445;&#26356;&#20934;&#30830;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#36328;&#35821;&#35328;&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#25237;&#24433;&#28041;&#21450;&#21516;&#26102;&#33719;&#21462;&#32763;&#35793;&#26631;&#31614;&#21644;&#25991;&#26412;&#65292;&#23545;&#20110;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#20419;&#36827;&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#26631;&#31614;&#25237;&#24433;&#26102;&#36890;&#24120;&#36890;&#36807;&#20559;&#29233;&#31616;&#21270;&#26631;&#31614;&#32763;&#35793;&#25110;&#20165;&#20381;&#36182;&#20110;&#21333;&#35789;&#32423;&#21035;&#23545;&#40784;&#26469;&#29306;&#29298;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#31614;&#25237;&#24433;&#26041;&#27861;CLaP&#65292;&#23558;&#25991;&#26412;&#32763;&#35793;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#24182;&#21033;&#29992;&#24050;&#32763;&#35793;&#25991;&#26412;&#20316;&#20026;&#19978;&#19979;&#25991;&#23545;&#26631;&#31614;&#36827;&#34892;&#24773;&#22659;&#32763;&#35793;&#65292;&#30830;&#20445;&#32763;&#35793;&#26631;&#31614;&#30340;&#26356;&#22909;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#22810;&#35821;&#35328;&#33021;&#21147;&#30340;&#25351;&#23548;&#35843;&#26657;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25105;&#20204;&#30340;&#24773;&#22659;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#25351;&#23548;&#22312;&#32763;&#35793;&#25991;&#26412;&#20013;&#23384;&#22312;&#32763;&#35793;&#26631;&#31614;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#22312;39&#31181;&#35821;&#35328;&#19978;&#36890;&#36807;&#38646;-shot&#36328;&#35821;&#35328;&#36716;&#31227;&#23545;CLaP&#19982;&#20854;&#20182;&#26631;&#31614;&#25237;&#24433;&#25216;&#26415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.08943v2 Announce Type: replace  Abstract: Label projection, which involves obtaining translated labels and texts jointly, is essential for leveraging machine translation to facilitate cross-lingual transfer in structured prediction tasks. Prior research exploring label projection often compromise translation accuracy by favoring simplified label translation or relying solely on word-level alignments. In this paper, we introduce a novel label projection approach, CLaP, which translates text to the target language and performs contextual translation on the labels using the translated text as the context, ensuring better accuracy for the translated labels. We leverage instruction-tuned language models with multilingual capabilities as our contextual translator, imposing the constraint of the presence of translated labels in the translated text via instructions. We benchmark CLaP with other label projection techniques on zero-shot cross-lingual transfer across 39 languages on tw
&lt;/p&gt;</description></item><item><title>H2O-Danube-1.8B &#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#21644;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.16818</link><description>&lt;p&gt;
H2O-Danube-1.8B &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
H2O-Danube-1.8B Technical Report. (arXiv:2401.16818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16818
&lt;/p&gt;
&lt;p&gt;
H2O-Danube-1.8B &#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#21644;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; H2O-Danube-1.8B&#65292;&#36825;&#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#36981;&#24490; LLama 2 &#21644; Mistral &#30340;&#26680;&#24515;&#21407;&#21017;&#12290;&#25105;&#20204;&#21033;&#29992;&#21644;&#25913;&#36827;&#20102;&#21508;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#24635;&#26631;&#35760;&#25968;&#37327;&#26126;&#26174;&#23569;&#20110;&#30456;&#20284;&#35268;&#27169;&#30340;&#21442;&#32771;&#27169;&#22411;&#65292;&#20294;&#23427;&#22312;&#20247;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#12290;&#25105;&#20204;&#20197; Apache 2.0 &#35768;&#21487;&#35777;&#23558; H2O-Danube-1.8B &#24320;&#25918;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160; LLMs &#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#65292;&#35753;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present H2O-Danube-1.8B, a 1.8B language model trained on 1T tokens following the core principles of LLama 2 and Mistral. We leverage and refine various techniques for pre-training large language models. Although our model is trained on significantly fewer total tokens compared to reference models of similar size, it exhibits highly competitive metrics across a multitude of benchmarks. We additionally release a chat model trained with supervised fine-tuning followed by direct preference optimization. We make H2O-Danube-1.8B openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12798</link><description>&lt;p&gt;
&#33021;&#37327;&#30340;&#26799;&#24230;&#27969;&#65306;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#30340;&#36890;&#29992;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding. (arXiv:2401.12798v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12798
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26159;&#22312;&#38598;&#25104;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#35782;&#21035;&#36825;&#20123;&#22270;&#35889;&#20013;&#30340;&#31561;&#20215;&#23454;&#20307;&#23545;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23558;EA&#35270;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#22686;&#24378;&#22270;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;EA&#20013;&#35299;&#30721;&#36807;&#31243;-&#23545;&#20110;&#26377;&#25928;&#30340;&#25805;&#20316;&#21644;&#23545;&#40784;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;-&#24471;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#20173;&#28982;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#23454;&#20307;&#21644;&#39069;&#22806;&#30340;&#26174;&#24335;&#20851;&#31995;&#23884;&#20837;&#12290;&#36825;&#31181;&#29305;&#27530;&#24615;&#38480;&#21046;&#20102;&#23427;&#30340;&#36866;&#29992;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#21644;&#39640;&#25928;&#30340;EA&#35299;&#30721;&#26041;&#27861;&#65292;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#22312;&#22270;&#20869;&#24341;&#23548;&#26799;&#24230;&#27969;&#65292;&#20197;&#20419;&#36827;&#22270;&#21516;&#36136;&#24615;&#12290;&#26799;&#24230;&#27969;&#30340;&#31163;&#25955;&#21270;&#20135;&#29983;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19977;&#20803;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA), a pivotal process in integrating multi-source Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these graphs. Most existing approaches regard EA as a graph representation learning task, concentrating on enhancing graph encoders. However, the decoding process in EA - essential for effective operation and alignment accuracy - has received limited attention and remains tailored to specific datasets and model architectures, necessitating both entity and additional explicit relation embeddings. This specificity limits its applicability, particularly in GNN-based models. To address this gap, we introduce a novel, generalized, and efficient decoding approach for EA, relying solely on entity embeddings. Our method optimizes the decoding process by minimizing Dirichlet energy, leading to the gradient flow within the graph, to promote graph homophily. The discretization of the gradient flow produces a fast and scalable approach, termed Triple Feature
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2310.01444</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#27969;&#20351;LLM&#20195;&#29702;&#36866;&#24212;&#29615;&#22659;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapting LLM Agents Through Communication. (arXiv:2310.01444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#20154;&#31867;&#21270;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24110;&#21161;&#36825;&#20123;&#20195;&#29702;&#22312;&#27809;&#26377;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;LLM&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#12290;&#36890;&#36807;&#36845;&#20195;&#25506;&#32034;&#21644;PPO&#35757;&#32451;&#65292;LTC&#20351;&#20195;&#29702;&#33021;&#22815;&#23558;&#30701;&#26399;&#32463;&#39564;&#34701;&#20837;&#38271;&#26399;&#35760;&#24518;&#12290;&#20026;&#20102;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#30340;&#20195;&#29702;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#32467;&#26500;&#21270;&#30340;&#36890;&#20449;&#27169;&#24335;&#65306;&#29420;&#30333;&#65292;&#23545;&#35805;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.00785</link><description>&lt;p&gt;
BooookScore: LLM&#26102;&#20195;&#20013;&#23545;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#30340;&#31995;&#32479;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
BooookScore: A systematic exploration of book-length summarization in the era of LLMs. (arXiv:2310.00785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#38271;&#24230;&#25991;&#26723;&#65288;&gt;100K&#26631;&#35760;&#65289;&#36827;&#34892;&#25688;&#35201;&#38656;&#35201;&#39318;&#20808;&#23558;&#36755;&#20837;&#25991;&#26723;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#21512;&#24182;&#12289;&#26356;&#26032;&#21644;&#21387;&#32553;&#22359;&#32423;&#25688;&#35201;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#35780;&#20272;&#30340;&#22256;&#38590;&#65292;&#23427;&#23578;&#26410;&#24471;&#21040;&#26377;&#24847;&#20041;&#30340;&#30740;&#31350;&#65306;&#29616;&#26377;&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;BookSum&#65289;&#22312;&#22823;&#22810;&#25968;&#20844;&#20849;LLM&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#32780;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#29616;&#20195;LLM&#25688;&#35201;&#22120;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#30340;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#65306;&#65288;1&#65289;&#20998;&#23618;&#21512;&#24182;&#22359;&#32423;&#25688;&#35201;&#65292;&#65288;2&#65289;&#36880;&#27493;&#26356;&#26032;&#19968;&#20010;&#36816;&#34892;&#25688;&#35201;&#12290;&#25105;&#20204;&#23545;100&#26412;&#26368;&#36817;&#20986;&#29256;&#30340;&#20070;&#31821;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#33719;&#24471;&#20102;1193&#20010;&#32454;&#31890;&#24230;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#30830;&#23450;&#20102;LLMs&#20135;&#29983;&#30340;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing book-length documents (&gt;100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Bec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.10435</link><description>&lt;p&gt;
&#37325;&#22609;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65306;&#21033;&#29992;&#20869;&#23481;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#23398;&#20064;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;
&lt;/p&gt;
&lt;p&gt;
Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#22312;&#32447;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#39034;&#24207;&#25512;&#33616;&#30001;&#20110;&#20854;&#34920;&#36798;&#33021;&#21147;&#24378;&#22823;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;&#32780;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39034;&#24207;&#24314;&#27169;&#26041;&#27861;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#30340;&#21407;&#22240;&#26159;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#23545;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#29289;&#21697;&#30456;&#20851;&#25991;&#26412;&#20869;&#23481;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;LANCER&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20135;&#29983;&#20102;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#23545;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#27169;&#22411;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#27491;&#38754;&#24433;&#21709;&#65292;&#20294;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#19982;&#24494;&#35843;&#20998;&#24067;&#26368;&#25509;&#36817;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20559;&#21521;&#20110;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#23581;&#35797;&#24674;&#22797;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10105</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#25512;&#29702;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Understanding Catastrophic Forgetting in Language Models via Implicit Inference. (arXiv:2309.10105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#27169;&#22411;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#27491;&#38754;&#24433;&#21709;&#65292;&#20294;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#19982;&#24494;&#35843;&#20998;&#24067;&#26368;&#25509;&#36817;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20559;&#21521;&#20110;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#23581;&#35797;&#24674;&#22797;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#65288;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#25110;&#20174;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#31561;&#26041;&#27861;&#65289;&#26159;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#40065;&#26834;&#22320;&#25191;&#34892;&#25152;&#38656;&#20219;&#21153;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#24494;&#35843;&#30340;&#24433;&#21709;&#30340;&#31995;&#32479;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#29421;&#31364;&#30340;&#24494;&#35843;&#20998;&#24067;&#20043;&#22806;&#30340;&#20219;&#21153;&#19978;&#12290;&#22312;&#19968;&#20010;&#31616;&#21270;&#30340;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#24494;&#35843;&#25968;&#25454;&#20998;&#24067;&#20869;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#30340;&#21516;&#26102;&#65292;&#20250;&#25233;&#21046;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36864;&#21270;&#22312;&#19982;&#24494;&#35843;&#20998;&#24067;&#8220;&#26368;&#25509;&#36817;&#8221;&#30340;&#20219;&#21153;&#20013;&#23588;&#20026;&#26174;&#33879;&#12290;&#25105;&#20204;&#20551;&#35774;&#35821;&#35328;&#27169;&#22411;&#20250;&#38544;&#24335;&#25512;&#29702;&#20986;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24494;&#35843;&#36807;&#31243;&#20027;&#35201;&#20559;&#21521;&#20110;&#24494;&#35843;&#20998;&#24067;&#20013;&#30340;&#20219;&#21153;&#65292;&#20197;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#36717;&#25552;&#31034;&#20197;&#26597;&#30475;&#26159;&#21542;&#21487;&#20197;&#24674;&#22797;&#39044;&#35757;&#32451;&#30340;&#33021;&#21147;&#12290;&#20849;&#36717;&#25552;&#31034;&#20250;&#20154;&#20026;&#22320;&#20351;&#20219;&#21153;&#30475;&#36215;&#26469;&#19982;&#24494;&#35843;&#20998;&#24067;&#36739;&#36828;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest. However, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks. This degradation is especially pronounced for tasks "closest" to the fine-tuning distribution. We hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution. To test this hypothesis, we propose Conjugate Prompting to see if we can recover pretrained capabilities. Conjugate prompting artificially makes the task look farther from the fine-tun
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#21644;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06089</link><description>&lt;p&gt;
&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#33539;&#24335;&#20013;&#27979;&#37327;&#28798;&#38590;&#24615;&#36951;&#24536;&#65306;&#25506;&#32034;&#35843;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies. (arXiv:2309.06089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#21644;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#26159;&#19968;&#31181;&#35299;&#20915;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#20219;&#21153;&#30340;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#12290;&#22312;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19982;&#38646;&#23556;&#21644;&#20840;&#23556;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20316;&#20026;&#24494;&#35843;&#31574;&#30053;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21442;&#25968;&#25928;&#29575;&#36866;&#37197;&#22120;&#26041;&#27861;&#19982;&#25152;&#26377;&#21442;&#25968;&#24494;&#35843;&#12290;&#20316;&#20026;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#27599;&#20010;&#35821;&#35328;&#20381;&#27425;&#30340;&#20013;&#38388;&#35757;&#32451;&#65288;IT&#65289;&#21644;&#22312;&#24494;&#35843;&#30340;&#39564;&#35777;&#38454;&#27573;&#24050;&#32463;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#39564;&#35777;&#65288;CLV&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#20197;&#21450;&#28304;&#35821;&#35328;&#20013;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#32780;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#65292;&#21363;&#22312;&#23398;&#20064;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#26032;&#20449;&#24687;&#26102;&#20043;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#25439;&#22833;&#20102;&#22810;&#23569;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#65292;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#20135;&#21697;&#35780;&#35770;&#65292;&#20998;&#21035;&#21253;&#21547;&#20102;&#22810;&#20010;&#35821;&#31181;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\textit{IT}) that uses each language sequentially and cross-lingual validation (\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several lang
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03409</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#23548;&#25968;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#27809;&#26377;&#26799;&#24230;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;&#20248;&#21270;&#20219;&#21153;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#25551;&#36848;&#12290;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#65292;LLM&#20174;&#21253;&#21547;&#20808;&#21069;&#29983;&#25104;&#30340;&#35299;&#19982;&#20854;&#20540;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#26032;&#30340;&#35299;&#65292;&#28982;&#21518;&#23545;&#26032;&#30340;&#35299;&#36827;&#34892;&#35780;&#20272;&#24182;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#29992;&#20110;&#19979;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;OPRO&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#28982;&#21518;&#36716;&#21521;&#25552;&#31034;&#20248;&#21270;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#33021;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;LLM&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;OPRO&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#22312;GSM8K&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;8%&#65292;&#22312;Big-Bench Hard&#20219;&#21153;&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11767</link><description>&lt;p&gt;
&#25552;&#39640;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#26816;&#27979;&#30340;&#26041;&#27861;&#65306;&#24341;&#20837;xFakeBibs&#30417;&#30563;&#23398;&#20064;&#32593;&#32476;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#27491;&#22312;&#25104;&#20026;&#29616;&#23454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21306;&#20998;ChatGPT&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21644;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;100&#20010;&#30495;&#23454;&#20986;&#29256;&#29289;&#25688;&#35201;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#37319;&#29992;10&#20493;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#25509;&#21463;&#33539;&#22260;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#19982;ChatGPT&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#26126;&#26174;&#21487;&#35265;ChatGPT&#20165;&#36129;&#29486;&#20102;23\%&#30340;&#20108;&#20803;&#32452;&#20869;&#23481;&#65292;&#36825;&#27604;&#20854;&#20182;10&#20010;&#20132;&#21449;&#39564;&#35777;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#37117;&#23569;50\%&#12290;&#36825;&#20010;&#20998;&#26512;&#20984;&#26174;&#20102;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#19978;&#19982;&#30495;&#23454;&#31185;&#23398;&#30340;&#26126;&#26174;&#24046;&#24322;&#12290;&#22312;&#23545;&#27599;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;xFakeBibs&#31639;&#27861;&#20934;&#30830;&#22320;&#23558;98&#31687;&#20986;&#29256;&#29289;&#35782;&#21035;&#20026;&#20551;&#30340;&#65292;&#26377;2&#31687;&#25991;&#29486;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#30495;&#23454;&#20986;&#29256;&#29289;&#12290;&#23613;&#31649;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
&lt;/p&gt;</description></item><item><title>DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08043</link><description>&lt;p&gt;
DiagGPT:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08043
&lt;/p&gt;
&lt;p&gt;
DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;AI&#27169;&#22411;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36741;&#21161;&#20154;&#31867;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;AI&#20316;&#20026;&#32842;&#22825;&#20195;&#29702;&#20154;&#30340;&#37325;&#35201;&#24212;&#29992;&#26159;&#22238;&#31572;&#20154;&#31867;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;LLMs&#22312;&#22238;&#31572;&#19968;&#33324;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#29087;&#32451;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#35786;&#26029;&#22330;&#26223;(&#22914;&#27861;&#24459;&#25110;&#21307;&#30103;&#21672;&#35810;)&#20013;&#65292;&#22522;&#26412;&#30340;&#38382;&#31572;&#23545;&#35805;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;(TOD)&#65292;&#20854;&#20013;AI&#32842;&#22825;&#20195;&#29702;&#38656;&#35201;&#20027;&#21160;&#25552;&#38382;&#24182;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;TOD&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24403;&#21069;&#30340;LLMs&#24182;&#26410;&#22266;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiagGPT (Diagnosis GPT)&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLMs&#25512;&#24191;&#21040;TOD&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24773;&#32490;&#36716;&#31227;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65288;CFN-ESA&#65289;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#20027;&#35201;&#24773;&#24863;&#20449;&#24687;&#30340;&#26469;&#28304;&#65292;&#35270;&#35273;&#21644;&#22768;&#23398;&#27169;&#24577;&#20316;&#20026;&#27425;&#35201;&#20449;&#24687;&#30340;&#26469;&#28304;&#65292;&#24182;&#24341;&#20837;&#24773;&#32490;&#36716;&#31227;&#27169;&#22359;&#26469;&#35299;&#20915;&#24773;&#32490;&#36716;&#31227;&#22330;&#26223;&#19979;&#24773;&#24863;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15432</link><description>&lt;p&gt;
CFN-ESA&#65306;&#19968;&#31181;&#20855;&#26377;&#24773;&#32490;&#36716;&#31227;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CFN-ESA: A Cross-Modal Fusion Network with Emotion-Shift Awareness for Dialogue Emotion Recognition. (arXiv:2307.15432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24773;&#32490;&#36716;&#31227;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65288;CFN-ESA&#65289;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#20027;&#35201;&#24773;&#24863;&#20449;&#24687;&#30340;&#26469;&#28304;&#65292;&#35270;&#35273;&#21644;&#22768;&#23398;&#27169;&#24577;&#20316;&#20026;&#27425;&#35201;&#20449;&#24687;&#30340;&#26469;&#28304;&#65292;&#24182;&#24341;&#20837;&#24773;&#32490;&#36716;&#31227;&#27169;&#22359;&#26469;&#35299;&#20915;&#24773;&#32490;&#36716;&#31227;&#22330;&#26223;&#19979;&#24773;&#24863;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#21463;&#21040;&#20102;&#21508;&#39046;&#22495;&#30740;&#31350;&#30028;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24773;&#32490;&#36716;&#31227;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65288;CFN-ESA&#65289;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#12290;&#29616;&#26377;&#26041;&#27861;&#22343;&#24179;&#31561;&#22320;&#20351;&#29992;&#27599;&#20010;&#27169;&#24577;&#32780;&#26080;&#27861;&#21306;&#20998;&#24773;&#24863;&#20449;&#24687;&#30340;&#22810;&#23569;&#65292;&#20174;&#32780;&#38590;&#20197;&#20805;&#20998;&#25552;&#21462;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#21644;&#20851;&#32852;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;CFN-ESA&#20013;&#65292;&#25991;&#26412;&#27169;&#24577;&#34987;&#35270;&#20026;&#24773;&#24863;&#20449;&#24687;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#32780;&#35270;&#35273;&#21644;&#22768;&#23398;&#27169;&#24577;&#21017;&#34987;&#35270;&#20026;&#27425;&#35201;&#26469;&#28304;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#24573;&#35270;&#20102;&#24773;&#32490;&#36716;&#31227;&#20449;&#24687;&#65292;&#36807;&#24230;&#20851;&#27880;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23548;&#33268;&#22312;&#24773;&#32490;&#36716;&#31227;&#22330;&#26223;&#19979;&#24773;&#24863;&#35782;&#21035;&#22833;&#36133;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24773;&#32490;&#36716;&#31227;&#27169;&#22359;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;CFN-ESA&#20027;&#35201;&#21253;&#25324;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#65288;RUME&#65289;&#12289;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#65288;ACME&#65289;&#21644;&#24773;&#32490;&#36716;&#31227;&#27169;&#22359;&#65288;LESM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Emotion Recognition in Conversation (ERC) has garnered growing attention from research communities in various fields. In this paper, we propose a cross-modal fusion network with emotion-shift awareness (CFN-ESA) for ERC. Extant approaches employ each modality equally without distinguishing the amount of emotional information, rendering it hard to adequately extract complementary and associative information from multimodal data. To cope with this problem, in CFN-ESA, textual modalities are treated as the primary source of emotional information, while visual and acoustic modalities are taken as the secondary sources. Besides, most multimodal ERC models ignore emotion-shift information and overfocus on contextual information, leading to the failure of emotion recognition under emotion-shift scenario. We elaborate an emotion-shift module to address this challenge. CFN-ESA mainly consists of the unimodal encoder (RUME), cross-modal encoder (ACME), and emotion-shift module (LESM).
&lt;/p&gt;</description></item><item><title>FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10928</link><description>&lt;p&gt;
FLASK: &#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. (arXiv:2307.10928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10928
&lt;/p&gt;
&lt;p&gt;
FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25351;&#20196;&#38656;&#35201;&#19982;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#25216;&#33021;&#38598;&#26681;&#25454;&#25351;&#20196;&#32780;&#24322;&#65292;&#22240;&#27492;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#35780;&#20272;&#65288;&#21363;&#22522;&#20110;&#25972;&#20307;&#20559;&#22909;&#30340;&#35780;&#20272;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#26410;&#32771;&#34385;&#38656;&#35201;&#23454;&#20363;&#32423;&#25216;&#33021;&#32452;&#21512;&#30340;&#29992;&#25143;&#25351;&#20196;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FLASK&#65288;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32454;&#31890;&#24230;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#23427;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35780;&#20272;&#30340;&#32454;&#31890;&#24230;&#23545;&#20110;&#33719;&#24471;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;FLASK&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22810;&#20010;&#24320;&#28304;&#21644;&#19987;&#26377;LLMs&#65292;&#24182;&#35266;&#23519;&#21040;&#39640;&#24230;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.05052</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#24615;&#33021;&#20013;&#65292;&#21508;&#31181;&#28436;&#31034;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26631;&#31614;&#12289;&#36755;&#20837;&#20998;&#24067;&#21644;&#34917;&#20805;&#35299;&#37322;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#20123;&#22240;&#32032;&#34987;&#20462;&#25913;&#25110;&#25200;&#21160;&#26102;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22522;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#24037;&#20316;&#23545;&#20110;&#36825;&#20123;&#20803;&#32032;&#22914;&#20309;&#24433;&#21709;ICL&#32473;&#20986;&#20102;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(XNLP)&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#28436;&#31034;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#23545;&#36755;&#20837;&#20998;&#24067;&#36827;&#34892;&#20102;&#31890;&#24230;&#32423;&#21035;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;ICL&#26041;&#38754;&#30340;&#25928;&#26524;&#26159;&#23384;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boos
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#20316;&#32773;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.04891</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#21407;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning through the Bayesian Prism. (arXiv:2306.04891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#20316;&#32773;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20196;&#20154;&#24778;&#35766;&#19988;&#26377;&#29992;&#30340;&#29305;&#24615;&#20043;&#19968;&#12290;&#23427;&#30340;&#24037;&#20316;&#21407;&#29702;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36817;&#26399;&#65292;&#20154;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#39118;&#26684;&#21270;&#30340;&#31867;&#20803;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#23427;&#20204;&#20351;&#29992;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#20989;&#25968;&#23545;&#26469;&#33258;&#20989;&#25968;&#31867;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;$(x, f(x))$ &#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#35266;&#23519;&#27169;&#22411;&#23545;&#21516;&#19968;&#31867;&#20013;&#26410;&#35265;&#36807;&#30340;&#20989;&#25968;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#19968;&#30740;&#31350;&#32447;&#36335;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#23545;&#20110;&#35832;&#22914;&#32447;&#24615;&#22238;&#24402;&#31561;&#20960;&#20010;&#38382;&#39064;&#65292;&#35757;&#32451;&#22909;&#30340; Transformer &#23398;&#20064;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#36825;&#31181;&#34892;&#20026;&#30340;&#24402;&#32435;&#20559;&#24046;&#24182;&#19981;&#28165;&#26970;&#12290;&#25317;&#26377;&#26080;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#27169;&#22411;&#26159;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#65306;&#23427;&#23398;&#20064;&#20102;&#39044;&#35757;&#32451;&#20998;&#24067;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#39640;&#23481;&#37327;&#30340; Transformer &#27169;&#22411;&#22312;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#27169;&#25311;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#29702;&#24819;&#23398;&#20064;&#32773;&#30340;&#34892;&#20026;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#21253;&#25324;&#22806;&#25512;&#21644;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#21512;&#29702;&#20989;&#25968;&#30340;&#20808;&#39564;&#27010;&#29575;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#23567;&#21270;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#36827;&#19968;&#27493;&#25506;&#31350;&#36825;&#31181;&#32852;&#31995;&#65292;&#35777;&#26126;&#20351;&#29992;&#30495;&#23454;&#30340;&#36125;&#21494;&#26031;&#20808;&#39564;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#20351;&#29992;&#22266;&#23450;&#20808;&#39564;&#25110;&#27809;&#26377;&#20808;&#39564;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs $(x, f(x))$ from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers learn algorithms for learning functions in context. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. It has been shown that high-capacity transformers mimic the Bayesian predictor for linear regression. In this paper, we show empirical evidence of transformers exhibiting the behavior of this ideal learne
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;</title><link>http://arxiv.org/abs/2305.17493</link><description>&lt;p&gt;
&#36882;&#24402;&#30340;&#35781;&#21650;&#65306;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#35753;&#27169;&#22411;&#24536;&#35760;
&lt;/p&gt;
&lt;p&gt;
The Curse of Recursion: Training on Generated Data Makes Models Forget. (arXiv:2305.17493v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17493
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#20174;&#25551;&#36848;&#24615;&#25991;&#26412;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;GPT-2&#12289;GPT-3(.5)&#21644;GPT-4&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#24778;&#20154;&#12290;ChatGPT&#23558;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#22823;&#20247;&#35270;&#37326;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#19981;&#21487;&#36991;&#20813;&#24182;&#23558;&#24443;&#24213;&#25913;&#21464;&#22312;&#32447;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20010;&#29983;&#24577;&#31995;&#32479;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26410;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#20107;&#24773;&#12290;&#24403;LLMs&#21344;&#25454;&#20102;&#22312;&#32447;&#35821;&#35328;&#30340;&#22823;&#37096;&#20998;&#26102;&#65292;GPT-{n}&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#20250;&#23548;&#33268;&#25152;&#24471;&#27169;&#22411;&#20013;&#19981;&#21487;&#36870;&#32570;&#38519;&#65292;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#65292;&#24182;&#26174;&#31034;&#23427;&#21487;&#20197;&#21457;&#29983;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12289;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;LLMs&#20013;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29616;&#35937;&#32972;&#21518;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23601;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#32763;&#35793;&#25351;&#20196;&#25191;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#22810;&#35821;&#35328;LLMs&#20855;&#26377;&#36739;&#24378;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#36825;&#21462;&#20915;&#20110;&#35821;&#35328;&#19982;&#33521;&#35821;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#27492;&#22806;&#65292;&#25191;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.15083</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35821;&#35328;&#24494;&#35843;&#21644;&#32763;&#35793;&#25351;&#20196;&#35825;&#21457;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions. (arXiv:2305.15083v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#32763;&#35793;&#25351;&#20196;&#25191;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#22810;&#35821;&#35328;LLMs&#20855;&#26377;&#36739;&#24378;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#36825;&#21462;&#20915;&#20110;&#35821;&#35328;&#19982;&#33521;&#35821;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#27492;&#22806;&#65292;&#25191;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;ChatGPT&#21644;GPT4&#65292;&#23637;&#29616;&#20986;&#22312;&#22810;&#35821;&#35328;&#32763;&#35793;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#35757;&#32451;&#24182;&#34892;&#35821;&#26009;&#24211;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22914;&#20309;&#33719;&#24471;&#20854;&#23545;&#19981;&#21516;&#35821;&#35328;&#36827;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;XGLM-7B&#36827;&#34892;&#24494;&#35843;&#26469;&#25191;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;LLMs&#20855;&#26377;&#27604;&#20808;&#21069;&#23637;&#31034;&#30340;&#26356;&#24378;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;&#23545;&#20110;&#26576;&#31181;&#35821;&#35328;&#65292;&#20854;&#34920;&#29616;&#21462;&#20915;&#20110;&#20854;&#19982;&#33521;&#35821;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#25191;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23545;&#32763;&#35793;&#25351;&#20196;&#30340;&#29702;&#35299;&#20197;&#21450;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#22810;&#35821;&#35328;&#24494;&#35843;&#65292;LLMs&#33021;&#22815;&#23398;&#20064;&#24182;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#23545;&#20110;&#37027;&#20123;&#35821;&#35328;&#38388;&#24179;&#34892;&#35821;&#26009;&#36739;&#23569;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translations, without being explicitly trained on parallel corpora. It is interesting how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs' ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning, LLMs could learn to perform the translation task well even for those language pa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#20449;&#24687;&#21644;&#24378;&#35843;&#39044;&#27979;&#22120;&#23454;&#29616;&#34920;&#29616;&#21147;&#24378;&#19988;&#33258;&#28982;&#30340;TTS&#31995;&#32479;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12107</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#20449;&#24687;&#23454;&#29616;&#24378;&#28872;&#24863;&#24773;&#34920;&#29616;&#21644;&#35821;&#35843;&#30340;TTS&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
EE-TTS: Emphatic Expressive TTS with Linguistic Information. (arXiv:2305.12107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12107
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#20449;&#24687;&#21644;&#24378;&#35843;&#39044;&#27979;&#22120;&#23454;&#29616;&#34920;&#29616;&#21147;&#24378;&#19988;&#33258;&#28982;&#30340;TTS&#31995;&#32479;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;TTS&#31995;&#32479;&#33021;&#22815;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#65292;&#20294;&#35201;&#20135;&#29983;&#39640;&#24230;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24378;&#35843;&#26159;&#20915;&#23450;&#35821;&#38899;&#34920;&#29616;&#21147;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#28155;&#21152;&#20013;&#38388;&#29305;&#24449;&#26469;&#22686;&#24378;&#24378;&#35843;&#65292;&#20294;&#19981;&#33021;&#20445;&#35777;&#35821;&#38899;&#30340;&#25972;&#20307;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EE-TTS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21477;&#27861;&#21644;&#35821;&#20041;&#19978;&#30340;&#22810;&#23618;&#35821;&#35328;&#20449;&#24687;&#12290;EE-TTS&#21253;&#21547;&#19968;&#20010;&#24378;&#35843;&#20301;&#32622;&#39044;&#27979;&#22120;&#65292;&#21487;&#20197;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#20986;&#36866;&#24403;&#30340;&#24378;&#35843;&#20301;&#32622;&#65292;&#24182;&#20855;&#26377;&#19968;&#20010;&#26465;&#20214;&#22768;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#21512;&#25104;&#24102;&#24378;&#35843;&#21644;&#35821;&#35328;&#20449;&#24687;&#30340;&#34920;&#29616;&#21147;&#35821;&#38899;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EE-TTS&#22312;&#34920;&#29616;&#21147;&#21644;&#33258;&#28982;&#24230;&#26041;&#38754;&#27604;&#22522;&#32447;&#26377;0.49&#21644;0.67&#30340;MOS&#25552;&#21319;&#12290;EE-TTS&#36824;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26681;&#25454;AB&#27979;&#35797;&#32467;&#26524;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Current TTS systems perform well in synthesizing high-quality speech, producing highly expressive speech remains a challenge. Emphasis, as a critical factor in determining the expressiveness of speech, has attracted more attention nowadays. Previous works usually enhance the emphasis by adding intermediate features, but they can not guarantee the overall expressiveness of the speech. To resolve this matter, we propose Emphatic Expressive TTS (EE-TTS), which leverages multi-level linguistic information from syntax and semantics. EE-TTS contains an emphasis predictor that can identify appropriate emphasis positions from text and a conditioned acoustic model to synthesize expressive speech with emphasis and linguistic information. Experimental results indicate that EE-TTS outperforms baseline with MOS improvements of 0.49 and 0.67 in expressiveness and naturalness. EE-TTS also shows strong generalization across different datasets according to AB test results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#26377;&#25928;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.12888</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#23545;&#25239;&#21435;&#20559;&#32622;&#23454;&#29616;&#30340;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution Evidence-aware Fake News Detection via Dual Adversarial Debiasing. (arXiv:2304.12888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#26377;&#25928;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26088;&#22312;&#23545;&#26032;&#38395;&#21644;&#22522;&#20110;&#26032;&#38395;&#20869;&#23481;&#26816;&#32034;&#30340;&#35777;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#26597;&#25214;&#32479;&#19968;&#24615;&#25110;&#19981;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#35777;&#25454;&#24863;&#30693;&#26816;&#27979;&#27169;&#22411;&#20250;&#21463;&#21040;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#21363;&#26032;&#38395;/&#35777;&#25454;&#20869;&#23481;&#21644;&#30495;&#23454;/&#20551;&#26032;&#38395;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#24456;&#38590;&#25512;&#24191;&#21040;&#36234;&#30028;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;DAL&#20013;&#21152;&#20837;&#20102;&#26032;&#38395;&#26041;&#38754;&#21644;&#35777;&#25454;&#26041;&#38754;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#23427;&#20204;&#30340;&#30446;&#26631;&#37117;&#26159;&#30495;&#20551;&#26032;&#38395;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;DAL&#20250;&#36870;&#21521;&#20248;&#21270;&#26032;&#38395;&#26041;&#38754;&#21644;&#35777;&#25454;&#26041;&#38754;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#20197;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;DAL&#36824;&#20248;&#21270;&#20027;&#35201;&#30340;&#20551;&#26032;&#38395;&#39044;&#27979;&#22120;&#65292;&#35753;&#26032;&#38395;-&#35777;&#25454;&#20132;&#20114;&#27169;&#22359;&#33021;&#22815;&#34987;&#23398;&#20064;&#12290;&#36825;&#20010;&#36807;&#31243;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#25945;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#26032;&#38395;&#35777;&#25454;&#25512;&#29702;&#65292;&#24182;&#23558;&#26816;&#27979;&#20551;&#26032;&#38395;&#30340;&#36127;&#38754;&#24433;&#21709;&#38477;&#33267;&#26368;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence-aware fake news detection aims to conduct reasoning between news and evidence, which is retrieved based on news content, to find uniformity or inconsistency. However, we find evidence-aware detection models suffer from biases, i.e., spurious correlations between news/evidence contents and true/fake news labels, and are hard to be generalized to Out-Of-Distribution (OOD) situations. To deal with this, we propose a novel Dual Adversarial Learning (DAL) approach. We incorporate news-aspect and evidence-aspect debiasing discriminators, whose targets are both true/fake news labels, in DAL. Then, DAL reversely optimizes news-aspect and evidence-aspect debiasing discriminators to mitigate the impact of news and evidence content biases. At the same time, DAL also optimizes the main fake news predictor, so that the news-evidence interaction module can be learned. This process allows us to teach evidence-aware fake news detection models to better conduct news-evidence reasoning, and min
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20851;&#27880;&#22270;&#35299;&#26512;Transformer&#20013;&#30340;&#21069;&#39304;&#27169;&#22359;&#65292;&#25581;&#31034;&#20102;&#20854;&#20462;&#25913;&#36755;&#20837;&#35821;&#22659;&#21270;&#20197;&#24378;&#35843;&#29305;&#23450;&#31867;&#22411;&#35821;&#35328;&#32452;&#21512;&#30340;&#20316;&#29992;&#65292;&#24182;&#26263;&#31034;&#20102;Transformer&#23618;&#22788;&#29702;&#20013;&#30340;&#28508;&#22312;&#20887;&#20313;&#12290;</title><link>http://arxiv.org/abs/2302.00456</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#27880;&#22270;&#35299;&#26512;Transformer&#20013;&#30340;&#21069;&#39304;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Map. (arXiv:2302.00456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00456
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20851;&#27880;&#22270;&#35299;&#26512;Transformer&#20013;&#30340;&#21069;&#39304;&#27169;&#22359;&#65292;&#25581;&#31034;&#20102;&#20854;&#20462;&#25913;&#36755;&#20837;&#35821;&#22659;&#21270;&#20197;&#24378;&#35843;&#29305;&#23450;&#31867;&#22411;&#35821;&#35328;&#32452;&#21512;&#30340;&#20316;&#29992;&#65292;&#24182;&#26263;&#31034;&#20102;Transformer&#23618;&#22788;&#29702;&#20013;&#30340;&#28508;&#22312;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;Transformer&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#35299;&#37322;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29305;&#23450;&#32452;&#20214;&#65292;&#21069;&#39304;(FF)&#27169;&#22359;&#65292;&#23613;&#31649;&#23427;&#20204;&#26377;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#20294;&#36890;&#24120;&#34987;&#20998;&#26512;&#24471;&#36739;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;FF&#27169;&#22359;&#22312;&#20851;&#27880;&#22270;&#20013;&#28210;&#26579;&#20986;&#26469;&#20316;&#20026;&#19968;&#31181;&#26131;&#20110;&#29702;&#35299;&#30340;&#21487;&#35270;&#21270;&#26041;&#26696;&#65292;&#26469;&#20998;&#26512;FF&#27169;&#22359;&#30340;&#36755;&#20837;&#35821;&#22659;&#25928;&#26524;&#12290;&#25105;&#20204;&#23545;&#26377;&#23631;&#34109;&#21644;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FF&#32593;&#32476;&#20462;&#25913;&#20102;&#36755;&#20837;&#30340;&#35821;&#22659;&#21270;&#20197;&#24378;&#35843;&#29305;&#23450;&#31867;&#22411;&#30340;&#35821;&#35328;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;FF&#27169;&#22359;&#21450;&#20854;&#21608;&#22260;&#30340;&#32452;&#20214;&#24448;&#24448;&#20250;&#20114;&#30456;&#25269;&#28040;&#25928;&#26524;&#65292;&#34920;&#26126;Transformer&#23618;&#30340;&#22788;&#29702;&#20013;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#30340;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given that Transformers are ubiquitous in wide tasks, interpreting their internals is a pivotal issue. Still, their particular components, feed-forward (FF) blocks, have typically been less analyzed despite their substantial parameter amounts. We analyze the input contextualization effects of FF blocks by rendering them in the attention maps as a human-friendly visualization scheme. Our experiments with both masked- and causal-language models reveal that FF networks modify the input contextualization to emphasize specific types of linguistic compositions. In addition, FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.13854</link><description>&lt;p&gt;
ComCLIP: &#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#26041;&#38754;&#30340;&#24456;&#22909;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558; CLIP &#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#38656;&#35201;&#27169;&#22411;&#29702;&#35299;&#32452;&#21512;&#35789;&#27010;&#24565;&#21644;&#35270;&#35273;&#32452;&#20214;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#65306;&#21333;&#20010;&#23454;&#20307;&#30340;&#38169;&#35823;&#35821;&#20041;&#26412;&#36136;&#19978;&#26159;&#23548;&#33268;&#21305;&#37197;&#22833;&#36133;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26080;&#38656;&#35757;&#32451;&#8221;&#30340;&#32452;&#21512; CLIP &#27169;&#22411;&#65288;ComCLIP&#65289;&#12290;ComCLIP&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32452;&#21512; CLIP &#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#22312;&#32452;&#21512;&#25991;&#26412;&#23884;&#20837;&#21644;&#23376;&#22270;&#20687;&#23884;&#20837;&#20043;&#19978;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ComCLIP &#21487;&#20197;&#20943;&#36731;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-lanaguage pretrained models like CLIP to compositional image and text matching -- a more challenging image and text matching task requiring the model understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel \textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action sub-images and composes CLIP's vision encoder and text encoder to perform evolving matching over compositional text embedding and sub-image embeddings. In this way, ComCLIP can mitigate spurio
&lt;/p&gt;</description></item></channel></rss>