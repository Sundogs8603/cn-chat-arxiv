<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36776;&#21035;&#21644;&#34920;&#36798;&#20854;&#20869;&#37096;&#30693;&#35782;&#29366;&#24577;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#20855;&#26377;&#33258;&#25105;&#24847;&#35782;&#30340;&#20869;&#37096;&#30693;&#35782;&#29366;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#21644;&#35802;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15449</link><description>&lt;p&gt;
&#23398;&#20250;&#30456;&#20449;&#20320;&#30340;&#24863;&#35273;&#65306;&#21033;&#29992;&#33258;&#25105;&#24847;&#35782;&#22312;&#23545;&#25239;LMM&#20013;&#20943;&#36731;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation. (arXiv:2401.15449v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15449
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36776;&#21035;&#21644;&#34920;&#36798;&#20854;&#20869;&#37096;&#30693;&#35782;&#29366;&#24577;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#20855;&#26377;&#33258;&#25105;&#24847;&#35782;&#30340;&#20869;&#37096;&#30693;&#35782;&#29366;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#21644;&#35802;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36776;&#21035;&#21644;&#34920;&#36798;&#20854;&#20869;&#37096;&#30693;&#35782;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#23545;&#25239;&#20107;&#23454;&#24615;&#24187;&#35273;&#21644;&#30830;&#20445;LLMs&#21487;&#38752;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#20855;&#26377;&#31283;&#20581;&#30340;&#20869;&#37096;&#30693;&#35782;&#29366;&#24577;&#30340;&#33258;&#25105;&#24847;&#35782;&#65292;&#36890;&#36807;&#36229;&#36807;85%&#30340;&#30693;&#35782;&#25506;&#32034;&#20934;&#30830;&#24230;&#26469;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#24120;&#24120;&#26080;&#27861;&#34920;&#36798;&#20854;&#20869;&#37096;&#30693;&#35782;&#65292;&#23548;&#33268;&#20102;&#20107;&#23454;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#24187;&#35273;&#26631;&#27880;&#24037;&#20855;"Dreamcatcher"&#65292;&#23427;&#23558;&#30693;&#35782;&#25506;&#27979;&#21644;&#19968;&#33268;&#24615;&#26816;&#26597;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#23545;&#20107;&#23454;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#25490;&#21517;&#12290;&#36890;&#36807;&#23558;&#30693;&#35782;&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30693;&#35782;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLKF&#65289;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;LLMs&#30340;&#20107;&#23454;&#24615;&#21644;&#35802;&#23454;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RLKF&#35757;&#32451;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#21033;&#29992;&#20854;&#20869;&#37096;&#30693;&#35782;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85% accuracy in knowledge probing. However, LLMs often fail to express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, Dreamcatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#23454;&#20307;&#25110;&#20851;&#31995;&#21305;&#37197;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20107;&#23454;&#25913;&#36827;&#23545;&#29305;&#23450;&#39046;&#22495;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#65292;&#23613;&#31649;&#27809;&#26377;&#20381;&#36182;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.15439</link><description>&lt;p&gt;
Pre-training and Diagnosing Knowledge Base Completion Models. (arXiv:2401.15439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Pre-training and Diagnosing Knowledge Base Completion Models. (arXiv:2401.15439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#23454;&#20307;&#25110;&#20851;&#31995;&#21305;&#37197;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20107;&#23454;&#25913;&#36827;&#23545;&#29305;&#23450;&#39046;&#22495;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#65292;&#23613;&#31649;&#27809;&#26377;&#20381;&#36182;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#26080;&#38656;&#23454;&#20307;&#25110;&#20851;&#31995;&#21305;&#37197;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#19968;&#20010;&#20107;&#23454;&#38598;&#21512;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20107;&#23454;&#38598;&#21512;&#20013;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#35268;&#33539;&#21270;&#30693;&#35782;&#24211;&#21644;&#38750;&#35268;&#33539;&#21270;&#25110;&#24320;&#25918;&#24335;&#30693;&#35782;&#24211;&#65292;&#21363;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#23454;&#20307;&#25110;&#20851;&#31995;&#30340;&#30693;&#35782;&#24211;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25910;&#38598;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20107;&#23454;&#65292;&#25913;&#36827;&#23545;&#29305;&#23450;&#39046;&#22495;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;&#24341;&#20837;&#30340;&#26041;&#27861;&#22312;&#23567;&#25968;&#25454;&#38598;&#65288;&#22914;ReVerb20k&#65289;&#19978;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#23613;&#31649;&#27809;&#26377;&#20381;&#36182;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;Bert&#65292;&#20294;&#24179;&#22343;&#20498;&#25968;&#25490;&#21517;&#22686;&#21152;&#20102;6&#20010;&#30334;&#20998;&#28857;&#65292;&#24179;&#22343;&#25490;&#21517;&#30456;&#23545;&#20943;&#23569;&#20102;65%&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#24320;&#25918;&#24335;&#30693;&#35782;&#24211;&#23436;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;...
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce and analyze an approach to knowledge transfer from one collection of facts to another without the need for entity or relation matching. The method works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge bases where more than one copy of a real-world entity or relation may exist. The main contribution is a method that can make use of large-scale pre-training on facts, which were collected from unstructured text, to improve predictions on structured data from a specific domain. The introduced method is most impactful on small datasets such as ReVerb20k, where a 6% absolute increase of mean reciprocal rank and 65% relative decrease of mean rank over the previously best method was achieved, despite not relying on large pre-trained models like Bert. To understand the obtained pre-trained models better, we then introduce a novel dataset for the analysis of pre-trained models for Open Knowledge Base Completion, calle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PT-Pump-Up&#65292;&#19968;&#22871;&#26088;&#22312;&#25552;&#39640;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#21487;&#35775;&#38382;&#24615;&#30340;&#24037;&#20855;&#65292;&#24182;&#21253;&#25324;&#20102;&#19968;&#20010;Web&#24179;&#21488;&#12289;&#19968;&#20010;&#23458;&#25143;&#31471;Python&#36719;&#20214;&#21253;&#12289;&#19968;&#20010;&#31649;&#29702;&#24179;&#21488;&#30340;&#31649;&#29702;Python&#36719;&#20214;&#21253;&#21644;&#19968;&#20010;&#20844;&#20849;GitHub&#23384;&#20648;&#24211;&#12290;</title><link>http://arxiv.org/abs/2401.15400</link><description>&lt;p&gt;
&#20351;&#29992;PT-Pump-Up&#32034;&#24341;&#33889;&#33796;&#29273;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Indexing Portuguese NLP Resources with PT-Pump-Up. (arXiv:2401.15400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PT-Pump-Up&#65292;&#19968;&#22871;&#26088;&#22312;&#25552;&#39640;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#21487;&#35775;&#38382;&#24615;&#30340;&#24037;&#20855;&#65292;&#24182;&#21253;&#25324;&#20102;&#19968;&#20010;Web&#24179;&#21488;&#12289;&#19968;&#20010;&#23458;&#25143;&#31471;Python&#36719;&#20214;&#21253;&#12289;&#19968;&#20010;&#31649;&#29702;&#24179;&#21488;&#30340;&#31649;&#29702;Python&#36719;&#20214;&#21253;&#21644;&#19968;&#20010;&#20844;&#20849;GitHub&#23384;&#20648;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#38656;&#35201;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#35757;&#32451;&#36807;&#31243;&#30456;&#20851;&#12290;&#30001;&#20110;&#36164;&#28304;&#20998;&#25955;&#21644;&#38656;&#35201;&#32500;&#25252;&#36825;&#20123;&#22522;&#30784;&#35774;&#26045;&#30340;&#22312;&#32447;&#21644;&#26356;&#26032;&#65292;&#35775;&#38382;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#19981;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#12290;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#21644;&#36866;&#24403;&#30340;&#36164;&#28304;&#31649;&#29702;&#22522;&#30784;&#35774;&#26045;&#65292;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#33889;&#33796;&#29273;&#35821;&#65289;&#22312;NLP&#26041;&#38754;&#30340;&#26032;&#21457;&#23637;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PT-Pump-Up&#65292;&#19968;&#22871;&#26088;&#22312;&#20943;&#23569;&#36164;&#28304;&#20998;&#25955;&#24182;&#25552;&#39640;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#21487;&#35775;&#38382;&#24615;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#20998;&#20026;&#22235;&#20010;&#36719;&#20214;&#32452;&#20214;&#65306;a&#65289;&#19968;&#20010;&#21015;&#20986;&#21487;&#29992;&#36164;&#28304;&#30340;Web&#24179;&#21488;&#65307;b&#65289;&#19968;&#20010;&#23458;&#25143;&#31471;Python&#36719;&#20214;&#21253;&#65292;&#31616;&#21270;&#33889;&#33796;&#29273;&#35821;NLP&#36164;&#28304;&#30340;&#21152;&#36733;&#65307;c&#65289;&#19968;&#20010;&#31649;&#29702;&#24179;&#21488;&#30340;&#31649;&#29702;Python&#36719;&#20214;&#21253;&#65307;d&#65289;&#19968;&#20010;&#20844;&#20849;GitHub&#23384;&#20648;&#24211;
&lt;/p&gt;
&lt;p&gt;
The recent advances in natural language processing (NLP) are linked to training processes that require vast amounts of corpora. Access to this data is commonly not a trivial process due to resource dispersion and the need to maintain these infrastructures online and up-to-date. New developments in NLP are often compromised due to the scarcity of data or lack of a shared repository that works as an entry point to the community. This is especially true in low and mid-resource languages, such as Portuguese, which lack data and proper resource management infrastructures. In this work, we propose PT-Pump-Up, a set of tools that aim to reduce resource dispersion and improve the accessibility to Portuguese NLP resources. Our proposal is divided into four software components: a) a web platform to list the available resources; b) a client-side Python package to simplify the loading of Portuguese NLP resources; c) an administrative Python package to manage the platform and d) a public GitHub rep
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#22788;&#29702;&#22810;&#35789;&#34920;&#36798;&#24335;&#30340;&#35821;&#20041;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25429;&#25417;MWE&#35821;&#20041;&#26041;&#38754;&#23384;&#22312;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;MWE&#30340;&#21547;&#20041;&#20027;&#35201;&#38598;&#20013;&#22312;&#20307;&#31995;&#32467;&#26500;&#30340;&#26089;&#26399;&#23618;&#20013;&#12290;&#36825;&#20010;&#21457;&#29616;&#23545;&#20110;Transformer&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21487;&#38752;&#25429;&#25417;&#32454;&#31890;&#24230;&#35821;&#20041;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>http://arxiv.org/abs/2401.15393</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#35821;&#20041;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Semantics of Multiword Expressions in Transformer-Based Models: A Survey. (arXiv:2401.15393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15393
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#22788;&#29702;&#22810;&#35789;&#34920;&#36798;&#24335;&#30340;&#35821;&#20041;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25429;&#25417;MWE&#35821;&#20041;&#26041;&#38754;&#23384;&#22312;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;MWE&#30340;&#21547;&#20041;&#20027;&#35201;&#38598;&#20013;&#22312;&#20307;&#31995;&#32467;&#26500;&#30340;&#26089;&#26399;&#23618;&#20013;&#12290;&#36825;&#20010;&#21457;&#29616;&#23545;&#20110;Transformer&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21487;&#38752;&#25429;&#25417;&#32454;&#31890;&#24230;&#35821;&#20041;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35789;&#34920;&#36798;&#24335;&#65288;MWEs&#65289;&#30001;&#22810;&#20010;&#35789;&#32452;&#25104;&#65292;&#20855;&#26377;&#21487;&#21464;&#30340;&#32452;&#21512;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#21547;&#20041;&#38590;&#20197;&#24314;&#27169;&#65292;&#32780;&#19988;&#19981;&#28165;&#26970;&#36825;&#20010;&#38382;&#39064;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;&#38024;&#23545;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#20351;&#29992;Transformer&#27169;&#22411;&#22788;&#29702;MWE&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#25429;&#25417;MWE&#35821;&#20041;&#26041;&#38754;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#34920;&#38754;&#27169;&#24335;&#21644;&#35760;&#24518;&#20449;&#24687;&#20026;&#20381;&#36182;&#12290;MWE&#30340;&#21547;&#20041;&#20063;&#22312;&#20307;&#31995;&#32467;&#26500;&#30340;&#26089;&#26399;&#23618;&#20013;&#23616;&#37096;&#21270;&#12290;&#34920;&#31034;&#25910;&#30410;&#20110;&#29305;&#23450;&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#20363;&#22914;&#30446;&#26631;&#34920;&#36798;&#24335;&#30340;&#36739;&#20302;&#35821;&#20041;&#24322;&#36136;&#24615;&#21644;&#27495;&#20041;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#24635;&#20307;&#19978;&#36136;&#30097;Transformer&#27169;&#22411;&#23545;&#32454;&#31890;&#24230;&#35821;&#20041;&#30340;&#21487;&#38752;&#25429;&#25417;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#38656;&#35201;&#26356;&#30452;&#25509;&#21487;&#27604;&#30340;&#35780;&#20272;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiword expressions (MWEs) are composed of multiple words and exhibit variable degrees of compositionality. As such, their meanings are notoriously difficult to model, and it is unclear to what extent this issue affects transformer architectures. Addressing this gap, we provide the first in-depth survey of MWE processing with transformer models. We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information. MWE meaning is also strongly localized, predominantly in early layers of the architecture. Representations benefit from specific linguistic properties, such as lower semantic idiosyncrasy and ambiguity of target expressions. Our findings overall question the ability of transformer models to robustly capture fine-grained semantics. Furthermore, we highlight the need for more directly comparable evaluation setups.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#36339;&#26597;&#35810;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#26469;&#25552;&#39640;&#21709;&#24212;&#36136;&#37327;&#12290;&#36890;&#36807;&#26500;&#24314;MultiHop-RAG&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#30693;&#35782;&#24211;&#12289;&#22810;&#20010;&#22810;&#36339;&#26597;&#35810;&#12289;&#30495;&#23454;&#31572;&#26696;&#21644;&#25903;&#25345;&#35777;&#25454;&#65292;&#23637;&#31034;&#20102;&#35813;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15391</link><description>&lt;p&gt;
MultiHop-RAG: &#29992;&#20110;&#22810;&#36339;&#26597;&#35810;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries. (arXiv:2401.15391v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#36339;&#26597;&#35810;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#26469;&#25552;&#39640;&#21709;&#24212;&#36136;&#37327;&#12290;&#36890;&#36807;&#26500;&#24314;MultiHop-RAG&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#30693;&#35782;&#24211;&#12289;&#22810;&#20010;&#22810;&#36339;&#26597;&#35810;&#12289;&#30495;&#23454;&#31572;&#26696;&#21644;&#25903;&#25345;&#35777;&#25454;&#65292;&#23637;&#31034;&#20102;&#35813;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#32531;&#35299;LLM&#30340;&#24187;&#35273;&#21644;&#25552;&#39640;&#21709;&#24212;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;LLM&#22312;&#23454;&#36341;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;RAG&#31995;&#32479;&#22312;&#22238;&#31572;&#22810;&#36339;&#26597;&#35810;&#26041;&#38754;&#19981;&#36275;&#65292;&#36825;&#38656;&#35201;&#26816;&#32034;&#21644;&#25512;&#29702;&#22810;&#20010;&#25903;&#25345;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#20219;&#20309;&#29616;&#26377;&#30340;RAG&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#22810;&#36339;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;MultiHop-RAG&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#30693;&#35782;&#24211;&#12289;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#36339;&#26597;&#35810;&#38598;&#21512;&#12289;&#23427;&#20204;&#30340;&#30495;&#23454;&#31572;&#26696;&#21644;&#30456;&#20851;&#30340;&#25903;&#25345;&#35777;&#25454;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#26412;&#30340;RAG&#30693;&#35782;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;MultiHop-RAG&#30340;&#22522;&#20934;&#27979;&#35797;&#23454;&#29992;&#24615;&#12290;&#31532;&#19968;&#20010;&#23454;&#39564;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#31572;&#26696;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#35821;&#38899;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24207;&#21015;&#21040;&#32467;&#26500;&#29983;&#25104;&#21644;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#35821;&#20041;&#20107;&#20214;&#30340;&#25361;&#25112;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15385</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#29615;&#22659;&#20013;&#25552;&#21462;&#20107;&#20214;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Event Extraction from Speech with Contextual Clues. (arXiv:2401.15385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#35821;&#38899;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24207;&#21015;&#21040;&#32467;&#26500;&#29983;&#25104;&#21644;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#35821;&#20041;&#20107;&#20214;&#30340;&#25361;&#25112;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;&#25991;&#26412;&#30340;&#20107;&#20214;&#25552;&#21462;&#24050;&#25104;&#20026;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#20294;&#30452;&#25509;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#35821;&#20041;&#20107;&#20214;&#26159;&#19968;&#20010;&#23578;&#26410;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#38899;&#20107;&#20214;&#25552;&#21462;&#65288;SpeechEE&#65289;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19977;&#20010;&#21512;&#25104;&#35757;&#32451;&#38598;&#21644;&#19968;&#20010;&#30001;&#20154;&#31867;&#26391;&#35835;&#30340;&#27979;&#35797;&#38598;&#12290;&#19982;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20107;&#20214;&#30456;&#27604;&#65292;SpeechEE&#20027;&#35201;&#30001;&#20110;&#36830;&#32493;&#19988;&#27809;&#26377;&#35789;&#36793;&#30028;&#30340;&#22797;&#26434;&#35821;&#38899;&#20449;&#21495;&#32780;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#19982;&#21487;&#24863;&#30693;&#30340;&#22768;&#38899;&#20107;&#20214;&#19981;&#21516;&#65292;&#35821;&#20041;&#20107;&#20214;&#26356;&#21152;&#24494;&#22937;&#65292;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#32467;&#26500;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#20135;&#29983;&#20107;&#20214;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#35821;&#38899;&#35782;&#21035;&#36716;&#24405;&#20316;&#20026;&#35821;&#22659;&#32447;&#32034;&#30340;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#35758;&#20197;&#25153;&#24179;&#30340;&#26684;&#24335;&#34920;&#31034;&#20107;&#20214;&#65292;&#20351;&#36755;&#20986;&#26356;&#20687;&#33258;&#28982;&#35821;&#35328;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#38899;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While text-based event extraction has been an active research area and has seen successful application in many domains, extracting semantic events from speech directly is an under-explored problem. In this paper, we introduce the Speech Event Extraction (SpeechEE) task and construct three synthetic training sets and one human-spoken test set. Compared to event extraction from text, SpeechEE poses greater challenges mainly due to complex speech signals that are continuous and have no word boundaries. Additionally, unlike perceptible sound events, semantic events are more subtle and require a deeper understanding. To tackle these challenges, we introduce a sequence-to-structure generation paradigm that can produce events from speech signals in an end-to-end manner, together with a conditioned generation method that utilizes speech recognition transcripts as the contextual clue. We further propose to represent events with a flat format to make outputs more natural language-like. Our exper
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15378</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#25552;&#26696;&#65306;MufassirQAS LLM
&lt;/p&gt;
&lt;p&gt;
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15378
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#29702;&#35299;&#23447;&#25945;&#23384;&#22312;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#30340;&#25361;&#25112;&#12290;&#38382;&#31572;&#26426;&#22120;&#20154;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24314;&#31435;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#29992;&#20110;&#23447;&#25945;&#21551;&#33945;&#30340;&#38382;&#39064;&#22238;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;LLM&#20063;&#26377;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20542;&#21521;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#21487;&#33021;&#21253;&#21547;&#20398;&#36785;&#20010;&#20154;&#23447;&#25945;&#20449;&#20208;&#12289;&#36328;&#23447;&#27966;&#20914;&#31361;&#21644;&#26377;&#20105;&#35758;&#25110;&#25935;&#24863;&#30340;&#35805;&#39064;&#30340;&#20869;&#23481;&#12290;&#23427;&#38656;&#35201;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#23459;&#25196;&#20167;&#24680;&#35328;&#35770;&#25110;&#20882;&#29359;&#26576;&#20123;&#32676;&#20307;&#30340;&#20154;&#25110;&#20182;&#20204;&#30340;&#20449;&#20208;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#30340;&#38382;&#31572;&#31995;&#32479;&#31216;&#20026;"MufassirQAS"&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#23447;&#25945;&#34892;&#19994;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
&lt;/p&gt;</description></item><item><title>LegalDuet&#26159;&#19968;&#31181;&#36890;&#36807;&#21452;&#35270;&#35282;&#27861;&#24459;&#32447;&#32034;&#25512;&#29702;&#27169;&#22411;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#23450;&#21046;&#23884;&#20837;&#31354;&#38388;&#26469;&#36827;&#34892;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#27861;&#24459;&#26696;&#20363;&#25512;&#29702;&#21644;&#27861;&#24459;&#22522;&#30784;&#25512;&#29702;&#20004;&#20010;&#25512;&#29702;&#38142;&#36827;&#34892;&#21028;&#20915;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;LegalDuet&#22312;CAIL2018&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.15371</link><description>&lt;p&gt;
LegalDuet: &#36890;&#36807;&#21452;&#35270;&#35282;&#27861;&#24459;&#32447;&#32034;&#25512;&#29702;&#23398;&#20064;&#26377;&#25928;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
LegalDuet: Learning Effective Representations for Legal Judgment Prediction through a Dual-View Legal Clue Reasoning. (arXiv:2401.15371v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15371
&lt;/p&gt;
&lt;p&gt;
LegalDuet&#26159;&#19968;&#31181;&#36890;&#36807;&#21452;&#35270;&#35282;&#27861;&#24459;&#32447;&#32034;&#25512;&#29702;&#27169;&#22411;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#23450;&#21046;&#23884;&#20837;&#31354;&#38388;&#26469;&#36827;&#34892;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#27861;&#24459;&#26696;&#20363;&#25512;&#29702;&#21644;&#27861;&#24459;&#22522;&#30784;&#25512;&#29702;&#20004;&#20010;&#25512;&#29702;&#38142;&#36827;&#34892;&#21028;&#20915;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;LegalDuet&#22312;CAIL2018&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#27169;&#22411;&#20391;&#37325;&#20110;&#21457;&#29616;&#21009;&#20107;&#20107;&#23454;&#25551;&#36848;&#20013;&#30340;&#27861;&#24459;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#19987;&#19994;&#27861;&#23448;&#19981;&#20165;&#38656;&#35201;&#21560;&#25910;&#36807;&#21435;&#21028;&#20915;&#30340;&#27861;&#24459;&#26696;&#20363;&#32463;&#39564;&#65292;&#36824;&#20381;&#36182;&#20110;&#20174;&#19987;&#19994;&#27861;&#24459;&#30693;&#35782;&#20013;&#23398;&#21040;&#30340;&#19987;&#19994;&#27861;&#24459;&#22522;&#30784;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LegalDuet&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#23398;&#20064;&#29992;&#20110;&#36827;&#34892;&#27861;&#24459;&#21028;&#20915;&#30340;&#23450;&#21046;&#23884;&#20837;&#31354;&#38388;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#35270;&#35282;&#27861;&#24459;&#32447;&#32034;&#25512;&#29702;&#26426;&#21046;&#65292;&#30001;&#20004;&#20010;&#25512;&#29702;&#38142;&#32452;&#25104;&#65306;1&#65289;&#27861;&#24459;&#26696;&#20363;&#25512;&#29702;&#65292;&#26681;&#25454;&#20174;&#31867;&#27604;/&#28151;&#28102;&#30340;&#27861;&#24459;&#26696;&#20363;&#20013;&#23398;&#21040;&#30340;&#21028;&#20915;&#32463;&#39564;&#36827;&#34892;&#27861;&#24459;&#21028;&#20915;&#65307;2&#65289;&#27861;&#24459;&#22522;&#30784;&#25512;&#29702;&#65292;&#36890;&#36807;&#21305;&#37197;&#21009;&#20107;&#26696;&#20214;&#21644;&#27861;&#24459;&#20915;&#23450;&#20043;&#38388;&#30340;&#27861;&#24459;&#32447;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LegalDuet&#22312;CAIL2018&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing Legal Judgment Prediction (LJP) models focus on discovering the legal triggers in the criminal fact description. However, in real-world scenarios, a professional judge not only needs to assimilate the law case experience that thrives on past sentenced legal judgments but also depends on the professional legal grounded reasoning that learned from professional legal knowledge. In this paper, we propose a LegalDuet model, which pretrains language models to learn a tailored embedding space for making legal judgments. It proposes a dual-view legal clue reasoning mechanism, which derives from two reasoning chains of judges: 1) Law Case Reasoning, which makes legal judgments according to the judgment experiences learned from analogy/confusing legal cases; 2) Legal Ground Reasoning, which lies in matching the legal clues between criminal cases and legal decisions. Our experiments show that LegalDuet achieves state-of-the-art performance on the CAIL2018 dataset and outperforms bas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26631;&#35760;&#37325;&#35201;&#24615;&#20449;&#24687;&#30340;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#38544;&#34255;&#29366;&#24577;&#21644;&#35757;&#32451;&#26799;&#24230;&#30340;&#33539;&#25968;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#21477;&#32423;&#21644;&#25991;&#26723;&#32423;BLEU&#19978;&#20248;&#20110;&#24378;&#22823;&#30340;&#22522;&#32447;&#21644;&#20960;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.15360</link><description>&lt;p&gt;
&#37325;&#35201;&#24615;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Importance-Aware Data Augmentation for Document-Level Neural Machine Translation. (arXiv:2401.15360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26631;&#35760;&#37325;&#35201;&#24615;&#20449;&#24687;&#30340;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#38544;&#34255;&#29366;&#24577;&#21644;&#35757;&#32451;&#26799;&#24230;&#30340;&#33539;&#25968;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#21477;&#32423;&#21644;&#25991;&#26723;&#32423;BLEU&#19978;&#20248;&#20110;&#24378;&#22823;&#30340;&#22522;&#32447;&#21644;&#20960;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;DocNMT&#65289;&#26088;&#22312;&#29983;&#25104;&#36830;&#36143;&#19988;&#36830;&#36143;&#30340;&#32763;&#35793;&#65292;&#19982;&#20854;&#21477;&#32423;&#23545;&#24212;&#29289;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36739;&#38271;&#30340;&#36755;&#20837;&#38271;&#24230;&#21644;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;DocNMT&#24120;&#24120;&#38754;&#20020;&#30528;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26631;&#35760;&#37325;&#35201;&#24615;&#20449;&#24687;&#30340;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#22686;&#24378;&#65288;IADA&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35745;&#31639;&#38544;&#34255;&#29366;&#24577;&#21644;&#35757;&#32451;&#26799;&#24230;&#30340;&#33539;&#25968;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;DocNMT&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;IADA&#31639;&#27861;&#22312;&#21477;&#32423;&#21644;&#25991;&#26723;&#32423;BLEU&#19978;&#22343;&#20248;&#20110;&#24378;&#22823;&#30340;DocNMT&#22522;&#32447;&#21644;&#20960;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20855;&#26377;&#32479;&#35745;&#23398;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level neural machine translation (DocNMT) aims to generate translations that are both coherent and cohesive, in contrast to its sentence-level counterpart. However, due to its longer input length and limited availability of training data, DocNMT often faces the challenge of data sparsity. To overcome this issue, we propose a novel Importance-Aware Data Augmentation (IADA) algorithm for DocNMT that augments the training data based on token importance information estimated by the norm of hidden states and training gradients. We conduct comprehensive experiments on three widely-used DocNMT benchmarks. Our empirical results show that our proposed IADA outperforms strong DocNMT baselines as well as several data augmentation approaches, with statistical significance on both sentence-level and document-level BLEU.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#32452;&#32455;&#21644;&#20171;&#32461;&#65292;&#24182;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15351</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#32508;&#36848;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Neural Topic Models: Methods, Applications, and Challenges. (arXiv:2401.15351v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#32452;&#32455;&#21644;&#20171;&#32461;&#65292;&#24182;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#21644;&#25512;&#26029;&#25991;&#26723;&#30340;&#20027;&#39064;&#27604;&#20363;&#12290;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#19978;&#19979;&#25991;&#25512;&#33616;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#23835;&#36215;&#20419;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#8212;&#8212;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;(NTMs)&#30340;&#20986;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;&#20027;&#39064;&#27169;&#22411;&#19981;&#21516;&#65292;NTMs&#30452;&#25509;&#20248;&#21270;&#21442;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#27169;&#22411;&#29305;&#23450;&#30340;&#25512;&#23548;&#12290;&#36825;&#20351;&#24471;NTMs&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#24182;&#20135;&#29983;&#20102;&#20016;&#23500;&#30340;&#26032;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#30740;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26681;&#25454;&#32593;&#32476;&#32467;&#26500;&#31995;&#32479;&#22320;&#32452;&#32455;&#20102;&#24403;&#21069;NTM&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#38024;&#23545;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#30340;NTMs&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been prevalent for decades to discover latent topics and infer topic proportions of documents in an unsupervised fashion. They have been widely used in various applications like text analysis and context recommendation. Recently, the rise of neural networks has facilitated the emergence of a new research field -- Neural Topic Models (NTMs). Different from conventional topic models, NTMs directly optimize parameters without requiring model-specific derivations. This endows NTMs with better scalability and flexibility, resulting in significant research attention and plentiful new methods and applications. In this paper, we present a comprehensive survey on neural topic models concerning methods, applications, and challenges. Specifically, we systematically organize current NTM methods according to their network structures and introduce the NTMs for various scenarios like short texts and cross-lingual documents. We also discuss a wide range of popular applications built 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#21387;&#32553;&#31639;&#27861;&#30340;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#24635;&#32467;&#20102;&#21508;&#20010;&#31639;&#27861;&#30340;&#25972;&#20307;&#36235;&#21183;&#21644;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.15347</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Compression Algorithms for Language Models. (arXiv:2401.15347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15347
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#21387;&#32553;&#31639;&#27861;&#30340;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#24635;&#32467;&#20102;&#21508;&#20010;&#31639;&#27861;&#30340;&#25972;&#20307;&#36235;&#21183;&#21644;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#65311;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#31639;&#27861;&#25968;&#37327;&#27491;&#22312;&#24555;&#36895;&#22686;&#38271;&#65292;&#20197;&#20174;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#33879;&#36827;&#23637;&#20013;&#21463;&#30410;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#24222;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21103;&#20316;&#29992;&#65292;&#27604;&#22914;&#22686;&#21152;&#30340;&#30899;&#25490;&#25918;&#21644;&#26114;&#36149;&#30340;&#32500;&#25252;&#36153;&#29992;&#12290;&#34429;&#28982;&#35768;&#22810;&#21387;&#32553;&#31639;&#27861;&#22312;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#36807;&#22810;&#30340;&#31639;&#27861;&#65292;&#37096;&#20998;&#30340;&#38590;&#39064;&#22312;&#20110;&#25429;&#25417;&#26032;&#20852;&#36235;&#21183;&#24182;&#35782;&#21035;&#20854;&#22522;&#26412;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#12289;&#20302;&#31209;&#36924;&#36817;&#12289;&#21442;&#25968;&#20849;&#20139;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22312;&#20869;&#30340;&#22810;&#31181;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#19981;&#20165;&#24635;&#32467;&#20102;&#21508;&#31181;&#21387;&#32553;&#31639;&#27861;&#30340;&#25972;&#20307;&#36235;&#21183;&#65292;&#36824;&#36873;&#25321;&#20102;&#20195;&#34920;&#24615;&#31639;&#27861;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27599;&#20010;&#31867;&#21035;&#30340;&#21387;&#32553;&#31639;&#27861;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of com
&lt;/p&gt;</description></item><item><title>&#22312;&#37329;&#34701;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#65292;&#36890;&#36807;&#20026;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#65292;&#25105;&#20204;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#24322;&#26500;&#37329;&#34701;&#25968;&#25454;&#21644;&#20445;&#35777;&#31934;&#24230;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.15328</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance. (arXiv:2401.15328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15328
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#65292;&#36890;&#36807;&#20026;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#65292;&#25105;&#20204;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#24322;&#26500;&#37329;&#34701;&#25968;&#25454;&#21644;&#20445;&#35777;&#31934;&#24230;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#37329;&#34701;&#31561;&#19987;&#19994;&#39046;&#22495;&#20013;&#36935;&#21040;&#20102;&#38169;&#35823;&#20256;&#25773;&#21644;&#20135;&#29983;&#24187;&#35273;&#31561;&#25361;&#25112;&#65292;&#20854;&#20013;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#31934;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22806;&#37096;&#24037;&#20855;&#22686;&#24378;&#33021;&#21147;&#65292;&#23558;&#26576;&#20123;&#25512;&#29702;&#27493;&#39588;&#36716;&#31227;&#21040;&#26356;&#36866;&#21512;&#35813;&#20219;&#21153;&#30340;&#22806;&#37096;&#24037;&#20855;&#19978;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;LLM&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#22312;LLaMA-2 13B Chat&#27169;&#22411;&#19978;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#20351;&#20854;&#26082;&#20805;&#24403;&#8220;&#20219;&#21153;&#36335;&#30001;&#22120;&#8221;&#21448;&#20805;&#24403;&#8220;&#20219;&#21153;&#35299;&#20915;&#22120;&#8221;&#12290;&#35813;&#8220;&#20219;&#21153;&#36335;&#30001;&#22120;&#8221;&#21160;&#24577;&#23558;&#38382;&#39064;&#23450;&#21521;&#21040;LLM&#20869;&#37096;&#22238;&#31572;&#25110;&#36890;&#36807;&#24037;&#20855;&#38598;&#20013;&#30340;&#27491;&#30830;&#24037;&#20855;&#22806;&#37096;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;SFT&#27169;&#22411;Raven&#30456;&#27604;&#22522;&#30784;&#27169;&#22411;&#21644;&#20165;&#26377;SFT&#30340;&#22522;&#20934;&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;&#20102;35.2%&#21644;5.06%&#65292;&#22312;&#31454;&#20105;&#21147;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the LLM's inherent abilities. More concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and 'task solver'. The 'task router' dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with st
&lt;/p&gt;</description></item><item><title>UNSEE&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#32593;&#32476;&#35299;&#20915;&#20102;&#34920;&#31034;&#22349;&#22604;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;&#19982;&#23545;&#27604;&#30446;&#26631;&#30456;&#24403;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.15316</link><description>&lt;p&gt;
UNSEE: &#26080;&#30417;&#30563;&#30340;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
UNSEE: Unsupervised Non-contrastive Sentence Embeddings. (arXiv:2401.15316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15316
&lt;/p&gt;
&lt;p&gt;
UNSEE&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#32593;&#32476;&#35299;&#20915;&#20102;&#34920;&#31034;&#22349;&#22604;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;&#19982;&#23545;&#27604;&#30446;&#26631;&#30456;&#24403;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNSEE&#65288;Unsupervised Non-Contrastive Sentence Embeddings&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;SimCSE&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#20915;&#20102;SimCSE&#20013;&#26367;&#25442;&#23545;&#27604;&#30446;&#26631;&#20026;&#38750;&#23545;&#27604;&#30446;&#26631;&#26102;&#20986;&#29616;&#30340;&#34920;&#31034;&#22349;&#22604;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30446;&#26631;&#32593;&#32476;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#34920;&#31034;&#22349;&#22604;&#12290;&#30446;&#26631;&#32593;&#32476;&#30340;&#24341;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#38750;&#23545;&#27604;&#30446;&#26631;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19982;&#23545;&#27604;&#30446;&#26631;&#30456;&#24403;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#31934;&#24515;&#35843;&#25972;&#21644;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;&#19978;&#36798;&#21040;&#20102;&#24005;&#23792;&#24615;&#33021;&#12290;&#36825;&#19968;&#20840;&#38754;&#30340;&#21162;&#21147;&#20135;&#29983;&#20102;&#20986;&#33394;&#30340;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novel approach that outperforms SimCSE in the Massive Text Embedding benchmark. Our exploration begins by addressing the challenge of representation collapse, a phenomenon observed when contrastive objectives in SimCSE are replaced with non-contrastive objectives. To counter this issue, we propose a straightforward solution known as the target network, effectively mitigating representation collapse. The introduction of the target network allows us to leverage non-contrastive objectives, maintaining training stability while achieving performance improvements comparable to contrastive objectives. Our method has achieved peak performance in non-contrastive sentence embeddings through meticulous fine-tuning and optimization. This comprehensive effort has yielded superior sentence representation models, showcasing the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#20107;&#23454;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#32570;&#38519;&#35782;&#21035;&#21644;&#35299;&#37322;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20256;&#35328;&#21644;&#27450;&#39575;&#24615;&#20027;&#24352;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#24341;&#20837;&#20102;RefuteClaim&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;FlawCheck&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;RefuteClaim&#22312;&#20998;&#31867;&#21644;&#38416;&#26126;&#34394;&#20551;&#20027;&#24352;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15312</link><description>&lt;p&gt;
&#22914;&#20309;&#21453;&#39539;&#20027;&#24352;&#65306;&#36890;&#36807;&#32570;&#38519;&#35782;&#21035;&#21644;&#35299;&#37322;&#36827;&#34892;&#33258;&#21160;&#20107;&#23454;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
How We Refute Claims: Automatic Fact-Checking through Flaw Identification and Explanation. (arXiv:2401.15312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#20107;&#23454;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#32570;&#38519;&#35782;&#21035;&#21644;&#35299;&#37322;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20256;&#35328;&#21644;&#27450;&#39575;&#24615;&#20027;&#24352;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#24341;&#20837;&#20102;RefuteClaim&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;FlawCheck&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;RefuteClaim&#22312;&#20998;&#31867;&#21644;&#38416;&#26126;&#34394;&#20551;&#20027;&#24352;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20107;&#23454;&#26816;&#39564;&#26159;&#27835;&#29702;&#20114;&#32852;&#32593;&#20869;&#23481;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#23613;&#31649;&#21508;&#31181;&#30740;&#31350;&#21033;&#29992;&#20808;&#36827;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#20256;&#35328;&#21644;&#27450;&#39575;&#24615;&#20027;&#24352;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#32570;&#38519;&#30340;&#20107;&#23454;&#26816;&#39564;&#30340;&#26032;&#20219;&#21153;&#65292;&#21253;&#25324;&#26041;&#38754;&#29983;&#25104;&#21644;&#32570;&#38519;&#35782;&#21035;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;RefuteClaim&#65292;&#19968;&#20010;&#19987;&#20026;&#36825;&#19968;&#20219;&#21153;&#35774;&#35745;&#30340;&#26032;&#26694;&#26550;&#12290;&#37492;&#20110;&#30446;&#21069;&#19981;&#23384;&#22312;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlawCheck&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20174;&#19987;&#23478;&#35780;&#35770;&#20013;&#25552;&#21462;&#21644;&#36716;&#25442;&#35265;&#35299;&#32780;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#33719;&#21462;&#30456;&#20851;&#26041;&#38754;&#21644;&#24050;&#30830;&#23450;&#30340;&#32570;&#38519;&#12290;&#23454;&#39564;&#32467;&#26524;&#24378;&#35843;&#20102;RefuteClaim&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#31867;&#21644;&#38416;&#26126;&#34394;&#20551;&#20027;&#24352;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated fact-checking is a crucial task in the governance of internet content. Although various studies utilize advanced models to tackle this issue, a significant gap persists in addressing complex real-world rumors and deceptive claims. To address this challenge, this paper explores the novel task of flaw-oriented fact-checking, including aspect generation and flaw identification. We also introduce RefuteClaim, a new framework designed specifically for this task. Given the absence of an existing dataset, we present FlawCheck, a dataset created by extracting and transforming insights from expert reviews into relevant aspects and identified flaws. The experimental results underscore the efficacy of RefuteClaim, particularly in classifying and elucidating false claims.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;</title><link>http://arxiv.org/abs/2401.15269</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25913;&#21892;&#21307;&#30103;&#25512;&#29702;&#33021;&#21147;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19987;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;GPT-4&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#20174;&#22810;&#39033;&#36873;&#25321;&#39064;&#21040;&#38271;&#31687;&#29983;&#25104;&#31561;&#22810;&#26679;&#21270;&#25361;&#25112;&#30340;&#37324;&#31243;&#30865;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#32534;&#30721;&#30693;&#35782;&#26080;&#27861;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#25628;&#32034;&#25991;&#26723;&#24182;&#26080;&#26465;&#20214;&#25110;&#26377;&#36873;&#25321;&#22320;&#23558;&#20854;&#38468;&#21152;&#21040;LLMs&#30340;&#36755;&#20837;&#26469;&#36827;&#34892;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#26102;&#65292;&#20986;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#33719;&#21462;&#19981;&#27491;&#30830;&#30340;&#25991;&#26723;&#25110;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#21028;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#21307;&#23398;&#25991;&#26412;&#26694;&#26550;Self-BioRAG&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#21644;&#33258;&#25105;&#21453;&#24605;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;84k&#20010;&#32463;&#36807;&#36807;&#28388;&#30340;&#29983;&#29289;&#21307;&#23398;&#25351;&#20196;&#38598;&#26469;&#35757;&#32451;Self-BioRAG&#65292;&#23427;&#20855;&#22791;&#35780;&#20272;&#33258;&#24049;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;UnTrac&#65292;&#36890;&#36807;&#21453;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UnTrac&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.15241</link><description>&lt;p&gt;
&#21453;&#23398;&#20064;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlearning Reveals the Influential Training Data of Language Models. (arXiv:2401.15241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;UnTrac&#65292;&#36890;&#36807;&#21453;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UnTrac&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#35782;&#21035;&#21738;&#20123;&#35757;&#32451;&#25968;&#25454;&#38598;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20174;&#35757;&#32451;&#20013;&#31227;&#38500;&#27599;&#20010;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#20854;&#24433;&#21709;;&#28982;&#32780;&#65292;&#22810;&#27425;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UnTrac&#65292;&#36890;&#36807;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21462;&#28040;&#23398;&#20064;&#26469;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;UnTrac&#38750;&#24120;&#31616;&#21333;; &#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#26469;&#21462;&#28040;&#23398;&#20064;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#22312;&#21462;&#28040;&#23398;&#20064;&#21518;&#27169;&#22411;&#30340;&#39044;&#27979;&#21457;&#29983;&#20102;&#22810;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21542;&#33021;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#27602;&#12289;&#26377;&#20559;&#35265;&#21644;&#19981;&#30495;&#23454;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#31354;&#38388;&#25110;&#22810;&#20010;&#27169;&#22411;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to enhance the performance of language models while mitigating the risks of generating harmful content, it is crucial to identify which training dataset affects the model's outputs. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac, which estimates the influence of a training dataset by unlearning it from the trained model. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. We empirically examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Experimental results demonstrate that our method estimates their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple model checkpoints.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15222</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#20197;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#30149;&#20363;&#26816;&#27979;&#20026;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#23454;&#20307;&#30340;&#35821;&#20041;&#21487;&#33021;&#20250;&#21463;&#21040;&#20462;&#39280;&#35821;&#30340;&#26174;&#33879;&#25913;&#21464;&#65292;&#21253;&#25324;&#23454;&#20307;&#30340;&#21542;&#23450;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#26465;&#20214;&#24615;&#12289;&#20005;&#37325;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#29616;&#26377;&#30340;&#30830;&#23450;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#27169;&#22411;&#28041;&#21450;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#25110;&#29305;&#24449;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#20462;&#39280;&#35821;&#30340;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#21464;&#25442;&#22120;&#26550;&#26500;&#35774;&#35745;&#65292;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;SemEval 2015&#20219;&#21153;14&#35821;&#26009;&#24211;&#21644;&#19968;&#20010;&#26032;&#30340;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#65288;OUD&#65289;&#25968;&#25454;&#38598;&#19978;&#20849;&#21516;&#23398;&#20064;&#21644;&#39044;&#27979;&#20462;&#39280;&#35821;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;SemEval&#20849;&#20139;&#30340;&#20462;&#39280;&#35821;&#20197;&#21450;OUD&#29305;&#23450;&#30340;&#26032;&#20462;&#39280;&#35821;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#19982;&#20197;&#21069;&#21457;&#34920;&#30340;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#24182;&#35780;&#20272;&#20102;&#20165;&#20849;&#20139;&#37096;&#20998;&#20020;&#24202;&#20462;&#39280;&#35821;&#26102;&#30340;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;SemEval 2015&#30340;ShARe&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#24615;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#30456;&#27604;&#20110;GPT-3.5&#65292;GPT-4&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#12290;&#26080;&#35770;&#27169;&#22411;&#35268;&#27169;&#22823;&#23567;&#65292;&#21482;&#35201;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15170</link><description>&lt;p&gt;
LLMs&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#23450;&#24615;&#32534;&#30721;&#65306;&#24605;&#32500;&#38142;&#25512;&#29702;&#22312;&#26576;&#20123;&#35299;&#37322;&#23398;&#20219;&#21153;&#20013;&#33021;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks. (arXiv:2401.15170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#24615;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#30456;&#27604;&#20110;GPT-3.5&#65292;GPT-4&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#12290;&#26080;&#35770;&#27169;&#22411;&#35268;&#27169;&#22823;&#23567;&#65292;&#21482;&#35201;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#24615;&#32534;&#30721;&#25110;&#20869;&#23481;&#20998;&#26512;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21547;&#20041;&#65292;&#20197;&#35782;&#21035;&#36328;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#23450;&#37327;&#27169;&#24335;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#20026;&#33258;&#21160;&#21270;&#32534;&#30721;&#36807;&#31243;&#65288;&#23545;&#25991;&#26412;&#24212;&#29992;&#31867;&#21035;&#26631;&#31614;&#65289;&#25552;&#20379;&#20102;&#28508;&#21147;&#65292;&#20174;&#32780;&#20351;&#20154;&#31867;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#19987;&#27880;&#20110;&#26356;&#26377;&#21019;&#36896;&#21147;&#30340;&#30740;&#31350;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#36825;&#20123;&#35299;&#37322;&#20219;&#21153;&#22996;&#25176;&#32473;&#20154;&#24037;&#26234;&#33021;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#21253;&#25324;&#23545;&#20154;&#25991;&#23398;&#30740;&#31350;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#23494;&#38598;&#27573;&#33853;&#30340;&#19968;&#32452;&#31038;&#20250;&#21382;&#21490;&#32534;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#33021;&#22815;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#65292;&#32780;GPT-3.5&#21017;&#19981;&#33021;&#12290;&#19982;&#25105;&#20204;&#30001;&#20154;&#31867;&#33719;&#24471;&#30340;&#37329;&#26631;&#20934;&#30456;&#27604;&#65292;GPT-4&#22312;3&#20010;&#32534;&#30721;&#20013;&#20855;&#26377;&#20248;&#31168;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#65288;Cohen's &#954; &#8805; 0.79&#65289;&#65292;&#22312;9&#20010;&#32534;&#30721;&#20013;&#26377;8&#20010;&#20855;&#26377;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#65288;&#954; &#8805; 0.6&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;GPT-3.5&#22312;&#25152;&#26377;&#32534;&#30721;&#20013;&#34920;&#29616;&#19981;&#20339;&#65288;mean(&#954;) = 0.34&#65307;max(&#954;) = 0.55&#65289;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#32534;&#30721;&#30340;&#20934;&#30830;&#24615;&#19981;&#21463;&#27169;&#22411;&#35268;&#27169;&#24433;&#21709;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Qualitative coding, or content analysis, extracts meaning from text to discern quantitative patterns across a corpus of texts. Recently, advances in the interpretive abilities of large language models (LLMs) offer potential for automating the coding process (applying category labels to texts), thereby enabling human researchers to concentrate on more creative research aspects, while delegating these interpretive tasks to AI. Our case study comprises a set of socio-historical codes on dense, paragraph-long passages representative of a humanistic study. We show that GPT-4 is capable of human-equivalent interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq 0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8 of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes ($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding fidelity
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#32452;&#20214;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#65292;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#65292;&#19981;&#30456;&#20851;&#25991;&#26723;&#30340;&#21253;&#21547;&#21487;&#33021;&#20250;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.14887</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#37325;&#26032;&#23450;&#20041;RAG&#31995;&#32479;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Redefining Retrieval for RAG Systems. (arXiv:2401.14887v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#32452;&#20214;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#65292;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#65292;&#19981;&#30456;&#20851;&#25991;&#26723;&#30340;&#21253;&#21547;&#21487;&#33021;&#20250;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#36827;&#27493;&#12290;RAG&#31995;&#32479;&#36890;&#36807;&#25972;&#21512;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#38454;&#27573;&#26816;&#32034;&#30340;&#22806;&#37096;&#25968;&#25454;&#26469;&#22686;&#24378;&#20854;&#29983;&#25104;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;LLMs&#30340;&#38480;&#21046;&#65292;&#21518;&#32773;&#20165;&#38480;&#20110;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;RAG&#31995;&#32479;&#20869;LLMs&#30340;&#29983;&#25104;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#20840;&#38754;&#32780;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;IR&#32452;&#20214;&#23545;RAG&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#26816;&#32034;&#22120;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#24212;&#35813;&#20855;&#22791;&#30340;&#29305;&#24449;&#65292;&#37325;&#28857;&#20851;&#27880;&#24212;&#35813;&#26816;&#32034;&#21738;&#31181;&#31867;&#22411;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#25991;&#26723;&#19982;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#30340;&#20301;&#32622;&#20197;&#21450;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20986;&#65292;&#21253;&#21547;&#19981;&#30456;&#20851;&#30340;&#25991;&#26723;&#21487;&#33021;&#20250;&#8230;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs). RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window. Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems. This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved. We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context. Our findings reveal, among other insights, that including irrelevant documents can
&lt;/p&gt;</description></item><item><title>MaLLaM&#26159;&#39532;&#26469;&#35199;&#20122;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;BPE&#20998;&#35789;&#22120;&#65292;&#22312;&#39532;&#26469;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25429;&#25417;&#21644;&#29702;&#35299;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.14680</link><description>&lt;p&gt;
MaLLaM -- &#39532;&#26469;&#35199;&#20122;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MaLLaM -- Malaysia Large Language Model. (arXiv:2401.14680v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14680
&lt;/p&gt;
&lt;p&gt;
MaLLaM&#26159;&#39532;&#26469;&#35199;&#20122;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;BPE&#20998;&#35789;&#22120;&#65292;&#22312;&#39532;&#26469;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25429;&#25417;&#21644;&#29702;&#35299;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39532;&#26469;&#35199;&#20122;&#35821;&#22659;&#19979;&#20174;&#22836;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#20351;&#29992;349GB&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#30456;&#24403;&#20110;90&#20159;&#20010;&#26631;&#35760;&#65289;&#65292;&#20351;&#29992;&#25105;&#20204;&#39044;&#35757;&#32451;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289;&#20998;&#35789;&#22120;&#65292;&#22312;1.1&#20159;&#12289;30&#20159;&#21644;50&#20159;&#21442;&#25968;&#19978;&#35757;&#32451;&#20102;&#27169;&#22411;&#12290;MaLLaM&#22312;&#39532;&#26469;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#23613;&#31649;&#21482;&#26159;&#20351;&#29992;&#20102;90&#20159;&#20010;&#26631;&#35760;&#30340;&#36739;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#25105;&#20204;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;MaLLaM&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;&#19982;ChatGPT3.5&#21644;Malaysian Mistral&#30456;&#27604;&#65292;MaLLaM&#30340;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#29087;&#32451;&#24230;&#65292;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#25429;&#25417;&#21644;&#29702;&#35299;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;MaLLaM&#27169;&#22411;&#22312;&#35813;&#39046;&#22495;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#19982;&#39532;&#26469;&#35199;&#20122;&#35821;&#22659;&#32039;&#23494;&#32852;&#31995;&#30340;&#20840;&#38754;&#35821;&#35328;&#34920;&#31034;&#12290;&#36825;&#39033;&#21162;&#21147;&#26088;&#22312;&#20026;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the gap in Large Language Model pretrained from scratch with Malaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion parameters on a substantial 349GB dataset, equivalent to 90 billion tokens based on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch. MaLLaM contributes to enhanced natural language understanding and generation tasks in the Malay language. Although trained on a smaller dataset of 90 billion tokens, our instruction-tuned MaLLaM models perform competitively. When compared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models demonstrate notable proficiency, underscoring the effectiveness of our approach in capturing and understanding the nuances of the Malaysian language. MaLLaM models mark a significant contribution to the field, providing comprehensive language representations grounded in Malaysian context. This endeavor aims to pave the way for enhanced natural language understanding and generation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.14215</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#20010;&#24615;&#21270;&#32454;&#21270;&#65292;&#22686;&#24378;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#24120;&#35782;&#22686;&#24378;&#24615;&#20869;&#23384;&#26500;&#24314;&#21644;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#65292;&#35760;&#24518;&#21644;&#21033;&#29992;&#35828;&#35805;&#32773;&#30340;&#35282;&#33394;&#26159;&#29983;&#25104;&#22238;&#24212;&#30340;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#32534;&#20889;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#25552;&#20379;&#26080;&#20449;&#24687;&#30340;&#35282;&#33394;&#21477;&#23376;&#65292;&#36825;&#22952;&#30861;&#20102;&#22238;&#24212;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#26469;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#36825;&#20123;&#38382;&#39064;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#19981;&#20135;&#29983;&#19982;&#20854;&#20182;&#35282;&#33394;&#30456;&#30683;&#30462;&#30340;&#35282;&#33394;&#65292;&#25105;&#20204;&#20391;&#37325;&#20110;&#26681;&#25454;&#35774;&#35745;&#30340;&#31574;&#30053;&#65292;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#27492;&#26469;&#32454;&#21270;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#32972;&#26223;&#12290;&#20316;&#20026;&#22810;&#20250;&#35805;&#24773;&#22659;&#20013;&#35282;&#33394;&#25193;&#23637;&#30340;&#20808;&#39537;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#31867;&#20154;&#20010;&#24615;&#32454;&#21270;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.
&lt;/p&gt;</description></item><item><title>BayesPrompt&#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14166</link><description>&lt;p&gt;
BayesPrompt: &#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#19978;&#25351;&#23548;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14166
&lt;/p&gt;
&lt;p&gt;
BayesPrompt&#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;prompt-tuning&#26088;&#22312;&#32553;&#23567;&#19979;&#28216;&#20219;&#21153;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;prompt-tuning&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25345;&#32493;&#36827;&#23637;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#25345;&#20037;&#30340;&#32570;&#38519;&#65306;prompt-tuning&#26041;&#27861;&#26080;&#27861;&#27867;&#21270;&#21040;&#29305;&#23450;&#30340;&#23569;&#26679;&#26412;&#27169;&#24335;&#12290;&#20174;&#20998;&#24067;&#20998;&#26512;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#32972;&#21518;&#30340;&#20869;&#22312;&#38382;&#39064;&#26159;PLMs&#20013;&#21253;&#21547;&#36807;&#22810;&#30340;&#27010;&#24565;&#30693;&#35782;&#21644;&#30446;&#26631;&#19979;&#28216;&#39046;&#22495;&#30340;&#32553;&#20943;&#30693;&#35782;&#65292;&#20004;&#32773;&#20849;&#21516;&#23548;&#33268;PLMs&#22312;&#26222;&#36941;&#30340;&#30693;&#35782;&#23884;&#20837;&#31354;&#38388;&#20013;&#38169;&#35823;&#22320;&#23450;&#20301;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#23545;&#24212;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30452;&#35266;&#22320;&#25506;&#32034;&#20102;&#20197;&#26080;&#20559;&#26041;&#24335;&#36924;&#36817;&#19979;&#28216;&#20219;&#21153;&#30340;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25277;&#35937;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#26377;&#21306;&#21035;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26080;&#27495;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous
&lt;/p&gt;</description></item><item><title>WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13919</link><description>&lt;p&gt;
WebVoyager&#65306;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#30340;Web Agent
&lt;/p&gt;
&lt;p&gt;
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13919
&lt;/p&gt;
&lt;p&gt;
WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#24341;&#39046;&#20102;&#19968;&#20010;&#30001;&#30495;&#23454;&#19990;&#30028;&#20013;&#33258;&#20027;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#25152;&#26631;&#24535;&#30340;&#26032;&#26102;&#20195;&#65292;&#25512;&#21160;&#20102;&#22522;&#20110;&#32593;&#32476;&#30340;&#39640;&#32423;&#20195;&#29702;&#30340;&#21019;&#26032;&#12290;&#29616;&#26377;&#30340;&#32593;&#32476;&#20195;&#29702;&#36890;&#24120;&#21482;&#22788;&#29702;&#19968;&#20010;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#19988;&#20165;&#22312;&#31616;&#21270;&#30340;&#32593;&#32476;&#27169;&#25311;&#22120;&#25110;&#38745;&#24577;&#30340;&#32593;&#32476;&#24555;&#29031;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WebVoyager&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;Web&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#24335;Web&#20195;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#25361;&#25112;&#65292;&#21033;&#29992;&#20102;GPT-4V&#30340;&#24378;&#22823;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;15&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32593;&#31449;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;WebVoyager&#23454;&#29616;&#20102;55.7&#65285;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#26174;&#33879;&#22320;.....
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13802</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#20195;&#30721;&#29983;&#25104;&#12290;LLMs&#20027;&#35201;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#33539;&#24335;&#20013;&#34987;&#29992;&#20110;&#25351;&#23548;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65288;CCD&#65289;&#36825;&#19968;&#38750;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Mistral 7B&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#36827;&#23637;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#23545;&#25552;&#21319;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#27604;&#20102;&#20854;&#22312;Tatabahasa&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13565</link><description>&lt;p&gt;
&#22522;&#20110;Mistral&#30340;&#22823;&#35268;&#27169;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#21319;&#26412;&#22320;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding. (arXiv:2401.13565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Mistral 7B&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#36827;&#23637;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#23545;&#25552;&#21319;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#27604;&#20102;&#20854;&#22312;Tatabahasa&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Mistral 7B&#30340;&#39044;&#35757;&#32451;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#20351;&#29992;&#20102;32.6GB&#30340;&#25968;&#25454;&#38598;&#65292;&#30456;&#24403;&#20110;11&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#24067;&#20102;&#19978;&#19979;&#25991;&#38271;&#24230;&#20026;4096&#21644;32768&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;16384&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39532;&#26469;&#35199;&#20122;Mistral&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#23545;Mistral 7B&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#19987;&#38376;&#35843;&#25972;&#20102;16384&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#25429;&#25417;&#24494;&#22937;&#35821;&#35328;&#32454;&#33410;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#23545;&#27604;&#20102;&#39532;&#26469;&#35199;&#20122;Mistral&#19982;ChatGPT3.5&#21644;Claude 2&#31561;&#33879;&#21517;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21576;&#29616;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#34920;&#26126;&#39532;&#26469;&#35199;&#20122;Mistral&#22312;Tatabahasa&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present significant advancements in the pretraining of Mistral 7B, a large-scale language model, using a dataset of 32.6 GB, equivalent to 1.1 billion tokens. We explore the impact of extending the context length, releasing models with context lengths of 4096 and 32768 tokens, and further refining performance with a specialized 16384 context length instruction-tuned model, we called it Malaysian Mistral.  Our experiments demonstrate the efficacy of continue pretraining and the influence of extended context lengths on Mistral 7B's language understanding capabilities. Additionally, we release a model specifically tuned with a 16384 context length instruction, showcasing its potential for capturing nuanced language intricacies.  Furthermore, our research contributes to the benchmarking of Malaysian Mistral against prominent language models, including ChatGPT3.5 and Claude 2. We present compelling results indicating Malaysian Mistral's superior performance on Tatabahasa (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;"I don't know"&#65288;Idk&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;AI&#21161;&#25163;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2401.13275</link><description>&lt;p&gt;
AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;?
&lt;/p&gt;
&lt;p&gt;
Can AI Assistants Know What They Don't Know?. (arXiv:2401.13275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;"I don't know"&#65288;Idk&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;AI&#21161;&#25163;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;AI&#21161;&#25163;&#22312;&#23545;&#35805;&#12289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#32534;&#20889;&#20195;&#30721;&#21644;&#20351;&#29992;&#24037;&#20855;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#28145;&#20837;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#22312;&#38754;&#23545;&#26576;&#20123;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65289;&#26102;&#20173;&#28982;&#20250;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#12290;AI&#21161;&#25163;&#30340;&#36825;&#31181;&#19981;&#30495;&#23454;&#22238;&#31572;&#21487;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36896;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;AI&#21161;&#25163;&#25298;&#32477;&#22238;&#31572;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#38382;&#39064;&#26159;&#20943;&#23569;&#24187;&#35273;&#21644;&#20351;&#21161;&#25163;&#30495;&#23454;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#8220;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#21161;&#25163;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;&#8220;I don't know&#8221;(Idk)&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#38382;&#39064;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#21161;&#25163;&#19982;&#20854;&#30456;&#24212;&#30340;Idk&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools. Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications. We believe that an AI assistant's refusal to answer questions it does not know is a crucial method for reducing hallucinations and making the assistant truthful. Therefore, in this paper, we ask the question "Can AI assistants know what they don't know and express them through natural language?" To answer this question, we construct a model-specific "I don't know" (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets. Then we align the assistant with its corresponding I
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#30456;&#20851;&#38169;&#35823;&#65292;&#20197;&#23391;&#21152;&#25289;&#35821;&#20026;&#20363;&#65292;&#35752;&#35770;&#20102;&#24615;&#21035;&#30340;&#20551;&#35774;&#21644;&#25512;&#26029;&#65292;&#20197;&#21450;&#36825;&#20123;&#38169;&#35823;&#23548;&#33268;&#30340;&#21518;&#27542;&#27665;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#25552;&#21319;&#35821;&#35328;&#22320;&#20301;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.13165</link><description>&lt;p&gt;
&#22312;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;&#26426;&#22120;&#32763;&#35793;&#20013;&#31216;&#21628;&#38169;&#35823;&#21644;&#24615;&#21035;&#20551;&#35774;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Misgendering and Assuming Gender in Machine Translation when Working with Low-Resource Languages. (arXiv:2401.13165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#30456;&#20851;&#38169;&#35823;&#65292;&#20197;&#23391;&#21152;&#25289;&#35821;&#20026;&#20363;&#65292;&#35752;&#35770;&#20102;&#24615;&#21035;&#30340;&#20551;&#35774;&#21644;&#25512;&#26029;&#65292;&#20197;&#21450;&#36825;&#20123;&#38169;&#35823;&#23548;&#33268;&#30340;&#21518;&#27542;&#27665;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#25552;&#21319;&#35821;&#35328;&#22320;&#20301;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#35770;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#30456;&#20851;&#38169;&#35823;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#37322;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#31181;&#35821;&#35328;&#23618;&#32423;&#30340;&#31038;&#20250;&#21644;&#35745;&#31639;&#22240;&#32032;&#30340;&#19981;&#21487;&#20998;&#21106;&#24615;&#12290;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#27597;&#35821;&#23391;&#21152;&#25289;&#35821;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#28304;&#25991;&#26412;&#20013;&#27809;&#26377;&#25552;&#20379;&#30456;&#24212;&#20449;&#24687;&#26102;&#65292;&#24615;&#21035;&#22914;&#20309;&#34987;&#20551;&#35774;&#21644;&#25512;&#26029;&#20986;&#26469;&#65292;&#24182;&#20197;&#39640;&#36164;&#28304;&#33521;&#35821;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#38169;&#35823;&#23548;&#33268;&#35821;&#35328;&#28040;&#22833;&#21644;&#34920;&#24449;&#20260;&#23475;&#30340;&#21518;&#27542;&#27665;&#21644;&#31038;&#20250;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;&#25552;&#21319;&#35821;&#35328;&#22320;&#20301;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#36171;&#20104;&#35821;&#35328;&#26356;&#22810;&#30340;&#26435;&#23041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter focuses on gender-related errors in machine translation (MT) in the context of low-resource languages. We begin by explaining what low-resource languages are, examining the inseparable social and computational factors that create such linguistic hierarchies. We demonstrate through a case study of our mother tongue Bengali, a global language spoken by almost 300 million people but still classified as low-resource, how gender is assumed and inferred in translations to and from the high(est)-resource English when no such information is provided in source texts. We discuss the postcolonial and societal impacts of such errors leading to linguistic erasure and representational harms, and conclude by discussing potential solutions towards uplifting languages by providing them more agency in MT conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11864</link><description>&lt;p&gt;
&#36890;&#36807;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#21387;&#32553;&#21040;&#20855;&#26377;&#23567;&#20110;&#21313;&#20159;&#21442;&#25968;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#65292;&#23558;&#25512;&#29702;&#36807;&#31243;&#23553;&#35013;&#20026;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;EoTD&#25968;&#25454;&#38598;&#26469;&#23545;SLMs&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;SLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36825;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#31243;&#24207;&#21644;&#24605;&#32500;&#26041;&#31243;&#65289;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;EoTD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;ETD&#20351;&#36825;&#20123;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10893</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Location Sensitive Embedding for Knowledge Graph Embedding. (arXiv:2401.10893v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23558;&#30693;&#35782;&#22270;&#35889;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#12289;&#20302;&#32500;&#24230;&#30340;&#31354;&#38388;&#65292;&#26377;&#21161;&#20110;&#25512;&#29702;&#21644;&#34917;&#20840;&#20219;&#21153;&#12290;&#35813;&#39046;&#22495;&#20027;&#35201;&#20998;&#20026;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#21644;&#35821;&#20041;&#21305;&#37197;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#26080;&#27861;&#26377;&#25928;&#21306;&#20998;&#22270;&#35889;&#20013;&#30340;&#8220;&#22836;&#23454;&#20307;&#8221;&#21644;&#8220;&#23614;&#23454;&#20307;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#12290;LSE&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#32780;&#19981;&#20165;&#20165;&#26159;&#24179;&#31227;&#12290;LSE&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#21253;&#25324;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;&#19982;&#29616;&#26377;&#27169;&#22411;&#30340;&#32852;&#31995;&#65292;&#37117;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#19968;&#31181;&#26356;&#31616;&#21270;&#30340;&#21464;&#20307;LSEd&#21033;&#29992;&#23545;&#35282;&#30697;&#38453;&#36827;&#34892;&#21464;&#25442;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#33021;&#12290;&#22312;&#23545;&#22235;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#27979;&#35797;&#20013;&#65292;LSEd&#35201;&#20040;&#34920;&#29616;&#26356;&#22909;&#65292;&#35201;&#20040;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding transforms knowledge graphs into a continuous, low-dimensional space, facilitating inference and completion tasks. This field is mainly divided into translational distance models and semantic matching models. A key challenge in translational distance models is their inability to effectively differentiate between 'head' and 'tail' entities in graphs. To address this, the novel location-sensitive embedding (LSE) method has been developed. LSE innovatively modifies the head entity using relation-specific mappings, conceptualizing relations as linear transformations rather than mere translations. The theoretical foundations of LSE, including its representational capabilities and its connections to existing models, have been thoroughly examined. A more streamlined variant, LSEd, employs a diagonal matrix for transformations to enhance practical efficiency. In tests conducted on four large-scale datasets for link prediction, LSEd either outperforms or is competitive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#22806;&#37096;&#30693;&#35782;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KCA&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10768</link><description>&lt;p&gt;
&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65306;&#36890;&#36807;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment. (arXiv:2401.10768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#22806;&#37096;&#30693;&#35782;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KCA&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#40784;&#21518;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#21487;&#33021;&#20135;&#29983;&#19982;&#19978;&#19979;&#25991;&#25110;&#19990;&#30028;&#30693;&#35782;&#33258;&#20449;&#30683;&#30462;&#30340;&#21709;&#24212;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#29616;&#35937;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22806;&#37096;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32487;&#25215;&#30340;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#23545;&#40784;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26681;&#25454;&#22806;&#37096;&#30693;&#35782;&#33258;&#21160;&#21046;&#23450;&#32771;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23545;&#20110;&#21253;&#21547;&#30693;&#35782;&#19981;&#19968;&#33268;&#24615;&#30340;&#25968;&#25454;&#65292;KCA&#23454;&#26045;&#20102;&#20960;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22788;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#21644;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20845;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;KCA&#26041;&#27861;&#22312;&#32531;&#35299;&#24187;&#35273;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have proven to be exceptional on a variety of tasks after alignment, they may still produce responses that contradict the context or world knowledge confidently, a phenomenon known as ``hallucination''. In this paper, we demonstrate that reducing the inconsistency between the external knowledge encapsulated in the training data and the intrinsic knowledge inherited in the pretraining corpus could mitigate hallucination in alignment. Specifically, we introduce a novel knowledge consistent alignment (KCA) approach, which involves automatically formulating examinations based on external knowledge for accessing the comprehension of LLMs. For data encompassing knowledge inconsistency, KCA implements several simple yet efficient strategies for processing. We illustrate the superior performance of the proposed KCA approach in mitigating hallucinations across six benchmarks using LLMs of different backbones and scales. Furthermore, we confirm the correlation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.10711</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26088;&#22312;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#23613;&#31649;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22270;&#20687;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#65292;&#20165;&#20165;&#26159;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#38024;&#23545;&#38382;&#39064;&#20851;&#38190;&#26102;&#38388;&#25139;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#24378;&#21046;LMMs&#20351;&#29992;&#38382;&#39064;&#20851;&#38190;&#26102;&#21051;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#21512;&#24182;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#20197;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#36825;&#20123;&#26102;&#21051;&#23558;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20266;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#24369;&#30417;&#30563;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#12290;GCG&#23398;&#20064;&#22810;&#20010;&#39640;&#26031;&#20989;&#25968;&#26469;&#25551;&#36848;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#36890;&#36807;&#21512;&#24182;&#33410;&#28857;&#29305;&#24449;&#12289;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#21644;&#36845;&#20195;&#28436;&#21270;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;KGLN&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;AUC&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;</title><link>http://arxiv.org/abs/2401.10244</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#39537;&#21160;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph driven recommendation model of graph neural network. (arXiv:2401.10244v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#36890;&#36807;&#21512;&#24182;&#33410;&#28857;&#29305;&#24449;&#12289;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#21644;&#36845;&#20195;&#28436;&#21270;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;KGLN&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;AUC&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#21033;&#29992;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#20013;&#30340;&#20010;&#20307;&#33410;&#28857;&#29305;&#24449;&#21512;&#24182;&#65292;&#28982;&#21518;&#36890;&#36807;&#32467;&#21512;&#24433;&#21709;&#22240;&#32032;&#35843;&#25972;&#30456;&#37051;&#23454;&#20307;&#30340;&#32858;&#21512;&#26435;&#37325;&#12290;&#36890;&#36807;&#36845;&#20195;&#65292;&#27169;&#22411;&#20174;&#21333;&#23618;&#36880;&#28176;&#28436;&#21464;&#20026;&#22810;&#23618;&#65292;&#20351;&#23454;&#20307;&#33021;&#22815;&#33719;&#21462;&#20016;&#23500;&#30340;&#22810;&#38454;&#20851;&#32852;&#23454;&#20307;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#23558;&#23454;&#20307;&#21644;&#29992;&#25143;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#20135;&#29983;&#25512;&#33616;&#20998;&#25968;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#21644;&#24433;&#21709;&#22240;&#32032;&#30340;&#25928;&#26524;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;MovieLen-1M&#21644;Book-Crossing&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;KGLN&#30456;&#23545;&#20110;LibFM&#21644;D&#31561;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;AUC&#65288;ROC&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65289;&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
A new graph neural network-based recommendation model called KGLN, which leverages Knowledge Graph (KG) information, was developed to enhance the accuracy and effectiveness of personalized recommendations. This model begins by using a single-layer neural network to merge individual node features in the graph. It then adjusts the aggregation weights of neighboring entities by incorporating influence factors. The model evolves from a single layer to multiple layers through iteration, enabling entities to access extensive multi-order associated entity information. The final step involves integrating features of entities and users to produce a recommendation score. The model's performance was evaluated by comparing its effects on various aggregation methods and influence factors. In tests using the MovieLen-1M and Book-Crossing datasets, KGLN showed an AUC (Area Under the ROC curve) improvement of 0.3% to 5.9% and 1.1% to 8.2%, respectively, over established benchmark methods like LibFM, D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#21040;&#27573;&#26144;&#23556;&#26469;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#30693;&#35782;&#22270;&#35889;&#12290;&#35813;&#26041;&#27861;&#20391;&#37325;&#20110;&#35821;&#20041;&#35299;&#26512;&#65292;&#35299;&#20915;&#20102;&#29702;&#35299;&#38382;&#39064;&#20013;&#30340;&#38544;&#21547;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#22797;&#26434;&#32422;&#26463;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32467;&#21512;&#35268;&#21017;&#21644;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#26500;&#24314;&#20102;&#39640;&#31934;&#24230;&#21644;&#20840;&#38754;&#30340;&#35821;&#20041;&#27573;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#38472;&#36848;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.06772</link><description>&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Semantic Parsing for Question Answering over Knowledge Graphs. (arXiv:2401.06772v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#21040;&#27573;&#26144;&#23556;&#26469;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#30693;&#35782;&#22270;&#35889;&#12290;&#35813;&#26041;&#27861;&#20391;&#37325;&#20110;&#35821;&#20041;&#35299;&#26512;&#65292;&#35299;&#20915;&#20102;&#29702;&#35299;&#38382;&#39064;&#20013;&#30340;&#38544;&#21547;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#22797;&#26434;&#32422;&#26463;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32467;&#21512;&#35268;&#21017;&#21644;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#26500;&#24314;&#20102;&#39640;&#31934;&#24230;&#21644;&#20840;&#38754;&#30340;&#35821;&#20041;&#27573;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#38472;&#36848;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#21040;&#27573;&#26144;&#23556;&#26469;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#30693;&#35782;&#22270;&#35889;&#12290;&#35813;&#26041;&#27861;&#20391;&#37325;&#20110;&#35821;&#20041;&#35299;&#26512;&#65292;&#36825;&#26159;&#35299;&#37322;&#38382;&#39064;&#38472;&#36848;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#25361;&#25112;&#22312;&#20110;&#29702;&#35299;&#38382;&#39064;&#20013;&#30340;&#38544;&#21547;&#23454;&#20307;&#12289;&#20851;&#31995;&#20197;&#21450;&#26102;&#38388;&#12289;&#25490;&#24207;&#21644;&#32858;&#21512;&#31561;&#22797;&#26434;&#32422;&#26463;&#65292;&#36825;&#20123;&#32422;&#26463;&#22312;&#30693;&#35782;&#22270;&#35889;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#22522;&#20110;&#35268;&#21017;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#35299;&#26512;&#24182;&#26500;&#24314;&#20102;&#39640;&#31934;&#24230;&#21644;&#20840;&#38754;&#30340;&#35821;&#20041;&#27573;&#24207;&#21015;&#12290;&#36825;&#20123;&#24207;&#21015;&#24418;&#25104;&#35821;&#20041;&#26597;&#35810;&#22270;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#38382;&#39064;&#38472;&#36848;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#35821;&#20041;&#35299;&#26512;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#35821;&#20041;&#27573;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#23545;&#38544;&#21547;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#35299;&#26512;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel method with graph-to-segment mapping for question answering over knowledge graphs, which helps understanding question utterances. This method centers on semantic parsing, a key approach for interpreting these utterances. The challenges lie in comprehending implicit entities, relationships, and complex constraints like time, ordinality, and aggregation within questions, contextualized by the knowledge graph. Our framework employs a combination of rule-based and neural-based techniques to parse and construct highly accurate and comprehensive semantic segment sequences. These sequences form semantic query graphs, effectively representing question utterances. We approach question semantic parsing as a sequence generation task, utilizing an encoder-decoder neural network to transform natural language questions into semantic segments. Moreover, to enhance the parsing of implicit entities and relations, we incorporate a graph neural network that leverages t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#21644;&#25913;&#36827;&#30340;&#27880;&#24847;&#26426;&#21046;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#27880;&#24847;&#26426;&#21046;&#21644;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03591</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#21644;&#25913;&#36827;&#30340;&#27880;&#24847;&#26426;&#21046;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text Classification Based on Knowledge Graphs and Improved Attention Mechanism. (arXiv:2401.03591v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#21644;&#25913;&#36827;&#30340;&#27880;&#24847;&#26426;&#21046;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#27880;&#24847;&#26426;&#21046;&#21644;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#27495;&#20041;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21019;&#26032;&#22320;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;&#25913;&#36827;&#30340;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#24211;&#26469;&#20016;&#23500;&#25991;&#26412;&#19982;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#27010;&#24565;&#30340;&#20869;&#23481;&#12290;&#35813;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#21333;&#35789;&#32423;&#21035;&#19978;&#25805;&#20316;&#65292;&#36890;&#36807;&#25972;&#21512;&#27010;&#24565;&#26469;&#21152;&#28145;&#20854;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#20449;&#24687;&#22686;&#30410;&#26469;&#36873;&#25321;&#37325;&#35201;&#30340;&#21333;&#35789;&#12290;&#28982;&#21518;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#23545;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#21253;&#21547;&#30456;&#20851;&#30340;&#27010;&#24565;&#12290;&#23616;&#37096;&#27880;&#24847;&#26426;&#21046;&#35843;&#25972;&#27599;&#20010;&#27010;&#24565;&#30340;&#26435;&#37325;&#65292;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#20943;&#23569;&#26080;&#20851;&#25110;&#22122;&#38899;&#27010;&#24565;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#23616;&#37096;&#33258;&#27880;&#24847;&#26426;&#21046;&#20013;&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#35745;&#31639;&#20844;&#24335;&#65292;&#30830;&#20445;&#22312;&#25991;&#26412;&#20013;&#20986;&#29616;&#39057;&#29575;&#19981;&#21516;&#30340;&#21333;&#35789;&#33719;&#24471;&#26356;&#39640;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#26368;&#21518;&#65292;&#27169;&#22411;&#37319;&#29992;&#20102;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;Bi-GRU&#65289;&#65292;&#26377;&#25928;&#22320;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#29305;&#24449;&#20197;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
To resolve the semantic ambiguity in texts, we propose a model, which innovatively combines a knowledge graph with an improved attention mechanism. An existing knowledge base is utilized to enrich the text with relevant contextual concepts. The model operates at both character and word levels to deepen its understanding by integrating the concepts. We first adopt information gain to select import words. Then an encoder-decoder framework is used to encode the text along with the related concepts. The local attention mechanism adjusts the weight of each concept, reducing the influence of irrelevant or noisy concepts during classification. We improve the calculation formula for attention scores in the local self-attention mechanism, ensuring that words with different frequencies of occurrence in the text receive higher attention scores. Finally, the model employs a Bi-directional Gated Recurrent Unit (Bi-GRU), which is effective in feature extraction from texts for improved classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#19978;&#19979;&#25991;&#21644;&#23383;&#38754;&#20449;&#24687;&#23481;&#32435;&#21040;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#20013;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#35268;&#21017;&#21644;&#23383;&#38754;&#20449;&#24687;&#30340;&#34920;&#31034;&#35745;&#31639;&#32622;&#20449;&#24230;&#21644;&#30456;&#20851;&#24615;&#25351;&#26631;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02968</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#35268;&#21017;&#24341;&#23548;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rule-Guided Joint Embedding Learning of Knowledge Graphs. (arXiv:2401.02968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#19978;&#19979;&#25991;&#21644;&#23383;&#38754;&#20449;&#24687;&#23481;&#32435;&#21040;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#20013;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#35268;&#21017;&#21644;&#23383;&#38754;&#20449;&#24687;&#30340;&#34920;&#31034;&#35745;&#31639;&#32622;&#20449;&#24230;&#21644;&#30456;&#20851;&#24615;&#25351;&#26631;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#20851;&#27880;&#28857;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23398;&#20064;&#19978;&#65292;&#35813;&#23398;&#20064;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#32534;&#30721;&#20026;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#12290;&#23613;&#31649;&#24403;&#21069;&#27169;&#22411;&#20027;&#35201;&#32771;&#34385;&#36825;&#20123;&#22270;&#35889;&#30340;&#32467;&#26500;&#26041;&#38754;&#65292;&#20294;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21644;&#23383;&#38754;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#20110;&#26356;&#26377;&#25928;&#30340;&#23884;&#20837;&#23398;&#20064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#19978;&#19979;&#25991;&#21644;&#23383;&#38754;&#20449;&#24687;&#23481;&#32435;&#21040;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#20013;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25105;&#20204;&#36890;&#36807;&#32622;&#20449;&#24230;&#21644;&#30456;&#20851;&#24615;&#25351;&#26631;&#35780;&#20272;&#20854;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#32622;&#20449;&#24230;&#25351;&#26631;&#65292;&#24182;&#20174;&#23383;&#38754;&#20449;&#24687;&#30340;&#34920;&#31034;&#20013;&#24471;&#20986;&#30456;&#20851;&#24615;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35814;&#23613;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent studies, the focus has been on enhancing knowledge graph embedding learning, which encodes entities and relations in knowledge graphs into low-dimensional vector spaces. While current models mainly consider the structural aspects of these graphs, there's a wealth of contextual and literal information in knowledge graphs that can be utilized for more effective embeddings. This paper introduces a novel model that incorporates both contextual and literal information into entity and relation embeddings, utilizing graph convolutional networks. Specifically, for contextual information, we assess its significance through confidence and relatedness metrics. A unique rule-based method is developed to calculate the confidence metric, and the relatedness metric is derived from the literal information's representations. We validated our model's performance with thorough experiments on two established benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TEILP&#30340;&#36923;&#36753;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;&#30693;&#35782;&#22270;&#35889;&#36716;&#25442;&#20026;&#26102;&#38388;&#20107;&#20214;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#21644;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2312.15816</link><description>&lt;p&gt;
TEILP:&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TEILP: Time Prediction over Knowledge Graphs via Logical Reasoning. (arXiv:2312.15816v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15816
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TEILP&#30340;&#36923;&#36753;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;&#30693;&#35782;&#22270;&#35889;&#36716;&#25442;&#20026;&#26102;&#38388;&#20107;&#20214;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#21644;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#27169;&#22411;&#30340;&#26041;&#27861;&#23558;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#20013;&#30340;&#20107;&#20214;&#26102;&#38388;&#39044;&#27979;&#35270;&#20026;&#19968;&#31181;&#25490;&#24207;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#35832;&#22914;&#39034;&#24207;&#21644;&#36317;&#31163;&#31561;&#37325;&#35201;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TEILP&#65292;&#36825;&#26159;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#23558;&#36825;&#20123;&#26102;&#38388;&#35201;&#32032;&#34701;&#20837;&#21040;&#30693;&#35782;&#22270;&#35889;&#30340;&#39044;&#27979;&#20013;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;TKG&#36716;&#25442;&#20026;&#19968;&#20010;&#20855;&#26377;&#26356;&#26126;&#30830;&#26102;&#38388;&#34920;&#31034;&#30340;&#26102;&#38388;&#20107;&#20214;&#30693;&#35782;&#22270;&#35889;&#65288;TEKG&#65289;&#12290;TEKG&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#26469;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19982;&#26597;&#35810;&#26102;&#38388;&#38388;&#38548;&#30456;&#20851;&#30340;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#36890;&#36807;&#36825;&#20123;&#20989;&#25968;&#24471;&#20986;&#26102;&#38388;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;TEILP&#19982;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#35299;&#37322;&#30340;&#21516;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional embedding-based models approach event time prediction in temporal knowledge graphs (TKGs) as a ranking problem. However, they often fall short in capturing essential temporal relationships such as order and distance. In this paper, we propose TEILP, a logical reasoning framework that naturally integrates such temporal elements into knowledge graph predictions. We first convert TKGs into a temporal event knowledge graph (TEKG) which has a more explicit representation of time in term of nodes of the graph. The TEKG equips us to develop a differentiable random walk approach to time prediction. Finally, we introduce conditional probability density functions, associated with the logical rules involving the query interval, using which we arrive at the time prediction. We compare TEILP with state-of-the-art methods on five benchmark datasets. We show that our model achieves a significant improvement over baselines while providing interpretable explanations. In particular, we cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#21516;&#20041;&#35789;&#25968;&#25454;&#65292;&#22312;&#35299;&#20915;&#19978;&#19979;&#25991;&#29305;&#23450;&#35789;&#20041;&#30340;&#25361;&#25112;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10048</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Enhanced Aspect-Level Sentiment Analysis. (arXiv:2312.10048v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#21516;&#20041;&#35789;&#25968;&#25454;&#65292;&#22312;&#35299;&#20915;&#19978;&#19979;&#25991;&#29305;&#23450;&#35789;&#20041;&#30340;&#25361;&#25112;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#19978;&#19979;&#25991;&#29305;&#23450;&#35789;&#20041;&#30340;&#25361;&#25112;&#65292;&#22686;&#24378;&#24773;&#24863;&#20998;&#26512;&#12290;&#23427;&#23558;BERT&#27169;&#22411;&#30340;&#20248;&#21183;&#19982;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21516;&#20041;&#35789;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#21327;&#21516;&#20316;&#29992;&#21033;&#29992;&#21160;&#24577;&#27880;&#24847;&#26426;&#21046;&#26469;&#26500;&#24314;&#19968;&#20010;&#30693;&#35782;&#39537;&#21160;&#30340;&#29366;&#24577;&#21521;&#37327;&#12290;&#20026;&#20102;&#23545;&#29305;&#23450;&#26041;&#38754;&#38142;&#25509;&#30340;&#24773;&#24863;&#36827;&#34892;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#20301;&#32622;&#25968;&#25454;&#30340;&#35760;&#24518;&#24211;&#12290;&#28982;&#21518;&#20351;&#29992;DCGRU&#20998;&#26512;&#25968;&#25454;&#65292;&#20197;&#30830;&#23450;&#19982;&#29305;&#23450;&#26041;&#38754;&#26415;&#35821;&#30456;&#20851;&#30340;&#24773;&#24863;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#31867;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method to enhance sentiment analysis by addressing the challenge of context-specific word meanings. It combines the advantages of a BERT model with a knowledge graph based synonym data. This synergy leverages a dynamic attention mechanism to develop a knowledge-driven state vector. For classifying sentiments linked to specific aspects, the approach constructs a memory bank integrating positional data. The data are then analyzed using a DCGRU to pinpoint sentiment characteristics related to specific aspect terms. Experiments on three widely used datasets demonstrate the superior performance of our method in sentiment classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20266;&#35821;&#20041;&#25439;&#22833;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#23616;&#37096;&#36817;&#20284;&#19978;&#20248;&#21270;&#32422;&#26463;&#30340;&#20284;&#28982;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.03905</link><description>&lt;p&gt;
&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20266;&#35821;&#20041;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints. (arXiv:2312.03905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20266;&#35821;&#20041;&#25439;&#22833;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#23616;&#37096;&#36817;&#20284;&#19978;&#20248;&#21270;&#32422;&#26463;&#30340;&#20284;&#28982;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#65288;neuro-symbolic AI&#65289;&#22635;&#34917;&#20102;&#32431;&#31526;&#21495;&#21644;&#31070;&#32463;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#36825;&#36890;&#24120;&#38656;&#35201;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#26041;&#38754;&#26368;&#22823;&#21270;&#23545;&#31526;&#21495;&#32422;&#26463;&#30340;&#20284;&#28982;&#12290;&#36825;&#20123;&#36755;&#20986;&#20998;&#24067;&#36890;&#24120;&#34987;&#20551;&#35774;&#20026;&#23436;&#20840;&#22240;&#23376;&#21270;&#30340;&#12290;&#36825;&#38480;&#21046;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#22312;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#33258;&#22238;&#24402;&#20998;&#24067;&#65288;&#20363;&#22914;transformers&#65289;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#26679;&#30340;&#20998;&#24067;&#19979;&#65292;&#29978;&#33267;&#31616;&#21333;&#32422;&#26463;&#30340;&#27010;&#29575;&#20284;&#28982;&#30340;&#35745;&#31639;&#26159;#P-hard&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#19981;&#26159;&#35797;&#22270;&#23558;&#32422;&#26463;&#24378;&#21152;&#22312;&#25972;&#20010;&#36755;&#20986;&#20998;&#24067;&#19978;&#65292;&#32780;&#26159;&#22312;&#20854;&#38543;&#26426;&#30340;&#23616;&#37096;&#36817;&#20284;&#19978;&#36825;&#26679;&#20570;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#22312;&#20197;&#27169;&#22411;&#26679;&#26412;&#20026;&#20013;&#24515;&#30340;&#22522;&#20110;&#20266;&#20284;&#28982;&#30340;&#36817;&#20284;&#20013;&#20248;&#21270;&#32422;&#26463;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#30340;&#36817;&#20284;&#26159;&#22240;&#23376;&#21270;&#30340;&#65292;&#21487;&#20197;&#37325;&#29992;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26159;&#39640;&#25928;&#35745;&#31639;&#31070;&#32463;&#31526;&#21495;&#25439;&#22833;&#30340;&#20027;&#35201;&#21407;&#21017;&#12290;&#27492;&#22806;&#65292;&#23427;&#26159;&#19968;&#20010;&#23616;&#37096;&#30340;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#20284;&#28982;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive autoregressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is #P-hard. Instead of attempting to enforce the constraint on the entire output distribution, we propose to do so on a random, local approximation thereof. More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihoo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.03731</link><description>&lt;p&gt;
&#22810;&#20010;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#22270;&#24418;&#25552;&#31034;&#30340;MultiGPrompt
&lt;/p&gt;
&lt;p&gt;
MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#21487;&#20197;&#22266;&#26377;&#22320;&#23545;Web&#19978;&#30456;&#20114;&#36830;&#25509;&#30340;&#23545;&#35937;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25903;&#25345;&#19968;&#31995;&#21015;Web&#24212;&#29992;&#65292;&#27604;&#22914;&#32593;&#32476;&#20998;&#26512;&#21644;&#20869;&#23481;&#25512;&#33616;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#27969;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#31471;&#21040;&#31471;&#30417;&#30563;&#26694;&#26550;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#19982;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#30340;&#21487;&#29992;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#24182;&#22686;&#24378;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#22522;&#20110;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#32780;&#25552;&#31034;&#21017;&#34987;&#25552;&#20986;&#26469;&#36827;&#19968;&#27493;&#32553;&#23567;&#39044;&#35757;&#32451;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#30446;&#26631;&#24046;&#36317;&#12290;&#34429;&#28982;&#24050;&#32463;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#24418;&#23398;&#20064;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25506;&#32034;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#21033;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23548;&#33268;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#33021;&#23398;&#20064;&#30340;&#36890;&#29992;&#30693;&#35782;&#30340;&#23376;&#38598;&#21463;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;MultiGPrompt&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#22270;&#24418;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#22312;JAX&#20013;&#25193;&#23637;&#20102;&#33258;&#21160;&#24494;&#20998;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#24494;&#20998;&#39640;&#38454;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#21407;&#22987;&#31639;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20989;&#25968;&#24494;&#20998;&#30340;&#32447;&#24615;&#21270;&#21644;&#36716;&#32622;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#22312;&#20989;&#25968;&#23548;&#25968;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#21644;&#31616;&#21333;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.18727</link><description>&lt;p&gt;
JAX&#20013;&#30340;&#33258;&#21160;&#20989;&#25968;&#24494;&#20998;
&lt;/p&gt;
&lt;p&gt;
Automatic Functional Differentiation in JAX. (arXiv:2311.18727v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18727
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;JAX&#20013;&#25193;&#23637;&#20102;&#33258;&#21160;&#24494;&#20998;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#24494;&#20998;&#39640;&#38454;&#20989;&#25968;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#21407;&#22987;&#31639;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20989;&#25968;&#24494;&#20998;&#30340;&#32447;&#24615;&#21270;&#21644;&#36716;&#32622;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#22312;&#20989;&#25968;&#23548;&#25968;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#21644;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25193;&#23637;&#20102;JAX&#30340;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#24494;&#20998;&#39640;&#38454;&#20989;&#25968;&#65288;&#20989;&#25968;&#31639;&#23376;&#21644;&#31639;&#31526;&#65289;&#12290;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#25968;&#32452;&#30340;&#25512;&#24191;&#65292;&#25105;&#20204;&#26080;&#32541;&#22320;&#20351;&#29992;JAX&#30340;&#29616;&#26377;&#21407;&#35821;&#31995;&#32479;&#26469;&#23454;&#29616;&#39640;&#38454;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#21407;&#22987;&#31639;&#23376;&#65292;&#20316;&#20026;&#26500;&#36896;&#20960;&#31181;&#20851;&#38190;&#31867;&#22411;&#30340;&#20989;&#25968;&#31639;&#23376;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#23545;&#20110;&#27599;&#20010;&#24341;&#20837;&#30340;&#21407;&#22987;&#31639;&#23376;&#65292;&#25105;&#20204;&#25512;&#23548;&#24182;&#23454;&#29616;&#20102;&#32447;&#24615;&#21270;&#21644;&#36716;&#32622;&#35268;&#21017;&#65292;&#19982;JAX&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#30340;&#20869;&#37096;&#21327;&#35758;&#20445;&#25345;&#19968;&#33268;&#12290;&#36825;&#20010;&#22686;&#24378;&#21151;&#33021;&#20801;&#35768;&#20351;&#29992;&#20256;&#32479;&#29992;&#20110;&#20989;&#25968;&#30340;&#30456;&#21516;&#35821;&#27861;&#36827;&#34892;&#20989;&#25968;&#24494;&#20998;&#12290;&#24471;&#21040;&#30340;&#20989;&#25968;&#26799;&#24230;&#26412;&#36523;&#23601;&#26159;&#21487;&#20197;&#22312;python&#20013;&#35843;&#29992;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#24212;&#29992;&#23637;&#31034;&#20102;&#36825;&#20010;&#24037;&#20855;&#30340;&#25928;&#26524;&#21644;&#31616;&#21333;&#24615;&#65292;&#20854;&#20013;&#20989;&#25968;&#23548;&#25968;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#27492;&#24037;&#20316;&#30340;&#28304;&#20195;&#30721;&#24050;&#22312;https://github.com/sail-sg/autofd&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend JAX with the capability to automatically differentiate higher-order functions (functionals and operators). By representing functions as a generalization of arrays, we seamlessly use JAX's existing primitive system to implement higher-order functions. We present a set of primitive operators that serve as foundational building blocks for constructing several key types of functionals. For every introduced primitive operator, we derive and implement both linearization and transposition rules, aligning with JAX's internal protocols for forward and reverse mode automatic differentiation. This enhancement allows for functional differentiation in the same syntax traditionally use for functions. The resulting functional gradients are themselves functions ready to be invoked in python. We showcase this tool's efficacy and simplicity through applications where functional derivatives are indispensable. The source code of this work is released at https://github.com/sail-sg/autofd .
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2311.16522</link><description>&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#20013;&#21160;&#24577;&#25925;&#38556;&#29305;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#36816;&#32500;&#30340;&#26234;&#33021;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#30693;&#35782;&#22270;&#35889;&#26469;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#20026;&#20102;&#39564;&#35777;&#33410;&#28857;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#36827;&#34892;&#20102;&#27599;&#20010;&#33410;&#28857;&#36755;&#20986;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#22330;&#26223;&#20013;&#20934;&#30830;&#22320;&#23450;&#20301;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#24314;&#27169;&#21487;&#20197;&#23450;&#24615;&#22320;&#32771;&#23519;&#25925;&#38556;&#22914;&#20309;&#22312;&#33410;&#28857;&#38388;&#20256;&#25773;&#65292;&#20026;&#20998;&#26512;&#25925;&#38556;&#33410;&#28857;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance the intelligence degree in operation and maintenance, a novel method for fault detection in power grids is proposed. The proposed GNN-based approach first identifies fault nodes through a specialized feature extraction method coupled with a knowledge graph. By incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to help current fault detection. To validate the effectiveness of the node features, a correlation analysis of the output features from each node was conducted. The results from experiments show that this method can accurately locate fault nodes in simulation scenarios with a remarkable accuracy. Additionally, the graph neural network based feature modeling allows for a qualitative examination of how faults spread across nodes, which provides valuable insights for analyzing fault nodes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27468;&#35789;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#36866;&#21512;&#27468;&#35789;&#27468;&#26354;&#30340;&#33410;&#25293;&#35760;&#21495;&#65292;&#24182;&#25581;&#31034;&#28508;&#22312;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2311.15480</link><description>&lt;p&gt;
&#20351;&#29992;&#27468;&#35789;&#33258;&#21160;&#30830;&#23450;&#26032;&#26354;&#35889;&#30340;&#33410;&#25293;&#35760;&#21495;
&lt;/p&gt;
&lt;p&gt;
Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure. (arXiv:2311.15480v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27468;&#35789;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#36866;&#21512;&#27468;&#35789;&#27468;&#26354;&#30340;&#33410;&#25293;&#35760;&#21495;&#65292;&#24182;&#25581;&#31034;&#28508;&#22312;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;(AIGC)&#30340;&#20852;&#36259;&#24613;&#21095;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#23545;&#38899;&#20048;&#32452;&#25104;&#37096;&#20998;&#22914;&#33410;&#25293;&#35760;&#21495;&#36827;&#34892;&#36275;&#22815;&#30340;&#30740;&#31350;&#65292;&#20197;&#21046;&#23450;&#26032;&#20316;&#21697;&#30340;&#31639;&#27861;&#30830;&#23450;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#27468;&#35789;&#27468;&#26354;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#24573;&#35270;&#20102;&#38899;&#20048;&#32454;&#33410;&#65292;&#32780;&#38899;&#20048;&#32454;&#33410;&#23545;&#20110;&#26500;&#24314;&#24378;&#22823;&#30340;&#26694;&#26550;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#33410;&#25293;&#35760;&#21495;&#20026;&#27468;&#26354;&#30340;&#20960;&#20046;&#25152;&#26377;&#26041;&#38754;(&#21253;&#25324;&#30701;&#35821;&#21644;&#38899;&#31526;)&#24314;&#31435;&#20102;&#22522;&#30784;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#27468;&#35789;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#36866;&#21512;&#27468;&#35789;&#27468;&#26354;&#30340;&#33410;&#25293;&#35760;&#21495;&#65292;&#24182;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25581;&#31034;&#28508;&#22312;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#31181;&#19982;&#21457;&#29616;&#27468;&#35789;&#27169;&#24335;&#21644;&#21019;&#24314;&#21516;&#26102;&#21253;&#21547;&#27468;&#35789;&#12289;&#33410;&#22863;&#21644;&#32479;&#35745;&#20449;&#24687;&#30340;&#26032;&#29305;&#24449;&#30456;&#20851;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
There has recently been a sharp increase in interest in Artificial Intelligence-Generated Content (AIGC). Despite this, musical components such as time signatures have not been studied sufficiently to form an algorithmic determination approach for new compositions, especially lyrical songs. This is likely because of the neglect of musical details, which is critical for constructing a robust framework. Specifically, time signatures establish the fundamental rhythmic structure for almost all aspects of a song, including the phrases and notes. In this paper, we propose a novel approach that only uses lyrics as input to automatically generate a fitting time signature for lyrical songs and uncover the latent rhythmic structure utilizing explainable machine learning models. In particular, we devise multiple methods that are associated with discovering lyrical patterns and creating new features that simultaneously contain lyrical, rhythmic, and statistical information. In this approach, the b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21464;&#30005;&#31449;&#21160;&#24577;&#25925;&#38556;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#36890;&#36807;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#21033;&#29992;Neo4j&#22270;&#25968;&#25454;&#24211;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#23454;&#29616;&#23545;&#21464;&#30005;&#31449;&#20013;&#38544;&#34255;&#21361;&#38505;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2311.13708</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21464;&#30005;&#31449;&#21160;&#24577;&#25925;&#38556;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dynamic Fault Analysis in Substations Based on Knowledge Graphs. (arXiv:2311.13708v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21464;&#30005;&#31449;&#21160;&#24577;&#25925;&#38556;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#36890;&#36807;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#21033;&#29992;Neo4j&#22270;&#25968;&#25454;&#24211;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#23454;&#29616;&#23545;&#21464;&#30005;&#31449;&#20013;&#38544;&#34255;&#21361;&#38505;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#21464;&#30005;&#31449;&#38544;&#34255;&#21361;&#38505;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#20998;&#26512;&#26041;&#27861;&#12290;&#39318;&#20808;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#28982;&#21518;&#21033;&#29992;&#22522;&#20110;Elastic-Search&#26500;&#24314;&#30340;&#28789;&#27963;&#20998;&#24067;&#24335;&#25628;&#32034;&#24341;&#25806;&#22788;&#29702;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#20351;&#29992;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#26469;&#35757;&#32451;&#24341;&#25806;&#20013;&#30340;&#25968;&#25454;&#12290;&#32500;&#29305;&#27604;&#31639;&#27861;&#34987;&#25972;&#21512;&#36827;&#26469;&#35299;&#23494;&#38544;&#34255;&#29366;&#24577;&#24207;&#21015;&#65292;&#20415;&#20110;&#23545;&#19982;&#38544;&#34255;&#21361;&#38505;&#30456;&#20851;&#30340;&#23454;&#20307;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#27880;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;Neo4j&#22270;&#25968;&#25454;&#24211;&#21160;&#24577;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#26469;&#21487;&#35270;&#21270;&#21464;&#30005;&#31449;&#20013;&#30340;&#38544;&#34255;&#21361;&#38505;&#12290;&#36890;&#36807;&#23545;&#25991;&#26412;&#35760;&#24405;&#20013;&#25581;&#31034;&#30340;&#20855;&#20307;&#21464;&#30005;&#31449;&#30340;&#38544;&#34255;&#21361;&#38505;&#36827;&#34892;&#26696;&#20363;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the challenge of identifying hidden danger in substations from unstructured text, a novel dynamic analysis method is proposed. We first extract relevant information from the unstructured text, and then leverages a flexible distributed search engine built on Elastic-Search to handle the data. Following this, the hidden Markov model is employed to train the data within the engine. The Viterbi algorithm is integrated to decipher the hidden state sequences, facilitating the segmentation and labeling of entities related to hidden dangers. The final step involves using the Neo4j graph database to dynamically create a knowledge graph that visualizes hidden dangers in the substation. The effectiveness of the proposed method is demonstrated through a case analysis from a specific substation with hidden dangers revealed in the text records.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#20462;&#21098;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25688;&#35201;&#20219;&#21153;&#20013;&#20135;&#29983;&#24187;&#35273;&#30340;&#24773;&#20917;&#36739;&#21407;&#22987;&#27169;&#22411;&#35201;&#23569;&#65292;&#34920;&#29616;&#26356;&#21487;&#38752;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#31232;&#30095;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.09335</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#24187;&#35273;&#22312;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization. (arXiv:2311.09335v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#20462;&#21098;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25688;&#35201;&#20219;&#21153;&#20013;&#20135;&#29983;&#24187;&#35273;&#30340;&#24773;&#20917;&#36739;&#21407;&#22987;&#27169;&#22411;&#35201;&#23569;&#65292;&#34920;&#29616;&#26356;&#21487;&#38752;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#31232;&#30095;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#27169;&#22411;&#24222;&#22823;&#21644;&#26131;&#20135;&#29983;&#24187;&#35273;&#12290;&#24187;&#35273;&#26159;&#20196;&#20154;&#25285;&#24551;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38477;&#20302;&#20102;&#21487;&#38752;&#24615;&#24182;&#24341;&#21457;&#23433;&#20840;&#38382;&#39064;&#12290;&#20462;&#21098;&#26159;&#19968;&#31181;&#36890;&#36807;&#21435;&#38500;&#20887;&#20313;&#26435;&#37325;&#26469;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#31232;&#30095;&#25512;&#29702;&#30340;&#25216;&#26415;&#12290;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19978;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;&#22240;&#27492;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#25104;&#20026;&#29702;&#24819;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20462;&#21098;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25688;&#35201;&#20013;&#20135;&#29983;&#24187;&#35273;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20116;&#20010;&#25688;&#35201;&#25968;&#25454;&#38598;&#12289;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#26041;&#27861;&#21644;&#20116;&#20010;&#32463;&#35843;&#35797;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20462;&#21098;&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#24773;&#20917;&#36739;&#21407;&#22987;&#27169;&#22411;&#35201;&#23569;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20381;&#36182;&#25351;&#23548;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode reliability and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights, enabling more efficient sparse inference. Pruned models yield downstream task performance comparable to the original, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study across five summarization datasets, two state-of-the-art pruning methods, and five instruction-tuned LLMs. Surprisingly, we find that hallucinations from pruned LLMs are less prevalent than the original models. Our analysis suggests that pruned models tend to depend more on th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.08724</link><description>&lt;p&gt;
&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Construction in Power Distribution Networks. (arXiv:2311.08724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#21253;&#25324;&#20854;&#35821;&#20041;&#12289;&#38899;&#38901;&#21644;&#21477;&#27861;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#12290;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#24378;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#23558;&#20998;&#37197;&#25991;&#26412;&#23454;&#20307;&#19982;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21305;&#37197;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30005;&#21147;&#20998;&#37197;&#22330;&#26223;&#20013;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38142;&#25509;&#21508;&#31181;&#23454;&#20307;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a method for knowledge graph construction in power distribution networks. This method leverages entity features, which involve their semantic, phonetic, and syntactic characteristics, in both the knowledge graph of distribution network and the dispatching texts. An enhanced model based on Convolutional Neural Network, is utilized for effectively matching dispatch text entities with those in the knowledge graph. The effectiveness of this model is evaluated through experiments in real-world power distribution dispatch scenarios. The results indicate that, compared with the baselines, the proposed model excels in linking a variety of entity types, demonstrating high overall accuracy in power distribution knowledge graph construction task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;GPT-4V&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20174;&#24120;&#35782;&#30693;&#35782;&#12289;&#32454;&#31890;&#24230;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#20840;&#38754;&#30693;&#35782;&#19982;&#20915;&#31574;&#29702;&#25454;&#19977;&#20010;&#26041;&#38754;&#23545;&#20854;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#32771;&#23519;&#12290;</title><link>http://arxiv.org/abs/2311.07536</link><description>&lt;p&gt;
&#23545;GPT-4V&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering. (arXiv:2311.07536v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;GPT-4V&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20174;&#24120;&#35782;&#30693;&#35782;&#12289;&#32454;&#31890;&#24230;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#20840;&#38754;&#30693;&#35782;&#19982;&#20915;&#31574;&#29702;&#25454;&#19977;&#20010;&#26041;&#38754;&#23545;&#20854;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#32771;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#27169;&#22411;&#65288;MLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#35270;&#35273;&#29702;&#35299;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#39046;&#22495;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30495;&#27491;&#30340;&#25361;&#25112;&#22312;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;VQA&#20219;&#21153;&#65292;&#24182;&#19981;&#20165;&#38656;&#35201;&#35782;&#21035;&#35270;&#35273;&#20803;&#32032;&#65292;&#36824;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#35270;&#35273;&#20449;&#24687;&#24182;&#32467;&#21512;&#20016;&#23500;&#30340;&#23398;&#20064;&#30693;&#35782;&#24211;&#12290;&#20026;&#20102;&#25581;&#31034;MLMs&#29305;&#21035;&#26159;&#26032;&#24341;&#20837;&#30340;GPT-4V&#30340;&#36825;&#20123;&#33021;&#21147;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65306;1&#65289;&#24120;&#35782;&#30693;&#35782;&#65292;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#35270;&#35273;&#32447;&#32034;&#24182;&#36830;&#25509;&#21040;&#36890;&#29992;&#30693;&#35782;&#30340;&#33021;&#21147;&#65307;2&#65289;&#32454;&#31890;&#24230;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#27979;&#35797;&#27169;&#22411;&#20174;&#22270;&#20687;&#20013;&#25512;&#29702;&#20986;&#20855;&#20307;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20854;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#65307;3&#65289;&#20840;&#38754;&#30693;&#35782;&#19982;&#20915;&#31574;&#29702;&#25454;&#65292;&#26816;&#26597;&#27169;&#22411;&#25552;&#20379;&#36923;&#36753;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of multimodal large models (MLMs) has significantly advanced the field of visual understanding, offering remarkable capabilities in the realm of visual question answering (VQA). Yet, the true challenge lies in the domain of knowledge-intensive VQA tasks, which necessitate not just recognition of visual elements, but also a deep comprehension of the visual information in conjunction with a vast repository of learned knowledge. To uncover such capabilities of MLMs, particularly the newly introduced GPT-4V, we provide an in-depth evaluation from three perspectives: 1) Commonsense Knowledge, which assesses how well models can understand visual cues and connect to general knowledge; 2) Fine-grained World Knowledge, which tests the model's skill in reasoning out specific knowledge from images, showcasing their proficiency across various specialized fields; 3) Comprehensive Knowledge with Decision-making Rationales, which examines model's capability to provide logical explanatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#24191;&#27867;&#30740;&#31350;&#20102;Persona&#20998;&#37197;&#23545;LLMs&#25191;&#34892;&#22522;&#26412;&#25512;&#29702;&#20219;&#21153;&#33021;&#21147;&#30340;&#24847;&#22806;&#21103;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#34429;&#28982;&#22312;&#26126;&#30830;&#35201;&#27714;&#26102;&#25298;&#32477;&#21051;&#26495;&#21360;&#35937;&#65292;&#20294;&#22312;&#25198;&#28436;&#29305;&#23450;&#35282;&#33394;&#26102;&#20250;&#34920;&#29616;&#20986;&#21051;&#26495;&#21360;&#35937;&#21644;&#38169;&#35823;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2311.04892</link><description>&lt;p&gt;
&#20559;&#35265;&#28145;&#20837;&#65306;Persona&#20998;&#37197;&#30340;LLMs&#20013;&#30340;&#38544;&#24335;&#25512;&#29702;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs. (arXiv:2311.04892v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04892
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#24191;&#27867;&#30740;&#31350;&#20102;Persona&#20998;&#37197;&#23545;LLMs&#25191;&#34892;&#22522;&#26412;&#25512;&#29702;&#20219;&#21153;&#33021;&#21147;&#30340;&#24847;&#22806;&#21103;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#34429;&#28982;&#22312;&#26126;&#30830;&#35201;&#27714;&#26102;&#25298;&#32477;&#21051;&#26495;&#21360;&#35937;&#65292;&#20294;&#22312;&#25198;&#28436;&#29305;&#23450;&#35282;&#33394;&#26102;&#20250;&#34920;&#29616;&#20986;&#21051;&#26495;&#21360;&#35937;&#21644;&#38169;&#35823;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;LLMs&#20855;&#22791;&#22312;&#20854;&#22238;&#31572;&#20013;&#20307;&#29616;&#19981;&#21516;&#35282;&#33394;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#36890;&#36807;&#8220;&#20320;&#26159;Yoda&#12290;&#35299;&#37322;&#30456;&#23545;&#35770;&#12290;&#8221;&#36825;&#26679;&#30340;&#25552;&#31034;&#12290;&#34429;&#28982;&#36825;&#31181;&#33021;&#21147;&#20801;&#35768;LLMs&#20010;&#24615;&#21270;&#65292;&#24182;&#23454;&#29616;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20294;&#20854;&#23545;LLMs&#33021;&#21147;&#30340;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;Persona&#20998;&#37197;&#23545;LLMs&#25191;&#34892;&#22522;&#26412;&#25512;&#29702;&#20219;&#21153;&#33021;&#21147;&#30340;&#24847;&#22806;&#21103;&#20316;&#29992;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;24&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#12289;4&#20010;LLMs&#21644;19&#20010;&#19981;&#21516;&#30340;&#35282;&#33394;&#65288;&#22914;&#20122;&#27954;&#20154;&#65289;&#65292;&#36328;&#36234;&#20102;5&#20010;&#31038;&#20250;&#20154;&#21475;&#32676;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#22312;&#20844;&#24179;&#30340;&#24140;&#23376;&#19979;&#65292;LLMs&#20013;&#23384;&#22312;&#26681;&#28145;&#33922;&#22266;&#30340;&#20559;&#35265;&#12290;&#34429;&#28982;&#24403;&#26126;&#30830;&#35201;&#27714;&#26102;&#23427;&#20204;&#20844;&#28982;&#25298;&#32477;&#21051;&#26495;&#21360;&#35937;&#65288;&#20363;&#22914;&#8220;&#40657;&#20154;&#22312;&#25968;&#23398;&#19978;&#26159;&#21542;&#19981;&#25797;&#38271;&#65311;&#8221;&#65289;&#65292;&#20294;&#24403;&#20182;&#20204;&#34987;&#35201;&#27714;&#22312;&#25198;&#28436;&#29305;&#23450;&#35282;&#33394;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#21051;&#26495;&#21360;&#35937;&#21644;&#38169;&#35823;&#30340;&#20551;&#35774;&#12290;&#36825;&#20123;&#21487;&#20197;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#34920;&#29616;&#20026;&#25512;&#29702;&#32467;&#26524;&#30340;&#24323;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CORECT&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20851;&#31995;&#26102;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36741;&#21161;&#36328;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2311.04507</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#36741;&#21161;&#36328;&#27169;&#24577;&#20132;&#20114;&#30340;&#20851;&#31995;&#26102;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction. (arXiv:2311.04507v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CORECT&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20851;&#31995;&#26102;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36741;&#21161;&#36328;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35782;&#21035;&#26159;&#20154;&#31867;&#23545;&#35805;&#29702;&#35299;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#38543;&#30528;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24341;&#20837;&#65292;&#22914;&#35821;&#35328;&#12289;&#22768;&#38899;&#21644;&#38754;&#37096;&#34920;&#24773;&#65292;&#36825;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20316;&#20026;&#19968;&#31181;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#39044;&#27979;&#23545;&#35805;&#20013;&#27599;&#20010;&#21477;&#23376;&#65288;&#21363;&#35805;&#35821;&#65289;&#30340;&#24773;&#32490;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20840;&#23616;&#34920;&#31034;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#23545;&#35805;&#32423;&#21035;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#26469;&#25429;&#25417;&#12290;&#23616;&#37096;&#34920;&#31034;&#36890;&#24120;&#26159;&#36890;&#36807;&#35828;&#35805;&#32773;&#25110;&#24773;&#32490;&#21464;&#21270;&#30340;&#26102;&#38388;&#20449;&#24687;&#26469;&#25512;&#26029;&#30340;&#65292;&#24573;&#35270;&#20102;&#35805;&#35821;&#32423;&#21035;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#32479;&#19968;&#36755;&#20837;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#29305;&#24449;&#65292;&#32780;&#19981;&#21033;&#29992;&#27169;&#24577;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#26102;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#36741;&#21161;&#36328;&#27169;&#24577;&#20132;&#20114;&#65288;CORECT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#25928;&#25429;&#25417;&#23545;&#35805;&#20013;&#24773;&#24863;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition is a crucial task for human conversation understanding. It becomes more challenging with the notion of multimodal data, e.g., language, voice, and facial expressions. As a typical solution, the global- and the local context information are exploited to predict the emotional label for every single sentence, i.e., utterance, in the dialogue. Specifically, the global representation could be captured via modeling of cross-modal interactions at the conversation level. The local one is often inferred using the temporal information of speakers or emotional shifts, which neglects vital factors at the utterance level. Additionally, most existing approaches take fused features of multiple modalities in an unified input without leveraging modality-specific representations. Motivating from these problems, we propose the Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), an novel neural network framework that effectively captures convers
&lt;/p&gt;</description></item><item><title>GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;</title><link>http://arxiv.org/abs/2311.01927</link><description>&lt;p&gt;
GateLoop: &#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01927
&lt;/p&gt;
&lt;p&gt;
GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#38271;&#24207;&#21015;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#29616;&#26377;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20854;&#28508;&#21147;&#12290;&#22312;&#36825;&#19968;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GateLoop&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#30784;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#25511;&#21046;&#30340;&#29366;&#24577;&#36716;&#25442;&#26469;&#25512;&#24191;&#32447;&#24615;&#36882;&#24402;&#27169;&#22411;&#65292;&#22914;S4&#12289;S5&#12289;LRU&#21644;RetNet&#12290;&#21033;&#29992;&#36825;&#19968;&#29702;&#35770;&#36827;&#27493;&#65292;GateLoop&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#22312;&#23454;&#35777;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#25104;&#26412;&#30340;$O(l)$&#36882;&#24402;&#27169;&#24335;&#21644;&#39640;&#24230;&#20248;&#21270;&#30340;&#20851;&#32852;&#25195;&#25551;&#23454;&#29616;&#30340;&#39640;&#25928;$O(l \log_{2} l)$&#24182;&#34892;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;$O(l^2)$&#30340;&#20195;&#29702;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23545;Transformer&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#26550;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21521;Attention&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;&#32780;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#26080;&#20851;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#25968;&#25454;&#27745;&#26579;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;15&#20010;&#28909;&#38376;&#27169;&#22411;&#23545;&#20845;&#20010;&#24120;&#35265;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#30340;&#27745;&#26579;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25968;&#25454;&#27745;&#26579;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#27745;&#26579;&#31243;&#24230;&#19981;&#26029;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2310.17589</link><description>&lt;p&gt;
&#19968;&#20221;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#25968;&#25454;&#27745;&#26579;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
An Open Source Data Contamination Report for Large Language Models. (arXiv:2310.17589v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#25968;&#25454;&#27745;&#26579;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;15&#20010;&#28909;&#38376;&#27169;&#22411;&#23545;&#20845;&#20010;&#24120;&#35265;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#30340;&#27745;&#26579;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25968;&#25454;&#27745;&#26579;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#27745;&#26579;&#31243;&#24230;&#19981;&#26029;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#23427;&#20801;&#35768;&#27169;&#22411;&#36890;&#36807;&#35760;&#24518;&#32780;&#19981;&#26159;&#23637;&#31034;&#30495;&#27491;&#30340;&#33021;&#21147;&#26469;"&#20316;&#24330;"&#12290;&#22240;&#27492;&#65292;&#27745;&#26579;&#20998;&#26512;&#24050;&#25104;&#20026;&#21487;&#38752;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#29992;&#20110;&#39564;&#35777;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27745;&#26579;&#20998;&#26512;&#36890;&#24120;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#32773;&#20869;&#37096;&#36827;&#34892;&#65292;&#24448;&#24448;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#23436;&#25972;&#24615;&#12290;&#26412;&#25991;&#20026;&#20845;&#20010;&#24120;&#35265;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#36229;&#36807;15&#20010;&#28909;&#38376;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35814;&#32454;&#25968;&#25454;&#27745;&#26579;&#25253;&#21578;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#27969;&#31243;&#65292;&#20351;&#31038;&#21306;&#33021;&#22815;&#23545;&#33258;&#23450;&#20041;&#25968;&#25454;&#21644;&#27169;&#22411;&#36827;&#34892;&#27745;&#26579;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#22522;&#20934;&#27979;&#35797;&#20013;&#19981;&#21516;&#30340;&#27745;&#26579;&#27700;&#24179;&#65292;&#33539;&#22260;&#20174;1%&#21040;45%&#65292;&#27745;&#26579;&#31243;&#24230;&#38543;&#26102;&#38388;&#36805;&#36895;&#22686;&#21152;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20998;&#26512;&#34920;&#26126;&#65292;&#25968;&#25454;&#27745;&#26579;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to "cheat" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1\% to 45\% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#24352;&#23558;&#35789;&#27719;&#27979;&#35797;&#20316;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#35780;&#20272;&#19971;&#20010;LLM&#27169;&#22411;&#26102;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#35789;&#27719;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#65292;&#24182;&#25506;&#35752;&#20102;LLM&#35789;&#27719;&#34920;&#31034;&#12289;&#23398;&#20064;&#26426;&#21046;&#21644;&#24615;&#33021;&#21464;&#21270;&#30340;&#32454;&#33410;&#12290;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35789;&#27719;&#27979;&#35797;&#20026;&#25193;&#23637;&#36825;&#19968;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.14703</link><description>&lt;p&gt;
&#23558;&#35789;&#27719;&#27979;&#35797;&#20316;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models. (arXiv:2310.14703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#24352;&#23558;&#35789;&#27719;&#27979;&#35797;&#20316;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#35780;&#20272;&#19971;&#20010;LLM&#27169;&#22411;&#26102;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#35789;&#27719;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#65292;&#24182;&#25506;&#35752;&#20102;LLM&#35789;&#27719;&#34920;&#31034;&#12289;&#23398;&#20064;&#26426;&#21046;&#21644;&#24615;&#33021;&#21464;&#21270;&#30340;&#32454;&#33410;&#12290;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35789;&#27719;&#27979;&#35797;&#20026;&#25193;&#23637;&#36825;&#19968;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Llama&#12289;Mistral&#21644;GPT&#20027;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29615;&#22659;&#20013;&#65292;&#35789;&#27719;&#27979;&#35797;&#19968;&#24230;&#34987;&#24573;&#35270;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;LLM&#35780;&#20272;&#22522;&#20934;&#20851;&#27880;&#29305;&#23450;&#20219;&#21153;&#25110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#24573;&#30053;&#20102;&#35821;&#35328;&#29702;&#35299;&#21644;&#20135;&#20986;&#30340;&#22522;&#26412;&#35821;&#35328;&#23398;&#26041;&#38754;&#12290;&#26412;&#25991;&#20027;&#24352;&#24674;&#22797;&#35789;&#27719;&#27979;&#35797;&#20316;&#20026;&#35780;&#20272;LLM&#24615;&#33021;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#35789;&#27719;&#27979;&#35797;&#26684;&#24335;&#35780;&#20272;&#20102;&#19971;&#20010;LLM&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#35789;&#27719;&#30693;&#35782;&#19978;&#30340;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#24046;&#36317;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;LLM&#35789;&#27719;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#12289;&#23427;&#20204;&#30340;&#23398;&#20064;&#26426;&#21046;&#20197;&#21450;&#27169;&#22411;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35789;&#27719;&#27979;&#35797;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#25193;&#23637;&#36825;&#19968;&#26041;&#27861;&#24182;&#25552;&#20379;LLM&#35821;&#35328;&#25216;&#33021;&#26356;&#23436;&#25972;&#30011;&#38754;&#30340;&#26032;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vocabulary tests, once a cornerstone of language modeling evaluation, have been largely overlooked in the current landscape of Large Language Models (LLMs) like Llama, Mistral, and GPT. While most LLM evaluation benchmarks focus on specific tasks or domain-specific knowledge, they often neglect the fundamental linguistic aspects of language understanding and production. In this paper, we advocate for the revival of vocabulary tests as a valuable tool for assessing LLM performance. We evaluate seven LLMs using two vocabulary test formats across two languages and uncover surprising gaps in their lexical knowledge. These findings shed light on the intricacies of LLM word representations, their learning mechanisms, and performance variations across models and languages. Moreover, the ability to automatically generate and perform vocabulary tests offers new opportunities to expand the approach and provide a more complete picture of LLMs' language skills.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#21019;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#12290;</title><link>http://arxiv.org/abs/2310.08383</link><description>&lt;p&gt;
&#37325;&#26500;&#26448;&#26009;&#22235;&#38754;&#20307;&#65306;&#26448;&#26009;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Materials Tetrahedron: Challenges in Materials Information Extraction. (arXiv:2310.08383v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#21019;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#26448;&#26009;&#30340;&#21457;&#29616;&#24050;&#32463;&#26377;&#30528;&#25345;&#32493;&#20960;&#20010;&#19990;&#32426;&#20035;&#33267;&#26356;&#38271;&#26102;&#38388;&#30340;&#25512;&#21160;&#20154;&#31867;&#36827;&#27493;&#30340;&#21382;&#21490;&#12290;&#26448;&#26009;&#30340;&#34892;&#20026;&#21462;&#20915;&#20110;&#20854;&#32452;&#25104;&#12289;&#32467;&#26500;&#21644;&#24615;&#36136;&#65292;&#32780;&#36825;&#20123;&#21448;&#20381;&#36182;&#20110;&#20854;&#21152;&#24037;&#21644;&#27979;&#35797;&#26465;&#20214;&#12290;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21457;&#23637;&#20351;&#24471;&#20174;&#20986;&#29256;&#30340;&#25991;&#29486;&#20013;&#65288;&#22914;&#21516;&#34892;&#35780;&#23457;&#20986;&#29256;&#29289;&#12289;&#22270;&#20070;&#21644;&#19987;&#21033;&#65289;&#22823;&#35268;&#27169;&#25552;&#21462;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20449;&#24687;&#20197;&#19981;&#21516;&#30340;&#26684;&#24335;&#65288;&#22914;&#34920;&#26684;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#23637;&#29616;&#65292;&#24182;&#19988;&#22312;&#25253;&#21578;&#26679;&#24335;&#19978;&#32570;&#20047;&#19968;&#33268;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#12289;&#37327;&#21270;&#21644;&#35760;&#24405;&#20102;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#33258;&#21160;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#26399;&#21019;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20174;&#25991;&#26412;&#21644;&#34920;&#26684;&#20013;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20102;&#20960;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#30740;&#31350;&#33021;&#28608;&#21457;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovery of new materials has a documented history of propelling human progress for centuries and more. The behaviour of a material is a function of its composition, structure, and properties, which further depend on its processing and testing conditions. Recent developments in deep learning and natural language processing have enabled information extraction at scale from published literature such as peer-reviewed publications, books, and patents. However, this information is spread in multiple formats, such as tables, text, and images, and with little or no uniformity in reporting style giving rise to several machine learning challenges. Here, we discuss, quantify, and document these challenges in automated information extraction (IE) from materials science literature towards the creation of a large materials science knowledge base. Specifically, we focus on IE from text and tables and outline several challenges with examples. We hope the present work inspires researchers to address 
&lt;/p&gt;</description></item><item><title>BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07276</link><description>&lt;p&gt;
BioT5&#65306;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#36328;&#27169;&#24577;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07276
&lt;/p&gt;
&lt;p&gt;
BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21033;&#29992;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#25972;&#21512;&#26469;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#29983;&#25104;&#26080;&#25928;&#30340;&#20998;&#23376;SMILES&#12289;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#23545;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#31561;&#37327;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;BioT5&#65292;&#23427;&#36890;&#36807;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#29983;&#29289;&#23398;&#20013;&#30340;&#36328;&#27169;&#24577;&#25972;&#21512;&#12290;BioT5&#21033;&#29992;SELFIES&#36827;&#34892;100%&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#29983;&#29289;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#29289;&#23454;&#20307;&#21608;&#22260;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;BioT5&#21306;&#20998;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20449;&#24687;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;BioT5&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;</title><link>http://arxiv.org/abs/2310.02446</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#36234;&#29425; GPT-4
&lt;/p&gt;
&lt;p&gt;
Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02446
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#22521;&#35757;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32418;&#38431;&#27979;&#35797;&#26159;&#20943;&#23569;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23558;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25104;&#21151;&#32469;&#36807;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#30340;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#22312;AdvBenchmark&#20013;&#65292;GPT-4&#38024;&#23545;&#19981;&#23433;&#20840;&#30340;&#32763;&#35793;&#36755;&#20837;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#19988;79%&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#26696;&#65292;&#20351;&#29992;&#25143;&#23454;&#29616;&#20854;&#26377;&#23475;&#30446;&#26631;&#65292;&#36825;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#20854;&#20182;&#39640;/&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#26174;&#33879;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#36328;&#35821;&#35328;&#28431;&#27934;&#20027;&#35201;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20197;&#21069;&#65292;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#38480;&#35757;&#32451;&#20027;&#35201;&#24433;&#21709;&#37027;&#20123;&#20351;&#29992;&#36825;&#20123;&#35821;&#35328;&#30340;&#20154;&#65292;&#36896;&#25104;&#25216;&#26415;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#36716;&#21464;&#65306;
&lt;/p&gt;
&lt;p&gt;
AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift:
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#26512;&#21644;&#32467;&#26500;&#35782;&#21035;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#24615;&#30340;KV&#32531;&#23384;&#65292;&#36890;&#36807;&#28165;&#38500;&#21644;&#20002;&#24323;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;&#21482;&#23545;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#26631;&#20934;KV&#32531;&#23384;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.01801</link><description>&lt;p&gt;
&#27169;&#22411;&#21578;&#35785;&#20320;&#35813;&#20002;&#24323;&#20160;&#20040;&#65306;&#36866;&#24212;&#24615;KV&#32531;&#23384;&#21387;&#32553;&#29992;&#20110;LLMs
&lt;/p&gt;
&lt;p&gt;
Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01801
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#26512;&#21644;&#32467;&#26500;&#35782;&#21035;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#24615;&#30340;KV&#32531;&#23384;&#65292;&#36890;&#36807;&#28165;&#38500;&#21644;&#20002;&#24323;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;&#21482;&#23545;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#26631;&#20934;KV&#32531;&#23384;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#25512;&#29702;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#19982;&#20256;&#32479;&#30340;KV&#32531;&#23384;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#26512;&#26469;&#35782;&#21035;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#22522;&#20110;&#35782;&#21035;&#20986;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#26500;&#24314;KV&#32531;&#23384;&#65306;&#22312;&#24378;&#35843;&#26412;&#22320;&#19978;&#19979;&#25991;&#30340;&#27880;&#24847;&#21147;&#22836;&#19978;&#28165;&#38500;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#65292;&#22312;&#20197;&#29305;&#27530;&#26631;&#35760;&#20026;&#20013;&#24515;&#30340;&#27880;&#24847;&#21147;&#22836;&#19978;&#20002;&#24323;&#38750;&#29305;&#27530;&#26631;&#35760;&#65292;&#24182;&#19988;&#20165;&#23545;&#24191;&#27867;&#20851;&#27880;&#25152;&#26377;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#26631;&#20934;&#30340;KV&#32531;&#23384;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#27880;&#24847;&#21147;&#20998;&#26512;&#26469;&#25351;&#23548;&#33258;&#36866;&#24212;KV&#32531;&#23384;&#30340;&#26500;&#24314;&#65292;FastGen&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;FastGen&#22312;GPU&#20869;&#23384;&#28040;&#32791;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#20174;&#26059;&#24459;&#20013;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#25506;&#32034;ChatGPT&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00863</link><description>&lt;p&gt;
&#20174;&#26059;&#24459;&#20013;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;
&lt;/p&gt;
&lt;p&gt;
Syllable-level lyrics generation from melody exploiting character-level language model. (arXiv:2310.00863v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#20174;&#26059;&#24459;&#20013;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#25506;&#32034;ChatGPT&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#19982;&#20276;&#22863;&#26059;&#24459;&#32039;&#23494;&#30456;&#20851;&#30340;&#27468;&#35789;&#28041;&#21450;&#24314;&#31435;&#38899;&#20048;&#38899;&#31526;&#19982;&#27468;&#35789;&#38899;&#33410;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#23545;&#38899;&#33410;&#32423;&#12289;&#35789;&#32423;&#21644;&#21477;&#32423;&#35821;&#20041;&#24847;&#20041;&#19978;&#30340;&#38899;&#20048;&#32422;&#26463;&#21644;&#35821;&#20041;&#27169;&#24335;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#20844;&#24320;&#30340;&#38899;&#33410;&#32423;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20197;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#32423;&#27468;&#35789;&#29983;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#30693;&#35782;&#34701;&#20837;&#38899;&#33410;&#32423;Transformer&#29983;&#25104;&#22120;&#32593;&#32476;&#30340;&#26463;&#25628;&#32034;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25506;&#32034;&#22522;&#20110;ChatGPT&#30340;&#29983;&#25104;&#27468;&#35789;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#20027;&#35266;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#28040;&#38500;&#20102;&#35757;&#32451;&#26114;&#36149;&#30340;&#26032;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of lyrics tightly connected to accompanying melodies involves establishing a mapping between musical notes and syllables of lyrics. This process requires a deep understanding of music constraints and semantic patterns at syllable-level, word-level, and sentence-level semantic meanings. However, pre-trained language models specifically designed at the syllable level are publicly unavailable. To solve these challenging issues, we propose to exploit fine-tuning character-level language models for syllable-level lyrics generation from symbolic melody. In particular, our method endeavors to incorporate linguistic knowledge of the language model into the beam search process of a syllable-level Transformer generator network. Additionally, by exploring ChatGPT-based evaluation for generated lyrics, along with human subjective evaluation, we demonstrate that our approach enhances the coherence and correctness of the generated lyrics, eliminating the need to train expensive new la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#23548;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.13340</link><description>&lt;p&gt;
&#38754;&#21521;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;LLM&#24341;&#23548;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-guided Causal Explainability for Black-box Text Classifiers. (arXiv:2309.13340v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#23548;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22823;&#19988;&#26356;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#27604;&#22914;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#20687;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36825;&#26679;&#30340;&#27169;&#22411;&#36136;&#37327;&#65292;&#23613;&#31649;&#38750;&#24120;&#20196;&#20154;&#21521;&#24448;&#65292;&#20294;&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#35299;&#20915;&#12290;&#20363;&#22914;&#65292;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26159;&#35774;&#35745;&#20026;&#40657;&#30418;&#12290;&#23613;&#31649;&#26631;&#20934;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#19968;&#23450;&#31243;&#24230;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#65292;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#26377;&#38480;&#12290;&#22240;&#26524;&#35299;&#37322;&#33021;&#21147;&#26159;&#26356;&#29702;&#24819;&#30340;&#30446;&#26631;&#65292;&#20294;&#22312;NLP&#39046;&#22495;&#21364;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26377;&#24456;&#22810;&#12290;&#21463;&#21040;&#26368;&#36817;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19987;&#23478;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#26368;&#26032;&#30340;LLMs&#30340;&#25351;&#23548;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#23454;&#29616;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#27969;&#31243;&#65292;
&lt;/p&gt;
&lt;p&gt;
With the advent of larger and more complex deep learning models, such as in Natural Language Processing (NLP), model qualities like explainability and interpretability, albeit highly desirable, are becoming harder challenges to tackle and solve. For example, state-of-the-art models in text classification are black-box by design. Although standard explanation methods provide some degree of explainability, these are mostly correlation-based methods and do not provide much insight into the model. The alternative of causal explainability is more desirable to achieve but extremely challenging in NLP due to a variety of reasons. Inspired by recent endeavors to utilize Large Language Models (LLMs) as experts, in this work, we aim to leverage the instruction-following and textual understanding capabilities of recent state-of-the-art LLMs to facilitate causal explainability via counterfactual explanation generation for black-box text classifiers. To do this, we propose a three-step pipeline via
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#12298;Media of Langue&#12299;&#36825;&#19968;&#20840;&#26032;&#35789;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#36890;&#36807;&#25551;&#36848;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#12290;</title><link>http://arxiv.org/abs/2309.08609</link><description>&lt;p&gt;
&#12298;Media of Langue&#12299;&#30340;&#23186;&#20307;
&lt;/p&gt;
&lt;p&gt;
Media of Langue. (arXiv:2309.08609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#12298;Media of Langue&#12299;&#36825;&#19968;&#20840;&#26032;&#35789;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#36890;&#36807;&#25551;&#36848;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23384;&#26723;Goki Muramoto&#31561;&#20154;&#30340;&#12298;Media of Langue&#12299;&#21518;&#38754;&#30340;&#26448;&#26009;&#12290;&#12298;Media of Langue&#12299;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#23383;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#23427;&#20165;&#20174;&#8220;&#36825;&#20010;&#35789;&#34987;&#32763;&#35793;&#25104;&#37027;&#20010;&#35789;&#8221;&#30340;&#24191;&#27867;&#20107;&#20214;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#19978;&#25551;&#36848;&#20986;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#12290;&#39318;&#20808;&#65292;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#24182;&#23558;&#20854;&#19982;&#23383;&#20856;&#12289;&#35821;&#20041;&#31354;&#38388;&#21644;&#35821;&#20041;&#32593;&#32476;&#30340;&#19977;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25509;&#19979;&#26469;&#65292;&#25551;&#36848;&#20102;&#35813;&#20316;&#21697;&#20013;&#23454;&#26045;&#30340;&#20855;&#20307;&#31639;&#27861;&#21644;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to archive the materials behind "Media of Langue" by Goki Muramoto et al. Media of Langue is a new dictionary and public sculpture that depicts the map of meaning on the boundary between languages solely from the vast events of "this word was translated into that word" and two forces: repulsion between all words in the same language and attraction between translated words in different languages. First, the three new concepts proposed, Inter-Langue Map/Dictionary, Inter-Langue Space, and then Inter-Langue Network, are introduced, comparing them to the three domains of dictionary, semantic space, and semantic network. Next, the specific algorithms and designs implemented in the work were described.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#28304;&#23545;&#27604;&#21644;&#35821;&#35328;&#23545;&#27604;&#35299;&#30721;&#26469;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#25233;&#21046;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2309.07098</link><description>&lt;p&gt;
&#36890;&#36807;&#28304;&#23545;&#27604;&#21644;&#35821;&#35328;&#23545;&#27604;&#35299;&#30721;&#26469;&#32531;&#35299;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding. (arXiv:2309.07098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#28304;&#23545;&#27604;&#21644;&#35821;&#35328;&#23545;&#27604;&#35299;&#30721;&#26469;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#25233;&#21046;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#32763;&#35793;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#35299;&#30721;&#30446;&#26631;&#26469;&#32531;&#35299;&#36825;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#22806;&#37096;&#27169;&#22411;&#12290;&#22312;&#28304;&#23545;&#27604;&#35299;&#30721;&#20013;&#65292;&#25105;&#20204;&#23547;&#25214;&#19968;&#20010;&#32763;&#35793;&#65292;&#22312;&#32473;&#23450;&#27491;&#30830;&#36755;&#20837;&#26102;&#26159;&#21487;&#20449;&#30340;&#65292;&#20294;&#22312;&#38543;&#26426;&#36755;&#20837;&#29255;&#27573;&#32473;&#23450;&#26102;&#26159;&#19981;&#21487;&#20449;&#30340;&#65292;&#20551;&#35774;&#24187;&#35273;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#26159;&#21516;&#26679;&#21487;&#20449;&#30340;&#12290;&#22312;&#35821;&#35328;&#23545;&#27604;&#35299;&#30721;&#20013;&#65292;&#25105;&#20204;&#23547;&#25214;&#19968;&#20010;&#32763;&#35793;&#65292;&#22312;&#32473;&#23450;&#27491;&#30830;&#35821;&#35328;&#25351;&#31034;&#31526;&#20196;&#29260;&#26102;&#26159;&#21487;&#20449;&#30340;&#65292;&#20294;&#32473;&#23450;&#38169;&#35823;&#35821;&#35328;&#25351;&#31034;&#31526;&#20196;&#29260;&#26102;&#26159;&#19981;&#21487;&#20449;&#30340;&#12290;&#22312;&#23545;M2M-100 (418M)&#21644;SMaLL-100&#36827;&#34892;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#25233;&#21046;&#20102;&#24187;&#35273;&#21644;&#20559;&#31163;&#30446;&#26631;&#30340;&#32763;&#35793;&#65292;&#24179;&#22343;&#22312;57&#20010;&#27979;&#35797;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#25552;&#39640;&#20102;1.7&#21644;1.4&#20010;chrF2&#20998;&#25968;&#12290;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#25233;&#21046;&#20559;&#31163;&#30446;&#26631;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations and off-target translation remain unsolved problems in machine translation, especially for low-resource languages and massively multilingual models. In this paper, we introduce methods to mitigate both failure cases with a modified decoding objective, without requiring retraining or external models. In source-contrastive decoding, we search for a translation that is probable given the correct input, but improbable given a random input segment, hypothesising that hallucinations will be similarly probable given either. In language-contrastive decoding, we search for a translation that is probable, but improbable given the wrong language indicator token. In experiments on M2M-100 (418M) and SMaLL-100, we find that these methods effectively suppress hallucinations and off-target translations, improving chrF2 by 1.7 and 1.4 points on average across 57 tested translation directions. In a proof of concept on English--German, we also show that we can suppress off-target translat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06054</link><description>&lt;p&gt;
&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#23545;&#21512;&#25104;&#20219;&#21153;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21363;&#20174;&#19978;&#19979;&#25991;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#26159;Transformer&#30340;&#19968;&#39033;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39537;&#21160;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#35282;&#24230;&#36827;&#34892;&#35843;&#26597;&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#34920;&#31034;&#26356;&#21152;&#22797;&#26434;&#65292;&#34920;&#31034;&#21487;&#20197;&#21463;&#21040;&#27169;&#22411;&#26435;&#37325;&#21644;&#19978;&#19979;&#25991;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#19978;&#36848;&#20004;&#20010;&#27010;&#24565;&#26041;&#38754;&#30340;&#34920;&#31034;&#20998;&#21035;&#31216;&#20026;&#26435;&#37325;&#20869;&#37096;&#25104;&#20998;&#21644;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20004;&#20010;&#25104;&#20998;&#22914;&#20309;&#24433;&#21709;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#35774;&#35745;&#20004;&#20010;&#25506;&#38024;&#65292;&#21363;&#26435;&#37325;&#20869;&#37096;&#25506;&#38024;&#21644;&#19978;&#19979;&#25991;&#25506;&#38024;&#65292;&#20998;&#21035;&#35780;&#20272;&#36825;&#20004;&#20010;&#25104;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#30340;&#22909;&#22351;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#65292;&#36825;&#34920;&#26126;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32416;&#32544;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#25928;&#26524;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2309.04174</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#30340;&#26080;&#35843;&#21442;&#25552;&#31034;&#20998;&#31867;&#30340;&#37325;&#26032;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification. (arXiv:2309.04174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#20998;&#31867;&#36890;&#36807;&#21033;&#29992;[MASK]&#26631;&#35760;&#30340;&#36951;&#28431;&#38382;&#39064;&#24418;&#24335;&#26469;&#36866;&#24212;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23558;&#22635;&#20805;&#30340;&#26631;&#35760;&#26144;&#23556;&#21040;&#26631;&#31614;&#19978;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26469;&#20943;&#23569;&#36825;&#19968;&#36807;&#31243;&#20013;&#30340;&#21171;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#38468;&#21152;&#21487;&#35757;&#32451;&#23884;&#20837;&#36827;&#34892;&#35843;&#21442;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#34920;&#31034;&#31354;&#38388;&#20013;&#28508;&#22312;&#30340;&#38750;&#32447;&#24615;&#27969;&#24418;&#65292;&#39640;&#32500;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#19981;&#24212;&#35813;&#20351;&#29992;&#27431;&#27663;&#36317;&#31163;&#26469;&#34913;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35843;&#21442;&#22522;&#20110;&#27969;&#24418;&#30340;&#31354;&#38388;&#37325;&#26032;&#23884;&#20837;&#26041;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#20869;&#31867;&#36817;&#37051;&#32422;&#26463;&#30340;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65288;LLE-INC&#65289;&#65292;&#29992;&#20110;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#65292;&#23427;&#20445;&#30041;&#20102;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#20316;&#20026;&#20998;&#31867;&#30340;&#24341;&#23548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#35843;&#20248;&#65292;&#25105;&#20204;&#30340;LLE-INC&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based classification adapts tasks to a cloze question format utilizing the [MASK] token and the filled tokens are then mapped to labels through pre-defined verbalizers. Recent studies have explored the use of verbalizer embeddings to reduce labor in this process. However, all existing studies require a tuning process for either the pre-trained models or additional trainable embeddings. Meanwhile, the distance between high-dimensional verbalizer embeddings should not be measured by Euclidean distance due to the potential for non-linear manifolds in the representation space. In this study, we propose a tuning-free manifold-based space re-embedding method called Locally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for verbalizer embeddings, which preserves local properties within the same class as guidance for classification. Experimental results indicate that even without tuning any parameters, our LLE-INC is on par with automated verbalizers with parameter 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10335</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#30721;&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21644;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#38750;&#20961;&#33021;&#21147;&#12290;&#24403;&#36935;&#21040;&#32534;&#30721;&#38382;&#39064;&#26102;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#24120;&#24120;&#20250;&#21672;&#35810;LLMs&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#21162;&#21147;&#26469;&#36991;&#20813;&#35821;&#27861;&#38169;&#35823;&#24182;&#20351;&#20195;&#30721;&#19982;&#39044;&#26399;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#20294;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#29615;&#22659;&#20013;&#65292;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#31561;&#21516;&#20110;&#21487;&#38752;&#21644;&#40065;&#26834;&#30340;&#20195;&#30721;&#12290;&#22312;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#28389;&#29992;API&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22914;&#36164;&#28304;&#27844;&#28431;&#12289;&#31243;&#24207;&#23849;&#28291;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;LLM&#20195;&#30721;&#29983;&#25104;&#26381;&#21153;&#30340;&#29992;&#25143;&#23454;&#38469;&#19978;&#26159;&#26368;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#30475;&#20284;&#27491;&#30830;&#30340;&#20195;&#30721;&#24433;&#21709;&#30340;&#24320;&#21457;&#32773;&#8212;&#8212;&#20182;&#20204;&#36890;&#24120;&#26159;&#19981;&#29087;&#24713;LLMs&#20026;&#20182;&#20204;&#29983;&#25104;&#20195;&#30721;&#30340;API&#30340;&#21021;&#32423;&#24320;&#21457;&#32773;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;API&#30340;&#28389;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#35789;&#30340;&#24847;&#20041;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#21644;&#20799;&#31461;&#22914;&#20309;&#23398;&#20064;&#36825;&#20123;&#35789;&#27719;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20197;&#35270;&#35273;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#36882;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#38656;&#35201;&#31354;&#38388;&#21644;&#25968;&#23383;&#25512;&#29702;&#30340;&#21151;&#33021;&#35789;&#30340;&#26799;&#24230;&#35821;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#36923;&#36753;&#25512;&#29702;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21040;"&#21644;"&#21644;"&#25110;"&#30340;&#24847;&#20041;&#65292;&#20197;&#21450;&#36805;&#36895;&#21457;&#23637;&#20986;&#26367;&#25442;&#25512;&#35770;&#30340;&#33021;&#21147;&#30340;&#26089;&#26399;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08628</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#20013;&#20197;&#22522;&#20110;&#35821;&#22659;&#35821;&#35328;&#23398;&#20064;&#21151;&#33021;&#35789;&#30340;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
Learning the meanings of function words from grounded language using a visual question answering model. (arXiv:2308.08628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#35789;&#30340;&#24847;&#20041;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#21644;&#20799;&#31461;&#22914;&#20309;&#23398;&#20064;&#36825;&#20123;&#35789;&#27719;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20197;&#35270;&#35273;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#36882;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#38656;&#35201;&#31354;&#38388;&#21644;&#25968;&#23383;&#25512;&#29702;&#30340;&#21151;&#33021;&#35789;&#30340;&#26799;&#24230;&#35821;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#36923;&#36753;&#25512;&#29702;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21040;"&#21644;"&#21644;"&#25110;"&#30340;&#24847;&#20041;&#65292;&#20197;&#21450;&#36805;&#36895;&#21457;&#23637;&#20986;&#26367;&#25442;&#25512;&#35770;&#30340;&#33021;&#21147;&#30340;&#26089;&#26399;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#30340;&#21151;&#33021;&#35789;&#65292;&#22914;&#8220;&#25110;&#32773;&#8221;&#65292;&#8220;&#22312;......&#21518;&#38754;&#8221;&#65292;&#25110;&#8220;&#26356;&#22810;&#8221;&#21487;&#33021;&#38656;&#35201;&#36923;&#36753;&#12289;&#25968;&#23383;&#21644;&#20851;&#31995;&#25512;&#29702;&#12290;&#20799;&#31461;&#22914;&#20309;&#23398;&#20064;&#36825;&#26679;&#30340;&#35789;&#27719;&#65311;&#26082;&#24448;&#30340;&#20064;&#24471;&#29702;&#35770;&#36890;&#24120;&#20381;&#36182;&#20110;&#35748;&#20026;&#20855;&#26377;&#20808;&#22825;&#30693;&#35782;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26174;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21151;&#33021;&#35789;&#26469;&#22238;&#31572;&#20851;&#20110;&#22797;&#26434;&#35270;&#35273;&#22330;&#26223;&#30340;&#38382;&#39064;&#32780;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#21151;&#33021;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#24076;&#26395;&#26356;&#22909;&#22320;&#20102;&#35299;&#36825;&#20123;&#35789;&#27719;&#30340;&#24847;&#20041;&#22914;&#20309;&#34987;&#27169;&#22411;&#21644;&#20799;&#31461;&#25152;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20197;&#35270;&#35273;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#36882;&#24402;&#27169;&#22411;&#23398;&#20064;&#20102;&#38656;&#35201;&#31354;&#38388;&#21644;&#25968;&#23383;&#25512;&#29702;&#30340;&#21151;&#33021;&#35789;&#30340;&#26799;&#24230;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#36923;&#36753;&#25512;&#29702;&#30340;&#20808;&#39564;&#30693;&#35782;&#19979;&#23398;&#20064;&#21040;"&#21644;"&#21644;"&#25110;"&#30340;&#24847;&#20041;&#65292;&#24182;&#36805;&#36895;&#21457;&#23637;&#20986;&#36827;&#34892;&#26367;&#25442;&#25512;&#35770;&#30340;&#33021;&#21147;&#30340;&#26089;&#26399;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting a seemingly-simple function word like "or", "behind", or "more" can require logical, numerical, and relational reasoning. How are such words learned by children? Prior acquisition theories have often relied on positing a foundation of innate knowledge. Yet recent neural-network based visual question answering models apparently can learn to use function words as part of answering questions about complex visual scenes. In this paper, we study what these models learn about function words, in the hope of better understanding how the meanings of these words can be learnt by both models and children. We show that recurrent models trained on visually grounded language learn gradient semantics for function words requiring spacial and numerical reasoning. Furthermore, we find that these models can learn the meanings of logical connectives "and" and "or" without any prior knowledge of logical reasoning, as well as early evidence that they can develop the ability to reason about alte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26368;&#20248;&#24494;&#35843;&#27169;&#22411;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#32988;&#36807;GPT-3.5&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.14385</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#39044;&#27979;&#24515;&#29702;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26368;&#20248;&#24494;&#35843;&#27169;&#22411;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#32988;&#36807;GPT-3.5&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#25552;&#21319;&#20351;&#24471;&#22810;&#31181;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#25913;&#36827;&#30740;&#31350;&#20960;&#20046;&#27809;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#22810;&#31181;LLM&#65288;&#21253;&#25324;Alpaca&#65292;Alpaca-LoRA&#21644;GPT-3.5&#65289;&#22312;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#22810;&#20010;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#38646;-shot&#25552;&#31034;&#12289;&#23569;-shot&#25552;&#31034;&#21644;&#25351;&#20196;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#38646;-shot&#21644;&#23569;-shot&#25552;&#31034;&#35774;&#35745;&#19978;&#22312;&#24515;&#29702;&#20581;&#24247;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26377;&#38480;&#20294;&#26377;&#21069;&#26223;&#30340;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;LLM&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;Mental-Alpaca&#65292;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#27604;GPT-3.5&#65288;&#20307;&#31215;&#22823;25&#20493;&#65289;&#39640;&#20986;16.7\%&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;&#25105;&#20204;&#24635;&#32467;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent technology boost of large language models (LLMs) has empowered a variety of applications. However, there is very little research on understanding and improving LLMs' capability for the mental health domain. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, and GPT-3.5, on various mental health prediction tasks via online text data. We conduct a wide range of experiments, covering zero-shot prompting, few-shot prompting, and instruction finetuning. The results indicate the promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned model, Mental-Alpaca, outperforms GPT-3.5 (25 times bigger) by 16.7\% on balanced accuracy and performs on par with the state-of-the-art task-specific model. We summarize our find
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#21019;&#26032;&#26159;&#22522;&#20110;Hapax Rate&#27169;&#22411;&#24341;&#20837;&#20102;&#23545;Zipf&#21644;Heaps&#23450;&#24459;&#30340;&#20462;&#27491;&#65292;&#24182;&#21457;&#29616;&#36923;&#36753;&#27169;&#22411;&#25311;&#21512;&#25928;&#26524;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.12896</link><description>&lt;p&gt;
&#20174;Hapax Rate&#27169;&#22411;&#23548;&#20986;&#30340;Zipf&#21644;Heaps&#23450;&#24459;&#30340;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Corrections of Zipf's and Heaps' Laws Derived from Hapax Rate Models. (arXiv:2307.12896v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#21019;&#26032;&#26159;&#22522;&#20110;Hapax Rate&#27169;&#22411;&#24341;&#20837;&#20102;&#23545;Zipf&#21644;Heaps&#23450;&#24459;&#30340;&#20462;&#27491;&#65292;&#24182;&#21457;&#29616;&#36923;&#36753;&#27169;&#22411;&#25311;&#21512;&#25928;&#26524;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Hapax Rate&#27169;&#22411;&#24341;&#20837;&#20102;&#23545;Zipf&#21644;Heaps&#23450;&#24459;&#30340;&#20462;&#27491;&#12290;&#25512;&#23548;&#22522;&#20110;&#20004;&#20010;&#20551;&#35774;&#65306;&#31532;&#19968;&#20010;&#20551;&#35774;&#26159;&#26631;&#20934;&#30340;&#29934;&#27169;&#22411;&#65292;&#39044;&#27979;&#36739;&#30701;&#25991;&#26412;&#30340;&#36793;&#38469;&#35789;&#39057;&#20998;&#24067;&#30475;&#36215;&#26469;&#23601;&#20687;&#26159;&#20174;&#19968;&#20010;&#32473;&#23450;&#30340;&#36739;&#38271;&#25991;&#26412;&#20013;&#30450;&#30446;&#37319;&#26679;&#35789;&#20803;&#12290;&#31532;&#20108;&#20010;&#20551;&#35774;&#20551;&#23450;Hapax&#30340;&#39057;&#29575;&#26159;&#25991;&#26412;&#22823;&#23567;&#30340;&#31616;&#21333;&#20989;&#25968;&#12290;&#35752;&#35770;&#20102;&#22235;&#20010;&#36825;&#26679;&#30340;&#20989;&#25968;&#65306;&#24120;&#25968;&#27169;&#22411;&#12289;Davis&#27169;&#22411;&#12289;&#32447;&#24615;&#27169;&#22411;&#21644;&#36923;&#36753;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#36923;&#36753;&#27169;&#22411;&#25311;&#21512;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article introduces corrections to Zipf's and Heaps' laws based on systematic models of the hapax rate. The derivation rests on two assumptions: The first one is the standard urn model which predicts that marginal frequency distributions for shorter texts look as if word tokens were sampled blindly from a given longer text. The second assumption posits that the rate of hapaxes is a simple function of the text size. Four such functions are discussed: the constant model, the Davis model, the linear model, and the logistic model. It is shown that the logistic model yields the best fit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#24182;&#21457;&#29616;&#20004;&#32773;&#20043;&#19968;&#36890;&#24120;&#26159;&#27491;&#20540;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00168</link><description>&lt;p&gt;
&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#38754;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring the Robustness of Natural Language Processing Models to Domain Shifts. (arXiv:2306.00168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#24182;&#21457;&#29616;&#20004;&#32773;&#20043;&#19968;&#36890;&#24120;&#26159;&#27491;&#20540;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39046;&#22495;&#40065;&#26834;&#24615;&#65288;DR&#65289;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;DR&#30740;&#31350;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#35774;&#32622;&#12289;&#32570;&#20047;&#35780;&#20272;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#36807;&#22810;&#20381;&#38752;&#25361;&#25112;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33258;&#28982;&#39046;&#22495;&#36716;&#31227;&#35774;&#32622;&#19979;&#24494;&#35843;&#21644;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30340;DR&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;DR&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#26679;&#21270;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#23376;&#21644;&#26631;&#35760;&#32423;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#29983;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#30001;&#20960;&#20010;&#39046;&#22495;&#32452;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#28304;&#22495;&#38477;&#20302;&#65288;SD&#65289;&#21644;&#30446;&#26631;&#22495;&#38477;&#20302;&#65288;TD&#65289;&#65292;&#23427;&#20204;&#20132;&#26367;&#20316;&#20026;&#21442;&#32771;&#28857;&#26469;&#27604;&#36739;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#37325;&#22823;&#27604;&#20363;&#30340;&#39046;&#22495;&#36716;&#31227;&#20013;&#65292;SD&#25110;TD&#20043;&#19968;&#26159;&#27491;&#30340;&#65292;&#20294;&#19981;&#26159;&#20004;&#32773;&#37117;&#27491;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;DR&#25361;&#25112;&#30340;&#20004;&#20010;&#35270;&#35282;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#20801;&#35768;&#22312;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35774;&#32622;&#19978;&#20844;&#24179;&#27604;&#36739;DR&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;NLP&#27169;&#22411;DR&#24615;&#36136;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown promising performance on various tasks, including fine-tuning, few-shot learning, and zero-shot learning. However, their performance on domains without labeled data still lags behind those with labeled data, which we refer as the Domain Robustness (DR) challenge. Existing research on DR suffers from disparate setups, lack of evaluation task variety, and reliance on challenge sets. In this paper, we explore the DR challenge of both fine-tuned and few-shot learning models in natural domain shift settings. We introduce a DR benchmark comprising diverse NLP tasks, including sentence and token-level classification, QA, and generation, each task consists of several domains. We propose two views of the DR challenge: Source Drop (SD) and Target Drop (TD), which alternate between the source and target in-domain performance as reference points. We find that in significant proportions of domain shifts, either SD or TD is positive, but not both, emphasizing the imp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28508;&#22312;&#26816;&#32034;&#26469;&#25913;&#36827;&#20107;&#23454;&#26680;&#26597;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#28304;&#25991;&#26723;&#30340;&#20840;&#25991;&#20316;&#20026;&#35777;&#25454;&#65292;&#24182;&#24341;&#20837;&#22810;&#35821;&#35328;&#21644;&#21333;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#21363;&#20351;&#27809;&#26377;&#27880;&#37322;&#30340;&#40644;&#37329;&#35777;&#25454;&#21477;&#23376;&#65292;&#20063;&#33021;&#36827;&#34892;&#20934;&#30830;&#30340;&#20027;&#24352;&#39564;&#35777;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.16128</link><description>&lt;p&gt;
&#32473;&#25105;&#26356;&#22810;&#32454;&#33410;&#65306;&#21033;&#29992;&#28508;&#22312;&#26816;&#32034;&#25552;&#39640;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Give Me More Details: Improving Fact-Checking with Latent Retrieval. (arXiv:2305.16128v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16128
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28508;&#22312;&#26816;&#32034;&#26469;&#25913;&#36827;&#20107;&#23454;&#26680;&#26597;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#28304;&#25991;&#26723;&#30340;&#20840;&#25991;&#20316;&#20026;&#35777;&#25454;&#65292;&#24182;&#24341;&#20837;&#22810;&#35821;&#35328;&#21644;&#21333;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#21363;&#20351;&#27809;&#26377;&#27880;&#37322;&#30340;&#40644;&#37329;&#35777;&#25454;&#21477;&#23376;&#65292;&#20063;&#33021;&#36827;&#34892;&#20934;&#30830;&#30340;&#20027;&#24352;&#39564;&#35777;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#25454;&#22312;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#39564;&#35777;&#29616;&#23454;&#19990;&#30028;&#30340;&#20027;&#24352;&#26102;&#65292;&#29616;&#26377;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#35201;&#20040;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#21477;&#23376;&#65292;&#35201;&#20040;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#36820;&#22238;&#30340;&#25628;&#32034;&#29255;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#25910;&#38598;&#35777;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#21487;&#33021;&#27809;&#26377;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#39564;&#35777;&#29616;&#23454;&#19990;&#30028;&#30340;&#20027;&#24352;&#12290;&#20026;&#20102;&#26500;&#24314;&#19968;&#20010;&#26356;&#22909;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#28304;&#25991;&#26723;&#30340;&#20840;&#25991;&#20316;&#20026;&#35777;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#12290;&#31532;&#19968;&#20010;&#26159;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#32780;&#31532;&#20108;&#20010;&#26159;&#21333;&#35821;&#35328;&#65288;&#33521;&#35821;&#65289;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20174;&#25991;&#26723;&#20013;&#32852;&#21512;&#25552;&#21462;&#35777;&#25454;&#21477;&#23376;&#24182;&#36827;&#34892;&#20027;&#24352;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#27880;&#37322;&#30340;&#40644;&#37329;&#35777;&#25454;&#21477;&#23376;&#65292;&#21253;&#25324;&#28304;&#25991;&#26723;&#22312;&#20869;&#21487;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#33021;&#22815;&#22312;&#26368;&#20339;&#25253;&#21578;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence plays a crucial role in automated fact-checking. When verifying real-world claims, existing fact-checking systems either assume the evidence sentences are given or use the search snippets returned by the search engine. Such methods ignore the challenges of collecting evidence and may not provide sufficient information to verify real-world claims. Aiming at building a better fact-checking system, we propose to incorporate full text from source documents as evidence and introduce two enriched datasets. The first one is a multilingual dataset, while the second one is monolingual (English). We further develop a latent variable model to jointly extract evidence sentences from documents and perform claim verification. Experiments indicate that including source documents can provide sufficient contextual clues even when gold evidence sentences are not annotated. The proposed system is able to achieve significant improvements upon best-reported models under different settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;ConvGQR&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#37325;&#26032;&#26500;&#36896;&#26597;&#35810;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2305.15645</link><description>&lt;p&gt;
ConvGQR&#65306;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;&#29983;&#25104;&#24335;&#26597;&#35810;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
ConvGQR: Generative Query Reformulation for Conversational Search. (arXiv:2305.15645v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;ConvGQR&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#37325;&#26032;&#26500;&#36896;&#26597;&#35810;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35805;&#25628;&#32034;&#20013;&#65292;&#29992;&#25143;&#24403;&#21069;&#25628;&#32034;&#24847;&#22270;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#23545;&#35805;&#21382;&#21490;&#12290;&#20174;&#25972;&#20010;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#30830;&#23450;&#19968;&#20010;&#33391;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#36991;&#20813;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26114;&#36149;&#37325;&#26032;&#35757;&#32451;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#23398;&#20064;&#19968;&#20010;&#37325;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#20223;&#25163;&#21160;&#26597;&#35810;&#37325;&#20889;&#26469;&#21435;&#38500;&#24403;&#21069;&#26597;&#35810;&#30340;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#37325;&#20889;&#30340;&#26597;&#35810;&#24182;&#19981;&#24635;&#26159;&#26368;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#35757;&#32451;&#37325;&#20889;&#27169;&#22411;&#20250;&#38480;&#21046;&#27169;&#22411;&#20135;&#29983;&#33391;&#22909;&#25628;&#32034;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ConvGQR&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#37325;&#20889;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#28508;&#22312;&#31572;&#26696;&#65292;&#20197;&#37325;&#26032;&#26500;&#36896;&#20250;&#35805;&#26597;&#35810;&#12290;&#36890;&#36807;&#32467;&#21512;&#20004;&#32773;&#65292;ConvGQR&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#26597;&#35810;&#37325;&#26500;&#19982;&#26816;&#32034;&#24615;&#33021;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#27169;&#22411;&#65292;&#29992;&#20110;&#39564;&#35777;ConvGQR&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational search, the user's real search intent for the current turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Training a rewriting model on them would limit the model's ability to produce good search queries. Another useful hint is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to retrieval performance, we propose a 
&lt;/p&gt;</description></item><item><title>mPLM-Sim&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#20174;mPLMs&#20013;&#24341;&#23548;&#20986;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21487;&#29992;&#20110;&#36873;&#25321;&#28304;&#35821;&#35328;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#20855;&#26377;&#20013;&#31561;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#19981;&#21516;&#30340;mPLMs&#21644;&#23618;&#20135;&#29983;&#19981;&#21516;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13684</link><description>&lt;p&gt;
mPLM-Sim: &#25581;&#31034;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26356;&#22909;&#30340;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models. (arXiv:2305.13684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13684
&lt;/p&gt;
&lt;p&gt;
mPLM-Sim&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#20174;mPLMs&#20013;&#24341;&#23548;&#20986;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21487;&#29992;&#20110;&#36873;&#25321;&#28304;&#35821;&#35328;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#20855;&#26377;&#20013;&#31561;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#19981;&#21516;&#30340;mPLMs&#21644;&#23618;&#20135;&#29983;&#19981;&#21516;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#29305;&#23450;&#35821;&#35328;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#24182;&#27809;&#26377;&#34987;&#26126;&#30830;&#25552;&#20379;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#23558;mPLMs&#29992;&#20110;&#27979;&#37327;&#35821;&#35328;&#30456;&#20284;&#24615;&#65292;&#24182;&#38543;&#21518;&#20351;&#29992;&#30456;&#20284;&#24615;&#32467;&#26524;&#36873;&#25321;&#28304;&#35821;&#35328;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;mPLM-Sim&#65292;&#23427;&#21033;&#29992;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#20174;mPLMs&#20013;&#24341;&#23548;&#20986;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;mPLM-Sim&#19982;&#35789;&#27719;&#32479;&#35745;&#12289;&#35821;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#31561;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#20855;&#26377;&#20013;&#31561;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#35821;&#35328;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#35266;&#23519;&#21040;mPLM-Sim&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#20284;&#24615;&#32467;&#26524;&#22240;&#19981;&#21516;&#30340;mPLMs&#21644;mPLM&#20013;&#30340;&#19981;&#21516;&#23618;&#32780;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35843;&#26597;&#20102;mPLMs&#23545;&#35821;&#35328;&#36801;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent multilingual pretrained language models (mPLMs) have been shown to encode strong language-specific signals, which are not explicitly provided during pretraining. It remains an open question whether it is feasible to employ mPLMs to measure language similarity, and subsequently use the similarity results to select source languages for boosting cross-lingual transfer. To investigate this, we propose mPLM-Sim, a new language similarity measure that induces the similarities across languages from mPLMs using multi-parallel corpora. Our study shows that mPLM-Sim exhibits moderately high correlations with linguistic similarity measures, such as lexicostatistics, genealogical language family, and geographical sprachbund. We also conduct a case study on languages with low correlation and observe that mPLM-Sim yields more accurate similarity results. Additionally, we find that similarity results vary across different mPLMs and different layers within an mPLM. We further investigate whethe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CEO&#30340;&#20107;&#20214;&#26412;&#20307;&#35825;&#23548;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#25918;&#26494;&#39044;&#23450;&#20041;&#20107;&#20214;&#26412;&#20307;&#25152;&#24378;&#21152;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26816;&#27979;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#26174;&#33879;&#30340;&#20107;&#20214;&#65292;&#24182;&#35825;&#23548;&#20855;&#26377;&#26377;&#24847;&#20041;&#21517;&#31216;&#30340;&#20998;&#23618;&#20107;&#20214;&#26412;&#20307;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#35825;&#23548;&#30340;&#27169;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13521</link><description>&lt;p&gt;
CEO&#65306;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#24320;&#25918;&#22495;&#20107;&#20214;&#26412;&#20307;&#35825;&#23548;
&lt;/p&gt;
&lt;p&gt;
CEO: Corpus-based Open-Domain Event Ontology Induction. (arXiv:2305.13521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CEO&#30340;&#20107;&#20214;&#26412;&#20307;&#35825;&#23548;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#25918;&#26494;&#39044;&#23450;&#20041;&#20107;&#20214;&#26412;&#20307;&#25152;&#24378;&#21152;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26816;&#27979;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#26174;&#33879;&#30340;&#20107;&#20214;&#65292;&#24182;&#35825;&#23548;&#20855;&#26377;&#26377;&#24847;&#20041;&#21517;&#31216;&#30340;&#20998;&#23618;&#20107;&#20214;&#26412;&#20307;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#35825;&#23548;&#30340;&#27169;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38754;&#21521;&#20107;&#20214;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36890;&#24120;&#20165;&#36866;&#29992;&#20110;&#39044;&#23450;&#20041;&#26412;&#20307;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#20107;&#20214;&#26412;&#20307;&#35825;&#23548;&#27169;&#22411;CEO&#65292;&#20197;&#25918;&#26494;&#39044;&#23450;&#20041;&#20107;&#20214;&#26412;&#20307;&#25152;&#24378;&#21152;&#30340;&#38480;&#21046;&#12290;&#22312;&#27809;&#26377;&#30452;&#25509;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;CEO&#21033;&#29992;&#21487;&#29992;&#25688;&#35201;&#25968;&#25454;&#38598;&#30340;&#36828;&#31243;&#30417;&#30563;&#26469;&#26816;&#27979;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#26174;&#33879;&#30340;&#20107;&#20214;&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#20107;&#20214;&#30693;&#35782;&#20351;&#36317;&#31163;&#30701;&#30340;&#20107;&#20214;&#20855;&#26377;&#30456;&#20284;&#30340;&#23884;&#20837;&#12290;&#23545;&#19977;&#20010;&#24120;&#29992;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CEO&#35825;&#23548;&#30340;&#27169;&#24335;&#20855;&#26377;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;CEO&#26159;&#31532;&#19968;&#20010;&#33021;&#22312;&#21313;&#19968;&#20010;&#24320;&#25918;&#22495;&#35821;&#26009;&#24211;&#19978;&#35825;&#23548;&#20855;&#26377;&#26377;&#24847;&#20041;&#21517;&#31216;&#30340;&#20998;&#23618;&#20107;&#20214;&#26412;&#20307;&#30340;&#20107;&#20214;&#26412;&#20307;&#35825;&#23548;&#27169;&#22411;&#65292;&#20351;&#35825;&#23548;&#30340;&#27169;&#24335;&#26356;&#20540;&#24471;&#20449;&#36182;&#24182;&#26356;&#26131;&#20110;&#36827;&#19968;&#27493;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing event-centric NLP models often only apply to the pre-defined ontology, which significantly restricts their generalization capabilities. This paper presents CEO, a novel Corpus-based Event Ontology induction model to relax the restriction imposed by pre-defined event ontologies. Without direct supervision, CEO leverages distant supervision from available summary datasets to detect corpus-wise salient events and exploits external event knowledge to force events within a short distance to have close embeddings. Experiments on three popular event datasets show that the schema induced by CEO has better coverage and higher accuracy than previous methods. Moreover, CEO is the first event ontology induction model that can induce a hierarchical event ontology with meaningful names on eleven open-domain corpora, making the induced schema more trustworthy and easier to be further curated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EnCore&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#23454;&#20307;&#32534;&#30721;&#22120;&#24182;&#20351;&#29992;&#35880;&#24910;&#31579;&#36873;&#30340;&#20849;&#25351;&#38142;&#25509;&#65292;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#26631;&#27880;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12924</link><description>&lt;p&gt;
EnCore:&#36890;&#36807;&#22312;&#25351;&#20195;&#38142;&#19978;&#39044;&#35757;&#32451;&#23454;&#20307;&#32534;&#30721;&#22120;&#26469;&#36827;&#34892;&#31934;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
EnCore: Fine-Grained Entity Typing by Pre-Training Entity Encoders on Coreference Chains. (arXiv:2305.12924v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EnCore&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#23454;&#20307;&#32534;&#30721;&#22120;&#24182;&#20351;&#29992;&#35880;&#24910;&#31579;&#36873;&#30340;&#20849;&#25351;&#38142;&#25509;&#65292;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#26631;&#27880;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#31867;&#22411;&#26631;&#27880;&#26159;&#23558;&#35821;&#20041;&#31867;&#22411;&#20998;&#37197;&#32473;&#25991;&#26412;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#22312;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#26631;&#27880;&#65288;FET&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#20505;&#36873;&#31867;&#22411;&#26631;&#31614;&#38598;&#21512;&#12290;&#30001;&#20110;&#33719;&#21462;&#36275;&#22815;&#30340;&#25163;&#21160;&#27880;&#37322;&#24448;&#24448;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;FET&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#36828;&#31243;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#39044;&#35757;&#32451;&#23454;&#20307;&#32534;&#30721;&#22120;&#65292;&#20351;&#24471;&#20849;&#25351;&#23454;&#20307;&#30340;&#23884;&#20837;&#26356;&#30456;&#20284;&#20110;&#24444;&#27492;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#23454;&#20307;&#65292;&#26469;&#25913;&#36827;&#36825;&#20010;&#36807;&#31243;&#12290;&#36825;&#31181;&#31574;&#30053;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#39044;&#27979;&#30340;&#20849;&#25351;&#38142;&#25509;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#12290;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#25216;&#24039;&#21487;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#65306;&#25105;&#20204;&#21482;&#32771;&#34385;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;&#29616;&#25104;&#31995;&#32479;&#39044;&#27979;&#30340;&#20849;&#25351;&#38142;&#25509;&#12290;&#36890;&#36807;&#35880;&#24910;&#20351;&#29992;&#20849;&#25351;&#38142;&#25509;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#20351;&#25105;&#20204;&#33021;&#22815;&#25913;&#36827;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Entity typing is the task of assigning semantic types to the entities that are mentioned in a text. In the case of fine-grained entity typing (FET), a large set of candidate type labels is considered. Since obtaining sufficient amounts of manual annotations is then prohibitively expensive, FET models are typically trained using distant supervision. In this paper, we propose to improve on this process by pre-training an entity encoder such that embeddings of coreferring entities are more similar to each other than to the embeddings of other entities. The main problem with this strategy, which helps to explain why it has not previously been considered, is that predicted coreference links are often too noisy. We show that this problem can be addressed by using a simple trick: we only consider coreference links that are predicted by two different off-the-shelf systems. With this prudent use of coreference links, our pre-training strategy allows us to improve the state-of-the-art in benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35752;&#35770;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38590;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36827;&#34892;&#23545;&#35805;&#24182;&#20462;&#27491;&#39044;&#27979;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#35752;&#35770;&#25552;&#39640;&#20934;&#30830;&#24615;&#39640;&#36798;25%&#12290;</title><link>http://arxiv.org/abs/2305.11789</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38590;&#39064;&#65306;&#19968;&#31181;&#22522;&#20110;&#35752;&#35770;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach. (arXiv:2305.11789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35752;&#35770;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38590;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36827;&#34892;&#23545;&#35805;&#24182;&#20462;&#27491;&#39044;&#27979;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#35752;&#35770;&#25552;&#39640;&#20934;&#30830;&#24615;&#39640;&#36798;25%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#35752;&#35770;&#12289;&#35299;&#37322;&#24182;&#30456;&#20114;&#36190;&#21516;&#25110;&#21453;&#23545;&#31561;&#26041;&#24335;&#20849;&#21516;&#35299;&#20915;&#20849;&#21516;&#38382;&#39064;&#12290;&#21516;&#26679;&#65292;&#22914;&#26524;&#31995;&#32479;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#33021;&#19982;&#20154;&#31867;&#36827;&#34892;&#35752;&#35770;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#20043;&#21069;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20013;&#65292;&#31995;&#32479;&#21482;&#33021;&#20570;&#20986;&#39044;&#27979;&#65292;&#20154;&#31867;&#21482;&#33021;&#23601;&#36825;&#20123;&#39044;&#27979;&#25552;&#38382;&#65292;&#32780;&#27809;&#26377;&#24444;&#27492;&#38388;&#30340;&#24847;&#35265;&#20132;&#25442;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#20351;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#36827;&#34892;&#35752;&#35770;&#21644;&#20462;&#27491;&#39044;&#27979;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#19982;&#20154;&#31867;&#36827;&#34892;&#26377;&#30410;&#30340;&#35752;&#35770;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#39640;&#36798;25&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10163</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20013;&#22269;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;ChatGPT&#24050;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#27969;&#20013;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#20854;&#24494;&#35843;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#35821;&#20041;&#30693;&#35782;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#65288;CNMLE&#65289;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#65292;&#21363;&#20174;&#20004;&#20010;&#26041;&#38754;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#23558;&#21307;&#23398;&#32972;&#26223;&#30693;&#35782;&#25552;&#21462;&#20026;&#35821;&#20041;&#25351;&#20196;&#26469;&#25351;&#23548;ChatGPT&#30340;&#25512;&#26029;&#12290;&#31867;&#20284;&#22320;&#65292;&#30456;&#20851;&#30340;&#21307;&#30103;&#38382;&#39064;&#34987;&#35782;&#21035;&#24182;&#20316;&#20026;&#28436;&#31034;&#36755;&#20837;&#32473;ChatGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#26080;&#27861;&#22312;CNMLE&#19978;&#33719;&#24471;&#21512;&#26684;&#20998;&#25968;&#65288;51&#20998;&#65289;&#65292;&#21482;&#26377;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#21151;&#36890;&#36807;&#32771;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated Instruction Tuning (FedIT)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#23545;LLMs&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#33719;&#21462;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#39640;&#35843;&#25972;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05644</link><description>&lt;p&gt;
&#20026;&#24314;&#31435;&#32852;&#37030; GPT &#20570;&#20986;&#21162;&#21147;&#65306;&#32852;&#37030;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Towards Building the Federated GPT: Federated Instruction Tuning. (arXiv:2305.05644v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated Instruction Tuning (FedIT)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#23545;LLMs&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#33719;&#21462;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#39640;&#35843;&#25972;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#8220;&#25351;&#20196;&#35843;&#25972;&#8221;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#26032;&#20219;&#21153;&#27010;&#25324;&#33021;&#21147;&#65292;&#20294;&#35757;&#32451;&#38454;&#27573;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#22810;&#26679;&#21644;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65288;&#22914;ChatGPT&#21644;GPT-4&#65289;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#25776;&#20889;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#20250;&#38754;&#20020;&#26174;&#33879;&#30340;&#25104;&#26412;&#21644;&#21487;&#35775;&#38382;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#19982;&#38544;&#31169;&#26377;&#20851;&#30340;&#38382;&#39064;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#38480;&#21046;&#23545;&#36825;&#20123;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#20351;&#24471;&#33719;&#21462;&#25968;&#25454;&#30340;&#36807;&#31243;&#21464;&#24471;&#22797;&#26434;&#32780;&#24494;&#22937;&#12290;&#22240;&#27492;&#65292;&#36825;&#38480;&#21046;&#20102;&#35843;&#25972;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#24182;&#21487;&#33021;&#38480;&#21046;&#20854;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#30340;&#25928;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#32852;&#37030;&#25351;&#20196;&#35843;&#25972;&#65288;FedIT&#65289;&#65292;&#23427;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;LLMs&#25351;&#20196;&#35843;&#25972;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#36825;&#26159;FL&#22312;LLMs&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#39318;&#27425;&#25506;&#32034;&#12290;&#36825;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;...
&lt;/p&gt;
&lt;p&gt;
While ``instruction-tuned" generative large language models (LLMs) have demonstrated an impressive ability to generalize to new tasks, the training phases heavily rely on large amounts of diverse and high-quality instruction data (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data, especially when it comes to human-written data, can pose significant challenges both in terms of cost and accessibility. Moreover, concerns related to privacy can further limit access to such data, making the process of obtaining it a complex and nuanced undertaking. Consequently, this hinders the generality of the tuned models and may restrict their effectiveness in certain contexts. To tackle this issue, our study introduces a new approach called Federated Instruction Tuning (FedIT), which leverages federated learning (FL) as the learning framework for the instruction tuning of LLMs. This marks the first exploration of FL-based instruction tuning for LLMs. This is especially important s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;LaMini-LM&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21387;&#32553;&#27169;&#22411;&#32676;&#38598;&#65292;&#20174;&#25351;&#20196;&#24494;&#35843;&#36807;&#30340;LLMs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#22312;15&#20010;&#19981;&#21516;&#30340;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#20307;&#31215;&#32422;&#23567;&#20102;10&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.14402</link><description>&lt;p&gt;
LaMini-LM: &#22522;&#20110;&#22823;&#35268;&#27169;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21387;&#32553;&#27169;&#22411;&#32676;&#38598;
&lt;/p&gt;
&lt;p&gt;
LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. (arXiv:2304.14402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;LaMini-LM&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21387;&#32553;&#27169;&#22411;&#32676;&#38598;&#65292;&#20174;&#25351;&#20196;&#24494;&#35843;&#36807;&#30340;LLMs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#22312;15&#20010;&#19981;&#21516;&#30340;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#20307;&#31215;&#32422;&#23567;&#20102;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20174;&#24494;&#35843;&#36807;&#30340;LLMs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20180;&#32454;&#24320;&#21457;&#20102;&#19968;&#32452;258&#19975;&#20221;&#22522;&#20110;&#29616;&#26377;&#21644;&#26032;&#29983;&#25104;&#30340;&#25351;&#20196;&#12290;&#38500;&#20102;&#35268;&#27169;&#22823;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#24191;&#27867;&#30340;&#35805;&#39064;&#65292;&#20197;&#30830;&#20445;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#20351;&#29992;gpt-3.5-turbo&#20026;&#36825;&#20123;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25351;&#20196;&#26469;&#24494;&#35843;&#22810;&#20010;&#27169;&#22411;&#65292;&#21363;LaMini-LM&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#31995;&#21015;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#65288;&#22312;15&#20010;&#19981;&#21516;&#30340;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#65289;&#21644;&#25163;&#21160;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;LaMini-LM&#19982;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#24403;&#65292;&#32780;&#19988;&#20307;&#31215;&#32422;&#23567;&#20102;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with instruction finetuning demonstrate superior generative capabilities. However, these models are resource intensive. To alleviate this issue, we explore distilling knowledge from instruction-tuned LLMs to much smaller ones. To this end, we carefully develop a large set of 2.58M instructions based on both existing and newly-generated instructions. In addition to being sizeable, we design our instructions to cover a broad set of topics to ensure. A thorough investigation of our instruction data demonstrate their diversity, and we generate responses for these instructions using gpt-3.5-turbo. We then exploit the instructions to tune a host of models, dubbed LaMini-LM, of varying sizes, both from the encoder-decoder as well as the decoder-only families. We evaluate our models both automatically (on 15 different NLP benchmarks) and manually. Results show that our proposed LaMini-LM are on par with competitive baselines while being nearly 10 times smaller in s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01295</link><description>&lt;p&gt;
&#26377;&#25928;&#22320;&#23545;&#40784;&#36328;&#35821;&#35328;&#20250;&#35805;&#20219;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20250;&#35805;&#20219;&#21153;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;XSGD&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;Schema-Guided Dialogue&#65288;SGD&#65289;&#32763;&#35793;&#25104;105&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#27979;&#35797;&#20102;&#23545;&#40784;&#25552;&#31034;&#25152;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23545;&#35805;&#20219;&#21153;&#65288;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaGoNN&#30340;&#20462;&#25913;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#26368;&#36817;&#37051;&#30340;&#20449;&#24687;&#25913;&#21464;&#36755;&#20837;&#25991;&#26412;&#65292;&#20174;&#32780;&#20351;&#26032;&#39062;&#30340;&#25968;&#25454;&#30475;&#36215;&#26469;&#31867;&#20284;&#20110;&#27169;&#22411;&#20248;&#21270;&#30340;&#23454;&#20363;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#26631;&#35760;&#19981;&#38656;&#35201;&#30340;&#20869;&#23481;&#21644;&#25991;&#26412;&#20998;&#31867;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.08957</link><description>&lt;p&gt;
&#22914;&#21516;&#19968;&#20010;&#22909;&#37051;&#23621;&#65306;&#23454;&#29992;&#30340;&#20869;&#23481;&#23457;&#26680;&#21644;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Like a Good Nearest Neighbor: Practical Content Moderation and Text Classification. (arXiv:2302.08957v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08957
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaGoNN&#30340;&#20462;&#25913;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#26368;&#36817;&#37051;&#30340;&#20449;&#24687;&#25913;&#21464;&#36755;&#20837;&#25991;&#26412;&#65292;&#20174;&#32780;&#20351;&#26032;&#39062;&#30340;&#25968;&#25454;&#30475;&#36215;&#26469;&#31867;&#20284;&#20110;&#27169;&#22411;&#20248;&#21270;&#30340;&#23454;&#20363;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#26631;&#35760;&#19981;&#38656;&#35201;&#30340;&#20869;&#23481;&#21644;&#25991;&#26412;&#20998;&#31867;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20381;&#36182;&#20110;&#25552;&#31034;&#21644;&#21313;&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#27492;&#26080;&#27861;&#21487;&#38752;&#22320;&#37096;&#32626;&#21644;&#20351;&#29992;&#12290;SetFit&#65288;Tunstall&#31561;&#65292;2022&#24180;&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#23427;&#22312;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#19979;&#24494;&#35843;&#20102;&#19968;&#20010;&#21477;&#23376;&#36716;&#25442;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#26356;&#38590;&#31649;&#29702;&#30340;&#31995;&#32479;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#24265;&#20215;&#30340;&#25991;&#26412;&#20998;&#31867;&#23545;&#20110;&#35299;&#20915;&#25152;&#26377;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#39046;&#22495;&#28418;&#31227;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#26377;&#23475;&#20869;&#23481;&#26041;&#38754;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20687;&#19968;&#20010;&#22909;&#37051;&#23621;&#65288;LaGoNN&#65289;&#36825;&#26679;&#30340;&#20462;&#25913;SetFit&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#20294;&#26159;&#36890;&#36807;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#26368;&#36817;&#37051;&#30340;&#20449;&#24687;&#25913;&#21464;&#36755;&#20837;&#25991;&#26412;&#65292;&#20363;&#22914;&#26631;&#31614;&#21644;&#25991;&#26412;&#65292;&#20351;&#26032;&#39062;&#30340;&#25968;&#25454;&#30475;&#36215;&#26469;&#31867;&#20284;&#20110;&#27169;&#22411;&#20248;&#21270;&#30340;&#23454;&#20363;&#12290;LaGoNN&#22312;&#26631;&#35760;&#19981;&#38656;&#35201;&#30340;&#20869;&#23481;&#21644;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot text classification systems have impressive capabilities but are infeasible to deploy and use reliably due to their dependence on prompting and billion-parameter language models. SetFit (Tunstall et al., 2022) is a recent, practical approach that fine-tunes a Sentence Transformer under a contrastive learning paradigm and achieves similar results to more unwieldy systems. Inexpensive text classification is important for addressing the problem of domain drift in all classification tasks, and especially in detecting harmful content, which plagues social media platforms. Here, we propose Like a Good Nearest Neighbor (LaGoNN), a modification to SetFit that introduces no learnable parameters but alters input text with information from its nearest neighbor, for example, the label and text, in the training data, making novel data appear similar to an instance on which the model was optimized. LaGoNN is effective at flagging undesirable content and text classification, and improves the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.04914</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models. (arXiv:2302.04914v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#24211;&#23545;&#20110;&#26448;&#26009;&#31185;&#23398;&#21644;&#24037;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#26469;&#24320;&#21457;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20174;&#30740;&#31350;&#35770;&#25991;&#30340;&#20840;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#24555;&#36895;&#24320;&#21457;&#35268;&#27169;&#36866;&#20013;&#30340;&#25968;&#25454;&#24211;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#65292;&#19981;&#38656;&#35201;&#20851;&#20110;&#25552;&#21462;&#23646;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;&#35813;&#26041;&#27861;&#26159;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#65292;&#38500;&#20102;&#19968;&#20010;&#38656;&#35201;&#20154;&#24037;&#36741;&#21161;&#30340;&#27493;&#39588;&#65292;&#36890;&#24120;&#21482;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#30340;&#20154;&#21147;&#21171;&#21160;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20960;&#20046;&#21487;&#20197;&#19982;&#20219;&#20309;&#27492;&#31867;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#12290;&#36825;&#37324;&#35780;&#20272;&#20102;GPT-3/3.5&#12289;bart&#21644;DeBERTaV3&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#21462;&#20307;&#27169;&#37327;&#25968;&#25454;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#39640;&#36798;90%&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and comprehensive material databases extracted from research papers are critical for materials science and engineering but require significant human effort to develop. In this paper we present a simple method of extracting materials data from full texts of research papers suitable for quickly developing modest-sized databases. The method requires minimal to no coding, prior knowledge about the extracted property, or model training, and provides high recall and almost perfect precision in the resultant database. The method is fully automated except for one human-assisted step, which typically requires just a few hours of human labor. The method builds on top of natural language processing and large general language models but can work with almost any such model. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for comparison. We provide a detailed detailed analysis of the methods performance in extracting bulk modulus data, obtaining up to 90% precision at 9
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;SBERT&#21644;CNN&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#24086;&#23376;&#33258;&#21160;&#35782;&#21035;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2302.02759</link><description>&lt;p&gt;
&#29992;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;SBERT-CNN&#26816;&#27979;Reddit&#29992;&#25143;&#30340;&#25233;&#37057;&#30151;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Detecting Reddit Users with Depression Using a Hybrid Neural Network SBERT-CNN. (arXiv:2302.02759v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;SBERT&#21644;CNN&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#24086;&#23376;&#33258;&#21160;&#35782;&#21035;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#65292;&#24433;&#21709;&#20102;&#20840;&#29699;&#20272;&#35745;3.8%&#30340;&#20154;&#21475;&#12290;&#23427;&#20063;&#26159;&#20840;&#29699;&#27531;&#30142;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#20010;&#20154;&#36234;&#26469;&#36234;&#21916;&#27426;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65288;&#22914;Reddit&#65289;&#26469;&#34920;&#36798;&#20182;&#20204;&#30340;&#22256;&#38590;&#21644;&#20581;&#24247;&#38382;&#39064;&#65288;&#22914;&#25233;&#37057;&#30151;&#65289;&#65292;&#24182;&#22312;&#22312;&#32447;&#31038;&#21306;&#23547;&#27714;&#20854;&#20182;&#29992;&#25143;&#30340;&#25903;&#25345;&#12290;&#36825;&#20026;&#36890;&#36807;&#20998;&#26512;&#25968;&#30334;&#19975;&#24086;&#23376;&#20197;&#23547;&#25214;&#28508;&#22312;&#30340;&#24178;&#39044;&#26426;&#20250;&#65292;&#33258;&#21160;&#35782;&#21035;&#20855;&#26377;&#25233;&#37057;&#30151;&#30340;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#26426;&#20250;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22240;&#20854;&#26131;&#29992;&#24615;&#12289;&#39640;&#25928;&#22788;&#29702;&#33021;&#21147;&#21644;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#32780;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#22987;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;BERT&#65288;SBERT&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36890;&#36807;&#20182;&#20204;&#22312;Reddit&#19978;&#30340;&#24086;&#23376;&#26816;&#27979;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;&#21477;&#23376;BERT&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#30340;&#24847;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a widespread mental health issue, affecting an estimated 3.8% of the global population. It is also one of the main contributors to disability worldwide. Recently it is becoming popular for individuals to use social media platforms (e.g., Reddit) to express their difficulties and health issues (e.g., depression) and seek support from other users in online communities. It opens great opportunities to automatically identify social media users with depression by parsing millions of posts for potential interventions. Deep learning methods have begun to dominate in the field of machine learning and natural language processing (NLP) because of their ease of use, efficient processing, and state-of-the-art results on many NLP tasks. In this work, we propose a hybrid deep learning model which combines a pretrained sentence BERT (SBERT) and convolutional neural network (CNN) to detect individuals with depression with their Reddit posts. The sentence BERT is used to learn the meaning
&lt;/p&gt;</description></item><item><title>AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12132</link><description>&lt;p&gt;
AutoPEFT&#65306;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#33258;&#21160;&#37197;&#32622;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12132
&lt;/p&gt;
&lt;p&gt;
AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19987;&#38376;&#30340;&#24494;&#35843;&#29992;&#20110;&#19979;&#28216;NLP&#20219;&#21153;&#65292;&#20294;&#36825;&#26679;&#30340;&#36807;&#31243;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#26368;&#36817;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#27604;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#65288;FFT&#65289;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;PEFT&#37197;&#32622;&#26041;&#38754;&#20570;&#20986;&#26126;&#26234;&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#19981;&#23481;&#26131;&#30340;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#29978;&#33267;&#26159;PEFT&#27169;&#22359;&#25554;&#20837;&#30340;&#22270;&#23618;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#25163;&#21160;&#35774;&#35745;&#37197;&#32622;&#24456;&#21487;&#33021;&#22312;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#26041;&#38754;&#26159;&#27425;&#20248;&#30340;&#12290;&#21463;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoPEFT&#26469;&#33258;&#21160;&#36873;&#25321;PEFT&#37197;&#32622;&#65306;&#39318;&#20808;&#35774;&#35745;&#20855;&#26377;&#22810;&#20010;&#20195;&#34920;&#24615;PEFT&#27169;&#22359;&#30340;&#34920;&#36798;&#37197;&#32622;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#21518;&#20351;&#29992;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#20302;&#25104;&#26412;&#30340;&#35774;&#32622;&#65292;&#20174;&#32780;&#21457;&#29616;&#20248;&#21270;&#20219;&#21153;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#30340;Pareto&#20248;&#21270;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#19978;&#35780;&#20272;&#20102;AutoPEFT&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#25163;&#21160;&#35774;&#35745;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained language models are widely used in downstream NLP tasks via task-specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating a much smaller number of parameters compared to full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: we first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimisation in a low-cost setup, we then disc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.09949</link><description>&lt;p&gt;
&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Compressing Transformer-based self-supervised models for speech processing. (arXiv:2211.09949v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#26159;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#35774;&#22791;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30446;&#21069;&#24050;&#26377;&#19968;&#20123;&#23396;&#31435;&#30340;&#23581;&#35797;&#26469;&#21387;&#32553;Transformer&#65292;&#20294;&#30740;&#31350;&#20013;&#30340;&#35774;&#32622;&#21644;&#25351;&#26631;&#21508;&#19981;&#30456;&#21516;&#12290;&#27492;&#21069;&#30340;&#24037;&#20316;&#24456;&#23569;&#28041;&#21450;&#19981;&#21516;&#21387;&#32553;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36825;&#20351;&#24471;&#27604;&#36739;&#21387;&#32553;&#25216;&#26415;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#36825;&#20123;&#23396;&#31435;&#32467;&#26524;&#25552;&#20379;&#32972;&#26223;&#65292;&#30740;&#31350;&#20960;&#31181;&#24120;&#29992;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#19981;&#21516;&#21387;&#32553;&#29575;&#19979;&#30340;&#26435;&#34913;&#65292;&#21253;&#25324;&#22681;&#38047;&#26102;&#38388;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#20056;&#21152;&#25805;&#20316;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#36817;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20960;&#31181;&#21387;&#32553;&#26041;&#27861;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Transformers in self- supervised learning with applications to various downstream tasks, the computational cost of training and inference remains a major challenge for applying these models to a wide spectrum of devices. Several isolated attempts have been made to compress Transformers, but the settings and metrics are different across studies. Trade-off at various compression rates are also largely missing in prior work, making it difficult to compare compression techniques. In this work, we aim to provide context for the isolated results, studying several commonly used compression techniques, including weight pruning, head pruning, low-rank approximation, and knowledge distillation. We report trade- off at various compression rate, including wall-clock time, the number of parameters, and the number of multiply-accumulate operations. Our results show that compared to recent approaches, basic compression techniques are strong baselines. We further present several
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35760;&#24405;&#20102;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#23383;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20351;&#29992;&#22806;&#35266;&#19978;&#30456;&#20284;&#20294;&#35268;&#33539;&#19978;&#19981;&#31561;&#20215;&#30340;&#23383;&#27597;&#20197;&#21450;&#28151;&#21512;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#27491;&#23383;&#27861;&#30340;&#23383;&#27597;&#12290;&#23545;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23383;&#24418;&#35268;&#33539;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#23545;&#19981;&#21516;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.12273</link><description>&lt;p&gt;
&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#23383;&#30340;&#23383;&#24418;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graphemic Normalization of the Perso-Arabic Script. (arXiv:2210.12273v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35760;&#24405;&#20102;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#23383;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20351;&#29992;&#22806;&#35266;&#19978;&#30456;&#20284;&#20294;&#35268;&#33539;&#19978;&#19981;&#31561;&#20215;&#30340;&#23383;&#27597;&#20197;&#21450;&#28151;&#21512;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#27491;&#23383;&#27861;&#30340;&#23383;&#27597;&#12290;&#23545;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23383;&#24418;&#35268;&#33539;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#23545;&#19981;&#21516;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;1991&#24180;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#65292;Unicode&#20013;&#30340;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#23383;&#34920;&#31034;&#24050;&#32463;&#20174;169&#20010;&#25193;&#23637;&#21040;&#20102;&#36229;&#36807;440&#20010;&#21407;&#23376;&#23396;&#31435;&#23383;&#31526;&#65292;&#20998;&#24067;&#22312;&#22810;&#20010;&#20195;&#30721;&#39029;&#20013;&#65292;&#20195;&#34920;&#20102;&#21407;&#22987;&#38463;&#25289;&#20271;&#35821;&#21644;&#35768;&#22810;&#20854;&#20182;&#22320;&#21306;&#27491;&#23383;&#27861;&#20256;&#32479;&#30340;&#26631;&#20934;&#23383;&#27597;&#12289;&#21508;&#31181;&#21464;&#38899;&#31526;&#21495;&#21644;&#26631;&#28857;&#31526;&#21495;&#12290;&#26412;&#25991;&#35760;&#24405;&#20102;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#23383;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#36229;&#20986;&#20102;&#38463;&#25289;&#20271;&#35821;&#21644;&#27874;&#26031;&#35821;&#31561;&#26368;&#21463;&#30740;&#31350;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#23588;&#20854;&#20851;&#27880;NLP&#20013;&#30340;&#24773;&#20917;&#65292;&#35813;&#38382;&#39064;&#21463;&#21040;&#22810;&#20010;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#22914;&#20351;&#29992;&#22806;&#35266;&#19978;&#27169;&#31946;&#20294;&#35268;&#33539;&#19978;&#19981;&#31561;&#20215;&#30340;&#23383;&#27597;&#20197;&#21450;&#28151;&#21512;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#27491;&#23383;&#27861;&#30340;&#23383;&#27597;&#12290;&#36896;&#25104;&#36825;&#31181;&#28151;&#28102;&#30340;&#22240;&#32032;&#21253;&#25324;&#32570;&#20047;&#36755;&#20837;&#26041;&#27861;&#12289;&#29616;&#20195;&#27491;&#23383;&#27861;&#30340;&#19981;&#31283;&#23450;&#24615;&#12289;&#35782;&#23383;&#33021;&#21147;&#19981;&#36275;&#20197;&#21450;&#27491;&#23383;&#20256;&#32479;&#30340;&#20002;&#22833;&#25110;&#32570;&#20047;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23383;&#24418;&#35268;&#33539;&#21270;&#23545;&#20843;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its original appearance in 1991, the Perso-Arabic script representation in Unicode has grown from 169 to over 440 atomic isolated characters spread over several code pages representing standard letters, various diacritics and punctuation for the original Arabic and numerous other regional orthographic traditions. This paper documents the challenges that Perso-Arabic presents beyond the best-documented languages, such as Arabic and Persian, building on earlier work by the expert community. We particularly focus on the situation in natural language processing (NLP), which is affected by multiple, often neglected, issues such as the use of visually ambiguous yet canonically nonequivalent letters and the mixing of letters from different orthographies. Among the contributing conflating factors are the lack of input methods, the instability of modern orthographies, insufficient literacy, and loss or lack of orthographic tradition. We evaluate the effects of script normalization on eigh
&lt;/p&gt;</description></item><item><title>&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;ICL&#23545;&#22810;&#31181;&#25200;&#21160;&#31867;&#22411;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#26631;&#31614;&#20559;&#24046;&#23548;&#33268;&#36807;&#21435;&#30340;&#30740;&#31350;&#20302;&#20272;&#20102;ICL&#30340;&#25935;&#24863;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#30340;&#25935;&#24863;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21576;&#29616;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;SenSel&#65292;&#23427;&#22312;&#25918;&#24323;&#25935;&#24863;&#39044;&#27979;&#20915;&#31574;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#24120;&#29992;&#22522;&#20934;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.07661</link><description>&lt;p&gt;
&#20851;&#20110;&#25935;&#24863;&#24615;&#19982;&#20934;&#30830;&#24615;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07661
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;ICL&#23545;&#22810;&#31181;&#25200;&#21160;&#31867;&#22411;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#26631;&#31614;&#20559;&#24046;&#23548;&#33268;&#36807;&#21435;&#30340;&#30740;&#31350;&#20302;&#20272;&#20102;ICL&#30340;&#25935;&#24863;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#30340;&#25935;&#24863;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21576;&#29616;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;SenSel&#65292;&#23427;&#22312;&#25918;&#24323;&#25935;&#24863;&#39044;&#27979;&#20915;&#31574;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#24120;&#29992;&#22522;&#20934;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064; (In-context learning, ICL) &#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24120;&#24120;&#21463;&#21040;&#25552;&#31034;&#30340;&#36807;&#24230;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ICL&#23545;&#22810;&#31181;&#25200;&#21160;&#31867;&#22411;&#30340;&#25935;&#24863;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#31614;&#20559;&#24046;&#25513;&#30422;&#20102;&#30495;&#23454;&#30340;&#25935;&#24863;&#24615;&#65292;&#22240;&#27492;&#20043;&#21069;&#30340;&#30740;&#31350;&#21487;&#33021;&#22823;&#22823;&#20302;&#20272;&#20102;ICL&#30340;&#25935;&#24863;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#30340;&#25935;&#24863;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#65306;&#23545;&#25200;&#21160;&#25935;&#24863;&#30340;&#39044;&#27979;&#26356;&#19981;&#23481;&#26131;&#27491;&#30830;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SenSel&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#25935;&#24863;&#39044;&#27979;&#30340;&#20351;&#29992;&#12290;&#22312;&#21313;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SenSel&#22312;&#25918;&#24323;&#39044;&#27979;&#20915;&#31574;&#19978;&#22987;&#32456;&#20248;&#20110;&#20004;&#31181;&#24120;&#29992;&#30340;&#22522;&#20110;&#32622;&#20449;&#24230;&#21644;&#22522;&#20110;&#29109;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) suffers from oversensitivity to the prompt, making it unreliable in real-world scenarios. We study the sensitivity of ICL with respect to multiple perturbation types. First, we find that label bias obscures the true sensitivity, and therefore prior work may have significantly underestimated ICL sensitivity. Second, we observe a strong negative correlation between ICL sensitivity and accuracy: predictions sensitive to perturbations are less likely to be correct. Motivated by these findings, we propose \textsc{SenSel}, a few-shot selective prediction method that abstains from sensitive predictions. Experiments on ten classification datasets show that \textsc{SenSel} consistently outperforms two commonly used confidence-based and entropy-based baselines on abstention decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#25361;&#25112;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#24335;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#8212;&#8212;DiSCoMaT&#12290;</title><link>http://arxiv.org/abs/2207.01079</link><description>&lt;p&gt;
DiSCoMaT&#65306;&#26448;&#26009;&#31185;&#23398;&#25991;&#31456;&#20013;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#34920;&#26684;&#32452;&#25104;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#25361;&#25112;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#24335;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#8212;&#8212;DiSCoMaT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31185;&#23398;&#39046;&#22495;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#26159;&#30693;&#35782;&#24211;&#31574;&#21010;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34920;&#26684;&#25552;&#21462;&#22120;&#20551;&#23450;&#24744;&#24050;&#32463;&#20102;&#35299;&#34920;&#26684;&#32467;&#26500;&#21644;&#26684;&#24335;&#65292;&#32780;&#31185;&#23398;&#34920;&#26684;&#20013;&#21487;&#33021;&#27809;&#26377;&#36825;&#20123;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#34920;&#26684;&#25552;&#21462;&#38382;&#39064;&#65306;&#25552;&#21462;&#26448;&#26009;&#65288;&#20363;&#22914;&#29627;&#29827;&#65292;&#21512;&#37329;&#65289;&#30340;&#32452;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#21508;&#31181;&#34920;&#26684;&#26679;&#24335;&#32452;&#32455;&#31867;&#20284;&#30340;&#32452;&#25104;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#26234;&#33021;&#27169;&#22411;&#26469;&#29702;&#35299;&#34920;&#26684;&#21644;&#25552;&#21462;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26032;&#22411;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#30001;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#32452;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DiSCoMaT&#65292;&#23427;&#26159;&#19968;&#20010;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial component in the curation of KB for a scientific domain is information extraction from tables in the domain's published articles -- tables carry important information (often numeric), which must be adequately extracted for a comprehensive machine understanding of an article. Existing table extractors assume prior knowledge of table structure and format, which may not be known in scientific tables. We study a specific and challenging table extraction problem: extracting compositions of materials (e.g., glasses, alloys). We first observe that materials science researchers organize similar compositions in a wide variety of table styles, necessitating an intelligent model for table understanding and composition extraction. Consequently, we define this novel task as a challenge for the ML community and create a training dataset comprising 4,408 distantly supervised tables, along with 1,475 manually annotated dev and test tables. We also present DiSCoMaT, a strong baseline geared t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;MiniDisc&#30340;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2205.14570</link><description>&lt;p&gt;
MiniDisc: &#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
MiniDisc: Minimal Distillation Schedule for Language Model Compression. (arXiv:2205.14570v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;MiniDisc&#30340;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#23481;&#37327;&#24046;&#36317;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#25928;&#26524;&#19981;&#20339;&#65292;&#24341;&#20837;&#20102;&#25945;&#24072;&#21161;&#25163;&#36741;&#21161;&#33976;&#39311;&#26469;&#24357;&#34917;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25945;&#24072;&#21161;&#25163;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#23581;&#35797;&#25165;&#33021;&#35843;&#24230;&#20986;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65288;MiniDisc&#65289;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;MiniDisc&#26159;&#22522;&#20110;&#25945;&#24072;&#21161;&#25163;&#30340;&#35268;&#27169;-&#24615;&#33021;&#30340;&#26435;&#34913;&#26469;&#24230;&#37327;&#25945;&#24072;&#21161;&#25163;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#19981;&#23545;&#23398;&#29983;&#36827;&#34892;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have uncovered that language model distillation is less effective when facing a large capacity gap between the teacher and the student, and introduced teacher assistant-based distillation to bridge the gap. As a connection, the scale and the performance of the teacher assistant is of vital importance to bring the knowledge from the teacher to the student. However, existing teacher assistant-based methods require maximally many trials before scheduling an optimal teacher assistant. To this end, we propose a minimal distillation schedule (MiniDisc) for scheduling the optimal teacher assistant in minimally one trial. In particular, motivated by the finding that the performance of the student is positively correlated to the scale-performance tradeoff of the teacher assistant, MiniDisc is designed with a $\lambda$-tradeoff to measure the optimality of the teacher assistant without trial distillation to the student. MiniDisc then can schedule the optimal teacher assistant with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#22320;&#29702;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#65292;&#25104;&#21151;&#22320;&#27880;&#20837;&#20102;&#22320;&#29702;&#35821;&#35328;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.08565</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Geographic Adaptation of Pretrained Language Models. (arXiv:2203.08565v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#22320;&#29702;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#65292;&#25104;&#21151;&#22320;&#27880;&#20837;&#20102;&#22320;&#29702;&#35821;&#35328;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#20294;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#24573;&#35270;&#20102;&#19982;&#35821;&#35328;&#26080;&#20851;&#30340;&#22320;&#29702;&#30693;&#35782;&#65292;&#36825;&#20123;&#30693;&#35782;&#36890;&#24120;&#38590;&#20197;&#36890;&#36807;&#20165;&#23545;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#33719;&#24471;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#32467;&#21512;&#35821;&#35328;&#24314;&#27169;&#21644;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#22320;&#29702;&#36866;&#24212;&#24615;&#65288;geoadaptation&#65289;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#19977;&#20010;&#22320;&#29702;&#21306;&#22495;&#30340;&#35821;&#35328;&#32452;&#30340;&#22235;&#20010;PLMs&#36827;&#34892;&#22320;&#29702;&#36866;&#24212;&#65292;&#24182;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#35780;&#20272;&#23427;&#20204;&#65306;&#32463;&#36807;&#24494;&#35843;&#30340;&#65288;&#21363;&#26377;&#30417;&#30563;&#30340;&#65289;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#12289;&#38646;&#26679;&#26412;&#65288;&#21363;&#26080;&#30417;&#30563;&#30340;&#65289;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#35782;&#21035;&#12289;&#38646;&#26679;&#26412;&#35821;&#35328;&#35782;&#21035;&#20197;&#21450;&#38646;&#26679;&#26412;&#26041;&#35328;&#29305;&#24449;&#39044;&#27979;&#12290;&#22320;&#29702;&#36866;&#24212;&#24615;&#38750;&#24120;&#25104;&#21151;&#22320;&#23558;&#22320;&#29702;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;PLMs&#20013;&#65306;&#32463;&#36807;&#22320;&#29702;&#36866;&#24212;&#30340;PLMs&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pretrained language models (PLMs) have been shown to possess a plethora of linguistic knowledge, the existing body of research has largely neglected extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone. Here, we contribute to closing this gap by examining geolinguistic knowledge, i.e., knowledge about geographic variation in language. We introduce geoadaptation, an intermediate training step that couples language modeling with geolocation prediction in a multi-task learning setup. We geoadapt four PLMs, covering language groups from three geographic areas, and evaluate them on five different tasks: fine-tuned (i.e., supervised) geolocation prediction, zero-shot (i.e., unsupervised) geolocation prediction, fine-tuned language identification, zero-shot language identification, and zero-shot prediction of dialect features. Geoadaptation is very successful at injecting geolinguistic knowledge into the PLMs: the geoadapted PLMs consistently ou
&lt;/p&gt;</description></item></channel></rss>