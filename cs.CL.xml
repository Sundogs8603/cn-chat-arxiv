<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PALO&#30340;&#22823;&#22411;&#22810;&#35821;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;10&#31181;&#20027;&#35201;&#35821;&#35328;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;&#32422;50&#20159;&#20154;&#21475;&#12290;&#20854;&#36890;&#36807;&#21322;&#33258;&#21160;&#21270;&#32763;&#35793;&#26041;&#27861;&#65292;&#23558;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20174;&#33521;&#35821;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#65292;&#20197;&#25552;&#21319;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14818</link><description>&lt;p&gt;
PALO&#65306;&#19968;&#20010;&#38024;&#23545;50&#20159;&#20154;&#30340;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PALO: A Polyglot Large Multimodal Model for 5B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PALO&#30340;&#22823;&#22411;&#22810;&#35821;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;10&#31181;&#20027;&#35201;&#35821;&#35328;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;&#32422;50&#20159;&#20154;&#21475;&#12290;&#20854;&#36890;&#36807;&#21322;&#33258;&#21160;&#21270;&#32763;&#35793;&#26041;&#27861;&#65292;&#23558;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20174;&#33521;&#35821;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#65292;&#20197;&#25552;&#21319;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;\Palo &#30340;&#22823;&#22411;&#22810;&#35821;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;Palo &#22312;&#21253;&#25324;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#21360;&#22320;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#27861;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#20420;&#35821;&#12289;&#20044;&#23572;&#37117;&#35821;&#21644;&#26085;&#35821;&#22312;&#20869;&#30340;10&#31181;&#20027;&#35201;&#35821;&#35328;&#20013;&#25552;&#20379;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;&#32422;50&#20159;&#20154;&#21475;&#65288;&#20840;&#29699;&#20154;&#21475;&#30340;65%&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#21322;&#33258;&#21160;&#21270;&#32763;&#35793;&#26041;&#27861;&#65292;&#21033;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#25968;&#25454;&#38598;&#20174;&#33521;&#35821;&#32763;&#35793;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;&#36739;&#39640;&#30340;&#35821;&#35328;&#20445;&#30495;&#24230;&#65292;&#21516;&#26102;&#30001;&#20110;&#20943;&#23569;&#20102;&#25163;&#21160;&#24037;&#20316;&#65292;&#20351;&#21487;&#25193;&#23637;&#24615;&#26356;&#24378;&#12290;&#24341;&#20837;&#22810;&#26679;&#21270;&#30340;&#25351;&#23548;&#38598;&#26377;&#21161;&#20110;&#25552;&#21319;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#24635;&#20307;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#37027;&#20123;&#27424;&#20195;&#34920;&#30340;&#35821;&#35328;&#22914;&#21360;&#22320;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#20044;&#23572;&#37117;&#35821;&#12290;&#26368;&#32456;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#35268;&#27169;&#65288;17B&#12289;70B&#21644;130B&#21442;&#25968;&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23637;&#31034;&#20854;&#27867;&#29992;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14818v1 Announce Type: new  Abstract: In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \textsc{Palo}. \textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the gen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#24433;&#21709;&#23454;&#20307;&#36319;&#36394;&#31561;&#20869;&#37096;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#33021;&#22815;&#22312;&#25968;&#23398;&#20219;&#21153;&#19978;&#23454;&#29616;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.14811</link><description>&lt;p&gt;
&#24494;&#35843;&#22686;&#24378;&#29616;&#26377;&#26426;&#21046;&#65306;&#23454;&#20307;&#36319;&#36394;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#24433;&#21709;&#23454;&#20307;&#36319;&#36394;&#31561;&#20869;&#37096;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#33021;&#22815;&#22312;&#25968;&#23398;&#20219;&#21153;&#19978;&#23454;&#29616;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#21270;&#22312;&#35832;&#22914;&#36981;&#24490;&#25351;&#20196;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#25968;&#23398;&#31561;&#24191;&#20041;&#20219;&#21153;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#31181;&#24494;&#35843;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#20013;&#20869;&#37096;&#35745;&#31639;&#30340;&#35299;&#37322;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23454;&#20307;&#36319;&#36394;&#30340;&#29305;&#24615;&#65292;&#36825;&#26159;&#35821;&#35328;&#29702;&#35299;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#22312;&#25968;&#23398;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#35782;&#21035;&#20986;&#20102;&#23454;&#29616;&#23454;&#20307;&#36319;&#36394;&#30340;&#26426;&#21046;&#65292;&#24182;&#26174;&#31034;&#20986;&#65288;i&#65289;&#21407;&#22987;&#27169;&#22411;&#21644;&#20854;&#31934;&#32454;&#35843;&#25972;&#29256;&#26412;&#20027;&#35201;&#23454;&#29616;&#23454;&#20307;&#36319;&#36394;&#30340;&#26159;&#30456;&#21516;&#30340;&#30005;&#36335;&#12290;&#20107;&#23454;&#19978;&#65292;&#21407;&#22987;&#27169;&#22411;&#30340;&#23454;&#20307;&#36319;&#36394;&#30005;&#36335;&#22312;&#32463;&#36807;&#24494;&#35843;&#30340;&#29256;&#26412;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#23436;&#25972;&#30340;&#21407;&#22987;&#27169;&#22411;&#12290;&#65288;ii&#65289;&#25152;&#26377;&#27169;&#22411;&#30340;&#30005;&#36335;&#23454;&#29616;&#22823;&#33268;&#30456;&#21516;&#30340;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14811v1 Announce Type: new  Abstract: Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionalit
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;RelayAttention&#31639;&#27861;&#26088;&#22312;&#25913;&#21892;&#28041;&#21450;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#20174;DRAM&#35835;&#21462;&#38544;&#34255;&#29366;&#24577;&#26469;&#28040;&#38500;&#29616;&#26377;&#22240;&#26524;&#27880;&#24847;&#21147;&#31639;&#27861;&#20013;&#30340;&#20869;&#23384;&#35775;&#38382;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2402.14808</link><description>&lt;p&gt;
RelayAttention&#65306;&#29992;&#20110;&#39640;&#25928;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#19982;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
RelayAttention for Efficient Large Language Model Serving with Long System Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;RelayAttention&#31639;&#27861;&#26088;&#22312;&#25913;&#21892;&#28041;&#21450;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#20174;DRAM&#35835;&#21462;&#38544;&#34255;&#29366;&#24577;&#26469;&#28040;&#38500;&#29616;&#26377;&#22240;&#26524;&#27880;&#24847;&#21147;&#31639;&#27861;&#20013;&#30340;&#20869;&#23384;&#35775;&#38382;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#21487;&#33021;&#28041;&#21450;&#19968;&#20010;&#38271;&#30340;&#31995;&#32479;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#21547;&#20219;&#21153;&#30340;&#25351;&#31034;&#12289;&#31034;&#20363;&#21644;&#30693;&#35782;&#25991;&#26723;&#65292;&#24182;&#22312;&#35768;&#22810;&#35831;&#27714;&#20013;&#22797;&#29992;&#12290;&#28982;&#32780;&#65292;&#38271;&#31995;&#32479;&#25552;&#31034;&#20250;&#23548;&#33268;&#21534;&#21520;&#37327;/&#24310;&#36831;&#29942;&#39048;&#65292;&#22240;&#20026;&#29983;&#25104;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#25104;&#26412;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#38271;&#32780;&#22686;&#21152;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#28041;&#21450;&#38271;&#31995;&#32479;&#25552;&#31034;&#30340;LLM&#26381;&#21153;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#22788;&#29702;&#36825;&#20123;&#31995;&#32479;&#25552;&#31034;&#22312;&#29616;&#26377;&#22240;&#26524;&#27880;&#24847;&#21147;&#35745;&#31639;&#31639;&#27861;&#20013;&#38656;&#35201;&#22823;&#37327;&#20887;&#20313;&#30340;&#20869;&#23384;&#35775;&#38382;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#25209;&#37327;&#35831;&#27714;&#65292;&#31995;&#32479;&#25552;&#31034;&#30340;&#32531;&#23384;&#38544;&#34255;&#29366;&#24577;&#65288;&#21363;&#38190;-&#20540;&#23545;&#65289;&#34987;&#22810;&#27425;&#20174;&#33455;&#29255;&#22806;&#30340;DRAM&#20256;&#36755;&#21040;&#33455;&#29255;&#19978;&#30340;SRAM&#65292;&#27599;&#27425;&#23545;&#24212;&#19968;&#20010;&#21333;&#29420;&#30340;&#35831;&#27714;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RelayAttention&#65292;&#19968;&#31181;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#20801;&#35768;&#20165;&#20174;DRAM&#35835;&#21462;&#36825;&#20123;&#38544;&#34255;&#29366;&#24577;&#19968;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14808v1 Announce Type: new  Abstract: Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#29305;&#24449;</title><link>https://arxiv.org/abs/2402.14805</link><description>&lt;p&gt;
&#36890;&#36807;&#22806;&#37096;&#35780;&#20272;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#22810;&#37325;&#20154;&#26684;
&lt;/p&gt;
&lt;p&gt;
Identifying Multiple Personalities in Large Language Models with External Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#19982;&#20154;&#31867;&#26085;&#24120;&#24212;&#29992;&#25972;&#21512;&#65292;&#20851;&#20110;LLMs&#34892;&#20026;&#30340;&#35768;&#22810;&#31038;&#20250;&#21644;&#20262;&#29702;&#20851;&#20999;&#34987;&#25552;&#20986;&#12290;&#20102;&#35299;LLMs&#34892;&#20026;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#20998;&#26512;&#23427;&#20204;&#30340;&#20154;&#26684;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20026;&#20154;&#31867;&#21019;&#24314;&#30340;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26469;&#37327;&#21270;LLMs&#30340;&#20154;&#26684;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25209;&#35780;&#36136;&#30097;&#23558;&#36825;&#20123;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#24212;&#29992;&#20110;LLMs&#26102;&#30340;&#36866;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26367;&#20195;&#30340;&#20154;&#26684;&#27979;&#37327;&#26041;&#27861;&#26469;&#30740;&#31350;LLMs&#30340;&#20154;&#26684;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#65292;&#36825;&#37324;&#25105;&#20204;&#19981;&#26159;&#36890;&#36807;&#22312;&#26446;&#20811;&#29305;&#37327;&#34920;&#19978;&#25552;&#31034;LLMs&#22238;&#31572;&#22810;&#36873;&#39064;&#65292;&#32780;&#26159;&#36890;&#36807;&#20998;&#26512;LLMs&#23545;&#22806;&#37096;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#30340;&#24320;&#25918;&#24335;&#24773;&#22659;&#38382;&#39064;&#30340;&#22238;&#31572;&#26469;&#35780;&#20272;LLMs&#30340;&#20154;&#26684;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;Llama2-7B&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20316;&#20026;MBTI&#20154;&#26684;&#39044;&#27979;&#22120;&#65292;&#35813;&#39044;&#27979;&#22120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14805v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs. One of the ways to comprehend LLMs' behavior is to analyze their personalities. Many recent studies quantify LLMs' personalities using self-assessment tests that are created for humans. Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs. In this paper, we investigate LLM personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of prompting LLMs with multiple-choice questions in the Likert scale, we evaluate LLMs' personalities by analyzing their responses toward open-ended situational questions using an external machine learning model. We first fine-tuned a Llama2-7B model as the MBTI personality predictor that outperforms the state-of-the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MATH-Vision&#65288;MATH-V&#65289;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#24403;&#21069;LMMs&#21644;&#20154;&#31867;&#22312;MATH-V&#19978;&#30340;&#34920;&#29616;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.14804</link><description>&lt;p&gt;
&#20351;&#29992;MATH-Vision&#25968;&#25454;&#38598;&#27979;&#37327;&#22810;&#27169;&#24577;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MATH-Vision&#65288;MATH-V&#65289;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#24403;&#21069;LMMs&#21644;&#20154;&#31867;&#22312;MATH-V&#19978;&#30340;&#34920;&#29616;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#35270;&#35273;&#32972;&#26223;&#19979;&#30340;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;MathVista&#65289;&#19978;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#22312;&#38382;&#39064;&#22810;&#26679;&#24615;&#21644;&#28085;&#30422;&#23398;&#31185;&#33539;&#22260;&#26041;&#38754;&#23384;&#22312;&#26174;&#30528;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MATH-Vision&#65288;MATH-V&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25910;&#38598;&#20102;&#26469;&#33258;&#30495;&#23454;&#25968;&#23398;&#31454;&#36187;&#30340;3,040&#20010;&#39640;&#36136;&#37327;&#25968;&#23398;&#38382;&#39064;&#21644;&#35270;&#35273;&#32972;&#26223;&#30340;&#25968;&#25454;&#38598;&#12290;&#36328;&#36234;16&#20010;&#19981;&#21516;&#30340;&#25968;&#23398;&#23398;&#31185;&#65292;&#20998;&#20026;5&#20010;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20998;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20026;&#35780;&#20272;LMMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#19988;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24403;&#21069;LMMs&#19982;MATH-V&#19978;&#20154;&#31867;&#34920;&#29616;&#20043;&#38388;&#30340;&#26174;&#33879;&#34920;&#29616;&#24046;&#36317;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#25512;&#36827;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14804v1 Announce Type: cross  Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements i
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14800</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#19987;&#23478;&#37117;&#30456;&#31561;: &#28151;&#21512;&#19987;&#23478;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;
&lt;/p&gt;
&lt;p&gt;
Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14800
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#23637;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#26159;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;LLMs&#30340;&#20986;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;LLMs&#30456;&#27604;&#65292;MoE LLMs&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#65292;&#20173;&#28982;&#24456;&#38590;&#37096;&#32626;&#23427;&#20204;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#19987;&#38376;&#35774;&#35745;&#30340;&#30828;&#20214;&#30340;&#26435;&#37325;&#21098;&#26525;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#20027;&#35201;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#21363;&#25554;&#21363;&#29992;&#30340;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#26469;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;MoE LLMs&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#37096;&#32626;&#25928;&#29575;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#22686;&#21152;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#39281;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14800v1 Announce Type: cross  Abstract: A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining sat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.14798</link><description>&lt;p&gt;
&#21033;&#29992;&#38750;&#27491;&#24335;&#36923;&#36753;&#22686;&#24378;&#31995;&#32479;&#21270;&#20998;&#35299;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20026;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20363;&#22914;&#22312;&#19981;&#20381;&#36182;&#33030;&#24369;&#30340;&#24418;&#24335;&#36923;&#36753;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21644;&#35780;&#20272;&#30452;&#35266;&#30340;&#12289;&#31867;&#20284;&#35777;&#26126;&#30340;&#25991;&#26412;&#34164;&#28085;&#26641;&#12290;&#28982;&#32780;&#65292;&#27839;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#36827;&#23637;&#21463;&#21040;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#32570;&#20047;&#26126;&#30830;&#30340;&#30830;&#23450;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#28165;&#26224;&#21327;&#35758;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25968;&#25454;&#38598;RDTE (Recognizing Decompositional Textual Entailment) &#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#27604;&#20808;&#21069;&#30340;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#39640;&#24471;&#22810;&#65288;+9%&#65289;&#65292;&#34920;&#26126;RDTE&#22312;&#38271;&#26399;&#23384;&#22312;&#30340;&#20851;&#20110;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14798v1 Announce Type: cross  Abstract: Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference. We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#25903;&#25345;&#19979;&#65292;&#33521;&#35821;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#30340;&#20934;&#30830;&#12289;&#26377;&#29992;&#22238;&#24212;&#65292;&#20294;&#23384;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.14778</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-lingual transfer in instruction tuning of large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#25903;&#25345;&#19979;&#65292;&#33521;&#35821;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#30340;&#20934;&#30830;&#12289;&#26377;&#29992;&#22238;&#24212;&#65292;&#20294;&#23384;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#65288;IT&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25945;&#23548;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#20219;&#24847;&#25351;&#20196;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;IT&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#24403;LLM&#22312;&#20165;&#33521;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#28982;&#21518;&#22312;&#20854;&#20182;&#35821;&#35328;&#29992;&#25143;&#25552;&#31034;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27169;&#22411;&#37197;&#32622;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#35780;&#20272;&#31574;&#30053;&#29992;&#20110;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#27169;&#22411;&#35757;&#32451;&#30340;&#25152;&#26377;&#38454;&#27573;&#37117;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#22312;IT&#20013;&#20063;&#20250;&#25104;&#21151;&#21457;&#29983;&#65292;&#20294;&#21482;&#26377;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#32771;&#34385;&#21040;&#22810;&#35821;&#35328;&#24615;&#20197;&#21450;&#26377;&#36275;&#22815;&#22823;&#30340;IT&#25968;&#25454;&#26102;&#25165;&#20250;&#21457;&#29983;&#12290;&#32463;&#36807;&#33521;&#35821;&#35757;&#32451;&#30340;LLMs&#33021;&#22815;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#29983;&#25104;&#20934;&#30830;&#12289;&#20840;&#38754;&#19988;&#26377;&#24110;&#21161;&#30340;&#22238;&#24212;&#65292;&#20294;&#32570;&#20047;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20598;&#23572;&#21487;&#33021;&#23384;&#22312;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14778v1 Announce Type: cross  Abstract: Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings. In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages. We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual instruction following. We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.
&lt;/p&gt;</description></item><item><title>Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14776</link><description>&lt;p&gt;
2D Matryoshka&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
2D Matryoshka Sentence Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14776
&lt;/p&gt;
&lt;p&gt;
Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#24120;&#35265;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#22266;&#23450;&#38271;&#24230;&#30340;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#21477;&#23376;&#23884;&#20837;&#65292;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#26410;&#30693;&#30340;&#35745;&#31639;&#32422;&#26463;&#21644;&#39044;&#31639;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#28789;&#27963;&#24615;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)(Kusupati&#31561;&#20154;&#65292;2022)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#21363;&#20351;&#29992;&#36739;&#20302;&#30340;&#23884;&#20837;&#32500;&#24230;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#12290;&#21487;&#20197;&#36890;&#36807;&#36739;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#36798;&#21040;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#21152;&#24555;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#25913;&#36827;&#20102;&#25928;&#29575;&#65292;MRL&#20173;&#35201;&#22312;&#33719;&#24471;&#23884;&#20837;&#20043;&#21069;&#36941;&#21382;&#25152;&#26377;Transformer&#23618;&#65292;&#36825;&#20173;&#28982;&#26159;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36825;&#24341;&#21457;&#20102;&#26159;&#21542;&#22266;&#23450;&#25968;&#37327;&#30340;Transformer&#23618;&#20250;&#24433;&#21709;&#34920;&#31034;&#36136;&#37327;&#20197;&#21450;&#20351;&#29992;&#20013;&#38388;&#23618;&#36827;&#34892;&#21477;&#23376;&#34920;&#31034;&#26159;&#21542;&#21487;&#34892;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 Announce Type: new  Abstract: Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MT-Bench-101&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#21253;&#21547;4208&#36718;&#23545;&#35805;&#25968;&#25454;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#20102;21&#31181;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19981;&#21516;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14762</link><description>&lt;p&gt;
MT-Bench-101: &#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MT-Bench-101&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#21253;&#21547;4208&#36718;&#23545;&#35805;&#25968;&#25454;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#20102;21&#31181;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19981;&#21516;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22823;&#22823;&#22686;&#24378;&#20102;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#36718;&#23545;&#35805;&#25110;&#32773;&#25552;&#20379;&#31895;&#31890;&#24230;&#21644;&#19981;&#23436;&#25972;&#30340;&#22810;&#36718;&#23545;&#35805;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#30495;&#23454;&#23545;&#35805;&#30340;&#22797;&#26434;&#24615;&#21644;&#32454;&#24494;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MT-Bench-101&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#22810;&#36718;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;13&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;1388&#20010;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;4208&#36718;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#12290;&#28982;&#21518;&#25105;&#20204;&#22522;&#20110;MT-Bench-101&#35780;&#20272;&#20102;21&#20010;&#27969;&#34892;&#30340;LLMs&#65292;&#20174;&#33021;&#21147;&#21644;&#20219;&#21153;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#35266;&#23519;&#21040;LLMs&#22312;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14762v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns with
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.14760</link><description>&lt;p&gt;
&#27867;&#21270;&#22870;&#21169;&#24314;&#27169;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalizing Reward Modeling for Out-of-Distribution Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;(PL)&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26088;&#22312;&#20351;LLMs&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#20197;&#24448;&#26377;&#20851;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#30340;&#30740;&#31350;&#24050;&#22312;&#20998;&#24067;&#20869;&#30340;PL&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#30340;&#38590;&#24230;&#65292;&#20026;&#27599;&#20010;&#36935;&#21040;&#30340;&#20998;&#24067;&#31163;&#25955;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;(OOD) PL&#20013;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#26469;&#22686;&#24378;LLMs&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#23454;&#29992;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;OOD PL&#38382;&#39064;&#12290;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#65292;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#24341;&#23548;&#31574;&#30053;&#23398;&#20064;&#20197;&#20351;&#20043;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#22312;&#36935;&#21040;&#27979;&#35797;&#20998;&#24067;&#26102;&#65292;&#20803;&#27979;&#35797;&#36807;&#31243;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14760v1 Announce Type: cross  Abstract: Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14746</link><description>&lt;p&gt;
&#25193;&#23637;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14746
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#21363;&#22823;&#37096;&#20998;&#21442;&#25968;&#20026;&#38646;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21363;&#37027;&#20123;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#35757;&#32451;&#25439;&#22833;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20272;&#35745;&#65292;&#20197;&#33719;&#24471;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#20013;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#19978;&#19979;&#30028;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#65306;(1)&#35201;&#22312;&#35757;&#32451;&#35821;&#26009;&#20013;&#34920;&#31034;&#30340;&#25216;&#33021;&#25968;&#37327;&#32763;&#20493;&#65292;&#38656;&#35201;&#23558;&#35821;&#26009;&#35268;&#27169;&#22823;&#32422;&#25193;&#23637;&#19977;&#21040;&#20116;&#20493;&#65292;(2)&#23545;&#20110;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;$N$&#21644;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;$D$&#28385;&#36275;$N \sim D^{0.58}$&#30340;&#20851;&#31995;&#65292;(3)&#22914;&#26524;&#19968;&#20010;LLM&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#35821;&#26009;&#20013;&#30340;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#65292;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#20986;&#26032;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.14744</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22478;&#24066;&#23621;&#27665;&#65306;&#29992;&#20110;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#30340;LLM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#28789;&#27963;&#39640;&#25928;&#30340;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#12290;LLMs&#36890;&#36807;&#39640;&#25928;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#24182;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#20379;&#22810;&#21151;&#33021;&#24615;, &#20811;&#26381;&#20102;&#20197;&#24448;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#36843;&#20999;&#38656;&#27714;, &#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;: &#23558;LLMs&#19982;&#20016;&#23500;&#30340;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;, &#24320;&#21457;&#21487;&#38752;&#30340;&#27963;&#21160;&#29983;&#25104;&#31574;&#30053;, &#20197;&#21450;&#25506;&#32034;LLMs&#22312;&#22478;&#24066;&#31227;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20195;&#29702;&#26694;&#26550;, &#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#20010;&#20307;&#27963;&#21160;&#27169;&#24335;&#21644;&#21160;&#26426;, &#21253;&#25324;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#33258;&#27965;&#26041;&#27861;&#21644;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;, &#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22810;&#35821;&#35328;BERT&#23545;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#36827;&#34892;&#20381;&#23384;&#26631;&#27880;&#65292;&#21152;&#36895;&#24182;&#31616;&#21270;&#20381;&#23384;&#26631;&#27880;&#36807;&#31243;&#65292;&#23558;&#20135;&#29983;&#30340;&#26641;&#24211;&#26377;&#21161;&#20110;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#25991;&#26723;&#30340;&#33258;&#21160;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.14743</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#35821;&#35328;BERT&#23545;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#36827;&#34892;&#20381;&#23384;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Dependency Annotation of Ottoman Turkish with Multilingual BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14743
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22810;&#35821;&#35328;BERT&#23545;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#36827;&#34892;&#20381;&#23384;&#26631;&#27880;&#65292;&#21152;&#36895;&#24182;&#31616;&#21270;&#20381;&#23384;&#26631;&#27880;&#36807;&#31243;&#65292;&#23558;&#20135;&#29983;&#30340;&#26641;&#24211;&#26377;&#21161;&#20110;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#25991;&#26723;&#30340;&#33258;&#21160;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#29992;&#20110;&#31532;&#19968;&#20010;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#20381;&#23384;&#26641;&#24211;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;i&#65289;&#20351;&#29992;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35299;&#26512;&#27169;&#22411;&#36827;&#34892;&#20266;&#26631;&#27880;&#25968;&#25454;&#65292;ii&#65289;&#25163;&#21160;&#20462;&#27491;&#20266;&#26631;&#27880;&#65292;&#20197;&#21450;iii&#65289;&#29992;&#20462;&#27491;&#30340;&#26631;&#27880;&#24494;&#35843;&#35299;&#26512;&#27169;&#22411;&#65292;&#25105;&#20204;&#21152;&#24555;&#24182;&#31616;&#21270;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20381;&#23384;&#26631;&#27880;&#36807;&#31243;&#12290;&#26368;&#32456;&#20135;&#29983;&#30340;&#26641;&#24211;&#65292;&#23558;&#25104;&#20026;&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#65288;UD&#65289;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#23558;&#20415;&#21033;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#25991;&#26723;&#30340;&#33258;&#21160;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#21382;&#21490;&#36951;&#20135;&#20013;&#34164;&#21547;&#30340;&#35821;&#35328;&#20016;&#23500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14743v1 Announce Type: new  Abstract: This study introduces a pretrained large language model-based annotation methodology for the first dependency treebank in Ottoman Turkish. Our experimental results show that, iteratively, i) pseudo-annotating data using a multilingual BERT-based parsing model, ii) manually correcting the pseudo-annotations, and iii) fine-tuning the parsing model with the corrected annotations, we speed up and simplify the challenging dependency annotation process. The resulting treebank, that will be a part of the Universal Dependencies (UD) project, will facilitate automated analysis of Ottoman Turkish documents, unlocking the linguistic richness embedded in this historical heritage.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#35789;&#27719;&#25193;&#23637;&#26041;&#27861;&#65288;EEVE&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#20854;&#22312;&#38889;&#25991;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14714</link><description>&lt;p&gt;
&#39640;&#25928;&#26377;&#25928;&#30340;&#35789;&#27719;&#25193;&#23637;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#35789;&#27719;&#25193;&#23637;&#26041;&#27861;&#65288;EEVE&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#20854;&#22312;&#38889;&#25991;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25253;&#21578;&#20171;&#32461;&#20102;\texttt{EEVE-Korean-v1.0}&#65292;&#36825;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38889;&#25991;&#36866;&#37197;&#29256;&#26412;&#65292;&#23637;&#29616;&#20986;&#22312;&#33521;&#25991;&#21644;&#38889;&#25991;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#35789;&#27719;&#25193;&#23637;&#26041;&#27861;&#65288;EEVE&#65289;&#65292;&#21253;&#25324;&#21442;&#25968;&#20923;&#32467;&#21644;&#23376;&#35789;&#21021;&#22987;&#21270;&#12290;&#19982;&#20808;&#21069;&#35748;&#20026;&#26032;&#23884;&#20837;&#38656;&#35201;&#19978;&#19975;&#20159;&#35757;&#32451;&#26631;&#35760;&#30340;&#21162;&#21147;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;20&#20159;&#26631;&#35760;&#20869;&#26174;&#30528;&#25552;&#21319;&#38750;&#33521;&#35821;&#29087;&#32451;&#24230;&#12290;&#25130;&#33267;2024&#24180;1&#26376;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\texttt{EEVE-Korean-10.8B-v1.0}&#22312;Open Ko-LLM&#27036;&#21333;&#19978;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#65292;&#25104;&#20026;&#20102;&#24320;&#28304;&#31038;&#21306;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#38889;&#25991;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26681;&#25454;Hugging Face&#30340;&#25490;&#34892;&#27036;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14714v1 Announce Type: cross  Abstract: This report introduces \texttt{EEVE-Korean-v1.0}, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model \texttt{EEVE-Korean-10.8B-v1.0} ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face's leaderboard. We ope
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#22312;&#21407;&#22987;&#21477;&#23376;&#20013;&#39044;&#27979;&#35789;&#27719;&#20462;&#25913;&#65292;&#24341;&#20837;LLM&#22686;&#24378;&#25439;&#22833;&#36827;&#34892;&#30693;&#35782;&#25552;&#28860;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#38590;&#24230;&#24863;&#30693;&#30340;&#22635;&#20805;&#27169;&#22359;&#23558;&#22797;&#26434;&#35789;&#26367;&#25442;&#20026;&#31616;&#21333;&#35789;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14704</link><description>&lt;p&gt;
&#19968;&#31181;LLM&#22686;&#24378;&#30340;&#35789;&#27719;&#31616;&#21270;&#23545;&#25239;&#32534;&#36753;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An LLM-Enhanced Adversarial Editing System for Lexical Simplification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#22312;&#21407;&#22987;&#21477;&#23376;&#20013;&#39044;&#27979;&#35789;&#27719;&#20462;&#25913;&#65292;&#24341;&#20837;LLM&#22686;&#24378;&#25439;&#22833;&#36827;&#34892;&#30693;&#35782;&#25552;&#28860;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#38590;&#24230;&#24863;&#30693;&#30340;&#22635;&#20805;&#27169;&#22359;&#23558;&#22797;&#26434;&#35789;&#26367;&#25442;&#20026;&#31616;&#21333;&#35789;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#31616;&#21270;&#65288;LS&#65289;&#26088;&#22312;&#22312;&#35789;&#27719;&#32423;&#21035;&#31616;&#21270;&#25991;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#26631;&#27880;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LS&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#23545;&#25239;&#32534;&#36753;&#31995;&#32479;&#65292;&#24182;&#32467;&#21512;&#28151;&#28102;&#25439;&#22833;&#21644;&#19981;&#21464;&#24615;&#25439;&#22833;&#26469;&#39044;&#27979;&#21407;&#22987;&#21477;&#23376;&#20013;&#30340;&#35789;&#27719;&#20462;&#25913;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#22686;&#24378;&#25439;&#22833;&#65292;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#25552;&#28860;&#25104;&#23567;&#22411;LS&#31995;&#32479;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21477;&#23376;&#20013;&#30340;&#22797;&#26434;&#35789;&#34987;&#23631;&#34109;&#65292;&#21046;&#20316;&#20102;&#19968;&#20010;&#22522;&#20110;&#38590;&#24230;&#24863;&#30693;&#30340;&#22635;&#20805;&#27169;&#22359;&#65292;&#29992;&#26356;&#31616;&#21333;&#30340;&#35789;&#26367;&#25442;&#23631;&#34109;&#20301;&#32622;&#12290;&#26368;&#21518;&#65292;&#23545;&#19977;&#20010;&#22522;&#20934;LS&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14704v1 Announce Type: new  Abstract: Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>InfFeed&#23558;&#24433;&#21709;&#20989;&#25968;&#20316;&#20026;&#21453;&#39304;&#65292;&#29992;&#20110;&#25913;&#21892;&#20027;&#35266;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#38656;&#35201;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#28857;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#35843;&#25972;&#26631;&#31614;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.14702</link><description>&lt;p&gt;
InfFeed&#65306;&#23558;&#24433;&#21709;&#20989;&#25968;&#20316;&#20026;&#21453;&#39304;&#20197;&#25913;&#21892;&#20027;&#35266;&#20219;&#21153;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14702
&lt;/p&gt;
&lt;p&gt;
InfFeed&#23558;&#24433;&#21709;&#20989;&#25968;&#20316;&#20026;&#21453;&#39304;&#65292;&#29992;&#20110;&#25913;&#21892;&#20027;&#35266;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#38656;&#35201;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#28857;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#35843;&#25972;&#26631;&#31614;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24433;&#21709;&#20989;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#24037;&#20855;&#65292;&#36890;&#36807;&#37327;&#21270;&#21487;&#33021;&#24433;&#21709;&#27979;&#35797;&#39044;&#27979;&#30340;&#20010;&#21035;&#35757;&#32451;&#23454;&#20363;&#30340;&#25200;&#21160;&#65292;&#23454;&#29616;&#23545;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#21452;&#37325;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24433;&#21709;&#20989;&#25968;&#20316;&#20026;&#21453;&#39304;&#24341;&#20837;&#27169;&#22411;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#22312;&#25968;&#25454;&#38598;&#25193;&#23637;&#32451;&#20064;&#20013;&#65292;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#33258;&#21160;&#35782;&#21035;&#26368;&#21021;&#30001;&#26576;&#20123;&#29616;&#26377;&#26041;&#27861;&#8216;&#38134;&#8217;&#27880;&#37322;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#38656;&#35201;&#26631;&#27880;&#32773;&#20132;&#21449;&#26816;&#26597;&#65288;&#21644;&#32416;&#27491;&#65289;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;InfFeed&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#35745;&#31639;&#30446;&#26631;&#23454;&#20363;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#23454;&#20363;&#12290;&#21521;&#31532;&#19968;&#20010;&#30446;&#26631;&#21162;&#21147;&#65292;&#25105;&#20204;&#26681;&#25454;&#20854;&#24433;&#21709;&#32773;&#30340;&#26631;&#31614;&#35843;&#25972;&#30446;&#26631;&#23454;&#20363;&#30340;&#26631;&#31614;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;InfFeed&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65288;&#21253;&#25324;LLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14702v1 Announce Type: new  Abstract: Recently, influence functions present an apparatus for achieving explainability for deep neural models by quantifying the perturbation of individual train instances that might impact a test prediction. Our objectives in this paper are twofold. First we incorporate influence functions as a feedback into the model to improve its performance. Second, in a dataset extension exercise, using influence functions to automatically identify data points that have been initially `silver' annotated by some existing method and need to be cross-checked (and corrected) by annotators to improve the model performance. To meet these objectives, in this paper, we introduce InfFeed, which uses influence functions to compute the influential instances for a target instance. Toward the first objective, we adjust the label of the target instance based on its influencer(s) label. In doing this, InfFeed outperforms the state-of-the-art baselines (including LLMs) b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14701</link><description>&lt;p&gt;
COMPASS&#65306;&#21033;&#29992;&#35821;&#35328;&#24314;&#27169;&#23545;&#24739;&#32773;-&#27835;&#30103;&#24072;&#32852;&#30431;&#31574;&#30053;&#36827;&#34892;&#35745;&#31639;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#26159;&#39044;&#27979;&#24515;&#29702;&#27835;&#30103;&#27835;&#30103;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20256;&#32479;&#19978;&#65292;&#24037;&#20316;&#32852;&#30431;&#35780;&#20272;&#20381;&#36182;&#20110;&#27835;&#30103;&#24072;&#21644;&#24739;&#32773;&#22635;&#20889;&#30340;&#38382;&#21367;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;COMPASS&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#30452;&#25509;&#20174;&#24515;&#29702;&#27835;&#30103;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#20013;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#30340;&#36716;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;&#24037;&#20316;&#32852;&#30431;&#28165;&#21333;&#20013;&#38472;&#36848;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#28085;&#30422;&#22810;&#31181;&#31934;&#31070;&#30142;&#30149;&#30340;&#36229;&#36807;950&#20010;&#20250;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#24494;&#22320;&#26144;&#23556;&#24739;&#32773;-&#27835;&#30103;&#24072;&#23545;&#40784;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#31070;&#32463;&#20027;&#39064;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#21306;&#22495;&#21010;&#20998;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#24212;&#35821;&#35328;&#33021;&#21147;&#30340;&#26680;&#24515;&#21306;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21435;&#38500;&#35813;&#21306;&#22495;&#20250;&#23548;&#33268;&#36328;30&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.14700</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Unveiling Linguistic Regions in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#21306;&#22495;&#21010;&#20998;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#24212;&#35821;&#35328;&#33021;&#21147;&#30340;&#26680;&#24515;&#21306;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21435;&#38500;&#35813;&#21306;&#22495;&#20250;&#23548;&#33268;&#36328;30&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20102;&#30456;&#24403;&#22823;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#21892;LLMs&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#19978;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#22914;&#20309;&#23454;&#29616;&#36328;&#35821;&#35328;&#23545;&#40784;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#32570;&#20047;&#30740;&#31350;&#12290;&#20174;&#21306;&#22495;&#21010;&#20998;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26412;&#25991;&#22312;LLMs&#30340;&#35821;&#35328;&#33021;&#21147;&#19978;&#36827;&#34892;&#20102;&#20960;&#39033;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#21306;&#22495;&#23545;&#24212;&#20110;&#35821;&#35328;&#33021;&#21147;&#65292;&#22823;&#32422;&#21344;&#24635;&#27169;&#22411;&#21442;&#25968;&#30340;1%&#12290;&#36890;&#36807;&#23558;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#26469;&#21435;&#38500;&#36825;&#20010;&#26680;&#24515;&#21306;&#22495;&#65292;&#20250;&#23548;&#33268;30&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26680;&#24515;&#21306;&#22495;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#65292;&#23545;&#29305;&#23450;&#32500;&#24230;&#19978;&#30340;&#21333;&#20010;&#21442;&#25968;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#35821;&#35328;&#33021;&#21147;&#30340;&#20007;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29420;&#29305;&#30340;&#21306;&#22495;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14700v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#28789;&#27963;&#35780;&#20272;&#26694;&#26550;UFO&#65292;&#29992;&#20110;&#39564;&#35777;&#20107;&#23454;&#24182;&#25903;&#25345;&#21363;&#25554;&#21363;&#29992;&#30340;&#20107;&#23454;&#26469;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.14690</link><description>&lt;p&gt;
UFO&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#30340;&#32479;&#19968;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#28789;&#27963;&#35780;&#20272;&#26694;&#26550;UFO&#65292;&#29992;&#20110;&#39564;&#35777;&#20107;&#23454;&#24182;&#25903;&#25345;&#21363;&#25554;&#21363;&#29992;&#30340;&#20107;&#23454;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#29983;&#25104;&#19982;&#20154;&#31867;&#30693;&#35782;&#19981;&#19968;&#33268;&#30340;&#25991;&#26412;&#65292;&#23548;&#33268;&#20107;&#23454;&#19981;&#20934;&#30830;&#25110;&#8220;&#24187;&#35273;&#8221;&#12290;&#29616;&#26377;&#30740;&#31350;&#35780;&#20272;LLMs&#30340;&#20107;&#23454;&#24615;&#28041;&#21450;&#20351;&#29992;LLM&#25552;&#21462;&#20107;&#23454;&#20027;&#24352;&#65292;&#24182;&#23558;&#20854;&#19982;&#39044;&#23450;&#20041;&#30340;&#20107;&#23454;&#26469;&#28304;&#36827;&#34892;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#25351;&#26631;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#24182;&#19988;&#19981;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#19981;&#21516;&#20219;&#21153;&#20013;&#20107;&#23454;&#26469;&#28304;&#30340;&#21487;&#26367;&#20195;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#22235;&#31181;&#21487;&#29992;&#20107;&#23454;&#26469;&#28304;&#36827;&#34892;&#20998;&#31867;&#65306;&#20154;&#31867;&#32534;&#20889;&#30340;&#35777;&#25454;&#12289;&#21442;&#32771;&#25991;&#29486;&#12289;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#21644;LLM&#30693;&#35782;&#65292;&#20197;&#21450;&#21253;&#21547;&#20845;&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#30340;&#20116;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\texttt{UFO}&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#32479;&#19968;&#28789;&#27963;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#21363;&#25554;&#21363;&#29992;&#30340;&#20107;&#23454;&#26469;&#28304;&#39564;&#35777;&#20107;&#23454;&#12290;&#25105;&#20204;&#22522;&#20110;&#35813;&#26694;&#26550;&#23454;&#26045;&#20102;&#20116;&#20010;&#35780;&#20272;&#22330;&#26223;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14690v1 Announce Type: new  Abstract: Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or \textit{hallucination}. Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources in different tasks is under-explored. To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and LLM knowledge, along with five text generation tasks containing six representative datasets. Then, we propose \texttt{UFO}, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources. We implement five evaluation scenarios based on this framework. Experimental results show that for most QA tasks, human-w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#20010;&#24615;&#29305;&#24449;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#35748;&#30693;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#25552;&#20986;&#23545;&#35266;&#23519;&#32467;&#26524;&#30340;&#24515;&#29702;&#29702;&#35770;&#21644;&#25351;&#26631;&#20551;&#35774;</title><link>https://arxiv.org/abs/2402.14679</link><description>&lt;p&gt;
&#35748;&#30693;&#19982;&#34892;&#20026;&#19968;&#33268;&#36824;&#26159;&#19981;&#19968;&#33268;&#65306;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14679
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#20010;&#24615;&#29305;&#24449;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#35748;&#30693;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#25552;&#20986;&#23545;&#35266;&#23519;&#32467;&#26524;&#30340;&#24515;&#29702;&#29702;&#35770;&#21644;&#25351;&#26631;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22238;&#31572;&#20154;&#26684;&#38382;&#21367;&#35843;&#26597;&#26469;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34920;&#36798;&#31867;&#20154;&#20010;&#24615;&#29305;&#24449;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;LLMs&#25152;&#34920;&#36798;&#30340;&#20010;&#24615;&#20542;&#21521;&#19982;&#23427;&#20204;&#23454;&#38469;&#8220;&#34892;&#20026;&#8221;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#26816;&#39564;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#31867;&#20154;&#20010;&#24615;&#27169;&#24335;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;LLM&#36755;&#20986;&#19982;&#24050;&#24314;&#31435;&#30340;&#20154;&#31867;&#22522;&#20934;&#20043;&#38388;&#30340;&#23545;&#27604;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;LLMs&#20013;&#35748;&#30693;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26681;&#25454;&#24515;&#29702;&#29702;&#35770;&#21644;&#25351;&#26631;&#23545;&#35266;&#23519;&#32467;&#26524;&#25552;&#20986;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14679v1 Announce Type: new  Abstract: In this study, we investigate the reliability of Large Language Models (LLMs) in professing human-like personality traits through responses to personality questionnaires. Our goal is to evaluate the consistency between LLMs' professed personality inclinations and their actual "behavior", examining the extent to which these models can emulate human-like personality patterns. Through a comprehensive analysis of LLM outputs against established human benchmarks, we seek to understand the cognition-action divergence in LLMs and propose hypotheses for the observed results based on psychological theories and metrics.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#21033;&#29992;&#24037;&#20855;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#24037;&#20855;&#26469;&#36741;&#21161;&#35821;&#35328;&#20195;&#29702;&#22312;&#24222;&#22823;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#30693;&#35782;&#24211;&#21644;&#25968;&#25454;&#24211;&#31561;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#20511;&#21161;&#24037;&#20855;&#22686;&#24378;&#35821;&#35328;&#20195;&#29702;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14672</link><description>&lt;p&gt;
&#35821;&#35328;&#20013;&#38388;&#20214;&#65306;&#24037;&#20855;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23545;&#35821;&#35328;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14672
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#21033;&#29992;&#24037;&#20855;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#24037;&#20855;&#26469;&#36741;&#21161;&#35821;&#35328;&#20195;&#29702;&#22312;&#24222;&#22823;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#30693;&#35782;&#24211;&#21644;&#25968;&#25454;&#24211;&#31561;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#20511;&#21161;&#24037;&#20855;&#22686;&#24378;&#35821;&#35328;&#20195;&#29702;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#24050;&#32463;&#36828;&#36828;&#36229;&#20986;&#20102;&#25991;&#26412;&#22788;&#29702;&#30340;&#33539;&#22260;&#65292;&#39044;&#31034;&#30528;&#19968;&#20010;&#26032;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#22312;&#36825;&#20010;&#26102;&#20195;&#65292;LLMs&#34987;&#35774;&#24819;&#20026;&#33021;&#22815;&#22312;&#22797;&#26434;&#29616;&#23454;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#36890;&#29992;&#35821;&#35328;&#20195;&#29702;&#12290;&#36825;&#20123;&#29615;&#22659;&#36890;&#24120;&#38750;&#24120;&#24191;&#38420;&#65292;&#20351;&#24471;LLM&#19981;&#21487;&#33021;&#22312;&#20854;&#30701;&#26399;&#35760;&#24518;&#20013;&#22788;&#29702;&#23427;&#20204;&#12290;&#21463;&#26368;&#36817;&#20851;&#20110;&#36890;&#36807;&#24037;&#20855;&#25193;&#23637;LLMs&#33021;&#21147;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#24037;&#20855;&#22312;&#22686;&#24378;LLMs&#22788;&#29702;&#36825;&#31181;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23450;&#21046;&#24037;&#20855;&#65292;&#20197;&#21327;&#21161;&#22312;&#36825;&#20123;&#24222;&#22823;&#29615;&#22659;&#20013;&#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#12290;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#20214;&#23618;&#65292;&#20351;LLM&#20813;&#21463;&#29615;&#22659;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#22797;&#26434;&#29615;&#22659;--&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#21644;&#25968;&#25454;&#24211;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20351;&#29992;&#24037;&#20855;&#22686;&#24378;&#35821;&#35328;&#20195;&#29702;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14672v1 Announce Type: cross  Abstract: The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist language agents capable of operating within complex real-world environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, this paper investigates the intriguing potential of tools to augment LLMs in handling such complexity. To this end, we design customized tools to aid in the proactive exploration within these massive environments. Such tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments -- knowledge bases (KBs) and databases -- we demonstrate the significant potential of augmenting language agents with tools in complex environments. N
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;ConceptMath&#65292;&#19968;&#31181;&#21452;&#35821;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2402.14660</link><description>&lt;p&gt;
ConceptMath&#65306;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#21452;&#35821;&#27010;&#24565;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14660
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;ConceptMath&#65292;&#19968;&#31181;&#21452;&#35821;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ConceptMath&#65292;&#36825;&#26159;&#19968;&#20010;&#21452;&#35821;&#65288;&#33521;&#35821;&#21644;&#20013;&#25991;&#65289;&#65292;&#32454;&#31890;&#24230;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#35780;&#20272;&#19968;&#33324;&#25968;&#23398;&#25512;&#29702;&#30340;&#20256;&#32479;&#22522;&#20934;&#19981;&#21516;&#65292;ConceptMath&#23558;&#25968;&#23398;&#38382;&#39064;&#31995;&#32479;&#22320;&#32452;&#32455;&#22312;&#25968;&#23398;&#27010;&#24565;&#30340;&#23618;&#27425;&#32467;&#26500;&#19979;&#65292;&#20174;&#32780;&#21487;&#20197;&#20197;&#27010;&#24565;&#20026;&#21333;&#20301;&#20934;&#30830;&#24615;&#35780;&#20272;&#25968;&#23398;&#25512;&#29702;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;ConceptMath&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24191;&#27867;&#33539;&#22260;&#30340;LLMs&#65292;&#24182;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;LLMs&#23613;&#31649;&#22312;&#20256;&#32479;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#39640;&#24179;&#22343;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#20005;&#37325;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24494;&#35843;&#31574;&#30053;&#26469;&#22686;&#24378;&#29616;&#26377;LLMs&#30340;&#24369;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24076;&#26395;ConceptMath&#33021;&#22815;&#25351;&#23548;&#24320;&#21457;&#32773;&#29702;&#35299;&#32454;&#33268;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14660v1 Announce Type: cross  Abstract: This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grai
&lt;/p&gt;</description></item><item><title>OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.14658</link><description>&lt;p&gt;
OpenCodeInterpreter&#65306;&#38598;&#25104;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14658
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24341;&#20837;&#26174;&#33879;&#25512;&#21160;&#20102;&#20195;&#30721;&#29983;&#25104;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#31867;&#20284;GPT-4 Code Interpreter&#36825;&#26679;&#30340;&#39640;&#32423;&#31995;&#32479;&#30340;&#25191;&#34892;&#33021;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenCodeInterpreter&#65292;&#36825;&#26159;&#19968;&#26063;&#26088;&#22312;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#36845;&#20195;&#32454;&#21270;&#20195;&#30721;&#30340;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#12290;&#36890;&#36807;Code-Feedback&#25903;&#25345;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#25191;&#34892;&#21644;&#20154;&#31867;&#21453;&#39304;&#65292;&#29992;&#20110;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#12290;&#25105;&#20204;&#23545;OpenCodeInterpreter&#22312;&#35832;&#22914;HumanEval&#12289;MBPP&#20197;&#21450;&#23427;&#20204;&#26469;&#33258;EvalPlus&#30340;&#22686;&#24378;&#29256;&#26412;&#31561;&#20851;&#38190;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#23454;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenCodeInterpreter-33B&#22312;HumanEval&#21644;MBPP&#30340;&#24179;&#22343;&#20540;&#65288;&#20197;&#21450;&#20854;&#22686;&#24378;&#29256;&#26412;&#65289;&#19978;&#21462;&#24471;&#20102;83.2&#65288;76.4&#65289;&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;GPT-4&#30340;84.2&#65288;76.2&#65289;&#32039;&#23494;&#21305;&#25932;&#65292;&#24182;&#19988;&#36890;&#36807;&#21512;&#25104;hum
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14658v1 Announce Type: cross  Abstract: The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized hum
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#22120;NeuScraper&#21487;&#20197;&#20174;&#32593;&#39029;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#36229;&#36807;20%&#30340;&#25913;&#36827;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36136;&#37327;</title><link>https://arxiv.org/abs/2402.14652</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#36827;&#34892;&#26356;&#28165;&#27905;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Cleaner Pretraining Corpus Curation with Neural Web Scraping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14652
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#22120;NeuScraper&#21487;&#20197;&#20174;&#32593;&#39029;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#36229;&#36807;20%&#30340;&#25913;&#36827;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14652v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#32593;&#32476;&#21253;&#21547;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#20016;&#23500;&#20449;&#24687;&#65292;&#20197;&#28385;&#36275;&#20154;&#31867;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#36890;&#36807;&#32454;&#33268;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#25972;&#29702;&#65292;&#32593;&#39029;&#21487;&#20197;&#34987;&#29992;&#20316;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;&#25968;&#25454;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#38754;&#23545;&#19981;&#26029;&#38761;&#26032;&#21644;&#22797;&#26434;&#30340;&#32593;&#39029;&#29305;&#24615;&#65292;&#22522;&#20110;&#35268;&#21017;/&#29305;&#24449;&#30340;&#32593;&#32476;&#25235;&#21462;&#22120;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#22120;&#65288;NeuScraper&#65289;&#65292;&#24110;&#21161;&#20174;&#32593;&#39029;&#20013;&#25552;&#21462;&#20027;&#35201;&#21644;&#24178;&#20928;&#30340;&#25991;&#26412;&#20869;&#23481;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NeuScraper&#36229;&#36234;&#20102;&#22522;&#20934;&#25235;&#21462;&#22120;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;20%&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21462;&#26356;&#39640;&#36136;&#37327;&#25968;&#25454;&#20197;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/OpenMatch/NeuScraper&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14652v1 Announce Type: new  Abstract: The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https://github.com/OpenMatch/NeuScraper.
&lt;/p&gt;</description></item><item><title>RoboScript&#26159;&#19968;&#20010;&#26088;&#22312;&#22635;&#34917;&#8220;&#29702;&#24819;&#21040;&#23454;&#38469;&#8221;&#24046;&#36317;&#30340;&#24179;&#21488;&#65292;&#25552;&#20379;&#21487;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27969;&#27700;&#32447;&#65292;&#24182;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.14623</link><description>&lt;p&gt;
RoboScript: &#36328;&#36234;&#30495;&#23454;&#21644;&#20223;&#30495;&#30340;&#33258;&#30001;&#24418;&#24335;&#25805;&#20316;&#20219;&#21153;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14623
&lt;/p&gt;
&lt;p&gt;
RoboScript&#26159;&#19968;&#20010;&#26088;&#22312;&#22635;&#34917;&#8220;&#29702;&#24819;&#21040;&#23454;&#38469;&#8221;&#24046;&#36317;&#30340;&#24179;&#21488;&#65292;&#25552;&#20379;&#21487;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27969;&#27700;&#32447;&#65292;&#24182;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20195;&#30721;&#29983;&#25104;&#22312;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#25110;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24120;&#35782;&#25512;&#29702;&#21644;&#20219;&#21153;&#35268;&#21010;&#33021;&#21147;&#19978;&#65292;&#23545;&#20110;&#29983;&#25104;&#20195;&#30721;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#33258;&#20027;&#26426;&#22120;&#20154;&#31995;&#32479;&#22522;&#26412;&#32452;&#20214;&#65288;&#21253;&#25324;&#26426;&#22120;&#20154;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#65289;&#19978;&#30340;&#37096;&#32626;&#24615;&#20184;&#20986;&#30340;&#21162;&#21147;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#8220;&#29702;&#24819;&#21040;&#23454;&#38469;&#8221;&#30340;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;RoboScript&#65292;&#19968;&#20010;&#24179;&#21488;&#65292;&#29992;&#20110;1&#65289;&#30001;&#20195;&#30721;&#29983;&#25104;&#39537;&#21160;&#30340;&#21487;&#37096;&#32626;&#26426;&#22120;&#20154;&#25805;&#20316;&#27969;&#27700;&#32447;&#65307;&#21644;2&#65289;&#33258;&#30001;&#24418;&#24335;&#33258;&#28982;&#35821;&#35328;&#20013;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#12290;RoboScript&#24179;&#21488;&#36890;&#36807;&#24378;&#35843;&#19982;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#32479;&#19968;&#25509;&#21475;&#35299;&#20915;&#20102;&#36825;&#19968;&#24046;&#36317;&#65292;&#22522;&#20110;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#65288;ROS&#65289;&#30340;&#25277;&#35937;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14623v1 Announce Type: cross  Abstract: Rapid progress in high-level task planning and code generation for open-world robot manipulation has been witnessed in Embodied AI. However, previous studies put much effort into general common sense reasoning and task planning capabilities of large-scale language or multi-modal models, relatively little effort on ensuring the deployability of generated code on real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control. To bridge this ``ideal-to-real'' gap, this paper presents \textbf{RobotScript}, a platform for 1) a deployable robot manipulation pipeline powered by code generation; and 2) a code generation benchmark for robot manipulation tasks in free-form natural language. The RobotScript platform addresses this gap by emphasizing the unified interface with both simulation and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31361;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#35760;&#24405;&#21644;&#20808;&#36827;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38761;&#26032;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14622</link><description>&lt;p&gt;
&#20174;&#20851;&#38190;&#35789;&#21040;&#32467;&#26500;&#21270;&#25688;&#35201;: &#31934;&#31616;&#23398;&#26415;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31361;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#35760;&#24405;&#21644;&#20808;&#36827;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38761;&#26032;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#30701;&#25991;&#24378;&#35843;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#26085;&#30410;&#37325;&#35201;&#65292;&#25351;&#20986;&#20256;&#32479;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#24341;&#25806;&#30001;&#20110;&#20986;&#29256;&#29289;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#32467;&#26500;&#21270;&#35760;&#24405;&#65292;&#25903;&#25345;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#20202;&#34920;&#26495;&#65292;&#20197;&#24443;&#24213;&#25913;&#21464;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#65292;&#21462;&#20195;&#20256;&#32479;&#30340;&#25991;&#26412;&#23494;&#38598;&#22411;&#26041;&#27861;&#12290;&#36825;&#19968;&#24895;&#26223;&#36890;&#36807;&#19968;&#20010;&#20197;&#8220;&#20256;&#26579;&#30149;&#30340;&#32321;&#27542;&#25968;&#20272;&#35745;&#8221;&#30740;&#31350;&#20027;&#39064;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#39564;&#35777;&#24471;&#20197;&#20307;&#29616;&#65292;&#20351;&#29992;&#32463;&#36807;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33258;&#21160;&#21019;&#24314;&#32467;&#26500;&#21270;&#35760;&#24405;&#20197;&#22635;&#20805;&#19968;&#20010;&#36229;&#36234;&#20851;&#38190;&#35789;&#30340;&#21518;&#31471;&#25968;&#25454;&#24211;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#22312;https://orkg.org/usecases/r0-estimates &#19978;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14622v1 Announce Type: cross  Abstract: This short paper highlights the growing importance of information retrieval (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications. The proposed solution involves structured records, underpinning advanced information technology (IT) tools, including visualization dashboards, to revolutionize how researchers access and filter articles, replacing the traditional text-heavy approach. This vision is exemplified through a proof of concept centered on the ``reproductive number estimate of infectious diseases'' research theme, using a fine-tuned large language model (LLM) to automate the creation of structured records to populate a backend database that now goes beyond keywords. The result is a next-generation IR method accessible at https://orkg.org/usecases/r0-estimates.
&lt;/p&gt;</description></item><item><title>&#21333;&#35789;&#20998;&#21106;&#23545;&#19978;&#19979;&#25991;&#21270;&#21333;&#35789;&#34920;&#31034;&#30340;&#35821;&#20041;&#20869;&#23481;&#26377;&#24433;&#21709;&#65292;&#30740;&#31350;&#21457;&#29616;&#34987;&#20998;&#21106;&#30340;&#21333;&#35789;&#34920;&#31034;&#36136;&#37327;&#36890;&#24120;&#27604;&#24050;&#30693;&#21333;&#35789;&#24046;&#65292;&#20294;&#30456;&#20284;&#24615;&#20540;&#38656;&#35880;&#24910;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.14616</link><description>&lt;p&gt;
&#21333;&#35789;&#20998;&#35789;&#23545;&#19978;&#19979;&#25991;&#21270;&#21333;&#35789;&#34920;&#31034;&#30340;&#35821;&#20041;&#20869;&#23481;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14616
&lt;/p&gt;
&lt;p&gt;
&#21333;&#35789;&#20998;&#21106;&#23545;&#19978;&#19979;&#25991;&#21270;&#21333;&#35789;&#34920;&#31034;&#30340;&#35821;&#20041;&#20869;&#23481;&#26377;&#24433;&#21709;&#65292;&#30740;&#31350;&#21457;&#29616;&#34987;&#20998;&#21106;&#30340;&#21333;&#35789;&#34920;&#31034;&#36136;&#37327;&#36890;&#24120;&#27604;&#24050;&#30693;&#21333;&#35789;&#24046;&#65292;&#20294;&#30456;&#20284;&#24615;&#20540;&#38656;&#35880;&#24910;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#19978;&#19979;&#25991;&#21270;&#30340;&#21333;&#35789;&#34920;&#31034;&#26102;&#65292;&#38656;&#35201;&#20915;&#23450;&#22914;&#20309;&#20026;&#34987;&#20998;&#25104;&#23376;&#35789;&#30340;&#23383;&#20856;&#22806;&#65288;OOV&#65289;&#21333;&#35789;&#33719;&#24471;&#34920;&#31034;&#12290;&#20160;&#20040;&#26159;&#34920;&#31034;&#36825;&#20123;&#21333;&#35789;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#36136;&#37327;&#26159;&#21542;&#27604;&#35789;&#20856;&#20869;&#21333;&#35789;&#30340;&#34920;&#31034;&#36136;&#37327;&#24046;&#65311;&#25105;&#20204;&#23545;&#26469;&#33258;&#19981;&#21516;&#27169;&#22411;&#30340;&#23884;&#20837;&#36827;&#34892;&#20102;&#20869;&#22312;&#35780;&#20272;&#65292;&#28041;&#21450;&#28041;&#21450;OOV&#21333;&#35789;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;&#65292;&#34920;&#31034;&#34987;&#20998;&#21106;&#30340;&#21333;&#35789;&#30340;&#36136;&#37327;&#36890;&#24120;&#65292;&#20294;&#24182;&#38750;&#22987;&#32456;&#27604;&#24050;&#30693;&#21333;&#35789;&#30340;&#23884;&#20837;&#30340;&#36136;&#37327;&#24046;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#20540;&#24517;&#39035;&#35880;&#24910;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14616v1 Announce Type: new  Abstract: When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words? We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#12298;Tokenization and the Noiseless Channel&#12299;&#25552;&#20986;&#30340;&#20351;&#29992;R&#233;nyi&#25928;&#29575;&#20316;&#20026;&#20998;&#35789;&#22120;&#35780;&#20272;&#26426;&#21046;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25551;&#36848;&#20102;&#20004;&#20010;BPE&#20998;&#35789;&#30340;&#21453;&#20363;&#65292;&#23637;&#31034;&#20102;R&#233;nyi&#25928;&#29575;&#26080;&#27861;&#25429;&#25417;&#21040;&#25152;&#26377;&#20248;&#31168;&#20998;&#35789;&#26041;&#26696;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.14614</link><description>&lt;p&gt;
&#12298;Tokenization and the Noiseless Channel&#12299;&#30340;&#20004;&#20010;&#21453;&#20363;
&lt;/p&gt;
&lt;p&gt;
Two Counterexamples to \textit{Tokenization and the Noiseless Channel}
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14614
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#12298;Tokenization and the Noiseless Channel&#12299;&#25552;&#20986;&#30340;&#20351;&#29992;R&#233;nyi&#25928;&#29575;&#20316;&#20026;&#20998;&#35789;&#22120;&#35780;&#20272;&#26426;&#21046;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25551;&#36848;&#20102;&#20004;&#20010;BPE&#20998;&#35789;&#30340;&#21453;&#20363;&#65292;&#23637;&#31034;&#20102;R&#233;nyi&#25928;&#29575;&#26080;&#27861;&#25429;&#25417;&#21040;&#25152;&#26377;&#20248;&#31168;&#20998;&#35789;&#26041;&#26696;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#12298;Tokenization and the Noiseless Channel&#12299;&#20013;&#65292;&#24314;&#35758;&#20351;&#29992;R&#233;nyi&#25928;&#29575;&#20316;&#20026;&#35780;&#20272;&#20998;&#35789;&#22120;&#30340;&#22266;&#26377;&#26426;&#21046;: &#23545;&#20110;NLP&#20219;&#21153;&#65292;&#24212;&#36873;&#25321;&#23548;&#33268;unigram&#20998;&#24067;R&#233;nyi&#25928;&#29575;&#26368;&#39640;&#30340;&#20998;&#35789;&#22120;&#12290;&#22240;&#27492;&#65292;R&#233;nyi&#25928;&#29575;&#34987;&#35270;&#20026;&#19979;&#28216;&#24615;&#33021;&#30340;&#39044;&#27979;&#22120;&#65288;&#20363;&#22914;&#65292;&#29992;&#20110;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#30340;BLEU&#65289;&#65292;&#32780;&#26080;&#38656;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#20998;&#35789;&#22120;&#30340;&#22810;&#20010;&#27169;&#22411;&#36825;&#19968;&#26114;&#36149;&#30340;&#27493;&#39588;&#12290;&#23613;&#31649;&#26377;&#29992;&#65292;&#20294;&#36825;&#19968;&#24230;&#37327;&#26631;&#20934;&#30340;&#39044;&#27979;&#33021;&#21147;&#24182;&#19981;&#23436;&#32654;&#65292;&#20316;&#32773;&#25351;&#20986;&#26377;&#20854;&#20182;&#20248;&#31168;&#20998;&#35789;&#26041;&#26696;&#30340;&#38468;&#21152;&#29305;&#36136;R&#233;nyi&#25928;&#29575;&#26412;&#36523;&#26080;&#27861;&#25429;&#25417;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#31181;BPE&#20998;&#35789;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#22312;&#22686;&#21152;R&#233;nyi&#25928;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#21453;&#20363;&#25581;&#31034;&#20102;R&#233;nyi&#25928;&#29575;&#20316;&#20026;&#22266;&#26377;&#20998;&#35789;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#22833;&#36133;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14614v1 Announce Type: new  Abstract: In \textit{Tokenization and the Noiseless Channel} \cite{zouhar-etal-2023-tokenization}, R\'enyi efficiency is suggested as an intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer which leads to the highest R\'enyi efficiency of the unigram distribution should be chosen. The R\'enyi efficiency is thus treated as a predictor of downstream performance (e.g., predicting BLEU for a machine translation task), without the expensive step of training multiple models with different tokenizers. Although useful, the predictive power of this metric is not perfect, and the authors note there are additional qualities of a good tokenization scheme that R\'enyi efficiency alone cannot capture.   We describe two variants of BPE tokenization which can arbitrarily increase R\'enyi efficiency while decreasing the downstream model performance. These counterexamples expose cases where R\'enyi efficiency fails as an intrinsic tokenizati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#26469;&#33258;&#21160;&#35780;&#20272;&#36741;&#23548;&#21592;&#20351;&#29992;&#31038;&#20132;&#24773;&#24863;&#36741;&#23548;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#36741;&#23548;&#23454;&#36341;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.14594</link><description>&lt;p&gt;
&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25913;&#36827;&#36741;&#23548;&#23454;&#36341;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Improving Assessment of Tutoring Practices using Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14594
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#26469;&#33258;&#21160;&#35780;&#20272;&#36741;&#23548;&#21592;&#20351;&#29992;&#31038;&#20132;&#24773;&#24863;&#36741;&#23548;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#36741;&#23548;&#23454;&#36341;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#23545;&#19968;&#36741;&#23548;&#26159;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#30340;&#26377;&#25928;&#25945;&#23398;&#26041;&#27861;&#65292;&#28982;&#32780;&#20854;&#25928;&#21147;&#21462;&#20915;&#20110;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#12290;&#26032;&#25163;&#25968;&#23398;&#36741;&#23548;&#21592;&#36890;&#24120;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#20869;&#23481;&#30340;&#25351;&#23548;&#65292;&#24573;&#35270;&#31038;&#20132;&#24773;&#24863;&#23398;&#20064;&#31561;&#26041;&#38754;&#12290;&#31038;&#20132;&#24773;&#24863;&#23398;&#20064;&#20419;&#36827;&#20102;&#20844;&#24179;&#21644;&#21253;&#23481;&#24615;&#65292;&#24182;&#22521;&#20859;&#19982;&#23398;&#29983;&#30340;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#23398;&#29983;&#25972;&#20307;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20934;&#30830;&#26377;&#25928;&#22320;&#35780;&#20272;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#21487;&#20197;&#25512;&#21160;&#23450;&#21046;&#30340;&#36741;&#23548;&#21592;&#22521;&#35757;&#35745;&#21010;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#36741;&#23548;&#20013;&#35780;&#20272;&#26032;&#25163;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#21442;&#19982;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#21021;&#27493;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#33258;&#21160;&#35780;&#20272;&#36741;&#23548;&#21592;&#20351;&#29992;&#31038;&#20132;&#24773;&#24863;&#36741;&#23548;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25253;&#21578;&#20102;&#36130;&#21153;&#32500;&#24230;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14594v1 Announce Type: cross  Abstract: One-on-one tutoring is an effective instructional method for enhancing learning, yet its efficacy hinges on tutor competencies. Novice math tutors often prioritize content-specific guidance, neglecting aspects such as social-emotional learning. Social-emotional learning promotes equity and inclusion and nurturing relationships with students, which is crucial for holistic student development. Assessing the competencies of tutors accurately and efficiently can drive the development of tailored tutor training programs. However, evaluating novice tutor ability during real-time tutoring remains challenging as it typically requires experts-in-the-loop. To address this challenge, this preliminary study aims to harness Generative Pre-trained Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess tutors' ability of using social-emotional tutoring strategies. Moreover, this study also reports on the financial dimensions an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Google&#24191;&#21578;&#20013;&#36827;&#34892;&#20869;&#23481;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#23457;&#26680;&#20195;&#34920;&#24615;&#24191;&#21578;&#24182;&#23558;&#20915;&#31574;&#20256;&#25773;&#22238;&#20854;&#32676;&#38598;&#65292;&#23558;&#23457;&#26680;&#25968;&#30446;&#20943;&#23569;&#20102;3&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;2&#20493;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14590</link><description>&lt;p&gt;
&#25193;&#23637;LLM&#23457;&#26680;&#20197;&#36827;&#34892;Google&#24191;&#21578;&#20869;&#23481;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Scaling Up LLM Reviews for Google Ads Content Moderation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Google&#24191;&#21578;&#20013;&#36827;&#34892;&#20869;&#23481;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#23457;&#26680;&#20195;&#34920;&#24615;&#24191;&#21578;&#24182;&#23558;&#20915;&#31574;&#20256;&#25773;&#22238;&#20854;&#32676;&#38598;&#65292;&#23558;&#23457;&#26680;&#25968;&#30446;&#20943;&#23569;&#20102;3&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;2&#20493;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#20869;&#23481;&#31649;&#29702;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#25104;&#26412;&#21644;&#24310;&#36831;&#20351;&#23427;&#20204;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#22914;Google Ads&#23384;&#20648;&#24211;&#65289;&#19978;&#30340;&#20020;&#26102;&#20351;&#29992;&#25104;&#26412;&#36807;&#39640;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#23637;LLM&#23457;&#26680;&#20197;&#22312;Google Ads&#20013;&#36827;&#34892;&#20869;&#23481;&#31649;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#36890;&#36807;&#36807;&#28388;&#21644;&#37325;&#22797;&#39033;&#21024;&#38500;&#26469;&#36873;&#25321;&#20505;&#36873;&#39033;&#65292;&#24182;&#20026;&#27492;&#21019;&#24314;&#24191;&#21578;&#32676;&#38598;&#65292;&#25105;&#20204;&#36873;&#25321;&#27599;&#20010;&#32676;&#38598;&#30340;&#19968;&#20010;&#20195;&#34920;&#24615;&#24191;&#21578;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#20165;&#23457;&#26680;&#20195;&#34920;&#24615;&#24191;&#21578;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20195;&#34920;&#24615;&#24191;&#21578;&#30340;LLM&#20915;&#31574;&#20256;&#25773;&#22238;&#23427;&#20204;&#30340;&#32676;&#38598;&#12290;&#35813;&#26041;&#27861;&#23558;&#23457;&#26680;&#25968;&#30446;&#20943;&#23569;&#20102;3&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#65292;&#21516;&#26102;&#19982;&#22522;&#32447;&#38750;LLM&#27169;&#22411;&#30456;&#27604;&#23454;&#29616;&#20102;2&#20493;&#30340;&#21484;&#22238;&#29575;&#12290;&#35813;&#26041;&#27861;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#32858;&#31867;&#21644;&#26631;&#31614;&#20256;&#25773;&#20013;&#20351;&#29992;&#30340;&#34920;&#31034;; &#25105;&#20204;&#21457;&#29616;&#20132;&#21449;&#27169;&#24577;&#30456;&#20284;&#24615;&#34920;&#31034;&#20135;&#29983;&#27604;&#21333;&#19968;&#27169;&#24577;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14590v1 Announce Type: cross  Abstract: Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;Transformer&#22312;&#31185;&#23398;&#22270;&#34920;&#20013;&#36827;&#34892;&#25991;&#26412;&#35282;&#33394;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;LayoutLMv3&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;UDOP&#65292;&#26368;&#39640;&#36798;&#21040;&#20102;82.87&#30340;F1-macro&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14579</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;Transformer&#22312;&#31185;&#23398;&#22270;&#34920;&#20013;&#36827;&#34892;&#25991;&#26412;&#35282;&#33394;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text Role Classification in Scientific Charts Using Multimodal Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14579
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;Transformer&#22312;&#31185;&#23398;&#22270;&#34920;&#20013;&#36827;&#34892;&#25991;&#26412;&#35282;&#33394;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;LayoutLMv3&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;UDOP&#65292;&#26368;&#39640;&#36798;&#21040;&#20102;82.87&#30340;F1-macro&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#35282;&#33394;&#20998;&#31867;&#28041;&#21450;&#23545;&#31185;&#23398;&#22270;&#34920;&#20013;&#30340;&#25991;&#26412;&#20803;&#32032;&#30340;&#35821;&#20041;&#35282;&#33394;&#36827;&#34892;&#20998;&#31867;&#12290;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#22270;&#34920;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#27169;&#22411;LayoutLMv3&#21644;UDOP&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20123;Transformer&#21033;&#29992;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#19977;&#31181;&#27169;&#24577;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#26041;&#27861;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#22270;&#34920;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LayoutLMv3&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#22343;&#20248;&#20110;UDOP&#12290;LayoutLMv3&#22312;ICPR22&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;82.87&#30340;&#26368;&#39640;F1-macro&#20998;&#25968;&#65292;&#36229;&#36807;&#20102;ICPR22 CHART-Infographics&#25361;&#25112;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22312;&#19968;&#20010;&#21512;&#25104;&#30340;&#22024;&#26434;&#25968;&#25454;&#38598;ICPR22-N&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#19977;&#20010;&#22270;&#34920;&#25968;&#25454;&#38598;CHIME-R&#12289;DeGruyter&#21644;EconBiz&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#25105;&#20204;&#20026;&#36825;&#20123;&#25968;&#25454;&#38598;&#28155;&#21152;&#20102;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14579v1 Announce Type: cross  Abstract: Text role classification involves classifying the semantic role of textual elements within scientific charts. For this task, we propose to finetune two pretrained multimodal document layout analysis models, LayoutLMv3 and UDOP, on chart datasets. The transformers utilize the three modalities of text, image, and layout as input. We further investigate whether data augmentation and balancing methods help the performance of the models. The models are evaluated on various chart datasets, and results show that LayoutLMv3 outperforms UDOP in all experiments. LayoutLMv3 achieves the highest F1-macro score of 82.87 on the ICPR22 test dataset, beating the best-performing model from the ICPR22 CHART-Infographics challenge. Moreover, the robustness of the models is tested on a synthetic noisy dataset ICPR22-N. Finally, the generalizability of the models is evaluated on three chart datasets, CHIME-R, DeGruyter, and EconBiz, for which we added labe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;$LLM-DA$&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#65292;&#22312;&#19978;&#19979;&#25991;&#21644;&#23454;&#20307;&#23618;&#38754;&#22686;&#24378;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.14568</link><description>&lt;p&gt;
LLM-DA: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14568
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;$LLM-DA$&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#65292;&#22312;&#19978;&#19979;&#25991;&#21644;&#23454;&#20307;&#23618;&#38754;&#22686;&#24378;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#23436;&#20840;&#20196;&#20154;&#28385;&#24847;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20986;&#33394;&#30340;&#37325;&#20889;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#20026;&#25913;&#36827;&#36825;&#20123;&#20219;&#21153;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$LLM-DA$&#65292;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#39062;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#25439;&#23475;&#35821;&#20041;&#23436;&#25972;&#24615;&#24182;&#35299;&#20915;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;NER&#20219;&#21153;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#22312;&#19978;&#19979;&#25991;&#21644;&#23454;&#20307;&#23618;&#38754;&#19978;&#22686;&#24378;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;14&#31181;&#19978;&#19979;&#25991;&#37325;&#20889;&#31574;&#30053;&#65292;&#35774;&#35745;&#30456;&#21516;&#31867;&#22411;&#30340;&#23454;&#20307;&#26367;&#25442;&#65292;&#24182;&#24341;&#20837;&#22122;&#22768;&#27880;&#20837;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;NER&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14568v1 Announce Type: new  Abstract: Despite the impressive capabilities of large language models (LLMs), their performance on information extraction tasks is still not entirely satisfactory. However, their remarkable rewriting capabilities and extensive world knowledge offer valuable insights to improve these tasks. In this paper, we propose $LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot NER task. To overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in LLM-generated text, we leverage the distinctive characteristics of the NER task by augmenting the original data at both the contextual and entity levels. Our approach involves employing 14 contextual rewriting strategies, designing entity replacements of the same type, and incorporating noise injection to enhance robustness. Extensive experiments demonstrate the effectiveness of our approach in enhancing NER mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#21033;&#29992;LLMs&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#65292;&#24182;&#36890;&#36807;&#23545;&#34892;&#19994;&#20174;&#19994;&#32773;&#35843;&#26597;&#20197;&#21450;&#23457;&#26597;&#22823;&#37327;&#34892;&#19994;&#35770;&#25991;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.14558</link><description>&lt;p&gt;
&#20855;&#26377;&#24037;&#19994;&#35270;&#35282;&#30340;LLMs&#65306;&#25581;&#31034;&#25361;&#25112;&#19982;&#21069;&#26223;--&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#21033;&#29992;LLMs&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#65292;&#24182;&#36890;&#36807;&#23545;&#34892;&#19994;&#20174;&#19994;&#32773;&#35843;&#26597;&#20197;&#21450;&#23457;&#26597;&#22823;&#37327;&#34892;&#19994;&#35770;&#25991;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#25512;&#21160;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#30340;&#31192;&#23494;&#27494;&#22120;&#65292;&#23637;&#31034;&#20986;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#24615;&#12290;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24773;&#24863;&#20998;&#26512;&#21040;&#20869;&#23481;&#29983;&#25104;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#23427;&#20204;&#26080;&#19982;&#20262;&#27604;&#30340;&#36866;&#24212;&#24615;&#20419;&#36827;&#20102;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;LLMs&#24102;&#26469;&#30340;&#36825;&#31181;&#36716;&#21464;&#24378;&#35843;&#20102;&#25506;&#32034;&#19982;&#21033;&#29992;&#20013;&#30340;&#22256;&#38590;&#21644;&#22686;&#24378;&#26426;&#20250;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25581;&#31034;&#21644;&#35780;&#20272;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#21033;&#29992;LLMs&#25152;&#38754;&#20020;&#30340;&#38556;&#30861;&#21644;&#26426;&#20250;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#28041;&#21450;&#19968;&#32452;&#34892;&#19994;&#20174;&#19994;&#32773;&#30340;&#35843;&#26597;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#26681;&#25454;&#25910;&#38598;&#21040;&#30340;&#35265;&#35299;&#23457;&#26597;&#20102;68&#31687;&#34892;&#19994;&#35770;&#25991;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14558v1 Announce Type: new  Abstract: Large language models (LLMs) have become the secret ingredient driving numerous industrial applications, showcasing their remarkable versatility across a diverse spectrum of tasks. From natural language processing and sentiment analysis to content generation and personalized recommendations, their unparalleled adaptability has facilitated widespread adoption across industries. This transformative shift driven by LLMs underscores the need to explore the underlying associated challenges and avenues for enhancement in their utilization. In this paper, our objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging LLMs within an industrial context. To this end, we conduct a survey involving a group of industry practitioners, develop four research questions derived from the insights gathered, and examine 68 industry papers to address these questions and derive meaningful conclusions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#38382;&#39064;&#65292;&#21457;&#29616;&#36890;&#36807;&#27169;&#22411;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#20316;&#20986;&#36866;&#24403;&#30340;EOS&#20915;&#31574;&#65292;&#21487;&#20197;&#20943;&#23569;&#25345;&#32493;&#36755;&#20986;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#32531;&#35299;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14545</link><description>&lt;p&gt;
&#20943;&#23569;&#26159;&#26377;&#30410;&#30340;&#65306;&#20174;EOS&#20915;&#31574;&#35282;&#24230;&#32531;&#35299;&#22810;&#27169;&#24577;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#38382;&#39064;&#65292;&#21457;&#29616;&#36890;&#36807;&#27169;&#22411;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#20316;&#20986;&#36866;&#24403;&#30340;EOS&#20915;&#31574;&#65292;&#21487;&#20197;&#20943;&#23569;&#25345;&#32493;&#36755;&#20986;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#32463;&#24120;&#36973;&#21463;&#22810;&#27169;&#24577;&#24187;&#35273;&#65292;&#21363;&#23427;&#20204;&#21487;&#33021;&#21019;&#36896;&#20986;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#24182;&#19981;&#23384;&#22312;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26032;&#35282;&#24230;&#65306;&#36807;&#20110;&#35814;&#32454;&#30340;&#35757;&#32451;&#25968;&#25454;&#22952;&#30861;&#20102;&#27169;&#22411;&#21450;&#26102;&#32456;&#27490;&#29983;&#25104;&#65292;&#23548;&#33268;&#36229;&#20986;&#35270;&#35273;&#24863;&#30693;&#38480;&#21046;&#30340;&#25345;&#32493;&#36755;&#20986;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;EOS&#65288;&#29305;&#27530;&#30340;&#21477;&#23376;&#32467;&#23614;&#26631;&#35760;&#65289;&#26469;&#20915;&#23450;&#32456;&#27490;&#29983;&#25104;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#25972;&#20010;&#24207;&#21015;&#30340;&#23436;&#25972;&#24615;&#12290;&#36825;&#19968;&#35266;&#23519;&#34920;&#26126;&#65292;&#27169;&#22411;&#20855;&#26377;&#22522;&#20110;&#20854;&#35270;&#35273;&#24863;&#30693;&#36827;&#34892;&#36866;&#24403;EOS&#20915;&#31574;&#30340;&#28508;&#21147;&#65292;&#20197;&#36991;&#20813;&#36807;&#38271;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#31181;&#28508;&#21147;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;&#32531;&#35299;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#23398;&#20064;&#24120;&#35268;&#25351;&#31034;&#23454;&#29616;&#27169;&#22411;&#20943;&#23569;&#24187;&#35273;&#30340;&#35757;&#32451;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14545v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) often suffer from multimodal hallucinations, wherein they may create content that is not present in the visual inputs. In this paper, we explore a new angle of this issue: overly detailed training data hinders the model's ability to timely terminate generation, leading to continued outputs beyond visual perception limits. By investigating how the model decides to terminate generation with EOS, the special end-of-sentence token, we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image. This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs. To take advantage of such potential, we explore two methods to mitigate multimodal hallucinations: a training objective that enables the model to reduce hallucinations by learning from regular instruc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21518;&#38376;&#35843;&#25972;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#24320;&#39046;&#22495;&#29305;&#23450;&#21644;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#24212;&#23545;&#39046;&#22495;&#36716;&#31227;</title><link>https://arxiv.org/abs/2402.14536</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#35843;&#25972;&#23454;&#29616;&#36328;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21518;&#38376;&#35843;&#25972;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#24320;&#39046;&#22495;&#29305;&#23450;&#21644;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#24212;&#23545;&#39046;&#22495;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36328;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#65292;&#23558;&#30693;&#35782;&#20174;&#28304;&#39046;&#22495;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#26159;&#22522;&#20110;&#30446;&#26631;&#65288;&#27979;&#35797;&#65289;&#39046;&#22495;&#24050;&#30693;&#30340;&#20551;&#35774;&#25552;&#20986;&#30340;&#65292;&#23548;&#33268;&#23427;&#20204;&#22312;&#26410;&#30693;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#65292;&#32780;&#22312;&#23454;&#36341;&#20013;&#36825;&#31181;&#24773;&#20917;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#12290;&#26412;&#25991;&#20851;&#27880;&#36328;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21518;&#38376;&#35843;&#25972;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#35299;&#24320;&#22312;&#35299;&#20915;&#39046;&#22495;&#36716;&#31227;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#21644;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#20102;&#36328;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#20197;&#24314;&#27169;&#19981;&#21516;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#19981;&#21464;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#21518;&#38376;&#35843;&#25972;&#21435;&#38500;&#39046;&#22495;&#28151;&#26434;&#22240;&#32032;&#65288;&#22914;&#39046;&#22495;&#30693;&#35782;&#65289;&#30340;&#24433;&#21709;&#12290;&#19968;&#31995;&#21015;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14536v1 Announce Type: new  Abstract: Domain adaption has been widely adapted for cross-domain sentiment analysis to transfer knowledge from the source domain to the target domain. Whereas, most methods are proposed under the assumption that the target (test) domain is known, making them fail to generalize well on unknown test data that is not always available in practice. In this paper, we focus on the problem of domain generalization for cross-domain sentiment analysis. Specifically, we propose a backdoor adjustment-based causal model to disentangle the domain-specific and domain-invariant representations that play essential roles in tackling domain shift. First, we rethink the cross-domain sentiment analysis task in a causal view to model the causal-and-effect relationships among different variables. Then, to learn an invariant feature representation, we remove the effect of domain confounders (e.g., domain knowledge) using the backdoor adjustment. A series of experiments
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;GPT-3.5&#12289;GPT-4&#21644;Bard&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#27604;&#36739;&#65292;&#21457;&#29616;&#19981;&#21516;&#30340;LLM&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#21487;&#20197;&#20197;88%&#30340;&#20934;&#30830;&#29575;&#36890;&#36807;&#31616;&#21333;&#30340;&#20998;&#31867;&#27169;&#22411;&#23558;&#25991;&#26412;&#24402;&#22240;&#20110;&#30456;&#24212;&#30340;LLM&#26469;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.14533</link><description>&lt;p&gt;
&#12298;&#23427;&#21040;&#24213;&#26159;&#35841;&#30340;LLM&#65311;GPT-3.5&#12289;GPT-4&#21644;Bard&#30340;&#35821;&#35328;&#27604;&#36739;&#21644;LLM&#23646;&#24615;&#24402;&#22240;&#12299;
&lt;/p&gt;
&lt;p&gt;
Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14533
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;GPT-3.5&#12289;GPT-4&#21644;Bard&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#27604;&#36739;&#65292;&#21457;&#29616;&#19981;&#21516;&#30340;LLM&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#21487;&#20197;&#20197;88%&#30340;&#20934;&#30830;&#29575;&#36890;&#36807;&#31616;&#21333;&#30340;&#20998;&#31867;&#27169;&#22411;&#23558;&#25991;&#26412;&#24402;&#22240;&#20110;&#30456;&#24212;&#30340;LLM&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#29983;&#25104;&#19982;&#25110;&#36229;&#36234;&#20154;&#31867;&#36136;&#37327;&#30456;&#20284;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;LLMs&#26159;&#21542;&#20542;&#21521;&#20110;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#20316;&#32773;&#30340;&#29420;&#29305;&#35821;&#35328;&#39118;&#26684;&#23578;&#19981;&#28165;&#26970;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#30001;&#24403;&#20170;&#26368;&#27969;&#34892;&#30340;&#19977;&#31181;LLMs&#65288;GPT-3.5&#12289;GPT-4&#21644;Bard&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#35789;&#27719;&#12289;&#35789;&#24615;&#20998;&#24067;&#12289;&#20381;&#36182;&#20998;&#24067;&#21644;&#24773;&#24863;&#19982;&#22810;&#26679;&#36755;&#20837;&#12290;&#32467;&#26524;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#36827;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#31616;&#21333;&#30340;&#29616;&#25104;&#20998;&#31867;&#27169;&#22411;&#20197;88%&#30340;&#20934;&#30830;&#29575;&#23558;&#32473;&#23450;&#25991;&#26412;&#24402;&#22240;&#20110;&#20854;LLM&#26469;&#28304;&#12290;&#35752;&#35770;&#20102;&#36825;&#19968;&#26377;&#36259;&#21457;&#29616;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14533v1 Announce Type: new  Abstract: Large Language Models (LLMs) are capable of generating text that is similar to or surpasses human quality. However, it is unclear whether LLMs tend to exhibit distinctive linguistic styles akin to how human authors do. Through a comprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech (POS) distribution, dependency distribution, and sentiment of texts generated by three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse inputs. The results point to significant linguistic variations which, in turn, enable us to attribute a given text to its LLM origin with a favorable 88\% accuracy using a simple off-the-shelf classification model. Theoretical and practical implications of this intriguing finding are discussed.
&lt;/p&gt;</description></item><item><title>&#31036;&#35980;&#27700;&#24179;&#23545;LLMs&#30340;&#34920;&#29616;&#26377;&#24433;&#21709;&#65292;&#31895;&#40065;&#30340;&#25552;&#31034;&#36890;&#24120;&#23548;&#33268;&#36739;&#24046;&#30340;&#34920;&#29616;&#65292;&#32780;&#36807;&#20110;&#31036;&#35980;&#30340;&#35821;&#35328;&#24182;&#19981;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#26368;&#20339;&#30340;&#31036;&#35980;&#27700;&#24179;&#26681;&#25454;&#35821;&#35328;&#32780;&#24322;&#65292;LLMs&#19981;&#20165;&#21453;&#26144;&#20154;&#31867;&#34892;&#20026;&#65292;&#36824;&#21463;&#35821;&#35328;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#21516;&#30340;&#25991;&#21270;&#32972;&#26223;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.14531</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#23562;&#37325;LLM&#21527;&#65311;&#20851;&#20110;&#25552;&#31034;&#31036;&#35980;&#23545;LLM&#34920;&#29616;&#24433;&#21709;&#30340;&#36328;&#35821;&#35328;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14531
&lt;/p&gt;
&lt;p&gt;
&#31036;&#35980;&#27700;&#24179;&#23545;LLMs&#30340;&#34920;&#29616;&#26377;&#24433;&#21709;&#65292;&#31895;&#40065;&#30340;&#25552;&#31034;&#36890;&#24120;&#23548;&#33268;&#36739;&#24046;&#30340;&#34920;&#29616;&#65292;&#32780;&#36807;&#20110;&#31036;&#35980;&#30340;&#35821;&#35328;&#24182;&#19981;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#26368;&#20339;&#30340;&#31036;&#35980;&#27700;&#24179;&#26681;&#25454;&#35821;&#35328;&#32780;&#24322;&#65292;LLMs&#19981;&#20165;&#21453;&#26144;&#20154;&#31867;&#34892;&#20026;&#65292;&#36824;&#21463;&#35821;&#35328;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#21516;&#30340;&#25991;&#21270;&#32972;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#25552;&#31034;&#20013;&#30340;&#31036;&#35980;&#31243;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#65292;&#31036;&#35980;&#30340;&#35821;&#35328;&#36890;&#24120;&#33021;&#33719;&#24471;&#26356;&#22810;&#30340;&#36981;&#20174;&#21644;&#26377;&#25928;&#24615;&#65292;&#32780;&#31895;&#40065;&#21487;&#33021;&#23548;&#33268;&#21388;&#24694;&#65292;&#24433;&#21709;&#22238;&#24212;&#36136;&#37327;&#12290;&#25105;&#20204;&#35748;&#20026;LLMs&#21453;&#26144;&#20102;&#20154;&#31867;&#30340;&#20132;&#27969;&#29305;&#24449;&#65292;&#26263;&#31034;&#23427;&#20204;&#19982;&#20154;&#31867;&#25991;&#21270;&#35268;&#33539;&#19968;&#33268;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#33521;&#35821;&#12289;&#20013;&#25991;&#21644;&#26085;&#35821;&#20219;&#21153;&#20013;&#25552;&#31034;&#20013;&#30340;&#31036;&#35980;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#31895;&#40065;&#30340;&#25552;&#31034;&#36890;&#24120;&#23548;&#33268;&#36739;&#24046;&#30340;&#34920;&#29616;&#65292;&#32780;&#36807;&#20110;&#31036;&#35980;&#30340;&#35821;&#35328;&#24182;&#19981;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#20339;&#30340;&#31036;&#35980;&#31243;&#24230;&#26681;&#25454;&#35821;&#35328;&#32780;&#24322;&#12290;&#36825;&#19968;&#29616;&#35937;&#34920;&#26126;LLMs&#19981;&#20165;&#21453;&#26144;&#20154;&#31867;&#34892;&#20026;&#65292;&#36824;&#21463;&#35821;&#35328;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#21516;&#30340;&#25991;&#21270;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#36328;&#25991;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;LLM&#20351;&#29992;&#20013;&#32771;&#34385;&#31036;&#35980;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14531v1 Announce Type: new  Abstract: We investigate the impact of politeness levels in prompts on the performance of large language models (LLMs). Polite language in human communications often garners more compliance and effectiveness, while rudeness can cause aversion, impacting response quality. We consider that LLMs mirror human communication traits, suggesting they align with human cultural norms. We assess the impact of politeness in prompts on LLMs across English, Chinese, and Japanese tasks. We observed that impolite prompts often result in poor performance, but overly polite language does not guarantee better outcomes. The best politeness level is different according to the language. This phenomenon suggests that LLMs not only reflect human behavior but are also influenced by language, particularly in different cultural contexts. Our findings highlight the need to factor in politeness for cross-cultural natural language processing and LLM usage.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClusterClip Sampling&#30340;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20026;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.14526</link><description>&lt;p&gt;
&#24102;&#32858;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24179;&#34913;&#25968;&#25454;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Balanced Data Sampling for Language Model Training with Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClusterClip Sampling&#30340;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20026;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#23613;&#31649;&#20154;&#20204;&#24050;&#32463;&#20851;&#27880;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#21644;&#32452;&#25104;&#65292;&#20294;&#30830;&#23450;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#25277;&#26679;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;LLM&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426;&#25277;&#26679;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25277;&#26679;&#31574;&#30053;&#24573;&#35270;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#24615;&#65292;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ClusterClip Sampling&#65292;&#20197;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ClusterClip Sampling&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#26469;&#21453;&#26144;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#26681;&#25454;&#32858;&#31867;&#32467;&#26524;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#24120;&#35265;&#26679;&#26412;&#21644;&#31232;&#26377;&#26679;&#26412;&#12290;&#24341;&#20837;&#20102;&#37325;&#22797;&#35009;&#21098;&#25805;&#20316;&#26469;&#20943;&#36731;&#30001;&#20110;&#26469;&#33258;&#26576;&#20123;&#32858;&#31867;&#30340;&#26679;&#26412;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;ClusterClip Sampling&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14526v1 Announce Type: cross  Abstract: Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Daisy-TTS&#35774;&#35745;&#65292;&#36890;&#36807;&#22768;&#35843;&#23884;&#20837;&#20998;&#35299;&#65292;&#27169;&#25311;&#20102;&#26356;&#24191;&#27867;&#30340;&#24773;&#24863;&#33539;&#22260;&#65292;&#21253;&#25324; primary emotions&#12289;secondary emotions&#12289;intensity-level &#21644; emotions polarity&#12290;</title><link>https://arxiv.org/abs/2402.14523</link><description>&lt;p&gt;
Daisy-TTS: &#36890;&#36807;&#22768;&#35843;&#23884;&#20837;&#20998;&#35299;&#27169;&#25311;&#26356;&#24191;&#27867;&#30340;&#24773;&#24863;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Daisy-TTS&#35774;&#35745;&#65292;&#36890;&#36807;&#22768;&#35843;&#23884;&#20837;&#20998;&#35299;&#65292;&#27169;&#25311;&#20102;&#26356;&#24191;&#27867;&#30340;&#24773;&#24863;&#33539;&#22260;&#65292;&#21253;&#25324; primary emotions&#12289;secondary emotions&#12289;intensity-level &#21644; emotions polarity&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32463;&#24120;&#20197;&#22810;&#26041;&#38754;&#30340;&#26041;&#24335;&#21475;&#22836;&#34920;&#36798;&#24773;&#24863;&#65292;&#23427;&#20204;&#22312;&#24378;&#24230;&#19978;&#21487;&#33021;&#26377;&#25152;&#21464;&#21270;&#65292;&#34920;&#36798;&#30340;&#19981;&#20165;&#26159;&#21333;&#19968;&#30340;&#24773;&#24863;&#65292;&#36824;&#21487;&#33021;&#26159;&#21508;&#31181;&#24773;&#24863;&#30340;&#28151;&#21512;&#20307;&#12290;&#36825;&#31181;&#24191;&#27867;&#30340;&#24773;&#24863;&#33539;&#22260;&#22312;&#24773;&#24863;&#32467;&#26500;&#27169;&#22411;&#20013;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#35813;&#27169;&#22411;&#23558;&#21508;&#31181;&#24773;&#24863;&#34920;&#31034;&#20026;&#21407;&#22987;&#24773;&#24863;&#30340;&#27966;&#29983;&#20135;&#21697;&#65292;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#24378;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25991;&#26412;&#36716;&#35821;&#38899;&#35774;&#35745;&#65292;&#26088;&#22312;&#27169;&#25311;&#22522;&#20110;&#32467;&#26500;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#24773;&#24863;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#35745;Daisy-TTS&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22768;&#35843;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#23398;&#20064;&#20316;&#20026;&#24773;&#24863;&#20195;&#29702;&#30340;&#21487;&#20998;&#31163;&#30340;&#22768;&#35843;&#23884;&#20837;&#12290;&#36825;&#31181;&#24773;&#24863;&#34920;&#31034;&#20351;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#65306;&#65288;1&#65289;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#21407;&#22987;&#24773;&#24863;&#65292;&#65288;2&#65289;&#20316;&#20026;&#21407;&#22987;&#24773;&#24863;&#30340;&#28151;&#21512;&#20307;&#30340;&#27425;&#32423;&#24773;&#24863;&#65292;&#65288;3&#65289;&#36890;&#36807;&#35843;&#25972;&#24773;&#24863;&#23884;&#20837;&#26469;&#23454;&#29616;&#24378;&#24230;&#32423;&#21035;&#65292;&#65288;4&#65289;&#36890;&#36807;&#21542;&#23450;&#24773;&#24863;&#23884;&#20837;&#26469;&#23454;&#29616;&#24773;&#24863;&#26497;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14523v1 Announce Type: new  Abstract: We often verbally express emotions in a multifaceted manner, they may vary in their intensities and may be expressed not just as a single but as a mixture of emotions. This wide spectrum of emotions is well-studied in the structural model of emotions, which represents variety of emotions as derivative products of primary emotions with varying degrees of intensity. In this paper, we propose an emotional text-to-speech design to simulate a wider spectrum of emotions grounded on the structural model. Our proposed design, Daisy-TTS, incorporates a prosody encoder to learn emotionally-separable prosody embedding as a proxy for emotion. This emotion representation allows the model to simulate: (1) Primary emotions, as learned from the training samples, (2) Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by scaling the emotion embedding, and (4) Emotions polarity, by negating the emotion embedding. Through a series of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#29992;&#20110;&#32479;&#19968;&#19981;&#21516;&#27169;&#22411;&#30340;&#20219;&#21153;&#23884;&#20837;&#65292;&#20351;&#24471;&#20219;&#21153;&#23884;&#20837;&#21487;&#20197;&#36328;&#36234;&#21508;&#31181;&#27169;&#22411;&#65292;&#24182;&#22312;&#21333;&#19968;&#21521;&#37327;&#31354;&#38388;&#20869;&#36827;&#34892;&#27604;&#36739;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.14522</link><description>&lt;p&gt;
&#36328;&#36234;&#22810;&#20010;&#27169;&#22411;&#30340;&#32479;&#19968;&#20219;&#21153;&#23884;&#20837;&#65306;&#24357;&#21512;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#23427;&#27169;&#22411;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14522
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#29992;&#20110;&#32479;&#19968;&#19981;&#21516;&#27169;&#22411;&#30340;&#20219;&#21153;&#23884;&#20837;&#65292;&#20351;&#24471;&#20219;&#21153;&#23884;&#20837;&#21487;&#20197;&#36328;&#36234;&#21508;&#31181;&#27169;&#22411;&#65292;&#24182;&#22312;&#21333;&#19968;&#21521;&#37327;&#31354;&#38388;&#20869;&#36827;&#34892;&#27604;&#36739;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23884;&#20837;&#26159;&#19968;&#31181;&#25429;&#25417;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#27169;&#22411;&#32534;&#36753;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#39046;&#22495;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#19968;&#20219;&#21153;&#23884;&#20837;&#65288;FUTE&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#21327;&#35843;&#26469;&#33258;&#21508;&#31181;&#27169;&#22411;&#65288;&#21253;&#25324;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20855;&#26377;&#19981;&#21516;&#25552;&#31034;&#30340;LLMs&#65289;&#30340;&#20219;&#21153;&#23884;&#20837;&#65292;&#20351;&#20854;&#22788;&#20110;&#21333;&#19968;&#21521;&#37327;&#31354;&#38388;&#12290;&#36825;&#31181;&#32479;&#19968;&#24615;&#20351;&#24471;&#21487;&#20197;&#27604;&#36739;&#21644;&#20998;&#26512;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#35299;&#20915;&#22810;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#33539;&#22260;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14522v1 Announce Type: new  Abstract: Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-mo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21547;&#26377;&#23454;&#20307;&#21644;&#20851;&#31995;&#26631;&#27880;&#30340;&#39532;&#26469;&#35199;&#20122;&#33521;&#35821;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;spaCy NER&#24037;&#20855;&#36827;&#34892;&#24494;&#35843;&#65292;&#26412;&#30740;&#31350;&#26174;&#33879;&#25913;&#21892;&#20102;&#39532;&#26469;&#35199;&#20122;&#33521;&#35821;&#20013;NER&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14521</link><description>&lt;p&gt;
&#39532;&#26469;&#35199;&#20122;&#33521;&#35821;&#26032;&#38395;&#35299;&#26512;&#65306;&#19968;&#20010;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#35821;&#35328;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Malaysian English News Decoded: A Linguistic Resource for Named Entity and Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21547;&#26377;&#23454;&#20307;&#21644;&#20851;&#31995;&#26631;&#27880;&#30340;&#39532;&#26469;&#35199;&#20122;&#33521;&#35821;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;spaCy NER&#24037;&#20855;&#36827;&#34892;&#24494;&#35843;&#65292;&#26412;&#30740;&#31350;&#26174;&#33879;&#25913;&#21892;&#20102;&#39532;&#26469;&#35199;&#20122;&#33521;&#35821;&#20013;NER&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#33521;&#35821;&#21644;&#39532;&#26469;&#35199;&#20122;&#33521;&#35821;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22312;&#39532;&#26469;&#35199;&#20122;&#33521;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;200&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#39532;&#26469;&#35199;&#20122;&#33521;&#35821;&#26032;&#38395;&#65288;MEN&#65289;&#25968;&#25454;&#38598;&#65292;&#25163;&#21160;&#23545;&#23454;&#20307;&#21644;&#20851;&#31995;&#36827;&#34892;&#20102;&#26631;&#27880;&#65292;&#24182;&#36890;&#36807;&#23545;spaCy NER&#24037;&#20855;&#36827;&#34892;&#24494;&#35843;&#39564;&#35777;&#20102;&#38024;&#23545;&#39532;&#26469;&#35199;&#20122;&#33521;&#35821;&#23450;&#21046;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;NER&#22312;&#39532;&#26469;&#35199;&#20122;&#33521;&#35821;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14521v1 Announce Type: new  Abstract: Standard English and Malaysian English exhibit notable differences, posing challenges for natural language processing (NLP) tasks on Malaysian English. Unfortunately, most of the existing datasets are mainly based on standard English and therefore inadequate for improving NLP tasks in Malaysian English. An experiment using state-of-the-art Named Entity Recognition (NER) solutions on Malaysian English news articles highlights that they cannot handle morphosyntactic variations in Malaysian English. To the best of our knowledge, there is no annotated dataset available to improvise the model. To address these issues, we constructed a Malaysian English News (MEN) dataset, which contains 200 news articles that are manually annotated with entities and relations. We then fine-tuned the spaCy NER tool and validated that having a dataset tailor-made for Malaysian English could improve the performance of NER in Malaysian English significantly. This
&lt;/p&gt;</description></item><item><title>&#31532;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#19981;&#19968;&#23450;&#20195;&#34920;&#26368;&#32456;&#25991;&#26412;&#36755;&#20986;&#65292;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#19982;&#29992;&#25143;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.14499</link><description>&lt;p&gt;
"&#25105;&#30340;&#31572;&#26696;&#26159;C": &#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31532;&#19968;&#20010;&#20196;&#29260;&#27010;&#29575;&#19982;&#25991;&#26412;&#31572;&#26696;&#19981;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
"My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14499
&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#19981;&#19968;&#23450;&#20195;&#34920;&#26368;&#32456;&#25991;&#26412;&#36755;&#20986;&#65292;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#19982;&#29992;&#25143;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14499v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#35821;&#35328;&#29983;&#25104;&#30340;&#24320;&#25918;&#24615;&#36136;&#20351;&#24471;&#35780;&#20272;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#20351;&#29992;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#38480;&#21046;&#21709;&#24212;&#31354;&#38388;&#12290;&#28982;&#21518;&#36890;&#36807;&#25490;&#21517;&#20505;&#36873;&#31572;&#26696;&#30340;&#31532;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#30340;&#23545;&#25968;&#27010;&#29575;&#26469;&#35780;&#20272;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#31532;&#19968;&#20010;&#20196;&#29260;&#21487;&#33021;&#19981;&#19968;&#33268;&#22320;&#21453;&#26144;&#26368;&#32456;&#30340;&#21709;&#24212;&#36755;&#20986;&#65292;&#22240;&#20026;&#27169;&#22411;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#21709;&#24212;&#39118;&#26684;&#65292;&#20363;&#22914;&#20197;"&#30830;&#23450;"&#24320;&#22836;&#25110;&#25298;&#32477;&#22238;&#31572;&#12290;&#22240;&#27492;&#65292;MCQ&#35780;&#20272;&#26080;&#27861;&#34920;&#26126;&#27169;&#22411;&#19982;&#29992;&#25143;&#20114;&#21160;&#26102;&#30340;&#34892;&#20026;&#12290;&#20294;&#24046;&#36317;&#26377;&#22810;&#22823;&#21602;&#65311;&#25105;&#20204;&#35780;&#20272;&#20102;&#31532;&#19968;&#20010;&#20196;&#29260;&#35780;&#20272;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#19982;&#25991;&#26412;&#36755;&#20986;&#30340;&#19968;&#33268;&#24615;&#65292;&#21363;&#26368;&#32456;&#36873;&#39033;&#36873;&#25321;&#12289;&#25298;&#32477;&#29575;&#12289;&#36873;&#25321;&#20998;&#24067;&#21644;&#22312;&#25552;&#31034;&#25200;&#21160;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#20005;&#37325;&#19981;&#19968;&#33268;&#65292;&#36798;&#21040;60%&#20197;&#19978;&#30340;&#19981;&#21305;&#37197;&#29575;&#12290;&#27169;&#22411;&#38750;&#24120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14499v1 Announce Type: new  Abstract: The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with "Sure" or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavil
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Noise-BERT&#26694;&#26550;&#65292;&#21253;&#21547;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#21644;&#23545;&#25239;&#25915;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14494</link><description>&lt;p&gt;
Noise-BERT: &#19968;&#31181;&#20855;&#26377;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#25200;&#21160;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22024;&#26434;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14494
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Noise-BERT&#26694;&#26550;&#65292;&#21253;&#21547;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#21644;&#23545;&#25239;&#25915;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#36755;&#20837;&#20449;&#24687;&#32463;&#24120;&#36973;&#21463;&#21508;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#25200;&#21160;&#65292;&#36825;&#24433;&#21709;&#20102;&#27133;&#22635;&#20805;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#20110;&#35268;&#21017;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#38754;&#23545;&#26410;&#30693;&#22122;&#22768;&#24178;&#25200;&#26102;&#65292;&#23427;&#20204;&#26080;&#27861;&#23637;&#29616;&#20986;&#26399;&#26395;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Noise-BERT&#26469;&#35299;&#20915;&#27133;&#22635;&#20805;&#20013;&#36755;&#20837;&#25200;&#21160;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#25200;&#21160;&#40065;&#26834;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;Noise Alignment&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#27133;&#23631;&#34109;&#39044;&#27979;&#21644;&#21477;&#23376;&#22024;&#26434;&#24230;&#21028;&#21035;&#65292;&#26088;&#22312;&#24341;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#20934;&#30830;&#30340;&#27133;&#20449;&#24687;&#21644;&#22122;&#22768;&#20998;&#24067;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#26469;&#22686;&#24378;&#23454;&#20307;&#21644;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25239;&#25915;&#20987;&#35757;&#32451;&#31574;&#30053;&#20197;&#25552;&#39640;&#35821;&#20041;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14494v1 Announce Type: new  Abstract: In a realistic dialogue system, the input information from users is often subject to various types of input perturbations, which affects the slot-filling task. Although rule-based data augmentation methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances. In this study, we address the challenges posed by input perturbations in slot filling by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and Sentence Noisiness Discrimination, aiming to guide the pre-trained language model in capturing accurate slot information and noise distribution. During fine-tuning, we employ a contrastive learning loss to enhance the semantic representation of entities and labels. Additionally, we introduce an adversarial attack training strategy to i
&lt;/p&gt;</description></item><item><title>INSTRAUG&#26159;&#19968;&#31181;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#27169;&#20219;&#21153;&#20013;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.14492</link><description>&lt;p&gt;
INSTRAUG&#65306;&#29992;&#20110;&#22810;&#27169;&#25351;&#20196;&#24494;&#35843;&#30340;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14492
&lt;/p&gt;
&lt;p&gt;
INSTRAUG&#26159;&#19968;&#31181;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#27169;&#20219;&#21153;&#20013;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20219;&#21153;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#23545;&#26032;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#26368;&#36817;&#20851;&#20110;&#39640;&#36136;&#37327;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#29983;&#25104;&#21644;&#36873;&#25321;&#30340;&#24037;&#20316;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#65292;&#20197;&#20026;&#32473;&#23450;&#20219;&#21153;&#26500;&#24605;&#27169;&#22411;&#21487;&#29702;&#35299;&#30340;&#25351;&#20196;&#65292;&#24182;&#35880;&#24910;&#36807;&#28388;LLM&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;INSTRAUG&#30340;&#22810;&#27169;&#20219;&#21153;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#12290;&#23427;&#20174;&#19968;&#20123;&#22522;&#26412;&#21644;&#31616;&#21333;&#30340;&#20803;&#25351;&#20196;&#24320;&#22987;&#65292;&#20294;&#33021;&#23558;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#25193;&#22823;30&#20493;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22810;&#27169;&#25351;&#20196;&#36319;&#38543;&#22522;&#20934;&#27979;&#35797;&#38598;MULTIINSTRUCT&#21644;InstructBLIP&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;INSTRAUG&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#36328;12&#20010;&#22810;&#27169;&#20219;&#21153;&#30340;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#23545;&#40784;&#65292;&#29978;&#33267;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14492v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) on multi-task instruction-following data has been proven to be a powerful learning paradigm for improving their zero-shot capabilities on new tasks. Recent works about high-quality instruction-following data generation and selection require amounts of human labor to conceive model-understandable instructions for the given tasks and carefully filter the LLM-generated data. In this work, we introduce an automatic instruction augmentation method named INSTRAUG in multimodal tasks. It starts from a handful of basic and straightforward meta instructions but can expand an instruction-following dataset by 30 times. Results on two popular multimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show that INSTRAUG can significantly improve the alignment of multimodal large language models (MLLMs) across 12 multimodal tasks, which is even equivalent to the benefits of scaling up training 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#22120;&#65292;&#26088;&#22312;&#20135;&#29983;&#20445;&#25345;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#20449;&#24687;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#22914;&#20309;&#25913;&#21464;&#12290;&#35843;&#26597;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#26377;&#29983;&#25104;&#20808;&#21069;&#31572;&#26696;&#20316;&#20026;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.14488</link><description>&lt;p&gt;
&#29983;&#25104;&#22120;&#26159;&#21542;&#20851;&#24515;&#20854;&#19978;&#19979;&#25991;&#65311;&#23545;&#19978;&#19979;&#25991;&#36716;&#31227;&#24773;&#20917;&#19979;&#29983;&#25104;&#27169;&#22411;&#24544;&#23454;&#24615;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Does the Generator Mind its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#22120;&#65292;&#26088;&#22312;&#20135;&#29983;&#20445;&#25345;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#20449;&#24687;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#22914;&#20309;&#25913;&#21464;&#12290;&#35843;&#26597;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#26377;&#29983;&#25104;&#20808;&#21069;&#31572;&#26696;&#20316;&#20026;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#22120;&#65292;&#26088;&#22312;&#20135;&#29983;&#20445;&#25345;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#20449;&#24687;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#22914;&#20309;&#25913;&#21464;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26816;&#39564;&#28304;&#33258;&#38745;&#24577;&#36755;&#20837;&#30340;&#24187;&#35273;&#65292;&#20363;&#22914;&#22312;&#25688;&#35201;&#25110;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21160;&#24577;&#30693;&#35782;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#24335;&#38382;&#31572;&#30340;&#24544;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#24403;&#19978;&#19979;&#25991;&#30693;&#35782;&#21457;&#29983;&#21464;&#21270;&#26102;&#30001;&#21442;&#25968;&#21270;&#20869;&#23384;&#20135;&#29983;&#24187;&#35273;&#30340;&#23384;&#22312;&#65292;&#24182;&#20998;&#26512;&#20854;&#21457;&#29983;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#31181;&#24187;&#35273;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#26377;&#29983;&#25104;&#20808;&#21069;&#31572;&#26696;&#20316;&#20026;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14488v1 Announce Type: new  Abstract: The present study introduces the knowledge-augmented generator, which is specifically designed to produce information that remains grounded in contextual knowledge, regardless of alterations in the context. Previous research has predominantly focused on examining hallucinations stemming from static input, such as in the domains of summarization or machine translation. However, our investigation delves into the faithfulness of generative question answering in the presence of dynamic knowledge. Our objective is to explore the existence of hallucinations arising from parametric memory when contextual knowledge undergoes changes, while also analyzing the underlying causes for their occurrence. In order to efficiently address this issue, we propose a straightforward yet effective measure for detecting such hallucinations. Intriguingly, our investigation uncovers that all models exhibit a tendency to generate previous answers as hallucinations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#30340;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#33021;&#21147;&#65292;&#21457;&#29616;ChatGPT&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24403;&#26377;&#36275;&#22815;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#20197;&#24448;&#30340;&#27169;&#22411;&#20173;&#28982;&#36229;&#36234;&#20102;&#23427;&#12290;</title><link>https://arxiv.org/abs/2402.14484</link><description>&lt;p&gt;
ChatGPT&#26159;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#30340;&#26410;&#26469;&#21527;&#65311;&#19968;&#39033;&#20840;&#38754;&#35780;&#20272;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#30340;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#33021;&#21147;&#65292;&#21457;&#29616;ChatGPT&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24403;&#26377;&#36275;&#22815;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#20197;&#24448;&#30340;&#27169;&#22411;&#20173;&#28982;&#36229;&#36234;&#20102;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#24615;&#22312;&#20154;&#31867;&#35748;&#30693;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#21508;&#31181;&#30740;&#31350;&#39046;&#22495;&#24341;&#36215;&#20851;&#27880;&#12290;&#38543;&#30528;&#25991;&#26412;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#35782;&#21035;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#22312;&#25552;&#21462;&#26377;&#24847;&#20041;&#27169;&#24335;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36229;&#20986;&#19968;&#33324;&#33521;&#35821;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#39046;&#22495;&#29305;&#23450;&#21644;&#38750;&#33521;&#35821;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#30830;&#20445;ChatGPT&#21644;&#20043;&#21069;&#26041;&#27861;&#20043;&#38388;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#27010;&#36848;&#20102;&#22312;&#24212;&#29992;ChatGPT&#36827;&#34892;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#26102;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ChatGPT&#23545;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;&#26469;&#35828;&#37117;&#26159;&#19968;&#20010;&#33391;&#22909;&#30340;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#37197;&#22791;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#20197;&#24448;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14484v1 Announce Type: new  Abstract: Causality is fundamental in human cognition and has drawn attention in diverse research fields. With growing volumes of textual data, discerning causalities within text data is crucial, and causal text mining plays a pivotal role in extracting meaningful patterns. This study conducts comprehensive evaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce a benchmark that extends beyond general English datasets, including domain-specific and non-English datasets. We also provide an evaluation framework to ensure fair comparisons between ChatGPT and previous approaches. Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance. Additionally, ChatG
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#25551;&#36848;&#12289;&#35299;&#37322;&#21644;&#35843;&#35797;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;GAMs&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#32467;&#21512;LLMs&#30340;&#28789;&#27963;&#24615;&#21644;GAMs&#20934;&#30830;&#25551;&#36848;&#30340;&#32479;&#35745;&#27169;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#38598;&#25688;&#35201;&#12289;&#38382;&#31572;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.14474</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#21644;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Data Science with LLMs and Interpretable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#25551;&#36848;&#12289;&#35299;&#37322;&#21644;&#35843;&#35797;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;GAMs&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#32467;&#21512;LLMs&#30340;&#28789;&#27963;&#24615;&#21644;GAMs&#20934;&#30830;&#25551;&#36848;&#30340;&#32479;&#35745;&#27169;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#38598;&#25688;&#35201;&#12289;&#38382;&#31572;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26088;&#22312;&#34987;&#20154;&#31867;&#36731;&#26494;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20351;&#29992;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#25551;&#36848;&#12289;&#35299;&#37322;&#21644;&#35843;&#35797;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;(GAMs)&#12290;&#23558;LLMs&#30340;&#28789;&#27963;&#24615;&#19982;GAMs&#20934;&#30830;&#25551;&#36848;&#30340;&#24191;&#27867;&#32479;&#35745;&#27169;&#24335;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#23454;&#29616;&#25968;&#25454;&#38598;&#25688;&#35201;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;LLMs&#36824;&#21487;&#20197;&#25913;&#21892;&#39046;&#22495;&#19987;&#23478;&#19982;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#20026;&#28508;&#22312;&#29616;&#35937;&#29983;&#25104;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#24320;&#28304;LLM-GAM&#25509;&#21475;\url{https://github.com/interpretml/TalkToEBM}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14474v1 Announce Type: cross  Abstract: Recent years have seen important advances in the building of interpretable models, machine learning models that are designed to be easily understood by humans. In this work, we show that large language models (LLMs) are remarkably good at working with interpretable models, too. In particular, we show that LLMs can describe, interpret, and debug Generalized Additive Models (GAMs). Combining the flexibility of LLMs with the breadth of statistical patterns accurately described by GAMs enables dataset summarization, question answering, and model critique. LLMs can also improve the interaction between domain experts and interpretable models, and generate hypotheses about the underlying phenomenon. We release \url{https://github.com/interpretml/TalkToEBM} as an open-source LLM-GAM interface.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26550;&#26500;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#33258;&#21160;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#30340;&#26377;&#25928;&#26041;&#27861;&#12289;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#35821;&#26009;&#24211;&#20197;&#21450;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#35770;&#35777;&#26550;&#26500;&#30340;&#22522;&#32447;&#21644;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14458</link><description>&lt;p&gt;
NLAS-multi&#65306;&#19968;&#20010;&#22810;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26550;&#26500;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language Argumentation Schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26550;&#26500;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#33258;&#21160;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#30340;&#26377;&#25928;&#26041;&#27861;&#12289;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#35821;&#26009;&#24211;&#20197;&#21450;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#35770;&#35777;&#26550;&#26500;&#30340;&#22522;&#32447;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35770;&#35777;&#25366;&#25496;&#12289;&#35770;&#35777;&#29983;&#25104;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#20998;&#26512;&#39046;&#22495;&#65292;&#19968;&#20123;&#20027;&#35201;&#38480;&#21046;&#28041;&#21450;&#27880;&#37322;&#23500;&#26377;&#35770;&#35777;&#24615;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12289;&#36825;&#20123;&#35821;&#26009;&#24211;&#30340;&#26377;&#38480;&#35268;&#27169;&#65292;&#20197;&#21450;&#20195;&#34920;&#36827;&#34892;&#27880;&#37322;&#30340;&#19981;&#21516;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20197;&#19979;&#36129;&#29486;&#65306;(i) &#22312;&#19981;&#21516;&#20027;&#39064;&#21644;&#35821;&#35328;&#20013;&#33258;&#21160;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#30340;&#26377;&#25928;&#26041;&#27861;&#35770;&#65292;(ii) &#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26550;&#26500;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;(iii) &#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#35770;&#35777;&#26550;&#26500;&#30340;&#19968;&#32452;&#21487;&#38752;&#22522;&#32447;&#21644;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14458v1 Announce Type: cross  Abstract: Some of the major limitations identified in the areas of argument mining, argument generation, and natural language argument analysis are related to the complexity of annotating argumentatively rich data, the limited size of these corpora, and the constraints that represent the different languages and domains in which these data is annotated. To address these limitations, in this paper we present the following contributions: (i) an effective methodology for the automatic generation of natural language arguments in different topics and languages, (ii) the largest publicly available corpus of natural language argumentation schemes, and (iii) a set of solid baselines and fine-tuned models for the automatic identification of argumentation schemes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#27880;&#26041;&#26696;&#65292;&#29992;&#20110;&#20998;&#31867;&#26465;&#27454;&#21644;&#26465;&#20214;&#21512;&#21516;&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#27454;&#65292;&#20197;&#25903;&#25345;&#27861;&#24459;&#19987;&#23478;&#24555;&#36895;&#35782;&#21035;&#21644;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#36825;&#20123;&#31867;&#21035;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14457</link><description>&lt;p&gt;
&#26631;&#27880;&#21644;&#20998;&#31867;&#26465;&#27454;&#21644;&#26465;&#20214;&#21512;&#21516;&#20013;&#30340;&#30456;&#20851;&#26465;&#27454;
&lt;/p&gt;
&lt;p&gt;
Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14457
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#27880;&#26041;&#26696;&#65292;&#29992;&#20110;&#20998;&#31867;&#26465;&#27454;&#21644;&#26465;&#20214;&#21512;&#21516;&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#27454;&#65292;&#20197;&#25903;&#25345;&#27861;&#24459;&#19987;&#23478;&#24555;&#36895;&#35782;&#21035;&#21644;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#36825;&#20123;&#31867;&#21035;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#27880;&#26041;&#26696;&#65292;&#29992;&#20110;&#20998;&#31867;&#26465;&#27454;&#21644;&#26465;&#20214;&#21512;&#21516;&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#27454;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#25903;&#25345;&#27861;&#24459;&#19987;&#23478;&#24555;&#36895;&#35782;&#21035;&#21644;&#35780;&#20272;&#36825;&#31867;&#27861;&#24459;&#25991;&#20214;&#20013;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#23567;&#35268;&#27169;&#30340;&#26465;&#27454;&#21644;&#26465;&#20214;&#21512;&#21516;&#35821;&#26009;&#24211;&#65292;&#24182;&#23436;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;14&#20010;&#31867;&#21035;&#30340;&#26631;&#27880;&#26041;&#26696;&#65292;&#26368;&#32456;&#36798;&#25104;&#20102;0.92&#30340;&#26631;&#27880;&#32773;&#38388;&#19968;&#33268;&#24615;&#12290;&#28982;&#21518;&#65292;&#38024;&#23545;&#20854;&#20013;&#30340;11&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;T5&#21644;&#20004;&#20010;&#22522;&#20110;BERT&#30340;LLM&#30340;&#20004;&#20010;&#24494;&#35843;&#29256;&#26412;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#23454;&#39564;&#65292;&#20854;&#20013;&#36824;&#21253;&#25324;&#24847;&#22823;&#21033;&#35821;&#30340;&#20004;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#39564;&#35777;&#20219;&#21153;&#20013;&#36798;&#21040;&#20174;.79&#21040;.95&#30340;&#20934;&#30830;&#29575;&#65292;&#33258;&#21160;&#23545;&#25105;&#20204;&#30340;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14457v1 Announce Type: new  Abstract: In this paper, we propose a new annotation scheme to classify different types of clauses in Terms-and-Conditions contracts with the ultimate goal of supporting legal experts to quickly identify and assess problematic issues in this type of legal documents. To this end, we built a small corpus of Terms-and-Conditions contracts and finalized an annotation scheme of 14 categories, eventually reaching an inter-annotator agreement of 0.92. Then, for 11 of them, we experimented with binary classification tasks using few-shot prompting with a multilingual T5 and two fine-tuned versions of two BERT-based LLMs for Italian. Our experiments showed the feasibility of automatic classification of our categories by reaching accuracies ranging from .79 to .95 on validation tasks.
&lt;/p&gt;</description></item><item><title>LLMs&#21487;&#20197;&#38544;&#24335;&#22788;&#29702;&#29992;&#25143;&#36755;&#20837;&#21644;&#29983;&#25104;&#30340;&#21709;&#24212;&#20043;&#38388;&#30340;&#25991;&#26412;&#38590;&#24230;&#65292;&#26377;&#20123;LLMs&#22312;&#22788;&#29702;&#25991;&#26412;&#38590;&#24230;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36234;&#20154;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.14453</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#38544;&#21547;&#22320;&#30830;&#23450;&#29992;&#25143;&#30340;&#36866;&#23452;&#25991;&#26412;&#38590;&#24230;?
&lt;/p&gt;
&lt;p&gt;
Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14453
&lt;/p&gt;
&lt;p&gt;
LLMs&#21487;&#20197;&#38544;&#24335;&#22788;&#29702;&#29992;&#25143;&#36755;&#20837;&#21644;&#29983;&#25104;&#30340;&#21709;&#24212;&#20043;&#38388;&#30340;&#25991;&#26412;&#38590;&#24230;&#65292;&#26377;&#20123;LLMs&#22312;&#22788;&#29702;&#25991;&#26412;&#38590;&#24230;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36234;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#20010;&#20307;&#23398;&#20064;&#27700;&#24179;&#30340;&#25945;&#32946;&#23545;&#25552;&#39640;&#23398;&#29983;&#30340;&#29702;&#35299;&#21147;&#26159;&#24517;&#35201;&#30340;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#30340;&#31532;&#19968;&#27493;&#26159;&#35843;&#25972;&#25991;&#26412;&#38590;&#24230;&#20197;&#36866;&#24212;&#23398;&#29983;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;LLMs&#22914;&#20309;&#22312;&#29992;&#25143;&#36755;&#20837;&#21644;&#29983;&#25104;&#30340;&#25991;&#26412;&#20043;&#38388;&#38544;&#24335;&#35843;&#25972;&#25991;&#26412;&#38590;&#24230;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#20174;Stack Overflow&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25506;&#32034;&#22522;&#20110;&#38382;&#31572;&#30340;&#23545;&#35805;&#24615;&#33021;&#12290;&#22312;Stack Overflow&#25968;&#25454;&#38598;&#21644;TSCC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#21253;&#25324;&#22810;&#36718;&#23545;&#35805;&#65292;&#34920;&#26126;LLMs&#21487;&#20197;&#38544;&#24335;&#22788;&#29702;&#29992;&#25143;&#36755;&#20837;&#21644;&#29983;&#25104;&#30340;&#21709;&#24212;&#20043;&#38388;&#30340;&#25991;&#26412;&#38590;&#24230;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#20123;LLMs&#22312;&#22788;&#29702;&#25991;&#26412;&#38590;&#24230;&#21644;&#25351;&#23548;&#35843;&#25972;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#21487;&#20197;&#36229;&#36234;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14453v1 Announce Type: new  Abstract: Education that suits the individual learning level is necessary to improve students' understanding. The first step in achieving this purpose by using large language models (LLMs) is to adjust the textual difficulty of the response to students. This work analyzes how LLMs can implicitly adjust text difficulty between user input and its generated text. To conduct the experiments, we created a new dataset from Stack-Overflow to explore the performance of question-answering-based conversation. Experimental results on the Stack-Overflow dataset and the TSCC dataset, including multi-turn conversation show that LLMs can implicitly handle text difficulty between user input and its generated response. We also observed that some LLMs can surpass humans in handling text difficulty and the importance of instruction-tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#35821;&#35328;&#27169;&#22411;&#27010;&#24565;&#24341;&#23548;&#26694;&#26550;&#25193;&#23637;&#21040;&#26356;&#20016;&#23500;&#30340;&#27010;&#24565;&#38598;&#65292;&#25506;&#32034;&#24403;&#21069;&#26816;&#27979;&#21644;&#24341;&#23548;&#31574;&#30053;&#22312;&#36866;&#24403;&#24615;&#12289;&#24189;&#40664;&#12289;&#21019;&#36896;&#21147;&#21644;&#36136;&#37327;&#31561;&#25361;&#25112;&#24615;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14433</link><description>&lt;p&gt;
&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#28508;&#22312;&#31354;&#38388;&#30340;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Language Model's Guide Through Latent Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#35821;&#35328;&#27169;&#22411;&#27010;&#24565;&#24341;&#23548;&#26694;&#26550;&#25193;&#23637;&#21040;&#26356;&#20016;&#23500;&#30340;&#27010;&#24565;&#38598;&#65292;&#25506;&#32034;&#24403;&#21069;&#26816;&#27979;&#21644;&#24341;&#23548;&#31574;&#30053;&#22312;&#36866;&#24403;&#24615;&#12289;&#24189;&#40664;&#12289;&#21019;&#36896;&#21147;&#21644;&#36136;&#37327;&#31561;&#25361;&#25112;&#24615;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#24341;&#23548;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24265;&#20215;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#30340;&#27010;&#24565;&#21521;&#37327;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#23427;&#20204;&#26469;&#25200;&#21160;&#28608;&#27963;&#65292;&#20174;&#32780;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#23558;&#21069;&#20154;&#24037;&#20316;&#30340;&#37325;&#28857;&#20174;&#30495;&#23454;&#24615;&#25193;&#23637;&#21040;&#20102;&#26356;&#20016;&#23500;&#30340;&#27010;&#24565;&#38598;&#65292;&#22914;&#24688;&#24403;&#24615;&#12289;&#24189;&#40664;&#12289;&#21019;&#36896;&#21147;&#21644;&#36136;&#37327;&#65292;&#25506;&#32034;&#24403;&#21069;&#26816;&#27979;&#21644;&#24341;&#23548;&#31574;&#30053;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#30340;&#24037;&#20316;&#31243;&#24230;&#12290;&#20026;&#20102;&#26041;&#20415;&#35780;&#20272;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32771;&#34385;&#27010;&#24565;&#24341;&#23548;&#25104;&#21151;&#31243;&#24230;&#20197;&#21450;&#24341;&#23548;&#27169;&#22411;&#27969;&#30021;&#24615;&#28508;&#22312;&#36864;&#21270;&#30340;&#26032;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#19968;&#20123;&#27010;&#24565;&#22914;&#30495;&#23454;&#24615;&#26356;&#23481;&#26131;&#36890;&#36807;&#24403;&#21069;&#25216;&#26415;&#36827;&#34892;&#24341;&#23548;&#65292;&#20294;&#20687;&#24688;&#24403;&#24615;&#25110;&#24189;&#40664;&#36825;&#26679;&#30340;&#26032;&#27010;&#24565;&#20173;&#28982;&#38590;&#20197;&#24341;&#20986;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14433v1 Announce Type: cross  Abstract: Concept guidance has emerged as a cheap and simple way to control the behavior of language models by probing their hidden representations for concept vectors and using them to perturb activations at inference time. While the focus of previous work has largely been on truthfulness, in this paper we extend this framework to a richer set of concepts such as appropriateness, humor, creativity and quality, and explore to what degree current detection and guidance strategies work in these challenging settings. To facilitate evaluation, we develop a novel metric for concept guidance that takes into account both the success of concept elicitation as well as the potential degradation in fluency of the guided model. Our extensive experiments reveal that while some concepts such as truthfulness more easily allow for guidance with current techniques, novel concepts such as appropriateness or humor either remain difficult to elicit, need extensive 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;KoCoSa&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14428</link><description>&lt;p&gt;
KoCoSa: &#38889;&#25991;&#19978;&#19979;&#25991;&#24863;&#30693;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KoCoSa: Korean Context-aware Sarcasm Detection Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;KoCoSa&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#26159;&#19968;&#31181;&#35328;&#35821;&#35773;&#21050;&#30340;&#26041;&#24335;&#65292;&#25351;&#30340;&#26159;&#26377;&#20154;&#35828;&#20102;&#21644;&#20182;&#20204;&#30340;&#26412;&#24847;&#30456;&#21453;&#30340;&#35805;&#65292;&#36890;&#24120;&#26159;&#20026;&#20102;&#22066;&#31505;&#19968;&#20010;&#20154;&#12289;&#24773;&#20917;&#25110;&#24819;&#27861;&#12290;&#26816;&#27979;&#23545;&#35805;&#20013;&#30340;&#35773;&#21050;&#36890;&#24120;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#26816;&#27979;&#35773;&#21050;&#24212;&#35813;&#21453;&#26144;&#19978;&#19979;&#25991;&#65288;&#21363;&#23545;&#35805;&#21382;&#21490;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#26032;&#25968;&#25454;&#38598;KoCoSa&#65288;&#38889;&#25991;&#19978;&#19979;&#25991;&#24863;&#30693;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#65289;&#65292;&#21253;&#25324;12.8K&#20010;&#26085;&#24120;&#38889;&#25991;&#23545;&#35805;&#20197;&#21450;&#35813;&#20219;&#21153;&#22312;&#26368;&#21518;&#19968;&#27425;&#22238;&#22797;&#19978;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#26500;&#24314;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65306;1&#65289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#28304;&#23545;&#35805;&#20013;&#29983;&#25104;&#26032;&#30340;&#35773;&#21050;&#23545;&#35805;&#65292;2&#65289;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#28388;&#24322;&#24120;&#21644;&#26377;&#27602;&#23545;&#35805;&#65292;3&#65289;&#20026;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#38024;&#23545;&#38889;&#25991;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#22522;&#32447;&#65292;&#35813;&#22522;&#32447;&#26159;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14428v1 Announce Type: cross  Abstract: Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Text-to-Pressure&#65288;T2P&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#22320;&#38754;&#21387;&#21147;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#19982;&#29983;&#25104;&#21160;&#20316;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14427</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#20154;&#31867;&#27963;&#21160;&#30340;&#22320;&#38754;&#21387;&#21147;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14427
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Text-to-Pressure&#65288;T2P&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#22320;&#38754;&#21387;&#21147;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#19982;&#29983;&#25104;&#21160;&#20316;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#20013;&#65292;&#20026;&#35757;&#32451;&#39640;&#25928;&#27169;&#22411;&#65292;&#24517;&#39035;&#26377;&#22823;&#37327;&#30340;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#29289;&#29702;&#20256;&#24863;&#22120;&#33719;&#21462;&#22320;&#38754;&#21387;&#21147;&#25968;&#25454;&#26412;&#36523;&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12289;&#32791;&#26102;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text-to-Pressure&#65288;T2P&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20154;&#31867;&#27963;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#22823;&#37327;&#22320;&#38754;&#21387;&#21147;&#24207;&#21015;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#30690;&#37327;&#37327;&#21270;&#19982;&#31616;&#21333;&#25991;&#26412;&#26465;&#20214;&#33258;&#22238;&#24402;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#20043;&#38388;&#30340;&#31163;&#25955;&#28508;&#22312;&#30456;&#20851;&#24615;&#33719;&#24471;&#39640;&#36136;&#37327;&#29983;&#25104;&#30340;&#21387;&#21147;&#24207;&#21015;&#19982;&#21387;&#21147;&#22320;&#22270;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#19982;&#29983;&#25104;&#21160;&#20316;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#34920;&#29616;&#65292;R squared &#20540;&#20026;0.722&#65292;Masked R squared &#20540;&#20026;0.892&#65292;FID &#20998;&#25968;&#20026;1.83&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14427v1 Announce Type: cross  Abstract: In human activity recognition (HAR), the availability of substantial ground truth is necessary for training efficient models. However, acquiring ground pressure data through physical sensors itself can be cost-prohibitive, time-consuming. To address this critical need, we introduce Text-to-Pressure (T2P), a framework designed to generate extensive ground pressure sequences from textual descriptions of human activities using deep learning techniques. We show that the combination of vector quantization of sensor data along with simple text conditioned auto regressive strategy allows us to obtain high-quality generated pressure sequences from textual descriptions with the help of discrete latent correlation between text and pressure maps. We achieved comparable performance on the consistency between text and generated motion with an R squared value of 0.722, Masked R squared value of 0.892, and FID score of 1.83. Additionally, we trained 
&lt;/p&gt;</description></item><item><title>J-UniMorph&#36890;&#36807;&#25552;&#20379;&#26356;&#24191;&#27867;&#19988;&#26356;&#39057;&#32321;&#20351;&#29992;&#30340;&#21160;&#35789;&#24418;&#24335;&#65292;&#21253;&#25324;&#25964;&#35821;&#12289;&#19968;&#31995;&#21015;&#31036;&#35980;&#27700;&#24179;&#21644;&#20854;&#20182;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#65292;&#24378;&#35843;&#20102;&#26085;&#35821;&#30340;&#29420;&#29305;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.14411</link><description>&lt;p&gt;
J-UniMorph: &#36890;&#36807;&#36890;&#29992;&#29305;&#24449;&#27169;&#24335;&#23545;&#26085;&#35821;&#36827;&#34892;&#24418;&#24577;&#23398;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
J-UniMorph: Japanese Morphological Annotation through the Universal Feature Schema
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14411
&lt;/p&gt;
&lt;p&gt;
J-UniMorph&#36890;&#36807;&#25552;&#20379;&#26356;&#24191;&#27867;&#19988;&#26356;&#39057;&#32321;&#20351;&#29992;&#30340;&#21160;&#35789;&#24418;&#24335;&#65292;&#21253;&#25324;&#25964;&#35821;&#12289;&#19968;&#31995;&#21015;&#31036;&#35980;&#27700;&#24179;&#21644;&#20854;&#20182;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#65292;&#24378;&#35843;&#20102;&#26085;&#35821;&#30340;&#29420;&#29305;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;UniMorph&#29305;&#24449;&#27169;&#24335;&#24320;&#21457;&#30340;&#26085;&#35821;&#24418;&#24577;&#23398;&#25968;&#25454;&#38598;J-UniMorph&#12290;&#35813;&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#36825;&#31181;&#35789;&#35821;&#20957;&#32858;&#24615;&#35821;&#35328;&#29305;&#26377;&#30340;&#29420;&#29305;&#21644;&#20016;&#23500;&#30340;&#21160;&#35789;&#24418;&#24335;&#12290;J-UniMorph&#19982;&#29616;&#26377;&#30340;&#20174;&#32500;&#22522;&#35789;&#20856;&#33258;&#21160;&#25552;&#21462;&#30340;UniMorph&#26085;&#35821;&#23376;&#38598;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#24120;&#65292;&#32500;&#22522;&#35789;&#20856;&#29256;&#26412;&#27599;&#20010;&#21333;&#35789;&#24179;&#22343;&#21253;&#21547;&#32422;12&#20010;&#23624;&#25240;&#24418;&#24335;&#65292;&#24182;&#19988;&#20027;&#35201;&#20197;[&#21517;&#35789;]+&#12377;&#12427;(&#20570;-&#29616;&#22312;&#26102;)&#30340;&#21517;&#35789;&#24615;&#21160;&#35789;&#20026;&#20027;&#23548;&#12290;&#20174;&#24418;&#24577;&#23398;&#19978;&#35762;&#65292;&#36825;&#31181;&#24418;&#24335;&#31561;&#21516;&#20110;&#21160;&#35789;&#12377;&#12427;(&#20570;)&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;J-UniMorph&#25506;&#32034;&#20102;&#26356;&#24191;&#27867;&#19988;&#26356;&#24120;&#29992;&#30340;&#21160;&#35789;&#24418;&#24335;&#33539;&#22260;&#65292;&#24179;&#22343;&#20026;&#27599;&#20010;&#21333;&#35789;&#25552;&#20379;&#20102;118&#20010;&#23624;&#25240;&#24418;&#24335;&#12290;&#23427;&#21253;&#25324;&#25964;&#35821;&#12289;&#19968;&#31995;&#21015;&#31036;&#35980;&#27700;&#24179;&#20197;&#21450;&#20854;&#20182;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#65292;&#24378;&#35843;&#20102;&#26085;&#35821;&#30340;&#29420;&#29305;&#29305;&#28857;&#12290;&#26412;&#25991;&#21576;&#29616;&#20102;J-UniMorph&#30340;&#35814;&#32454;&#32479;&#35745;&#25968;&#25454;&#21644;&#29305;&#24449;&#65292;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14411v1 Announce Type: new  Abstract: We introduce a Japanese Morphology dataset, J-UniMorph, developed based on the UniMorph feature schema. This dataset addresses the unique and rich verb forms characteristic of the language's agglutinative nature. J-UniMorph distinguishes itself from the existing Japanese subset of UniMorph, which is automatically extracted from Wiktionary. On average, the Wiktionary Edition features around 12 inflected forms for each word and is primarily dominated by denominal verbs (i.e., [noun] +suru (do-PRS)). Morphologically, this form is equivalent to the verb suru (do). In contrast, J-UniMorph explores a much broader and more frequently used range of verb forms, offering 118 inflected forms for each word on average. It includes honorifics, a range of politeness levels, and other linguistic nuances, emphasizing the distinctive characteristics of the Japanese language. This paper presents detailed statistics and characteristics of J-UniMorph, compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;RALMs&#23545;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#26469;&#28304;&#38388;&#30340;&#20914;&#31361;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#20250;&#20559;&#21521;&#38169;&#35823;&#30340;&#20869;&#37096;&#35760;&#24518;&#12290;</title><link>https://arxiv.org/abs/2402.14409</link><description>&lt;p&gt;
&#30693;&#35782;&#20043;&#38388;&#30340;&#25289;&#38191;&#25112;: &#25506;&#32034;&#21644;&#35299;&#20915;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;RALMs&#23545;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#26469;&#28304;&#38388;&#30340;&#20914;&#31361;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#20250;&#20559;&#21521;&#38169;&#35823;&#30340;&#20869;&#37096;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#24050;&#32463;&#22312;&#36890;&#36807;&#20174;&#22806;&#37096;&#26469;&#28304;&#26816;&#32034;&#35777;&#25454;&#26469;&#20248;&#21270;&#21644;&#25193;&#23637;&#20854;&#20869;&#37096;&#35760;&#24518;&#26041;&#38754;&#34920;&#29616;&#20986;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;RALMs&#22312;&#23558;&#20869;&#37096;&#35760;&#24518;&#19982;&#22806;&#37096;&#26469;&#28304;&#25972;&#21512;&#26102;&#24517;&#28982;&#20250;&#36935;&#21040;&#30693;&#35782;&#20914;&#31361;&#12290;&#30693;&#35782;&#20914;&#31361;&#20250;&#20351;RALMs&#38519;&#20837;&#30693;&#35782;&#20043;&#38388;&#30340;&#25289;&#38191;&#25112;&#65292;&#38480;&#21046;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#25506;&#32034;&#21644;&#35299;&#20915;RALMs&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#32500;&#24230;&#19978;&#30340;&#30693;&#35782;&#20914;&#31361;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#20197;&#19979;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;RALMs&#30340;&#34892;&#20026;&#21644;&#20559;&#22909;&#65306;&#65288;1&#65289;&#20869;&#37096;&#35760;&#24518;&#19982;&#22806;&#37096;&#26469;&#28304;&#20043;&#38388;&#30340;&#20914;&#31361;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#37011;&#23425;-&#20811;&#40065;&#26684;&#25928;&#24212;&#30340;&#22686;&#24378;&#65292;&#26356;&#24378;&#22823;&#30340;RALMs&#20250;&#25345;&#32493;&#20559;&#29233;&#20854;&#38169;&#35823;&#30340;&#20869;&#37096;&#35760;&#24518;&#65292;&#21363;&#20351;&#25552;&#20379;&#20102;&#27491;&#30830;&#30340;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;RALMs&#36824;&#34920;&#29616;&#20986;&#19968;&#31181;&#21487;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14409v1 Announce Type: cross  Abstract: Retrieval-augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources. However, RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability. In this paper, we focus on exploring and resolving knowledge conflicts in RALMs. First, we present an evaluation framework for assessing knowledge conflicts across various dimensions. Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided. Besides, RALMs exhibit an availability
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35789;&#27719;&#21305;&#37197;&#23558;BERT&#33021;&#21147;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#26377;&#25928;&#24357;&#21512;&#20302;&#36164;&#28304;&#35821;&#35328;&#35757;&#32451;&#22256;&#38590;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.14408</link><description>&lt;p&gt;
&#23558;BERT&#33021;&#21147;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#20351;&#29992;&#35789;&#27719;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14408
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35789;&#27719;&#21305;&#37197;&#23558;BERT&#33021;&#21147;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#26377;&#25928;&#24357;&#21512;&#20302;&#36164;&#28304;&#35821;&#35328;&#35757;&#32451;&#22256;&#38590;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#39046;&#22495;&#65292;&#20854;&#20013;&#26368;&#20026;&#26174;&#33879;&#30340;&#26159;BERT&#65288;&#21452;&#21521;&#32534;&#30721;&#22120;&#26469;&#33258;Transformer&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#20173;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#26377;&#38480;&#30340;&#25968;&#25454;&#38459;&#30861;&#20102;&#36825;&#31867;&#27169;&#22411;&#30340;&#26377;&#25928;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35789;&#27719;&#21305;&#37197;&#23558;BERT&#30340;&#33021;&#21147;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#35199;&#37324;&#35199;&#20122;&#35821;&#21644;&#21345;&#33298;&#27604;&#20122;&#35821;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#30446;&#26631;&#35821;&#35328;&#20165;&#26377;&#24456;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20063;&#33021;&#25913;&#21892;BERT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#35813;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;BERT&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#24471;&#20808;&#36827;&#30340;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#26356;&#20855;&#27665;&#20027;&#24615;&#22320;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14408v1 Announce Type: new  Abstract: Pre-trained language models have revolutionized the natural language understanding landscape, most notably BERT (Bidirectional Encoder Representations from Transformers). However, a significant challenge remains for low-resource languages, where limited data hinders the effective training of such models. This work presents a novel approach to bridge this gap by transferring BERT capabilities from high-resource to low-resource languages using vocabulary matching. We conduct experiments on the Silesian and Kashubian languages and demonstrate the effectiveness of our approach to improve the performance of BERT models even when the target language has minimal training data. Our results highlight the potential of the proposed technique to effectively train BERT models for low-resource languages, thus democratizing access to advanced language understanding models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#26377;&#20851;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#35813;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14404</link><description>&lt;p&gt;
&#22312;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#26512;&#27010;&#24565;&#34920;&#36798;&#65306;&#20511;&#21161;&#21453;&#21521;&#35789;&#20856;&#25506;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14404
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#26377;&#20851;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#35813;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#26597;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#26410;&#35299;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#26469;&#25506;&#26597;LLMs&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#35821;&#35328;&#25551;&#36848;&#20013;&#26263;&#31034;&#30340;&#23545;&#35937;&#27010;&#24565;&#30340;&#26415;&#35821;&#12290;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#31283;&#20581;&#22320;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#20851;&#20110;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#65292;&#23613;&#31649;&#27169;&#22411;&#22312;&#21477;&#27861;&#27867;&#21270;&#34892;&#20026;&#19978;&#34920;&#29616;&#30456;&#20284;&#12290;&#25506;&#32034;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#20351;&#29992;&#25551;&#36848;$\Rightarrow$&#21333;&#35789;&#31034;&#20363;&#21487;&#33021;&#20250;&#35825;&#23548;&#20986;&#36229;&#36234;&#20219;&#21153;&#26500;&#22411;&#34920;&#38754;&#24046;&#24322;&#30340;&#27867;&#21270;&#65292;&#24182;&#20419;&#36827;&#27169;&#22411;&#23545;&#26356;&#24191;&#27867;&#30340;&#20849;&#21516;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14404v1 Announce Type: cross  Abstract: Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commons
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21382;&#21490;&#38142;&#25512;&#29702;&#26469;&#22686;&#24378;&#26102;&#38388;&#30693;&#35782;&#22270;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#21033;&#29992;&#39640;&#38454;&#21382;&#21490;&#20449;&#24687;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#21382;&#21490;&#20449;&#24687;&#21644;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.14382</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#21382;&#21490;&#38142;&#25512;&#29702;&#22686;&#24378;&#26102;&#38388;&#30693;&#35782;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21382;&#21490;&#38142;&#25512;&#29702;&#26469;&#22686;&#24378;&#26102;&#38388;&#30693;&#35782;&#22270;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#21033;&#29992;&#39640;&#38454;&#21382;&#21490;&#20449;&#24687;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#21382;&#21490;&#20449;&#24687;&#21644;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#39044;&#27979;&#26088;&#22312;&#22522;&#20110;&#32473;&#23450;&#21382;&#21490;&#25968;&#25454;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#12290; &#26368;&#36817;&#30340;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#25797;&#38271;&#25429;&#25417;TKGs&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#32570;&#20047;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#12290;&#22914;&#20170;&#65292;&#38543;&#30528;LLMs&#30340;&#28608;&#22686;&#65292;&#22522;&#20110;LLMs&#30340;TKG&#39044;&#27979;&#27169;&#22411;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;LLMs&#30340;&#27169;&#22411;&#23384;&#22312;&#19977;&#20010;&#32570;&#28857;&#65306;&#65288;1&#65289;&#23427;&#21482;&#20851;&#27880;&#31532;&#19968;&#38454;&#21382;&#21490;&#20197;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#24573;&#30053;&#20102;&#39640;&#38454;&#21382;&#21490;&#20449;&#24687;&#65292;&#23548;&#33268;&#25552;&#20379;&#32473;LLMs&#30340;&#20449;&#24687;&#26497;&#20026;&#26377;&#38480;&#12290;&#65288;2&#65289;&#22312;&#22823;&#37327;&#21382;&#21490;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#24456;&#38590;&#36798;&#21040;&#26368;&#20339;&#25512;&#29702;&#24615;&#33021;&#12290;&#65288;3&#65289;&#23545;&#20110;TKG&#39044;&#27979;&#65292;&#21333;&#29420;&#20351;&#29992;LLM&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#24212;&#23545;&#21069;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21382;&#21490;&#38142;&#65288;CoH&#65289;&#25512;&#29702;&#65292;&#36880;&#27493;&#25506;&#32034;&#39640;&#38454;&#21382;&#21490;&#65292;&#23454;&#29616;LLMs&#23545;TKGs&#19978;&#39640;&#38454;&#21382;&#21490;&#20449;&#24687;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14382v1 Announce Type: new  Abstract: Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories. Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities. Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged. However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited. (2) LLMs struggle with optimal reasoning performance under heavy historical information loads. (3) For TKG prediction, the temporal reasoning capability of LLM alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TK
&lt;/p&gt;</description></item><item><title>Rad predstavlja novi jezi&#269;ki model za srpski jezik zasnovan na transformerima, obu&#269;en na resursima Dru&#353;tva za jezi&#269;ke resurse i tehnologije, koji &#263;e biti upore&#273;en sa deset odabranih modela vektorizacije na &#269;etiri zadatka obrade prirodnog jezika.</title><link>https://arxiv.org/abs/2402.14379</link><description>&lt;p&gt;
Novi jezi&#269;ki modeli za srpski jezik
&lt;/p&gt;
&lt;p&gt;
Novi jezi\v{c}ki modeli za srpski jezik
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14379
&lt;/p&gt;
&lt;p&gt;
Rad predstavlja novi jezi&#269;ki model za srpski jezik zasnovan na transformerima, obu&#269;en na resursima Dru&#353;tva za jezi&#269;ke resurse i tehnologije, koji &#263;e biti upore&#273;en sa deset odabranih modela vektorizacije na &#269;etiri zadatka obrade prirodnog jezika.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Rad &#263;e ukratko predstaviti istoriju razvoja modela jezika zasnovanih na transformatorima za srpski jezik. Tako&#273;e &#263;e biti predstavljeni novi modeli za generisanje teksta i vektorizaciju, obu&#269;eni na resursima Dru&#353;tva za jezi&#269;ke resurse i tehnologije. Bi&#263;e upore&#273;eno deset izabranih modela vektorizacije za srpski jezik, uklju&#269;uju&#263;i dva nova, na &#269;etiri zadatka obrade prirodnog jezika. Rad &#263;e analizirati koji modeli su najbolji za svaki izabrani zadatak, kako njihova veli&#269;ina i veli&#269;ina skupova za obuku uti&#269;u na performanse na tim zadacima, i koji je optimalni skup za obuku najboljih jezi&#269;kih modela za srpski jezik.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14379v1 Announce Type: new  Abstract: The paper will briefly present the development history of transformer-based language models for the Serbian language. Several new models for text generation and vectorization, trained on the resources of the Society for Language Resources and Technologies, will also be presented. Ten selected vectorization models for Serbian, including two new ones, will be compared on four natural language processing tasks. Paper will analyze which models are the best for each selected task, how does their size and the size of their training sets affect the performance on those tasks, and what is the optimal setting to train the best language models for the Serbian language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SLCoLM&#65292;&#19968;&#20010;&#27169;&#22411;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#8220;&#35757;&#32451;-&#25351;&#23548;-&#39044;&#27979;&#8221;&#31574;&#30053;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#32531;&#35299;&#20102;&#38271;&#23614;&#25968;&#25454;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#23454;&#20307;&#20851;&#31995;&#30340;&#25277;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.14373</link><description>&lt;p&gt;
&#23567;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#23454;&#20307;&#20851;&#31995;&#25277;&#21462;&#20013;&#26159;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#33391;&#22909;&#21521;&#23548;
&lt;/p&gt;
&lt;p&gt;
Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SLCoLM&#65292;&#19968;&#20010;&#27169;&#22411;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#8220;&#35757;&#32451;-&#25351;&#23548;-&#39044;&#27979;&#8221;&#31574;&#30053;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#32531;&#35299;&#20102;&#38271;&#23614;&#25968;&#25454;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#23454;&#20307;&#20851;&#31995;&#30340;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#12290;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#38271;&#23614;&#25968;&#25454;&#65292;&#28982;&#32780;&#30446;&#21069;&#24456;&#23569;&#26377;&#20851;&#27880;&#20351;&#29992;LLM&#26041;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLCoLM&#65292;&#19968;&#20010;&#27169;&#22411;&#21327;&#20316;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#38271;&#23614;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#35757;&#32451;-&#25351;&#23548;-&#39044;&#27979;&#8221;&#31574;&#30053;&#26469;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#20854;&#20013;&#19968;&#20010;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;PLM&#26694;&#26550;&#20805;&#24403;&#23548;&#24072;&#65292;&#23558;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#21040;LLM&#65292;&#24182;&#25351;&#23548;LLM&#25191;&#34892;RE&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#23500;&#21547;&#20851;&#31995;&#31867;&#22411;&#30340;RE&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26412;&#25991;&#20013;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#38271;&#23614;&#20851;&#31995;&#31867;&#22411;&#30340;RE&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14373v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have been successful in relational extraction (RE) tasks, especially in the few-shot learning. An important problem in the field of RE is long-tailed data, while not much attention is currently paid to this problem using LLM approaches. Therefore, in this paper, we propose SLCoLM, a model collaboration framework, to mitigate the data long-tail problem. In our framework, We use the ``\textit{Training-Guide-Predict}'' strategy to combine the strengths of pre-trained language models (PLMs) and LLMs, where a task-specific PLM framework acts as a tutor, transfers task knowledge to the LLM, and guides the LLM in performing RE tasks. Our experiments on a RE dataset rich in relation types show that the approach in this paper facilitates RE of long-tail relation types.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#24863;&#30693;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;FM&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25688;&#35201;&#36827;&#34892;&#39640;&#32423;&#35821;&#20041;&#21305;&#37197;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#35780;&#20272;&#31185;&#23398;&#25688;&#35201;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14359</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31185;&#23398;&#25688;&#35201;&#35780;&#20272;&#65306;&#22522;&#20110;&#26041;&#38754;&#24863;&#30693;&#22522;&#20934;&#30340;&#21487;&#35299;&#37322;&#24230;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#24863;&#30693;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;FM&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25688;&#35201;&#36827;&#34892;&#39640;&#32423;&#35821;&#20041;&#21305;&#37197;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#35780;&#20272;&#31185;&#23398;&#25688;&#35201;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25688;&#35201;&#33021;&#21147;&#22312;&#19968;&#33324;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#39564;&#35777;&#65292;&#20294;&#23427;&#20204;&#22312;&#28041;&#21450;&#22797;&#26434;&#21477;&#23376;&#21644;&#19987;&#19994;&#30693;&#35782;&#30340;&#31185;&#23398;&#35821;&#26009;&#24211;&#20013;&#30340;&#20351;&#29992;&#36739;&#23569;&#34987;&#35780;&#20272;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31185;&#23398;&#25688;&#35201;&#30340;&#27010;&#24565;&#21644;&#23454;&#39564;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#65288;&#22914;$n$-gram&#12289;&#23884;&#20837;&#27604;&#36739;&#21644;&#38382;&#31572;&#65289;&#22312;&#25552;&#20379;&#35299;&#37322;&#12289;&#25226;&#25569;&#31185;&#23398;&#27010;&#24565;&#25110;&#35782;&#21035;&#20851;&#38190;&#20869;&#23481;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Facet-aware Metric&#65288;FM&#65289;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#39640;&#32423;&#35821;&#20041;&#21305;&#37197;&#65292;&#26681;&#25454;&#19981;&#21516;&#26041;&#38754;&#35780;&#20272;&#25688;&#35201;&#12290;&#36825;&#31181;&#38754;&#21521;&#26041;&#38754;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#35780;&#20272;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#65292;&#20026;&#25688;&#35201;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#37492;&#20110;&#35813;&#39046;&#22495;&#32570;&#20047;&#35780;&#20272;&#22522;&#20934;&#65292;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#22522;&#20110;&#26041;&#38754;&#30340;&#31185;&#23398;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;FD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14359v1 Announce Type: new  Abstract: The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35762;&#25925;&#20107;&#26469;&#34920;&#36798;&#22266;&#26377;&#30340;&#24120;&#35782;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25925;&#20107;&#20248;&#20110;&#35268;&#21017;&#20316;&#20026;&#20174;LLMs&#26816;&#32034;&#24120;&#35782;&#30340;&#34920;&#36798;&#24418;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14355</link><description>&lt;p&gt;
&#26159;&#35268;&#21017;&#22909;&#36824;&#26159;&#25925;&#20107;&#26356;&#22909;&#30340;&#24120;&#35782;&#34920;&#36798;&#26041;&#24335;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35762;&#25925;&#20107;&#26469;&#34920;&#36798;&#22266;&#26377;&#30340;&#24120;&#35782;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25925;&#20107;&#20248;&#20110;&#35268;&#21017;&#20316;&#20026;&#20174;LLMs&#26816;&#32034;&#24120;&#35782;&#30340;&#34920;&#36798;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20855;&#22791;&#24120;&#35782;&#30340;&#26426;&#22120;&#19968;&#30452;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#20110;&#24120;&#35782;&#35268;&#21017;&#30340;&#25253;&#21578;&#20559;&#24046;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#24120;&#35782;&#25512;&#29702;&#30340;&#26292;&#38706;&#20559;&#24046;&#25152;&#33268;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#36890;&#36807;&#25925;&#20107;&#38544;&#21547;&#22320;&#20256;&#36882;&#21644;&#20256;&#25215;&#24120;&#35782;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#35762;&#25925;&#20107;&#26469;&#34920;&#36798;&#22266;&#26377;&#30340;&#24120;&#35782;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#20102;&#25925;&#20107;&#21644;&#35268;&#21017;&#22312;&#20174;LLMs&#26816;&#32034;&#21644;&#21033;&#29992;&#24120;&#35782;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#22312;28&#20010;&#24120;&#35782;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25925;&#20107;&#20248;&#20110;&#35268;&#21017;&#20316;&#20026;&#20174;LLMs&#26816;&#32034;&#24120;&#35782;&#30340;&#34920;&#36798;&#24418;&#24335;&#65292;&#22312;&#29983;&#25104;&#20449;&#24515;&#21644;&#24120;&#35782;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25925;&#20107;&#26159;&#22238;&#31572;&#26377;&#20851;&#26085;&#24120;&#20107;&#20214;&#30340;&#38382;&#39064;&#30340;&#26356;&#26377;&#25928;&#24120;&#35782;&#34920;&#36798;&#26041;&#24335;&#65292;&#32780;&#35268;&#21017;&#23545;&#20110;&#31185;&#23398;&#38382;&#39064;&#26356;&#26377;&#25928;&#12290;&#36825;&#19982;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#24120;&#35782;&#25253;&#21578;&#20559;&#24046;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14355v1 Announce Type: new  Abstract: Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22788;&#29702;&#24341;&#21457;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#32654;&#29702;&#30001;&#30340;&#26041;&#27861;&#65292;&#23454;&#26045;&#20102;&#20351;&#29992;&#29109;&#20998;&#25968;&#21644;&#27169;&#22411;&#20808;&#39564;&#20449;&#24565;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30456;&#23545;&#20110;&#25932;&#23545;&#29702;&#30001;&#30340;&#31283;&#20581;&#24615;&#33021;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.14337</link><description>&lt;p&gt;
AURA&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#30340;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14337
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22788;&#29702;&#24341;&#21457;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#32654;&#29702;&#30001;&#30340;&#26041;&#27861;&#65292;&#23454;&#26045;&#20102;&#20351;&#29992;&#29109;&#20998;&#25968;&#21644;&#27169;&#22411;&#20808;&#39564;&#20449;&#24565;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30456;&#23545;&#20110;&#25932;&#23545;&#29702;&#30001;&#30340;&#31283;&#20581;&#24615;&#33021;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31574;&#32972;&#21518;&#30340;&#29702;&#30001;&#19981;&#20165;&#35299;&#37322;&#20102;&#27169;&#22411;&#20915;&#31574;&#65292;&#32780;&#19988;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#26080;&#25032;&#21487;&#20987;&#30340;&#29702;&#30001;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#27492;&#22806;&#65292;&#20272;&#35745;&#29702;&#30001;&#36275;&#22815;&#24544;&#23454;&#20197;&#40723;&#21169;&#27169;&#22411;&#34920;&#29616;&#30340;&#31243;&#24230;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25512;&#29702;&#20219;&#21153;&#36890;&#24120;&#36843;&#20351;&#27169;&#22411;&#22312;&#19981;&#29702;&#24819;&#30340;&#29702;&#30001;&#19979;&#36755;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#24182;&#19988;&#19982;&#27169;&#22411;&#23436;&#20840;&#26377;&#33021;&#21147;&#30340;&#24773;&#20917;&#30456;&#27604;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22914;&#20309;&#24212;&#23545;&#24341;&#21457;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#32654;&#29702;&#30001;&#12290;&#25105;&#20204;&#39318;&#20808;&#29992;&#32473;&#23450;&#29702;&#30001;&#30340;&#29109;&#20998;&#25968;&#26469;&#23450;&#20041;&#27169;&#31946;&#30340;&#29702;&#30001;&#65292;&#20351;&#29992;&#27169;&#22411;&#20808;&#39564;&#20449;&#24565;&#20316;&#20026;&#20449;&#24687;&#37327;&#12290;&#28982;&#21518;&#26681;&#25454;&#29702;&#30001;&#30340;&#27169;&#31946;&#24615;&#26469;&#24341;&#23548;&#27169;&#22411;&#36873;&#25321;&#20004;&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#35770;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29702;&#30001;&#30340;&#25932;&#23545;&#36136;&#37327;&#20135;&#29983;&#20102;&#31283;&#20581;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14337v1 Announce Type: new  Abstract: Rationales behind answers not only explain model decisions but boost language models to reason well on complex reasoning tasks. However, obtaining impeccable rationales is often impossible. Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance. Thus, such reasoning tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of. In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty. We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness. We then guide models to select one of two different reasoning models according to the ambiguity of rationales. We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationale
&lt;/p&gt;</description></item><item><title>INSTRUCTIR&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.14334</link><description>&lt;p&gt;
INSTRUCTIR&#65306;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#25351;&#20196;&#36981;&#24490;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14334
&lt;/p&gt;
&lt;p&gt;
INSTRUCTIR&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23558;&#25628;&#32034;&#30446;&#26631;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#38656;&#27714;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26816;&#32034;&#22120;&#36890;&#24120;&#21482;&#20248;&#20808;&#32771;&#34385;&#26597;&#35810;&#20449;&#24687;&#65292;&#32780;&#19981;&#28145;&#20837;&#20102;&#35299;&#29992;&#25143;&#30340;&#39044;&#26399;&#25628;&#32034;&#19978;&#19979;&#25991;&#12290;&#22686;&#24378;&#26816;&#32034;&#22120;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#65292;&#26377;&#26395;&#20135;&#29983;&#26356;&#23545;&#40784;&#30340;&#25628;&#32034;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#25351;&#20196;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#22312;&#20219;&#21153;&#25551;&#36848;&#26684;&#24335;&#19978;&#65292;&#24573;&#30053;&#20102;&#22810;&#26679;&#21270;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#25628;&#32034;&#22330;&#26223;&#30340;&#24191;&#27867;&#32972;&#26223;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#35780;&#20272;&#30340;&#20027;&#27969;&#22522;&#20934;&#32570;&#20047;&#26126;&#30830;&#30340;&#23450;&#21046;&#20197;&#35780;&#20272;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#23616;&#38480;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;INSTRUCTIR&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#38024;&#23545;&#29992;&#25143;&#37327;&#36523;&#23450;&#21046;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14334v1 Announce Type: new  Abstract: Despite the critical need to align search targets with users' intention, retrievers often only prioritize query information without delving into the users' intended search context. Enhancing the capability of retrievers to understand intentions and preferences of users, akin to language model instructions, has the potential to yield more aligned search targets. Prior studies restrict the application of instructions in information retrieval to a task description format, neglecting the broader context of diverse and evolving search scenarios. Furthermore, the prevailing benchmarks utilized for evaluation lack explicit tailoring to assess instruction-following ability, thereby hindering progress in this field. In response to these limitations, we propose a novel benchmark,INSTRUCTIR, specifically designed to evaluate instruction-following ability in information retrieval tasks. Our approach focuses on user-aligned instructions tailored to e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807; Logit Lens &#21644;&#24178;&#39044;&#23454;&#39564;&#25581;&#31034;&#20102;LLMs&#20013;&#38544;&#24615;&#25512;&#29702;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20462;&#34917;&#32452;&#21512;&#25512;&#29702;&#38169;&#35823;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14328</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20462;&#34917;LLMs&#20013;&#30340;&#32452;&#25104;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Understanding and Patching Compositional Reasoning in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807; Logit Lens &#21644;&#24178;&#39044;&#23454;&#39564;&#25581;&#31034;&#20102;LLMs&#20013;&#38544;&#24615;&#25512;&#29702;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20462;&#34917;&#32452;&#21512;&#25512;&#29702;&#38169;&#35823;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807; Logit Lens &#21644;&#24178;&#39044;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#37096;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#25968;&#25512;&#29702;&#22833;&#36133;&#28304;&#33258;&#20110;&#19981;&#27491;&#30830;&#29983;&#25104;&#25110;&#21033;&#29992;&#30340;&#38544;&#24615;&#25512;&#29702;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#38544;&#24615;&#25512;&#29702;&#32467;&#26524;&#22312;&#20013;&#38388;&#23618;&#20013;&#30340;&#20986;&#29616;&#65292;&#24182;&#23545;&#26368;&#32456;&#26174;&#24335;&#25512;&#29702;&#32467;&#26524;&#30340;&#24418;&#25104;&#36215;&#21040;&#22240;&#26524;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102; MHSA &#27169;&#22359;&#22312;&#36825;&#20123;&#23618;&#20013;&#30340;&#23384;&#22312;&#65292;&#25104;&#20026;&#20934;&#30830;&#29983;&#25104;&#21644;&#21033;&#29992;&#38544;&#24615;&#25512;&#29702;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#20197;&#19978;&#21457;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102; CREME&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#36753;&#20301;&#20110;&#30340; MHSA &#27169;&#22359;&#26469;&#20462;&#34917;&#32452;&#21512;&#25512;&#29702;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14328v1 Announce Type: new  Abstract: LLMs have marked a revolutonary shift, yet they falter when faced with compositional reasoning tasks. Our research embarks on a quest to uncover the root causes of compositional reasoning failures of LLMs, uncovering that most of them stem from the improperly generated or leveraged implicit reasoning results. Inspired by our empirical findings, we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of LLMs. This deep dive reveals that implicit reasoning results indeed surface within middle layers and play a causative role in shaping the final explicit reasoning results. Our exploration further locates multi-head self-attention (MHSA) modules within these layers, which emerge as the linchpins in accurate generation and leveraing of implicit reasoning results. Grounded on the above findings, we develop CREME, a lightweight method to patch errors in compositional reasoning via editing the located MHSA modu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23558;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14327</link><description>&lt;p&gt;
&#23376;&#23545;&#35937;&#32423;&#22270;&#20687;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subobject-level Image Tokenization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14327
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23558;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#23558;&#22270;&#20687;&#26631;&#35760;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#26041;&#24418;&#34917;&#19969;&#20316;&#20026;&#36755;&#20837;&#21333;&#20803;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#36866;&#24212;&#24615;&#65292;&#24182;&#24573;&#30053;&#20102;&#22266;&#26377;&#30340;&#20687;&#32032;&#20998;&#32452;&#32467;&#26500;&#12290;&#21463;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#37319;&#29992;&#30340;&#23376;&#35789;&#26631;&#35760;&#21270;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#23376;&#23545;&#35937;&#30001;&#36890;&#36807;&#20998;&#21106;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#20998;&#21106;&#20219;&#20309;&#27169;&#22411;&#65289;&#33719;&#24471;&#30340;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#22270;&#20687;&#27573;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#22522;&#20110;&#23376;&#23545;&#35937;&#26631;&#35760;&#21270;&#30340;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#65288;SeqAE&#65289;&#65292;&#23558;&#19981;&#21516;&#22823;&#23567;&#21644;&#24418;&#29366;&#30340;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#23376;&#23545;&#35937;&#23884;&#20837;&#39304;&#36865;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23376;&#23545;&#35937;&#32423;&#21035;&#26631;&#35760;&#21270;&#26174;&#33879;&#20419;&#36827;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14327v1 Announce Type: cross  Abstract: Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning. Empirical results demonstrated that our subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descr
&lt;/p&gt;</description></item><item><title>Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14320</link><description>&lt;p&gt;
Triad: &#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14320
&lt;/p&gt;
&lt;p&gt;
Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#31572;&#30693;&#35782;&#24211;&#20013;&#38382;&#39064;&#30340;&#36816;&#29992;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#23454;&#29616;KBQA&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#20197;&#20219;&#21153;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triad&#65292;&#19968;&#20010;&#21033;&#29992;&#20855;&#26377;&#19977;&#20010;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#36827;&#34892;KBQA&#20219;&#21153;&#12290;&#20195;&#29702;&#34987;&#20998;&#37197;&#19977;&#20010;&#35282;&#33394;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;KBQA&#23376;&#20219;&#21153;&#65306;&#20316;&#20026;&#25484;&#25569;&#21508;&#31181;&#23376;&#20219;&#21153;&#30340;&#36890;&#25165;&#65292;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#32773;&#30340;&#20915;&#31574;&#32773;&#65292;&#20197;&#21450;&#20316;&#20026;&#22238;&#31572;&#24102;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#30340;&#39038;&#38382;&#12290;&#25105;&#20204;&#30340;KBQA&#26694;&#26550;&#22312;&#22235;&#20010;&#38454;&#27573;&#20013;&#25191;&#34892;&#65292;&#28041;&#21450;&#20195;&#29702;&#30340;&#22810;&#37325;&#35282;&#33394;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#27874;&#20848;&#35821;&#25991;&#26412;&#25490;&#21517;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#21644;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#32467;&#21512;&#65292;&#21487;&#20197;&#26500;&#24314;&#26082;&#20307;&#31215;&#23567;&#21448;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#26032;&#25490;&#21517;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.14318</link><description>&lt;p&gt;
&#35780;&#20272;&#27874;&#20848;&#35821;&#25991;&#26412;&#25490;&#21517;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing generalization capability of text ranking models in Polish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14318
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#27874;&#20848;&#35821;&#25991;&#26412;&#25490;&#21517;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#21644;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#32467;&#21512;&#65292;&#21487;&#20197;&#26500;&#24314;&#26082;&#20307;&#31215;&#23567;&#21448;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#26032;&#25490;&#21517;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14318v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25277;&#35937;&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#27491;&#25104;&#20026;&#23558;&#20869;&#37096;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#25216;&#26415;&#12290;&#22312;&#20856;&#22411;&#30340;RAG&#27969;&#27700;&#32447;&#20013;&#65292;&#20351;&#29992;&#19977;&#20010;&#27169;&#22411;&#65292;&#20998;&#21035;&#36127;&#36131;&#26816;&#32034;&#12289;&#37325;&#26032;&#25490;&#21517;&#21644;&#29983;&#25104;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#27874;&#20848;&#35821;&#30340;&#37325;&#26032;&#25490;&#21517;&#38382;&#39064;&#65292;&#30740;&#31350;&#37325;&#26032;&#25490;&#21517;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#32467;&#26524;&#19982;&#21487;&#29992;&#30340;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#27169;&#22411;&#21644;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21033;&#29992;&#20102;&#19968;&#20010;&#30001;41&#20010;&#22810;&#26679;&#21270;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#32452;&#25104;&#30340;&#27874;&#20848;&#35821;&#22522;&#20934;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#37117;&#38590;&#20197;&#22312;&#39046;&#22495;&#22806;&#36827;&#34892;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#21644;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#32467;&#21512;&#20801;&#35768;&#26500;&#24314;&#26082;&#20307;&#31215;&#23567;&#21448;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#26032;&#25490;&#21517;&#22120;&#12290;&#25105;&#20204;&#26368;&#20339;&#30340;&#27169;&#22411;&#20026;&#37325;&#26032;&#24314;&#31435;&#20102;&#37325;&#26032;&#25490;&#21517;&#30340;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14318v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) is becoming an increasingly popular technique for integrating internal knowledge bases with large language models. In a typical RAG pipeline, three models are used, responsible for the retrieval, reranking, and generation stages. In this article, we focus on the reranking problem for the Polish language, examining the performance of rerankers and comparing their results with available retrieval models. We conduct a comprehensive evaluation of existing models and those trained by us, utilizing a benchmark of 41 diverse information retrieval tasks for the Polish language. The results of our experiments show that most models struggle with out-of-domain generalization. However, a combination of effective optimization method and a large training dataset allows for building rerankers that are both compact in size and capable of generalization. The best of our models establishes a new state-of-the-art for re
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#25552;&#31034;-&#35299;&#20915;&#21069;&#25552;&#31034;&#65288;HSP&#65289;&#26041;&#27861;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#35299;&#20915;&#38382;&#39064;&#30340;&#25552;&#31034;&#24182;&#29983;&#25104;&#21253;&#21547;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14310</link><description>&lt;p&gt;
&#25552;&#31034;-&#35299;&#20915;&#21069;&#25552;&#31034;&#65306;&#24341;&#23548;LLMs&#26377;&#25928;&#21033;&#29992;&#32534;&#30721;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14310
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#25552;&#31034;-&#35299;&#20915;&#21069;&#25552;&#31034;&#65288;HSP&#65289;&#26041;&#27861;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#35299;&#20915;&#38382;&#39064;&#30340;&#25552;&#31034;&#24182;&#29983;&#25104;&#21253;&#21547;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#25317;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20294;LLMs&#20173;&#28982;&#38754;&#20020;&#30528;&#26377;&#25928;&#21033;&#29992;&#32534;&#30721;&#30693;&#35782;&#26469;&#21457;&#23637;&#20934;&#30830;&#21644;&#21512;&#20046;&#36923;&#36753;&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25552;&#31034;-&#35299;&#20915;&#21069;&#25552;&#31034;&#65288;HSP&#65289;&#65292;&#35813;&#26041;&#27861;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65288;&#20363;&#22914;&#65292;&#29305;&#23450;&#30693;&#35782;&#25110;&#20851;&#38190;&#24605;&#24819;&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#28982;&#21518;&#29983;&#25104;&#21253;&#21547;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30001;&#20110;HSP&#19982;&#25552;&#31034;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;Chain-of-Thought&#65288;CoT&#65289;&#65289;&#27491;&#20132;&#65292;&#25105;&#20204;&#23558;HSP&#24212;&#29992;&#20110;CoT&#12289;Least-to-Most&#12289;Plan-and-Solve&#21644;Standard&#25552;&#31034;&#12290;&#23545;6&#20010;&#25512;&#29702;&#22522;&#20934;&#21644;4&#20010;&#24320;&#28304;LLMs&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HSP&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65306;&#65288;1&#65289;&#36890;&#36807;&#23558;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#22686;&#24378;&#22411;HSP&#24212;&#29992;&#20110;CoT&#25552;&#31034;&#65292;Llama2-70B-Chat&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;9.7&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14310v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains. Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes. To mitigate this problem, we introduced Hint-before-Solving Prompting (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas) for solving the problem and then generate solutions containing intermediate reasoning steps. Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs demonstrate that HSP can effectively improve the accuracy of reasoning tasks: (1) By applying high-quality hint-enhanced HSP to CoT prompting, Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;TMPT&#26694;&#26550;&#22312;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14298</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65306;&#26032;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Stance Detection: New Datasets and Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;TMPT&#26694;&#26550;&#22312;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20013;&#35782;&#21035;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#30340;&#20844;&#20247;&#24847;&#35265;&#12290;&#20197;&#24448;&#30340;&#31435;&#22330;&#26816;&#27979;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#32431;&#25991;&#26412;&#19978;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25512;&#25991;&#30340;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65292;&#36825;&#22312;&#24403;&#20170;&#24555;&#36895;&#22686;&#38271;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#20154;&#20204;&#32463;&#24120;&#21457;&#24067;&#22810;&#27169;&#24335;&#28040;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;Twitter&#21019;&#24314;&#20102;&#20116;&#20010;&#26032;&#30340;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#31034;&#20363;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30446;&#26631;&#22810;&#27169;&#24335;&#25552;&#31034;&#35843;&#25972;&#65288;TMPT&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#20174;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#23398;&#20064;&#22810;&#27169;&#24335;&#31435;&#22330;&#29305;&#24449;&#12290;&#23545;&#25105;&#20204;&#30340;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TMPT&#22312;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14298v1 Announce Type: new  Abstract: Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today's fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our three benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#24182;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.14296</link><description>&lt;p&gt;
&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#36890;&#36807;&#26657;&#20934;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases of Large Language Models in Stance Detection with Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#24182;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#20250;&#29983;&#25104;&#20559;&#35265;&#31435;&#22330;&#65292;&#36825;&#26159;&#30001;&#20110;&#34394;&#20551;&#24773;&#24863;-&#31435;&#22330;&#30456;&#20851;&#24615;&#21644;&#23545;&#26576;&#20123;&#20010;&#20154;&#21644;&#20027;&#39064;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;LLMs&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#30340;&#20559;&#35265;&#65288;MB-Cal&#65289;&#12290;&#22312;&#20854;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#65292;&#20197;&#20943;&#36731;LLMs&#20135;&#29983;&#30340;&#31435;&#22330;&#25512;&#29702;&#32467;&#26524;&#19978;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;&#26657;&#20934;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#26469;&#30699;&#27491;&#31435;&#22330;&#20559;&#35265;&#12290;&#38024;&#23545;&#30446;&#26631;&#21644;&#38646;&#23556;&#20987;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MB-Cal&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;LLMs&#30340;&#20559;&#35265;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14296v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In which, a novel gated calibration network is devised to mitigate the biases on the stance reasoning results from LLMs. Further, to make the calibration more accurate and generalizable, we construct counterfactual augmented data to rectify stance biases. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#20013;&#36827;&#34892;&#27010;&#24565;&#22270;&#24674;&#22797;&#21644;&#38382;&#31572;&#65292;&#24182;&#25552;&#20986;&#20102;&#19987;&#38376;&#38024;&#23545;&#31185;&#23398;&#22270;&#25512;&#29702;&#21644;QA&#30340;&#26032;&#22522;&#20934;TutorQA&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#38646;-shot&#27010;&#24565;&#22270;&#24674;&#22797;&#21644;TutorQA&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.14293</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;NLP&#25945;&#32946;&#20013;&#30340;&#27010;&#24565;&#22270;&#24674;&#22797;&#21644;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#20013;&#36827;&#34892;&#27010;&#24565;&#22270;&#24674;&#22797;&#21644;&#38382;&#31572;&#65292;&#24182;&#25552;&#20986;&#20102;&#19987;&#38376;&#38024;&#23545;&#31185;&#23398;&#22270;&#25512;&#29702;&#21644;QA&#30340;&#26032;&#22522;&#20934;TutorQA&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#38646;-shot&#27010;&#24565;&#22270;&#24674;&#22797;&#21644;TutorQA&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25945;&#32946;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#39046;&#22495;&#29305;&#23450;&#26597;&#35810;&#30340;&#24212;&#29992;&#65292;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#24320;&#21457;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#27010;&#24565;&#22270;&#24674;&#22797;&#21644;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#21019;&#24314;&#39046;&#22495;&#29305;&#23450;&#27010;&#24565;&#22270;&#26041;&#38754;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#24182;&#20171;&#32461;&#20102;TutorQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#38024;&#23545;&#31185;&#23398;&#22270;&#25512;&#29702;&#21644;QA&#30340;NLP&#22522;&#20934;&#12290;TutorQA&#21253;&#21547;&#20116;&#20010;&#20219;&#21153;&#65292;&#20849;500&#20010;QA&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;TutorQA&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CGLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#27010;&#24565;&#22270;&#19982;LLMs&#38598;&#25104;&#20197;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#30340;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#38646;-shot&#27010;&#24565;&#22270;&#24674;&#22797;&#19982;&#30417;&#30563;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;3%&#12290;&#22312;TutorQA&#20219;&#21153;&#20013;&#65292;LLMs&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;&#39640;&#36798;26%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14293v1 Announce Type: new  Abstract: In the domain of Natural Language Processing (NLP), Large Language Models (LLMs) have demonstrated promise in text-generation tasks. However, their educational applications, particularly for domain-specific queries, remain underexplored. This study investigates LLMs' capabilities in educational scenarios, focusing on concept graph recovery and question-answering (QA). We assess LLMs' zero-shot performance in creating domain-specific concept graphs and introduce TutorQA, a new expert-verified NLP-focused benchmark for scientific graph reasoning and QA. TutorQA consists of five tasks with 500 QA pairs. To tackle TutorQA queries, we present CGLLM, a pipeline integrating concept graphs with LLMs for answering diverse questions. Our results indicate that LLMs' zero-shot concept graph recovery is competitive with supervised methods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMs achieve up to 26% F1 score enhancement. Moreo
&lt;/p&gt;</description></item><item><title>CEV-LM &#26159;&#19968;&#20010;&#36731;&#37327;&#12289;&#21322;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#21463;&#38480;&#21046;&#30340;&#32534;&#36753;&#21521;&#37327;&#25511;&#21046;&#25991;&#26412;&#30340;&#36895;&#24230;&#12289;&#38899;&#37327;&#21644;&#32469;&#22280;&#24230;&#37327;&#65292;&#20174;&#32780;&#26356;&#31934;&#20934;&#22320;&#23450;&#21046;&#29983;&#25104;&#30340;&#25991;&#26412;&#24418;&#29366;&#65292;&#27604;&#29616;&#26377;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25511;&#21046;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14290</link><description>&lt;p&gt;
CEV-LM: &#21463;&#25511;&#32534;&#36753;&#21521;&#37327;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22609;&#36896;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14290
&lt;/p&gt;
&lt;p&gt;
CEV-LM &#26159;&#19968;&#20010;&#36731;&#37327;&#12289;&#21322;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#21463;&#38480;&#21046;&#30340;&#32534;&#36753;&#21521;&#37327;&#25511;&#21046;&#25991;&#26412;&#30340;&#36895;&#24230;&#12289;&#38899;&#37327;&#21644;&#32469;&#22280;&#24230;&#37327;&#65292;&#20174;&#32780;&#26356;&#31934;&#20934;&#22320;&#23450;&#21046;&#29983;&#25104;&#30340;&#25991;&#26412;&#24418;&#29366;&#65292;&#27604;&#29616;&#26377;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25511;&#21046;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#25991;&#26412;&#29983;&#25104;&#30340;&#26631;&#20934;&#65292;&#38656;&#35201;&#26356;&#22810;&#22320;&#23450;&#21046;&#29983;&#25104;&#30340;&#32039;&#20945;&#24615;&#12289;&#38024;&#23545;&#24615;&#21644;&#20449;&#24687;&#24615;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#21463;&#20247;/&#24212;&#29992;&#31243;&#24207;&#12290;&#29616;&#26377;&#30340;&#25511;&#21046;&#26041;&#27861;&#20027;&#35201;&#35843;&#25972;&#25991;&#26412;&#30340;&#35821;&#20041;&#65288;&#22914;&#24773;&#24863;&#12289;&#20027;&#39064;&#65289;&#12289;&#32467;&#26500;&#65288;&#22914;&#21477;&#27861;&#26641;&#12289;&#35789;&#24615;&#65289;&#21644;&#35789;&#27719;&#65288;&#22914;&#20851;&#38190;&#35789;/&#30701;&#35821;&#21253;&#21547;&#65289;&#65292;&#20294;&#26080;&#27861;&#23454;&#29616;&#22797;&#26434;&#30340;&#30446;&#26631;&#65292;&#22914;&#25511;&#21046;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35835;&#24615;,&#25105;&#20204;&#24341;&#20837;&#20102;CEV-LM&#8212;&#8212;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#21322;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#21463;&#38480;&#21046;&#30340;&#32534;&#36753;&#21521;&#37327;&#26469;&#25511;&#21046;&#19977;&#20010;&#34917;&#20805;&#24230;&#37327;&#65288;&#36895;&#24230;&#12289;&#38899;&#37327;&#21644;&#32469;&#22280;&#65289;&#65292;&#20197;&#37327;&#21270;&#25991;&#26412;&#30340;&#24418;&#29366;&#65288;&#20363;&#22914;&#20869;&#23481;&#30340;&#33410;&#22863;&#65289;&#12290; &#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;CTG&#27169;&#22411;&#65292;&#21457;&#29616;CEV-LM &#21487;&#26174;&#33879;&#26356;&#26377;&#38024;&#23545;&#24615;&#21644;&#31934;&#30830;&#22320;&#25511;&#21046;&#36825;&#19977;&#20010;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14290v1 Announce Type: new  Abstract: As large-scale language models become the standard for text generation, there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application. Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text. In this paper, we introduce CEV-LM - a lightweight, semi-autoregressive language model that utilizes constrained edit vectors to control three complementary metrics (speed, volume, and circuitousness) that quantify the shape of text (e.g., pacing of content). We study an extensive set of state-of-the-art CTG models and find that CEV-LM provides significantly more targeted and precise control of these three m
&lt;/p&gt;</description></item><item><title>TinyLLaVA&#26694;&#26550;&#20351;&#24471;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26356;&#22909;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#35757;&#32451;&#26041;&#26696;&#36798;&#21040;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#26368;&#20339;&#27169;&#22411;TinyLLaVA-3.1B&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;7B&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14289</link><description>&lt;p&gt;
TinyLLaVA&#65306;&#23567;&#35268;&#27169;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TinyLLaVA: A Framework of Small-scale Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14289
&lt;/p&gt;
&lt;p&gt;
TinyLLaVA&#26694;&#26550;&#20351;&#24471;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26356;&#22909;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#35757;&#32451;&#26041;&#26696;&#36798;&#21040;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#26368;&#20339;&#27169;&#22411;TinyLLaVA-3.1B&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;7B&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TinyLLaVA&#26694;&#26550;&#65292;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#23567;&#35268;&#27169;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#25552;&#20379;&#20102;&#32479;&#19968;&#35270;&#35282;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#36830;&#25509;&#27169;&#22359;&#12289;&#35821;&#35328;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#35757;&#32451;&#26041;&#26696;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#26356;&#22909;&#36136;&#37327;&#30340;&#25968;&#25454;&#32467;&#21512;&#26356;&#22909;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;&#24471;&#36739;&#23567;&#30340;LMMs&#33021;&#22815;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#19982;&#26356;&#22823;&#30340;LMMs&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#23567;&#35268;&#27169;LMMs&#12290;&#25105;&#20204;&#26368;&#20339;&#27169;&#22411;TinyLLaVA-3.1B&#22312;&#19982;&#29616;&#26377;&#30340;7B&#27169;&#22411;&#65288;&#22914;LLaVA-1.5&#21644;Qwen-VL&#65289;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#22312;&#25968;&#25454;&#25193;&#23637;&#12289;&#35757;&#32451;&#35774;&#32622;&#21644;&#27169;&#22411;&#36873;&#25321;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26435;&#37325;&#21644;&#20195;&#30721;&#23558;&#34987;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14289v1 Announce Type: cross  Abstract: We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#32531;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14279</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#20943;&#32531;&#35821;&#35328;&#24046;&#24322;&#65292;&#23454;&#29616;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14279
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#32531;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#21892;&#22810;&#35821;&#35328;&#29702;&#35299;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#65292;&#20381;&#36182;&#22797;&#26434;&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#20551;&#35774;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21463;&#21040;&#36825;&#20123;&#35821;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65288;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;&#38899;&#32032;&#20316;&#20026;&#36755;&#20837;&#26631;&#35760;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#26159;&#23376;&#35789;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;&#23450;&#37327;&#35777;&#25454;&#23637;&#31034;&#20102;&#38899;&#32032;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#23545;&#36328;&#35821;&#35328;&#24615;&#33021;&#24046;&#36317;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14279v1 Announce Type: cross  Abstract: Approaches to improving multilingual language understanding often require multiple languages during the training phase, rely on complicated training techniques, and -- importantly -- struggle with significant performance gaps between high-resource and low-resource languages. We hypothesize that the performance gaps between languages are affected by linguistic gaps between those languages and provide a novel solution for robust multilingual language modeling by employing phonemic representations (specifically, using phonemes as input tokens to LMs rather than subwords). We present quantitative evidence from three cross-lingual tasks that demonstrate the effectiveness of phonemic representation, which is further justified by a theoretical analysis of the cross-lingual performance gap.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;GATE X-E&#25361;&#25112;&#38598;&#65292;&#21253;&#21547;&#20102;&#20174;&#22303;&#32819;&#20854;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#33452;&#20848;&#35821;&#21644;&#27874;&#26031;&#35821;&#32763;&#35793;&#25104;&#33521;&#35821;&#30340;&#20154;&#31867;&#32763;&#35793;&#65292;&#26088;&#22312;&#35780;&#20272;&#24369;&#24615;&#21035;&#35821;&#35328;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24182;&#25552;&#20986;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.14277</link><description>&lt;p&gt;
GATE X-E&#65306;&#24369;&#24615;&#21035;&#35821;&#35328;&#30340;&#24615;&#21035;&#20844;&#24179;&#32763;&#35793;&#25361;&#25112;&#38598;
&lt;/p&gt;
&lt;p&gt;
GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14277
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;GATE X-E&#25361;&#25112;&#38598;&#65292;&#21253;&#21547;&#20102;&#20174;&#22303;&#32819;&#20854;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#33452;&#20848;&#35821;&#21644;&#27874;&#26031;&#35821;&#32763;&#35793;&#25104;&#33521;&#35821;&#30340;&#20154;&#31867;&#32763;&#35793;&#65292;&#26088;&#22312;&#35780;&#20272;&#24369;&#24615;&#21035;&#35821;&#35328;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24182;&#25552;&#20986;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14277v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#36136;&#37327;&#21644;&#37319;&#29992;&#19978;&#25345;&#32493;&#25913;&#21892;&#65292;&#20294;&#24615;&#21035;&#20559;&#35265;&#30340;&#26080;&#24847;&#24310;&#32493;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#20174;&#24369;&#24615;&#21035;&#35821;&#35328;&#32763;&#35793;&#25104;&#33521;&#35821;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#29992;&#20110;&#35780;&#20272;&#36825;&#19968;&#29616;&#35937;&#25110;&#35780;&#20272;&#32531;&#35299;&#31574;&#30053;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GATE X-E&#65292;&#36825;&#26159;GATE&#65288;Rarrick&#31561;&#20154;&#65292;2023&#65289;&#35821;&#26009;&#24211;&#30340;&#25193;&#23637;&#65292;&#30001;&#20154;&#31867;&#32763;&#35793;&#32452;&#25104;&#65292;&#20174;&#22303;&#32819;&#20854;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#33452;&#20848;&#35821;&#21644;&#27874;&#26031;&#35821;&#32763;&#35793;&#25104;&#33521;&#35821;&#12290;&#27599;&#31181;&#32763;&#35793;&#37117;&#38468;&#26377;&#22899;&#24615;&#12289;&#30007;&#24615;&#21644;&#20013;&#24615;&#21464;&#20307;&#12290;&#35813;&#25968;&#25454;&#38598;&#27599;&#31181;&#35821;&#35328;&#23545;&#20043;&#38388;&#21253;&#21547;1250&#21040;1850&#20010;&#23454;&#20363;&#65292;&#21253;&#21547;&#20855;&#26377;&#21508;&#31181;&#21477;&#23376;&#38271;&#24230;&#21644;&#39046;&#22495;&#30340;&#33258;&#28982;&#21477;&#23376;&#65292;&#25361;&#25112;&#30528;&#32763;&#35793;&#37325;&#20889;&#32773;&#22312;&#21508;&#31181;&#35821;&#35328;&#29616;&#35937;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;GPT&#26500;&#24314;&#30340;&#32763;&#35793;&#24615;&#21035;&#37325;&#20889;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14277v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern. Despite numerous studies on gender bias in translations into English from weakly gendered-languages, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present a translation gender rewriting solution built with GPT
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23384;&#20648;&#12289;&#22238;&#24518;&#21644;&#25512;&#29702;&#22823;&#35268;&#27169;&#30693;&#35782;&#26041;&#38754;&#30340;&#34920;&#29616;&#34987;&#25506;&#35752;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#19968;&#23450;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14273</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20316;&#20026;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Act as Knowledge Bases at Scale?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14273
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23384;&#20648;&#12289;&#22238;&#24518;&#21644;&#25512;&#29702;&#22823;&#35268;&#27169;&#30693;&#35782;&#26041;&#38754;&#30340;&#34920;&#29616;&#34987;&#25506;&#35752;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#19968;&#23450;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#29702;&#35299;&#21644;&#29983;&#25104;&#22797;&#26434;&#26597;&#35810;&#30340;&#36807;&#31243;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#26126;&#30830;&#28085;&#30422;&#20016;&#23500;&#20107;&#23454;&#20449;&#24687;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#35760;&#24518;&#21644;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#30097;&#38382;&#12290;&#38024;&#23545;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#33021;&#22815;&#26377;&#25928;&#22320;&#23384;&#20648;&#12289;&#22238;&#24518;&#21644;&#25512;&#29702;&#22823;&#35268;&#27169;&#30693;&#35782;&#65292;&#19982;&#26368;&#26032;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#22914;Wikidata&#30456;&#23218;&#32654;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30740;&#31350;&#21487;&#34892;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;(1) LLMs&#22312;&#35760;&#24518;&#22823;&#35268;&#27169;KB&#20013;&#30830;&#20999;&#30693;&#35782;&#26041;&#38754;&#30340;&#25928;&#29575;&#65307;(2) &#22312;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#24212;&#20013;&#22238;&#24518;&#35760;&#24518;&#30693;&#35782;&#30340;&#28789;&#27963;&#24615;&#65307;(3) &#36890;&#36807;&#25512;&#29702;&#25512;&#26029;&#26032;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;LLMs&#20855;&#26377;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14273v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating responses to complex queries through large-scale pre-training. However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable. Addressing this gap, our research investigates whether LLMs can effectively store, recall, and reason with knowledge on a large scale comparable to latest knowledge bases (KBs) such as Wikidata. Specifically, we focus on three crucial aspects to study the viability: (1) the efficiency of LLMs with different sizes in memorizing the exact knowledge in the large-scale KB; (2) the flexibility of recalling the memorized knowledge in response to natural language queries; (3) the capability to infer new knowledge through reasoning. Our findings indicate that while LLMs hold promi
&lt;/p&gt;</description></item><item><title>Qsnail&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39034;&#24207;&#38382;&#39064;&#29983;&#25104;&#30340;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#31232;&#32570;&#23548;&#33268;&#30340;&#33258;&#21160;&#29983;&#25104;&#38382;&#21367;&#39046;&#22495;&#20851;&#27880;&#19981;&#36275;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.14272</link><description>&lt;p&gt;
Qsnail&#65306;&#29992;&#20110;&#39034;&#24207;&#38382;&#39064;&#29983;&#25104;&#30340;&#38382;&#21367;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Qsnail: A Questionnaire Dataset for Sequential Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14272
&lt;/p&gt;
&lt;p&gt;
Qsnail&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39034;&#24207;&#38382;&#39064;&#29983;&#25104;&#30340;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#31232;&#32570;&#23548;&#33268;&#30340;&#33258;&#21160;&#29983;&#25104;&#38382;&#21367;&#39046;&#22495;&#20851;&#27880;&#19981;&#36275;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#21367;&#26159;&#19968;&#31181;&#19987;&#19994;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#20154;&#31867;&#24847;&#35265;&#12289;&#20559;&#22909;&#12289;&#24577;&#24230;&#21644;&#34892;&#20026;&#36827;&#34892;&#23450;&#24615;&#19982;&#23450;&#37327;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#21644;&#35780;&#20272;&#38382;&#21367;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#21162;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#32467;&#26500;&#22797;&#26434;&#32780;&#38169;&#32508;&#22797;&#26434;&#12290;&#38382;&#21367;&#21253;&#21547;&#19968;&#31995;&#21015;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#24517;&#39035;&#31526;&#21512;&#28041;&#21450;&#38382;&#39064;&#12289;&#36873;&#39033;&#21644;&#25972;&#20307;&#32467;&#26500;&#30340;&#22797;&#26434;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38382;&#39064;&#24212;&#19982;&#32473;&#23450;&#30340;&#30740;&#31350;&#20027;&#39064;&#21644;&#24847;&#22270;&#30456;&#20851;&#19988;&#20855;&#20307;&#12290;&#36873;&#39033;&#24212;&#26681;&#25454;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#65292;&#30830;&#20445;&#23427;&#20204;&#26159;&#20114;&#26021;&#30340;&#12289;&#23436;&#25972;&#30340;&#65292;&#24182;&#19988;&#26377;&#21512;&#29702;&#30340;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;&#38382;&#39064;&#39034;&#24207;&#24212;&#36981;&#24490;&#36923;&#36753;&#39034;&#24207;&#65292;&#23558;&#30456;&#20284;&#20027;&#39064;&#20998;&#32452;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#29983;&#25104;&#38382;&#21367;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#30001;&#20110;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#35813;&#39046;&#22495;&#21463;&#21040;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14272v1 Announce Type: new  Abstract: The questionnaire is a professional research methodology used for both qualitative and quantitative analysis of human opinions, preferences, attitudes, and behaviors. However, designing and evaluating questionnaires demands significant effort due to their intricate and complex structure. Questionnaires entail a series of questions that must conform to intricate constraints involving the questions, options, and overall structure. Specifically, the questions should be relevant and specific to the given research topic and intent. The options should be tailored to the questions, ensuring they are mutually exclusive, completed, and ordered sensibly. Moreover, the sequence of questions should follow a logical order, grouping similar topics together. As a result, automatically generating questionnaires presents a significant challenge and this area has received limited attention primarily due to the scarcity of high-quality datasets. To address
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#31185;&#23398;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#32469;&#36807;&#29983;&#25104;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#27493;&#39588;&#65292;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14268</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Detect Misinformation in Scientific News Reporting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14268
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#31185;&#23398;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#32469;&#36807;&#29983;&#25104;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#27493;&#39588;&#65292;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20107;&#23454;&#32463;&#24120;&#34987;&#22312;&#27969;&#34892;&#23186;&#20307;&#20013;&#25805;&#32437;&#65292;&#24847;&#22270;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#21644;&#34892;&#21160;&#65292;&#27491;&#22914;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#25152;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20004;&#31181;&#23186;&#20307;&#31867;&#22411;&#30340;&#20889;&#20316;&#39118;&#26684;&#26377;&#30528;&#26126;&#26174;&#19981;&#21516;&#65292;&#24182;&#19988;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#30740;&#31350;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#26816;&#27979;&#31185;&#23398;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14268v1 Announce Type: cross  Abstract: Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustwo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#29992;&#20110;&#22312;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.14259</link><description>&lt;p&gt;
&#21333;&#35789;&#24207;&#21015;&#29109;&#65306;&#36208;&#21521;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#24212;&#29992;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#29992;&#20110;&#22312;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22312;&#30830;&#20445;&#23433;&#20840;&#20851;&#38190;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#21487;&#38752;&#24615;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#30001;&#24418;&#24335;&#30340;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#23578;&#26410;&#24314;&#31435;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#26080;&#20851;&#30340;&#35789;&#27719;&#21644;&#35821;&#24207;&#21547;&#26377;&#26377;&#38480;&#30340;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#36825;&#26159;&#30001;&#20110;&#29983;&#25104;&#19981;&#24179;&#31561;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#35821;&#20041;&#30456;&#20851;&#24615;&#22312;&#21333;&#35789;&#21644;&#24207;&#21015;&#32423;&#21035;&#19978;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#27604;&#20363;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26102;&#26356;&#21152;&#24378;&#35843;&#20851;&#38190;&#35789;&#21644;&#26356;&#30456;&#20851;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;5&#20010;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;&#21033;&#29992;7&#31181;&#8220;&#29616;&#25104;&#30340;&#8221;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;WSE&#19982;6&#31181;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;WSE&#22312;&#24615;&#33021;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14259v1 Announce Type: cross  Abstract: Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE exhibits superior performance on ac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#40560;&#25968;&#25454;&#38598;&#65292;&#20174;ChatGPT&#21644;&#29992;&#25143;&#30340;&#30495;&#23454;&#20114;&#21160;&#20013;&#25552;&#21462;&#65292;&#23637;&#31034;&#20102;&#31038;&#20250;&#20559;&#35265;&#12289;&#27602;&#24615;&#21644;&#19981;&#36947;&#24503;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#40560;&#25429;&#25417;&#21040;&#20102;&#29616;&#26377;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#36947;&#24503;&#25361;&#25112;&#30340;&#25968;&#25454;&#38598;&#25152;&#26410;&#35206;&#30422;&#30340;&#34917;&#20805;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.14258</link><description>&lt;p&gt;
&#40560;&#65306;&#26469;&#33258;&#30495;&#23454;&#20114;&#21160;&#30340;&#36947;&#24503;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Eagle: Ethical Dataset Given from Real Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14258
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#40560;&#25968;&#25454;&#38598;&#65292;&#20174;ChatGPT&#21644;&#29992;&#25143;&#30340;&#30495;&#23454;&#20114;&#21160;&#20013;&#25552;&#21462;&#65292;&#23637;&#31034;&#20102;&#31038;&#20250;&#20559;&#35265;&#12289;&#27602;&#24615;&#21644;&#19981;&#36947;&#24503;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#40560;&#25429;&#25417;&#21040;&#20102;&#29616;&#26377;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#36947;&#24503;&#25361;&#25112;&#30340;&#25968;&#25454;&#38598;&#25152;&#26410;&#35206;&#30422;&#30340;&#34917;&#20805;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#36947;&#24503;&#30456;&#20851;&#38382;&#39064;&#65292;&#22914;&#31038;&#20250;&#20559;&#35265;&#12289;&#32570;&#20047;&#36947;&#24503;&#25512;&#29702;&#21644;&#29983;&#25104;&#20855;&#26377;&#25915;&#20987;&#24615;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#26041;&#27861;&#22788;&#29702;&#36825;&#20123;&#36947;&#24503;&#25361;&#25112;&#20351;&#29992;&#35825;&#20351;&#20154;&#31867;&#21046;&#20316;&#21253;&#21547;&#36947;&#24503;&#38382;&#39064;&#30340;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25968;&#25454;&#24182;&#19981;&#21453;&#26144;&#29992;&#25143;&#22312;&#26085;&#24120;&#20351;&#29992;LLM&#26381;&#21153;&#26102;&#23454;&#38469;&#25552;&#20379;&#30340;&#25552;&#31034;&#12290;&#36825;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#24320;&#21457;&#33021;&#22815;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#20013;&#20986;&#29616;&#30340;&#36947;&#24503;&#25361;&#25112;&#30340;&#23433;&#20840;LLMs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20174;ChatGPT&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#30495;&#23454;&#20114;&#21160;&#20013;&#25552;&#21462;&#30340;&#40560;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#31038;&#20250;&#20559;&#35265;&#12289;&#27602;&#24615;&#21644;&#19981;&#36947;&#24503;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#40560;&#25429;&#25417;&#20102;&#29616;&#26377;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#36731;&#27492;&#31867;&#36947;&#24503;&#25361;&#25112;&#30340;&#25968;&#25454;&#38598;&#25152;&#26410;&#28085;&#30422;&#30340;&#34917;&#20805;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14258v1 Announce Type: new  Abstract: Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content. The existing evaluation metrics and methods to address these ethical challenges use datasets intentionally created by instructing humans to create instances including ethical problems. Therefore, the data does not reflect prompts that users actually provide when utilizing LLM services in everyday contexts. This may not lead to the development of safe LLMs that can address ethical challenges arising in real-world applications. In this paper, we create Eagle datasets extracted from real interactions between ChatGPT and users that exhibit social biases, toxicity, and immoral problems. Our experiments show that Eagle captures complementary aspects, not covered by existing datasets proposed for evaluation and mitigation of such ethical challenges. Our code is publ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#26088;&#22312;&#20998;&#26512;&#20027;&#27969;&#23186;&#20307;&#22312;&#25253;&#36947;&#32463;&#27982;&#28040;&#24687;&#26102;&#30340;&#32534;&#36753;&#36873;&#25321;&#65292;&#36890;&#36807;&#23545;&#32463;&#27982;&#25351;&#26631;&#30340;&#25253;&#36947;&#36827;&#34892;&#26694;&#26550;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#29702;&#35299;&#20986;&#29256;&#29289;&#36873;&#25321;&#21644;&#26500;&#26550;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14224</link><description>&lt;p&gt;
&#22312;&#25903;&#25345;&#25968;&#25454;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26694;&#26550;&#26500;&#24314;&#65306;&#20197;&#32654;&#22269;&#32463;&#27982;&#26032;&#38395;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#26088;&#22312;&#20998;&#26512;&#20027;&#27969;&#23186;&#20307;&#22312;&#25253;&#36947;&#32463;&#27982;&#28040;&#24687;&#26102;&#30340;&#32534;&#36753;&#36873;&#25321;&#65292;&#36890;&#36807;&#23545;&#32463;&#27982;&#25351;&#26631;&#30340;&#25253;&#36947;&#36827;&#34892;&#26694;&#26550;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#29702;&#35299;&#20986;&#29256;&#29289;&#36873;&#25321;&#21644;&#26500;&#26550;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#23186;&#20307;&#22312;&#36873;&#25321;&#20309;&#20107;&#29289;&#36827;&#34892;&#25253;&#36947;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#25253;&#36947;&#26041;&#38754;&#26377;&#24456;&#22823;&#30340;&#33258;&#30001;&#35009;&#37327;&#26435;&#12290;&#36825;&#20123;&#36873;&#25321;&#20250;&#23545;&#20154;&#20204;&#25152;&#20102;&#35299;&#30340;&#20449;&#24687;&#21644;&#38543;&#21518;&#30340;&#34892;&#20026;&#20135;&#29983;&#30495;&#23454;&#19990;&#30028;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23458;&#35266;&#30340;&#35780;&#20272;&#32534;&#36753;&#36873;&#25321;&#30340;&#24230;&#37327;&#20351;&#24471;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#29305;&#21035;&#22256;&#38590;&#12290;&#26412;&#25991;&#35748;&#20026;&#22312;&#19968;&#20123;&#26377;&#25903;&#25345;&#25968;&#25454;&#23384;&#22312;&#30340;&#20540;&#24471;&#25253;&#36947;&#30340;&#35805;&#39064;&#20013;&#65292;&#21487;&#20197;&#25552;&#20986;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#26469;&#20998;&#26512;&#32534;&#36753;&#36873;&#25321;&#12290;&#25105;&#20204;&#36873;&#25321;&#32463;&#27982;&#20316;&#20026;&#30740;&#31350;&#37325;&#28857;&#65292;&#22240;&#20026;&#32463;&#27982;&#25351;&#26631;&#30340;&#25253;&#36947;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#23545;&#23481;&#26131;&#30830;&#23450;&#21508;&#31181;&#20986;&#29256;&#29289;&#36873;&#25321;&#21644;&#26500;&#26550;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#25351;&#26631;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20851;&#32463;&#27982;&#34920;&#29616;&#30340;&#30495;&#23454;&#24773;&#20917;&#65292;&#30456;&#23545;&#20110;&#20986;&#29256;&#29289;&#23545;&#20854;&#36827;&#34892;&#25253;&#36947;&#30340;&#26041;&#24335;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#39044;&#27979;&#23450;&#20041;&#20026;&#19968;&#32452;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14224v1 Announce Type: new  Abstract: The mainstream media has much leeway in what it chooses to cover and how it covers it. These choices have real-world consequences on what people know and their subsequent behaviors. However, the lack of objective measures to evaluate editorial choices makes research in this area particularly difficult. In this paper, we argue that there are newsworthy topics where objective measures exist in the form of supporting data and propose a computational framework to analyze editorial choices in this setup. We focus on the economy because the reporting of economic indicators presents us with a relatively easy way to determine both the selection and framing of various publications. Their values provide a ground truth of how the economy is doing relative to how the publications choose to cover it. To do this, we define frame prediction as a set of interdependent tasks. At the article level, we learn to identify the reported stance towards the gene
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14208</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#20869;&#23481;&#26465;&#20214;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Conditional Debiasing for Fair Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14208
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20844;&#24179;&#30340;&#25991;&#26412;&#23884;&#20837;&#19978;&#65292;&#36825;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25928;&#29992;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#20294;&#30456;&#21516;&#20869;&#23481;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#19982;&#20854;&#23545;&#24212;&#20013;&#31435;&#25991;&#26412;&#30340;&#23884;&#20837;&#20445;&#25345;&#30456;&#21516;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#25991;&#26412;&#22686;&#24378;&#20026;&#19981;&#21516;&#30340;&#25935;&#24863;&#32452;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#23884;&#20837;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STORM&#30340;&#20889;&#20316;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#21512;&#25104;&#20027;&#39064;&#27010;&#35201;&#65292;&#20197;&#36741;&#21161;&#20174;&#22836;&#24320;&#22987;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#31456;&#12290;</title><link>https://arxiv.org/abs/2402.14207</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#36741;&#21161;&#25776;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;
&lt;/p&gt;
&lt;p&gt;
Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STORM&#30340;&#20889;&#20316;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#21512;&#25104;&#20027;&#39064;&#27010;&#35201;&#65292;&#20197;&#36741;&#21161;&#20174;&#22836;&#24320;&#22987;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#25776;&#20889;&#22522;&#20110;&#20107;&#23454;&#21644;&#26377;&#26465;&#29702;&#30340;&#38271;&#31687;&#25991;&#31456;&#65292;&#20351;&#20854;&#22312;&#24191;&#24230;&#21644;&#28145;&#24230;&#19978;&#19982;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#21487;&#23218;&#32654;&#12290;&#36825;&#19968;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#22312;&#25776;&#20889;&#21069;&#38454;&#27573;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22914;&#20309;&#30740;&#31350;&#20027;&#39064;&#24182;&#20934;&#22791;&#22823;&#32434;&#20197;&#20415;&#25776;&#20889;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STORM&#65292;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#36827;&#34892;&#20027;&#39064;&#27010;&#35201;&#21512;&#25104;&#30340;&#20889;&#20316;&#31995;&#32479;&#12290;STORM&#27169;&#25311;&#20102;&#25776;&#20889;&#21069;&#38454;&#27573;&#65292;&#20854;&#20013;&#65288;1&#65289;&#21457;&#29616;&#30740;&#31350;&#32473;&#23450;&#20027;&#39064;&#30340;&#22810;&#26679;&#21270;&#35266;&#28857;&#65292;&#65288;2&#65289;&#27169;&#25311;&#20250;&#35805;&#65292;&#25776;&#20889;&#25345;&#26377;&#19981;&#21516;&#35266;&#28857;&#30340;&#20316;&#32773;&#21521;&#22522;&#20110;&#21487;&#20449;&#20114;&#32852;&#32593;&#26469;&#28304;&#30340;&#20027;&#39064;&#19987;&#23478;&#25552;&#38382;&#65292;&#65288;3&#65289;&#25972;&#29702;&#25910;&#38598;&#21040;&#30340;&#20449;&#24687;&#20197;&#21019;&#24314;&#22823;&#32434;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;FreshWiki&#65292;&#19968;&#20010;&#21253;&#21547;&#26368;&#26032;&#39640;&#36136;&#37327;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#22823;&#32434;&#35780;&#20272;&#25351;&#26631;&#20197;&#35780;&#20272;&#25776;&#20889;&#21069;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14207v1 Announce Type: cross  Abstract: We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.   For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21361;&#26426;&#36741;&#23548;&#21592;&#19982;&#27714;&#21161;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#65292;&#23454;&#39564;&#35777;&#26126;&#21152;&#20837;&#39046;&#22495;&#30693;&#35782;&#21644;&#27169;&#22411;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#32422;15%&#12290;</title><link>https://arxiv.org/abs/2402.14200</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#35805;&#29702;&#35299;&#65306;&#39046;&#22495;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21361;&#26426;&#36741;&#23548;&#21592;&#19982;&#27714;&#21161;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#65292;&#23454;&#39564;&#35777;&#26126;&#21152;&#20837;&#39046;&#22495;&#30693;&#35782;&#21644;&#27169;&#22411;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#32422;15%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21672;&#35810;&#23545;&#35805;&#30340;&#21160;&#24577;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#36817;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;NLP&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#26816;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26356;&#22909;&#22320;&#34920;&#31034;&#21361;&#26426;&#36741;&#23548;&#21592;&#19982;&#27714;&#21161;&#32773;&#20043;&#38388;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;Transformer&#27169;&#22411;&#21644;GPT&#27169;&#22411;&#26410;&#33021;&#39044;&#27979;&#23545;&#35805;&#32467;&#26524;&#12290;&#20026;&#20102;&#20026;&#23545;&#35805;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#32467;&#21512;&#20154;&#24037;&#27880;&#37322;&#30340;&#39046;&#22495;&#30693;&#35782;&#21644;LLM&#29983;&#25104;&#30340;&#29305;&#24449;&#65307;&#31616;&#21333;&#22320;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;LLM&#29305;&#24449;&#23558;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#32422;15%&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24403;&#23427;&#20204;&#34987;&#29992;&#20316;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#26102;&#65292;&#39046;&#22495;&#30693;&#35782;&#21644;LLM&#29983;&#25104;&#30340;&#29305;&#24449;&#37117;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#26356;&#22909;&#22320;&#34920;&#24449;&#21672;&#35810;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14200v1 Announce Type: new  Abstract: Understanding the dynamics of counseling conversations is an important task, yet it is a challenging NLP problem regardless of the recent advance of Transformer-based pre-trained language models. This paper proposes a systematic approach to examine the efficacy of domain knowledge and large language models (LLMs) in better representing conversations between a crisis counselor and a help seeker. We empirically show that state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome. To provide richer context to conversations, we incorporate human-annotated domain knowledge and LLM-generated features; simple integration of domain knowledge and LLM features improves the model performance by approximately 15%. We argue that both domain knowledge and LLM-generated features can be exploited to better characterize counseling conversations when they are used as an additional context to c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#23398;&#20064;&#20943;&#23569;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20943;&#23569;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20197;&#25552;&#39640;&#22266;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14195</link><description>&lt;p&gt;
&#23398;&#20064;&#20943;&#23569;: &#22312;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#30340;&#26368;&#20339;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#23398;&#20064;&#20943;&#23569;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20943;&#23569;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20197;&#25552;&#39640;&#22266;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#29992;&#20316;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#23637;&#31034;&#20986;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#24456;&#38590;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;KG&#12289;&#34920;&#26684;&#12289;&#25968;&#25454;&#24211;&#65289;&#25972;&#21512;&#21040;&#20854;&#25552;&#31034;&#20013;&#65307;LLMs&#38656;&#35201;&#22312;&#25512;&#29702;&#20043;&#21069;&#35201;&#20040;&#29702;&#35299;&#38271;&#25991;&#26412;&#25968;&#25454;&#65292;&#35201;&#20040;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#32780;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#23398;&#20064;&#20943;&#23569;&#65288;Learning to Reduce&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#20219;&#21153;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#36755;&#20837;&#29983;&#25104;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#31934;&#31616;&#29256;&#26412;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;On-Policy&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#20943;&#23569;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#24182;&#26088;&#22312;&#25552;&#39640;&#22266;&#23450;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#20174;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36873;&#25321;&#30456;&#20851;&#35777;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14195v1 Announce Type: new  Abstract: Large Language Models (LLMs) have been widely used as general-purpose AI agents showing comparable performance on many downstream tasks. However, existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial.   In this paper, we propose a framework, Learning to Reduce, that fine-tunes a language model to generate a reduced version of an input context, given a task description and context input. The model learns to reduce the input context using On-Policy Reinforcement Learning and aims to improve the reasoning performance of a fixed LLM. Experimental results illustrate that our model not only achieves comparable accuracies in selecting the relevant evidence from an input context, but also shows generalizability on different datasets. We fur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#33521;&#25991;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#26032;&#34920;&#24773;&#31526;&#21495;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#24773;&#20917;&#65292;&#21457;&#29616;&#26089;&#26399;&#37319;&#32435;&#32773;&#35268;&#27169;&#21644;&#34920;&#24773;&#31526;&#21495;&#35821;&#20041;&#23545;&#20854;&#27969;&#34892;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#35299;&#37322;&#26032;&#34920;&#24773;&#31526;&#21495;&#65292;&#20174;&#32780;&#25913;&#21892;&#24773;&#24863;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14187</link><description>&lt;p&gt;
&#20174;&#37319;&#32435;&#21040;&#36866;&#24212;&#65306;&#36861;&#36394;&#26032;&#34920;&#24773;&#31526;&#21495;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
From Adoption to Adaption: Tracing the Diffusion of New Emojis on Twitter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#33521;&#25991;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#26032;&#34920;&#24773;&#31526;&#21495;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#24773;&#20917;&#65292;&#21457;&#29616;&#26089;&#26399;&#37319;&#32435;&#32773;&#35268;&#27169;&#21644;&#34920;&#24773;&#31526;&#21495;&#35821;&#20041;&#23545;&#20854;&#27969;&#34892;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#35299;&#37322;&#26032;&#34920;&#24773;&#31526;&#21495;&#65292;&#20174;&#32780;&#25913;&#21892;&#24773;&#24863;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;Unicode&#21457;&#24067;&#26032;&#34920;&#24773;&#31526;&#21495;&#29256;&#26412;&#25552;&#20379;&#20102;&#19968;&#20010;&#25506;&#32034;&#25968;&#23383;&#35821;&#35328;&#28436;&#21464;&#30340;&#32467;&#26500;&#21270;&#26426;&#20250;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25277;&#26679;&#30340;&#33521;&#25991;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26032;&#21457;&#24067;&#30340;&#34920;&#24773;&#31526;&#21495;&#22914;&#20309;&#33719;&#24471;&#20851;&#27880;&#24182;&#22914;&#20309;&#22312;&#21547;&#20041;&#19978;&#28436;&#21464;&#12290;&#25105;&#20204;&#21457;&#29616;&#26089;&#26399;&#37319;&#32435;&#32773;&#30340;&#31038;&#21306;&#35268;&#27169;&#21644;&#34920;&#24773;&#31526;&#21495;&#35821;&#20041;&#23545;&#20110;&#30830;&#23450;&#23427;&#20204;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#26576;&#20123;&#34920;&#24773;&#31526;&#21495;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#21547;&#20041;&#21464;&#21270;&#21644;&#24773;&#24863;&#20851;&#32852;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20855;&#26377;&#35821;&#20041;&#19978;&#30456;&#20284;&#19978;&#19979;&#25991;&#30340;&#21333;&#35789;&#21644;&#26082;&#26377;&#34920;&#24773;&#31526;&#21495;&#30340;&#26032;&#26694;&#26550;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#37322;&#26032;&#34920;&#24773;&#31526;&#21495;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29992;&#29087;&#24713;&#30340;&#34920;&#24773;&#31526;&#21495;&#26367;&#20195;&#26410;&#30693;&#30340;&#26032;&#34920;&#24773;&#31526;&#21495;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#20998;&#31867;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25105;&#20204;&#29702;&#35299;&#26032;&#35821;&#35328;&#30340;&#37319;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14187v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of social media, the introduction of new emojis in Unicode release versions presents a structured opportunity to explore digital language evolution. Analyzing a large dataset of sampled English tweets, we examine how newly released emojis gain traction and evolve in meaning. We find that community size of early adopters and emoji semantics are crucial in determining their popularity. Certain emojis experienced notable shifts in the meanings and sentiment associations during the diffusion process. Additionally, we propose a novel framework utilizing language models to extract words and pre-existing emojis with semantically similar contexts, which enhances interpretation of new emojis. The framework demonstrates its effectiveness in improving sentiment classification performance by substituting unknown new emojis with familiar ones. This study offers a new perspective in understanding how new language un
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#38598;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.14184</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#26679;&#24615;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14184
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#38598;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26159;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#24320;&#28304;&#20013;&#23384;&#22312;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#65292;&#38598;&#25104;&#26377;&#21161;&#20110;&#25552;&#21319;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#38598;&#25104;&#20013;&#27599;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#31616;&#21333;&#24179;&#22343;&#65292;&#23545;&#27599;&#20010;&#27169;&#22411;&#36171;&#20104;&#30456;&#21516;&#26435;&#37325;&#65292;&#24573;&#30053;&#20102;&#27169;&#22411;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19981;&#20165;&#21333;&#20010;&#27169;&#22411;&#34920;&#29616;&#30693;&#35782;&#65292;&#36824;&#20351;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#25105;&#20204;&#30340;&#38598;&#25104;&#12290;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#36136;&#37327;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14184v1 Announce Type: cross  Abstract: Ensembles are important tools for improving the performance of machine learning models. In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source. However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models. We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other. By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble. The quality improves for both text classification accuracy and relevant uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#27665;&#26063;&#23186;&#20307;&#39046;&#22495;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65292;&#20197;&#25552;&#21319;&#26032;&#38395;&#32763;&#35793;&#12289;&#25628;&#32034;&#21644;&#20998;&#31867;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14179</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;AI&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26063;&#35028;&#23186;&#20307;&#26426;&#22120;&#32763;&#35793;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14179
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#27665;&#26063;&#23186;&#20307;&#39046;&#22495;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65292;&#20197;&#25552;&#21319;&#26032;&#38395;&#32763;&#35793;&#12289;&#25628;&#32034;&#21644;&#20998;&#31867;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27665;&#26063;&#23186;&#20307;&#26159;&#20026;&#39547;&#30041;&#22312;&#19996;&#36947;&#22269;&#30340;&#20392;&#27665;&#31038;&#21306;&#25552;&#20379;&#26381;&#21153;&#30340;&#37325;&#35201;&#24179;&#21488;&#65292;&#26082;&#26381;&#21153;&#20110;&#36825;&#20123;&#31038;&#21306;&#21046;&#20316;&#20869;&#23481;&#65292;&#21448;&#35753;&#20182;&#20204;&#33719;&#21462;&#20449;&#24687;&#12290;&#19982;&#20351;&#29992;&#19996;&#36947;&#22269;&#35821;&#35328;&#19981;&#21516;&#65292;&#27665;&#26063;&#23186;&#20307;&#20197;&#31227;&#27665;&#31038;&#21306;&#30340;&#35821;&#35328;&#21457;&#24067;&#26032;&#38395;&#12290;&#20030;&#20363;&#26469;&#35828;&#65292;&#22312;&#32654;&#22269;&#65292;&#23391;&#21152;&#25289;&#26063;&#35028;&#23186;&#20307;&#20351;&#29992;&#23391;&#21152;&#25289;&#35821;&#32780;&#19981;&#26159;&#33521;&#35821;&#21457;&#24067;&#26032;&#38395;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27665;&#26063;&#23186;&#20307;&#39046;&#22495;&#28508;&#22312;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#30340;&#21487;&#33021;&#24615;&#12290;&#23427;&#30528;&#37325;&#25506;&#35752;&#20102;&#22312;&#26032;&#38395;&#32763;&#35793;&#12289;&#25628;&#32034;&#21644;&#20998;&#31867;&#30340;&#21508;&#20010;&#26041;&#38754;&#20013;&#20351;&#29992;LLM&#36827;&#34892;MMT&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;&#35770;&#25991;&#27010;&#36848;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;LLM&#21644;MMT&#25972;&#21512;&#21040;&#27665;&#26063;&#23186;&#20307;&#30340;&#26032;&#38395;&#25628;&#32034;&#21644;&#32763;&#35793;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#31616;&#35201;&#35752;&#35770;&#20102;&#19982;LLM&#21644;MMT&#25972;&#21512;&#30456;&#20851;&#30340;&#28508;&#22312;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14179v1 Announce Type: cross  Abstract: Ethnic media, which caters to diaspora communities in host nations, serves as a vital platform for these communities to both produce content and access information. Rather than utilizing the language of the host nation, ethnic media delivers news in the language of the immigrant community. For instance, in the USA, Bangla ethnic media presents news in Bangla rather than English. This research delves into the prospective integration of large language models (LLM) and multi-lingual machine translations (MMT) within the ethnic media industry. It centers on the transformative potential of using LLM in MMT in various facets of news translation, searching, and categorization. The paper outlines a theoretical framework elucidating the integration of LLM and MMT into the news searching and translation processes for ethnic media. Additionally, it briefly addresses the potential ethical challenges associated with the incorporation of LLM and MMT
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#39564;&#35777;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#20998;&#26032;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24615;&#33258;&#38382;&#38382;&#39064;&#26469;&#23454;&#29616;&#24037;&#20855;&#36873;&#25321;&#21644;&#21442;&#25968;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26032;&#24037;&#20855;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14158</link><description>&lt;p&gt;
TOOLVERIFIER: &#36890;&#36807;&#33258;&#39564;&#35777;&#23454;&#29616;&#23545;&#26032;&#24037;&#20855;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
TOOLVERIFIER: Generalization to New Tools via Self-Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14158
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#39564;&#35777;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#20998;&#26032;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24615;&#33258;&#38382;&#38382;&#39064;&#26469;&#23454;&#29616;&#24037;&#20855;&#36873;&#25321;&#21644;&#21442;&#25968;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26032;&#24037;&#20855;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#25945;&#20250;&#22914;&#20309;&#20351;&#29992;&#24037;&#20855;&#26159;&#36808;&#21521;&#26500;&#24314;&#36890;&#29992;&#21161;&#25163;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#20294;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#34429;&#28982;&#22312;&#38024;&#23545;&#29305;&#23450;&#24037;&#20855;&#30340;&#24494;&#35843;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#22312;&#22914;&#20309;&#20174;&#20165;&#26377;&#23569;&#25968;&#31034;&#20363;&#20013;&#24378;&#22823;&#22320;&#20351;&#29992;&#26032;&#24037;&#20855;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24037;&#20855;&#36873;&#25321;&#21644;&#21442;&#25968;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#27604;&#24615;&#33258;&#38382;&#38382;&#39064;&#26469;&#21306;&#20998;&#36817;&#20284;&#30340;&#20505;&#36873;&#24037;&#20855;&#12290;&#25105;&#20204;&#21033;&#29992;Llama-2 70B&#26500;&#24314;&#20102;&#21512;&#25104;&#30340;&#39640;&#36136;&#37327;&#33258;&#29983;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25171;&#31639;&#23558;&#20854;&#20844;&#24320;&#21457;&#24067;&#12290;&#22312;ToolBench&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;4&#39033;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;17&#20010;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#65292;&#23637;&#31034;&#20102;&#22312;&#23569;&#26679;&#26412;&#22522;&#32447;&#27979;&#35797;&#20013;&#24179;&#22343;&#25552;&#21319;&#20102;22%&#65292;&#21363;&#20351;&#22312;&#20505;&#36873;&#24037;&#20855;&#20043;&#38388;&#30340;&#21306;&#21035;&#24494;&#22937;&#20043;&#22788;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14158v1 Announce Type: new  Abstract: Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#19977;&#31181;&#39046;&#22495;&#25490;&#24207;&#31574;&#30053;&#23545;&#29983;&#25104;&#24335;&#24847;&#22270;&#35782;&#21035;&#27169;&#22411;&#32487;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#27492;&#26041;&#38754;&#26410;&#25506;&#32034;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.14155</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#39046;&#22495;&#25490;&#24207;&#33021;&#22815;&#20943;&#23569;&#24847;&#22270;&#35782;&#21035;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent Recognition?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14155
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#19977;&#31181;&#39046;&#22495;&#25490;&#24207;&#31574;&#30053;&#23545;&#29983;&#25104;&#24335;&#24847;&#22270;&#35782;&#21035;&#27169;&#22411;&#32487;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#27492;&#26041;&#38754;&#26410;&#25506;&#32034;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#34987;&#26399;&#26395;&#22312;&#37096;&#32626;&#21518;&#33021;&#22815;&#22788;&#29702;&#19981;&#26029;&#22686;&#38271;&#30340;&#24847;&#22270;&#21644;&#39046;&#22495;&#65292;&#29978;&#33267;&#22312;&#25903;&#25345;&#36234;&#26469;&#36234;&#22810;&#21151;&#33021;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#26399;&#26395;&#65292;&#23601;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#21435;&#20943;&#36731;&#22312;&#35832;&#22914;&#24847;&#22270;&#35782;&#21035;&#31561;&#20219;&#21153;&#30340;&#32487;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#35774;&#32622;&#20013;&#21457;&#29983;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65288;CF&#65289;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22522;&#20110;&#37325;&#25918;&#21644;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#20197;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#20294;&#39046;&#22495;&#25490;&#24207;&#23545;&#24847;&#22270;&#35782;&#21035;&#27169;&#22411;&#30340;&#32487;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22914;&#26524;&#29702;&#35299;&#24471;&#24403;&#65292;&#39046;&#22495;&#25490;&#24207;&#26377;&#28508;&#21147;&#25104;&#20026;&#19968;&#20010;&#33021;&#22815;&#19982;&#29616;&#26377;&#25216;&#26415;&#22914;&#32463;&#39564;&#37325;&#25918;&#24182;&#34892;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#39046;&#22495;&#25490;&#24207;&#31574;&#30053;&#65288;&#26368;&#23567;&#21644;&#36335;&#24452;&#12289;&#26368;&#22823;&#21644;&#36335;&#24452;&#12289;&#38543;&#26426;&#65289;&#23545;&#29983;&#25104;&#24335;&#24847;&#22270;&#35782;&#21035;&#27169;&#22411;&#30340;&#32487;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14155v1 Announce Type: cross  Abstract: Task-oriented dialogue systems are expected to handle a constantly expanding set of intents and domains even after they have been deployed to support more and more functionalities. To live up to this expectation, it becomes critical to mitigate the catastrophic forgetting problem (CF) that occurs in continual learning (CL) settings for a task such as intent recognition. While existing dialogue systems research has explored replay-based and regularization-based methods to this end, the effect of domain ordering on the CL performance of intent recognition models remains unexplored. If understood well, domain ordering has the potential to be an orthogonal technique that can be leveraged alongside existing techniques such as experience replay. Our work fills this gap by comparing the impact of three domain-ordering strategies (min-sum path, max-sum path, random) on the CL performance of a generative intent recognition model. Our findings r
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#21313;&#31181;&#22823;&#23567;&#21464;&#20307;&#30340;&#22235;&#20010;&#24320;&#28304;MLLMs&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14154</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14154
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#21313;&#31181;&#22823;&#23567;&#21464;&#20307;&#30340;&#22235;&#20010;&#24320;&#28304;MLLMs&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#22810;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#35270;&#39057;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#38590;&#20197;&#29702;&#35299;&#22312;&#32447;&#31354;&#38388;&#20013;&#20132;&#20114;&#25152;&#20851;&#32852;&#30340;&#20449;&#24687;&#25110;&#24773;&#32490;&#12290;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#31867;&#24773;&#32490;&#21644;&#35832;&#22914;&#34394;&#20551;&#20449;&#24687;&#31561;&#22797;&#26434;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;MLLMs&#23545;&#22810;&#27169;&#24577;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;MM-Soc&#25972;&#21512;&#20102;&#33879;&#21517;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#34701;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;YouTube&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#38024;&#23545;&#20174;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21040;&#31038;&#20132;&#19978;&#19979;&#25991;&#29983;&#25104;&#31561;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#24320;&#28304;MLLMs&#30340;&#21313;&#31181;&#19981;&#21516;&#35268;&#27169;&#21464;&#20307;&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20984;&#26174;&#20986;&#20102;&#23545;&#24615;&#33021;&#24179;&#34913;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14154v1 Announce Type: new  Abstract: Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need
&lt;/p&gt;</description></item><item><title>BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14151</link><description>&lt;p&gt;
BIRCO&#65306;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14151
&lt;/p&gt;
&lt;p&gt;
BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;(IR)&#20219;&#21153;&#22522;&#20934;(BIRCO)&#12290; BIRCO&#35780;&#20272;IR&#31995;&#32479;&#26681;&#25454;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#26816;&#32034;&#25991;&#26723;&#30340;&#33021;&#21147;&#12290; &#35813;&#22522;&#20934;&#30340;&#22797;&#26434;&#24615;&#21644;&#32039;&#20945;&#22823;&#23567;&#20351;&#20854;&#36866;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#21487;&#33021;&#24433;&#21709;LLM&#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19982;&#25110;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#26356;&#22797;&#26434;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290; &#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#20934;&#20219;&#21153;&#19978;&#22343;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#21644;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#22810;&#31181;&#39118;&#26684;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#22810;&#37325;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#21516;&#26102;&#25511;&#21046;&#22810;&#31181;&#39118;&#26684;&#12290;</title><link>https://arxiv.org/abs/2402.14146</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#22810;&#37325;&#22870;&#21169;&#21152;&#26435;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22810;&#26679;&#24335;&#21487;&#25511;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#22810;&#31181;&#39118;&#26684;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#22810;&#37325;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#21516;&#26102;&#25511;&#21046;&#22810;&#31181;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#26159;&#34920;&#36798;&#21508;&#31181;&#20449;&#24687;&#30340;&#25991;&#26412;&#20013;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#20154;&#38469;&#21160;&#24577;&#65288;&#20363;&#22914;&#27491;&#24335;&#24615;&#65289;&#21644;&#20316;&#32773;&#30340;&#24773;&#32490;&#25110;&#24577;&#24230;&#65288;&#20363;&#22914;&#21388;&#24694;&#65289;&#12290;&#20154;&#31867;&#32463;&#24120;&#21516;&#26102;&#37319;&#29992;&#22810;&#31181;&#39118;&#26684;&#12290;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#26126;&#30830;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#32534;&#32455;&#30446;&#26631;&#39118;&#26684;&#65306;&#20363;&#22914;&#65292;&#29983;&#25104;&#26082;&#28040;&#26497;&#21448;&#26080;&#27602;&#30340;&#25991;&#26412;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#23545;&#21333;&#19968;&#39118;&#26684;&#30340;&#25511;&#21046;&#29983;&#25104;&#65292;&#25110;&#32773;&#23545;&#39118;&#26684;&#21644;&#20854;&#20182;&#23646;&#24615;&#30340;&#25511;&#21046;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#25193;&#23637;&#21040;&#21516;&#26102;&#25511;&#21046;&#22810;&#31181;&#39118;&#26684;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#21463;&#25511;&#22810;&#26679;&#24335;&#29983;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#30340;&#22810;&#31181;&#39118;&#26684;&#22870;&#21169;&#30340;&#21508;&#31181;&#20844;&#24335;&#12290;&#36825;&#20123;&#22870;&#21169;&#20844;&#24335;&#21253;&#25324;&#26469;&#33258;&#37492;&#21035;&#22120;&#30340;&#26657;&#20934;&#36755;&#20986;&#20197;&#21450;&#36890;&#36807;&#37492;&#21035;&#22120;&#26799;&#24230;&#24133;&#24230;&#36827;&#34892;&#21160;&#24577;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14146v1 Announce Type: new  Abstract: Style is an integral component of text that expresses a diverse set of information, including interpersonal dynamics (e.g. formality) and the author's emotions or attitudes (e.g. disgust). Humans often employ multiple styles simultaneously. An open question is how large language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. Previous work investigates the controlled generation of a single style, or else controlled generation of a style and other attributes. In this paper, we expand this into controlling multiple styles simultaneously. Specifically, we investigate various formulations of multiple style rewards for a reinforcement learning (RL) approach to controlled multi-style generation. These reward formulations include calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. W
&lt;/p&gt;</description></item><item><title>GraphScholarBERT&#26159;&#19968;&#31181;&#32467;&#21512;&#35821;&#35328;&#21644;&#22270;&#27169;&#22411;&#30340;&#20449;&#24687;&#25277;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;Web&#19978;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#20013;&#25552;&#21462;&#30446;&#26631;&#20851;&#31995;&#65292;&#24182;&#22312;&#38646;&#23556;&#39046;&#22495;&#21644;&#38646;&#23556;&#32593;&#31449;&#35774;&#32622;&#20013;&#23558;&#25552;&#21462;F1&#24471;&#20998;&#25552;&#39640;34.8&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.14129</link><description>&lt;p&gt;
&#32467;&#21512;&#35821;&#35328;&#21644;&#22270;&#27169;&#22411;&#36827;&#34892;Web&#19978;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Combining Language and Graph Models for Semi-structured Information Extraction on the Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14129
&lt;/p&gt;
&lt;p&gt;
GraphScholarBERT&#26159;&#19968;&#31181;&#32467;&#21512;&#35821;&#35328;&#21644;&#22270;&#27169;&#22411;&#30340;&#20449;&#24687;&#25277;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;Web&#19978;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#20013;&#25552;&#21462;&#30446;&#26631;&#20851;&#31995;&#65292;&#24182;&#22312;&#38646;&#23556;&#39046;&#22495;&#21644;&#38646;&#23556;&#32593;&#31449;&#35774;&#32622;&#20013;&#23558;&#25552;&#21462;F1&#24471;&#20998;&#25552;&#39640;34.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#22312;&#32593;&#32476;&#19978;&#25366;&#25496;&#20154;&#31867;&#30693;&#35782;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#24335;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#20135;&#29983;&#22024;&#26434;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#20174;&#21322;&#32467;&#26500;&#21270;&#30340;&#32593;&#39029;&#20013;&#25552;&#21462;&#30446;&#26631;&#20851;&#31995;&#65292;&#20165;&#32473;&#20986;&#20851;&#31995;&#30340;&#31616;&#30701;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GraphScholarBERT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#22270;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;&#24320;&#25918;&#39046;&#22495;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#12290;GraphScholarBERT&#33021;&#22815;&#27867;&#21270;&#21040;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#65292;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25110;&#35757;&#32451;&#65292;&#24182;&#19988;&#20165;&#20135;&#29983;&#19982;&#25628;&#32034;&#20851;&#38190;&#23383;&#21305;&#37197;&#30340;&#24178;&#20928;&#25552;&#21462;&#32467;&#26524;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#38646;&#23556;&#39046;&#22495;&#21644;&#38646;&#23556;&#32593;&#31449;&#35774;&#32622;&#20013;&#30340;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#65292;GraphScholarBERT&#21487;&#20197;&#23558;&#25552;&#21462;&#30340;F1&#24471;&#20998;&#25552;&#39640;&#22810;&#36798;34.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14129v1 Announce Type: cross  Abstract: Relation extraction is an efficient way of mining the extraordinary wealth of human knowledge on the Web. Existing methods rely on domain-specific training data or produce noisy outputs. We focus here on extracting targeted relations from semi-structured web pages given only a short description of the relation. We present GraphScholarBERT, an open-domain information extraction method based on a joint graph and language model structure. GraphScholarBERT can generalize to previously unseen domains without additional data or training and produces only clean extraction results matched to the search keyword. Experiments show that GraphScholarBERT can improve extraction F1 scores by as much as 34.8\% compared to previous work in a zero-shot domain and zero-shot website setting.
&lt;/p&gt;</description></item><item><title>FanOutQA &#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#20132;&#21449;&#25991;&#26723;&#20381;&#36182;&#25512;&#29702;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.14116</link><description>&lt;p&gt;
FanOutQA&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14116
&lt;/p&gt;
&lt;p&gt;
FanOutQA &#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#20132;&#21449;&#25991;&#26723;&#20381;&#36182;&#25512;&#29702;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#24120;&#35265;&#20110;&#26085;&#24120;&#22330;&#26223;&#20013;&#30340;&#38382;&#39064;&#31867;&#22411;&#26159;&#8220;fan-out&#8221;&#38382;&#39064;&#65292;&#21363;&#22797;&#26434;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#25512;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#25214;&#21040;&#22823;&#37327;&#23454;&#20307;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#36164;&#28304;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;LLMs&#20013;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FanOutQA&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;fan-out&#38382;&#39064;-&#31572;&#26696;&#23545;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33521;&#25991;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#20154;&#24037;&#27880;&#37322;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#19978;&#21046;&#23450;&#20102;&#19977;&#31181;&#22522;&#20934;&#35774;&#32622;&#65292;&#24182;&#23545;7&#20010;LLMs&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;GPT-4&#12289;LLaMA 2&#12289;Claude-2.1&#21644;Mixtral-8x7B&#65292;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#25512;&#29702;&#36328;&#25991;&#26723;&#20381;&#36182;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20379;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#24320;&#28304;&#24037;&#20855;&#26469;&#36816;&#34892;&#27169;&#22411;&#65292;&#20197;&#40723;&#21169;&#22312;https://fanoutqa.com&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14116v1 Announce Type: cross  Abstract: One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#26631;&#27880;&#32773;&#35843;&#36866;&#26469;&#23454;&#29616;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#36827;&#34892;&#39640;&#25928;&#26631;&#27880;&#19982;&#24314;&#27169;&#65292;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#26631;&#27880;&#39044;&#31639;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14101</link><description>&lt;p&gt;
&#36890;&#36807;&#23569;&#26679;&#26412;&#26631;&#27880;&#32773;&#35843;&#36866;&#23454;&#29616;&#39640;&#25928;&#20027;&#35266;&#20219;&#21153;&#26631;&#27880;&#19982;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14101
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#26631;&#27880;&#32773;&#35843;&#36866;&#26469;&#23454;&#29616;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#36827;&#34892;&#39640;&#25928;&#26631;&#27880;&#19982;&#24314;&#27169;&#65292;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#26631;&#27880;&#39044;&#31639;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20027;&#35266;&#24615;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#19981;&#23384;&#22312;&#21333;&#19968;&#30340;&#26631;&#20934;&#31572;&#26696;&#65292;&#21253;&#25324;&#19981;&#21516;&#26631;&#27880;&#32773;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#20182;&#20204;&#29420;&#29305;&#30340;&#35270;&#35282;&#26174;&#33879;&#24433;&#21709;&#26631;&#27880;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26631;&#27880;&#39044;&#31639;&#36890;&#24120;&#25104;&#20026;&#20915;&#23450;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#35270;&#35282;&#25968;&#37327;&#65288;&#21363;&#26631;&#27880;&#32773;&#65289;&#21450;&#21518;&#32493;&#24314;&#27169;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#36827;&#34892;&#26631;&#27880;&#25910;&#38598;&#21644;&#24314;&#27169;&#65292;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#26631;&#27880;&#39044;&#31639;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20004;&#38454;&#27573;&#35774;&#35745;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20381;&#36182;&#19968;&#23567;&#32452;&#26631;&#27880;&#32773;&#26469;&#26500;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#27599;&#20010;&#26631;&#27880;&#32773;&#31574;&#30053;&#24615;&#22320;&#26631;&#27880;&#23569;&#37327;&#26679;&#26412;&#65292;&#26469;&#20026;&#26032;&#35270;&#35282;&#22686;&#24378;&#27169;&#22411;&#12290;&#20026;&#20102;&#22312;&#35268;&#27169;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#12298;&#36947;&#24503;&#22522;&#30784;&#20027;&#35266;&#35821;&#26009;&#24211;&#12299;&#65292;&#21253;&#21547;2000&#20010;Reddit&#24086;&#23376;&#65292;&#30001;24&#21517;&#26631;&#27880;&#32773;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14101v1 Announce Type: new  Abstract: In subjective NLP tasks, where a single ground truth does not exist, the inclusion of diverse annotators becomes crucial as their unique perspectives significantly influence the annotations. In realistic scenarios, the annotation budget often becomes the main determinant of the number of perspectives (i.e., annotators) included in the data and subsequent modeling. We introduce a novel framework for annotation collection and modeling in subjective tasks that aims to minimize the annotation budget while maximizing the predictive performance for each annotator. Our framework has a two-stage design: first, we rely on a small set of annotators to build a multitask model, and second, we augment the model for a new perspective by strategically annotating a few samples per annotator. To test our framework at scale, we introduce and release a unique dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by 24 annotators for 
&lt;/p&gt;</description></item><item><title>LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14086</link><description>&lt;p&gt;
LexC-Gen: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21452;&#35821;&#35789;&#27719;&#34920;&#20026;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#29983;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14086
&lt;/p&gt;
&lt;p&gt;
LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#21294;&#20047;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#35760;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#36880;&#23383;&#32763;&#35793;&#26469;&#35299;&#20915;&#65292;&#28982;&#32780;&#65292;&#21452;&#35821;&#35789;&#20856;&#36890;&#24120;&#19982;&#20219;&#21153;&#25968;&#25454;&#26377;&#38480;&#30340;&#35789;&#27719;&#37325;&#21472;&#65292;&#23548;&#33268;&#32763;&#35793;&#35206;&#30422;&#21644;&#35789;&#20856;&#21033;&#29992;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LexC-Gen&#30340;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LexC-Gen&#39318;&#20808;&#20351;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#21333;&#35789;&#29983;&#25104;&#19982;&#35789;&#20856;&#20860;&#23481;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21333;&#35789;&#32763;&#35793;&#23558;&#20854;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;17&#31181;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;LexC-Gen&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#19982;&#19987;&#23478;&#32763;&#35793;&#30340;&#40644;&#37329;&#25968;&#25454;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#19978;&#24179;&#22343;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#21333;&#35789;&#32763;&#35793;&#26041;&#27861;&#25552;&#39640;&#20102;5.6&#21644;8.9&#20010;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14086v1 Announce Type: cross  Abstract: Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23631;&#24149;&#25130;&#22270;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;Patch-and-Text Prediction&#65288;PTP&#65289;&#30446;&#26631;&#26469;&#25913;&#21892;&#25991;&#26412;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14073</link><description>&lt;p&gt;
&#20174;&#23631;&#24149;&#25130;&#22270;&#20013;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Language Understanding from Screenshots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23631;&#24149;&#25130;&#22270;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;Patch-and-Text Prediction&#65288;PTP&#65289;&#30446;&#26631;&#26469;&#25913;&#21892;&#25991;&#26412;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26032;&#20852;&#30340;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#65288;LMs&#65289;&#21487;&#20197;&#22788;&#29702;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#22312;&#21333;&#20010;&#35270;&#35273;&#35270;&#22270;&#20869;&#65292;&#26377;&#26395;&#25299;&#23485;&#22270;&#34920;&#29702;&#35299;&#21644;UI&#23548;&#33322;&#31561;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#27169;&#22411;&#20026;&#23631;&#24149;&#25130;&#22270;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#23631;&#24149;&#25130;&#22270;LMs&#22312;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#26126;&#26174;&#33853;&#21518;&#20110;&#20165;&#25991;&#26412;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#27169;&#22411;&#36755;&#20837;&#26159;&#32431;&#25991;&#26412;&#28210;&#26579;&#30340;&#23631;&#24149;&#25130;&#22270;&#65292;&#24182;&#38598;&#20013;&#22312;&#25552;&#39640;&#23631;&#24149;&#25130;&#22270;LMs&#30340;&#25991;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-and-Text Prediction&#65288;PTP&#65289;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#36974;&#30422;&#21644;&#24674;&#22797;&#23631;&#24149;&#25130;&#22270;&#20013;&#30340;&#22270;&#20687;&#22359;&#21644;&#25991;&#26412;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22823;&#37327;&#28040;&#34701;&#30740;&#31350;&#65292;&#28041;&#21450;&#36974;&#30422;&#29575;&#12289;&#22359;&#22823;&#23567;&#20197;&#21450;&#29992;&#20110;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20165;&#37319;&#29992;&#35270;&#35273;&#36755;&#20837;&#65292;&#23601;&#22312;8&#20010;GLUE&#20013;&#30340;6&#20010;&#19978;&#23454;&#29616;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14073v1 Announce Type: new  Abstract: An emerging family of language models (LMs), capable of processing both text and images within a single visual view, has the promise to unlock complex tasks such as chart understanding and UI navigation. We refer to these models as screenshot language models. Despite their appeal, existing screenshot LMs substantially lag behind text-only models on language understanding tasks. To close this gap, we adopt a simplified setting where the model inputs are plain-text-rendered screenshots, and we focus on improving the text ability of screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective, which masks and recovers both image patches of screenshots and text within screenshots. We also conduct extensive ablation studies on masking rates and patch sizes, as well as designs for improving training stability. Our pre-trained model, while solely taking visual inputs, achieves comparable performance with BERT on 6 out of 8 GLUE 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20013;&#21033;&#29992;&#20165;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#36164;&#28304;&#35774;&#32622;&#19979;&#39046;&#22495;&#20869;&#32534;&#30721;&#22120;-only&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14052</link><description>&lt;p&gt;
&#21033;&#29992;&#20165;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20013;&#21033;&#29992;&#20165;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#36164;&#28304;&#35774;&#32622;&#19979;&#39046;&#22495;&#20869;&#32534;&#30721;&#22120;-only&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39046;&#22495;&#29305;&#23450;&#32534;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#26356;&#24191;&#27867;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#20165;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24212;&#29992;&#20110;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#65288;KPG&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#65288;1&#65289;&#32534;&#30721;&#22120;-only PLMs&#22312;KPG&#20013;&#30340;&#21151;&#25928;&#65292;&#65288;2&#65289;&#22312;KPG&#20013;&#20351;&#29992;&#32534;&#30721;&#22120;-only PLMs&#30340;&#26368;&#20339;&#26550;&#26500;&#20915;&#31574;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19981;&#21516;&#36164;&#28304;&#35774;&#32622;&#19979;&#39046;&#22495;&#20869;&#32534;&#30721;&#22120;-only&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;PLMs&#20043;&#38388;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#36890;&#36807;&#22312;&#20004;&#20010;&#39046;&#22495;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#24471;&#20986;&#65292;&#34920;&#26126;&#23613;&#31649;&#24102;&#26377;&#26465;&#20214;&#38543;&#26426;&#22330;&#30340;KPE&#22312;&#35782;&#21035;&#24403;&#21069;&#20851;&#38190;&#30701;&#35821;&#26041;&#38754;&#31245;&#24494;&#20248;&#20110;&#20165;&#32534;&#30721;&#22120;PLMs&#65292;&#20294;KPG&#20844;&#24335;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#20851;&#38190;&#30701;&#35821;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20165;&#32534;&#30721;&#22120;PLMs&#30340;&#21069;&#32512;LM&#24494;&#35843;&#20986;&#29616;&#20026;KPG&#30340;&#24378;&#22823;&#19988;&#39640;&#25928;&#31574;&#30053;&#65292;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;seq2seq PLMs&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14052v1 Announce Type: new  Abstract: This study addresses the application of encoder-only Pre-trained Language Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of domain-tailored encoder-only models compared to encoder-decoder models. We investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder PLMs across varied resource settings. Our findings, derived from extensive experimentation in two domains reveal that with encoder-only PLMs, although KPE with Conditional Random Fields slightly excels in identifying present keyphrases, the KPG formulation renders a broader spectrum of keyphrase predictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs. We also identify
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13764</link><description>&lt;p&gt;
CriticBench: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#35770;&#23478;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Evaluating Large Language Models as Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13764
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102; CriticBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22235;&#20010;&#20851;&#38190;&#35780;&#35770;&#33021;&#21147;&#32500;&#24230;&#65288;&#21453;&#39304;&#12289;&#27604;&#36739;&#12289;&#25913;&#36827;&#21644;&#20803;&#21453;&#39304;&#65289;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;CriticBench&#21253;&#21547;&#20061;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#36136;&#37327;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#35770;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#26377;&#36259;&#30340;&#20851;&#31995;&#12290;CriticBench&#30340;&#25968;&#25454;&#38598;&#12289;&#36164;&#28304;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#23558;&#22312;https://github.com/gmftbyGMFTBY/Cri&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;$\infty$Bench&#65292;&#31532;&#19968;&#20010;&#20197;&#24179;&#22343;&#25968;&#25454;&#38271;&#24230;&#36229;&#36807;10&#19975;&#20010;&#20196;&#29260;&#30340;LLM&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13718</link><description>&lt;p&gt;
$\infty$Bench: &#23558;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#25193;&#23637;&#33267;&#36229;&#36807;10&#19975;&#20196;&#29260;
&lt;/p&gt;
&lt;p&gt;
$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;$\infty$Bench&#65292;&#31532;&#19968;&#20010;&#20197;&#24179;&#22343;&#25968;&#25454;&#38271;&#24230;&#36229;&#36807;10&#19975;&#20010;&#20196;&#29260;&#30340;LLM&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#21644;&#25512;&#29702;&#38271;&#19978;&#19979;&#25991;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#25991;&#26723;&#29702;&#35299;&#21644;&#20195;&#29702;&#26500;&#24314;&#12290;&#26412;&#25991;&#25552;&#20986;$\infty$Bench&#65292;&#31532;&#19968;&#20010;LLM&#22522;&#20934;&#65292;&#24179;&#22343;&#25968;&#25454;&#38271;&#24230;&#36229;&#36807;10&#19975;&#20010;&#20196;&#29260;&#12290;$\infty$Bench&#21253;&#21547;&#28085;&#30422;&#19981;&#21516;&#39046;&#22495;&#30340;&#21512;&#25104;&#21644;&#29616;&#23454;&#20219;&#21153;&#65292;&#20197;&#33521;&#25991;&#21644;&#20013;&#25991;&#21576;&#29616;&#12290;$\infty$Bench&#20013;&#30340;&#20219;&#21153;&#26088;&#22312;&#38656;&#35201;&#28145;&#21051;&#29702;&#35299;&#19978;&#19979;&#25991;&#20013;&#30340;&#38271;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#31616;&#21333;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#26377;&#38480;&#25968;&#37327;&#30340;&#27573;&#33853;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13718v1 Announce Type: new  Abstract: Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;CODIS&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#21033;&#29992;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#22522;&#20934;&#19978;&#34920;&#29616;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#65292;&#38656;&#35201;&#25552;&#21319;&#27169;&#22411;&#29702;&#35299;&#35270;&#35273;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13607</link><description>&lt;p&gt;
CODIS&#65306;&#20026;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#21270;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#35270;&#35273;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13607
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;CODIS&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#21033;&#29992;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#22522;&#20934;&#19978;&#34920;&#29616;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#65292;&#38656;&#35201;&#25552;&#21319;&#27169;&#22411;&#29702;&#35299;&#35270;&#35273;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#32467;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22312;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65292;&#23545;&#23427;&#20204;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#32771;&#34385;&#21040;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22270;&#20687;&#38656;&#35201;&#22312;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20013;&#34987;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CODIS&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#20351;&#29992;&#22312;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#20013;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;MLLMs&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#22987;&#32456;&#26080;&#27861;&#36798;&#21040;&#20154;&#31867;&#34920;&#29616;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#35777;&#23454;&#20102;&#36825;&#20123;&#27169;&#22411;&#38590;&#20197;&#26377;&#25928;&#25552;&#21462;&#21644;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25552;&#39640;&#23427;&#20204;&#23545;&#22270;&#20687;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#20984;&#26174;&#20102;&#25552;&#21319;MLLMs&#29702;&#35299;&#35270;&#35273;&#33021;&#21147;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13607v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a 
&lt;/p&gt;</description></item><item><title>KorNAT&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13605</link><description>&lt;p&gt;
KorNAT&#65306;&#38889;&#22269;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#30340;LLM&#23545;&#40784;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13605
&lt;/p&gt;
&lt;p&gt;
KorNAT&#26159;&#39318;&#20010;&#29992;&#20110;&#35780;&#20272;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#31038;&#20250;&#20215;&#20540;&#35266;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29305;&#23450;&#22269;&#23478;&#24471;&#20197;&#26377;&#25928;&#37096;&#32626;&#65292;&#23427;&#20204;&#24517;&#39035;&#20855;&#26377;&#23545;&#35813;&#22269;&#25991;&#21270;&#21644;&#22522;&#26412;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22269;&#23478;&#23545;&#40784;&#65288;National Alignment&#65289;&#65292;&#20174;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#21644;&#24120;&#35782;&#23545;&#40784;&#20004;&#20010;&#26041;&#38754;&#34913;&#37327;LLM&#19982;&#30446;&#26631;&#22269;&#23478;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#35780;&#20272;&#27169;&#22411;&#23545;&#29305;&#23450;&#22269;&#23478;&#31038;&#20250;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#31243;&#24230;&#65292;&#32780;&#24120;&#35782;&#23545;&#40784;&#21017;&#26816;&#39564;&#27169;&#22411;&#23545;&#30456;&#20851;&#22522;&#26412;&#22269;&#23478;&#30693;&#35782;&#30340;&#25226;&#25569;&#24773;&#20917;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;KorNAT&#65292;&#36825;&#26159;&#39318;&#20010;&#34913;&#37327;&#19982;&#38889;&#22269;&#22269;&#23478;&#23545;&#40784;&#30340;&#22522;&#20934;&#12290;&#23545;&#20110;&#31038;&#20250;&#20215;&#20540;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#21253;&#25324;6174&#21517;&#38889;&#22269;&#21442;&#19982;&#32773;&#22312;&#20869;&#30340;&#22823;&#35268;&#27169;&#35843;&#26597;&#20013;&#33719;&#24471;&#20102;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#23545;&#20110;&#24120;&#35782;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22522;&#20110;&#38889;&#22269;&#25945;&#31185;&#20070;&#21644;GED&#21442;&#32771;&#36164;&#26009;&#26500;&#24314;&#20102;&#26679;&#26412;&#12290;KorNAT&#21253;&#21547;4K&#21644;6K&#20010;&#38024;&#23545;&#31038;&#20250;&#20215;&#20540;&#21644;&#24120;&#35782;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13605v1 Announce Type: new  Abstract: For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for socia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#38271;&#25991;&#26412;&#35780;&#20272;&#30340;&#24046;&#36317;&#65292;&#24341;&#20837;&#20102;&#19968;&#22871;&#22522;&#20110;&#36830;&#36143;&#24615;&#12289;&#20957;&#32858;&#21147;&#21644;&#22797;&#26434;&#24615;&#31561;&#35821;&#35328;&#23398;&#32500;&#24230;&#30340;&#25351;&#26631;&#26469;&#31995;&#32479;&#24615;&#34913;&#37327;&#38271;&#25991;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;LongWanjuan&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#38271;&#25991;&#26412;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.13583</link><description>&lt;p&gt;
LongWanjuan: &#38754;&#21521;&#38271;&#25991;&#26412;&#36136;&#37327;&#30340;&#31995;&#32479;&#21270;&#34913;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LongWanjuan: Towards Systematic Measurement for Long Text Quality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#38271;&#25991;&#26412;&#35780;&#20272;&#30340;&#24046;&#36317;&#65292;&#24341;&#20837;&#20102;&#19968;&#22871;&#22522;&#20110;&#36830;&#36143;&#24615;&#12289;&#20957;&#32858;&#21147;&#21644;&#22797;&#26434;&#24615;&#31561;&#35821;&#35328;&#23398;&#32500;&#24230;&#30340;&#25351;&#26631;&#26469;&#31995;&#32479;&#24615;&#34913;&#37327;&#38271;&#25991;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;LongWanjuan&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#38271;&#25991;&#26412;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#22686;&#24378;&#22522;&#30784;&#27169;&#22411;&#30340;&#38271;&#25991;&#26412;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36890;&#36807;&#21551;&#21457;&#24335;&#35268;&#21017;&#21644;&#22522;&#20110;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#38590;&#24230;&#30340;&#35780;&#20272;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#30340;&#29616;&#26377;&#21162;&#21147;&#65292;&#20294;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#38271;&#25991;&#26412;&#35780;&#20272;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#36890;&#36807;&#35780;&#20272;&#19977;&#20010;&#22522;&#26412;&#35821;&#35328;&#23398;&#32500;&#24230;&#65288;&#36830;&#36143;&#24615;&#12289;&#20957;&#32858;&#21147;&#21644;&#22797;&#26434;&#24615;&#65289;&#31995;&#32479;&#24615;&#34913;&#37327;&#38271;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#21463;&#21040;&#36825;&#19977;&#20010;&#32500;&#24230;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#26088;&#22312;&#35780;&#20272;&#38271;&#25991;&#26412;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#21253;&#25324;&#32479;&#35745;&#21644;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#20123;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LongWanjuan&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#38271;&#25991;&#26412;&#20219;&#21153;&#35757;&#32451;&#30340;&#21452;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;1600&#20159;&#26631;&#35760;&#12290;&#22312;LongWanjuan&#20013;&#65292;&#25105;&#20204;&#23558;&#38271;&#25991;&#26412;&#20998;&#31867;&#20026;&#25972;&#20307;&#24615;&#12289;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13583v1 Announce Type: new  Abstract: The quality of training data are crucial for enhancing the long-text capabilities of foundation models. Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#65292;&#36890;&#36807;&#25366;&#25496;&#31038;&#20132;&#32593;&#32476;&#20013;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2402.13528</link><description>&lt;p&gt;
&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#65306;&#20174;&#32467;&#26500;&#28798;&#38590;&#21709;&#24212;&#20013;&#25366;&#25496;&#26410;&#26469;&#22833;&#25928;&#25285;&#24551;
&lt;/p&gt;
&lt;p&gt;
Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#65292;&#36890;&#36807;&#25366;&#25496;&#31038;&#20132;&#32593;&#32476;&#20013;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30740;&#31350;&#38598;&#20013;&#20110;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#19978;&#19982;&#32467;&#26500;&#22833;&#36133;&#30456;&#20851;&#30340;&#35752;&#35770;&#65292;&#20197;&#25913;&#36827;&#28798;&#38590;&#21709;&#24212;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#31038;&#20132;&#32593;&#32476;&#24086;&#23376;&#20013;&#35752;&#35770;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#26159;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#12290;&#22914;&#26524;&#36825;&#20123;&#25285;&#24551;&#34987;&#20256;&#36798;&#32473;&#36866;&#24403;&#30340;&#26426;&#26500;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#8212;&#8212;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#32654;&#22269;&#20960;&#36215;&#26368;&#36817;&#30340;&#32467;&#26500;&#22833;&#25928;&#20107;&#20214;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#20221;&#39318;&#21019;&#24615;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;Reddit&#21644;YouTube&#20013;&#25366;&#25496;&#30340;2,662&#20010;&#31038;&#20132;&#32593;&#32476;&#23454;&#20363;&#65292;&#29992;&#20110;&#36825;&#19968;&#26032;&#39062;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13528v1 Announce Type: cross  Abstract: Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2402.13463</link><description>&lt;p&gt;
RefuteBench&#65306;&#35780;&#20272;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39539;&#25351;&#20196;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#26085;&#30410;&#25193;&#22823;&#12290;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#26681;&#25454;&#27169;&#22411;&#30340;&#36755;&#20986;&#25552;&#20379;&#21453;&#39304;&#65292;&#24076;&#26395;&#24471;&#21040;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#39304;&#23436;&#25104;&#21709;&#24212;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33021;&#21542;&#24688;&#24403;&#22320;&#21709;&#24212;&#29992;&#25143;&#30340;&#21453;&#39539;&#21453;&#39304;&#24182;&#22987;&#32456;&#25191;&#34892;&#19979;&#21435;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;RefuteBench&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#20219;&#21153;&#12290;&#35780;&#20272;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#31215;&#26497;&#25509;&#21463;&#21453;&#39539;&#25351;&#20196;&#24418;&#24335;&#30340;&#21453;&#39304;&#65292;&#24182;&#26159;&#21542;&#33021;&#22815;&#22312;&#23545;&#35805;&#20013;&#22987;&#32456;&#36981;&#24490;&#29992;&#25143;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#20247;&#22810;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;LLMs&#20542;&#21521;&#22266;&#25191;&#65292;&#21363;&#20542;&#21521;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#65292;&#32463;&#24120;&#26410;&#33021;&#36981;&#23432;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13463v1 Announce Type: cross  Abstract: The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the leng
&lt;/p&gt;</description></item><item><title>EvoGrad&#26159;&#19968;&#20010;&#20197;&#20154;&#31867;&#23545;&#25163;&#20026;&#29305;&#28857;&#30340;&#29992;&#20110;&#35299;&#20915;Winograd Schema&#25361;&#25112;&#30340;&#21160;&#24577;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#22312;&#29615;&#20013;&#26041;&#27861;&#21019;&#24314;&#21160;&#24577;&#25968;&#25454;&#38598;&#65292;&#25299;&#23637;&#20219;&#21153;&#23454;&#20363;&#24182;&#24341;&#20837;&#38169;&#35823;&#28145;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#25552;&#20986;&#26032;&#30340;&#22810;&#26679;&#21270;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#31867;&#20219;&#21153;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13372</link><description>&lt;p&gt;
EvoGrad&#65306;&#20197;&#20154;&#31867;&#23545;&#25163;&#20026;&#29305;&#28857;&#30340;Winograd Schema&#25361;&#25112;&#30340;&#21160;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13372
&lt;/p&gt;
&lt;p&gt;
EvoGrad&#26159;&#19968;&#20010;&#20197;&#20154;&#31867;&#23545;&#25163;&#20026;&#29305;&#28857;&#30340;&#29992;&#20110;&#35299;&#20915;Winograd Schema&#25361;&#25112;&#30340;&#21160;&#24577;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#22312;&#29615;&#20013;&#26041;&#27861;&#21019;&#24314;&#21160;&#24577;&#25968;&#25454;&#38598;&#65292;&#25299;&#23637;&#20219;&#21153;&#23454;&#20363;&#24182;&#24341;&#20837;&#38169;&#35823;&#28145;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#25552;&#20986;&#26032;&#30340;&#22810;&#26679;&#21270;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#31867;&#20219;&#21153;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;Winograd Schema Challenge&#65288;WSC&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35813;&#20219;&#21153;&#36890;&#36807;&#20195;&#35789;&#28040;&#27495;&#20041;&#27979;&#35797;&#24120;&#35782;&#25512;&#29702;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#21253;&#21547;&#36731;&#24494;&#20462;&#25913;&#25110;&#25913;&#20889;&#30340;&#23454;&#20363;&#24863;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;EvoGrad&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#21033;&#29992;&#20154;&#22312;&#29615;&#20013;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#19968;&#20010;&#36866;&#29992;&#20110;&#36825;&#31181;&#20462;&#25913;&#21518;WSC&#23454;&#20363;&#30340;&#21160;&#24577;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;ChatGPT&#30340;&#21151;&#33021;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20219;&#21153;&#23454;&#20363;&#20174;182&#25193;&#23637;&#21040;3,691&#20010;&#65292;&#20026;&#22810;&#26679;&#21270;&#30340;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#35774;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38169;&#35823;&#28145;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#21160;&#24577;&#20219;&#21153;&#20013;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;EvoGrad&#25152;&#25552;&#20986;&#30340;&#25361;&#25112;&#65306;&#21363;&#20351;&#24615;&#33021;&#26368;&#20339;&#30340;LLM&#65292;GPT-3.5&#65292;&#22312;&#24179;&#22343;&#38169;&#35823;&#28145;&#24230;&#20026;7.2&#30340;&#24773;&#20917;&#19979;&#20165;&#36798;&#21040;65.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;&#20154;&#31867;92.8%&#30340;&#20934;&#30830;&#29575;&#24418;&#25104;&#20102;&#40092;&#26126;&#23545;&#27604;&#65292;&#20154;&#31867;&#20934;&#30830;&#29575;&#27809;&#26377;&#24178;&#25200;&#24615;&#38169;&#35823;&#12290;&#36825;&#31361;&#26174;&#20102;&#25345;&#32493;&#23384;&#22312;&#30340;&#27169;&#22411;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13372v1 Announce Type: new  Abstract: While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a human-in-the-loop approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging ChatGPT's capabilities, we expand our task instances from 182 to 3,691, setting a new benchmark for diverse common-sense reasoning datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92. 8% accuracy without perturbation errors. This highlights ongoing model limita
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12997</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#20877;&#25490;&#24207;&#65306;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#24323;&#26435;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#65288;NIR&#65289;&#24050;&#32463;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;IR&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22833;&#36133;&#20173;&#28982;&#39057;&#32321;&#21457;&#29983;&#65292;&#36890;&#24120;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26080;&#27861;&#26816;&#32034;&#19982;&#29992;&#25143;&#26597;&#35810;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29305;&#21035;&#24378;&#35843;&#20877;&#25490;&#24207;&#38454;&#27573;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21327;&#35758;&#65292;&#29992;&#20110;&#22312;&#40657;&#21283;&#23376;&#22330;&#26223;&#20013;&#35780;&#20272;&#24323;&#26435;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#22797;&#21046;&#21644;&#24323;&#26435;&#23454;&#26045;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#20419;&#36827;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#26041;&#27861;SlimPLM&#65292;&#36890;&#36807;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#26816;&#27979;LLMs&#20013;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#30693;&#35782;&#33719;&#21462;&#36807;&#31243;</title><link>https://arxiv.org/abs/2402.12052</link><description>&lt;p&gt;
&#23567;&#27169;&#22411;&#65292;&#22823;&#35265;&#35299;&#65306;&#21033;&#29992;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#30830;&#23450;LLMs&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#26041;&#27861;SlimPLM&#65292;&#36890;&#36807;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#26816;&#27979;LLMs&#20013;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#30693;&#35782;&#33719;&#21462;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12052v1 &#20844;&#21578;&#31867;&#22411;:&#26032;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#25972;&#21512;&#20195;&#34920;&#20102;&#30693;&#35782;&#33719;&#21462;&#26041;&#27861;&#30340;&#37325;&#35201;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;LLM&#24050;&#32463;&#20855;&#22791;&#30340;&#30693;&#35782;&#21644;&#38656;&#35201;&#25628;&#32034;&#24341;&#25806;&#24110;&#21161;&#30340;&#30693;&#35782;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;LLM&#26412;&#36523;&#39044;&#22788;&#29702;&#31572;&#26696;&#25110;&#25512;&#29702;&#30340;&#32467;&#26524;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#36807;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#26041;&#27861;&#65292;&#21363;SlimPLM&#65292;&#36890;&#36807;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#26816;&#27979;LLMs&#20013;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#30693;&#35782;&#33719;&#21462;&#36807;&#31243;&#12290;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#36828;&#36828;&#26356;&#23569;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#31572;&#26696;&#35270;&#20026;&#21551;&#21457;&#24335;&#31572;&#26696;&#12290;&#28982;&#21518;&#21033;&#29992;&#21551;&#21457;&#24335;&#31572;&#26696;&#26469;&#39044;&#27979;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;LLM&#20013;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#21482;&#20026;&#26410;&#30693;&#30693;&#35782;&#36827;&#34892;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12052v1 Announce Type: new  Abstract: The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the mis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#23558;LSC&#38382;&#39064;&#20998;&#35299;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;APD&#22312;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;XL-LEXEME&#22312;WiC&#12289;WSI&#21644;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;GPT-4&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.12011</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#30340;&#31995;&#32479;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#23558;LSC&#38382;&#39064;&#20998;&#35299;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;APD&#22312;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;XL-LEXEME&#22312;WiC&#12289;WSI&#21644;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;GPT-4&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#26159;&#24314;&#27169;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#65288;LSC&#65289;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#36890;&#24120;&#19987;&#27880;&#20110;&#31216;&#20026;&#20998;&#32423;&#21464;&#21270;&#26816;&#27979;&#65288;GCD&#65289;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#19981;&#21516;&#35774;&#32622;&#65292;&#36328;&#20316;&#21697;&#30340;&#24615;&#33021;&#27604;&#36739;&#32463;&#24120;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#35780;&#20272;&#20102;GCD&#30340;&#26368;&#26032;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;LSC&#38382;&#39064;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#35789;&#65288;WiC&#65289;&#21644;&#35789;&#20041;&#24402;&#32435;&#65288;WSI&#65289;&#20219;&#21153;&#65292;&#24182;&#27604;&#36739;&#36825;&#20123;&#19981;&#21516;&#32423;&#21035;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#21487;&#29992;&#30340;LSC&#22522;&#20934;&#27979;&#35797;&#20013;&#36328;&#19981;&#21516;&#35821;&#35328;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;APD&#22312;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#65288;ii&#65289;XL-LEXEME&#22312;WiC&#12289;WSI&#21644;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#19982;GPT-4&#30456;&#24403;&#65307;&#65288;iii&#65289;&#26126;&#26174;&#38656;&#35201;&#25913;&#36827;&#23545;&#35789;&#20041;&#24314;&#27169;&#20197;&#21450;&#20851;&#27880;&#36825;&#20123;&#24847;&#20041;&#20309;&#26102;&#12289;&#22914;&#20309;&#21644;&#20026;&#20309;&#21464;&#21270;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12011v1 Announce Type: new  Abstract: Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on how, when, and why these meanings change, rather t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.11753</link><description>&lt;p&gt;
ArtPrompt: &#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#23545;&#40784;LLMs&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11753
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#22914;&#25968;&#25454;&#36807;&#28388;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21152;&#24378;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24050;&#30693;&#30340;&#25216;&#26415;&#20551;&#35774;&#29992;&#20110;&#23545;&#40784;LLMs&#23433;&#20840;&#24615;&#30340;&#35821;&#26009;&#24211;&#20165;&#30001;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#19981;&#25104;&#31435;&#65292;&#23548;&#33268;LLMs&#23384;&#22312;&#20005;&#37325;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;Vision-in-Text Challenge&#65288;ViTC&#65289;&#26469;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#19981;&#33021;&#20165;&#36890;&#36807;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#30340;&#25552;&#31034;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20116;&#20010;SOTA LLMs&#65288;GPT-3.5&#12289;GPT-4&#12289;Gemini&#12289;Claude&#21644;Llama2&#65289;&#22312;&#35782;&#21035;&#20197;ASCII&#33402;&#26415;&#24418;&#24335;&#25552;&#20379;&#30340;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2402.11525</link><description>&lt;p&gt;
&#29992;RLHF&#25512;&#36827;&#32763;&#35793;&#20559;&#22909;&#24314;&#27169;&#65306;&#36808;&#21521;&#25104;&#26412;&#25928;&#30410;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#30495;&#23454;&#24615;&#12289;&#34920;&#36798;&#21147;&#21644;&#20248;&#38597;&#26159;&#26426;&#22120;&#32763;&#35793;&#20013;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#22914;BLEU&#24182;&#19981;&#20005;&#26684;&#31526;&#21512;&#20154;&#31867;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;&#25910;&#38598;&#20154;&#31867;&#23545;&#32763;&#35793;&#20043;&#38388;&#30340;&#27604;&#36739;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#26469;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26426;&#22120;&#32763;&#35793;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25351;&#23548;&#38543;&#21518;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RLHF&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#31181;&#25913;&#36827;&#20063;&#26377;&#30410;&#20110;&#20854;&#20182;&#32763;&#35793;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#23384;&#22312;&#19968;&#20010;&#20215;&#20540;&#20559;&#22909;&#30340;&#26426;&#21046;&#65292;&#20542;&#21521;&#20110;&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#65292;&#36825;&#31181;&#20559;&#24046;&#20250;&#23545;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11005</link><description>&lt;p&gt;
&#25506;&#31350;&#20215;&#20540;&#20559;&#22909;&#65306;LLMs&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Exploring Value Biases: How LLMs Deviate Towards the Ideal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11005
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#23384;&#22312;&#19968;&#20010;&#20215;&#20540;&#20559;&#22909;&#30340;&#26426;&#21046;&#65292;&#20542;&#21521;&#20110;&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#65292;&#36825;&#31181;&#20559;&#24046;&#20250;&#23545;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#37096;&#32626;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#21709;&#24212;&#23545;&#31038;&#20250;&#20135;&#29983;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#24433;&#21709;&#12290;&#29702;&#35299;LLMs&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#30340;&#38750;&#25925;&#24847;&#26426;&#21046;&#23545;&#20110;&#35299;&#37322;&#23427;&#20204;&#30340;&#24615;&#33021;&#24182;&#36776;&#21035;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20559;&#24046;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31867;&#20284;&#20110;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#36825;&#31181;&#26080;&#24847;&#35782;&#30340;&#21709;&#24212;&#34987;&#31216;&#20026;&#25277;&#26679;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#36825;&#31181;&#25277;&#26679;&#29616;&#35937;&#65292;&#21457;&#29616;LLMs&#30340;&#25277;&#26679;&#20542;&#21521;&#20110;&#20559;&#29233;&#39640;&#20215;&#20540;&#36873;&#39033;&#12290;&#20215;&#20540;&#20559;&#22909;&#23545;&#24212;&#20110;&#20174;&#26368;&#21487;&#33021;&#30340;&#21709;&#24212;&#21521;LLM&#20013;&#20195;&#34920;&#30340;&#29702;&#24819;&#20215;&#20540;&#30340;&#36716;&#21464;&#12290;&#23454;&#38469;&#19978;&#65292;&#21363;&#20415;&#26159;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#23398;&#20064;&#21040;&#30340;&#26032;&#23454;&#20307;&#65292;&#36825;&#31181;&#25928;&#26524;&#20063;&#33021;&#22815;&#20877;&#29616;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#20559;&#24046;&#34920;&#29616;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22320;&#26041;&#65292;&#24182;&#23545;&#36873;&#25321;&#20856;&#22411;&#23454;&#20363;&#31561;&#30456;&#20851;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20215;&#20540;&#20559;&#22909;&#22312;&#19981;&#21516;&#20998;&#31867;&#30340;LLMs&#20013;&#37117;&#24456;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11005v1 Announce Type: cross  Abstract: Large-Language-Models (LLMs) are deployed in a wide range of applications, and their response has an increasing social impact. Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications. This is analogous to human studies, where such inadvertent responses are referred to as sampling. We study this sampling of LLMs in light of value bias and show that the sampling of LLMs tends to favour high-value options. Value bias corresponds to this shift of response from the most likely towards an ideal value represented in the LLM. In fact, this effect can be reproduced even with new entities learnt via in-context prompting. We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars. The results show that value bias is strong in LLMs across different categor
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26631;&#35760;&#30340;&#34920;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#20855;&#22791;&#26377;&#20851;&#26631;&#35760;&#38271;&#24230;&#21644;&#23376;&#23383;&#31526;&#20018;&#30340;&#30693;&#35782;&#65292;&#20294;&#23545;&#26631;&#35760;&#32467;&#26500;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#35299;&#30721;&#22120;&#26041;&#38754;&#23384;&#22312;&#29942;&#39048;&#12290;</title><link>https://arxiv.org/abs/2402.09808</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26631;&#35760;&#30340;&#34920;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Knowledge of Pretrained Language Models on Surface Information of Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09808
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26631;&#35760;&#30340;&#34920;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#20855;&#22791;&#26377;&#20851;&#26631;&#35760;&#38271;&#24230;&#21644;&#23376;&#23383;&#31526;&#20018;&#30340;&#30693;&#35782;&#65292;&#20294;&#23545;&#26631;&#35760;&#32467;&#26500;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#35299;&#30721;&#22120;&#26041;&#38754;&#23384;&#22312;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#20851;&#20110;&#26631;&#35760;&#34920;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#20174;&#26631;&#35760;&#38271;&#24230;&#12289;&#23376;&#23383;&#31526;&#20018;&#21644;&#26631;&#35760;&#32467;&#26500;&#30340;&#35282;&#24230;&#26816;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#30340;&#35789;&#35821;&#25110;&#23376;&#35789;&#23884;&#20837;&#20013;&#20445;&#23384;&#30340;&#34920;&#38754;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#29983;&#25104;&#26631;&#35760;&#34920;&#38754;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20102;12&#20010;&#20027;&#35201;&#22312;&#33521;&#35821;&#21644;&#26085;&#35821;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26631;&#35760;&#38271;&#24230;&#21644;&#23376;&#23383;&#31526;&#20018;&#20855;&#26377;&#30693;&#35782;&#65292;&#20294;&#23545;&#20110;&#26631;&#35760;&#32467;&#26500;&#21017;&#27809;&#26377;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#25928;&#21033;&#29992;&#33719;&#21462;&#30340;&#30693;&#35782;&#26041;&#38754;&#65292;&#35299;&#30721;&#22120;&#26041;&#38754;&#23384;&#22312;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09808v1 Announce Type: new  Abstract: Do pretrained language models have knowledge regarding the surface information of tokens? We examined the surface information stored in word or subword embeddings acquired by pretrained language models from the perspectives of token length, substrings, and token constitution. Additionally, we evaluated the ability of models to generate knowledge regarding token surfaces. We focused on 12 pretrained language models that were mainly trained on English and Japanese corpora. Experimental results demonstrate that pretrained language models have knowledge regarding token length and substrings but not token constitution. Additionally, the results imply that there is a bottleneck on the decoder side in terms of effectively utilizing acquired knowledge.
&lt;/p&gt;</description></item><item><title>CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.09664</link><description>&lt;p&gt;
CodeMind:&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CodeMind: A Framework to Challenge Large Language Models for Code Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09664
&lt;/p&gt;
&lt;p&gt;
CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#30721;&#21512;&#25104;&#33021;&#21147;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#35780;&#20272;&#25110;&#20419;&#36827;&#20855;&#26377;&#25968;&#25454;&#27844;&#28431;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeMind&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;CodeMind&#30446;&#21069;&#25903;&#25345;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#65306;&#29420;&#31435;&#25191;&#34892;&#25512;&#29702;&#65288;IER&#65289;&#12289;&#20381;&#36182;&#25191;&#34892;&#25512;&#29702;&#65288;DER&#65289;&#21644;&#35268;&#33539;&#25512;&#29702;&#65288;SR&#65289;&#12290;&#21069;&#20004;&#32773;&#35780;&#20272;&#27169;&#22411;&#20197;&#39044;&#27979;&#20219;&#24847;&#20195;&#30721;&#30340;&#25191;&#34892;&#36755;&#20986;&#65292;&#25110;&#32773;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21512;&#25104;&#30340;&#20195;&#30721;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#23454;&#29616;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;CodeMind&#23545;&#20004;&#31181;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20116;&#20010;&#22522;&#20934;&#19979;&#30340;&#20061;&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07610</link><description>&lt;p&gt;
&#36393;&#33050;&#35843;&#26657;&#65306;&#36890;&#36807;&#33258;&#21161;&#24341;&#23548;&#25193;&#23637;LLM&#30340;&#33258;&#23545;&#40784;&#33021;&#21147;&#30340;&#35268;&#27169;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23545;&#40784;&#26159;&#19968;&#31181;&#38477;&#20302;&#20154;&#24037;&#27880;&#37322;&#25104;&#26412;&#24182;&#30830;&#20445;&#27169;&#22411;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#21333;&#27425;&#24490;&#29615;&#20013;&#23436;&#25104;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#27493;&#39588;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#33258;&#23545;&#40784;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#36827;&#34892;&#22810;&#27425;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#65292;&#20250;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#36824;&#26159;&#23548;&#33268;&#24555;&#36895;&#36864;&#21270;&#65311;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20445;&#35777;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#25381;&#33258;&#21161;&#24341;&#23548;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#24182;&#35843;&#25972;&#20102;&#25968;&#25454;&#30340;&#35757;&#32451;&#39034;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36393;&#33050;&#35843;&#26657;&#65288;SOFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27169;&#22411;&#30340;&#25345;&#32493;&#22686;&#24378;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;255&#31687;&#35770;&#25991;&#30340;&#20998;&#26512;&#21644;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#32771;&#34385;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31532;&#19968;&#24180;&#21457;&#24067;&#21518;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03927</link><description>&lt;p&gt;
&#27844;&#28431;&#12289;&#27450;&#39575;&#12289;&#37325;&#22797;&#65306;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;255&#31687;&#35770;&#25991;&#30340;&#20998;&#26512;&#21644;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#32771;&#34385;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31532;&#19968;&#24180;&#21457;&#24067;&#21518;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20854;&#20013;&#19968;&#20123;&#26368;&#21463;&#27426;&#36814;&#30340;&#27169;&#22411;&#26159;&#23436;&#20840;&#25110;&#37096;&#20998;&#23553;&#38381;&#28304;&#30340;&#12290;&#23545;&#20110;&#27169;&#22411;&#32454;&#33410;&#65292;&#29305;&#21035;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#35775;&#38382;&#26435;&#38480;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21453;&#22797;&#23545;&#25968;&#25454;&#27745;&#26579;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#19968;&#20123;&#23581;&#35797;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20165;&#38480;&#20110;&#20010;&#21035;&#26696;&#20363;&#21644;&#35797;&#38169;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#8220;&#38388;&#25509;&#8221;&#25968;&#25454;&#27844;&#28431;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#22312;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#20351;&#29992;&#19978;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#20998;&#26512;&#65292;&#36825;&#20123;&#26159;&#24403;&#20170;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#65292;&#24182;&#32771;&#34385;&#20102;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#65292;&#35814;&#32454;&#35760;&#24405;&#20102;&#27169;&#22411;&#21457;&#24067;&#21518;&#19968;&#24180;&#20869;&#27844;&#38706;&#32473;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20027;&#35201;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been g
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;MOOCs&#35780;&#20998;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As MOOCs Graders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#20026;&#25317;&#26377;&#30005;&#33041;&#21644;&#20114;&#32852;&#32593;&#35775;&#38382;&#26435;&#38480;&#30340;&#20840;&#29699;&#20219;&#20309;&#20154;&#25552;&#20379;&#20813;&#36153;&#25945;&#32946;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20123;&#35838;&#31243;&#30340;&#22823;&#35268;&#27169;&#27880;&#20876;&#24847;&#21619;&#30528;&#19968;&#20301;&#25945;&#24072;&#20960;&#20046;&#19981;&#21487;&#33021;&#35780;&#20272;&#27599;&#20010;&#23398;&#29983;&#30340;&#20889;&#20316;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21516;&#20276;&#35780;&#20998;&#36890;&#24120;&#26159;&#39318;&#36873;&#26041;&#27861;&#65292;&#36890;&#24120;&#30001;&#31616;&#21333;&#26126;&#20102;&#30340;&#35780;&#20998;&#26631;&#20934;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#21516;&#20276;&#35780;&#20998;&#22312;&#21487;&#38752;&#24230;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;18&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26367;&#20195;MOOCs&#20013;&#30340;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65306;GPT-4&#21644;GPT-3.5&#65292;&#24182;&#28085;&#30422;&#19977;&#38376;&#19981;&#21516;&#30340;&#35838;&#31243;&#65306;&#20837;&#38376;&#22825;&#25991;&#23398;&#65292;&#22825;&#20307;&#29983;&#29289;&#23398;&#20197;&#21450;&#22825;&#25991;&#23398;&#30340;&#21382;&#21490;&#19982;&#21746;&#23398;&#12290;&#20026;&#20102;&#35757;&#32451;LLMs&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#38646;-shot&#36830;&#32493;&#24605;&#32771;&#65288;Zero-shot-CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#30340;&#21464;&#31181;&#30340;&#19977;&#20010;&#19981;&#21516;&#25552;&#31034;&#65306;&#32467;&#21512;Zero-shot-CoT&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.03686</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#65306;&#37325;&#26032;&#24605;&#32771;&#35821;&#35328;&#27169;&#22411;&#22312;&#34164;&#21547;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minds versus Machines: Rethinking Entailment Verification with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#25991;&#26412;&#29702;&#35299;&#20013;&#36827;&#34892;&#22823;&#37327;&#30340;&#25512;&#29702;&#20197;&#29702;&#35299;&#35770;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#20154;&#31867;&#21644;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#36890;&#36807;&#32508;&#21512;&#31574;&#21010;&#30340;&#34164;&#21547;&#39564;&#35777;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20154;&#31867;&#21644;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#31867;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#19977;&#20010;&#31867;&#21035;&#65288;NLI&#12289;&#19978;&#19979;&#25991;QA&#21644;&#35299;&#37322;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22810;&#21477;&#21069;&#25552;&#21644;&#19981;&#21516;&#30340;&#30693;&#35782;&#31867;&#22411;&#65292;&#20174;&#32780;&#35780;&#20272;&#20102;&#22797;&#26434;&#25512;&#29702;&#24773;&#20917;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;LLM&#22312;&#36328;&#25193;&#23637;&#19978;&#19979;&#25991;&#30340;&#22810;&#36339;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#38656;&#35201;&#31616;&#21333;&#28436;&#32462;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Flan-T5&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#24182;&#19982;GPT-4&#23218;&#32654;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#20379;&#34164;&#21547;&#39564;&#35777;&#20351;&#29992;&#12290;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02611</link><description>&lt;p&gt;
PuzzleBench&#65306;LLMs&#33021;&#21542;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#65292;&#22914;&#36923;&#36753;&#38382;&#31572;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65292;&#19968;&#20010;&#20363;&#23376;&#26159;&#27969;&#34892;&#30340;&#25968;&#29420;&#35868;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#26377;&#19968;&#20010;&#30001;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22522;&#30784;&#19968;&#38454;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#20363;&#21270;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#25165;&#33021;&#36798;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PuzzleBench&#65292;&#19968;&#20010;&#21253;&#21547;31&#20010;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#22312;&#31526;&#21495;&#27714;&#35299;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;LLMs&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#30456;&#24403;&#31967;&#31957;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Puzzle-LM&#65292;&#23427;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.01719</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#36947;&#24503;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Moral Inconsistencies in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#35821;&#20041;&#31561;&#20215;&#30340;&#25552;&#31034;&#20135;&#29983;&#35821;&#20041;&#31561;&#20215;&#30340;&#21709;&#24212;&#65292;&#37027;&#20040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#34987;&#35748;&#20026;&#26159;&#19968;&#33268;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;LLMs&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#29983;&#25104;&#26041;&#38754;&#20063;&#23384;&#22312;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#23545;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#20934;&#30830;&#24230;&#26469;&#34913;&#37327;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#27809;&#26377;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#30340;&#36947;&#24503;&#24773;&#26223;&#65288;&#20363;&#22914;&#65292;&#36947;&#36335;&#20132;&#36816;&#38382;&#39064;&#65289;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#26469;&#34913;&#37327;LLM&#22312;&#36947;&#24503;&#24773;&#26223;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;RoTs&#65289;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#19982;&#20154;&#31867;&#21028;&#26029;&#22312;&#20116;&#20010;LLMs&#19978;&#26356;&#22909;&#22320;&#30456;&#20851;&#12290;&#22312;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
&lt;/p&gt;</description></item><item><title>HiGen&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#65292;&#22312;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#20013;&#32771;&#34385;&#20102;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;ENZYME&#12290;</title><link>https://arxiv.org/abs/2402.01696</link><description>&lt;p&gt;
HiGen: &#23618;&#27425;&#24863;&#30693;&#30340;&#23618;&#32423;&#25991;&#26412;&#20998;&#31867;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01696
&lt;/p&gt;
&lt;p&gt;
HiGen&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#65292;&#22312;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#20013;&#32771;&#34385;&#20102;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;ENZYME&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#65288;HTC&#65289;&#26159;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19968;&#20010;&#22797;&#26434;&#23376;&#20219;&#21153;&#65292;&#20854;&#29305;&#28857;&#26159;&#20855;&#26377;&#23618;&#32423;&#26631;&#31614;&#20998;&#31867;&#27861;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#25991;&#26723;&#21644;&#23618;&#32423;&#26631;&#31614;&#20449;&#24687;&#26469;&#23398;&#20064;&#38745;&#24577;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#21487;&#33021;&#22240;&#23618;&#32423;&#27700;&#24179;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#65292;&#38656;&#35201;&#21160;&#24577;&#30340;&#25991;&#26723;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiGen&#65292;&#19968;&#20010;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#30340;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25429;&#25417;&#25991;&#26412;&#21644;&#26631;&#31614;&#21517;&#31216;&#35821;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#21040;&#39046;&#22495;&#30693;&#35782;&#19978;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26679;&#26412;&#26377;&#38480;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21629;&#21517;&#20026;ENZYME&#30340;&#26032;&#39062;&#21644;&#26377;&#20215;&#20540;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#26469;&#33258;PubMed&#30340;&#25991;&#31456;&#32452;&#25104;&#65292;&#26088;&#22312;&#39044;&#27979;...
&lt;/p&gt;
&lt;p&gt;
Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;Python&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08190</link><description>&lt;p&gt;
MARIO: &#20855;&#26377;Python&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;&#25968;&#23398;&#25512;&#29702;&#36755;&#20986;--&#21487;&#37325;&#22797;&#30340;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;Python&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#33719;&#24471;&#30495;&#27491;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38656;&#35201;&#22635;&#34917;&#30340;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;&#26412;&#25991;&#36890;&#36807;&#20016;&#23500;&#25968;&#25454;&#26223;&#35266;&#21644;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#25968;&#25454;&#38598;&#22686;&#21152;&#20102;&#20351;&#29992;Python&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08190v2 Announce Type: replace  Abstract: Large language models (LLMs) have seen considerable advancements in natural language understanding tasks, yet there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints. In this paper, we address this challenge by enriching the data landscape and introducing a novel math dataset, enhanced with a capability to utilize a Python code interpreter. This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT-4 annotations, human review, and self-training processes, where the errors in the original GSM8K training set have been fixed. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Self-Imagine&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;Vision-Language&#27169;&#22411;&#29983;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#24182;&#23558;&#20854;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#20877;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#25968;&#23398;&#20219;&#21153;&#21644;&#36890;&#29992;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.08025</link><description>&lt;p&gt;
&#33258;&#25105;&#24819;&#35937;&#65306;&#21033;&#29992;&#33258;&#25105;&#24819;&#35937;&#36827;&#34892;&#22810;&#27169;&#22411;&#33258;&#28982;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Self-Imagine&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;Vision-Language&#27169;&#22411;&#29983;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#24182;&#23558;&#20854;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#20877;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#25968;&#23398;&#20219;&#21153;&#21644;&#36890;&#29992;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision-Language&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#28508;&#21147;&#22312;&#22788;&#29702;&#22797;&#26434;&#22522;&#20110;&#25991;&#26412;&#38382;&#39064;&#26102;&#24448;&#24448;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#23588;&#20854;&#26159;&#24403;&#36825;&#20123;&#38382;&#39064;&#33021;&#22815;&#20174;&#35270;&#35273;&#34920;&#36798;&#20013;&#33719;&#30410;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Self-Imagine&#65292;&#19982;&#20154;&#31867;&#36890;&#36807;&#21019;&#24314;&#38382;&#39064;&#30340;&#35270;&#35273;&#22270;&#24182;&#25512;&#26029;&#35299;&#20915;&#27493;&#39588;&#30340;&#33021;&#21147;&#30456; resonating&#12290;&#25105;&#20204;&#21033;&#29992;&#21333;&#19968;&#30340;Vision-Language&#27169;&#22411;&#65288;VLM&#65289;&#20351;&#29992;HTML&#29983;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;HTML&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#24182;&#26368;&#32456;&#20351;&#29992;&#30456;&#21516;&#30340;VLM&#26681;&#25454;&#38382;&#39064;&#21644;&#22270;&#20687;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#65288;LLAVA-1.5&#21644;GEMINI PRO&#65289;VLMs&#22312;&#19977;&#20010;&#25968;&#23398;&#20219;&#21153;&#21644;&#20061;&#20010;&#36890;&#29992;&#25512;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;LLAVA-1.5&#21644;GEMINI PRO&#22312;&#25152;&#26377;&#25968;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08025v2 Announce Type: replace  Abstract: The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on aver
&lt;/p&gt;</description></item><item><title>MAPO&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#23545;&#40784;&#20316;&#20026;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.06838</link><description>&lt;p&gt;
MAPO&#65306;&#36890;&#36807;&#22810;&#35821;&#35328;&#23545;&#40784;&#20316;&#20026;&#20559;&#22909;&#20248;&#21270;&#25512;&#36827;&#22810;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06838
&lt;/p&gt;
&lt;p&gt;
MAPO&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#23545;&#40784;&#20316;&#20026;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25512;&#29702;&#33021;&#21147;&#34987;&#35748;&#20026;&#19982;&#35821;&#35328;&#26080;&#20851;&#65292;&#20294;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#30340;&#25512;&#29702;&#33021;&#21147;&#19981;&#19968;&#33268;&#65292;&#20363;&#22914;&#65292;&#23545;&#20027;&#23548;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#36825;&#26159;&#30001;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#22686;&#24378;&#38750;&#20027;&#23548;&#35821;&#35328;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35821;&#35328;&#23545;&#40784;&#20316;&#20026;&#20559;&#22909;&#20248;&#21270;&#65288;MAPO&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#19982;&#20027;&#23548;&#35821;&#35328;&#23545;&#40784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#32763;&#35793;&#27169;&#22411;&#26469;&#20445;&#35777;&#38750;&#20027;&#23548;&#35821;&#35328;&#21644;&#20027;&#23548;&#35821;&#35328;&#20043;&#38388;&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#65292;&#36825;&#34987;&#37319;&#32435;&#20026;&#20248;&#21270;&#30340;&#20559;&#22909;&#65292;&#20363;&#22914;&#65292;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#25110;&#20020;&#36817;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MAPO&#22312;&#25152;&#26377;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65288;MSVAMP +16.2&#65285;&#65292;MGSM +6.1&#65285;&#65289;&#31283;&#23450;&#22320;&#23454;&#29616;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06838v2 Announce Type: replace  Abstract: Though reasoning abilities are considered language-agnostic, existing LLMs exhibit inconsistent reasoning abilities across different languages, e.g., reasoning in the dominant language like English is superior to other languages due to the imbalance of multilingual training data. To enhance reasoning abilities in non-dominant languages, we propose a Multilingual-Alignment-as-Preference Optimization framework (MAPO), aiming to align the reasoning processes in other languages with the dominant language. Specifically, we harness an off-the-shelf translation model for the consistency between answers in non-dominant and dominant languages, which we adopt as the preference for optimization, e.g., Direct Preference Optimization (DPO) or Proximal Policy Optimization (PPO). Experiments show that MAPO stably achieves significant improvements in the multilingual reasoning of various models on all three benchmarks (MSVAMP +16.2%, MGSM +6.1%, and
&lt;/p&gt;</description></item><item><title>PixT3&#26159;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#30340;&#22810;&#27169;&#24335;&#34920;&#26684;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#35270;&#20026;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#65292;&#28040;&#38500;&#20102;&#23383;&#31526;&#20018;&#26684;&#24335;&#30340;&#38656;&#27714;&#65292;&#20811;&#26381;&#20102;&#32447;&#24615;&#21270;&#21644;&#36755;&#20837;&#22823;&#23567;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.09808</link><description>&lt;p&gt;
PixT3&#65306;&#22522;&#20110;&#20687;&#32032;&#30340;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PixT3: Pixel-based Table To Text generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09808
&lt;/p&gt;
&lt;p&gt;
PixT3&#26159;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#30340;&#22810;&#27169;&#24335;&#34920;&#26684;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#35270;&#20026;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#65292;&#28040;&#38500;&#20102;&#23383;&#31526;&#20018;&#26684;&#24335;&#30340;&#38656;&#27714;&#65292;&#20811;&#26381;&#20102;&#32447;&#24615;&#21270;&#21644;&#36755;&#20837;&#22823;&#23567;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Table-to-text&#29983;&#25104;&#28041;&#21450;&#26681;&#25454;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#36866;&#24403;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#27969;&#34892;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20010;&#20849;&#21516;&#29305;&#28857;&#26159;&#23558;&#36755;&#20837;&#35270;&#20026;&#23383;&#31526;&#20018;&#65292;&#21363;&#36890;&#36807;&#37319;&#29992;&#32447;&#24615;&#21270;&#25216;&#26415;&#65292;&#19981;&#24635;&#26159;&#20445;&#30041;&#34920;&#26684;&#20013;&#30340;&#20449;&#24687;&#65292;&#36807;&#20110;&#20887;&#38271;&#65292;&#32570;&#20047;&#31354;&#38388;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#37325;&#26032;&#24605;&#32771;&#20026;&#19968;&#20010;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#65292;&#28040;&#38500;&#20102;&#23558;&#36755;&#20837;&#21576;&#29616;&#20026;&#23383;&#31526;&#20018;&#26684;&#24335;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PixT3&#65292;&#19968;&#31181;&#22810;&#27169;&#24335;&#34920;&#26684;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#36935;&#21040;&#30340;&#32447;&#24615;&#21270;&#21644;&#36755;&#20837;&#22823;&#23567;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;PixT3&#36890;&#36807;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21152;&#24378;&#34920;&#26684;&#32467;&#26500;&#24847;&#35782;&#65292;&#24182;&#36866;&#29992;&#20110;&#24320;&#25918;&#24335;&#21644;&#21463;&#25511;&#29983;&#25104;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09808v2 Announce Type: replace  Abstract: Table-to-text generation involves generating appropriate textual descriptions given structured tabular data. It has attracted increasing attention in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. A common feature across existing methods is their treatment of the input as a string, i.e., by employing linearization techniques that do not always preserve information in the table, are verbose, and lack space efficiency. We propose to rethink data-to-text generation as a visual recognition task, removing the need for rendering the input in a string format. We present PixT3, a multimodal table-to-text model that overcomes the challenges of linearization and input size limitations encountered by existing models. PixT3 is trained with a new self-supervised learning objective to reinforce table structure awareness and is applicable to open-ended and controlled generation settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31034;&#20363;&#35201;&#28857;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35201;&#28857;&#27169;&#22411;&#24418;&#25104;&#20102;&#26032;&#30340;&#20998;&#25968;&#35780;&#20272;&#26041;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#21160;&#24577;&#36873;&#25321;&#26368;&#20339;&#31034;&#20363;&#65292;&#25552;&#39640;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09606</link><description>&lt;p&gt;
GistScore&#65306;&#36890;&#36807;&#35201;&#28857;&#29942;&#39048;&#23398;&#20064;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09606
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31034;&#20363;&#35201;&#28857;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35201;&#28857;&#27169;&#22411;&#24418;&#25104;&#20102;&#26032;&#30340;&#20998;&#25968;&#35780;&#20272;&#26041;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#21160;&#24577;&#36873;&#25321;&#26368;&#20339;&#31034;&#20363;&#65292;&#25552;&#39640;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20197;&#21253;&#21547;&#23569;&#37327;&#20219;&#21153;&#31034;&#20363;&#30340;&#25552;&#31034;&#20026;&#26465;&#20214;&#26102;&#25191;&#34892;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;ICL&#30340;&#24615;&#33021;&#21487;&#33021;&#23545;&#31034;&#20363;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#21160;&#24577;&#22320;&#20026;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#36873;&#25321;&#26368;&#20339;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31034;&#20363;&#35201;&#28857;&#25552;&#21462;&#65288;Example Gisting&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20855;&#26377;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#29942;&#39048;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#30340;&#35757;&#32451;&#31034;&#20363;&#32534;&#30721;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#35201;&#28857;&#27169;&#22411;&#26500;&#25104;&#20102;GistScore&#30340;&#22522;&#30784;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20998;&#21644;&#36873;&#25321;&#20449;&#24687;&#31034;&#20363;&#30340;&#26032;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20004;&#31181;&#21464;&#20307;&#65306;&#65288;1&#65289;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#24494;&#35843;&#35201;&#28857;&#27169;&#22411;&#21644;&#65288;2&#65289;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22810;&#20219;&#21153;&#22521;&#35757;&#21333;&#19968;&#27169;&#22411;&#12290;&#21518;&#32773;&#21487;&#29992;&#20110;&#24320;&#31665;&#21363;&#29992;&#22320;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;ICL&#27969;&#27700;&#32447;&#12290;&#23545;&#28085;&#30422;9&#20010;&#20219;&#21153;&#21644;8&#31181;&#19981;&#21516;LLMs&#30340;21&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#39030;&#23574;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09606v2 Announce Type: replace  Abstract: In-context Learning (ICL) is the ability of Large Language Models (LLMs) to perform new tasks when conditioned on prompts comprising a few task examples. However, ICL performance can be critically sensitive to the choice of examples. To dynamically select the best examples for every test input, we propose Example Gisting, a novel approach for training example encoders through supervised fine-tuning with an attention bottleneck between the inputs and outputs. These gist models form the basis for GistScore, a novel metric for scoring and selecting informative examples. Further, we experiment with two variations: (1) fine-tuning gist models for each dataset and (2) multi-task training a single model on a large collection of datasets. The latter can be used for new tasks out-of-the-box, enabling a training-free ICL pipeline. Evaluations with 21 datasets spanning 9 tasks and 8 diverse LLMs show that our fine-tuned models get state-of-the-
&lt;/p&gt;</description></item><item><title>&#21363;&#20351;&#26159;&#20219;&#21153;&#32422;&#26463;&#20063;&#20250;&#24433;&#21709;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#21363;&#20351;&#36825;&#20123;&#32422;&#26463;&#19982;&#35268;&#36991;&#26080;&#20851;&#65292;&#20063;&#20250;&#23548;&#33268;&#29616;&#26377;&#26816;&#27979;&#22120;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;</title><link>https://arxiv.org/abs/2311.08369</link><description>&lt;p&gt;
&#25351;&#20196;&#26041;&#24335;&#30340;&#37325;&#35201;&#24615;&#65306;&#21363;&#20351;&#20219;&#21153;&#32422;&#26463;&#20063;&#20250;&#24433;&#21709;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08369
&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#26159;&#20219;&#21153;&#32422;&#26463;&#20063;&#20250;&#24433;&#21709;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#21363;&#20351;&#36825;&#20123;&#32422;&#26463;&#19982;&#35268;&#36991;&#26080;&#20851;&#65292;&#20063;&#20250;&#23548;&#33268;&#29616;&#26377;&#26816;&#27979;&#22120;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#24615;&#33021;&#21487;&#38752;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;&#24403;&#29992;&#25143;&#25351;&#31034;LLMs&#29983;&#25104;&#25991;&#26412;&#26102;&#65292;&#25351;&#20196;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#21253;&#21547;&#19981;&#21516;&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#20026;LLM&#26816;&#27979;&#21019;&#24314;&#25968;&#25454;&#38598;&#26102;&#24182;&#27809;&#26377;&#28085;&#30422;&#36825;&#31181;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#27169;&#24335;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#20219;&#21153;&#23548;&#21521;&#30340;&#32422;&#26463;&#8212;&#8212;&#36825;&#20123;&#32422;&#26463;&#33258;&#28982;&#20250;&#21253;&#21547;&#22312;&#25351;&#20196;&#20013;&#65292;&#24182;&#19988;&#19982;&#26816;&#27979;&#35268;&#36991;&#26080;&#20851;&#8212;&#8212;&#20063;&#20250;&#23548;&#33268;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#26816;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#36739;&#22823;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#20197;&#23398;&#29983;&#20316;&#25991;&#20889;&#20316;&#20026;&#29616;&#23454;&#39046;&#22495;&#65292;&#24182;&#26681;&#25454;&#20960;&#20010;&#22240;&#32032;&#25163;&#21160;&#21019;&#24314;&#22522;&#20110;&#20316;&#25991;&#36136;&#37327;&#30340;&#20219;&#21153;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24102;&#26377;&#36825;&#31181;&#32422;&#26463;&#30340;&#25351;&#20196;&#29983;&#25104;&#30340;&#25991;&#26412;&#19978;&#65292;&#24403;&#21069;&#26816;&#27979;&#22120;&#24615;&#33021;&#30340;&#26631;&#20934;&#20559;&#24046;&#65288;SD&#65289;&#26174;&#33879;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08369v2 Announce Type: replace  Abstract: To combat the misuse of Large Language Models (LLMs), many recent studies have presented LLM-generated-text detectors with promising performance. When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need. However, most recent studies do not cover such diverse instruction patterns when creating datasets for LLM detection. In this paper, we find that even task-oriented constraints -- constraints that would naturally be included in an instruction and are not related to detection-evasion -- cause existing detectors to have a large variance in detection performance. We focus on student essay writing as a realistic domain and manually create task-oriented constraints based on several factors for essay quality. Our experiments show that the standard deviation (SD) of current detector performance on texts generated by an instruction with such a constraint is significantly large
&lt;/p&gt;</description></item><item><title>Monkey&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#35270;&#35273;&#25429;&#25417;&#21644;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.06607</link><description>&lt;p&gt;
Monkey: &#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#25991;&#26412;&#26631;&#31614;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06607
&lt;/p&gt;
&lt;p&gt;
Monkey&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#35270;&#35273;&#25429;&#25417;&#21644;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#22312;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#21644;&#35814;&#32454;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Monkey&#26469;&#22686;&#24378;LMM&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;Monkey&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#21010;&#20998;&#20026;&#32479;&#19968;&#30340;&#34917;&#19969;&#26469;&#22788;&#29702;&#22270;&#20687;&#65292;&#27599;&#20010;&#34917;&#19969;&#30340;&#22823;&#23567;&#19982;&#21407;&#26469;&#35757;&#32451;&#33391;&#22909;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20351;&#29992;&#30340;&#22823;&#23567;(&#20363;&#22914;448x448)&#30456;&#21305;&#37197;&#12290;&#37197;&#22791;&#20102;&#27599;&#20010;&#34917;&#19969;&#30340;&#36866;&#37197;&#22120;&#65292;Monkey&#21487;&#20197;&#22788;&#29702;&#39640;&#36798;1344x896&#20687;&#32032;&#30340;&#26356;&#39640;&#20998;&#36776;&#29575;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#35270;&#35273;&#20449;&#24687;&#30340;&#35814;&#32454;&#25429;&#25417;&#12290;&#20854;&#27425;&#65292;&#23427;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#65292;&#20016;&#23500;&#20102;&#22330;&#26223;-&#23545;&#35937;&#20851;&#32852;&#30340;&#19978;&#19979;&#25991;&#12290;&#36825;&#31181;&#20004;&#37096;&#20998;&#31574;&#30053;&#30830;&#20445;&#20102;&#20174;&#29983;&#25104;&#25968;&#25454;&#20013;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#65306;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#20801;&#35768;&#23545;&#35270;&#35273;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#25429;&#25417;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20840;&#38754;&#25551;&#36848;&#30340;&#25928;&#26524;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06607v3 Announce Type: replace-cross  Abstract: Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative result
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25351;&#20196;&#35843;&#25972;&#23545;&#27599;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21508;&#39033;&#33021;&#21147;&#65288;&#22914;&#21019;&#24847;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#30340;&#21457;&#23637;&#24433;&#21709;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#21442;&#25968;&#35268;&#27169;&#21644;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2310.19651</link><description>&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#21160;&#24577;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27599;&#20010;&#33021;&#21147;&#37117;&#26377;&#20854;&#33258;&#24049;&#30340;&#22686;&#38271;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25351;&#20196;&#35843;&#25972;&#23545;&#27599;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21508;&#39033;&#33021;&#21147;&#65288;&#22914;&#21019;&#24847;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#30340;&#21457;&#23637;&#24433;&#21709;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#21442;&#25968;&#35268;&#27169;&#21644;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#26159;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#65292;&#29992;&#20110;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#36941;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#25968;&#25454;&#30340;&#21019;&#24314;&#20173;&#28982;&#20027;&#35201;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#23548;&#33268;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#25968;&#37327;&#21644;&#36136;&#37327;&#19978;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;&#25968;&#25454;&#37327;&#12289;&#21442;&#25968;&#22823;&#23567;&#21644;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;LLM&#30340;&#27599;&#20010;&#22522;&#26412;&#33021;&#21147;&#65288;&#22914;&#21019;&#24847;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#30340;&#21457;&#23637;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#26500;&#24314;&#20934;&#21017;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#33021;&#21147;&#30340;&#36229;&#36807;40k&#20010;&#23454;&#20363;&#65292;&#24182;&#30740;&#31350;&#20102;&#20855;&#26377;70&#20159;&#33267;330&#20159;&#21442;&#25968;&#30340;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19977;&#20010;&#20027;&#35201;&#21457;&#29616;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19651v2 Announce Type: replace  Abstract: Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quantity and quality across existing datasets. While some research advocates for expanding the number of instructions, others suggest that a small set of well-chosen examples is adequate. To better understand data construction guidelines, our research provides a granular analysis of how data volume, parameter size, and data construction methods influence the development of each underlying ability of LLM, such as creative writing, code generation, and logical reasoning. We present a meticulously curated dataset with over 40k instances across ten abilities and examine instruction-tuned models with 7b to 33b parameters. Our study reveals three primary findings: (i) Despite the models' overall performance being tied to data a
&lt;/p&gt;</description></item><item><title>MindfulDiary&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#31934;&#31070;&#30149;&#24739;&#32773;&#36890;&#36807;&#23545;&#35805;&#35760;&#24405;&#26085;&#24120;&#20307;&#39564;&#65292;&#24182;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#21462;&#24471;&#31215;&#26497;&#25928;&#26524;</title><link>https://arxiv.org/abs/2310.05231</link><description>&lt;p&gt;
MindfulDiary&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#31934;&#31070;&#30149;&#24739;&#32773;&#30340;&#26085;&#35760;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05231
&lt;/p&gt;
&lt;p&gt;
MindfulDiary&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#31934;&#31070;&#30149;&#24739;&#32773;&#36890;&#36807;&#23545;&#35805;&#35760;&#24405;&#26085;&#24120;&#20307;&#39564;&#65292;&#24182;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#21462;&#24471;&#31215;&#26497;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#28982;&#32780;&#20854;&#22797;&#26434;&#24615;&#21644;&#20302;&#21487;&#25511;&#24615;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MindfulDiary&#65292;&#19968;&#20010;&#31227;&#21160;&#26085;&#35760;&#24212;&#29992;&#65292;&#32467;&#21512;LLM&#24110;&#21161;&#31934;&#31070;&#30149;&#24739;&#32773;&#36890;&#36807;&#23545;&#35805;&#35760;&#24405;&#26085;&#24120;&#20307;&#39564;&#12290;&#19982;&#24515;&#29702;&#20581;&#24247;&#19987;&#19994;&#20154;&#22763;&#65288;MHPs&#65289;&#21512;&#20316;&#35774;&#35745;&#65292;MindfulDiary&#37319;&#21462;&#22522;&#20110;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#23433;&#20840;&#22320;&#36981;&#23432;&#19987;&#23478;&#25351;&#21335;&#65292;&#21516;&#26102;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#28041;&#21450;28&#21517;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;&#24739;&#32773;&#21644;5&#21517;&#31934;&#31070;&#31185;&#21307;&#29983;&#30340;&#20026;&#26399;&#22235;&#21608;&#30340;&#23454;&#22320;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;MindfulDiary&#25903;&#25345;&#24739;&#32773;&#25345;&#32493;&#20016;&#23500;&#20854;&#26085;&#24120;&#35760;&#24405;&#65292;&#24182;&#24110;&#21161;&#31934;&#31070;&#31185;&#21307;&#29983;&#36890;&#36807;&#29702;&#35299;&#20182;&#20204;&#30340;&#24819;&#27861;&#21644;&#26085;&#24120;&#32972;&#26223;&#26356;&#22909;&#22320;&#21516;&#24773;&#20182;&#20204;&#30340;&#24739;&#32773;&#12290;&#26681;&#25454;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05231v2 Announce Type: replace-cross  Abstract: In the mental health domain, Large Language Models (LLMs) offer promising new opportunities, though their inherent complexity and low controllability have raised questions about their suitability in clinical settings. We present MindfulDiary, a mobile journaling app incorporating an LLM to help psychiatric patients document daily experiences through conversation. Designed in collaboration with mental health professionals (MHPs), MindfulDiary takes a state-based approach to safely comply with the experts' guidelines while carrying on free-form conversations. Through a four-week field study involving 28 patients with major depressive disorder and five psychiatrists, we found that MindfulDiary supported patients in consistently enriching their daily records and helped psychiatrists better empathize with their patients through an understanding of their thoughts and daily contexts. Drawing on these findings, we discuss the implicati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#27010;&#29575;&#26354;&#29575;&#27010;&#24565;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fast-DetectGPT&#65292;&#19968;&#20010;&#20248;&#21270;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#30456;&#23545;&#20110;DetectGPT&#22312;&#30333;&#30418;&#21644;&#20854;&#20182;&#27979;&#35797;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#25552;&#21319;&#36798;&#21040;&#32422;75%&#12290;</title><link>https://arxiv.org/abs/2310.05130</link><description>&lt;p&gt;
Fast-DetectGPT: &#36890;&#36807;&#26465;&#20214;&#27010;&#29575;&#26354;&#29575;&#23454;&#29616;&#39640;&#25928;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05130
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#27010;&#29575;&#26354;&#29575;&#27010;&#24565;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fast-DetectGPT&#65292;&#19968;&#20010;&#20248;&#21270;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#30456;&#23545;&#20110;DetectGPT&#22312;&#30333;&#30418;&#21644;&#20854;&#20182;&#27979;&#35797;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#25552;&#21319;&#36798;&#21040;&#32422;75%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#29616;&#20986;&#20135;&#29983;&#27969;&#30021;&#12289;&#36830;&#36143;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#29983;&#20135;&#21147;&#26426;&#20250;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#31038;&#20250;&#39118;&#38505;&#12290;&#20026;&#26500;&#24314;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#26377;&#24517;&#35201;&#21306;&#20998;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#21019;&#20316;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#24341;&#20837;&#26465;&#20214;&#27010;&#29575;&#26354;&#29575;&#27010;&#24565;&#65292;&#20197;&#38416;&#26126;LLMs&#21644;&#20154;&#31867;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#35789;&#27719;&#36873;&#25321;&#24046;&#24322;&#12290;&#21033;&#29992;&#35813;&#26354;&#29575;&#20316;&#20026;&#22522;&#30784;&#24230;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;**Fast-DetectGPT**&#65292;&#19968;&#20010;&#20248;&#21270;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#23558;DetectGPT&#30340;&#25200;&#21160;&#27493;&#39588;&#26367;&#25442;&#20026;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#28304;&#27169;&#22411;&#21644;&#27979;&#35797;&#26465;&#20214;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Fast-DetectGPT&#22312;&#30333;&#30418;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05130v2 Announce Type: replace  Abstract: Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75% in both the white-box a
&lt;/p&gt;</description></item><item><title>&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#21487;&#20197;&#20351;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#36866;&#24212;&#19981;&#21516;&#35745;&#31639;&#36164;&#28304;&#21644;ASR&#24615;&#33021;&#38656;&#27714;&#65292;&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#37319;&#29992;&#26089;&#26399;&#36864;&#20986;&#30446;&#26631;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#25928;&#26524;</title><link>https://arxiv.org/abs/2309.09546</link><description>&lt;p&gt;
&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#35757;&#32451;&#21160;&#24577;&#27169;&#22411;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Training dynamic models using early exits for automatic speech recognition on resource-constrained devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09546
&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#21487;&#20197;&#20351;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#36866;&#24212;&#19981;&#21516;&#35745;&#31639;&#36164;&#28304;&#21644;ASR&#24615;&#33021;&#38656;&#27714;&#65292;&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#37319;&#29992;&#26089;&#26399;&#36864;&#20986;&#30446;&#26631;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#35843;&#25972;&#31070;&#32463;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#36127;&#36733;&#30340;&#33021;&#21147;&#23545;&#20110;&#35774;&#22791;&#22788;&#29702;&#36164;&#28304;&#26377;&#38480;&#19988;&#35745;&#31639;&#36164;&#28304;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22330;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290; &#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#22312;&#32534;&#30721;&#22120;&#30340;&#20013;&#38388;&#23618;&#38468;&#21152;&#20102;&#39069;&#22806;&#30340;&#36864;&#20986;&#20998;&#25903;&#12290; &#22312;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#65292;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#24320;&#21457;&#20986;&#21160;&#24577;&#27169;&#22411;&#65292;&#36825;&#20123;&#21160;&#24577;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#27700;&#24179;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;ASR&#24615;&#33021;&#38656;&#27714;&#26469;&#35843;&#25972;&#20854;&#22823;&#23567;&#21644;&#26550;&#26500;&#12290; &#20197;&#24448;&#20851;&#20110;&#26089;&#26399;&#36864;&#20986;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#12290; &#26412;&#25991;&#23545;&#39044;&#35757;&#32451;&#30340;&#39592;&#24178;&#32593;&#36827;&#34892;&#24494;&#35843;&#21644;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#30446;&#26631;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290; &#23454;&#39564;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09546v2 Announce Type: replace-cross  Abstract: The ability to dynamically adjust the computational load of neural models during inference is crucial for on-device processing scenarios characterised by limited and time-varying computational resources. A promising solution is presented by early-exit architectures, in which additional exit branches are appended to intermediate layers of the encoder. In self-attention models for automatic speech recognition (ASR), early-exit architectures enable the development of dynamic models capable of adapting their size and architecture to varying levels of computational resources and ASR performance demands. Previous research on early-exiting ASR models has relied on pre-trained self-supervised models, fine-tuned with an early-exit loss. In this paper, we undertake an experimental comparison between fine-tuning pre-trained backbones and training models from scratch with the early-exiting objective. Experiments conducted on public dataset
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.02233</link><description>&lt;p&gt;
&#29992;&#21307;&#23398;&#25945;&#31185;&#20070;&#22686;&#24378;&#40657;&#30418;LLMs&#36827;&#34892;&#20020;&#24202;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.02233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#31034;&#20986;&#26681;&#25454;&#20154;&#31867;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#32570;&#20047;&#29305;&#23450;&#12289;&#28145;&#20837;&#30340;&#30693;&#35782;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;LLM-AMT&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;&#19968;&#20010;&#26597;&#35810;&#22686;&#24378;&#22120;&#12289;&#19968;&#20010;&#28151;&#21512;&#25945;&#31185;&#20070;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#30693;&#35782;&#33258;&#25105;&#23436;&#21892;&#12290;&#23427;&#20204;&#20849;&#21516;&#25972;&#21512;&#26435;&#23041;&#21307;&#23398;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;LLMs&#38405;&#35835;&#22120;&#26377;&#21161;&#20110;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMAMT&#26174;&#33879;&#25552;&#39640;&#20102;&#21709;&#24212;&#36136;&#37327;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.6%&#21040;16.6%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;GPT-4-Turbo&#20026;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.02233v2 Announce Type: replace-cross  Abstract: Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge. In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains. LLM-AMT integrates authoritative medical textbooks into the LLMs' framework using plug-and-play modules. These modules include a Query Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together, they incorporate authoritative medical knowledge. Additionally, an LLM Reader aids in contextual understanding. Our experimental results on three medical QA tasks demonstrate that LLMAMT significantly improves response quality, with accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the base mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#30340;AED&#22522;&#20934;DONKII&#65292;&#24182;&#21457;&#29616;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#26126;&#26174;&#30340;&#38169;&#35823;&#65292;&#26377;&#26102;&#36825;&#20123;&#38169;&#35823;&#30452;&#25509;&#20256;&#25773;&#21040;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#20013;&#12290;</title><link>https://arxiv.org/abs/2309.01669</link><description>&lt;p&gt;
Donkii: &#27880;&#37322;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#33021;&#21542;&#21457;&#29616;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#65311;
&lt;/p&gt;
&lt;p&gt;
Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#30340;AED&#22522;&#20934;DONKII&#65292;&#24182;&#21457;&#29616;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#26126;&#26174;&#30340;&#38169;&#35823;&#65292;&#26377;&#26102;&#36825;&#20123;&#38169;&#35823;&#30452;&#25509;&#20256;&#25773;&#21040;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35757;&#32451;&#27969;&#31243;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#20135;&#29983;&#24378;&#22823;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290; &#22312;&#30740;&#31350;&#30340;&#21478;&#19968;&#26465;&#32447;&#19978;&#65292;&#27880;&#37322;&#38169;&#35823;&#26816;&#27979;&#65288;AED&#65289;&#24050;&#32463;&#25104;&#20026;&#26816;&#27979;&#40644;&#37329;&#26631;&#20934;&#26631;&#31614;&#20013;&#36136;&#37327;&#38382;&#39064;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;AED&#26041;&#27861;&#30340;&#24212;&#29992;&#24050;&#32463;&#34987;&#38480;&#21046;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290; &#22914;&#20309;&#35780;&#20272;AED&#26041;&#27861;&#22312;&#35821;&#35328;&#29983;&#25104;&#35774;&#32622;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#65292;&#36825;&#19968;&#35774;&#32622;&#36890;&#36807;LLMs&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#30340;AED&#30340;&#39318;&#20010;&#26032;&#39062;&#22522;&#20934;&#65306;DONKII&#12290; &#23427;&#21253;&#25324;&#19977;&#20010;&#30001;&#19987;&#23478;&#21644;&#21322;&#33258;&#21160;&#26041;&#27861;&#22686;&#24378;&#38169;&#35823;&#27880;&#37322;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290; &#25105;&#20204;&#36824;&#20026;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#38169;&#35823;&#31867;&#22411;&#20998;&#31867;&#27861;&#12290; &#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#26126;&#26174;&#30340;&#38169;&#35823;&#65292;&#26377;&#26102;&#36825;&#20123;&#38169;&#35823;&#30452;&#25509;&#20256;&#25773;&#21040;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01669v2 Announce Type: replace  Abstract: Instruction tuning has become an integral part of training pipelines for Large Language Models (LLMs) and has been shown to yield strong performance gains. In an orthogonal line of research, Annotation Error Detection (AED) has emerged as a tool for detecting quality problems in gold standard labels. So far, however, the application of AED methods has been limited to classification tasks. It is an open question how well AED methods generalize to language generation settings, which are becoming more widespread via LLMs. In this paper, we present a first and novel benchmark for AED on instruction tuning data: DONKII. It comprises three instruction-tuning datasets enriched with error annotations by experts and semi-automatic methods. We also provide a novel taxonomy of error types for instruction-tuning data. We find that all three datasets contain clear errors, which sometimes propagate directly into instruction-tuned LLMs. We propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2305.13168</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#65306;&#26368;&#26032;&#21151;&#33021;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26500;&#24314;&#21644;&#25512;&#29702;&#20013;&#30340;&#25968;&#37327;&#21270;&#21644;&#36136;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#28085;&#30422;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#38382;&#31572;&#22235;&#20010;&#20856;&#22411;&#20219;&#21153;&#65292;&#20174;&#32780;&#20840;&#38754;&#25506;&#32034;&#20102;LLMs&#22312;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#32463;&#39564;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;GPT-4&#20026;&#20195;&#34920;&#30340;LLMs&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34429;&#28982;GPT-4&#22312;&#19982;KG&#26500;&#24314;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#36824;&#25193;&#23637;&#21040;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#34394;&#25311;&#30693;&#35782;&#25552;&#21462;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
&lt;/p&gt;</description></item><item><title>&#27874;&#20848;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;PolQA&#65289;&#26159;&#29992;&#20110;OpenQA&#30340;&#31532;&#19968;&#20010;&#27874;&#20848;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;7,000&#20010;&#38382;&#39064;&#21644;87,525&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#35777;&#25454;&#27573;&#33853;&#65292;&#22312;QA&#31995;&#32479;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#27880;&#37322;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#27573;&#33853;&#30340;&#26816;&#32034;&#20934;&#30830;&#24230;@10&#24182;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2212.08897</link><description>&lt;p&gt;
PolQA&#65306;&#27874;&#20848;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PolQA: Polish Question Answering Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.08897
&lt;/p&gt;
&lt;p&gt;
&#27874;&#20848;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;PolQA&#65289;&#26159;&#29992;&#20110;OpenQA&#30340;&#31532;&#19968;&#20010;&#27874;&#20848;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;7,000&#20010;&#38382;&#39064;&#21644;87,525&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#35777;&#25454;&#27573;&#33853;&#65292;&#22312;QA&#31995;&#32479;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#27880;&#37322;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#27573;&#33853;&#30340;&#26816;&#32034;&#20934;&#30830;&#24230;@10&#24182;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#38754;&#21521;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OpenQA&#65289;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#27880;&#37322;&#34987;&#35748;&#20026;&#26159;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#65292;&#22240;&#27492;&#33719;&#21462;&#36215;&#26469;&#24456;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#36866;&#29992;&#20110;&#35821;&#35328;&#23569;&#25968;&#65288;&#20027;&#35201;&#26159;&#33521;&#35821;&#21644;&#27721;&#35821;&#65289;&#30340;&#25968;&#25454;&#38598;&#20165;&#26377;&#23569;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#21644;&#20844;&#24320;&#21457;&#24067;&#20102;&#27874;&#20848;&#38382;&#31572;&#65288;PolQA&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#29992;&#20110;OpenQA&#30340;&#31532;&#19968;&#20010;&#27874;&#20848;&#35821;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;7,000&#20010;&#38382;&#39064;&#65292;87,525&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#35777;&#25454;&#27573;&#33853;&#65292;&#20197;&#21450;&#36229;&#36807;7,097,322&#20010;&#20505;&#36873;&#27573;&#33853;&#30340;&#35821;&#26009;&#24211;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#26681;&#25454;&#20854;&#20844;&#24335;&#12289;&#31867;&#22411;&#20197;&#21450;&#31572;&#26696;&#30340;&#23454;&#20307;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#36825;&#19968;&#36164;&#28304;&#20801;&#35768;&#25105;&#20204;&#35780;&#20272;&#19981;&#21516;&#27880;&#37322;&#36873;&#25321;&#23545;QA&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#27880;&#37322;&#31574;&#30053;&#65292;&#23558;&#27573;&#33853;&#30340;&#26816;&#32034;&#20934;&#30830;&#24230;@10&#25552;&#39640;&#20102;10.55&#20010;&#30334;&#20998;&#28857;&#65292;&#21516;&#26102;&#23558;&#27880;&#37322;&#25104;&#26412;&#38477;&#20302;&#20102;82%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.08897v2 Announce Type: replace  Abstract: Recently proposed systems for open-domain question answering (OpenQA) require large amounts of training data to achieve state-of-the-art performance. However, data annotation is known to be time-consuming and therefore expensive to acquire. As a result, the appropriate datasets are available only for a handful of languages (mainly English and Chinese). In this work, we introduce and publicly release PolQA, the first Polish dataset for OpenQA. It consists of 7,000 questions, 87,525 manually labeled evidence passages, and a corpus of over 7,097,322 candidate passages. Each question is classified according to its formulation, type, as well as entity type of the answer. This resource allows us to evaluate the impact of different annotation choices on the performance of the QA system and propose an efficient annotation strategy that increases the passage retrieval accuracy@10 by 10.55 p.p. while reducing the annotation cost by 82%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E5&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#23545;&#27604;&#35757;&#32451;&#26041;&#24335;&#65292;&#22312;&#26410;&#32463;&#36807;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;BEIR&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#19978;&#20987;&#36133;BM25&#22522;&#32447;&#30340;&#27169;&#22411;&#65292;&#22312;&#24494;&#35843;&#21518;&#22312;MTEB&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2212.03533</link><description>&lt;p&gt;
&#29992;&#24369;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Text Embeddings by Weakly-Supervised Contrastive Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E5&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#23545;&#27604;&#35757;&#32451;&#26041;&#24335;&#65292;&#22312;&#26410;&#32463;&#36807;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;BEIR&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#19978;&#20987;&#36133;BM25&#22522;&#32447;&#30340;&#27169;&#22411;&#65292;&#22312;&#24494;&#35843;&#21518;&#22312;MTEB&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;E5&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#36801;&#31227;&#21040;&#21508;&#31181;&#20219;&#21153;&#20013;&#12290;&#35813;&#27169;&#22411;&#20197;&#23545;&#27604;&#26041;&#24335;&#35757;&#32451;&#65292;&#20351;&#29992;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65288;&#21517;&#20026;CCPairs&#65289;&#30340;&#24369;&#30417;&#30563;&#20449;&#21495;&#12290;E5&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#21333;&#19968;&#25991;&#26412;&#21521;&#37327;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#22914;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#65292;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#22312;BEIR&#21644;MTEB&#22522;&#20934;&#27979;&#35797;&#30340;56&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;E5&#26159;&#31532;&#19968;&#20010;&#22312;BEIR&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#19978;&#20987;&#36133;&#24378;&#22823;&#30340;BM25&#22522;&#32447;&#19988;&#19981;&#20351;&#29992;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;E5&#22312;MTEB&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#32988;&#36807;&#20855;&#26377;40&#20493;&#21442;&#25968;&#30340;&#29616;&#26377;&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03533v2 Announce Type: replace  Abstract: This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.
&lt;/p&gt;</description></item><item><title>ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2209.08199</link><description>&lt;p&gt;
ScreenQA: &#31227;&#21160;&#24212;&#29992;&#25130;&#22270;&#19978;&#30340;&#22823;&#35268;&#27169;&#38382;&#31572;&#23545;
&lt;/p&gt;
&lt;p&gt;
ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08199
&lt;/p&gt;
&lt;p&gt;
ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;ScreenQA&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#26469;&#29702;&#35299;&#23631;&#24149;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#23631;&#24149;&#25968;&#25454;&#38598;&#35201;&#20040;&#20391;&#37325;&#20110;&#32467;&#26500;&#21644;&#32452;&#20214;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#35201;&#20040;&#20391;&#37325;&#20110;&#20687;&#23548;&#33322;&#21644;&#20219;&#21153;&#23436;&#25104;&#20043;&#31867;&#30340;&#26356;&#39640;&#32423;&#21035;&#30340;&#32452;&#21512;&#20219;&#21153;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;86K&#20010;&#38382;&#31572;&#23545;&#26469;&#24357;&#21512;&#36825;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24076;&#26395;&#33021;&#22815;&#22522;&#20934;&#21270;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#21644;&#20219;&#24847;&#38271;&#30340;&#24207;&#21015;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#39044;&#27979;&#24615;&#32500;&#25252;&#20219;&#21153;&#20013;&#22810;&#28304;&#25968;&#25454;&#27969;&#30340;&#34701;&#21512;&#21644;&#36328;&#26102;&#38388;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2110.08021</link><description>&lt;p&gt;
StreaMulT: &#27969;&#24335;&#22810;&#27169;&#24577;Transformer&#29992;&#20110;&#24322;&#26500;&#21644;&#20219;&#24847;&#38271;&#30340;&#24207;&#21015;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
StreaMulT: Streaming Multimodal Transformer for Heterogeneous and Arbitrary Long Sequential Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#21644;&#20219;&#24847;&#38271;&#30340;&#24207;&#21015;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#39044;&#27979;&#24615;&#32500;&#25252;&#20219;&#21153;&#20013;&#22810;&#28304;&#25968;&#25454;&#27969;&#30340;&#34701;&#21512;&#21644;&#36328;&#26102;&#38388;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24037;&#19994;4.0&#31995;&#32479;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#24615;&#32500;&#25252;&#20219;&#21153;&#65288;&#22914;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65289;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#30456;&#24212;&#32780;&#23454;&#38469;&#30340;&#24773;&#26223;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#22810;&#28304;&#25968;&#25454;&#27969;&#65292;&#22914;&#20256;&#24863;&#22120;&#27979;&#37327;&#26102;&#38388;&#24207;&#21015;&#12289;&#26426;&#22120;&#22270;&#20687;&#12289;&#25991;&#26412;&#32500;&#25252;&#25253;&#21578;&#31561;&#12290;&#36825;&#20123;&#24322;&#26500;&#22810;&#27169;&#24577;&#27969;&#22312;&#20854;&#37319;&#38598;&#39057;&#29575;&#19978;&#20063;&#19981;&#21516;&#65292;&#21487;&#33021;&#21253;&#21547;&#26102;&#38388;&#19978;&#19981;&#23545;&#40784;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#20219;&#24847;&#38271;&#65292;&#21462;&#20915;&#20110;&#25152;&#32771;&#34385;&#30340;&#31995;&#32479;&#21644;&#20219;&#21153;&#12290;&#34429;&#28982;&#22810;&#27169;&#24577;&#34701;&#21512;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#20013;&#27809;&#26377;&#32771;&#34385;&#36807;&#19982;&#30456;&#20851;&#20219;&#21153;&#65288;&#22914;&#36328;&#26102;&#38388;&#39044;&#27979;&#65289;&#19968;&#36215;&#32771;&#34385;&#20219;&#24847;&#38271;&#30340;&#22810;&#27169;&#24577;&#27969;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36825;&#31181;&#24322;&#26500;&#22810;&#27169;&#24577;&#23398;&#20064;&#33539;&#24335;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#27969;&#24335;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.08021v2 Announce Type: replace-cross  Abstract: The increasing complexity of Industry 4.0 systems brings new challenges regarding predictive maintenance tasks such as fault detection and diagnosis. A corresponding and realistic setting includes multi-source data streams from different modalities, such as sensors measurements time series, machine images, textual maintenance reports, etc. These heterogeneous multimodal streams also differ in their acquisition frequency, may embed temporally unaligned information and can be arbitrarily long, depending on the considered system and task. Whereas multimodal fusion has been largely studied in a static setting, to the best of our knowledge, there exists no previous work considering arbitrarily long multimodal streams alongside with related tasks such as prediction across time. Thus, in this paper, we first formalize this paradigm of heterogeneous multimodal learning in a streaming setting as a new one. To tackle this challenge, we p
&lt;/p&gt;</description></item><item><title>TAT-LLM&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#32780;&#20026;&#20102;&#24212;&#23545;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TAT-LLM&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#36739;&#23567;LLM&#12290;</title><link>http://arxiv.org/abs/2401.13223</link><description>&lt;p&gt;
TAT-LLM: &#19968;&#31181;&#38024;&#23545;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. (arXiv:2401.13223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13223
&lt;/p&gt;
&lt;p&gt;
TAT-LLM&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#32780;&#20026;&#20102;&#24212;&#23545;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TAT-LLM&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#36739;&#23567;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#38382;&#31572;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;Web&#19978;&#38750;&#24120;&#24120;&#35265;&#65288;&#22914;SEC&#25991;&#20214;&#65289;&#65292;&#36890;&#24120;&#38656;&#35201;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#22810;&#27493;&#39588;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38754;&#21521;&#34920;&#26684;&#21644;&#25991;&#26412;&#38382;&#31572;&#30340;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#25277;&#35937;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#19977;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#24182;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20221;&#25351;&#20196;&#26469;&#23454;&#20363;&#21270;&#35813;&#27969;&#27700;&#32447;&#24182;&#39564;&#35777;GPT-4&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#20687;GPT-4&#36825;&#26679;&#30340;&#22312;&#32447;LLM&#23384;&#22312;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#21508;&#31181;&#25361;&#25112;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#24320;&#21457;&#36739;&#23567;&#30340;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29616;&#26377;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#33258;&#21160;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;LLaMA 2&#36827;&#34892;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;TAT-LLM&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-w
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#35813;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.12874</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#21040;&#24212;&#29992;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Understanding to Utilization: A Survey on Explainability for Large Language Models. (arXiv:2401.12874v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#35813;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;LLMs&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20854;&#8220;&#40657;&#30418;&#8221;&#24615;&#36136;&#24341;&#21457;&#20102;&#23545;&#36879;&#26126;&#24615;&#21644;&#20262;&#29702;&#20351;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;LLMs&#65292;&#22914;LLaMA&#65292;&#20854;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#20351;&#20854;&#38754;&#20020;&#29420;&#29305;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20195;&#34920;&#24615;&#30340;&#35780;&#20215;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#26412;&#32508;&#36848;&#30340;&#30446;&#26631;&#26159;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20379;&#20174;&#25216;&#26415;&#35282;&#24230;&#24635;&#32467;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper delves into the burgeoning field of explainability for Large Language Models (LLMs), a critical yet challenging aspect of natural language processing. With LLMs playing a pivotal role in various applications, their "black-box" nature raises concerns about transparency and ethical use. This paper emphasizes the necessity for enhanced explainability in LLMs, addressing both the general public's trust and the technical community's need for a deeper understanding of these models. We concentrate on pre-trained Transformer-based LLMs, such as LLaMA, which present unique interpretability challenges due to their scale and complexity. Our review categorizes existing explainability methods and discusses their application in improving model transparency and reliability. We also discuss representative evaluation methods, highlighting their strengths and limitations. The goal of this survey is to bridge the gap between theoretical understanding and practical application, offering 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12873</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#30340;&#21453;&#39304;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;: &#23558;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#20805;&#20998;&#24314;&#27169;&#20154;&#31867;&#20559;&#22909;&#23548;&#33268;&#22870;&#21169;&#27169;&#22411;&#22312;&#21033;&#29992;&#20154;&#30340;&#21453;&#39304;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36136;&#37327;&#20272;&#35745;(QE)&#22312;&#36807;&#21435;&#20004;&#24180;&#20013;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#23601;&#33021;&#20934;&#30830;&#39044;&#27979;&#32473;&#23450;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;QE&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;(&#22522;&#20110;QE&#30340;&#22870;&#21169;&#27169;&#22411;)&#26469;&#39044;&#27979;&#20154;&#30340;&#20559;&#22909;&#20197;&#36827;&#34892;&#21453;&#39304;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#20102;&#22312;&#22522;&#20110;QE&#30340;&#21453;&#39304;&#35757;&#32451;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#34920;&#29616;&#20026;&#22870;&#21169;&#30340;&#22686;&#21152;&#32780;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35748;&#20026;QE&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#32763;&#35793;&#30340;&#39640;&#22870;&#21169;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#20248;&#21270;&#21644;&#38169;&#35823;&#20256;&#25773;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#65292;&#24182;&#20026;QE&#27169;&#22411;&#28155;&#21152;&#20102;&#19968;&#20010;&#24809;&#32602;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the Q
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35328;&#36766;&#36827;&#34892;&#35780;&#20998;&#24182;&#29983;&#25104;&#35299;&#37322;&#24615;&#25688;&#35201;&#65292;&#26469;&#22686;&#21152;&#35875;&#35328;&#39564;&#35777;&#30340;&#20449;&#24687;&#20016;&#23500;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12713</link><description>&lt;p&gt;
&#29983;&#25104;&#26080;&#30417;&#30563;&#30340;&#35328;&#36766;&#35299;&#37322;&#29992;&#20110;&#35875;&#35328;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Generating Unsupervised Abstractive Explanations for Rumour Verification. (arXiv:2401.12713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35328;&#36766;&#36827;&#34892;&#35780;&#20998;&#24182;&#29983;&#25104;&#35299;&#37322;&#24615;&#25688;&#35201;&#65292;&#26469;&#22686;&#21152;&#35875;&#35328;&#39564;&#35777;&#30340;&#20449;&#24687;&#20016;&#23500;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#35875;&#35328;&#39564;&#35777;&#30340;&#20219;&#21153;&#28041;&#21450;&#26681;&#25454;&#30001;&#35813;&#35875;&#35328;&#24341;&#36215;&#30340;&#23545;&#35805;&#32447;&#31243;&#35780;&#20272;&#20854;&#30495;&#23454;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#19987;&#27880;&#20110;&#39044;&#27979;&#30495;&#23454;&#24615;&#26631;&#31614;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#37324;&#37325;&#26032;&#21046;&#23450;&#20102;&#20219;&#21153;&#65292;&#20197;&#29983;&#25104;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#35875;&#35328;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#21033;&#29992;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#23545;&#32447;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#24086;&#23376;&#36827;&#34892;&#35780;&#20998;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#24086;&#23376;&#36890;&#36807;&#20351;&#29992;&#27169;&#26495;&#24341;&#23548;&#24635;&#32467;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#35299;&#37322;&#24615;&#25688;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#35299;&#37322;&#24615;&#25688;&#35201;&#30340;&#20449;&#24687;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#25688;&#35201;&#26102;&#21487;&#20197;&#19982;&#20154;&#31867;&#36798;&#21040;&#31867;&#20284;&#30340;&#19968;&#33268;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35299;&#37322;&#24615;&#30340;&#27010;&#25324;&#25688;&#35201;&#27604;&#20165;&#20351;&#29992;&#32447;&#31243;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#24086;&#23376;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24182;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#39044;&#27979;&#30340;&#35875;&#35328;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity. We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26694;&#26550;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#30340;&#30701;&#35821;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#22312;&#26080;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#30701;&#35821;&#34920;&#31034;&#65292;&#36890;&#36807;&#30701;&#35821;&#31867;&#22411;&#20998;&#31867;&#21644;&#26377;&#25928;&#22320;&#34701;&#21512;&#23383;&#31526;&#32423;&#20449;&#24687;&#26469;&#25552;&#39640;&#34920;&#31034;&#30340;&#31934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#19977;&#31181;&#31890;&#24230;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10407</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#30340;&#30701;&#35821;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning High-Quality and General-Purpose Phrase Representations. (arXiv:2401.10407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26694;&#26550;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#30340;&#30701;&#35821;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#22312;&#26080;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#30701;&#35821;&#34920;&#31034;&#65292;&#36890;&#36807;&#30701;&#35821;&#31867;&#22411;&#20998;&#31867;&#21644;&#26377;&#25928;&#22320;&#34701;&#21512;&#23383;&#31526;&#32423;&#20449;&#24687;&#26469;&#25552;&#39640;&#34920;&#31034;&#30340;&#31934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#19977;&#31181;&#31890;&#24230;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35821;&#34920;&#31034;&#22312;&#25968;&#25454;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26377;&#21033;&#20110;&#23454;&#20307;&#23545;&#40784;&#12289;&#35760;&#24405;&#38142;&#25509;&#12289;&#27169;&#31946;&#36830;&#25509;&#21644;&#37322;&#20041;&#20998;&#31867;&#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#33719;&#21462;&#30701;&#35821;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24050;&#32463;&#21457;&#29616;&#20102;&#38656;&#35201;&#25913;&#36827;&#30340;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#22797;&#26434;&#65292;&#24182;&#38656;&#35201;&#22312;&#20855;&#26377;&#19978;&#19979;&#25991;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#30701;&#35821;&#31867;&#22411;&#21644;&#24418;&#24577;&#32473;&#20986;&#26356;&#31934;&#30830;&#21644;&#26356;&#28789;&#27963;&#30340;&#30701;&#35821;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26694;&#26550;&#20197;&#20197;&#26080;&#19978;&#19979;&#25991;&#30340;&#26041;&#24335;&#23398;&#20064;&#30701;&#35821;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#23558;&#30701;&#35821;&#31867;&#22411;&#20998;&#31867;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#24182;&#26356;&#26377;&#25928;&#22320;&#23558;&#23383;&#31526;&#32423;&#20449;&#24687;&#34701;&#20837;&#30701;&#35821;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#31890;&#24230;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phrase representations play an important role in data science and natural language processing, benefiting various tasks like Entity Alignment, Record Linkage, Fuzzy Joins, and Paraphrase Classification. The current state-of-the-art method involves fine-tuning pre-trained language models for phrasal embeddings using contrastive learning. However, we have identified areas for improvement. First, these pre-trained models tend to be unnecessarily complex and require to be pre-trained on a corpus with context sentences. Second, leveraging the phrase type and morphology gives phrase representations that are both more precise and more flexible. We propose an improved framework to learn phrase representations in a context-free fashion. The framework employs phrase type classification as an auxiliary task and incorporates character-level information more effectively into the phrase representation. Furthermore, we design three granularities of data augmentation to increase the diversity of train
&lt;/p&gt;</description></item><item><title>E^2-LLM&#26159;&#19968;&#31181;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#21644;&#19981;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#20943;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#65292;E^2-LLM&#21482;&#38656;&#35201;&#36739;&#30701;&#30340;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;</title><link>http://arxiv.org/abs/2401.06951</link><description>&lt;p&gt;
E^2-LLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
E^2-LLM: Efficient and Extreme Length Extension of Large Language Models. (arXiv:2401.06951v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06951
&lt;/p&gt;
&lt;p&gt;
E^2-LLM&#26159;&#19968;&#31181;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#21644;&#19981;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#20943;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#65292;E^2-LLM&#21482;&#38656;&#35201;&#36739;&#30701;&#30340;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;GPU&#36164;&#28304;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#25193;&#23637;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#36807;&#31243;&#26469;&#25903;&#25345;&#30456;&#24212;&#30340;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#35757;&#32451;&#25968;&#25454;&#65288;&#20363;&#22914;32k&#65289;&#65292;&#24182;&#19988;&#20551;&#23450;&#26377;&#39640;&#26114;&#30340;GPU&#35757;&#32451;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E^2-LLM&#30340;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20063;&#19981;&#38656;&#35201;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;E^2-LLM&#30340;&#35757;&#32451;&#25968;&#25454;&#21482;&#38656;&#35201;&#24456;&#30701;&#30340;&#38271;&#24230;&#65288;&#20363;&#22914;4k&#65289;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35843;&#25972;&#25104;&#26412;&#12290;&#20854;&#27425;&#65292;&#22312;&#30701;&#35757;&#32451;&#19978;&#19979;&#25991;&#31383;&#21475;&#19978;&#30340;&#35757;&#32451;&#36807;&#31243;&#21482;&#25191;&#34892;&#19968;&#27425;&#65292;&#25105;&#20204;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#31532;&#19977;&#65292;&#22312;E^2-LLM&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. Existing long-context extension methods usually need additional training procedures to support corresponding long-context windows, where the long-context training data (e.g., 32k) is needed, and high GPU training costs are assumed. To address the aforementioned issues, we propose an Efficient and Extreme length extension method for Large Language Models, called E 2 -LLM, with only one training procedure and dramatically reduced computation cost, which also removes the need to collect long-context data. Concretely, first, the training data of our E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost greatly. Second, the training procedure on the short training context window is performed only once time, and we can support different evaluation context windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings, we 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#33258;&#21160;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#22823;&#37096;&#20998;&#24187;&#35273;&#23646;&#20110;&#23569;&#26377;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2401.06855</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#19982;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Hallucination Detection and Editing for Language Models. (arXiv:2401.06855v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06855
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#33258;&#21160;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#22823;&#37096;&#20998;&#24187;&#35273;&#23646;&#20110;&#23569;&#26377;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#29983;&#25104;&#22810;&#26679;&#30340;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#65292;&#34987;&#24191;&#27867;&#31216;&#20026;&#24187;&#35273;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#30340;&#33258;&#21160;&#24187;&#35273;&#26816;&#27979;&#25110;&#32534;&#36753;&#19978;&#65292;&#24573;&#35270;&#20102;&#32454;&#24494;&#30340;&#38169;&#35823;&#32423;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#8212;&#8212;&#33258;&#21160;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20845;&#20010;&#23618;&#27425;&#20998;&#26126;&#30340;&#24187;&#35273;&#31867;&#22411;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#12290;&#20026;&#20102;&#20415;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#32454;&#31890;&#24230;&#20154;&#24037;&#21028;&#26029;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;ChatGPT&#21644;Llama 2-Chat&#30340;&#36755;&#20986;&#20013;&#26377;60%&#21644;75%&#30340;&#24187;&#35273;&#65292;&#20854;&#20013;&#22810;&#25968;&#24187;&#35273;&#23646;&#20110;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#31867;&#21035;&#12290;&#20316;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21021;&#22987;&#27493;&#39588;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;FAVA&#65292;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491;&#32454;&#31890;&#24230;&#24187;&#35273;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are prone to generate diverse factually incorrect statements, which are widely called hallucinations. Current approaches predominantly focus on coarse-grained automatic hallucination detection or editing, overlooking nuanced error levels. In this paper, we propose a novel task -- automatic fine-grained hallucination detection -- and present a comprehensive taxonomy encompassing six hierarchically defined types of hallucination. To facilitate evaluation, we introduce a new benchmark that includes fine-grained human judgments on two LM outputs across various domains. Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in 60% and 75% of their outputs, respectively, and a majority of these hallucinations fall into categories that have been underexplored. As an initial step to address this, we train FAVA, a retrieval-augmented LM by carefully designing synthetic data generations to detect and correct fine-grained hallucinations. On our bench
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.05930</link><description>&lt;p&gt;
SH2: &#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#24110;&#21161;&#24744;&#26356;&#20934;&#30830;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05930
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#26041;&#27861;&#65292;&#21363;&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;(SH2)&#65292;&#20197;&#24110;&#21161;LLMs&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;SH2&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#20013;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#23454;&#65292;&#21363;&#23545;&#20110;LLMs&#32780;&#35328;&#65292;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#24448;&#24448;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLMs&#32473;&#20104;&#36739;&#20302;&#27010;&#29575;&#30340;&#26631;&#35760;&#26356;&#26377;&#21487;&#33021;&#19982;&#20107;&#23454;&#20449;&#24687;&#65288;&#22914;&#21517;&#35789;&#12289;&#19987;&#26377;&#21517;&#35789;&#21644;&#24418;&#23481;&#35789;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#36873;&#25321;&#27010;&#29575;&#26368;&#20302;&#30340;&#26631;&#35760;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#21407;&#22987;&#19978;&#19979;&#25991;&#20013;&#26469;&#8220;&#31361;&#20986;&#8221;&#20107;&#23454;&#20449;&#24687;&#65292;&#20174;&#32780;&#36843;&#20351;&#27169;&#22411;&#22312;&#29983;&#25104;&#20043;&#21069;&#22810;&#27425;&#38405;&#35835;&#21644;&#29369;&#35947;&#36825;&#20123;&#26631;&#35760;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#23545;&#27604;&#35299;&#30721;&#30340;&#26041;&#24335;&#26469;&#24378;&#35843;&#30001;&#29369;&#35947;&#24102;&#26469;&#30340;&#36755;&#20986;&#27010;&#29575;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.
&lt;/p&gt;</description></item><item><title>LLaVA-$\phi$&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#21363;&#20351;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#23427;&#20063;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2401.02330</link><description>&lt;p&gt;
LLaVA-$\phi$: &#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#19982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-$\phi$: Efficient Multi-Modal Assistant with Small Language Model. (arXiv:2401.02330v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02330
&lt;/p&gt;
&lt;p&gt;
LLaVA-$\phi$&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#21363;&#20351;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#23427;&#20063;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LLaVA-$\phi$&#65288;LLaVA-Phi&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#20808;&#36827;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;&#12290;LLaVA-Phi&#22312;&#32039;&#20945;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#39046;&#22495;&#20013;&#26631;&#24535;&#30528;&#37325;&#35201;&#36827;&#23637;&#12290;&#23427;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#20010;&#21442;&#25968;&#21482;&#26377;27&#20159;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#26377;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#21442;&#19982;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#30340;&#22797;&#26434;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21253;&#25324;&#35270;&#35273;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#24863;&#30693;&#31561;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#38500;&#20102;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#65288;&#22914;&#23454;&#20307;&#20195;&#29702;&#65289;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;&#23427;&#31361;&#26174;&#20102;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#32423;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models. It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora. Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception. Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in time-sensitive environments and systems that require real-time interaction, such as embodied agents. It highlights the potential of smaller language models to achieve sophisticated levels of understanding and inte
&lt;/p&gt;</description></item><item><title>TREC iKAT 2023&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#36866;&#24212;&#29992;&#25143;&#20132;&#20114;&#21644;&#19978;&#19979;&#25991;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#35813;&#20219;&#21153;&#36824;&#24378;&#35843;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#29992;&#25143;&#36890;&#36807;&#31579;&#36873;&#25968;&#25454;&#21644;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#21644;&#25191;&#34892;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.01330</link><description>&lt;p&gt;
TREC iKAT 2023: &#20132;&#20114;&#24335;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview. (arXiv:2401.01330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01330
&lt;/p&gt;
&lt;p&gt;
TREC iKAT 2023&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#36866;&#24212;&#29992;&#25143;&#20132;&#20114;&#21644;&#19978;&#19979;&#25991;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#35813;&#20219;&#21153;&#36824;&#24378;&#35843;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#29992;&#25143;&#36890;&#36807;&#31579;&#36873;&#25968;&#25454;&#21644;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#21644;&#25191;&#34892;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20449;&#24687;&#26597;&#35810;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#20063;&#26377;&#24456;&#22823;&#30340;&#36129;&#29486;&#12290;TREC&#20132;&#20114;&#24335;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65288;iKAT&#65289;&#24314;&#31435;&#22312;TREC&#20250;&#35805;&#36741;&#21161;&#20219;&#21153;&#65288;CAsT&#65289;&#30340;&#22522;&#30784;&#19978;&#12290;&#28982;&#32780;&#65292;iKAT&#30528;&#37325;&#20110;&#21019;&#24314;&#21644;&#30740;&#31350;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#20043;&#21069;&#30340;&#20132;&#20114;&#21644;&#24403;&#21069;&#24773;&#22659;&#33258;&#36866;&#24212;&#21709;&#24212;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#25361;&#25112;&#22312;&#20110;&#20351;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#33021;&#22815;&#23558;&#20010;&#24615;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#34701;&#20837;&#21040;&#30456;&#24212;&#20013;&#65292;&#20197;&#39640;&#25928;&#22320;&#24341;&#23548;&#29992;&#25143;&#33719;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;iKAT&#36824;&#30528;&#37325;&#20110;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#21363;&#29992;&#25143;&#36890;&#36807;&#25968;&#25454;&#21644;&#20449;&#24687;&#31579;&#36873;&#26469;&#34913;&#37327;&#21508;&#31181;&#36873;&#25321;&#65292;&#20197;&#36798;&#21040;&#32467;&#35770;&#25110;&#25191;&#34892;&#21160;&#20316;&#12290;&#36825;&#20123;&#20219;&#21153;&#22312;&#26085;&#24120;&#20449;&#24687;&#25628;&#32034;&#20915;&#31574;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#26080;&#35770;&#26159;&#26053;&#28216;&#12289;&#20581;&#24247;&#36824;&#26159;&#36141;&#29289;&#31561;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#32452;&#39640;&#32423;&#20449;&#24687;&#25805;&#20316;&#31526;&#65292;&#20854;&#20013;&#26597;&#35810;&#25110;&#38382;&#39064;&#21487;&#33021;&#20250;
&lt;/p&gt;
&lt;p&gt;
Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works. The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context. The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them. iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action. These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>BLP 2023&#20219;&#21153;2&#26159;&#20851;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#12290;&#21442;&#19982;&#32773;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20219;&#21153;&#30340;&#35814;&#32454;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#25552;&#20132;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.16183</link><description>&lt;p&gt;
BLP 2023&#20219;&#21153;2&#65306;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BLP 2023 Task 2: Sentiment Analysis. (arXiv:2310.16183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16183
&lt;/p&gt;
&lt;p&gt;
BLP 2023&#20219;&#21153;2&#26159;&#20851;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#12290;&#21442;&#19982;&#32773;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20219;&#21153;&#30340;&#35814;&#32454;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#25552;&#20132;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24635;&#32467;&#20102;&#20316;&#20026;BLP 2023&#21019;&#26032;&#24037;&#20316;&#22346;&#30340;&#19968;&#37096;&#20998;&#20030;&#21150;&#30340;BLP&#24773;&#24863;&#20849;&#20139;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#30340;&#23450;&#20041;&#26159;&#22312;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#12290;&#35813;&#20219;&#21153;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#38454;&#27573;&#20998;&#21035;&#26377;29&#20010;&#21644;30&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#12290;&#24635;&#20849;&#65292;&#21442;&#19982;&#32773;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24635;&#20849;&#26377;15&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#25551;&#36848;&#35770;&#25991;&#12290;&#25552;&#20132;&#30340;&#31995;&#32479;&#28085;&#30422;&#20102;&#20174;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#35813;&#20219;&#21153;&#30340;&#35774;&#32622;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#21442;&#19982;&#32773;&#25552;&#20132;&#30340;&#31995;&#32479;&#12290;&#20849;&#20139;&#20219;&#21153;&#30340;&#25152;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#33050;&#26412;&#24050;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an overview of the BLP Sentiment Shared Task, organized as part of the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is defined as the detection of sentiment in a given piece of social media text. This task attracted interest from 71 participants, among whom 29 and 30 teams submitted systems during the development and evaluation phases, respectively. In total, participants submitted 597 runs. However, a total of 15 teams submitted system description papers. The range of approaches in the submitted systems spans from classical machine learning models, fine-tuning pre-trained models, to leveraging Large Language Model (LLMs) in zero- and few-shot settings. In this paper, we provide a detailed account of the task setup, including dataset development and evaluation setup. Additionally, we provide a brief overview of the systems submitted by the participants. All datasets and evaluation scripts from the shared task have been made publicly available for the res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.10690</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#29983;&#24314;&#27169;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20174;&#19968;&#27425;&#24615;&#35266;&#23519;&#20013;&#21512;&#25104;&#35270;&#35273;&#32534;&#31243;&#20013;&#23398;&#29983;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. (arXiv:2310.10690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#24314;&#27169;&#23545;&#20110;&#35768;&#22810;&#25945;&#32946;&#25216;&#26415;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#23398;&#20064;&#32467;&#26524;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#23398;&#29983;&#34920;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#19988;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#23398;&#20064;&#25216;&#33021;&#38598;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LLM-SS&#65292;&#21033;&#29992;LLMs&#21512;&#25104;&#23398;&#29983;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#29305;&#23450;&#23398;&#29983;&#22312;&#21442;&#32771;&#20219;&#21153;&#19978;&#30340;&#35299;&#20915;&#23581;&#35797;&#20316;&#20026;&#35266;&#23519;&#65292;&#30446;&#26631;&#26159;&#21512;&#25104;&#35813;&#23398;&#29983;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;LLMs&#32467;&#21512;&#20351;&#29992;&#65307;&#32780;&#19988;&#65292;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#23427;&#20204;&#23545;&#39046;&#22495;&#32972;&#26223;&#21644;&#23398;&#29983;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#20855;&#20307;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Student modeling is central to many educational technologies as it enables the prediction of future learning outcomes and targeted instructional strategies. However, open-ended learning environments pose challenges for accurately modeling students due to the diverse behaviors exhibited by students and the absence of a well-defined set of learning skills. To approach these challenges, we explore the application of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. We introduce a novel framework, LLM-SS, that leverages LLMs for synthesizing student's behavior. More concretely, given a particular student's solving attempt on a reference task as observation, the goal is to synthesize the student's attempt on a target task. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs using domain-specific expertise to boost their understanding of domain background and student behaviors. We evaluate several concrete methods bas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#35843;&#26597;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#30340;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#25913;&#36827;&#32972;&#21518;&#30340;&#28508;&#22312;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21442;&#25968;&#21270;&#25506;&#27979;&#21644;&#27880;&#24847;&#21147;&#27604;&#29575;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00313</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;: &#23545;&#34920;&#31034;&#30340;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#24335;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations. (arXiv:2310.00313v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#35843;&#26597;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#30340;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#25913;&#36827;&#32972;&#21518;&#30340;&#28508;&#22312;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21442;&#25968;&#21270;&#25506;&#27979;&#21644;&#27880;&#24847;&#21147;&#27604;&#29575;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#29305;&#23450;&#20219;&#21153;&#31034;&#20363;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25913;&#36827;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;Llama-2 70B&#21644;Vicuna 13B&#20013;&#30340;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#30340;&#21464;&#21270;&#20197;&#21450;&#36825;&#20123;&#21464;&#21270;&#22914;&#20309;&#35843;&#35299;&#34892;&#20026;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21463;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#22914;&#34920;&#31034;&#30456;&#20284;&#24615;&#20998;&#26512;&#65288;RSA&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#25506;&#27979;&#21644;&#27880;&#24847;&#21147;&#27604;&#29575;&#20998;&#26512;&#65288;ARA&#65292;&#34913;&#37327;&#20851;&#27880;&#30456;&#20851;&#19982;&#26080;&#20851;&#20449;&#24687;&#30340;&#27604;&#29575;&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#20855;&#26377;&#26465;&#20214;&#20043;&#38388;&#20808;&#39564;&#20851;&#31995;&#30340;&#20219;&#21153;&#65306;&#38405;&#35835;&#29702;&#35299;&#65292;&#32447;&#24615;&#22238;&#24402;&#21644;&#23545;&#25239;&#25552;&#31034;&#27880;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#20219;&#21153;&#34920;&#31034;&#20013;&#39044;&#26399;&#30456;&#20284;&#24615;&#30340;&#20551;&#35774;&#65292;&#20197;&#30740;&#31350;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#20013;&#30340;&#28508;&#22312;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attenti
&lt;/p&gt;</description></item><item><title>DiLu&#26159;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#37319;&#29992;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#29702;&#21644;&#21453;&#24605;&#27169;&#22359;&#36827;&#34892;&#20915;&#31574;&#65292;&#31215;&#32047;&#32463;&#39564;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16292</link><description>&lt;p&gt;
DiLu: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#30340;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models. (arXiv:2309.16292v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16292
&lt;/p&gt;
&lt;p&gt;
DiLu&#26159;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#37319;&#29992;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#29702;&#21644;&#21453;&#24605;&#27169;&#22359;&#36827;&#34892;&#20915;&#31574;&#65292;&#31215;&#32047;&#32463;&#39564;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#20381;&#36182;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#34429;&#28982;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#38754;&#20020;&#25968;&#25454;&#38598;&#20559;&#35265;&#12289;&#36807;&#25311;&#21512;&#21644;&#19981;&#21487;&#35299;&#37322;&#24615;&#31561;&#25361;&#25112;&#12290;&#21463;&#20154;&#31867;&#39550;&#39542;&#30693;&#35782;&#39537;&#21160;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#22914;&#20309;&#23558;&#31867;&#20284;&#30340;&#33021;&#21147;&#27880;&#20837;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20114;&#21160;&#29615;&#22659;&#12289;&#39550;&#39542;&#21592;&#20195;&#29702;&#21644;&#35760;&#24518;&#32452;&#20214;&#30340;&#33539;&#20363;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#26032;&#20852;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiLu&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#25512;&#29702;&#27169;&#22359;&#21644;&#21453;&#24605;&#27169;&#22359;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#20381;&#25454;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#20915;&#31574;&#65292;&#24182;&#25345;&#32493;&#28436;&#21270;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;DiLu&#33021;&#22815;&#31215;&#32047;&#32463;&#39564;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;DiLu&#33021;&#22815;&#30452;&#25509;&#20174;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets w
&lt;/p&gt;</description></item><item><title>ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.13007</link><description>&lt;p&gt;
ReConcile&#65306;&#22278;&#26700;&#20250;&#35758;&#36890;&#36807;&#22810;&#20803;LLM&#30340;&#20849;&#35782;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13007
&lt;/p&gt;
&lt;p&gt;
ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#21463;&#21040;&#24515;&#26234;&#31038;&#20250;&#29702;&#35770;&#65288;Minsky, 1988&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReConcile&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22810;&#26679;&#30340;LLM&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#22278;&#26700;&#20250;&#35758;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#24605;&#24819;&#21644;&#35752;&#35770;&#65292;&#20174;&#32780;&#25913;&#36827;&#19968;&#33268;&#24615;&#12290;ReConcile&#36890;&#36807;&#36827;&#34892;&#22810;&#36718;&#35752;&#35770;&#12289;&#23398;&#20064;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#25913;&#36827;&#31572;&#26696;&#20197;&#21450;&#37319;&#29992;&#32622;&#20449;&#24230;&#21152;&#26435;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;ReConcile&#36890;&#36807;&#8220;&#35752;&#35770;&#25552;&#31034;&#8221;&#26469;&#21551;&#21160;&#20195;&#29702;&#20154;&#38388;&#30340;&#35752;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#19978;&#19968;&#36718;&#27599;&#20010;&#20195;&#29702;&#20154;&#29983;&#25104;&#30340;&#31572;&#26696;&#21644;&#35299;&#37322;&#30340;&#20998;&#32452;&#12289;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#29992;&#20110;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#31572;&#26696;&#20462;&#27491;&#20154;&#31867;&#35299;&#37322;&#30340;&#28436;&#31034;&#12290;&#36825;&#20010;&#35752;&#35770;&#25552;&#31034;&#20351;&#27599;&#20010;&#20195;&#29702;&#20154;&#33021;&#22815;&#26681;&#25454;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#35265;&#35299;&#20462;&#35746;&#33258;&#24049;&#30340;&#22238;&#31572;&#12290;&#19968;&#26086;&#36798;&#25104;&#19968;&#33268;&#24182;&#32467;&#26463;&#35752;&#35770;&#65292;ReConcile&#25191;&#34892;&#19968;&#27425;&#20840;&#20307;&#25237;&#31080;&#20197;&#30830;&#23450;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
&lt;/p&gt;</description></item><item><title>SilverRetriever&#26159;&#19968;&#20010;&#29305;&#20026;&#27874;&#20848;&#35821;&#38382;&#31572;&#31995;&#32479;&#24320;&#21457;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08469</link><description>&lt;p&gt;
SilverRetriever&#65306;&#25552;&#21319;&#27874;&#20848;&#38382;&#31572;&#31995;&#32479;&#30340;&#31070;&#32463;&#36890;&#36947;&#26816;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SilverRetriever: Advancing Neural Passage Retrieval for Polish Question Answering. (arXiv:2309.08469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08469
&lt;/p&gt;
&lt;p&gt;
SilverRetriever&#26159;&#19968;&#20010;&#29305;&#20026;&#27874;&#20848;&#35821;&#38382;&#31572;&#31995;&#32479;&#24320;&#21457;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#20110;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#32452;&#20214;&#26469;&#25214;&#21040;&#21253;&#21547;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#31070;&#32463;&#26816;&#32034;&#22120;&#27604;&#35789;&#27719;&#26367;&#20195;&#26041;&#24335;&#26356;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#27969;&#34892;&#35821;&#35328;&#22914;&#33521;&#35821;&#25110;&#20013;&#25991;&#19978;&#65292;&#23545;&#20110;&#20854;&#20182;&#35821;&#35328;&#22914;&#27874;&#20848;&#35821;&#65292;&#21487;&#29992;&#30340;&#27169;&#22411;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SilverRetriever&#65292;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#25163;&#21160;&#26631;&#35760;&#25110;&#24369;&#26631;&#35760;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27874;&#20848;&#35821;&#31070;&#32463;&#26816;&#32034;&#22120;&#12290;SilverRetriever&#22312;&#27874;&#20848;&#35821;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#19982;&#35813;&#27169;&#22411;&#19968;&#36215;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;&#20116;&#20010;&#26032;&#30340;&#27573;&#33853;&#26816;&#32034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern open-domain question answering systems often rely on accurate and efficient retrieval components to find passages containing the facts necessary to answer the question. Recently, neural retrievers have gained popularity over lexical alternatives due to their superior performance. However, most of the work concerns popular languages such as English or Chinese. For others, such as Polish, few models are available. In this work, we present SilverRetriever, a neural retriever for Polish trained on a diverse collection of manually or weakly labeled datasets. SilverRetriever achieves much better results than other Polish models and is competitive with larger multilingual models. Together with the model, we open-source five new passage retrieval datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#12290;&#30740;&#31350;&#25351;&#20986;&#36825;&#19968;&#20559;&#24046;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PriDe&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03882</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Large Language Models' Selection Bias in Multi-Choice Questions. (arXiv:2309.03882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#12290;&#30740;&#31350;&#25351;&#20986;&#36825;&#19968;&#20559;&#24046;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PriDe&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQs&#65289;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30740;&#31350;&#20013;&#24120;&#35265;&#19988;&#37325;&#35201;&#30340;&#20219;&#21153;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;LLMs&#22312;MCQs&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#8220;&#36873;&#25321;&#20559;&#24046;&#8221;&#65292;&#21363;LLMs&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#65288;&#22914;&#8220;&#36873;&#39033;C&#8221;&#65289;&#12290;&#36825;&#31181;&#20559;&#24046;&#22312;&#21508;&#31181;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;MCQs&#20013;&#23545;&#36873;&#39033;&#20301;&#32622;&#21464;&#21270;&#30340;&#24615;&#33021;&#21464;&#24471;&#33030;&#24369;&#12290;&#25105;&#20204;&#21457;&#29616;&#23548;&#33268;&#36873;&#25321;&#20559;&#24046;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#21363;&#19982;&#36873;&#39033;&#30456;&#20851;&#30340;ID&#31526;&#21495;A/B/C/D&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#31216;&#20026;PriDe&#12290;PriDe&#39318;&#20808;&#23558;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#20998;&#35299;&#20026;&#23545;&#36873;&#39033;&#20869;&#23481;&#30340;&#20869;&#22312;&#39044;&#27979;&#21644;&#23545;&#36873;&#39033;ID&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#22312;&#23569;&#37327;&#27979;&#35797;&#26679;&#26412;&#19978;&#23545;&#36873;&#39033;&#20869;&#23481;&#36827;&#34892;&#25490;&#21015;&#32452;&#21512;&#26469;&#20272;&#35745;&#20808;&#39564;&#65292;&#20174;&#32780;&#29992;&#20110;&#28040;&#38500;&#21518;&#32493;&#27979;&#35797;&#26679;&#26412;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20316;&#20026;&#19968;&#31181;&#26080;&#26631;&#31614;&#12289;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;PriDe&#21487;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#19988;&#31283;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-choice questions (MCQs) serve as a common yet important task format in the research of large language models (LLMs). Our work shows that LLMs exhibit an inherent "selection bias" in MCQs, which refers to LLMs' preferences to select options located at specific positions (like "Option C"). This bias is prevalent across various LLMs, making their performance vulnerable to option position changes in MCQs. We identify that one primary cause resulting in selection bias is option numbering, i.e., the ID symbols A/B/C/D associated with the options. To mitigate selection bias, we propose a new method called PriDe. PriDe first decomposes the observed model prediction distribution into an intrinsic prediction over option contents and a prior distribution over option IDs. It then estimates the prior by permutating option contents on a small number of test samples, which is used to debias the subsequent test samples. We demonstrate that, as a label-free, inference-time method, PriDe achieves 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.16898</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;"Attention Is All You Need"&#20013;&#24341;&#20837;&#36716;&#25442;&#22120;&#26550;&#26500;&#20197;&#26469;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#25509;&#21463;&#36755;&#20837;&#20196;&#29260;&#24207;&#21015;$X$&#24182;&#36890;&#36807;&#35745;&#31639;softmax$(XQK^\top X^\top)$&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;$(K,Q)$&#26159;&#21487;&#35757;&#32451;&#30340;&#38190;-&#26597;&#35810;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#20248;&#21270;&#20960;&#20309;&#21644;&#19968;&#20010;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#20196;&#29260;&#23545;&#30340;&#22806;&#31215;&#26045;&#21152;&#32447;&#24615;&#32422;&#26463;&#65292;&#23558;&#26368;&#20339;&#36755;&#20837;&#20196;&#29260;&#19982;&#38750;&#26368;&#20339;&#20196;&#29260;&#20998;&#31163;&#12290;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#21333;&#23618;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#65306;(1)&#20248;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#20351;&#29992;&#21487;&#21464;&#27491;&#21017;&#21270;&#21442;&#25968;$(K,Q)$&#65292;&#25910;&#25947;&#30340;&#26041;&#21521;&#26159;&#19968;&#20010;&#26368;&#23567;&#21270;&#32508;&#21512;&#21442;&#25968;$W=KQ^\top$&#30340;&#26680;&#33539;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#35299;&#20915;&#26041;&#26696;&#12290;&#32780;&#30452;&#25509;&#20351;&#29992;$W$&#36827;&#34892;&#21442;&#25968;&#21270;&#21017;&#26368;&#23567;&#21270;&#19968;&#20010;Frobenius&#33539;&#25968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#24341;&#23548;&#30340;&#23545;&#25239;&#25915;&#20987;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#25972;&#20307;&#24847;&#20041;&#29983;&#25104;&#20445;&#25345;&#35821;&#20041;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20174;&#32780;&#20351;&#24471;&#32763;&#35793;&#32467;&#26524;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.15246</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#24341;&#23548;&#30340;&#23545;&#25239;&#25915;&#20987;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation. (arXiv:2308.15246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#24341;&#23548;&#30340;&#23545;&#25239;&#25915;&#20987;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#25972;&#20307;&#24847;&#20041;&#29983;&#25104;&#20445;&#25345;&#35821;&#20041;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20174;&#32780;&#20351;&#24471;&#32763;&#35793;&#32467;&#26524;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#36755;&#20837;&#25200;&#21160;&#26469;&#35823;&#23548;&#30446;&#26631;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ACT&#30340;&#26032;&#22411;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#38024;&#23545;NMT&#31995;&#32479;&#36827;&#34892;&#25915;&#20987;&#65292;&#25915;&#20987;&#36807;&#31243;&#20013;&#24341;&#23548;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#12290;&#22312;&#25105;&#20204;&#30340;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#26088;&#22312;&#29983;&#25104;&#20445;&#25345;&#35821;&#20041;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#24471;NMT&#27169;&#22411;&#30340;&#32763;&#35793;&#32467;&#26524;&#19982;&#30446;&#26631;&#35821;&#35328;&#20013;&#30340;&#21407;&#22987;&#32763;&#35793;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#19982;&#20043;&#21069;&#30340;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#26356;&#33021;&#25913;&#21464;&#25972;&#20307;&#24847;&#20041;&#65292;&#20174;&#32780;&#36890;&#36807;&#20998;&#31867;&#22120;&#23558;&#20854;&#24402;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#35780;&#20272;NMT&#27169;&#22411;&#23545;&#35813;&#25915;&#20987;&#30340;&#25269;&#25239;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377;&#22522;&#20110;&#21333;&#35789;&#26367;&#25442;&#30340;&#40657;&#30418;&#25915;&#20987;&#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#24341;&#20837;&#30446;&#26631;NMT&#27169;&#22411;&#30340;&#36755;&#20986;&#32763;&#35793;&#21644;&#19968;&#20010;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;logit&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation (NMT) models have been shown to be vulnerable to adversarial attacks, wherein carefully crafted perturbations of the input can mislead the target model. In this paper, we introduce ACT, a novel adversarial attack framework against NMT systems guided by a classifier. In our attack, the adversary aims to craft meaning-preserving adversarial examples whose translations by the NMT model belong to a different class than the original translations in the target language. Unlike previous attacks, our new approach has a more substantial effect on the translation by altering the overall meaning, which leads to a different class determined by a classifier. To evaluate the robustness of NMT models to this attack, we propose enhancements to existing black-box word-replacement-based attacks by incorporating output translations of the target NMT model and the output logits of a classifier within the attack process. Extensive experiments in various settings, including a comp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#29983;&#25104;&#35843;&#30740;&#25991;&#31456;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;GPT-4&#20248;&#20110;GPT-3.5&#65292;&#24182;&#19988;&#25351;&#20986;&#20102;GPT&#22312;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2308.10410</link><description>&lt;p&gt;
&#22522;&#20110;&#32500;&#22522;&#30334;&#31185;&#39118;&#26684;&#30340;&#35843;&#30740;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27010;&#24565;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts. (arXiv:2308.10410v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#29983;&#25104;&#35843;&#30740;&#25991;&#31456;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;GPT-4&#20248;&#20110;GPT-3.5&#65292;&#24182;&#19988;&#25351;&#20986;&#20102;GPT&#22312;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#12290;&#34429;&#28982;LLMs&#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#20173;&#22312;&#25506;&#32034;&#20013;&#12290;&#27492;&#22806;&#65292;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#21644;&#19981;&#23454;&#20449;&#24687;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;-NLP&#39046;&#22495;&#20013;&#29983;&#25104;&#31616;&#27905;&#35843;&#30740;&#25991;&#31456;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;20&#20010;&#36873;&#23450;&#30340;&#20027;&#39064;&#12290;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;GPT-4&#22312;&#19982;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#20248;&#20110;GPT-3.5&#12290;&#27492;&#22806;&#65292;&#22235;&#20301;&#20154;&#31867;&#35780;&#20272;&#32773;&#20174;&#22235;&#20010;&#27169;&#22411;&#37197;&#32622;&#30340;&#20845;&#20010;&#35282;&#24230;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#34429;&#28982;GPT&#36890;&#24120;&#33021;&#20135;&#29983;&#21487;&#31216;&#36190;&#30340;&#32467;&#26524;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#22914;&#20449;&#24687;&#19981;&#23436;&#25972;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved significant success across various natural language processing (NLP) tasks, encompassing question-answering, summarization, and machine translation, among others. While LLMs excel in general tasks, their efficacy in domain-specific applications remains under exploration. Additionally, LLM-generated text sometimes exhibits issues like hallucination and disinformation. In this study, we assess LLMs' capability of producing concise survey articles within the computer science-NLP domain, focusing on 20 chosen topics. Automated evaluations indicate that GPT-4 outperforms GPT-3.5 when benchmarked against the ground truth. Furthermore, four human evaluators provide insights from six perspectives across four model configurations. Through case studies, we demonstrate that while GPT often yields commendable results, there are instances of shortcomings, such as incomplete information and the exhibition of lapses in factual accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2308.08493</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#26102;&#38388;&#26053;&#34892;&#65306;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#26159;&#25351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#29702;&#35299;LLMs&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#36890;&#36807;&#35782;&#21035;&#20174;&#23567;&#30340;&#38543;&#26426;&#26679;&#26412;&#20013;&#25277;&#21462;&#30340;&#21333;&#20010;&#23454;&#20363;&#20013;&#30340;&#28508;&#22312;&#27745;&#26579;&#65292;&#28982;&#21518;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20272;&#35745;&#21333;&#20010;&#23454;&#20363;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#65306;&#21363;&#19968;&#20010;&#30001;&#25968;&#25454;&#38598;&#21517;&#31216;&#12289;&#20998;&#21306;&#31867;&#22411;&#21644;&#21442;&#32771;&#23454;&#20363;&#30340;&#21021;&#22987;&#37096;&#20998;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#35201;&#27714;LLM&#23436;&#25104;&#23427;&#12290;&#22914;&#26524;LLM&#30340;&#36755;&#20986;&#19982;&#21442;&#32771;&#23454;&#20363;&#30340;&#21518;&#19968;&#37096;&#20998;&#23436;&#20840;&#25110;&#25509;&#36817;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#23454;&#20363;&#34987;&#26631;&#35760;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20102;&#35299;&#25972;&#20010;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24819;&#27861;&#12290;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#26631;&#35760;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#20013;&#30340;&#23454;&#20363;&#22823;&#22810;&#25968;&#37117;&#34987;&#21028;&#26029;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#26041;&#27861;&#26469;&#28608;&#27963;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08241</link><description>&lt;p&gt;
TEST: &#25991;&#26412;&#21407;&#22411;&#23545;&#40784;&#23884;&#20837;&#20197;&#28608;&#27963;LLM&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. (arXiv:2308.08241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#26041;&#27861;&#26469;&#28608;&#27963;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#65288;TS&#65289;&#20219;&#21153;&#30340;&#31574;&#30053;&#65306;LLM-for-TS&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#19968;&#20010;&#38024;&#23545;TS&#25968;&#25454;&#30340;&#22522;&#30784;&#22823;&#27169;&#22411;&#65307;TS-for-LLM&#65292;&#20351;&#39044;&#35757;&#32451;&#30340;LLM&#33021;&#22815;&#22788;&#29702;TS&#25968;&#25454;&#12290;&#37492;&#20110;&#25968;&#25454;&#31215;&#32047;&#19981;&#36275;&#12289;&#36164;&#28304;&#26377;&#38480;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;TS-for-LLM&#26041;&#27861;&#65292;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;LLM&#30340;TS&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#28608;&#27963;LLM&#23545;TS&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;TEST&#12290;&#23427;&#39318;&#20808;&#23545;TS&#36827;&#34892;&#26631;&#35760;&#21270;&#22788;&#29702;&#65292;&#24314;&#31435;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23454;&#20363;&#12289;&#29305;&#24449;&#21644;&#25991;&#26412;&#21407;&#22411;&#23545;&#40784;&#23545;&#23427;&#20204;&#36827;&#34892;&#23884;&#20837;&#65292;&#28982;&#21518;&#21019;&#24314;&#25552;&#31034;&#20197;&#20351;LLM&#26356;&#23481;&#26131;&#25509;&#21463;&#23884;&#20837;&#65292;&#24182;&#26368;&#32456;&#23454;&#26045;TS&#20219;&#21153;&#12290;&#20351;&#29992;8&#20010;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#22823;&#23567;&#30340;LLM&#23545;TS&#20998;&#31867;&#21644;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23613;&#31649;&#20854;&#32467;&#26524;&#19981;&#33021;&#26174;&#33879;&#36229;&#36234;&#24403;&#21069;&#20026;TS&#20219;&#21153;&#23450;&#21046;&#30340;SOTA&#27169;&#22411;&#65292;&#20294;&#36890;&#36807;&#23558;LLM&#35270;&#20026;&#27169;&#24335;&#26426;&#22120;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;TS&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work summarizes two strategies for completing time-series (TS) tasks using today's language model (LLM): LLM-for-TS, design and train a fundamental large model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS data. Considering the insufficient data accumulation, limited resources, and semantic context requirements, this work focuses on TS-for-LLM methods, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed them by instance-wise, feature-wise, and text-prototype-aligned contrast, and then creates prompts to make LLM more open to embeddings, and finally implements TS tasks. Experiments are carried out on TS classification and forecasting tasks using 8 LLMs with different structures and sizes. Although its results cannot significantly outperform the current SOTA models customized for TS tasks, by treating LLM as the pattern machine, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06032</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;ChatGPT&#33021;&#21542;&#21462;&#20195;&#24459;&#24072;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#23545;&#27861;&#24459;&#31995;&#32479;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#22312;&#36827;&#34892;&#27861;&#24459;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30740;&#31350;&#28041;&#21450;&#21152;&#23494;&#36135;&#24065;&#30340;&#35777;&#21048;&#26696;&#20214;&#65292;&#20316;&#20026;AI&#21487;&#20197;&#25903;&#25345;&#27861;&#24459;&#36807;&#31243;&#30340;&#20247;&#22810;&#24773;&#22659;&#20043;&#19968;&#65292;&#30740;&#31350;LLMs&#30340;&#27861;&#24459;&#25512;&#29702;&#21644;&#36215;&#33609;&#33021;&#21147;&#12290;&#25105;&#20204;&#26816;&#26597;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;a&#65289;LLM&#33021;&#21542;&#20934;&#30830;&#30830;&#23450;&#20107;&#23454;&#27169;&#24335;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36829;&#27861;&#34892;&#20026;&#65292;b&#65289;&#22522;&#20110;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#65292;&#38506;&#23457;&#22242;&#30340;&#20915;&#31574;&#26159;&#21542;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#30495;&#23454;&#26696;&#20363;&#20013;&#30340;&#20107;&#23454;&#27169;&#24335;&#36755;&#20837;GPT-3.5&#65292;&#24182;&#35780;&#20272;&#20854;&#30830;&#23450;&#27491;&#30830;&#28508;&#22312;&#36829;&#27861;&#34892;&#20026;&#24182;&#25490;&#38500;&#34394;&#20551;&#36829;&#27861;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35831;&#27169;&#25311;&#38506;&#23457;&#21592;&#35780;&#20272;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#12290;GPT-3.5&#30340;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#36739;&#24369;&#65292;&#20294;&#25105;&#20204;&#39044;&#26399;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#24314;&#35758;&#30340;&#36829;&#27861;&#34892;&#20026;&#24448;&#24448;&#26159;&#27491;&#30830;&#30340;&#65288;&#23427;&#20165;&#20165;&#36807;&#20110;&#20445;&#23432;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09312</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65306;&#25972;&#21512;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#21464;&#25442;&#22120;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09312
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#20110;&#22270;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65288;mDT&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#26631;&#35760;&#35780;&#35770;&#20026;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#22260;&#32469;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20307;&#20998;&#26512;&#23637;&#24320;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#22270;&#21464;&#25442;&#22120;&#26469;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#20132;&#32455;&#34701;&#21512;&#23618;&#26469;&#32452;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25552;&#20379;&#31038;&#20250;&#20215;&#20540;&#30340;&#26410;&#26469;&#24037;&#20316;&#65292;&#24182;&#35748;&#20026;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01938</link><description>&lt;p&gt;
Doc2SoarGraph&#65306;&#22522;&#20110;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#30340;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26723;&#30340;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs. (arXiv:2305.01938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20004;&#24180;&#26469;&#65292;&#23545;&#20110;&#34920;&#26684;&#25991;&#26412;&#25991;&#26723;&#65288;&#20363;&#22914;&#36130;&#21153;&#25253;&#21578;&#65289;&#30340;&#31163;&#25955;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22823;&#22810;&#36890;&#36807;&#25163;&#21160;&#36873;&#25321;&#21644;&#36716;&#25442;&#25991;&#26723;&#39029;&#38754;&#21040;&#32467;&#26500;&#21270;&#30340;&#34920;&#26684;&#21644;&#27573;&#33853;&#26469;&#31616;&#21270;&#36825;&#19968;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#19968;&#31181;&#26356;&#20026;&#29616;&#23454;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#21363;&#20197; TAT-DQA &#30340;&#24418;&#24335;&#22238;&#31572;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Doc2SoarGraph &#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#19981;&#21516;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#20854;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545; TAT-DQA &#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#31934;&#30830;&#21305;&#37197;&#65288;EM&#65289;&#21644; F1 &#24471;&#20998;&#26041;&#38754;&#20998;&#21035;&#27604;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;&#20102; 17.73% &#21644; 16.91%&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete reasoning over table-text documents (e.g., financial reports) gains increasing attention in recent two years. Existing works mostly simplify this challenge by manually selecting and transforming document pages to structured tables and paragraphs, hindering their practical application. In this work, we explore a more realistic problem setting in the form of TAT-DQA, i.e. to answer the question over a visually-rich table-text document. Specifically, we propose a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements (e.g., quantities, dates) of the given question and document with Semantic-oriented hierarchical Graph structures. We conduct extensive experiments on TAT-DQA dataset, and the results show that our proposed framework outperforms the best baseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score respectively on the test set, achieving the new state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SMILE&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT&#23558;&#20844;&#20849;&#21333;&#36718;&#23545;&#35805;&#25193;&#23637;&#20026;&#22810;&#36718;&#23545;&#35805;&#65292;&#29983;&#25104;&#20102;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#25509;&#36817;&#30495;&#23454;&#29983;&#27963;&#30340;&#22810;&#36718;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#19987;&#38376;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.00450</link><description>&lt;p&gt;
SMILE&#65306;&#21033;&#29992;ChatGPT&#23454;&#29616;&#21333;&#36718;&#21040;&#22810;&#36718;&#21253;&#23481;&#24615;&#35821;&#35328;&#25193;&#23637;&#30340;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support. (arXiv:2305.00450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SMILE&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT&#23558;&#20844;&#20849;&#21333;&#36718;&#23545;&#35805;&#25193;&#23637;&#20026;&#22810;&#36718;&#23545;&#35805;&#65292;&#29983;&#25104;&#20102;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#25509;&#36817;&#30495;&#23454;&#29983;&#27963;&#30340;&#22810;&#36718;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#19987;&#38376;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19987;&#38376;&#30340;&#23545;&#35805;&#31995;&#32479;&#20197;&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#24050;&#25104;&#20026;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#28857;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20010;&#20154;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#33719;&#21462;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#22810;&#36718;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SMILE&#26041;&#27861;&#65292;&#19968;&#31181;&#20351;&#29992;ChatGPT&#23558;&#20844;&#20849;&#21333;&#36718;&#23545;&#35805;&#25193;&#23637;&#20026;&#22810;&#36718;&#23545;&#35805;&#30340;&#21253;&#23481;&#24615;&#35821;&#35328;&#25193;&#23637;&#25216;&#26415;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;SMILE&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#21644;&#26410;&#20351;&#29992;SMILE&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#31995;&#32479;&#30340;&#23545;&#27604;&#20998;&#26512;&#65292;&#35777;&#26126;SMILE&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#25509;&#36817;&#30495;&#23454;&#29983;&#27963;&#30340;&#22810;&#36718;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#23545;&#35805;&#20027;&#39064;&#12289;&#35789;&#27719;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25910;&#38598;&#30340;&#35821;&#26009;&#24211;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#19987;&#38376;&#30340;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an increasing research interest in developing specialized dialogue systems that can offer mental health support. However, gathering large-scale and real-life multi-turn conversations for mental health support poses challenges due to the sensitivity of personal information, as well as the time and cost involved. To address these issues, we introduce the SMILE approach, an inclusive language expansion technique that employs ChatGPT to extend public single-turn dialogues into multi-turn ones. Our research first presents a preliminary exploratory study that validates the effectiveness of the SMILE approach. Furthermore, we conduct a comprehensive and systematic contrastive analysis of datasets generated with and without the SMILE approach, demonstrating that the SMILE method results in a large-scale, diverse, and close-to-real-life multi-turn mental health support conversation corpus, including dialog topics, lexical and semantic features. Finally, we use the collected corpu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16133</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#35299;&#20915;&#32479;&#19968;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#20219;&#21153;&#19981;&#19968;&#33268;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#29992;&#30340;&#35270;&#35273;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#25928;&#65292;&#20445;&#35777;&#23427;&#20204;&#22312;&#21508;&#33258;&#25903;&#25345;&#30340;&#20219;&#21153;&#20013;&#30340;&#19968;&#33268;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#20154;&#20204;&#35748;&#20026;&#19981;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#36825;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#36755;&#20986;&#30340;&#22823;&#22411;&#31995;&#32479;&#26469;&#35828;&#26159;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#39044;&#27979;&#32467;&#26524;&#26159;&#21542;&#19968;&#33268;&#65292;&#22240;&#27492;&#65292;&#35780;&#20272;&#21487;&#33021;&#21253;&#25324;&#19981;&#21516;&#27169;&#24577;&#36755;&#20986;&#30340;&#38750;&#24120;&#24322;&#26500;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#27979;&#35797;&#23454;&#20363;&#36827;&#34892;&#23567;&#22411;&#20294;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#20462;&#25913;&#26469;&#21019;&#24314;&#23545;&#27604;&#38598;&#65292;&#20197;&#26356;&#25913;&#37329;&#26631;&#31614;&#65292;&#24182;&#27010;&#36848;&#20102;&#29992;&#20110;&#36890;&#36807;&#23545;&#27604;&#25509;&#36817;&#21407;&#22987;&#21644;&#20462;&#25913;&#21518;&#30340;&#23454;&#20363;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, COCOCON, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#30446;&#26631;&#35770;&#25454;&#22312;&#35875;&#35328;&#31435;&#22330;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;&#65292;&#35777;&#26126;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36807;&#20110;&#20381;&#36182;&#34920;&#38754;&#20449;&#21495;&#65292;&#30446;&#26631;&#19981;&#29420;&#31435;&#30340;&#20107;&#20214;&#33258;&#28982;&#39640;&#21457;&#29983;&#29575;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2303.12665</link><description>&lt;p&gt;
&#35780;&#20272;&#30446;&#26631;&#35770;&#25454;&#22312;&#35875;&#35328;&#31435;&#22330;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Role of Target Arguments in Rumour Stance Classification. (arXiv:2303.12665v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#30446;&#26631;&#35770;&#25454;&#22312;&#35875;&#35328;&#31435;&#22330;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;&#65292;&#35777;&#26126;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36807;&#20110;&#20381;&#36182;&#34920;&#38754;&#20449;&#21495;&#65292;&#30446;&#26631;&#19981;&#29420;&#31435;&#30340;&#20107;&#20214;&#33258;&#28982;&#39640;&#21457;&#29983;&#29575;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19968;&#32452;&#23545;&#35805;&#65292;&#31435;&#22330;&#20998;&#31867;&#26088;&#22312;&#30830;&#23450;&#31572;&#22797;&#23545;&#32473;&#23450;&#30446;&#26631;&#30340;&#24847;&#35265;&#65288;&#20363;&#22914;&#21516;&#24847;&#25110;&#19981;&#21516;&#24847;&#65289;&#12290;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#65292;&#31435;&#22330;&#30340;&#30446;&#26631;&#39044;&#35745;&#26159;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#26159;&#20351;&#20854;&#19982;&#24773;&#24863;&#20998;&#26512;&#19981;&#21516;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#20010;&#24573;&#30053;&#30446;&#26631;&#30340;&#27169;&#22411;&#20248;&#20110;&#30446;&#26631;&#24863;&#30693;&#27169;&#22411;&#65292;&#34920;&#26126;&#22312;&#39044;&#27979;&#31435;&#22330;&#26102;&#30446;&#26631;&#24182;&#19981;&#26377;&#29992;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#31435;&#22330;&#20998;&#31867;&#65288;RSC&#65289;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#28304;&#25512;&#25991;&#20013;&#38544;&#21547;&#30340;&#35875;&#35328;&#25925;&#20107;&#12290;&#25105;&#20204;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#25552;&#20986;&#20102;&#23545;&#25239;&#25915;&#20987;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#35780;&#20272;&#25968;&#25454;&#22312;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21253;&#25324;&#20351;&#29992;&#25972;&#20010;&#23545;&#35805;&#32447;&#31243;&#30340;&#26041;&#27861;&#22312;&#20869;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36807;&#20110;&#20381;&#36182;&#34920;&#38754;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#30446;&#26631;&#19981;&#29420;&#31435;&#30340;&#20107;&#20214;&#33258;&#28982;&#39640;&#21457;&#29983;&#29575;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Considering a conversation thread, stance classification aims to identify the opinion (e.g. agree or disagree) of replies towards a given target. The target of the stance is expected to be an essential component in this task, being one of the main factors that make it different from sentiment analysis. However, a recent study shows that a target-oblivious model outperforms target-aware models, suggesting that targets are not useful when predicting stance. This paper re-examines this phenomenon for rumour stance classification (RSC) on social media, where a target is a rumour story implied by the source tweet in the conversation. We propose adversarial attacks in the test data, aiming to assess the models robustness and evaluate the role of the data in the models performance. Results show that state-of-the-art models, including approaches that use the entire conversation thread, overly relying on superficial signals. Our hypothesis is that the naturally high occurrence of target-indepen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#26469;&#24110;&#21161;&#26816;&#27979;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#24182;&#22312;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00131</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65306;&#19968;&#20010;&#20197;&#22240;&#26524;&#20851;&#31995;&#20026;&#22522;&#30784;&#30340;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution. (arXiv:2210.00131v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#26469;&#24110;&#21161;&#26816;&#27979;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#24182;&#22312;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#24120;&#24120;&#23384;&#22312;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#38382;&#39064;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#26631;&#35760;&#39044;&#27979;&#65292;&#22312;&#25512;&#26029;&#26102;&#21487;&#33021;&#26377;&#22810;&#20010;&#21333;&#35789;&#31526;&#21512;&#29992;&#25143;&#20135;&#29983;&#33258;&#28982;&#35821;&#35328;&#30340;&#24847;&#22270;&#65292;&#28982;&#32780;&#22312;&#35757;&#32451;&#26102;&#21482;&#26377;&#19968;&#20010;&#21333;&#35789;&#33021;&#22815;&#26368;&#23567;&#21270;&#20219;&#21153;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#21512;&#29702;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#25551;&#36848;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#22312;&#29983;&#25104;&#34394;&#20551;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#20854;&#31616;&#27905;&#24615;&#65292;&#25105;&#20204;&#30340;&#22240;&#26524;&#27169;&#22411;&#30452;&#25509;&#25351;&#23548;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#20013;&#30340;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#19978;&#65292;&#20197;&#24110;&#21161; 1) &#26816;&#27979;&#25512;&#26029;&#26102;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#21033;&#29992;&#20102; 2&#65289;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#28085;&#30422;&#20102; A&#65289;&#19981;&#21516;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;BERT-base&#21040;GPT 3.5&#65292;B&#65289;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20174;&#36974;&#34109;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#21040;&#36825;&#20123;&#30446;&#26631;&#30340;&#28151;&#21512;&#65292;&#20197;&#21450;C&#65289;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20174;&#20165;&#39044;&#35757;&#32451;&#21040;&#22686;&#24378;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language modeling tasks are often underspecified: for a given token prediction, many words may satisfy the user's intent of producing natural language at inference time, however only one word would minimize the task's loss function at training time. We provide a simple yet plausible causal mechanism describing the role underspecification plays in the generation of spurious correlations. Despite its simplicity, our causal model directly informs the development of two lightweight black-box evaluation methods, that we apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aid in the detection of inference-time task underspecification by exploiting 2) previously unreported gender vs. time and gender vs. location spurious correlations on LLMs with a range of A) sizes: from BERT-base to GPT 3.5, B) pre-training objectives: from masked &amp; autoregressive language modeling to a mixture of these objectives, and C) training stages: from pre-training only to reinforcement l
&lt;/p&gt;</description></item></channel></rss>