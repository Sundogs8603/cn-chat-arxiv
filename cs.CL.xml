<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GenAI&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#29256;&#26435;&#27861;&#24459;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#35299;&#20915;&#29256;&#26435;&#20405;&#26435;&#32416;&#32439;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;</title><link>https://arxiv.org/abs/2403.17691</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#30456;&#20284;&#20043;&#22788;&#37117;&#26159;&#19968;&#26679;&#30340;&#65306;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#20559;&#35265;&#26469;&#25351;&#23548;GenAI&#29256;&#26435;&#32416;&#32439;
&lt;/p&gt;
&lt;p&gt;
Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GenAI&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#29256;&#26435;&#27861;&#24459;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#35299;&#20915;&#29256;&#26435;&#20405;&#26435;&#32416;&#32439;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17691v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#23398;&#31185; &#25277;&#35937;: &#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#21253;&#25324; GitHub Copilot&#12289;OpenAI GPT &#21644; Stable Diffusion&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#20869;&#23481;&#21019;&#20316;&#65292;&#20351;&#38750;&#19987;&#19994;&#20154;&#22763;&#33021;&#22815;&#22312;&#21508;&#20010;&#39046;&#22495;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#38761;&#21629;&#24615;&#25216;&#26415;&#23548;&#33268;&#21512;&#25104;&#20869;&#23481;&#28608;&#22686;&#65292;&#24341;&#21457;&#29256;&#26435;&#20405;&#26435;&#30340;&#27861;&#24459;&#32416;&#32439;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GenAI&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#29256;&#26435;&#27861;&#24459;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#28436;&#31034;&#20102;GPT2&#21644;Stable Diffusion&#27169;&#22411;&#12290;&#29256;&#26435;&#27861;&#21306;&#20998;&#21407;&#22987;&#34920;&#36798;&#21644;&#36890;&#29992;&#34920;&#36798;&#65288;Sc\`enes \`a faire&#65289;&#65292;&#20445;&#25252;&#21069;&#32773;&#24182;&#20801;&#35768;&#21518;&#32773;&#30340;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#21306;&#21035;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#24456;&#38590;&#19968;&#33268;&#22320;&#20570;&#20986;&#65292;&#23548;&#33268;&#29256;&#26435;&#20316;&#21697;&#34987;&#36807;&#24230;&#20445;&#25252;&#12290;GenAI&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#22686;&#24378;&#36825;&#31181;&#27861;&#24459;&#20998;&#26512;&#65292;&#25581;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17691v1 Announce Type: cross  Abstract: The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains. This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement. To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models. Copyright law distinguishes between original expressions and generic ones (Sc\`enes \`a faire), protecting the former and permitting reproduction of the latter. However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works. GenAI offers an unprecedented opportunity to enhance this legal analysis by reveal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#26415;&#30005;&#36335;&#32422;&#26463;&#32534;&#30721;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#35745;&#31639;&#22312;&#26377;&#38480;&#22495;&#19978;&#35299;&#20915;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#20197;&#31934;&#30830;&#23450;&#20301;ZKP&#30005;&#36335;&#20013;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.15676</link><description>&lt;p&gt;
AC4&#65306;&#29992;&#20110;ZKP&#20013;&#30005;&#36335;&#32422;&#26463;&#30340;&#20195;&#25968;&#35745;&#31639;&#26816;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#26415;&#30005;&#36335;&#32422;&#26463;&#32534;&#30721;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#35745;&#31639;&#22312;&#26377;&#38480;&#22495;&#19978;&#35299;&#20915;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#20197;&#31934;&#30830;&#23450;&#20301;ZKP&#30005;&#36335;&#20013;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ZKP&#31995;&#32479;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22312;&#24403;&#20195;&#23494;&#30721;&#23398;&#20013;&#21457;&#25381;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290; Zk-SNARK&#21327;&#35758;&#20027;&#23548;&#20102;ZKP&#30340;&#20351;&#29992;&#65292;&#36890;&#24120;&#36890;&#36807;&#31639;&#26415;&#30005;&#36335;&#32534;&#31243;&#33539;&#24335;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#27424;&#32422;&#26463;&#25110;&#36807;&#32422;&#26463;&#30340;&#30005;&#36335;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#12290; &#27424;&#32422;&#26463;&#30340;&#30005;&#36335;&#25351;&#30340;&#26159;&#32570;&#20047;&#24517;&#35201;&#32422;&#26463;&#30340;&#30005;&#36335;&#65292;&#23548;&#33268;&#30005;&#36335;&#20013;&#20986;&#29616;&#24847;&#22806;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23548;&#33268;&#39564;&#35777;&#32773;&#25509;&#21463;&#38169;&#35823;&#35265;&#35777;&#12290; &#36807;&#32422;&#26463;&#30340;&#30005;&#36335;&#26159;&#25351;&#32422;&#26463;&#36807;&#24230;&#30340;&#30005;&#36335;&#65292;&#23548;&#33268;&#30005;&#36335;&#32570;&#20047;&#24517;&#35201;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23548;&#33268;&#39564;&#35777;&#32773;&#25509;&#21463;&#27809;&#26377;&#35265;&#35777;&#65292;&#20351;&#30005;&#36335;&#27627;&#26080;&#24847;&#20041;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25214;&#20986;ZKP&#30005;&#36335;&#20013;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290; &#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#31639;&#26415;&#30005;&#36335;&#32422;&#26463;&#32534;&#30721;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#35745;&#31639;&#22312;&#26377;&#38480;&#22495;&#19978;&#35299;&#20915;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15676v1 Announce Type: cross  Abstract: ZKP systems have surged attention and held a fundamental role in contemporary cryptography. Zk-SNARK protocols dominate the ZKP usage, often implemented through arithmetic circuit programming paradigm. However, underconstrained or overconstrained circuits may lead to bugs. Underconstrained circuits refer to circuits that lack the necessary constraints, resulting in unexpected solutions in the circuit and causing the verifier to accept a bogus witness. Overconstrained circuits refer to circuits that are constrained excessively, resulting in the circuit lacking necessary solutions and causing the verifier to accept no witness, rendering the circuit meaningless. This paper introduces a novel approach for pinpointing two distinct types of bugs in ZKP circuits. The method involves encoding the arithmetic circuit constraints to polynomial equation systems and solving polynomial equation systems over a finite field by algebraic computation. T
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15246</link><description>&lt;p&gt;
FollowIR: &#35780;&#20272;&#21644;&#25945;&#25480;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#20197;&#36981;&#24490;&#35828;&#26126;&#20070;
&lt;/p&gt;
&lt;p&gt;
FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36981;&#24490;&#38271;&#19988;&#22797;&#26434;&#30340;&#35828;&#26126;&#20070;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#20351;&#29992;LLMs&#20316;&#20026;&#20854;&#26550;&#26500;&#30340;&#25903;&#26609;&#65292;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#21482;&#25509;&#21463;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#27809;&#26377;&#35828;&#26126;&#20070;&#12290;&#23545;&#20110;&#26368;&#36817;&#19968;&#20123;&#25509;&#21463;&#35828;&#26126;&#20070;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#23427;&#20204;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#35828;&#26126;&#20070;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;IR&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#22320;&#36981;&#24490;&#29616;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;FollowIR&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#24736;&#20037;&#21382;&#21490;&#65306;&#27491;&#22914;TREC&#20026;&#20154;&#31867;&#26631;&#27880;&#21592;&#25552;&#20379;&#35828;&#26126;&#20070;&#65288;&#20063;&#31216;&#20026;&#21465;&#36848;&#65289;&#26469;&#21028;&#26029;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#19968;&#26679;&#65292;&#22240;&#27492;IR&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#36825;&#20123;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#30830;&#23450;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20934;&#20174;&#19977;&#20010;&#32463;&#36807;&#28145;&#24230;&#21028;&#26029;&#30340;TREC&#25910;&#34255;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15246v1 Announce Type: cross  Abstract: Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and al
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRIP&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#25972;&#21512;&#29992;&#25143;&#24863;&#30693;&#30340;&#25112;&#30053;&#35268;&#21010;&#27169;&#22359;&#21644;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#38750;&#21512;&#20316;&#23545;&#35805;&#20195;&#29702;&#21830;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#28385;&#36275;&#22810;&#26679;&#21270;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.06769</link><description>&lt;p&gt;
&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#21147;&#37327;&#65281;&#36890;&#36807;&#23450;&#21046;&#31574;&#30053;&#35268;&#21010;&#23454;&#29616;&#26377;&#25928;&#30340;&#38750;&#21512;&#20316;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRIP&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#25972;&#21512;&#29992;&#25143;&#24863;&#30693;&#30340;&#25112;&#30053;&#35268;&#21010;&#27169;&#22359;&#21644;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#38750;&#21512;&#20316;&#23545;&#35805;&#20195;&#29702;&#21830;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#28385;&#36275;&#22810;&#26679;&#21270;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#21512;&#20316;&#23545;&#35805;&#20195;&#29702;&#21830;&#65292;&#20182;&#20204;&#24517;&#39035;&#36827;&#34892;&#23450;&#21046;&#30340;&#25112;&#30053;&#35268;&#21010;&#65292;&#20197;&#30830;&#20445;&#19982;&#22810;&#26679;&#21270;&#29992;&#25143;&#36798;&#25104;&#26377;&#21033;&#30340;&#21327;&#35758;&#12290;&#36825;&#23545;&#29616;&#26377;&#30340;&#23545;&#35805;&#20195;&#29702;&#21830;&#26500;&#25104;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26377;&#20004;&#28857;&#65306;&#20182;&#20204;&#26080;&#27861;&#23558;&#29992;&#25143;&#29305;&#23450;&#29305;&#24449;&#25972;&#21512;&#21040;&#25112;&#30053;&#35268;&#21010;&#20013;&#65292;&#20197;&#21450;&#20182;&#20204;&#30340;&#35757;&#32451;&#33539;&#24335;&#26410;&#33021;&#20135;&#29983;&#21487;&#20197;&#27867;&#21270;&#21040;&#22810;&#26679;&#21270;&#29992;&#25143;&#30340;&#25112;&#30053;&#35268;&#21010;&#32773;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TRIP&#20197;&#22686;&#24378;&#23450;&#21046;&#25112;&#30053;&#35268;&#21010;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#29992;&#25143;&#24863;&#30693;&#30340;&#25112;&#30053;&#35268;&#21010;&#27169;&#22359;&#21644;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#38750;&#21512;&#20316;&#23545;&#35805;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TRIP&#22312;&#36814;&#21512;&#22810;&#26679;&#21270;&#29992;&#25143;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06769v1 Announce Type: new  Abstract: We investigate non-collaborative dialogue agents that must engage in tailored strategic planning for diverse users to secure a favorable agreement. This poses challenges for existing dialogue agents due to two main reasons: their inability to integrate user-specific characteristics into their strategic planning and their training paradigm's failure to produce strategic planners that can generalize to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06064</link><description>&lt;p&gt;
L$^2$GC: &#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#29992;&#20110;&#23545;&#22270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32447;&#24615;GCN&#27169;&#22411;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#65292;&#36825;&#24182;&#27809;&#26377;&#26126;&#30830;&#25429;&#25417;&#21040;&#20316;&#20026;&#22270;&#27169;&#22411;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#31867;&#20284;&#26641;&#29366;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#23581;&#35797;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;GCN&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22270;&#33410;&#28857;&#30340;&#23398;&#20064;&#29305;&#24449;&#26144;&#23556;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27931;&#20262;&#20857;&#32447;&#24615;&#29305;&#24449;&#21464;&#25442;&#65292;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#28508;&#22312;&#26641;&#29366;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#24341;&#25991;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Citeseer&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;74.7%&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;PubMed&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;81.3%&#30340;&#20934;&#30830;&#24230;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#33267;&#23569;&#36798;&#21040;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06064v1 Announce Type: cross  Abstract: Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and 81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#28151;&#21512;&#30721;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#26816;&#27979;&#12289;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#21644;&#34920;&#36798;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;</title><link>https://arxiv.org/abs/2403.04872</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#30721;&#35843;&#26597;&#34920;&#26126;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#28151;&#21512;&#30721;&#25991;&#26412;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04872
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#28151;&#21512;&#30721;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#26816;&#27979;&#12289;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#21644;&#34920;&#36798;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#30721;&#26159;&#19968;&#31181;&#26222;&#36941;&#30340;&#35821;&#35328;&#29616;&#35937;&#65292;&#22810;&#35821;&#31181;&#20010;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#35821;&#35328;&#20043;&#38388;&#20132;&#26367;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#28151;&#21512;&#30721;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#27169;&#22411;&#26816;&#27979;&#28151;&#21512;&#30721;&#25991;&#26412;&#30340;&#33021;&#21147;&#12289;&#27169;&#22411;&#21033;&#29992;&#30340;&#32467;&#26500;&#20449;&#24687;&#21464;&#21270;&#20197;&#25429;&#25417;&#28151;&#21512;&#30721;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#28151;&#21512;&#30721;&#25991;&#26412;&#20013;&#35821;&#20041;&#20449;&#24687;&#34920;&#36798;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#33258;&#28982;&#24418;&#24577;&#30340;&#28151;&#21512;&#30721;&#25991;&#26412;&#19982;&#28304;&#35821;&#35328;&#30340;&#24179;&#34892;&#32763;&#35793;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#21270;&#21644;&#25511;&#21046;&#24615;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#28151;&#21512;&#30721;&#25991;&#26412;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04872v1 Announce Type: new  Abstract: Code-switching is a prevalent linguistic phenomenon in which multilingual individuals seamlessly alternate between languages. Despite its widespread use online and recent research trends in this area, research in code-switching presents unique challenges, primarily stemming from the scarcity of labelled data and available resources. In this study we investigate how pre-trained Language Models handle code-switched text in three dimensions: a) the ability of PLMs to detect code-switched text, b) variations in the structural information that PLMs utilise to capture code-switched text, and c) the consistency of semantic information representation in code-switched text. To conduct a systematic and controlled evaluation of the language models in question, we create a novel dataset of well-formed naturalistic code-switched text along with parallel translations into the source languages. Our findings reveal that pre-trained language models are e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.03506</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#28041;&#21450;&#24102;&#26377;&#26377;&#38480;&#36793;&#30028;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30740;&#31350;&#24212;&#35206;&#30422;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#19981;&#21516;&#31867;&#22411;&#28151;&#21512;&#25991;&#26412;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;CoAuthor&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36890;&#36807;&#20154;&#31867;&#20316;&#32773;&#21644;&#26234;&#33021;&#20889;&#20316;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#29983;&#25104;&#30340;&#22810;&#36718;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#22810;&#26679;&#21270;&#12289;&#30495;&#23454;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#27493;&#20998;&#21106;&#20026;&#22522;&#30784;&#30340;&#27969;&#31243;&#65306;(i)&#26816;&#27979;&#32473;&#23450;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#21508;&#20010;&#27573;&#33853;&#65292;&#20854;&#20013;&#27599;&#20010;&#27573;&#33853;&#21253;&#21547;&#19968;&#33268;&#20316;&#32773;&#30340;&#21477;&#23376;&#65292;&#20197;&#21450;(ii)&#20998;&#31867;&#27599;&#20010;&#30830;&#23450;&#27573;&#33853;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03506v1 Announce Type: cross  Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#30740;&#31350;&#20102;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20102;&#32972;&#26223;&#27010;&#36848;&#12289;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#12289;&#20197;&#21450;&#26410;&#26469;&#23637;&#26395;</title><link>https://arxiv.org/abs/2402.18267</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#30340;&#35843;&#26597;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Neural Question Generation: Methods, Applications, and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#30740;&#31350;&#20102;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20102;&#32972;&#26223;&#27010;&#36848;&#12289;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#12289;&#20197;&#21450;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#26816;&#26597;&#65292;&#36825;&#19968;&#39046;&#22495;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#20174;&#21508;&#31181;&#26469;&#28304;&#65292;&#22914;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#29983;&#25104;&#30456;&#20851;&#38382;&#39064;&#12290;&#35843;&#26597;&#20174;NQG&#32972;&#26223;&#27010;&#36848;&#24320;&#22987;&#65292;&#21253;&#25324;&#20219;&#21153;&#30340;&#38382;&#39064;&#21046;&#23450;&#12289;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#24050;&#24314;&#31435;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#26174;&#33879;&#24212;&#29992;&#12290;&#28982;&#21518;&#65292;&#31995;&#32479;&#22320;&#23558;NQG&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#32467;&#26500;&#21270;NQG&#65292;&#21033;&#29992;&#26377;&#32452;&#32455;&#30340;&#25968;&#25454;&#28304;&#65292;&#38750;&#32467;&#26500;&#21270;NQG&#65292;&#19987;&#27880;&#20110;&#26356;&#26494;&#25955;&#32467;&#26500;&#30340;&#36755;&#20837;&#65292;&#22914;&#25991;&#26412;&#25110;&#35270;&#35273;&#20869;&#23481;&#65292;&#20197;&#21450;&#28151;&#21512;NQG&#65292;&#21033;&#29992;&#22810;&#26679;&#30340;&#36755;&#20837;&#27169;&#24335;&#12290;&#36825;&#19968;&#20998;&#31867;&#21518;&#26159;&#23545;&#20026;&#27599;&#20010;&#31867;&#21035;&#37327;&#36523;&#23450;&#21046;&#30340;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#35752;&#35770;&#23427;&#20204;&#22266;&#26377;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#23616;&#38480;&#24615;&#12290;&#35843;&#26597;&#20197;&#23637;&#26395;&#26410;&#26469;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18267v1 Announce Type: cross  Abstract: In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking persp
&lt;/p&gt;</description></item><item><title>TrustScore &#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#19968;&#33268;&#24615;&#27010;&#24565;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21709;&#24212;&#26159;&#21542;&#19982;&#20854;&#20869;&#22312;&#30693;&#35782;&#30456;&#19968;&#33268;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#30340;&#21487;&#20449;&#24230;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.12545</link><description>&lt;p&gt;
TrustScore: &#26080;&#21442;&#32771;&#35780;&#20272;LLM&#21709;&#24212;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12545
&lt;/p&gt;
&lt;p&gt;
TrustScore &#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#19968;&#33268;&#24615;&#27010;&#24565;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21709;&#24212;&#26159;&#21542;&#19982;&#20854;&#20869;&#22312;&#30693;&#35782;&#30456;&#19968;&#33268;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#30340;&#21487;&#20449;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;LLMs&#36755;&#20986;&#30340;&#21487;&#20449;&#24230;&#20135;&#29983;&#20102;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#23553;&#38381;&#20070;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#65292;&#38750;&#19987;&#23478;&#21487;&#33021;&#22240;&#32570;&#23569;&#19978;&#19979;&#25991;&#25110;&#22522;&#20934;&#20449;&#24687;&#32780;&#38590;&#20197;&#35782;&#21035;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TrustScore&#65292;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#19968;&#33268;&#24615;&#27010;&#24565;&#30340;&#26694;&#26550;&#65292;&#35780;&#20272;LLMs&#30340;&#21709;&#24212;&#26159;&#21542;&#19982;&#20854;&#20869;&#22312;&#30693;&#35782;&#30456;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;TrustScore&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#38598;&#25104;&#65292;&#35780;&#20272;&#19982;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#30340;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;TrustScore&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#24378;&#22823;&#30340;&#30456;&#20851;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26080;&#21442;&#32771;&#25351;&#26631;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#22522;&#20934;&#25351;&#26631;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12545v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, prompting a surge in their practical applications. However, concerns have arisen regarding the trustworthiness of LLMs outputs, particularly in closed-book question-answering tasks, where non-experts may struggle to identify inaccuracies due to the absence of contextual or ground truth information. This paper introduces TrustScore, a framework based on the concept of Behavioral Consistency, which evaluates whether an LLMs response aligns with its intrinsic knowledge. Additionally, TrustScore can seamlessly integrate with fact-checking methods, which assesses alignment with external knowledge sources. The experimental results show that TrustScore achieves strong correlations with human judgments, surpassing existing reference-free metrics, and achieving results on par with reference-based metrics.
&lt;/p&gt;</description></item><item><title>&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11355</link><description>&lt;p&gt;
&#25913;&#21464;&#20102;&#20160;&#20040;&#65311;&#23558;&#34920;&#24449;&#24178;&#39044;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
What Changed? Converting Representational Interventions to Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11355
&lt;/p&gt;
&lt;p&gt;
&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#24449;&#31354;&#38388;&#30340;&#24178;&#39044;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#29992;&#26469;&#28040;&#38500;&#25110;&#25913;&#21464;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#32534;&#30721;&#65292;&#21019;&#24314;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24178;&#39044;&#25805;&#20316;&#22312;&#34920;&#31034;&#31354;&#38388;&#20869;&#65292;&#20934;&#30830;&#29702;&#35299;&#23427;&#20462;&#25913;&#20102;&#21738;&#20123;&#29305;&#24449;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23545;&#24212;&#20110;&#32473;&#23450;&#34920;&#31034;&#31354;&#38388;&#24178;&#39044;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#35299;&#37322;&#29992;&#20110;&#32534;&#30721;&#29305;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#29992;&#20110;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06025</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#27010;&#29575;&#25512;&#29702;&#36827;&#34892;&#23454;&#39564;&#19982;&#20462;&#35746;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;&#35813;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#65292;&#21363;&#20351;&#35268;&#21017;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#23398;&#20064;&#32773;&#20063;&#20250;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#27169;&#31946;&#27010;&#29575;&#35268;&#21017;&#65292;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#26681;&#25454;&#36817;&#20284;&#36125;&#21494;&#26031;&#21407;&#21017;&#22312;&#27599;&#27425;&#23454;&#39564;&#21518;&#22312;&#32447;&#26356;&#26032;&#33258;&#24049;&#30340;&#20551;&#35774;&#12290;&#22312;&#21516;&#19968;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#26681;&#25454;&#20449;&#24687;&#35770;&#20934;&#21017;&#24314;&#31435;&#20102;&#23454;&#39564;&#35774;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#21407;&#21017;&#30340;&#32452;&#21512;&#8212;&#8212;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#8212;&#8212;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#21435;&#25481;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#32452;&#20214;&#37117;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01801</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Time Series: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;LLM&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#65292;&#36824;&#20855;&#26377;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#27668;&#20505;&#12289;&#29289;&#32852;&#32593;&#12289;&#21307;&#30103;&#12289;&#20132;&#36890;&#12289;&#38899;&#39057;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#21463;&#30410;&#12290;&#26412;&#35843;&#30740;&#35770;&#25991;&#23545;&#21033;&#29992;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#21644;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;LLM&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;LLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#25552;&#21462;&#21040;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#25552;&#31034;LLM&#65292;&#65288;2&#65289;&#26102;&#38388;&#24207;&#21015;&#37327;&#21270;&#65292;&#65288;3&#65289;&#23545;&#40784;&#25216;&#26415;&#65292;&#65288;4&#65289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#20316;&#20026;&#26725;&#25509;&#26426;&#21046;&#65292;&#21644;&#65288;5&#65289;&#32467;&#21512;LLM&#19982;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28041;&#21450;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.01705</link><description>&lt;p&gt;
&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#34920;&#24449;&#20260;&#23475;&#65306;&#24230;&#37327;&#21644;&#20943;&#36731;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#21644;&#20943;&#36731;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#36825;&#20123;&#20260;&#23475;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#25514;&#26045;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20260;&#23475;&#36890;&#24120;&#34987;&#20998;&#20026;&#37197;&#32622;&#24615;&#25110;&#34920;&#24449;&#24615;&#12290;&#26412;&#30740;&#31350;&#19987;&#38376;&#38024;&#23545;&#21518;&#32773;&#65292;&#37325;&#28857;&#22312;&#20110;&#23545;&#24403;&#21069;&#34920;&#24449;&#24615;&#20260;&#23475;&#23450;&#20041;&#30340;&#23457;&#26597;&#65292;&#20197;&#30830;&#23450;&#20854;&#20013;&#21253;&#21547;&#20160;&#20040;&#21644;&#19981;&#21253;&#21547;&#20160;&#20040;&#12290;&#36825;&#20010;&#20998;&#26512;&#20419;&#20351;&#25105;&#20204;&#25193;&#23637;&#36229;&#36234;&#34892;&#20026;&#20027;&#20041;&#30340;&#23450;&#20041;&#33539;&#22260;&#65292;&#21253;&#25324;&#23545;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#30340;&#20260;&#23475;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24230;&#37327;&#30340;&#39640;&#32423;&#35201;&#27714;&#65306;&#30830;&#23450;&#23454;&#26045;&#36825;&#31181;&#26041;&#27861;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#35828;&#26126;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26045;&#34920;&#24449;&#24615;&#20260;&#23475;&#26102;&#30340;&#29420;&#29305;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#20260;&#23475;&#26410;&#34987;&#24230;&#37327;&#21644;&#20943;&#36731;&#26102;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#20943;&#36731;&#25514;&#26045;&#24182;&#30028;&#23450;&#20309;&#26102;&#20351;&#29992;&#23427;&#20204;&#26469;&#32467;&#26463;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#26694;&#26550;&#65292;&#25193;&#22823;&#34920;&#24449;&#24615;&#20260;&#23475;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#20844;&#24179;&#30740;&#31350;&#30340;&#35265;&#35299;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06692</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25351;&#23548;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22312;&#23454;&#29616;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#38646;&#23556;&#20987;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20026;&#25351;&#20196;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22238;&#31572;&#25152;&#38656;&#30340;&#27880;&#37322;&#24037;&#20316;&#27491;&#22312;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#25351;&#20196;&#25968;&#25454;&#38598;&#25152;&#28085;&#30422;&#30340;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#27744;&#20013;&#30830;&#23450;&#26377;&#29992;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#20854;&#22312;LLMs&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#23569;SFT&#30340;&#27880;&#37322;&#25104;&#26412;&#24182;&#35268;&#36991;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#12290;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#36890;&#24120;&#26368;&#22823;&#21270;&#26576;&#31181;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#31181;&#29616;&#26377;&#21644;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2311.00208</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#22120;&#65306;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#23427;&#20204;&#33021;&#21542;&#35299;&#20915;&#38382;&#39064;&#65292;&#23558;&#38382;&#39064;&#35270;&#20026;&#24418;&#24335;&#35821;&#35328;&#12290;&#25506;&#32034;&#36825;&#31867;&#38382;&#39064;&#23558;&#26377;&#21161;&#20110;&#27604;&#36739;transformers&#19982;&#20854;&#20182;&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#21464;&#31181;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#36825;&#20010;&#23376;&#39046;&#22495;&#30340;&#24037;&#20316;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35760;&#24405;&#20102;&#19981;&#21516;&#32467;&#26524;&#32972;&#21518;&#30340;&#21508;&#31181;&#20551;&#35774;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#21327;&#35843;&#30475;&#20284;&#30456;&#20114;&#30683;&#30462;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring questions such as this will help to compare transformers with other models, and transformer variants with one another, for various tasks. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ZzzGPT&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21644;&#21453;&#39304;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29992;&#25143;&#23548;&#21521;&#35774;&#35745;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16242</link><description>&lt;p&gt;
ZzzGPT: &#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality. (arXiv:2310.16242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ZzzGPT&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21644;&#21453;&#39304;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29992;&#25143;&#23548;&#21521;&#35774;&#35745;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#19990;&#30028;&#20013;&#65292;&#30561;&#30496;&#36136;&#37327;&#23545;&#24635;&#20307;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25552;&#20379;&#23454;&#26102;&#30417;&#27979;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#35265;&#35299;&#65292;&#23548;&#33268;&#29992;&#25143;&#25918;&#24323;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25216;&#26415;&#22312;&#29702;&#35299;&#30561;&#30496;&#27169;&#24335;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#26088;&#22312;&#25552;&#20379;&#20934;&#30830;&#30340;&#30561;&#30496;&#39044;&#27979;&#21644;&#26377;&#20215;&#20540;&#30340;&#21453;&#39304;&#12290;&#21033;&#29992;GLOBEM&#25968;&#25454;&#38598;&#21644;LLMs&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;XGBoost&#31561;&#27169;&#22411;&#30456;&#27604;&#30340;&#22686;&#24378;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#19982;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#30456;&#32467;&#21512;&#65292;&#23558;&#31185;&#23398;&#20934;&#30830;&#24615;&#19982;&#23454;&#29992;&#24615;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's world, sleep quality is pivotal for overall well-being. While wearable sensors offer real-time monitoring, they often lack actionable insights, leading to user abandonment. This paper delves into the role of technology in understanding sleep patterns. We introduce a two-stage framework, utilizing Large Language Models (LLMs), aiming to provide accurate sleep predictions with actionable feedback. Leveraging the GLOBEM dataset and synthetic data from LLMs, we highlight enhanced results with models like XGBoost. Our approach merges advanced machine learning with user-centric design, blending scientific accuracy with practicality.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13023</link><description>&lt;p&gt;
GraphGPT: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
GraphGPT: Graph Instruction Tuning for Large Language Models. (arXiv:2310.13023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36882;&#24402;&#20449;&#24687;&#20132;&#25442;&#21644;&#32858;&#21512;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35299;&#22270;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#22270;&#23884;&#20837;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#26631;&#31614;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#25552;&#21319;&#22270;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;LLM&#65292;&#21363;&#20351;&#27809;&#26377;&#26469;&#33258;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#20219;&#20309;&#20449;&#24687;&#65292;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23558;LLM&#19982;&#22270;&#32467;&#26500;&#30693;&#35782;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05950</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;VLMs &#30340;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#25805;&#20316;&#65292;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; VLMs &#20381;&#36182;&#20110;&#19987;&#26377;&#25968;&#25454;&#19988;&#19981;&#24320;&#28304;&#65292;&#38480;&#21046;&#20102;&#20351;&#29992;&#30333;&#30418;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#37492;&#20110;&#20687; ChatGPT &#36825;&#26679;&#30340;&#21463;&#27426;&#36814;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20173;&#28982;&#25552;&#20379;&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340; VLMs &#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#29305;&#24449;&#23884;&#20837;&#25110;&#36755;&#20986; logits &#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLMs &#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#20197;&#22312;&#20351;&#29992; CLIP &#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#20219;&#21153;&#20013;&#23547;&#25214;&#26368;&#20339;&#25991;&#26412;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;"&#29228;&#23665;"&#31243;&#24207;&#65292;&#23427;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#30340;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13551</link><description>&lt;p&gt;
EntRED: &#29992;&#26356;&#23569;&#30340;&#25463;&#24452;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EntRED: Benchmarking Relation Extraction with Fewer Shortcuts. (arXiv:2305.13551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21517;&#31216;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#36215;&#30528;&#26377;&#25928;&#30340;&#20316;&#29992;&#65292;&#24182;&#24120;&#24120;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#38598;&#20013;&#30340;&#23454;&#20307;&#21517;&#31216;&#26174;&#33879;&#24433;&#21709;&#20102;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26631;&#20934;&#30340;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#22823;&#37327;&#38169;&#35823;&#30340;&#23454;&#20307;&#27880;&#37322;&#65292;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19982;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EntRED&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36739;&#23569;&#25463;&#24452;&#21644;&#26356;&#39640;&#23454;&#20307;&#22810;&#26679;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#26500;&#24314;EntRED&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#65288;CI&#65289;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26367;&#25442;&#31649;&#36947;&#65306;ERIC&#12290;ERIC&#23545;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#32422;&#26463;&#26367;&#25442;&#65292;&#20197;&#20943;&#23569;&#20174;&#23454;&#20307;&#20559;&#24046;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;ERIC&#22312;&#20004;&#20010;&#26041;&#38754;&#24212;&#29992;CI&#65306;1&#65289;&#38024;&#23545;&#38656;&#35201;&#23454;&#20307;&#26367;&#25442;&#30340;&#23454;&#20363;&#65292;2&#65289;&#30830;&#23450;&#20505;&#36873;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity names play an effective role in relation extraction (RE) and often influence model performance. As a result, the entity names in the benchmarks' test sets significantly influence the evaluation of RE models. In this work, we find that the standard RE benchmarks' datasets have a large portion of incorrect entity annotations, low entity name diversity, and are prone to have shortcuts from entity names to ground-truth relations. These issues make the standard benchmarks far from reflecting the real-world scenarios. Hence, in this work, we present EntRED, a challenging RE benchmark with reduced shortcuts and higher diversity of entities. To build EntRED, we propose an end-to-end entity replacement pipeline based on causal inference (CI): ERIC. ERIC performs type-constrained replacements on entities to reduce the shortcuts from entity bias to ground-truth relations. ERIC applies CI in two aspects: 1) targeting the instances that need entity replacements, and 2) determining the candid
&lt;/p&gt;</description></item></channel></rss>