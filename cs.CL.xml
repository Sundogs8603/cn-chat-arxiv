<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01118</link><description>&lt;p&gt;
Pok\'eLLMon&#65306;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Pok\'emon&#23545;&#25112;&#30340;&#19982;&#20154;&#31867;&#33021;&#21147;&#30456;&#24403;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01118
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;\textsc{Pok\'eLLMon}&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#20197;Pok\'emon&#23545;&#25112;&#20026;&#20363;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290; \textsc{Pok\'eLLMon}&#30340;&#35774;&#35745;&#37319;&#29992;&#20102;&#19977;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#65288;i&#65289;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#21363;&#26102;&#20351;&#29992;&#20174;&#23545;&#25112;&#20013;&#33719;&#24471;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21453;&#39304;&#26469;&#36880;&#27493;&#23436;&#21892;&#31574;&#30053;&#65307;&#65288;ii&#65289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#65292;&#21363;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#20197;&#23545;&#25239;&#20135;&#29983;&#24187;&#35273;&#29616;&#35937;&#65292;&#24182;&#20351;&#20195;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#21450;&#26102;&#27491;&#30830;&#22320;&#34892;&#21160;&#65307;&#65288;iii&#65289;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#65292;&#20197;&#20943;&#36731;&#20195;&#29702;&#26426;&#22120;&#20154;&#38754;&#23545;&#24378;&#25932;&#26102;&#30340;&#8220;&#24778;&#24908;&#25442;&#25163;&#8221;&#29616;&#35937;&#65292;&#20351;&#20854;&#21487;&#20197;&#36867;&#36991;&#25112;&#26007;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#36827;&#34892;&#30340;&#22312;&#32447;&#23545;&#25112;&#20013;&#65292;\textsc{Pok\'eLLMon}&#37319;&#29992;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#20854;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#21487;&#29609;&#30340;&#25112;&#26007;&#26085;&#24535;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#25214;&#21040;&#65306;\url{https://gith
&lt;/p&gt;
&lt;p&gt;
We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://gith
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2404.02138</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Topic-based Watermarks for LLM-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#36755;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#38590;&#20197;&#20998;&#36776;&#12290;&#27700;&#21360;&#31639;&#27861;&#26159;&#28508;&#22312;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#23884;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#21487;&#20197;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#22312;&#24050;&#30693;&#25915;&#20987;&#19979;&#32570;&#20047;&#20581;&#22766;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;LLM&#27599;&#22825;&#29983;&#25104;&#25968;&#19975;&#20010;&#25991;&#26412;&#36755;&#20986;&#65292;&#27700;&#21360;&#31639;&#27861;&#38656;&#35201;&#35760;&#24518;&#27599;&#20010;&#36755;&#20986;&#25165;&#33021;&#35753;&#26816;&#27979;&#27491;&#24120;&#24037;&#20316;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;&#8220;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#8221;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2404.02127</link><description>&lt;p&gt;
FLawN-T5: &#26377;&#25928;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#28151;&#21512;&#22312;&#27861;&#24459;&#25512;&#29702;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02127v1  &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#25351;&#23548;&#35843;&#25972;&#26159;&#20351;&#35821;&#35328;&#27169;&#22411;&#23545;&#30452;&#25509;&#29992;&#25143;&#20132;&#20114;&#26377;&#25928;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27861;&#24459;&#20219;&#21153;&#20173;&#28982;&#36229;&#20986;&#20102;&#22823;&#22810;&#25968;&#24320;&#25918;&#24335;LLMs&#30340;&#33539;&#22260;&#65292;&#32780;&#19988;&#30446;&#21069;&#35813;&#39046;&#22495;&#36824;&#27809;&#26377;&#20219;&#20309;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#35813;&#24212;&#29992;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#12289;24&#31181;&#35821;&#35328;&#65292;&#24635;&#35745;1200&#19975;&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#21576;&#29616;&#35777;&#25454;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#33021;&#22815;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#23558;Flan-T5 XL&#22312;&#22522;&#20934;&#32447;&#19978;&#25552;&#39640;8&#20010;&#28857;&#25110;16%&#12290;&#28982;&#32780;&#65292;&#35813;&#25928;&#24212;&#24182;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#12289;&#35757;&#32451;&#27169;&#24335;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;LawInstruct&#26159;&#19968;&#20010;&#36164;&#28304;&#65292;&#21487;&#20197;&#21152;&#36895;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02127v1 Announce Type: cross  Abstract: Instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. In this work, we curate LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. We present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;AMR&#30456;&#20284;&#24230;&#24230;&#37327;rematch&#20197;&#25552;&#39640;&#32467;&#26500;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#32467;&#26500;&#30456;&#20284;&#24615;&#25490;&#21517;&#31532;&#20108;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#26368;&#20248;&#65292;&#24555;&#20116;&#20493;&#20110;&#19979;&#19968;&#20010;&#26368;&#39640;&#25928;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02126</link><description>&lt;p&gt;
&#37325;&#26032;&#27604;&#36187;&#65306;&#25913;&#36827;&#26412;&#22320;&#30693;&#35782;&#22270;&#30340;&#40065;&#26834;&#21644;&#39640;&#25928;&#21305;&#37197;&#20197;&#25552;&#39640;&#32467;&#26500;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02126
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;AMR&#30456;&#20284;&#24230;&#24230;&#37327;rematch&#20197;&#25552;&#39640;&#32467;&#26500;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#32467;&#26500;&#30456;&#20284;&#24615;&#25490;&#21517;&#31532;&#20108;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#26368;&#20248;&#65292;&#24555;&#20116;&#20493;&#20110;&#19979;&#19968;&#20010;&#26368;&#39640;&#25928;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20363;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#23558;&#25991;&#26412;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#12290;&#35780;&#20272;&#36825;&#20123;&#22270;&#30340;&#36136;&#37327;&#28041;&#21450;&#23558;&#23427;&#20204;&#32467;&#26500;&#21270;&#21305;&#37197;&#21040;&#24444;&#27492;&#21644;&#35821;&#20041;&#21305;&#37197;&#21040;&#28304;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;AMR&#24230;&#37327;&#25928;&#29575;&#20302;&#65292;&#38590;&#20197;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20063;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;AMR&#22270;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;AMR&#30456;&#20284;&#24230;&#24230;&#37327;rematch&#65292;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#26032;&#35780;&#20272;&#26041;&#27861;RARE&#12290;&#22312;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#20013;&#65292;rematch&#22312;&#32467;&#26500;&#30456;&#20284;&#24615;&#20013;&#25490;&#21517;&#31532;&#20108;&#65307;&#24182;&#19988;&#22312;STS-B&#21644;SICK-R&#22522;&#20934;&#19978;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26368;&#39640;&#65292;&#19982;&#19979;&#19968;&#20010;&#26368;&#39640;&#25928;&#24230;&#37327;&#30456;&#27604;&#24555;&#20116;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02126v1 Announce Type: new  Abstract: Knowledge graphs play a pivotal role in various applications, such as question-answering and fact-checking. Abstract Meaning Representation (AMR) represents text as knowledge graphs. Evaluating the quality of these graphs involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation benchmark for assessing structural similarity between AMR graphs. To overcome these limitations, we introduce a novel AMR similarity metric, rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics, rematch ranks second in structural similarity; and first in semantic similarity by 1--5 percentage points on the STS-B and SICK-R benchmarks. Rematch is also five times faster than the next most efficient metric.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;&#65292;&#21457;&#29616;&#34429;&#28982;LLMs&#21487;&#20197;&#29983;&#25104;&#19968;&#20123;&#25968;&#23398;&#19978;&#26377;&#25928;&#30340;&#24178;&#25200;&#39033;&#65292;&#20294;&#22312;&#39044;&#27979;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;</title><link>https://arxiv.org/abs/2404.02124</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;
&lt;/p&gt;
&lt;p&gt;
Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;&#65292;&#21457;&#29616;&#34429;&#28982;LLMs&#21487;&#20197;&#29983;&#25104;&#19968;&#20123;&#25968;&#23398;&#19978;&#26377;&#25928;&#30340;&#24178;&#25200;&#39033;&#65292;&#20294;&#22312;&#39044;&#27979;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#22312;&#20960;&#20046;&#25152;&#26377;&#25945;&#32946;&#23618;&#27425;&#20013;&#37117;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26131;&#20110;&#31649;&#29702;&#12289;&#35780;&#20998;&#65292;&#24182;&#19988;&#26159;&#35780;&#20272;&#21644;&#23454;&#36341;&#20013;&#21487;&#38752;&#30340;&#26684;&#24335;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#38754;&#20043;&#19968;&#26159;&#24178;&#25200;&#39033;&#65292;&#21363;&#38024;&#23545;&#30495;&#23454;&#23398;&#29983;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#32780;&#35774;&#35745;&#30340;&#19981;&#27491;&#30830;&#36873;&#39033;&#12290;&#30446;&#21069;&#65292;&#21046;&#20316;&#39640;&#36136;&#37327;&#24178;&#25200;&#39033;&#30340;&#20219;&#21153;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#28982;&#26159;&#25945;&#24072;&#21644;&#23398;&#20064;&#20869;&#23481;&#35774;&#35745;&#32773;&#30340;&#21171;&#21160;&#21644;&#32791;&#26102;&#24037;&#20316;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#39046;&#22495;&#20013;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;&#30340;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#21508;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#21040;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21457;&#29616;&#34429;&#28982;LLM&#21487;&#20197;&#29983;&#25104;&#19968;&#20123;&#25968;&#23398;&#19978;&#26377;&#25928;&#30340;&#24178;&#25200;&#39033;&#65292;&#20294;&#23427;&#20204;&#22312;&#39044;&#27979;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02124v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconcept
&lt;/p&gt;</description></item><item><title>GINopic&#26159;&#19968;&#31181;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02115</link><description>&lt;p&gt;
GINopic&#65306;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GINopic: Topic Modeling with Graph Isomorphism Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02115
&lt;/p&gt;
&lt;p&gt;
GINopic&#26159;&#19968;&#31181;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#20998;&#26512;&#21644;&#25506;&#32034;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#30340;&#24191;&#27867;&#20351;&#29992;&#26041;&#27861;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#23558;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#23884;&#20837;&#65292;&#32435;&#20837;&#20027;&#39064;&#24314;&#27169;&#20013;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#21333;&#35789;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#20256;&#36798;&#30340;&#22266;&#26377;&#20449;&#24687;&#20215;&#20540;&#12290; &#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GINopic&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290; &#36890;&#36807;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20869;&#22312;&#30340;&#65288;&#23450;&#37327;&#21644;&#23450;&#24615;&#65289;&#21644;&#22806;&#37096;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#30456;&#27604;&#65292;GINopic&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02115v1 Announce Type: new  Abstract: Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ClapNQ&#65292;&#19968;&#20010;&#29992;&#20110;&#23436;&#25972;RAG&#31649;&#36947;&#30340;&#22522;&#20934;&#38271;&#26684;&#24335;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#35201;&#27714;RAG&#27169;&#22411;&#33021;&#36866;&#24212;&#21253;&#25324;&#31616;&#27905;&#12289;&#19968;&#33268;&#21644;&#19981;&#36830;&#32493;&#27573;&#33853;&#29255;&#27573;&#30340;&#31572;&#26696;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02103</link><description>&lt;p&gt;
CLAPNQ&#65306;&#33258;&#28982;&#38382;&#31572;&#20013;&#30340;&#27573;&#33853;&#20869;&#19968;&#33268;&#38271;&#26684;&#24335;&#31572;&#26696;&#36866;&#29992;&#20110;RAG&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02103
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ClapNQ&#65292;&#19968;&#20010;&#29992;&#20110;&#23436;&#25972;RAG&#31649;&#36947;&#30340;&#22522;&#20934;&#38271;&#26684;&#24335;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#35201;&#27714;RAG&#27169;&#22411;&#33021;&#36866;&#24212;&#21253;&#25324;&#31616;&#27905;&#12289;&#19968;&#33268;&#21644;&#19981;&#36830;&#32493;&#27573;&#33853;&#29255;&#27573;&#30340;&#31572;&#26696;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28909;&#38376;&#24212;&#29992;&#12290;&#25104;&#21151;&#30340;RAG&#31995;&#32479;&#26368;&#22909;&#25552;&#20379;&#30001;&#27573;&#33853;&#25903;&#25345;&#19988;&#27809;&#26377;&#38169;&#35273;&#30340;&#20934;&#30830;&#31572;&#26696;&#12290;&#20026;&#20102;&#26500;&#24314;&#23436;&#25972;&#30340;RAG&#31649;&#36947;&#65292;&#38656;&#35201;&#24320;&#23637;&#22823;&#37327;&#24037;&#20316;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#33021;&#22815;&#23545;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ClapNQ&#65292;&#19968;&#20010;&#29992;&#20110;&#23436;&#25972;RAG&#31649;&#36947;&#30340;&#22522;&#20934;&#38271;&#26684;&#24335;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;ClapNQ&#21253;&#25324;&#20855;&#26377;&#33258;&#28982;&#38382;&#39064;&#65288;NQ&#65289;&#20013;&#22522;&#20110;&#27573;&#33853;&#30340;&#37329;&#26631;&#27573;&#33853;&#30340;&#38271;&#31572;&#26696;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#25191;&#34892;&#26816;&#32034;&#12289;&#29983;&#25104;&#25110;&#23436;&#25972;RAG&#31649;&#36947;&#30340;&#35821;&#26009;&#24211;&#12290;ClapNQ&#30340;&#31572;&#26696;&#31616;&#27905;&#65292;&#27604;&#23436;&#25972;&#27573;&#33853;&#23567;3&#20493;&#65292;&#24182;&#19988;&#19968;&#33268;&#65292;&#21253;&#21547;&#19981;&#36830;&#32493;&#30340;&#22810;&#20010;&#27573;&#33853;&#29255;&#27573;&#12290;RAG&#27169;&#22411;&#24517;&#39035;&#36866;&#24212;&#36825;&#20123;&#29305;&#24615;&#25165;&#33021;&#22312;ClapNQ&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#25105;&#20204;&#20026;ClapNQ&#25552;&#20986;&#20102;&#22522;&#32447;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#20173;&#26377;&#26174;&#33879;&#25361;&#25112;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02103v1 Announce Type: new  Abstract: Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG pipeline. ClapNQ includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SemEval 2024&#24341;&#20837;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#21407;&#22240;&#20998;&#26512;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#23558;&#20854;&#20316;&#20026;&#35805;&#35821;&#26631;&#35760;&#21644;&#24207;&#21015;&#26631;&#35760;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2404.02088</link><description>&lt;p&gt;
&#22312;SemEval-2024&#20219;&#21153;3&#20013;&#30340;LastResort: &#25506;&#32034;&#20316;&#20026;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#23545;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SemEval 2024&#24341;&#20837;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#21407;&#22240;&#20998;&#26512;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#23558;&#20854;&#20316;&#20026;&#35805;&#35821;&#26631;&#35760;&#21644;&#24207;&#21015;&#26631;&#35760;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02088v1 &#36890;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#23545;&#35805;&#26159;&#26368;&#33258;&#28982;&#30340;&#20154;&#31867;&#20132;&#27969;&#24418;&#24335;&#65292;&#27599;&#20010;&#35805;&#35821;&#21487;&#20197;&#28085;&#30422;&#22810;&#31181;&#21487;&#33021;&#30340;&#24773;&#24863;&#12290;&#34429;&#28982;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#26041;&#38754;&#24050;&#32463;&#20570;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20294;&#22312;&#25214;&#20986;&#25152;&#36848;&#24773;&#24863;&#30340;&#21407;&#22240;&#26041;&#38754;&#20570;&#30340;&#24037;&#20316;&#30456;&#23545;&#36739;&#23569;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#12290;SemEval 2024&#24341;&#20837;&#20102;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#21407;&#22240;&#20998;&#26512;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#21462;&#21453;&#26144;&#22312;&#28041;&#21450;&#22810;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#65289;&#30340;&#23545;&#35805;&#20013;&#20010;&#21035;&#35805;&#35821;&#20013;&#30340;&#24773;&#24863;&#65292;&#20197;&#21450;&#23548;&#33268;&#35813;&#24773;&#24863;&#30340;&#30456;&#24212;&#35805;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#27169;&#22411;&#65292;&#23558;&#36825;&#19968;&#20219;&#21153;&#20316;&#20026;&#35805;&#35821;&#26631;&#35760;&#21644;&#24207;&#21015;&#26631;&#35760;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#24182;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#20351;&#29992;&#19981;&#21516;&#32534;&#30721;&#22120;&#30340;&#22522;&#32447;&#65292;&#20351;&#29992;BiLSTM&#28155;&#21152;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#26368;&#21518;&#28155;&#21152;CRF&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02088v1 Announce Type: new  Abstract: Conversation is the most natural form of human communication, where each utterance can range over a variety of possible emotions. While significant work has been done towards the detection of emotions in text, relatively little work has been done towards finding the cause of the said emotions, especially in multimodal settings. SemEval 2024 introduces the task of Multimodal Emotion Cause Analysis in Conversations, which aims to extract emotions reflected in individual utterances in a conversation involving multiple modalities (textual, audio, and visual modalities) along with the corresponding utterances that were the cause for the emotion. In this paper, we propose models that tackle this task as an utterance labeling and a sequence labeling problem and perform a comparative study of these models, involving baselines using different encoders, using BiLSTM for adding contextual information of the conversation, and finally adding a CRF la
&lt;/p&gt;</description></item><item><title>&#26032;&#25512;&#20986;&#30340;Eurus&#27169;&#22411;&#36890;&#36807;&#22522;&#20110;&#39318;&#36873;&#26641;&#30340;&#25512;&#29702;&#20248;&#21270;&#65292;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25104;&#26524;&#65292;&#23588;&#20854;&#22312;&#20987;&#36133;&#20102;GPT-3.5 Turbo&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31361;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.02078</link><description>&lt;p&gt;
&#36890;&#36807;&#39318;&#36873;&#26641;&#25512;&#36827;LLM&#25512;&#29702;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Advancing LLM Reasoning Generalists with Preference Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02078
&lt;/p&gt;
&lt;p&gt;
&#26032;&#25512;&#20986;&#30340;Eurus&#27169;&#22411;&#36890;&#36807;&#22522;&#20110;&#39318;&#36873;&#26641;&#30340;&#25512;&#29702;&#20248;&#21270;&#65292;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25104;&#26524;&#65292;&#23588;&#20854;&#22312;&#20987;&#36133;&#20102;GPT-3.5 Turbo&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Eurus&#65292;&#19968;&#22871;&#19987;&#20026;&#25512;&#29702;&#20248;&#21270;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#32463;&#36807;Mistral-7B&#21644;CodeLlama-70B&#30340;&#24494;&#35843;&#65292;Eurus&#27169;&#22411;&#22312;&#28085;&#30422;&#25968;&#23398;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25104;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Eurus-70B&#22312;&#36890;&#36807;&#28085;&#30422;&#20116;&#39033;&#20219;&#21153;&#30340;12&#20010;&#27979;&#35797;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#20013;&#20987;&#36133;&#20102;GPT-3.5 Turbo&#65292;&#24182;&#22312;LeetCode&#21644;TheoremQA&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20998;&#21035;&#23454;&#29616;&#20102;33.3%&#21644;32.6%&#30340;pass@1&#20934;&#30830;&#29575;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#24320;&#28304;&#27169;&#22411;&#36229;&#36807;13.3%&#30340;&#36793;&#38469;&#12290;Eurus&#30340;&#24378;&#22823;&#24615;&#33021;&#20027;&#35201;&#24402;&#21151;&#20110;&#25105;&#20204;&#26032;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#23545;&#40784;&#25968;&#25454;&#38598;UltraInteract&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#38376;&#20026;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#32780;&#35774;&#35745;&#12290;UltraInteract&#21487;&#29992;&#20110;&#30417;&#30563;&#24494;&#35843;&#21644;&#39318;&#36873;&#23398;&#20064;&#12290;&#23545;&#20110;&#27599;&#20010;&#25351;&#20196;&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#39318;&#36873;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02078v1 Announce Type: new  Abstract: We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21033;&#29992;&#35299;&#37322;&#26041;&#27861;&#21644;&#40644;&#37329;&#35299;&#37322;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2404.02068</link><description>&lt;p&gt;
&#20351;&#29992;&#35299;&#37322;&#26041;&#27861;&#22686;&#24378;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Interpretation Methods for Model Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02068
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#35299;&#37322;&#26041;&#27861;&#21644;&#40644;&#37329;&#35299;&#37322;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26102;&#20195;&#65292;&#26377;&#35768;&#22810;&#20316;&#21697;&#35797;&#22270;&#25512;&#23548;&#31070;&#32463;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#30452;&#35266;&#22320;&#65292;&#24403;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#40644;&#37329;&#35299;&#37322;&#26102;&#65292;&#21487;&#20197;&#39069;&#22806;&#35757;&#32451;&#27169;&#22411;&#20351;&#20854;&#19982;&#35299;&#37322;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#35266;&#30340;&#24819;&#27861;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35299;&#37322;&#26041;&#27861;&#21644;&#40644;&#37329;&#35299;&#37322;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#25972;&#21512;&#21508;&#31181;&#35299;&#37322;&#26041;&#27861;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#35777;&#26126;&#26159;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21033;&#29992;&#21478;&#22806;&#20004;&#31181;&#31867;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#21363;&#25830;&#38500;/&#26367;&#25442;&#22411;&#21644;&#25552;&#21462;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#20004;&#20010;&#26032;&#39062;&#23454;&#20363;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02068v1 Announce Type: new  Abstract: In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models. Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales. However, this intuitive idea has not been fully explored. In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models. Our framework is very general in the sense that it can incorporate various interpretation methods. Previously proposed gradient-based methods can be shown as an instance of our framework. We also propose two novel instances utilizing two other types of interpretation methods, erasure/replace-based and extractor-based methods, for model enhancement. We conduct comprehensive experiments on a variety of tasks. Experimental results show that our framework is effective especially in low-resource settings in en
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2404.02060</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Long-context LLMs Struggle with Long In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#36229;&#36807;32K&#26631;&#35760;&#30340;&#38271;&#24207;&#21015;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#35780;&#20272;&#20027;&#35201;&#23616;&#38480;&#22312;&#22256;&#24785;&#24230;&#21644;&#21512;&#25104;&#20219;&#21153;&#31561;&#25351;&#26631;&#19978;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#23427;&#20204;&#22312;&#26356;&#24494;&#22937;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934;&#65288;LIConBench&#65289;&#65292;&#30528;&#37325;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#12290;&#25105;&#20204;&#31934;&#24515;&#36873;&#25321;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#26631;&#31614;&#33539;&#22260;&#36328;&#24230;&#20026;28&#33267;174&#31867;&#65292;&#28085;&#30422;&#20102;&#20174;2K&#21040;50K&#30340;&#19981;&#21516;&#36755;&#20837;&#65288;&#23569;&#37327;&#28436;&#31034;&#65289;&#38271;&#24230;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35201;&#27714;LLMs&#29702;&#35299;&#25972;&#20010;&#36755;&#20837;&#65292;&#20197;&#35782;&#21035;&#24222;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#20197;&#36827;&#34892;&#27491;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;13&#20010;&#38271;&#19978;&#19979;&#25991;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;&#38271;&#19978;&#19979;&#25991;LLMs&#22312;&#26631;&#35760;&#38271;&#24230;&#20026;20K&#20197;&#19979;&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#65292;&#24182;&#19988;&#21033;&#29992;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#20250;&#24102;&#26469;&#24615;&#33021;&#19978;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02060v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However,
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#24050;&#32463;&#23548;&#33268;AI&#21161;&#25163;&#30340;&#24613;&#21095;&#22686;&#38271;&#65292;&#20854;&#40065;&#26834;&#24615;&#37096;&#20998;&#24402;&#22240;&#20110;&#23545;&#40784;&#25216;&#26415;&#65292;&#28982;&#32780;&#36825;&#20123;&#21161;&#25163;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#21364;&#36739;&#20026;&#33030;&#24369;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#39592;&#24178;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.02054</link><description>&lt;p&gt;
&#25286;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;: &#36890;&#36807;&#30772;&#22351;&#29702;&#35299;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Deconstructing In-Context Learning: Understanding Prompts via Corruption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02054
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#24050;&#32463;&#23548;&#33268;AI&#21161;&#25163;&#30340;&#24613;&#21095;&#22686;&#38271;&#65292;&#20854;&#40065;&#26834;&#24615;&#37096;&#20998;&#24402;&#22240;&#20110;&#23545;&#40784;&#25216;&#26415;&#65292;&#28982;&#32780;&#36825;&#20123;&#21161;&#25163;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#21364;&#36739;&#20026;&#33030;&#24369;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#39592;&#24178;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#25552;&#20379;&#30340;&#25552;&#31034;&#8220;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#8221;&#30340;&#33021;&#21147;&#24050;&#32463;&#23548;&#33268;&#23427;&#20204;&#30340;&#20351;&#29992;&#25968;&#37327;&#24613;&#21095;&#22686;&#38271;&#65292;&#26368;&#32456;&#23548;&#33268;AI&#21161;&#25163;&#22914;ChatGPT&#12289;Claude&#21644;Bard&#30340;&#22823;&#37327;&#20986;&#29616;&#12290;&#36825;&#20123;AI&#21161;&#25163;&#34987;&#35748;&#20026;&#23545;&#25552;&#31034;&#30340;&#36731;&#24494;&#20462;&#25913;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20351;&#29992;&#20102;&#20154;&#31867;&#21453;&#39304;&#30340;&#23545;&#40784;&#25216;&#26415;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#20204;&#20351;&#29992;&#20316;&#20026;&#39592;&#24178;&#30340;&#22522;&#30784;&#39044;&#35757;&#32451;LLMs&#34987;&#35748;&#20026;&#22312;&#36825;&#26041;&#38754;&#27604;&#36739;&#33030;&#24369;&#12290;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#39592;&#24178;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#35780;&#20272;&#20854;&#36136;&#37327;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#36827;&#34892;&#23569;&#26679;&#26412;&#35780;&#20272;&#12290;&#36825;&#31181;&#35780;&#20272;&#20197;&#23545;&#36731;&#24494;&#25552;&#31034;&#20462;&#25913;&#21644;&#29305;&#23450;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#30340;&#39640;&#24230;&#25935;&#24863;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#20462;&#25913;&#25552;&#31034;&#30340;&#19981;&#21516;&#20803;&#32032;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36739;&#26089;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#20855;&#20307;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02054v1 Announce Type: new  Abstract: The ability of large language models (LLMs) to "learn in context" based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;BERTopic&#20998;&#26512;&#32929;&#24066;&#35780;&#35770;&#20013;&#30340;&#24773;&#24863;&#65292;&#25972;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26174;&#31034;&#24773;&#24863;&#20998;&#26512;&#26174;&#33879;&#25552;&#21319;&#20102;&#32929;&#24066;&#39044;&#27979;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;NLP&#22312;&#20016;&#23500;&#37329;&#34701;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02053</link><description>&lt;p&gt;
BERTopic&#39537;&#21160;&#30340;&#32929;&#24066;&#39044;&#27979;&#65306;&#35299;&#26512;&#24773;&#24863;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;BERTopic&#20998;&#26512;&#32929;&#24066;&#35780;&#35770;&#20013;&#30340;&#24773;&#24863;&#65292;&#25972;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26174;&#31034;&#24773;&#24863;&#20998;&#26512;&#26174;&#33879;&#25552;&#21319;&#20102;&#32929;&#24066;&#39044;&#27979;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;NLP&#22312;&#20016;&#23500;&#37329;&#34701;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#37329;&#34701;&#20998;&#26512;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#37325;&#28857;&#20851;&#27880;&#24773;&#24863;&#20998;&#26512;&#22312;&#32929;&#20215;&#39044;&#27979;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;BERTopic&#65292;&#19968;&#31181;&#20808;&#36827;&#30340;NLP&#25216;&#26415;&#65292;&#20998;&#26512;&#20174;&#32929;&#24066;&#35780;&#35770;&#20013;&#24471;&#20986;&#30340;&#20027;&#39064;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36825;&#31181;&#24773;&#24863;&#20998;&#26512;&#19982;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#25972;&#21512;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#21644;&#32929;&#31080;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#32780;&#38395;&#21517;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25972;&#21512;&#20027;&#39064;&#24773;&#24863;&#26174;&#33879;&#25552;&#21319;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32929;&#24066;&#35780;&#35770;&#20013;&#30340;&#20027;&#39064;&#25552;&#20379;&#20102;&#23545;&#32929;&#24066;&#27874;&#21160;&#21644;&#20215;&#26684;&#36235;&#21183;&#30340;&#38544;&#21547;&#12289;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23637;&#31034;NLP&#22312;&#20016;&#23500;&#37329;&#34701;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20026;&#23454;&#26102;&#24773;&#24863;&#20998;&#26512;&#21644;&#25506;&#32034;&#24773;&#24863;&#21644;&#24773;&#26223;&#30456;&#20851;&#24615;&#25171;&#24320;&#20102;&#30740;&#31350;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02053v1 Announce Type: new  Abstract: This paper explores the intersection of Natural Language Processing (NLP) and financial analysis, focusing on the impact of sentiment analysis in stock price prediction. We employ BERTopic, an advanced NLP technique, to analyze the sentiment of topics derived from stock market comments. Our methodology integrates this sentiment analysis with various deep learning models, renowned for their effectiveness in time series and stock prediction tasks. Through comprehensive experiments, we demonstrate that incorporating topic sentiment notably enhances the performance of these models. The results indicate that topics in stock market comments provide implicit, valuable insights into stock market volatility and price trends. This study contributes to the field by showcasing the potential of NLP in enriching financial analysis and opens up avenues for further research into real-time sentiment analysis and the exploration of emotional and contextua
&lt;/p&gt;</description></item><item><title>&#20044;&#20811;&#20848;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#25506;&#32034;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;NLP&#25216;&#26415;&#65292;&#27979;&#35797;&#20102;&#22312;&#27602;&#24615;&#20998;&#31867;&#12289;&#25991;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#26368;&#20339;&#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2404.02043</link><description>&lt;p&gt;
&#20044;&#20811;&#20848;&#25991;&#26412;&#20998;&#31867;&#65306;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02043
&lt;/p&gt;
&lt;p&gt;
&#20044;&#20811;&#20848;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#25506;&#32034;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;NLP&#25216;&#26415;&#65292;&#27979;&#35797;&#20102;&#22312;&#27602;&#24615;&#20998;&#31867;&#12289;&#25991;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#26368;&#20339;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20294;&#21508;&#31181;&#35821;&#35328;&#21487;&#29992;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#20381;&#28982;&#26174;&#32780;&#26131;&#35265;&#12290;&#20044;&#20811;&#20848;&#35821;&#20316;&#20026;&#19968;&#31181;&#20173;&#21487;&#20174;&#36328;&#35821;&#35328;&#26041;&#27861;&#30340;&#25345;&#32493;&#23436;&#21892;&#20013;&#21463;&#30410;&#30340;&#35821;&#35328;&#12290;&#37492;&#20110;&#25105;&#20204;&#25152;&#20102;&#35299;&#65292;&#38024;&#23545;&#20856;&#22411;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20044;&#20811;&#20848;&#35821;&#35821;&#26009;&#24211;&#26497;&#24230;&#21294;&#20047;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#32034;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#36991;&#20813;&#25163;&#21160;&#25968;&#25454;&#25972;&#29702;&#65306;&#22823;&#22411;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#21644;&#32763;&#35793;&#31995;&#32479;&#12289;LLMs&#65292;&#20197;&#21450;&#35821;&#35328;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;--&#27602;&#24615;&#20998;&#31867;&#12289;&#25991;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;--&#25552;&#20379;&#20102;&#26368;&#20339;&#35774;&#32622;&#30340;"&#37197;&#26041;"&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02043v1 Announce Type: cross  Abstract: Despite the extensive amount of labeled datasets in the NLP text classification field, the persistent imbalance in data availability across various languages remains evident. Ukrainian, in particular, stands as a language that still can benefit from the continued refinement of cross-lingual methodologies. Due to our knowledge, there is a tremendous lack of Ukrainian corpora for typical text classification tasks. In this work, we leverage the state-of-the-art advances in NLP, exploring cross-lingual knowledge transfer methods avoiding manual data curation: large multilingual encoders and translation systems, LLMs, and language adapters. We test the approaches on three text classification tasks -- toxicity classification, formality classification, and natural language inference -- providing the "recipe" for the optimal setups.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiParaDetox&#65292;&#23558;ParaDetox&#31649;&#36947;&#25193;&#23637;&#21040;&#22810;&#31181;&#35821;&#35328;&#65292;&#20197;&#33258;&#21160;&#21270;&#25910;&#38598;&#28508;&#22312;&#20219;&#20309;&#35821;&#35328;&#30340;&#24182;&#34892;&#20928;&#21270;&#35821;&#26009;&#24211;&#12290;</title><link>https://arxiv.org/abs/2404.02037</link><description>&lt;p&gt;
MultiParaDetox&#65306;&#23558;&#25991;&#26412;&#20928;&#21270;&#19982;&#24182;&#34892;&#25968;&#25454;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiParaDetox&#65292;&#23558;ParaDetox&#31649;&#36947;&#25193;&#23637;&#21040;&#22810;&#31181;&#35821;&#35328;&#65292;&#20197;&#33258;&#21160;&#21270;&#25910;&#38598;&#28508;&#22312;&#20219;&#20309;&#35821;&#35328;&#30340;&#24182;&#34892;&#20928;&#21270;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20928;&#21270;&#26159;&#19968;&#39033;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#65288;TST&#65289;&#20219;&#21153;&#65292;&#20854;&#20013;&#25991;&#26412;&#34987;&#20174;&#26377;&#27602;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#20363;&#22914;&#21253;&#21547;&#31895;&#40065;&#35789;&#27719;&#65292;&#36716;&#36848;&#20026;&#20013;&#24615;&#35821;&#20307;&#12290;&#26368;&#36817;&#65292;&#25991;&#26412;&#20928;&#21270;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25214;&#21040;&#20102;&#24212;&#29992;&#65292;&#27604;&#22914;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65288;Leong&#31561;&#65292;2023&#65307;He&#31561;&#65292;2024&#65307;Tang&#31561;&#65292;2023&#65289;&#20197;&#21450;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#23545;&#25239;&#26377;&#27602;&#35328;&#35770;&#65288;Deng&#31561;&#65292;2023&#65307;Mun&#31561;&#65292;2023&#65307;Agarwal&#31561;&#65292;2023&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#24212;&#29992;&#23545;&#20110;&#30830;&#20445;&#29616;&#20195;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#23433;&#20840;&#27807;&#36890;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#29992;&#20110;&#24182;&#34892;&#25991;&#26412;&#20928;&#21270;&#35821;&#26009;&#24211;&#25910;&#38598;&#30340;&#26041;&#27861;-- ParaDetox&#65288;Logacheva&#31561;&#65292;2022&#65289;&#21644;APPADIA&#65288;Atwell&#31561;&#65292;2022&#65289;-- &#20165;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;ParaDetox&#27969;&#31243;&#25193;&#23637;&#21040;&#22810;&#31181;&#35821;&#35328;&#65292;&#25552;&#20986;MultiParaDetox&#20197;&#33258;&#21160;&#21270;&#28508;&#22312;&#20219;&#20309;&#35821;&#35328;&#30340;&#24182;&#34892;&#20928;&#21270;&#35821;&#26009;&#24211;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02037v1 Announce Type: cross  Abstract: Text detoxification is a textual style transfer (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register. Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023). All these applications are extremely important to ensure safe communication in modern digital worlds. However, the previous approaches for parallel text detoxification corpora collection -- ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in monolingual setup. In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#19988;&#20415;&#21033;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#22411;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#20351;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35206;&#30422;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#25552;&#39640;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02022</link><description>&lt;p&gt;
&#20248;&#21270;&#21521;&#37327;&#21270;&#19978;&#19979;&#25991;&#30340;&#26816;&#32034;&#22686;&#24378;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#19988;&#20415;&#21033;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#22411;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#20351;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35206;&#30422;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#25552;&#39640;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#24212;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31561;&#25216;&#26415;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#38382;&#39064;&#12290;&#30001;&#20110;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#36164;&#28304;&#31561;&#32422;&#26463;&#65292;&#19978;&#19979;&#25991;&#38271;&#24230;&#36890;&#24120;&#21463;&#38480;&#65292;&#35753;&#27169;&#22411;&#35206;&#30422;&#36807;&#38271;&#30340;&#19978;&#19979;&#25991;&#24182;&#22238;&#31572;&#26469;&#33258;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#39064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#35206;&#30422;&#26356;&#38271;&#19978;&#19979;&#25991;&#30340;&#36890;&#29992;&#12289;&#26041;&#20415;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#19968;&#20010;&#23567;&#22411;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#32534;&#30721;&#19978;&#19979;&#25991;&#65292;&#24182;&#23545;&#21407;&#22987;&#36755;&#20837;&#24212;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35206;&#30422;&#20960;&#20493;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#22522;&#32447;&#25509;&#36817;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#24615;&#33021;&#22312;&#20004;&#20010;&#20445;&#23384;&#30340;&#25968;&#25454;&#38598;&#12289;&#22235;&#20010;&#20445;&#30041;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#20004;&#20010;In Context
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02022v1 Announce Type: new  Abstract: In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Contex
&lt;/p&gt;</description></item><item><title>&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;CNN&#21644;BiLSTM&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#26377;&#25928;&#27169;&#25311;&#20102;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#21644;&#39034;&#24207;&#27169;&#24335;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#21360;&#22320;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#21360;&#24230;&#33521;&#35821;&#22312;&#32447;&#31354;&#38388;&#20013;&#30340;&#24615;&#21035;&#34384;&#24453;&#12290;</title><link>https://arxiv.org/abs/2404.02013</link><description>&lt;p&gt;
&#25171;&#30772;&#27785;&#40664;&#65306;&#26816;&#27979;&#21644;&#32531;&#35299;&#21360;&#22320;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#21360;&#24230;&#33521;&#35821;&#22312;&#32447;&#31354;&#38388;&#20013;&#30340;&#24615;&#21035;&#34384;&#24453;
&lt;/p&gt;
&lt;p&gt;
Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02013
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;CNN&#21644;BiLSTM&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#26377;&#25928;&#27169;&#25311;&#20102;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#21644;&#39034;&#24207;&#27169;&#24335;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#21360;&#22320;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#21360;&#24230;&#33521;&#35821;&#22312;&#32447;&#31354;&#38388;&#20013;&#30340;&#24615;&#21035;&#34384;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24615;&#21035;&#39578;&#25200;&#26159;&#19968;&#20010;&#24191;&#27867;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#22899;&#24615;&#21644;&#36793;&#32536;&#24615;&#21035;&#22312;&#25968;&#23383;&#31354;&#38388;&#20013;&#30340;&#33258;&#30001;&#34920;&#36798;&#21644;&#21442;&#19982;&#12290;&#26816;&#27979;&#36825;&#31867;&#28389;&#29992;&#20869;&#23481;&#21487;&#20197;&#24110;&#21161;&#24179;&#21488;&#36943;&#21046;&#36825;&#19968;&#31096;&#23475;&#12290;&#25105;&#20204;&#21442;&#21152;&#20102; ICON2023 &#30340;Indic&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#34384;&#24453;&#26816;&#27979;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#25552;&#20379;&#20102;&#22312;&#33521;&#35821;&#12289;&#21360;&#22320;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#20013;&#36827;&#34892;&#20998;&#31867;&#20197;&#35782;&#21035;&#24615;&#21035;&#34384;&#24453;&#30340;Twitter&#24086;&#23376;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;CNLP-NITS-PP&#24320;&#21457;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;CNN&#21644;BiLSTM&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#21644;&#39034;&#24207;&#27169;&#24335;&#36827;&#34892;&#24314;&#27169;&#12290;&#21367;&#31215;&#28388;&#27874;&#22120;&#22312;&#23884;&#20837;&#36755;&#20837;&#25991;&#26412;&#19978;&#24212;&#29992;&#65292;CNN&#25429;&#33719;&#25351;&#31034;&#28389;&#29992;&#35821;&#35328;&#30340;&#23616;&#37096;&#29305;&#24449;&#12290;&#20026;&#20102;&#30830;&#23450;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20882;&#29359;&#24615;&#65292;BiLSTM&#20998;&#26512;&#36825;&#20010;&#24207;&#21015;&#20197;&#20102;&#35299;&#21333;&#35789;&#21644;&#30701;&#35821;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#27599;&#31181;&#35821;&#35328;&#20351;&#29992;&#20102;FastText&#21644;GloVe&#35789;&#23884;&#20837;&#36827;&#34892;&#20102;&#22810;&#20010;&#21464;&#20307;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02013v1 Announce Type: new  Abstract: Online gender-based harassment is a widespread issue limiting the free expression and participation of women and marginalized genders in digital spaces. Detecting such abusive content can enable platforms to curb this menace. We participated in the Gendered Abuse Detection in Indic Languages shared task at ICON2023 that provided datasets of annotated Twitter posts in English, Hindi and Tamil for building classifiers to identify gendered abuse. Our team CNLP-NITS-PP developed an ensemble approach combining CNN and BiLSTM networks that can effectively model semantic and sequential patterns in textual data. The CNN captures localized features indicative of abusive language through its convolution filters applied on embedded input text. To determine context-based offensiveness, the BiLSTM analyzes this sequence for dependencies among words and phrases. Multiple variations were trained using FastText and GloVe word embeddings for each languag
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22622;&#20869;&#21152;&#23572;&#20351;&#29992;&#30340;&#20027;&#35201;&#36890;&#29992;&#35821;&#35328;Wolof&#20013;&#24314;&#31435;&#30340;&#31532;&#19968;&#20010;&#33258;&#21160;&#35821;&#38899;&#21161;&#25163;&#30340;&#27010;&#24565;&#35777;&#26126;&#65292;&#26088;&#22312;&#20026;Orange&#23458;&#25143;&#25552;&#20379;&#20851;&#20110;Orange Senegal&#30340;Sargal&#24544;&#35802;&#35745;&#21010;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#21021;&#27493;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2404.02009</link><description>&lt;p&gt;
&#29992;Wolof&#35821;&#23545;&#35805;&#30340;&#35821;&#38899;&#26426;&#22120;&#20154;&#30340;&#27010;&#24565;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Preuve de concept d'un bot vocal dialoguant en wolof
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02009
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22622;&#20869;&#21152;&#23572;&#20351;&#29992;&#30340;&#20027;&#35201;&#36890;&#29992;&#35821;&#35328;Wolof&#20013;&#24314;&#31435;&#30340;&#31532;&#19968;&#20010;&#33258;&#21160;&#35821;&#38899;&#21161;&#25163;&#30340;&#27010;&#24565;&#35777;&#26126;&#65292;&#26088;&#22312;&#20026;Orange&#23458;&#25143;&#25552;&#20379;&#20851;&#20110;Orange Senegal&#30340;Sargal&#24544;&#35802;&#35745;&#21010;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#21021;&#27493;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22622;&#20869;&#21152;&#23572;&#20351;&#29992;&#30340;&#20027;&#35201;&#36890;&#29992;&#35821;&#35328;Wolof&#20013;&#24314;&#31435;&#30340;&#31532;&#19968;&#20010;&#33258;&#21160;&#35821;&#38899;&#21161;&#25163;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;&#35813;&#35821;&#38899;&#26426;&#22120;&#20154;&#26159;&#27861;&#22269;Orange Innovation&#12289;&#22622;&#20869;&#21152;&#23572;Orange&#65288;&#21448;&#21517;Sonatel&#65289;&#21644;&#24635;&#37096;&#20301;&#20110;&#22622;&#20869;&#21152;&#23572;&#36798;&#21888;&#23572;&#30340;&#23567;&#22411;IT&#20844;&#21496;ADNCorp&#20043;&#38388;&#21512;&#20316;&#30740;&#31350;&#39033;&#30446;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#35821;&#38899;&#26426;&#22120;&#20154;&#30340;&#30446;&#30340;&#26159;&#21521;Orange&#23458;&#25143;&#25552;&#20379;&#20851;&#20110;Orange Senegal&#30340;Sargal&#24544;&#35802;&#35745;&#21010;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#33258;&#28982;&#30340;&#20132;&#27969;&#26041;&#24335;&#65306;&#35821;&#38899;&#12290;&#35821;&#38899;&#26426;&#22120;&#20154;&#25509;&#25910;&#23458;&#25143;&#30340;&#21475;&#22836;&#35831;&#27714;&#20316;&#20026;&#36755;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;SLU&#31995;&#32479;&#22788;&#29702;&#36825;&#20123;&#35831;&#27714;&#65292;&#20351;&#29992;&#38899;&#39057;&#35760;&#24405;&#26469;&#22238;&#22797;&#23458;&#25143;&#12290;&#35813;&#27010;&#24565;&#35777;&#26126;&#30340;&#21021;&#27493;&#32467;&#26524;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#65292;&#25105;&#20204;&#22312;ASR&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;22%&#30340;WER&#65292;NLU&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;78%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02009v1 Announce Type: new  Abstract: This paper presents the proof-of-concept of the first automatic voice assistant ever built in Wolof language, the main vehicular language spoken in Senegal. This voicebot is the result of a collaborative research project between Orange Innovation in France, Orange Senegal (aka Sonatel) and ADNCorp, a small IT company based in Dakar, Senegal. The purpose of the voicebot is to provide information to Orange customers about the Sargal loyalty program of Orange Senegal by using the most natural mean to communicate: speech. The voicebot receives in input the customer's oral request that is then processed by a SLU system to reply to the customer's request using audio recordings. The first results of this proof-of-concept are encouraging as we achieved 22\% of WER for the ASR task and 78\% of F1-score on the NLU task.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#26356;&#39640;&#25928;&#24182;&#22312;ASR&#21644;LID&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02000</link><description>&lt;p&gt;
&#38750;&#27954;&#20013;&#24515;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#22312;&#25746;&#21704;&#25289;&#20197;&#21335;&#22320;&#21306;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#34920;&#24449;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02000
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#26356;&#39640;&#25928;&#24182;&#22312;ASR&#21644;LID&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20174;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#22320;&#21306;&#35762;&#35805;&#30340;21&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#23398;&#20064;&#20102;&#36817;60,000&#23567;&#26102;&#30340;&#26410;&#26631;&#35760;&#35821;&#38899;&#29255;&#27573;&#12290;&#22312;FLEURS-102&#25968;&#25454;&#38598;&#30340;SSA&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;HuBERT$_{base}$ (0.09B) &#26550;&#26500;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#19982;FLEURS&#22522;&#20934;&#25552;&#20986;&#30340;w2v-bert-51 (0.6B) &#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;ASR&#19979;&#28216;&#20219;&#21153;&#20013;&#26356;&#21152;&#39640;&#25928;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#23569;7&#20493;&#65292;&#21442;&#25968;&#23569;6&#20493;&#12290;&#27492;&#22806;&#65292;&#22312;LID&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#36229;&#36807;FLEURS&#22522;&#32447;&#36229;&#36807;22%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02000v1 Announce Type: new  Abstract: We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS benchmark, while being more efficient by using 7x less data and 6x less parameters. Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22\%.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#30340;DELAN&#26694;&#26550;&#25552;&#20986;&#20102;&#21452;&#23618;&#23545;&#40784;&#26041;&#27861;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#34892;&#21160;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2404.01994</link><description>&lt;p&gt;
DELAN: &#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#21452;&#23618;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01994
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#30340;DELAN&#26694;&#26550;&#25552;&#20986;&#20102;&#21452;&#23618;&#23545;&#40784;&#26041;&#27861;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#34892;&#21160;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01994v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: &#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;(VLN)&#35201;&#27714;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#31034;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;&#20026;&#20102;&#23436;&#25104;&#20219;&#21153;&#65292;&#20195;&#29702;&#38656;&#35201;&#23545;&#40784;&#21644;&#25972;&#21512;&#21508;&#31181;&#23548;&#33322;&#27169;&#24577;&#65292;&#21253;&#25324;&#25351;&#31034;&#12289;&#35266;&#23519;&#21644;&#23548;&#33322;&#21382;&#21490;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#34701;&#21512;&#38454;&#27573;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#65292;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30001;&#19981;&#21516;&#21333;&#19968;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#27169;&#24577;&#29305;&#24449;&#20301;&#20110;&#21508;&#33258;&#30340;&#31354;&#38388;&#20013;&#65292;&#23548;&#33268;&#36328;&#27169;&#24577;&#34701;&#21512;&#21644;&#20915;&#31574;&#36136;&#37327;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#30340;Dual-levEL AligNment (DELAN)&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#22312;&#34701;&#21512;&#20043;&#21069;&#23545;&#40784;&#21508;&#31181;&#19982;&#23548;&#33322;&#30456;&#20851;&#30340;&#27169;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#34892;&#21160;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#39044;&#34701;&#21512;&#23545;&#40784;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;: &#25351;&#31034;-&#21382;&#21490;&#23618;&#21644;&#22320;&#26631;-&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01994v1 Announce Type: cross  Abstract: Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;CONPARE-LAMA&#65292;&#30830;&#23450;&#20102;&#37322;&#20041;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#23545;&#30693;&#35782;&#26816;&#32034;&#30340;&#29420;&#31435;&#24433;&#21709;</title><link>https://arxiv.org/abs/2404.01992</link><description>&lt;p&gt;
&#23545;&#37322;&#20041;&#30340;&#21078;&#26512;&#65306;&#25552;&#31034;&#35821;&#27861;&#21644;&#34917;&#20805;&#20449;&#24687;&#23545;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#32034;&#30693;&#35782;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01992
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;CONPARE-LAMA&#65292;&#30830;&#23450;&#20102;&#37322;&#20041;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#23545;&#30693;&#35782;&#26816;&#32034;&#30340;&#29420;&#31435;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#34987;&#35748;&#20026;&#21253;&#21547;&#21508;&#31181;&#30693;&#35782;&#12290;&#19968;&#31181;&#25512;&#26029;&#20851;&#31995;&#30693;&#35782;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22635;&#31354;&#24335;&#25552;&#31034;&#65292;&#27169;&#22411;&#34987;&#35201;&#27714;&#39044;&#27979;&#32570;&#22833;&#30340;&#20027;&#35821;&#25110;&#23486;&#35821;&#12290;&#35774;&#35745;&#36825;&#20123;&#25552;&#31034;&#36890;&#24120;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#35821;&#27861;&#25110;&#35821;&#20041;&#19978;&#30340;&#32454;&#24494;&#24046;&#24322;&#21487;&#33021;&#20250;&#23545;&#30693;&#35782;&#26816;&#32034;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#25552;&#31034;&#35821;&#27861;&#25110;&#20449;&#24687;&#30340;&#24433;&#21709;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;CONPARE-LAMA - &#19968;&#20010;&#19987;&#38376;&#30340;&#25506;&#38024;&#65292;&#30001;3400&#19975;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#32452;&#25104;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#37322;&#20041;&#20043;&#38388;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#20123;&#37322;&#20041;&#36981;&#24490;&#32479;&#19968;&#30340;&#20803;&#27169;&#26495;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#20851;&#31995;&#20013;&#36890;&#36807;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#21463;&#25511;&#21464;&#21270;&#12290;CONPARE-LAMA&#20351;&#20154;&#33021;&#22815;&#27934;&#23519;&#37322;&#20041;&#30340;&#35821;&#27861;&#24418;&#24335;&#25110;&#35821;&#20041;&#20449;&#24687;&#23545;&#30693;&#35782;&#26816;&#32034;&#30340;&#29420;&#31435;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01992v1 Announce Type: new  Abstract: Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARE-LAMA - a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARE-LAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval 
&lt;/p&gt;</description></item><item><title>Kallaama&#39033;&#30446;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;125&#23567;&#26102;&#24405;&#38899;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22635;&#34917;&#22622;&#20869;&#21152;&#23572;&#19977;&#31181;&#20027;&#35201;&#35821;&#35328;&#20013;&#20851;&#20110;&#20892;&#19994;&#39046;&#22495;&#30340;&#26426;&#22120;&#21487;&#35835;&#25968;&#25454;&#19981;&#36275;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.01991</link><description>&lt;p&gt;
Kallaama&#65306;&#22622;&#20869;&#21152;&#23572;&#19977;&#31181;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#35328;&#20013;&#20851;&#20110;&#20892;&#19994;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Kallaama: A Transcribed Speech Dataset about Agriculture in the Three Most Widely Spoken Languages in Senegal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01991
&lt;/p&gt;
&lt;p&gt;
Kallaama&#39033;&#30446;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;125&#23567;&#26102;&#24405;&#38899;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22635;&#34917;&#22622;&#20869;&#21152;&#23572;&#19977;&#31181;&#20027;&#35201;&#35821;&#35328;&#20013;&#20851;&#20110;&#20892;&#19994;&#39046;&#22495;&#30340;&#26426;&#22120;&#21487;&#35835;&#25968;&#25454;&#19981;&#36275;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26159;Kallaama&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#26088;&#22312;&#20026;&#20892;&#19994;&#39046;&#22495;&#30340;&#35821;&#38899;&#25216;&#26415;&#21457;&#23637;&#29983;&#20135;&#21644;&#20256;&#25773;&#22269;&#23478;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;&#38500;&#20102;&#27779;&#27931;&#22827;&#35821;&#22806;&#65288;&#27779;&#27931;&#22827;&#35821;&#24050;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20123;&#35821;&#35328;&#25968;&#25454;&#65289;&#65292;&#22622;&#20869;&#21152;&#23572;&#30340;&#22269;&#23478;&#35821;&#35328;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#35821;&#35328;&#25216;&#26415;&#25552;&#20379;&#21830;&#24573;&#35270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#26159;&#20445;&#25252;&#12289;&#20419;&#36827;&#21644;&#25945;&#25480;&#36825;&#20123;&#35821;&#35328;&#30340;&#20851;&#38190;&#12290;Kallaama&#20851;&#27880;&#22622;&#20869;&#21152;&#23572;&#20154;&#21475;&#20013;&#20351;&#29992;&#26368;&#22810;&#30340;3&#31181;&#20027;&#35201;&#35821;&#35328;&#65306;&#27779;&#27931;&#22827;&#35821;&#12289;&#26222;&#25289;&#35821;&#21644;&#29791;&#37324;&#23572;&#35821;&#12290;&#36825;&#20123;&#35821;&#35328;&#34987;&#20154;&#21475;&#24191;&#27867;&#20351;&#29992;&#65292;&#32422;&#26377;1000&#19975;&#22622;&#20869;&#21152;&#23572;&#26412;&#22320;&#20351;&#29992;&#32773;&#65292;&#26356;&#19981;&#29992;&#35828;&#22269;&#22806;&#30340;&#20154;&#20102;&#12290;&#28982;&#32780;&#65292;&#22312;&#20892;&#19994;&#39046;&#22495;&#65292;&#36825;&#20123;&#35821;&#35328;&#22312;&#26426;&#22120;&#21487;&#35835;&#25968;&#25454;&#26041;&#38754;&#20173;&#28982;&#36164;&#28304;&#21294;&#20047;&#65292;&#26080;&#27861;&#29992;&#20110;&#33258;&#21160;&#22788;&#29702;&#21644;&#35821;&#35328;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01991v1 Announce Type: new  Abstract: This work is part of the Kallaama project, whose objective is to produce and disseminate national languages corpora for speech technologies developments, in the field of agriculture. Except for Wolof, which benefits from some language data for natural language processing, national languages of Senegal are largely ignored by language technology providers. However, such technologies are keys to the protection, promotion and teaching of these languages. Kallaama focuses on the 3 main spoken languages by Senegalese people: Wolof, Pulaar and Sereer. These languages are widely spoken by the population, with around 10 million of native Senegalese speakers, not to mention those outside the country. However, they remain under-resourced in terms of machine-readable data that can be used for automatic processing and language technologies, all the more so in the agricultural sector. We release a transcribed speech dataset containing 125 hours of rec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;GPT4&#36827;&#34892;&#22522;&#20110;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#27665;&#20107;&#35785;&#35772;&#20013;&#30340;&#36777;&#35777;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#25552;&#31034;&#31574;&#30053;&#30340;&#38598;&#25104;&#12290;&#22312;SemEval&#20219;&#21153;5&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;0.8095&#30340;Macro F1&#20540;&#65292;&#24182;&#22312;&#26368;&#32456;&#27979;&#35797;&#38598;&#20013;&#25490;&#21517;&#31532;5&#12290;</title><link>https://arxiv.org/abs/2404.01961</link><description>&lt;p&gt;
Team UTSA-NLP&#22312;SemEval 2024&#20219;&#21153;5&#20013;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;GPT4&#30340;&#27665;&#20107;&#35785;&#35772;&#20013;&#30340;&#36777;&#35777;&#25512;&#29702;&#30340;&#25552;&#31034;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01961
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;GPT4&#36827;&#34892;&#22522;&#20110;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#27665;&#20107;&#35785;&#35772;&#20013;&#30340;&#36777;&#35777;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#25552;&#31034;&#31574;&#30053;&#30340;&#38598;&#25104;&#12290;&#22312;SemEval&#20219;&#21153;5&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;0.8095&#30340;Macro F1&#20540;&#65292;&#24182;&#22312;&#26368;&#32456;&#27979;&#35797;&#38598;&#20013;&#25490;&#21517;&#31532;5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;SemEval&#20219;&#21153;5&#65288;&#27665;&#20107;&#35785;&#35772;&#20013;&#30340;&#27861;&#24459;&#36777;&#35777;&#20219;&#21153;&#25361;&#25112;&#65289;&#30340;&#31995;&#32479;&#12290;&#27861;&#24459;&#36777;&#35777;&#25512;&#29702;&#26159;&#25152;&#26377;&#27861;&#23398;&#29983;&#37117;&#24517;&#39035;&#25484;&#25569;&#30340;&#22522;&#26412;&#25216;&#33021;&#12290;&#27492;&#22806;&#65292;&#24320;&#21457;&#33021;&#22815;&#26681;&#25454;&#31616;&#27905;&#30340;&#39046;&#22495;&#29305;&#23450;&#19978;&#19979;&#25991;&#20449;&#24687;&#25512;&#29702;&#38382;&#39064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35299;&#20915;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#25506;&#32034;&#20102;&#20351;&#29992;GPT4&#36827;&#34892;&#22522;&#20110;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25512;&#29702;&#27861;&#24459;&#35770;&#28857;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#25552;&#31034;&#31574;&#30053;&#30340;&#38598;&#21512;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;Macro F1&#20540;&#20026;0.8095&#65292;&#22312;&#26368;&#32456;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;0.7315&#65288;21&#20010;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;5&#65289;&#12290;&#27492;&#39033;&#30446;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/danschumac1/CivilPromptReasoningGPT4 &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01961v1 Announce Type: new  Abstract: In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge. Legal argument reasoning is an essential skill that all law students must master. Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information. Our system explores a prompt-based solution using GPT4 to reason over legal arguments. We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning. Overall, our system results in a Macro F1 of .8095 on the validation dataset and .7315 (5th out of 21 teams) on the final test set. Code for this project is available at https://github.com/danschumac1/CivilPromptReasoningGPT4.
&lt;/p&gt;</description></item><item><title>HyperCLOVA X &#26159;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#33021;&#21147;&#30340;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;&#65292;&#20854;&#25512;&#29702;&#33021;&#21147;&#24378;&#22823;&#19988;&#20855;&#26377;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01954</link><description>&lt;p&gt;
HyperCLOVA X &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
HyperCLOVA X Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01954
&lt;/p&gt;
&lt;p&gt;
HyperCLOVA X &#26159;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#33021;&#21147;&#30340;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;&#65292;&#20854;&#25512;&#29702;&#33021;&#21147;&#24378;&#22823;&#19988;&#20855;&#26377;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; HyperCLOVA X&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#20855;&#26377;&#22312;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#26041;&#38754;&#30340;&#31454;&#20105;&#33021;&#21147;&#12290;HyperCLOVA X &#22312;&#24179;&#34913;&#28151;&#21512;&#30340;&#38889;&#35821;&#12289;&#33521;&#35821;&#21644;&#20195;&#30721;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#65292;&#21516;&#26102;&#36981;&#23432;&#20005;&#26684;&#30340;&#23433;&#20840;&#20934;&#21017;&#65292;&#20307;&#29616;&#20102;&#25105;&#20204;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#25215;&#35834;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#32508;&#21512;&#25512;&#29702;&#12289;&#30693;&#35782;&#12289;&#24120;&#35782;&#12289;&#30495;&#23454;&#24615;&#12289;&#32534;&#30721;&#12289;&#25968;&#23398;&#12289;&#32842;&#22825;&#12289;&#36981;&#24490;&#25351;&#20196;&#21644;&#26080;&#23475;&#24615;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28085;&#30422;&#38889;&#35821;&#21644;&#33521;&#35821;&#12290;HyperCLOVA X &#22312;&#38889;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24471;&#30410;&#20110;&#23545;&#35821;&#35328;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#24322;&#30340;&#28145;&#21051;&#29702;&#35299;&#12290;&#23545;&#22266;&#26377;&#30340;&#21452;&#35821;&#29305;&#24615;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#21450;&#20854;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#30340;&#30740;&#31350;&#31361;&#26174;&#20986;&#35813;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#29087;&#32451;&#24615;&#21644;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#26410;&#23450;&#21521;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01954v1 Announce Type: cross  Abstract: We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted la
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65292;&#26412;&#25991;&#25552;&#20986;&#23558;&#21333;&#35789;&#20998;&#21106;&#25104;&#23383;&#32032;&#30340;&#26041;&#27861;&#65292;&#27491;&#30830;&#39044;&#27979;&#30340;&#20934;&#30830;&#29575;&#20026;50.18&#65285;&#65292;93.51&#65285;&#30340;&#39044;&#27979;&#22312;&#27491;&#30830;&#20998;&#31867;&#30340;&#33539;&#22260;&#20869;&#12290;</title><link>https://arxiv.org/abs/2404.01953</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#23545;&#33521;&#35821;&#21333;&#35789;&#20013;&#30340;&#23383;&#32032;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Graphemes in English Words Through the Application of a Fuzzy Inference System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01953
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65292;&#26412;&#25991;&#25552;&#20986;&#23558;&#21333;&#35789;&#20998;&#21106;&#25104;&#23383;&#32032;&#30340;&#26041;&#27861;&#65292;&#27491;&#30830;&#39044;&#27979;&#30340;&#20934;&#30830;&#29575;&#20026;50.18&#65285;&#65292;93.51&#65285;&#30340;&#39044;&#27979;&#22312;&#27491;&#30830;&#20998;&#31867;&#30340;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#23398;&#20013;&#65292;&#23383;&#32032;&#26159;&#19982;&#38899;&#32032;&#23545;&#24212;&#30340;&#20070;&#20889;&#31995;&#32479;&#30340;&#20070;&#38754;&#21333;&#20301;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#20070;&#38754;&#35821;&#35328;&#36890;&#36807;&#21333;&#35789;&#20998;&#26512;&#21644;&#23383;&#31526;&#20998;&#26512;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#25991;&#20851;&#27880;&#31532;&#19977;&#31181;&#26041;&#27861;&#65292;&#21363;&#23545;&#23383;&#32032;&#36827;&#34892;&#20998;&#26512;&#12290;&#23383;&#32032;&#30456;&#23545;&#20110;&#21333;&#35789;&#21644;&#23383;&#31526;&#20998;&#26512;&#20855;&#26377;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#35821;&#38899;&#30340;&#33258;&#21253;&#21547;&#34920;&#31034;&#12290;&#30001;&#20110;&#23558;&#21333;&#35789;&#20998;&#21106;&#25104;&#23383;&#32032;&#30340;&#24615;&#36136;&#22522;&#20110;&#22797;&#26434;&#30340;&#38750;&#20108;&#36827;&#21046;&#35268;&#21017;&#65292;&#24212;&#29992;&#27169;&#31946;&#36923;&#36753;&#23558;&#25552;&#20379;&#19968;&#20010;&#21512;&#36866;&#30340;&#20171;&#36136;&#65292;&#29992;&#20110;&#39044;&#27979;&#21333;&#35789;&#20013;&#30340;&#23383;&#32032;&#25968;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#24212;&#29992;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#23558;&#21333;&#35789;&#20998;&#21106;&#25104;&#20854;&#23383;&#32032;&#12290;&#36825;&#31181;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19968;&#21322;&#30340;&#24773;&#20917;&#19979;&#27491;&#30830;&#39044;&#27979;&#21333;&#35789;&#20013;&#30340;&#23383;&#32032;&#25968;&#37327;&#65292;&#20854;&#20013;93.51%&#30340;&#20998;&#31867;&#22312;&#27491;&#30830;&#20998;&#31867;&#30340;+- 1&#20043;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01953v1 Announce Type: new  Abstract: In Linguistics, a grapheme is a written unit of a writing system corresponding to a phonological sound. In Natural Language Processing tasks, written language is analysed through two different mediums, word analysis, and character analysis. This paper focuses on a third approach, the analysis of graphemes. Graphemes have advantages over word and character analysis by being self-contained representations of phonetic sounds. Due to the nature of splitting a word into graphemes being based on complex, non-binary rules, the application of fuzzy logic would provide a suitable medium upon which to predict the number of graphemes in a word. This paper proposes the application of a Fuzzy Inference System to split words into their graphemes. This Fuzzy Inference System results in a correct prediction of the number of graphemes in a word 50.18% of the time, with 93.51% being within a margin of +- 1 from the correct classification. Given the variet
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#32593;&#32476;&#29359;&#32618;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#30340;&#32763;&#35793;&#65292;&#32467;&#26524;&#26174;&#31034;&#33021;&#22815;&#26356;&#22909;&#12289;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#24182;&#22823;&#24133;&#24230;&#38477;&#20302;&#32763;&#35793;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.01940</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#29702;&#35299;&#32593;&#32476;&#29359;&#32618;&#65306;&#31934;&#32454;&#35843;&#25972;&#30340;LLMs&#22312;&#32763;&#35793;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#32593;&#32476;&#29359;&#32618;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#30340;&#32763;&#35793;&#65292;&#32467;&#26524;&#26174;&#31034;&#33021;&#22815;&#26356;&#22909;&#12289;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#24182;&#22823;&#24133;&#24230;&#38477;&#20302;&#32763;&#35793;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#32593;&#32476;&#29359;&#32618;&#36890;&#20449;&#23545;&#32593;&#32476;&#23433;&#20840;&#38450;&#24481;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#36890;&#24120;&#28041;&#21450;&#23558;&#36890;&#20449;&#32763;&#35793;&#25104;&#33521;&#25991;&#36827;&#34892;&#22788;&#29702;&#12289;&#35299;&#37322;&#21644;&#29983;&#25104;&#21450;&#26102;&#24773;&#25253;&#12290;&#38382;&#39064;&#22312;&#20110;&#32763;&#35793;&#24456;&#22256;&#38590;&#12290;&#20154;&#24037;&#32763;&#35793;&#32531;&#24930;&#12289;&#26114;&#36149;&#19988;&#31232;&#32570;&#12290;&#26426;&#22120;&#32763;&#35793;&#19981;&#20934;&#30830;&#19988;&#23384;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#32593;&#32476;&#29359;&#32618;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#24212;&#29992;&#20110;&#26469;&#33258;NoName057(16)&#20420;&#35821;&#40657;&#23458;&#32452;&#32455;&#30340;&#20844;&#24320;&#32842;&#22825;&#35760;&#24405;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31934;&#32454;&#35843;&#25972;&#30340;LLM&#27169;&#22411;&#26356;&#22909;&#12289;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#65292;&#24182;&#33021;&#22815;&#25429;&#25417;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#65292;&#36890;&#36807;&#19982;&#20154;&#24037;&#32763;&#35793;&#30456;&#27604;&#27604;&#21487;&#23454;&#29616;&#39640;&#20445;&#30495;&#32763;&#35793;&#65292;&#24182;&#23558;&#25104;&#26412;&#26174;&#33879;&#38477;&#20302;430&#33267;23,000&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01940v1 Announce Type: new  Abstract: Understanding cybercrime communications is paramount for cybersecurity defence. This often involves translating communications into English for processing, interpreting, and generating timely intelligence. The problem is that translation is hard. Human translation is slow, expensive, and scarce. Machine translation is inaccurate and biased. We propose using fine-tuned Large Language Models (LLM) to generate translations that can accurately capture the nuances of cybercrime language. We apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist group. Our results show that our fine-tuned LLM model is better, faster, more accurate, and able to capture nuances of the language. Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SGSH&#26694;&#26550;&#65292;&#36890;&#36807;&#39592;&#26550;&#21551;&#21457;&#26469;&#28608;&#21457;GPT-3.5&#29983;&#25104;&#30693;&#35782;&#24211;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#32452;&#32455;&#21644;&#21033;&#29992;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2404.01923</link><description>&lt;p&gt;
SGSH&#65306;&#29992;&#39592;&#26550;&#21551;&#21457;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SGSH&#26694;&#26550;&#65292;&#36890;&#36807;&#39592;&#26550;&#21551;&#21457;&#26469;&#28608;&#21457;GPT-3.5&#29983;&#25104;&#30693;&#35782;&#24211;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#32452;&#32455;&#21644;&#21033;&#29992;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#65288;KBQG&#65289;&#26088;&#22312;&#20174;&#20174;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#30340;&#19977;&#20803;&#32452;&#20107;&#23454;&#38598;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26174;&#33879;&#25552;&#21319;&#20102;KBQG&#30340;&#24615;&#33021;&#65292;&#24471;&#30410;&#20110;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SGSH--&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39592;&#26550;&#21551;&#21457;&#26469;&#22686;&#24378;KBQG&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#8220;&#39592;&#26550;&#21551;&#21457;&#8221;&#65292;&#25552;&#20379;&#20102;&#19982;&#27599;&#20010;&#36755;&#20837;&#30456;&#20851;&#30340;&#26356;&#31934;&#32454;&#30340;&#25351;&#23548;&#65292;&#20197;&#28608;&#21457;LLMs&#29983;&#25104;&#26368;&#20339;&#38382;&#39064;&#65292;&#21253;&#25324;&#38382;&#39064;&#30701;&#35821;&#21644;&#21161;&#21160;&#35789;&#31561;&#20851;&#38190;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01923v1 Announce Type: cross  Abstract: Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH--a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates "skeleton heuristics", which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29702;&#24615;&#20013;&#24515;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#35782;&#21035;&#34394;&#20551;&#21644;&#22240;&#26524;&#20851;&#32852;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#36328;&#25991;&#26723;&#20107;&#20214;&#20851;&#32852;&#28040;&#35299;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01921</link><description>&lt;p&gt;
&#22522;&#20110;&#29702;&#24615;&#20013;&#24515;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#36328;&#25991;&#26723;&#20107;&#20214;&#20851;&#32852;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29702;&#24615;&#20013;&#24515;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#35782;&#21035;&#34394;&#20551;&#21644;&#22240;&#26524;&#20851;&#32852;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#36328;&#25991;&#26723;&#20107;&#20214;&#20851;&#32852;&#28040;&#35299;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#20107;&#20214;&#20851;&#32852;&#28040;&#35299;&#65288;ECR&#65289;&#31995;&#32479;&#22312;&#36328;&#25991;&#26723;&#20013;&#32858;&#31867;&#25351;&#20195;&#24615;&#20107;&#20214;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#36755;&#20837;&#25552;&#21450;&#23545;&#25991;&#26412;&#20013;&#36807;&#20110;&#20381;&#36182;&#8220;&#35302;&#21457;&#35789;&#35789;&#27719;&#21305;&#37197;&#8221;&#36825;&#19968;&#34394;&#20551;&#27169;&#24335;&#12290;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#23545;&#22522;&#20934;ECR&#31995;&#32479;&#30340;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#24418;&#24335;&#21270;&#65292;&#26088;&#22312;&#35782;&#21035;ECR&#20219;&#21153;&#20013;&#30340;&#34394;&#20551;&#21644;&#22240;&#26524;&#20851;&#32852;&#65288;&#21363;&#65292;&#29702;&#24615;&#65289;&#12290;&#21033;&#29992;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29702;&#24615;&#20013;&#24515;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;LLM&#12290;&#35813;&#26041;&#27861;&#19987;&#20026;ECR&#31995;&#32479;&#20013;&#30340;&#20004;&#20004;&#36755;&#20837;&#35774;&#35745;&#65292;&#25105;&#20204;&#22312;&#35302;&#21457;&#35789;&#21644;&#19978;&#19979;&#25991;&#19978;&#36827;&#34892;&#30452;&#25509;&#24178;&#39044;&#65292;&#20197;&#20943;&#36731;&#34394;&#20551;&#20851;&#32852;&#65292;&#24378;&#35843;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01921v1 Announce Type: new  Abstract: Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents. However, the existing system exhibits an excessive reliance on the `triggers lexical matching' spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop. This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three po
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SCANNER&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#23454;&#20307;&#20505;&#36873;&#24182;&#21033;&#29992;&#30693;&#35782;&#20174;&#22810;&#31181;&#26469;&#28304;&#33719;&#21462;&#30693;&#35782;&#65292;&#22686;&#24378;&#20102;&#23545;&#26410;&#35265;&#23454;&#20307;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;NER&#25968;&#25454;&#38598;&#20013;&#24102;&#26377;&#22122;&#22768;&#27880;&#37322;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01914</link><description>&lt;p&gt;
SCANNER&#65306;&#29992;&#20110;&#24378;&#22823;&#30340;&#22810;&#27169;&#24335;&#20855;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SCANNER&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#23454;&#20307;&#20505;&#36873;&#24182;&#21033;&#29992;&#30693;&#35782;&#20174;&#22810;&#31181;&#26469;&#28304;&#33719;&#21462;&#30693;&#35782;&#65292;&#22686;&#24378;&#20102;&#23545;&#26410;&#35265;&#23454;&#20307;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;NER&#25968;&#25454;&#38598;&#20013;&#24102;&#26377;&#22122;&#22768;&#27880;&#37322;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20855;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#30340;&#36827;&#23637;&#25512;&#21160;&#20102;&#35813;&#20219;&#21153;&#30340;&#36793;&#30028;&#65292;&#23558;&#35270;&#35273;&#20449;&#21495;&#32435;&#20837;&#20854;&#20013;&#65292;&#23548;&#33268;&#35768;&#22810;&#21464;&#20307;&#65292;&#21253;&#25324;&#22810;&#27169;&#24335;NER&#65288;MNER&#65289;&#25110;&#22522;&#20110;&#23454;&#20307;&#30340;MNER&#65288;GMNER&#65289;&#12290; &#36825;&#20123;&#20219;&#21153;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#27169;&#22411;&#24212;&#33021;&#22815;&#25512;&#24191;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#23454;&#20307;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#35757;&#32451;&#26679;&#26412;&#20013;&#24102;&#26377;&#22122;&#22768;&#27880;&#37322;&#30340;&#24773;&#20917;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SCANNER&#65288;&#29992;&#20110;NER&#30340;SpAN CANdidate&#26816;&#27979;&#21644;&#35782;&#21035;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25152;&#26377;&#19977;&#31181;NER&#21464;&#20307;&#30340;&#27169;&#22411;&#12290; SCANNER&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#32467;&#26500;&#65307;&#25105;&#20204;&#22312;&#31532;&#19968;&#38454;&#27573;&#25552;&#21462;&#23454;&#20307;&#20505;&#36873;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#26597;&#35810;&#20197;&#33719;&#21462;&#30693;&#35782;&#65292;&#26377;&#25928;&#22320;&#20174;&#21508;&#31181;&#26469;&#28304;&#20013;&#25552;&#21462;&#30693;&#35782;&#12290; &#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25552;&#21462;&#30693;&#35782;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#26410;&#35265;&#23454;&#20307;&#12290; &#27492;&#22806;&#65292;&#20026;&#20102;&#24212;&#23545;NER&#25968;&#25454;&#38598;&#20013;&#30001;&#22024;&#26434;&#27880;&#37322;&#24341;&#36215;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01914v1 Announce Type: cross  Abstract: Recent advances in named entity recognition (NER) have pushed the boundary of the task to incorporate visual signals, leading to many variants, including multi-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks is that the model should be able to generalize to the entities unseen during the training, and should be able to handle the training samples with noisy annotations. To address this obstacle, we propose SCANNER (Span CANdidate detection and recognition for NER), a model capable of effectively handling all three NER variants. SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge, effectively pulling knowledge from various sources. We can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities. Furthermore, to tackle the challenges arising from noisy annotations in NER datasets, we introduce a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#20197;&#35268;&#36991;&#26816;&#27979;&#30340;&#24191;&#27867;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#30333;&#30418;&#21644;&#40657;&#30418;&#20004;&#31181;&#25915;&#20987;&#35774;&#32622;&#20197;&#21450;&#23545;&#25239;&#23398;&#20064;&#22312;&#21160;&#24577;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#28508;&#21147;&#22686;&#24378;</title><link>https://arxiv.org/abs/2404.01907</link><description>&lt;p&gt;
&#20154;&#24615;&#21270;&#26426;&#22120;&#29983;&#25104;&#30340;&#20869;&#23481;&#65306;&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#35268;&#36991;AI&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#20197;&#35268;&#36991;&#26816;&#27979;&#30340;&#24191;&#27867;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#30333;&#30418;&#21644;&#40657;&#30418;&#20004;&#31181;&#25915;&#20987;&#35774;&#32622;&#20197;&#21450;&#23545;&#25239;&#23398;&#20064;&#22312;&#21160;&#24577;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#28508;&#21147;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#22312;&#38754;&#23545;&#35832;&#22914;&#35823;&#20256;&#20449;&#24687;&#12289;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#21644;&#39044;&#38450;&#23398;&#26415;&#25220;&#34989;&#31561;&#24694;&#24847;&#29992;&#20363;&#26102;&#21464;&#24471;&#26085;&#30410;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#22312;&#26410;&#30693;&#27979;&#35797;&#25968;&#25454;&#19978;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#22312;&#22788;&#29702;&#23545;&#25239;&#25915;&#20987;&#65288;&#22914;&#37322;&#20041;&#65289;&#26102;&#23384;&#22312;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#31867;&#21035;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#26088;&#22312;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#20869;&#23481;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#20197;&#35268;&#36991;&#26816;&#27979;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#25915;&#20987;&#35774;&#32622;&#65306;&#30333;&#30418;&#21644;&#40657;&#30418;&#65292;&#24182;&#22312;&#21160;&#24577;&#22330;&#26223;&#20013;&#37319;&#29992;&#23545;&#25239;&#23398;&#20064;&#26469;&#35780;&#20272;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#28508;&#21147;&#22686;&#24378;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01907v1 Announce Type: new  Abstract: With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model's robustness against such attacks. The empirical results reveal that the current detection mode
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#65292;&#36890;&#36807;&#32534;&#36753;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#26469;&#25913;&#21892;CodeLLMs&#22312;&#20195;&#30721;&#31867;&#22411;&#39044;&#27979;&#20013;&#23545;&#20110;&#35821;&#27861;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;Python&#21644;TypeScript&#30340;&#31867;&#22411;&#39044;&#27979;&#65292;&#23558;&#31867;&#22411;&#35823;&#24046;&#29575;&#32416;&#27491;&#39640;&#36798;90%&#12290;</title><link>https://arxiv.org/abs/2404.01903</link><description>&lt;p&gt;
&#22312;CodeLLMs&#20013;&#23454;&#29616;&#31867;&#22411;&#39044;&#27979;&#30340;&#40065;&#26834;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Activation Steering for Robust Type Prediction in CodeLLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01903
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#65292;&#36890;&#36807;&#32534;&#36753;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#26469;&#25913;&#21892;CodeLLMs&#22312;&#20195;&#30721;&#31867;&#22411;&#39044;&#27979;&#20013;&#23545;&#20110;&#35821;&#27861;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;Python&#21644;TypeScript&#30340;&#31867;&#22411;&#39044;&#27979;&#65292;&#23558;&#31867;&#22411;&#35823;&#24046;&#29575;&#32416;&#27491;&#39640;&#36798;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#20195;&#30721;&#19978;&#30340;&#29616;&#20195;LLMs&#33021;&#22815;&#25104;&#21151;&#22320;&#23436;&#25104;&#21508;&#31181;&#32534;&#31243;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#35821;&#27861;&#29305;&#24449;&#38750;&#24120;&#25935;&#24863;&#65292;&#20363;&#22914;&#21464;&#37327;&#21644;&#31867;&#22411;&#30340;&#21517;&#31216;&#12289;&#20195;&#30721;&#32467;&#26500;&#20197;&#21450;&#31867;&#22411;&#25552;&#31034;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25216;&#26415;&#65292;&#20351;CodeLLMs&#26356;&#33021;&#25269;&#24481;&#35821;&#27861;&#24178;&#25200;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#19982;&#35821;&#20041;&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#28608;&#27963;&#23548;&#21521;&#65292;&#28041;&#21450;&#32534;&#36753;&#20869;&#37096;&#27169;&#22411;&#28608;&#27963;&#20197;&#23558;&#27169;&#22411;&#24341;&#23548;&#21040;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#31361;&#21464;&#27979;&#35797;&#20013;&#27762;&#21462;&#28789;&#24863;&#26500;&#24314;&#28608;&#27963;&#21521;&#37327;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#26368;&#23567;&#30340;&#30772;&#22351;&#35821;&#20041;&#30340;&#20195;&#30721;&#32534;&#36753;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#20174;&#20445;&#30041;&#35821;&#20041;&#30340;&#20195;&#30721;&#32534;&#36753;&#20013;&#26500;&#24314;&#28608;&#27963;&#21521;&#37327;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#36880;&#28176;&#31867;&#22411;&#21270;&#35821;&#35328;Python&#21644;TypeScript&#30340;&#31867;&#22411;&#39044;&#27979;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#32416;&#27491;&#39640;&#36798;90%&#30340;&#31867;&#22411;&#38169;&#35823;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01903v1 Announce Type: new  Abstract: Contemporary LLMs pretrained on code are capable of succeeding at a wide variety of programming tasks. However, their performance is very sensitive to syntactic features, such as the names of variables and types, the structure of code, and presence of type hints. We contribute an inference-time technique to make CodeLLMs more robust to syntactic distractors that are semantically irrelevant. Our methodology relies on activation steering, which involves editing internal model activations to steer the model towards the correct prediction. We contribute a novel way to construct steering vectors by taking inspiration from mutation testing, which constructs minimal semantics-breaking code edits. In contrast, we construct steering vectors from semantics-preserving code edits. We apply our approach to the task of type prediction for the gradually typed languages Python and TypeScript. This approach corrects up to 90% of type mispredictions. Fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#20102;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01869</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;--&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#20102;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#28041;&#21450;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#28608;&#28872;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#28145;&#24230;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#37096;&#20998;&#28304;&#33258;&#23545;&#27169;&#22411;&#25512;&#29702;&#34892;&#20026;&#30340;&#28145;&#20837;&#35843;&#26597;&#32780;&#38750;&#20165;&#20165;&#36890;&#36807;&#34920;&#38754;&#20934;&#30830;&#24615;&#25351;&#26631;&#26469;&#34913;&#37327;&#20219;&#21153;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35780;&#20272;LLMs&#25512;&#29702;&#34892;&#20026;&#30340;&#20027;&#35201;&#26041;&#27861;&#35770;&#65292;&#24378;&#35843;&#20102;&#24403;&#21069;&#23545;&#26356;&#32454;&#33268;&#25512;&#29702;&#20998;&#26512;&#30340;&#36235;&#21183;&#21644;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#34920;&#26126;&#65292;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01869v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rat
&lt;/p&gt;</description></item><item><title>Self-StrAE &#22312;&#33258;&#32534;&#30721;&#22120;&#20013;&#28155;&#21152;&#37325;&#26500;&#20316;&#20026;&#36741;&#21161;&#30446;&#26631;&#65292;&#21516;&#26102;&#22686;&#21152;&#29420;&#31435;&#36890;&#36947;&#25968;&#37327;&#20197;&#26174;&#33879;&#25552;&#39640;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#25104;&#21151;&#23558;&#38750;&#23884;&#20837;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#21040;&#19971;&#20010;&#12290;</title><link>https://arxiv.org/abs/2404.01860</link><description>&lt;p&gt;
Self-StrAE&#22312;SemEval-2024&#20219;&#21153;1&#20013;&#65306;&#35753;&#33258;&#25105;&#32467;&#26500;&#21270;&#33258;&#32534;&#30721;&#22120;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#23398;&#20064;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn More With Less
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01860
&lt;/p&gt;
&lt;p&gt;
Self-StrAE &#22312;&#33258;&#32534;&#30721;&#22120;&#20013;&#28155;&#21152;&#37325;&#26500;&#20316;&#20026;&#36741;&#21161;&#30446;&#26631;&#65292;&#21516;&#26102;&#22686;&#21152;&#29420;&#31435;&#36890;&#36947;&#25968;&#37327;&#20197;&#26174;&#33879;&#25552;&#39640;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#25104;&#21151;&#23558;&#38750;&#23884;&#20837;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#21040;&#19971;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#39033;&#23545;&#33258;&#25105;&#32467;&#26500;&#21270;&#33258;&#32534;&#30721;&#22120;(Self-StrAE)&#36827;&#34892;&#25913;&#36827;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#23558;&#37325;&#26500;&#20316;&#20026;&#36741;&#21161;&#30446;&#26631;&#21253;&#25324;&#21040;&#35789;&#27719;&#20013;&#21487;&#20197;&#25552;&#39640;&#34920;&#31034;&#36136;&#37327;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#22686;&#21152;&#29420;&#31435;&#36890;&#36947;&#30340;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23884;&#20837;&#36136;&#37327;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#36235;&#21183;&#21487;&#20197;&#34987;&#26497;&#31471;&#22320;&#36981;&#24490;&#65292;&#29978;&#33267;&#21487;&#20197;&#23558;&#38750;&#23884;&#20837;&#21442;&#25968;&#30340;&#24635;&#25968;&#20943;&#23569;&#33267;&#19971;&#20010;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#33267;&#23569;1000&#19975;&#20010;&#36755;&#20837;&#25968;&#25454;&#26631;&#35760;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#21335;&#38750;&#33655;&#20848;&#35821;&#20043;&#38388;&#34920;&#29616;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01860v1 Announce Type: new  Abstract: This paper presents two simple improvements to the Self-Structuring AutoEncoder (Self-StrAE). Firstly, we show that including reconstruction to the vocabulary as an auxiliary objective improves representation quality. Secondly, we demonstrate that increasing the number of independent channels leads to significant improvements in embedding quality, while simultaneously reducing the number of parameters. Surprisingly, we demonstrate that this trend can be followed to the extreme, even to point of reducing the total number of non-embedding parameters to seven. Our system can be pre-trained from scratch with as little as 10M tokens of input data, and proves effective across English, Spanish and Afrikaans.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#30740;&#31350;&#21457;&#29616;&#35838;&#31243;&#35780;&#20215;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#27604;&#36739;&#33521;&#35821;&#21644;&#29790;&#20856;&#35838;&#31243;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;&#35838;&#31243;&#30340;&#35780;&#20215;&#20027;&#35266;&#24615;&#22240;&#32771;&#23448;&#24615;&#21035;&#32780;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.01857</link><description>&lt;p&gt;
&#22312;&#35838;&#31243;&#35780;&#20215;&#20013;&#26816;&#27979;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Detecting Gender Bias in Course Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01857
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#30740;&#31350;&#21457;&#29616;&#35838;&#31243;&#35780;&#20215;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#27604;&#36739;&#33521;&#35821;&#21644;&#29790;&#20856;&#35838;&#31243;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;&#35838;&#31243;&#30340;&#35780;&#20215;&#20027;&#35266;&#24615;&#22240;&#32771;&#23448;&#24615;&#21035;&#32780;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#30805;&#22763;&#35770;&#25991;&#30740;&#31350;&#20013;&#21457;&#29616;&#35838;&#31243;&#35780;&#20215;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#25130;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#25506;&#32034;&#25968;&#25454;&#65292;&#21457;&#29616;&#23398;&#29983;&#23545;&#35838;&#31243;&#30340;&#35780;&#20215;&#20250;&#22240;&#32771;&#23448;&#30340;&#24615;&#21035;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#26469;&#33258;&#33521;&#35821;&#21644;&#29790;&#20856;&#35838;&#31243;&#30340;&#25968;&#25454;&#65292;&#20197;&#25429;&#25417;&#21487;&#33021;&#23384;&#22312;&#30340;&#24615;&#21035;&#20559;&#35265;&#20013;&#26356;&#22810;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#21040;&#30446;&#21069;&#20026;&#27490;&#30340;&#24037;&#20316;&#32467;&#26524;&#65292;&#20294;&#36825;&#26159;&#19968;&#20010;&#27491;&#22312;&#36827;&#34892;&#30340;&#39033;&#30446;&#65292;&#36824;&#26377;&#26356;&#22810;&#30340;&#24037;&#20316;&#35201;&#20570;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01857v1 Announce Type: cross  Abstract: An outtake from the findnings of a master thesis studying gender bias in course evaluations through the lense of machine learning and nlp. We use different methods to examine and explore the data and find differences in what students write about courses depending on gender of the examiner. Data from English and Swedish courses are evaluated and compared, in order to capture more nuance in the gender bias that might be found. Here we present the results from the work so far, but this is an ongoing project and there is more work to do.
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#31181;&#35757;&#32451;&#30340;Poro 34B&#27169;&#22411;&#22312;&#33452;&#20848;&#35821;&#31561;&#23567;&#35821;&#31181;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#20855;&#26377;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01856</link><description>&lt;p&gt;
Poro 34B&#21644;&#22810;&#35821;&#31181;&#30340;&#31069;&#31119;
&lt;/p&gt;
&lt;p&gt;
Poro 34B and the Blessing of Multilinguality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01856
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#35757;&#32451;&#30340;Poro 34B&#27169;&#22411;&#22312;&#33452;&#20848;&#35821;&#31561;&#23567;&#35821;&#31181;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#20855;&#26377;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#29616;&#22312;&#38656;&#35201;&#25968;&#19975;&#20159;&#23383;&#30340;&#25991;&#26412;&#65292;&#36825;&#27604;&#32477;&#22823;&#22810;&#25968;&#35821;&#35328;&#21487;&#33719;&#24471;&#30340;&#25991;&#26412;&#25968;&#37327;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#23613;&#31649;&#21253;&#21547;&#22810;&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#26159;&#33719;&#21462;&#26356;&#22810;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#26126;&#26174;&#26041;&#27861;&#65292;&#20294;&#22810;&#35821;&#31181;&#24448;&#24448;&#34987;&#35270;&#20026;&#19968;&#31181;&#35781;&#21650;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#35757;&#32451;&#24037;&#20316;&#20173;&#28982;&#20027;&#35201;&#38598;&#20013;&#22312;&#20010;&#21035;&#22823;&#35821;&#31181;&#19978;&#12290;&#25105;&#20204;&#30456;&#20449;&#22810;&#35821;&#31181;&#21487;&#20197;&#26159;&#19968;&#31181;&#31069;&#31119;&#65292;&#24182;&#19988;&#24212;&#35813;&#26377;&#21487;&#33021;&#36890;&#36807;&#22810;&#35821;&#31181;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#23567;&#35821;&#31181;&#30340;&#27169;&#22411;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Poro 34B&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;1&#19975;&#20159;&#20010;&#33452;&#20848;&#35821;&#12289;&#33521;&#35821;&#21644;&#32534;&#31243;&#35821;&#35328;&#26631;&#35760;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#25317;&#26377;340&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#22810;&#35821;&#31181;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#27169;&#22411;&#65292;&#19981;&#20165;&#22312;&#33452;&#20848;&#35821;&#30340;&#29616;&#26377;&#27169;&#22411;&#33021;&#21147;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#32780;&#19988;&#22312;&#34920;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01856v1 Announce Type: new  Abstract: The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;IndoCulture&#39033;&#30446;&#65292;&#26088;&#22312;&#36890;&#36807;&#24403;&#22320;&#20154;&#25163;&#21160;&#25910;&#38598;&#25968;&#25454;&#65292;&#25506;&#32034;&#21360;&#23612;&#21313;&#19968;&#20010;&#30465;&#20221;&#38388;&#22320;&#29702;&#24433;&#21709;&#30340;&#25991;&#21270;&#24120;&#35782;&#25512;&#29702;&#12290;&#35780;&#20272;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#22312;&#29305;&#23450;&#30465;&#20221;&#19978;&#34920;&#29616;&#26356;&#20934;&#30830;&#65292;&#32780;&#28155;&#21152;&#22320;&#29702;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01854</link><description>&lt;p&gt;
IndoCulture: &#25506;&#32034;&#21360;&#23612;&#21313;&#19968;&#20010;&#30465;&#20221;&#38388;&#22320;&#29702;&#24433;&#21709;&#30340;&#25991;&#21270;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;IndoCulture&#39033;&#30446;&#65292;&#26088;&#22312;&#36890;&#36807;&#24403;&#22320;&#20154;&#25163;&#21160;&#25910;&#38598;&#25968;&#25454;&#65292;&#25506;&#32034;&#21360;&#23612;&#21313;&#19968;&#20010;&#30465;&#20221;&#38388;&#22320;&#29702;&#24433;&#21709;&#30340;&#25991;&#21270;&#24120;&#35782;&#25512;&#29702;&#12290;&#35780;&#20272;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#22312;&#29305;&#23450;&#30465;&#20221;&#19978;&#34920;&#29616;&#26356;&#20934;&#30830;&#65292;&#32780;&#28155;&#21152;&#22320;&#29702;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24120;&#35782;&#25512;&#29702;&#21463;&#25991;&#21270;&#21644;&#22320;&#29702;&#22240;&#32032;&#30340;&#26497;&#22823;&#24433;&#21709;&#65292;&#20808;&#21069;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#25991;&#21270;&#19978;&#65292;&#21487;&#33021;&#23548;&#33268;&#19968;&#31181;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IndoCulture&#65292;&#26088;&#22312;&#29702;&#35299;&#22320;&#29702;&#22240;&#32032;&#23545;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#21313;&#19968;&#20010;&#21360;&#23612;&#30465;&#20221;&#20869;&#25152;&#21457;&#29616;&#30340;&#22810;&#26679;&#25991;&#21270;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#27169;&#26495;&#65288;Yin&#31561;&#65292;2022&#65289;&#21644;&#22312;&#32447;&#25235;&#21462;&#65288;Fung&#31561;&#65292;2024&#65289;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#35810;&#38382;&#24403;&#22320;&#20154;&#25163;&#21160;&#24320;&#21457;&#39044;&#23450;&#20041;&#20027;&#39064;&#30340;&#19978;&#19979;&#25991;&#21644;&#21512;&#29702;&#36873;&#39033;&#26469;&#21019;&#24314;IndoCulture&#12290;&#23545;23&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#20960;&#20010;&#35265;&#35299;&#65306;&#65288;1&#65289;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#24320;&#28304;&#27169;&#22411;&#20063;&#38590;&#20197;&#36798;&#21040;53.2&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#65288;2&#65289;&#27169;&#22411;&#36890;&#24120;&#20026;&#29305;&#23450;&#30465;&#20221;&#65288;&#22914;&#24052;&#21400;&#23707;&#21644;&#35199;&#29226;&#21703;&#65289;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#65288;3&#65289;&#21253;&#21547;&#22320;&#29702;&#20449;&#24687;&#21487;&#26174;&#30528;&#25913;&#21892;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01854v1 Announce Type: new  Abstract: Although commonsense reasoning is greatly shaped by cultural and geographical factors, previous studies on language models have predominantly centered on English cultures, potentially resulting in an Anglocentric bias. In this paper, we introduce IndoCulture, aimed at understanding the influence of geographical factors on language model reasoning ability, with a specific emphasis on the diverse cultures found within eleven Indonesian provinces. In contrast to prior works that relied on templates (Yin et al., 2022) and online scrapping (Fung et al., 2024), we created IndoCulture by asking local people to manually develop the context and plausible options based on predefined topics. Evaluations of 23 language models reveal several insights: (1) even the best open-source model struggles with an accuracy of 53.2%, (2) models often provide more accurate predictions for specific provinces, such as Bali and West Java, and (3) the inclusion of l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#31038;&#21306;&#27169;&#22411;&#27867;&#21270;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#27979;&#35797;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01822</link><description>&lt;p&gt;
&#38754;&#21521;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#31038;&#21306;&#27169;&#22411;&#27867;&#21270;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#35780;&#20272;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01822
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#31038;&#21306;&#27169;&#22411;&#27867;&#21270;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#27979;&#35797;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#30340;&#31038;&#21306;&#27169;&#22411;&#32771;&#34385;&#31038;&#20132;&#22270;&#20013;&#30340;&#19978;&#19979;&#25991;&#20197;&#21450;&#20869;&#23481;&#26412;&#36523;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#34394;&#20551;&#20449;&#24687;&#21644;&#20167;&#24680;&#35328;&#35770;&#20173;&#22312;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#19978;&#20256;&#25773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25105;&#20204;&#30340;&#23569;&#26679;&#26412;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#30340;&#27169;&#22411;&#27867;&#21270;&#30340;&#26032;&#22411;&#35780;&#20272;&#35774;&#32622;&#12290;&#36825;&#20010;&#35774;&#32622;&#36890;&#36807;&#22312;&#26356;&#22823;&#22270;&#30340;&#23616;&#37096;&#25506;&#32034;&#20013;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#65292;&#27169;&#25311;&#26356;&#20026;&#29616;&#23454;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#20197;&#27979;&#35797;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01822v1 Announce Type: cross  Abstract: Community models for malicious content detection, which take into account the context from a social graph alongside the content itself, have shown remarkable performance on benchmark datasets. Yet, misinformation and hate speech continue to propagate on social media networks. This mismatch can be partially attributed to the limitations of current evaluation setups that neglect the rapid evolution of online content and the underlying social graph. In this paper, we propose a novel evaluation setup for model generalisation based on our few-shot subgraph sampling approach. This setup tests for generalisation through few labelled examples in local explorations of a larger graph, emulating more realistic application settings. We show this to be a challenging inductive setup, wherein strong performance on the training graph is not indicative of performance on unseen tasks, domains, or graph structures. Lastly, we show that graph meta-learner
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;ChatGPT&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#25581;&#31034;&#20102;&#31185;&#23398;&#25991;&#31456;&#24341;&#29992;&#20013;&#30340;&#20559;&#35265;&#21644;&#21033;&#30410;&#20914;&#31361;&#65292;&#22686;&#24378;&#20102;&#31185;&#23398;&#25991;&#29486;&#35780;&#20272;&#30340;&#23458;&#35266;&#24615;&#21644;&#21487;&#38752;&#24615;</title><link>https://arxiv.org/abs/2404.01800</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#23545;&#31185;&#23398;&#25991;&#31456;&#24341;&#29992;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65306;&#35782;&#21035;&#28508;&#22312;&#20559;&#35265;&#21644;&#21033;&#30410;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01800
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#25581;&#31034;&#20102;&#31185;&#23398;&#25991;&#31456;&#24341;&#29992;&#20013;&#30340;&#20559;&#35265;&#21644;&#21033;&#30410;&#20914;&#31361;&#65292;&#22686;&#24378;&#20102;&#31185;&#23398;&#25991;&#29486;&#35780;&#20272;&#30340;&#23458;&#35266;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25991;&#31456;&#22312;&#25512;&#21160;&#30693;&#35782;&#21457;&#23637;&#21644;&#25351;&#23548;&#30740;&#31350;&#26041;&#21521;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#35780;&#20272;&#31185;&#23398;&#25991;&#31456;&#30340;&#20851;&#38190;&#26041;&#38754;&#20043;&#19968;&#26159;&#23545;&#24341;&#25991;&#36827;&#34892;&#20998;&#26512;&#65292;&#36825;&#25552;&#20379;&#20102;&#23545;&#34987;&#24341;&#29992;&#20316;&#21697;&#30340;&#24433;&#21709;&#21644;&#25509;&#21463;&#31243;&#24230;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;ChatGPT&#65292;&#22312;&#31185;&#23398;&#25991;&#31456;&#20013;&#23545;&#24341;&#25991;&#36827;&#34892;&#20840;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;ChatGPT&#33021;&#22815;&#36776;&#21035;&#24341;&#25991;&#30340;&#24494;&#22937;&#31215;&#26497;&#25110;&#28040;&#26497;&#24773;&#24863;&#65292;&#25552;&#20379;&#23545;&#34987;&#24341;&#29992;&#20316;&#21697;&#30340;&#25509;&#21463;&#31243;&#24230;&#21644;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#33021;&#21147;&#36824;&#21253;&#25324;&#26816;&#27979;&#24341;&#25991;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#21644;&#21033;&#30410;&#20914;&#31361;&#65292;&#22686;&#24378;&#31185;&#23398;&#25991;&#29486;&#35780;&#20272;&#30340;&#23458;&#35266;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#24037;&#20855;&#22312;&#22686;&#24378;&#24341;&#25991;&#20998;&#26512;&#21644;&#25552;&#39640;&#31185;&#23398;&#25991;&#29486;&#35780;&#20272;&#26041;&#38754;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01800v1 Announce Type: cross  Abstract: Scientific articles play a crucial role in advancing knowledge and informing research directions. One key aspect of evaluating scientific articles is the analysis of citations, which provides insights into the impact and reception of the cited works. This article introduces the innovative use of large language models, particularly ChatGPT, for comprehensive sentiment analysis of citations within scientific articles. By leveraging advanced natural language processing (NLP) techniques, ChatGPT can discern the nuanced positivity or negativity of citations, offering insights into the reception and impact of cited works. Furthermore, ChatGPT's capabilities extend to detecting potential biases and conflicts of interest in citations, enhancing the objectivity and reliability of scientific literature evaluation. This study showcases the transformative potential of artificial intelligence (AI)-powered tools in enhancing citation analysis and pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PATCH&#65292;&#29992;&#20110;&#23558;&#24515;&#29702;&#27979;&#37327;&#39046;&#22495;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#30340;&#27979;&#37327;&#36136;&#37327;&#12289;&#39033;&#30446;&#32423;&#21035;&#35780;&#20272;&#21644;&#21442;&#32771;&#20154;&#32676;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01799</link><description>&lt;p&gt;
PATCH -- &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#27979;&#37327;&#36741;&#21161;&#22522;&#20934;&#27979;&#35797;&#65306;&#25968;&#23398;&#33021;&#21147;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PATCH&#65292;&#29992;&#20110;&#23558;&#24515;&#29702;&#27979;&#37327;&#39046;&#22495;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#30340;&#27979;&#37327;&#36136;&#37327;&#12289;&#39033;&#30446;&#32423;&#21035;&#35780;&#20272;&#21644;&#21442;&#32771;&#20154;&#32676;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#22823;&#22411;&#65288;&#22810;&#27169;&#24577;&#65289;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22522;&#20934;&#27979;&#35797;&#30528;&#37325;&#20110;&#34913;&#37327;LLMs&#30340;&#23398;&#26415;&#33021;&#21147;&#65292;&#36890;&#24120;&#20063;&#23545;&#27604;&#36739;&#27169;&#22411;&#24615;&#33021;&#19982;&#20154;&#31867;&#32771;&#35797;&#32773;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#23545;LLMs&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#26377;&#38382;&#39064;&#30340;&#27979;&#37327;&#36136;&#37327;&#65288;&#20363;&#22914;&#65292;&#23427;&#20204;&#26159;&#21542;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#34913;&#37327;&#25152;&#38656;&#30340;&#20869;&#23481;&#65311;&#65289;&#12289;&#32570;&#20047;&#39033;&#30446;&#32423;&#21035;&#30340;&#36136;&#37327;&#35780;&#20272;&#65288;&#20363;&#22914;&#65292;&#26377;&#20123;&#39033;&#30446;&#26159;&#21542;&#27604;&#20854;&#20182;&#26356;&#37325;&#35201;&#25110;&#26356;&#22256;&#38590;&#65311;&#65289;&#20197;&#21450;&#20154;&#31867;&#20154;&#21475;&#21442;&#29031;&#27169;&#31946;&#65288;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#19982;&#35841;&#36827;&#34892;&#27604;&#36739;&#65311;&#65289;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#8212;&#8212;&#19968;&#38376;&#33268;&#21147;&#20110;&#27979;&#37327;&#28508;&#22312;&#21464;&#37327;&#22914;&#23398;&#26415;&#33021;&#21147;&#30340;&#39046;&#22495;&#8212;&#8212;&#26469;&#36827;&#34892;LLMs&#22522;&#20934;&#27979;&#35797;&#30340;&#24515;&#29702;&#27979;&#37327;&#36741;&#21161;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#19977;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PATCH&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#27979;&#37327;&#36741;&#21161;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01799v1 Announce Type: new  Abstract: Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers. While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking. We make three primary contributions. First, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#20256;&#32479;&#21644;&#29616;&#20195;&#30340;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#27599;&#31181;&#26041;&#27861;&#20248;&#21183;&#12289;&#21155;&#21183;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#23453;&#36149;&#35265;&#35299;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2404.01786</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;GPT-2&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;AI&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#20256;&#32479;&#21644;&#29616;&#20195;&#30340;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#27599;&#31181;&#26041;&#27861;&#20248;&#21183;&#12289;&#21155;&#21183;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#23453;&#36149;&#35265;&#35299;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#28145;&#20837;&#25506;&#35752;&#20102;&#33258;&#21160;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#65292;&#25506;&#32034;&#20102;&#20174;&#20256;&#32479;&#30830;&#23450;&#24615;&#26041;&#27861;&#21040;&#26356;&#29616;&#20195;&#30340;&#38543;&#26426;&#26041;&#27861;&#30340;&#21508;&#31181;&#25216;&#26415;&#12290;&#36890;&#36807;&#20998;&#26512;&#36138;&#23146;&#25628;&#32034;&#12289;&#26463;&#25628;&#32034;&#12289;top-k&#25277;&#26679;&#12289;top-p&#25277;&#26679;&#12289;&#23545;&#27604;&#25628;&#32034;&#21644;&#26412;&#22320;&#20856;&#22411;&#25628;&#32034;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12289;&#21155;&#21183;&#21644;&#28508;&#22312;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#27599;&#31181;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#37117;&#20351;&#29992;&#20960;&#20010;&#26631;&#20934;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#36824;&#30830;&#23450;&#20102;&#33258;&#21160;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30740;&#31350;&#30340;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01786v1 Announce Type: new  Abstract: This work delved into the realm of automatic text generation, exploring a variety of techniques ranging from traditional deterministic approaches to more modern stochastic methods. Through analysis of greedy search, beam search, top-k sampling, top-p sampling, contrastive searching, and locally typical searching, this work has provided valuable insights into the strengths, weaknesses, and potential applications of each method. Each text-generating method is evaluated using several standard metrics and a comparative study has been made on the performance of the approaches. Finally, some future directions of research in the field of automatic text generation are also identified.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#22312;&#35782;&#21035;&#30456;&#20851;&#25991;&#26412;&#23646;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#29305;&#21035;&#26159;&#27969;&#27966;&#21644;&#20027;&#39064;&#30340;&#27010;&#24565;&#65292;&#20197;&#35299;&#26512;&#25991;&#26412;&#39046;&#22495;&#30340;&#26680;&#24515;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2404.01785</link><description>&lt;p&gt;
&#20154;&#31867;&#33021;&#21542;&#35782;&#21035;&#39046;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Humans Identify Domains?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#22312;&#35782;&#21035;&#30456;&#20851;&#25991;&#26412;&#23646;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#29305;&#21035;&#26159;&#27969;&#27966;&#21644;&#20027;&#39064;&#30340;&#27010;&#24565;&#65292;&#20197;&#35299;&#26512;&#25991;&#26412;&#39046;&#22495;&#30340;&#26680;&#24515;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39046;&#22495;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#20013;&#30340;&#37325;&#35201;&#23646;&#24615;&#65292;&#22240;&#20854;&#23545;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#35813;&#27010;&#24565;&#26412;&#36523;&#23450;&#20041;&#19981;&#28165;&#26224;&#65292;&#22312;&#23454;&#36341;&#20013;&#28041;&#21450;&#21040;&#35832;&#22914;&#25991;&#26723;&#30340;&#27969;&#27966;&#12289;&#20027;&#39064;&#12289;&#23186;&#20171;&#25110;&#39118;&#26684;&#31561;&#38750;&#31867;&#22411;&#23398;&#23646;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20154;&#31867;&#22312;&#35782;&#21035;&#30456;&#20851;&#20869;&#22312;&#25991;&#26412;&#23646;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#24230;&#65292;&#29305;&#21035;&#26159;&#27969;&#27966;&#65288;&#20132;&#38469;&#30446;&#30340;&#65289;&#21644;&#20027;&#39064;&#65288;&#20027;&#39064;&#20869;&#23481;&#65289;&#30340;&#27010;&#24565;&#65292;&#26469;&#35843;&#26597;&#39046;&#22495;&#30340;&#26680;&#24515;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;*TGeGUM*&#20013;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#27880;&#37322;&#65306;&#36825;&#26159;&#26469;&#33258;GUM&#25968;&#25454;&#38598;&#65288;Zeldes, 2017&#65289;&#30340;9.1k&#21477;&#23376;&#30340;&#38598;&#21512;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#38024;&#23545;11&#31181;&#27969;&#27966;&#65288;&#28304;&#31867;&#22411;&#65289;&#30340;&#21333;&#21477;&#21644;&#36739;&#22823;&#32972;&#26223;&#65288;&#21363;&#25955;&#25991;&#65289;&#27880;&#37322;&#65292;&#20197;&#21450;&#25353;&#29031;&#26460;&#23041;&#21313;&#36827;&#21046;&#22270;&#20070;&#39302;&#20998;&#31867;&#31995;&#32479;&#65288;Dewey, 1979&#65289;&#30340;&#20027;&#39064;/&#23376;&#20027;&#39064;&#65292;&#21253;&#25324;&#20102;10/100&#20010;&#36882;&#22686;&#31890;&#24230;&#30340;&#20998;&#23618;&#20027;&#39064;&#12290;&#27599;&#20010;&#23454;&#20363;&#37117;&#30001;&#19977;&#20010;&#27880;&#37322;&#32773;&#27880;&#37322;&#65292;&#20849;32.7k&#26465;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01785v1 Announce Type: new  Abstract: Textual domain is a crucial property within the Natural Language Processing (NLP) community due to its effects on downstream model performance. The concept itself is, however, loosely defined and, in practice, refers to any non-typological property, such as genre, topic, medium or style of a document. We investigate the core notion of domains via human proficiency in identifying related intrinsic textual properties, specifically the concepts of genre (communicative purpose) and topic (subject matter). We publish our annotations in *TGeGUM*: A collection of 9.1k sentences from the GUM dataset (Zeldes, 2017) with single sentence and larger context (i.e., prose) annotations for one of 11 genres (source type), and its topic/subtopic as per the Dewey Decimal library classification system (Dewey, 1979), consisting of 10/100 hierarchical topics of increased granularity. Each instance is annotated by three annotators, for a total of 32.7k annota
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;Multi-Grain Stereotype&#65288;MGS&#65289;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24314;&#31435;&#38472;&#35268;&#26816;&#27979;&#30340;&#22522;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MGS&#25968;&#25454;&#35757;&#32451;&#30340;&#33521;&#25991;&#25991;&#26412;&#30340;&#38472;&#35268;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.01768</link><description>&lt;p&gt;
&#29992;&#20110;&#22686;&#24378;&#22522;&#20110;&#25991;&#26412;&#30340;&#38472;&#35268;&#26816;&#27979;&#21644;&#22522;&#20110;&#25506;&#27979;&#30340;&#20559;&#35265;&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01768
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;Multi-Grain Stereotype&#65288;MGS&#65289;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24314;&#31435;&#38472;&#35268;&#26816;&#27979;&#30340;&#22522;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MGS&#25968;&#25454;&#35757;&#32451;&#30340;&#33521;&#25991;&#25991;&#26412;&#30340;&#38472;&#35268;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#38754;&#21521;&#20154;&#31867;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#20250;&#22797;&#21046;&#29978;&#33267;&#21152;&#21095;&#33258;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38472;&#35268;&#36755;&#20986;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Multi-Grain Stereotype&#65288;MGS&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;51,867&#20010;&#23454;&#20363;&#65292;&#28085;&#30422;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#32844;&#19994;&#12289;&#23447;&#25945;&#21644;&#38472;&#35268;&#25991;&#26412;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#20808;&#21069;&#20844;&#24320;&#30340;&#38472;&#35268;&#26816;&#27979;&#25968;&#25454;&#38598;&#25910;&#38598;&#32780;&#26469;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26088;&#22312;&#20026;&#38472;&#35268;&#26816;&#27979;&#24314;&#31435;&#22522;&#32447;&#30340;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24494;&#35843;&#20102;&#22810;&#31181;&#26550;&#26500;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MGS&#35757;&#32451;&#30340;&#33521;&#25991;&#25991;&#26412;&#30340;&#38472;&#35268;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;&#20026;&#20102;&#20102;&#35299;&#25105;&#20204;&#30340;&#38472;&#35268;&#26816;&#27979;&#22120;&#26159;&#21542;&#25429;&#25417;&#21040;&#19982;&#20154;&#31867;&#24120;&#35782;&#19968;&#33268;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#21508;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01768v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) have significantly increased their presence in human-facing Artificial Intelligence (AI) applications. However, LLMs could reproduce and even exacerbate stereotypical outputs from training data. This work introduces the Multi-Grain Stereotype (MGS) dataset, encompassing 51,867 instances across gender, race, profession, religion, and stereotypical text, collected by fusing multiple previously publicly available stereotype detection datasets. We explore different machine learning approaches aimed at establishing baselines for stereotype detection, and fine-tune several language models of various architectures and model sizes, presenting in this work a series of stereotypes classifier models for English text trained on MGS. To understand whether our stereotype detectors capture relevant features (aligning with human common sense) we utilise a variety of explanainable AI tools, including 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;Prompt-KD&#26041;&#27861;&#35299;&#20915;&#20102;&#32769;&#30693;&#35782;&#36951;&#24536;&#21644;&#26032;&#31867;&#21035;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01767</link><description>&lt;p&gt;
&#22686;&#37327;&#24335;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Few-Shot Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01767
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;Prompt-KD&#26041;&#27861;&#35299;&#20915;&#20102;&#32769;&#30693;&#35782;&#36951;&#24536;&#21644;&#26032;&#31867;&#21035;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26816;&#27979;&#26159;&#20449;&#24687;&#25277;&#21462;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#29616;&#23454;&#30340;&#20107;&#20214;&#26816;&#27979;&#31995;&#32479;&#32463;&#24120;&#38656;&#35201;&#19981;&#26029;&#22788;&#29702;&#26032;&#30340;&#20107;&#20214;&#31867;&#21035;&#12290;&#36825;&#20123;&#26032;&#31867;&#21035;&#36890;&#24120;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#23454;&#20363;&#65292;&#22240;&#20026;&#26631;&#27880;&#22823;&#37327;&#26410;&#26631;&#35760;&#23454;&#20363;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#31216;&#20026;&#22686;&#37327;&#24335;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#38754;&#20020;&#30528;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#32769;&#30693;&#35782;&#36951;&#24536;&#21644;&#26032;&#31867;&#21035;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#21644;&#25552;&#31034;&#23398;&#20064;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;Prompt-KD&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#22788;&#29702;&#20851;&#20110;&#32769;&#30693;&#35782;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;Prompt-KD&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20854;&#20013;&#22312;&#25152;&#26377;&#23398;&#20064;&#20250;&#35805;&#20013;&#37325;&#22797;&#20351;&#29992;&#22312;&#22522;&#30784;&#31867;&#21035;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#31062;&#20808;&#25945;&#24072;&#27169;&#22411;&#65292;&#29238;&#25945;&#24072;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01767v1 Announce Type: new  Abstract: Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#25512;&#29305;&#24773;&#24863;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#22810;&#27169;&#24577;&#26684;&#24335;&#65292;&#24182;&#36827;&#34892;&#22522;&#20934;&#23454;&#39564;&#65292;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20351;&#29992;&#24773;&#24863;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#26102;&#65292;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2404.01753</link><description>&lt;p&gt;
M2SA: &#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#25512;&#29305;&#24773;&#24863;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#22810;&#27169;&#24577;&#26684;&#24335;&#65292;&#24182;&#36827;&#34892;&#22522;&#20934;&#23454;&#39564;&#65292;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20351;&#29992;&#24773;&#24863;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#26102;&#65292;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38754;&#21521;&#23398;&#20064;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20998;&#26512;&#22810;&#27169;&#24577;&#20219;&#21153;&#26102;&#38656;&#35201;&#26356;&#28165;&#26224;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#20808;&#21069;&#20851;&#20110;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#20294;&#26412;&#25991;&#36890;&#36807;&#31616;&#21333;&#30340;&#25972;&#29702;&#36807;&#31243;&#23558;&#29616;&#26377;&#30340;&#25991;&#26412;&#25512;&#29305;&#24773;&#24863;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#22810;&#27169;&#24577;&#26684;&#24335;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#20869;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#22686;&#24378;&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#23454;&#39564;&#24182;&#25253;&#21578;&#20102;&#30740;&#31350;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#27604;&#36739;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#37197;&#32622;&#26102;&#65292;&#20351;&#29992;&#24773;&#24863;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01753v1 Announce Type: new  Abstract: In recent years, multimodal natural language processing, aimed at learning from diverse data types, has garnered significant attention. However, there needs to be more clarity when it comes to analysing multimodal tasks in multi-lingual contexts. While prior studies on sentiment analysis of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter sentiment dataset into a multimodal format through a straightforward curation process. Our work opens up new avenues for sentiment-related research within the research community. Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings. Notably, our evaluations reveal that when comparing unimodal and multimodal configurations, using a sentiment-tuned large language model as a text encoder performs exceptionally well.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#36171;&#20104;&#36229;&#36234;GPT-4&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#24615;&#33021;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;95&#65285;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20989;&#25968;&#35843;&#29992;&#20013;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01744</link><description>&lt;p&gt;
Octopus v2&#65306;&#29992;&#20110;&#36229;&#32423;&#20195;&#29702;&#30340;&#35774;&#22791;&#19978;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Octopus v2: On-device language model for super agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#36171;&#20104;&#36229;&#36234;GPT-4&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#24615;&#33021;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;95&#65285;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20989;&#25968;&#35843;&#29992;&#20013;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#36719;&#20214;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#19982;&#33258;&#21160;&#24037;&#20316;&#27969;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#35843;&#29992;&#20989;&#25968;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#22312;&#21019;&#24314;AI&#20195;&#29702;&#26102;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20113;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#30528;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#24403;&#21069;&#29992;&#20110;&#20989;&#25968;&#35843;&#29992;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#38754;&#20020;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#36229;&#36234;&#20102;GPT-4&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;&#20102;95%&#12290;&#19982;&#22522;&#20110;RAG&#30340;&#20989;&#25968;&#35843;&#29992;&#26426;&#21046;&#30340;Llama-7B&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24310;&#36831;&#25552;&#39640;&#20102;35&#20493;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#24310;&#36831;&#38477;&#20302;&#21040;&#36866;&#21512;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#27700;&#24179;&#19978;&#65292;&#31526;&#21512;&#24615;&#33021;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26469;&#33258;Whisper&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#29992;&#20110;&#22312;&#35789;&#27719;&#21709;&#24212;&#27700;&#24179;&#19978;&#36827;&#34892;&#24494;&#35266;&#21487;&#25026;&#24615;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.01737</link><description>&lt;p&gt;
&#20174;Whisper&#36827;&#34892;&#30340;&#24494;&#35266;&#21487;&#25026;&#24615;&#39044;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning from Whisper for Microscopic Intelligibility Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26469;&#33258;Whisper&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#29992;&#20110;&#22312;&#35789;&#27719;&#21709;&#24212;&#27700;&#24179;&#19978;&#36827;&#34892;&#24494;&#35266;&#21487;&#25026;&#24615;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23439;&#35266;&#21487;&#25026;&#24615;&#27169;&#22411;&#39044;&#27979;&#32473;&#23450;&#35821;&#38899;&#22312;&#22122;&#22768;&#20013;&#30340;&#26399;&#26395;&#20154;&#31867;&#35789;&#38169;&#35823;&#29575;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24494;&#35266;&#21487;&#25026;&#24615;&#27169;&#22411;&#26088;&#22312;&#23545;&#21548;&#20247;&#30340;&#24863;&#30693;&#36827;&#34892;&#32454;&#31890;&#24230;&#39044;&#27979;&#65292;&#20363;&#22914;&#39044;&#27979;&#38899;&#32032;&#25110;&#35789;&#27719;&#21709;&#24212;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;Whisper&#36827;&#34892;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#29992;&#20110;&#22312;&#35789;&#27719;&#21709;&#24212;&#27700;&#24179;&#19978;&#36827;&#34892;&#24494;&#35266;&#21487;&#25026;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#32771;&#34385;&#30340;&#22522;&#32447;&#65292;&#29978;&#33267;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#24494;&#35843;&#20197;&#39044;&#27979;&#21548;&#20247;&#21709;&#24212;&#26102;&#30456;&#23545;&#25552;&#21319;&#39640;&#36798;66\%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#24494;&#35266;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01737v1 Announce Type: cross  Abstract: Macroscopic intelligibility models predict the expected human word-error-rate for a given speech-in-noise stimulus. In contrast, microscopic intelligibility models aim to make fine-grained predictions about listeners' perception, e.g. predicting phonetic or lexical responses. State-of-the-art macroscopic models use transfer learning from large scale deep learning models for speech processing, whereas such methods have rarely been used for microscopic modeling. In this paper, we study the use of transfer learning from Whisper, a state-of-the-art deep learning model for automatic speech recognition, for microscopic intelligibility prediction at the level of lexical responses. Our method outperforms the considered baselines, even in a zero-shot setup, and yields a relative improvement of up to 66\% when fine-tuned to predict listeners' responses. Our results showcase the promise of large scale deep learning based methods for microscopic i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#20851;&#31995;&#22270;&#30340;&#21477;&#23376;&#32423;&#23186;&#20307;&#20559;&#35265;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20107;&#20214;&#20851;&#31995;&#22270;&#21644;&#20107;&#20214;&#24863;&#30693;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#20559;&#35265;&#21477;&#30340;&#35782;&#21035;&#21644;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2404.01722</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#20851;&#31995;&#22270;&#30340;&#21477;&#23376;&#32423;&#23186;&#20307;&#20559;&#35265;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sentence-level Media Bias Analysis with Event Relation Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#20851;&#31995;&#22270;&#30340;&#21477;&#23376;&#32423;&#23186;&#20307;&#20559;&#35265;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20107;&#20214;&#20851;&#31995;&#22270;&#21644;&#20107;&#20214;&#24863;&#30693;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#20559;&#35265;&#21477;&#30340;&#35782;&#21035;&#21644;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#26426;&#26500;&#29616;&#22312;&#36234;&#26469;&#36234;&#20826;&#27966;&#21270;&#21644;&#26497;&#21270;&#12290;&#26412;&#25991;&#38024;&#23545;&#21477;&#23376;&#32423;&#21035;&#35782;&#21035;&#23186;&#20307;&#20559;&#35265;&#65292;&#23450;&#20301;&#26088;&#22312;&#24433;&#21709;&#35835;&#32773;&#35266;&#28857;&#30340;&#20559;&#35265;&#21477;&#12290;&#30001;&#20110;&#20559;&#35265;&#21477;&#36890;&#24120;&#20197;&#20013;&#31435;&#23458;&#35266;&#30340;&#26041;&#24335;&#34920;&#36798;&#65292;&#32771;&#34385;&#21477;&#23376;&#20043;&#22806;&#30340;&#26356;&#24191;&#27867;&#19978;&#19979;&#25991;&#21487;&#20197;&#24110;&#21161;&#25581;&#31034;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20559;&#35265;&#21477;&#20013;&#30340;&#20107;&#20214;&#38656;&#35201;&#19982;&#25991;&#26723;&#20013;&#30340;&#20854;&#20182;&#20107;&#20214;&#30456;&#20851;&#32852;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#26500;&#24314;&#19968;&#20010;&#20107;&#20214;&#20851;&#31995;&#22270;&#65292;&#20197;&#26126;&#30830;&#25512;&#29702;&#21477;&#23376;&#32423;&#20559;&#35265;&#35782;&#21035;&#20013;&#30340;&#20107;&#20214;-&#20107;&#20214;&#20851;&#31995;&#12290;&#35774;&#35745;&#30340;&#20107;&#20214;&#20851;&#31995;&#22270;&#30001;&#20107;&#20214;&#20316;&#20026;&#33410;&#28857;&#21644;&#22235;&#31181;&#24120;&#35265;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#65306;&#25351;&#20195;&#12289;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#23376;&#20107;&#20214;&#20851;&#31995;&#32452;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20107;&#20214;&#20851;&#31995;&#22270;&#32435;&#20837;&#20559;&#35265;&#21477;&#35782;&#21035;&#20013;&#65292;&#20998;&#20004;&#27493;&#36827;&#34892;&#65306;&#24314;&#31435;&#19968;&#20010;&#20107;&#20214;&#24863;&#30693;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#27880;&#20837;&#20107;&#20214;&#21644;&#20107;&#20214;&#20851;&#31995;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01722v1 Announce Type: new  Abstract: Media outlets are becoming more partisan and polarized nowadays. In this paper, we identify media bias at the sentence level, and pinpoint bias sentences that intend to sway readers' opinions. As bias sentences are often expressed in a neutral and factual way, considering broader context outside a sentence can help reveal the bias. In particular, we observe that events in a bias sentence need to be understood in associations with other events in the document. Therefore, we propose to construct an event relation graph to explicitly reason about event-event relations for sentence-level bias identification. The designed event relation graph consists of events as nodes and four common types of event relations: coreference, temporal, causal, and subevent relations. Then, we incorporate event relation graph for bias sentences identification in two steps: an event-aware language model is built to inject the events and event relations knowledge 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#22522;&#26412;&#30340;&#26102;&#38388;&#25805;&#20316;&#31526;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25552;&#21319;&#32534;&#31243;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#39064;&#22238;&#31572;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#29702;&#35299;&#38382;&#39064;&#20013;&#30340;&#32452;&#21512;&#26102;&#38388;&#32422;&#26463;</title><link>https://arxiv.org/abs/2404.01720</link><description>&lt;p&gt;
&#33258;&#25105;&#25552;&#21319;&#32534;&#31243;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Improvement Programming for Temporal Knowledge Graph Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01720
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#22522;&#26412;&#30340;&#26102;&#38388;&#25805;&#20316;&#31526;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25552;&#21319;&#32534;&#31243;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#39064;&#22238;&#31572;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#29702;&#35299;&#38382;&#39064;&#20013;&#30340;&#32452;&#21512;&#26102;&#38388;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#39064;&#22238;&#31572;&#65288;TKGQA&#65289;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#26102;&#38388;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#28041;&#21450;&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#12290;&#35813;&#20219;&#21153;&#30340;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#29702;&#35299;&#38382;&#39064;&#20013;&#20851;&#20110;&#22810;&#31181;&#31867;&#22411;&#26102;&#38388;&#32422;&#26463;&#65288;&#22914; before&#12289;first &#31561;&#65289;&#30340;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#38382;&#39064;&#21644;&#20505;&#36873;&#31572;&#26696;&#30340;&#26102;&#38388;&#24863;&#30693;&#23884;&#20837;&#38544;&#24335;&#24314;&#27169;&#26102;&#38388;&#32422;&#26463;&#65292;&#36825;&#31163;&#20840;&#38754;&#29702;&#35299;&#38382;&#39064;&#36824;&#26377;&#24456;&#22823;&#24046;&#36317;&#12290;&#21463;&#26174;&#24335;&#24314;&#27169;&#38382;&#39064;&#20013;&#32422;&#26463;&#30340;&#35821;&#20041;&#35299;&#26512;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#29983;&#25104;&#24102;&#26377;&#31526;&#21495;&#25805;&#20316;&#31526;&#30340;&#36923;&#36753;&#24418;&#24335;&#65292;&#25105;&#20204;&#20026;&#26102;&#38388;&#32422;&#26463;&#35774;&#35745;&#20102;&#22522;&#26412;&#30340;&#26102;&#38388;&#25805;&#20316;&#31526;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25552;&#21319;&#32534;&#31243;&#26041;&#27861;&#29992;&#20110; TKGQA&#65288;Prog-TQA&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Prog-TQA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#29702;&#35299;&#38382;&#39064;&#20013;&#30340;&#32452;&#21512;&#26102;&#38388;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01720v1 Announce Type: new  Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge of this task lies in understanding the complex semantic information regarding multiple types of time constraints (e.g., before, first) in questions. Existing end-to-end methods implicitly model the time constraints by learning time-aware embeddings of questions and candidate answers, which is far from understanding the question comprehensively. Motivated by semantic-parsing-based approaches that explicitly model constraints in questions by generating logical forms with symbolic operators, we design fundamental temporal operators for time constraints and introduce a novel self-improvement Programming method for TKGQA (Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of Large Language Models (LLMs) to understand the combinatory time constraints in the question
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#30340;&#26377;&#25928;ILM&#35757;&#32451;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#19982;&#26631;&#20934;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2404.01716</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#34701;&#21512;&#23545;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Effective internal language model training and fusion for factorized transducer model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01716
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#30340;&#26377;&#25928;ILM&#35757;&#32451;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#19982;&#26631;&#20934;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36716;&#24405;&#22120;&#30340;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#65288;ILM&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#23427;&#20027;&#35201;&#29992;&#20110;&#20272;&#35745;ILM&#20998;&#25968;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#38543;&#21518;&#34987;&#20943;&#21435;&#65292;&#20197;&#20419;&#36827;&#19982;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#38598;&#25104;&#12290;&#26368;&#36817;&#65292;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#65292;&#26126;&#30830;&#37319;&#29992;&#29420;&#31435;&#30340;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38750;&#31354;&#30333;&#20196;&#29260;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#37319;&#29992;&#20102;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#65292;&#19982;&#27973;&#23618;&#34701;&#21512;&#30456;&#27604;&#65292;&#25913;&#36827;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ILM&#35757;&#32451;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#31354;&#30333;&#12289;&#22768;&#23398;&#21644;ILM&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21033;&#29992;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;ILM&#21644;&#25152;&#25552;&#20986;&#30340;&#35299;&#30721;&#31574;&#30053;&#26102;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#35299;&#30721;&#26041;&#27861;&#65292;&#26377;17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#19982;&#24378;RNN-T ba&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01716v1 Announce Type: cross  Abstract: The internal language model (ILM) of the neural transducer has been widely studied. In most prior work, it is mainly used for estimating the ILM score and is subsequently subtracted during inference to facilitate improved integration with external language models. Recently, various of factorized transducer models have been proposed, which explicitly embrace a standalone internal language model for non-blank token prediction. However, even with the adoption of factorized transducer models, limited improvement has been observed compared to shallow fusion. In this paper, we propose a novel ILM training and decoding strategy for factorized transducer models, which effectively combines the blank, acoustic and ILM scores. Our experiments show a 17% relative improvement over the standard decoding method when utilizing a well-trained ILM and the proposed decoding strategy on LibriSpeech datasets. Furthermore, when compared to a strong RNN-T ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#20013;&#23545;&#20107;&#20214;&#30340;&#36947;&#24503;&#35266;&#28857;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20107;&#20214;&#32423;&#21035;&#36947;&#24503;&#35266;&#28857;&#30340;&#26032;&#25968;&#25454;&#38598;EMONA&#65292;&#20026;&#25552;&#21462;&#20107;&#20214;&#30340;&#36947;&#24503;&#24615;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#24314;&#31435;&#20102;&#22522;&#32447;&#27169;&#22411;&#21644;&#36827;&#34892;&#22806;&#37096;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2404.01715</link><description>&lt;p&gt;
EMONA: &#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20107;&#20214;&#32423;&#21035;&#36947;&#24503;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
EMONA: Event-level Moral Opinions in News Articles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#20013;&#23545;&#20107;&#20214;&#30340;&#36947;&#24503;&#35266;&#28857;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20107;&#20214;&#32423;&#21035;&#36947;&#24503;&#35266;&#28857;&#30340;&#26032;&#25968;&#25454;&#38598;EMONA&#65292;&#20026;&#25552;&#21462;&#20107;&#20214;&#30340;&#36947;&#24503;&#24615;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#24314;&#31435;&#20102;&#22522;&#32447;&#27169;&#22411;&#21644;&#36827;&#34892;&#22806;&#37096;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20851;&#20110;&#36947;&#24503;&#26694;&#26550;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#31038;&#20132;&#23186;&#20307;&#30340;&#30701;&#25991;&#26412;&#19978;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#36947;&#24503;&#24773;&#32490;&#12290;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#65292;&#20316;&#32773;&#32463;&#24120;&#36890;&#36807;&#23545;&#20107;&#20214;&#30340;&#36947;&#24503;&#21028;&#26029;&#26469;&#34920;&#36798;&#33258;&#24049;&#30340;&#35266;&#28857;&#25110;&#25919;&#27835;&#31435;&#22330;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#26681;&#25454;&#31038;&#20250;&#36947;&#24503;&#35268;&#21017;&#21028;&#26029;&#20107;&#20214;&#26159;&#23545;&#36824;&#26159;&#38169;&#12290;&#26412;&#25991;&#24320;&#22987;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#20013;&#23545;&#20107;&#20214;&#30340;&#36947;&#24503;&#35266;&#28857;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;EMONA&#65292;&#24182;&#23545;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20107;&#20214;&#32423;&#21035;&#36947;&#24503;&#35266;&#28857;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;400&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#21253;&#21547;&#36229;&#36807;1&#19975;&#20010;&#21477;&#23376;&#21644;4.5&#19975;&#20010;&#20107;&#20214;&#65292;&#20854;&#20013;9613&#20010;&#20107;&#20214;&#33719;&#24471;&#20102;&#36947;&#24503;&#22522;&#30784;&#26631;&#31614;&#12290;&#25552;&#21462;&#20107;&#20214;&#30340;&#36947;&#24503;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23545;&#20107;&#20214;&#30340;&#36947;&#24503;&#21028;&#26029;&#21487;&#33021;&#38750;&#24120;&#38544;&#26214;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29992;&#20110;&#20107;&#20214;&#36947;&#24503;&#35782;&#21035;&#21644;&#20998;&#31867;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22806;&#37096;&#35780;&#20272;&#65292;&#20197;&#25972;&#21512;&#20107;&#20214;&#32423;&#21035;&#30340;&#36947;&#24503;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01715v1 Announce Type: new  Abstract: Most previous research on moral frames has focused on social media short texts, little work has explored moral sentiment within news articles. In news articles, authors often express their opinions or political stance through moral judgment towards events, specifically whether the event is right or wrong according to social moral rules. This paper initiates a new task to understand moral opinions towards events in news articles. We have created a new dataset, EMONA, and annotated event-level moral opinions in news articles. This dataset consists of 400 news articles containing over 10k sentences and 45k events, among which 9,613 events received moral foundation labels. Extracting event morality is a challenging task, as moral judgment towards events can be very implicit. Baseline models were built for event moral identification and classification. In addition, we also conduct extrinsic evaluations to integrate event-level moral opinions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#20013;&#20943;&#23569;&#24102;&#23485;&#28040;&#32791;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2404.01713</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#65306;&#36890;&#36807;6G&#25506;&#32034;&#24863;&#30693;&#20114;&#32852;&#32593;&#30340;&#19979;&#19968;&#20010;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#20013;&#20943;&#23569;&#24102;&#23485;&#28040;&#32791;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#29289;&#32852;&#32593;(IoT)&#24050;&#32463;&#26159;&#19968;&#20010;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#27010;&#24565;&#65292;&#24403;&#25105;&#20204;&#36924;&#36817;2030&#24180;&#26102;&#65292;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#34987;&#31216;&#20026;&#24863;&#30693;&#20114;&#32852;&#32593;(IoS)&#27491;&#22312;&#20852;&#36215;&#12290;&#19982;&#20256;&#32479;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#19981;&#21516;&#65292;IoS&#26088;&#22312;&#25552;&#20379;&#22810;&#24863;&#23448;&#20307;&#39564;&#65292;&#35748;&#35782;&#21040;&#22312;&#25105;&#20204;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25105;&#20204;&#30340;&#24863;&#30693;&#36828;&#19981;&#27490;&#20110;&#35270;&#35273;&#21644;&#21548;&#35273;&#65307;&#23427;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#24863;&#35273;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25512;&#21160;&#27785;&#28024;&#24335;&#22810;&#24863;&#23448;&#23186;&#20307;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#30340;&#21151;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#36825;&#39033;&#25506;&#32034;&#21253;&#25324;&#20256;&#32479;&#27785;&#28024;&#24335;&#23186;&#20307;&#27969;&#19982;&#19968;&#20010;&#25552;&#20986;&#30340;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#35821;&#20041;&#20132;&#27969;&#30340;&#29992;&#20363;&#20043;&#38388;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#36825;&#39033;&#20998;&#26512;&#30340;&#37325;&#28857;&#26159;&#25152;&#25552;&#26041;&#26696;&#20013;&#24102;&#23485;&#28040;&#32791;&#20943;&#23569;&#20102;99.93%&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#25105;&#20204;&#26088;&#22312;&#24378;&#35843;&#35813;&#23454;&#29992;&#24212;&#29992;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01713v1 Announce Type: cross  Abstract: Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical appli
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26497;&#24615;&#26657;&#20934;&#27010;&#24565;&#65292;&#24320;&#21457;&#24378;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#34913;&#26497;&#24615;&#26657;&#20934;&#12289;&#20869;&#23481;&#20445;&#30041;&#21644;&#35821;&#35328;&#33258;&#28982;&#24615;&#65292;&#35299;&#20915;&#35266;&#28857;&#24635;&#32467;&#20013;&#25918;&#22823;&#26497;&#24615;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01706</link><description>&lt;p&gt;
&#35266;&#28857;&#24635;&#32467;&#30340;&#26497;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Polarity Calibration for Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01706
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26497;&#24615;&#26657;&#20934;&#27010;&#24565;&#65292;&#24320;&#21457;&#24378;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#34913;&#26497;&#24615;&#26657;&#20934;&#12289;&#20869;&#23481;&#20445;&#30041;&#21644;&#35821;&#35328;&#33258;&#28982;&#24615;&#65292;&#35299;&#20915;&#35266;&#28857;&#24635;&#32467;&#20013;&#25918;&#22823;&#26497;&#24615;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Opinion summarization &#26159;&#33258;&#21160;&#20174;&#21508;&#31181;&#20027;&#35266;&#20449;&#24687;&#20013;&#29983;&#25104;&#25688;&#35201;&#65292;&#22914;&#20135;&#21697;&#35780;&#35770;&#25110;&#25919;&#27835;&#35266;&#28857;&#12290;&#35266;&#28857;&#24635;&#32467;&#30340;&#25361;&#25112;&#22312;&#20110;&#21576;&#29616;&#19981;&#21516;&#25110;&#29978;&#33267;&#30456;&#20114;&#30683;&#30462;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#23545;&#20808;&#21069;&#30340;&#24635;&#32467;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#20542;&#21521;&#20110;&#25918;&#22823;&#26497;&#24615;&#20559;&#35265;&#65292;&#24378;&#35843;&#22823;&#22810;&#25968;&#24847;&#35265;&#65292;&#32780;&#24573;&#30053;&#23569;&#25968;&#27966;&#35266;&#28857;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#35753;&#24635;&#32467;&#22120;&#34920;&#36798;&#20004;&#26041;&#35266;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26497;&#24615;&#26657;&#20934;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;&#36755;&#20986;&#25688;&#35201;&#30340;&#26497;&#24615;&#19982;&#36755;&#20837;&#25991;&#26412;&#19968;&#33268;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24378;&#21270;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#26497;&#24615;&#26657;&#20934;&#12290;&#35813;&#26041;&#27861;&#23558;&#36755;&#20986;&#25688;&#35201;&#19982;&#36755;&#20837;&#25991;&#26412;&#20043;&#38388;&#30340;&#26497;&#24615;&#36317;&#31163;&#20316;&#20026;&#22870;&#21169;&#36755;&#20837;&#21040;&#24635;&#32467;&#22120;&#20013;&#65292;&#24182;&#24179;&#34913;&#26497;&#24615;&#26657;&#20934;&#12289;&#20869;&#23481;&#20445;&#30041;&#21644;&#35821;&#35328;&#33258;&#28982;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340; Polarit
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01706v1 Announce Type: new  Abstract: Opinion summarization is automatically generating summaries from a variety of subjective information, such as product reviews or political opinions. The challenge of opinions summarization lies in presenting divergent or even conflicting opinions. We conduct an analysis of previous summarization models, which reveals their inclination to amplify the polarity bias, emphasizing the majority opinions while ignoring the minority opinions. To address this issue and make the summarizer express both sides of opinions, we introduce the concept of polarity calibration, which aims to align the polarity of output summary with that of input text. Specifically, we develop a reinforcement training approach for polarity calibration. This approach feeds the polarity distance between output summary and input text as reward into the summarizer, and also balance polarity calibration with content preservation and language naturality. We evaluate our Polarit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#26032;&#39062;&#31574;&#30053;&#26469;&#36924;&#36817;&#25688;&#35201;&#20869;&#23481;&#21333;&#20803;&#65288;SCUs&#65289;&#65292;&#20998;&#21035;&#26159;&#20174;AMR&#24847;&#20041;&#34920;&#31034;&#65288;SMUs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SGUs&#65289;&#29983;&#25104;SCU&#36817;&#20284;&#12290;</title><link>https://arxiv.org/abs/2404.01701</link><description>&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#20013;&#24635;&#32467;&#20869;&#23481;&#21333;&#20803;&#22312;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Summary Content Units in Text Summarization Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#26032;&#39062;&#31574;&#30053;&#26469;&#36924;&#36817;&#25688;&#35201;&#20869;&#23481;&#21333;&#20803;&#65288;SCUs&#65289;&#65292;&#20998;&#21035;&#26159;&#20174;AMR&#24847;&#20041;&#34920;&#31034;&#65288;SMUs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SGUs&#65289;&#29983;&#25104;SCU&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#23383;&#22612;&#35780;&#20272;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20154;&#31867;&#25776;&#20889;&#30340;&#24635;&#32467;&#20869;&#23481;&#21333;&#20803;&#65288;SCUs&#65289;&#12290;&#36825;&#20123;SCUs&#26159;&#31616;&#27905;&#30340;&#21477;&#23376;&#65292;&#23558;&#19968;&#20010;&#25688;&#35201;&#20998;&#35299;&#20026;&#23567;&#20107;&#23454;&#12290;&#36825;&#20123;SCUs&#21487;&#20197;&#29992;&#26469;&#35780;&#21028;&#20505;&#36873;&#25688;&#35201;&#30340;&#36136;&#37327;&#65292;&#21487;&#33021;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#31995;&#32479;&#37096;&#20998;&#33258;&#21160;&#21270;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20026;&#20102;&#23436;&#20840;&#33258;&#21160;&#21270;&#37329;&#23383;&#22612;&#35780;&#20272;&#65292;Zhang&#21644;Bansal&#65288;2021&#65289;&#34920;&#26126;SCUs&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35821;&#20041;&#35282;&#33394;&#19977;&#20803;&#32452;&#65288;STUs&#65289;&#26469;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#26377;&#19968;&#20123;&#38382;&#39064;&#27809;&#26377;&#31572;&#26696;&#65292;&#29305;&#21035;&#26159;&#65306;i&#65289;&#26159;&#21542;&#26377;&#20854;&#20182;&#36924;&#36817;SCUs&#30340;&#26041;&#24335;&#33021;&#22815;&#25552;&#20379;&#20248;&#21183;&#65311;ii&#65289;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;SCUs&#65288;&#25110;&#23427;&#20204;&#30340;&#36817;&#20284;&#65289;&#25552;&#20379;&#26368;&#22823;&#20215;&#20540;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#26469;&#36924;&#36817;SCUs&#65306;&#20998;&#21035;&#20174;AMR&#24847;&#20041;&#34920;&#31034;&#65288;SMUs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SGUs&#65289;&#29983;&#25104;SCU&#36817;&#20284;&#12290;&#25105;&#20204;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01701v1 Announce Type: new  Abstract: At the heart of the Pyramid evaluation method for text summarization lie human written summary content units (SCUs). These SCUs are concise sentences that decompose a summary into small facts. Such SCUs can be used to judge the quality of a candidate summary, possibly partially automated via natural language inference (NLI) systems. Interestingly, with the aim to fully automate the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be approximated by automatically generated semantic role triplets (STUs). However, several questions currently lack answers, in particular: i) Are there other ways of approximating SCUs that can offer advantages? ii) Under which conditions are SCUs (or their approximations) offering the most value? In this work, we examine two novel strategies to approximate SCUs: generating SCU approximations from AMR meaning representations (SMUs) and from large language models (SGUs), respectively. We find that 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#27969;&#34892;&#30149;&#30456;&#20851;&#20107;&#20214;&#30340;&#26694;&#26550;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#20107;&#20214;&#26816;&#27979;&#25216;&#26415;&#22312;COVID-19&#27969;&#34892;&#30149;&#21644;&#20854;&#20182;&#26410;&#35265;&#27969;&#34892;&#30149;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#39044;&#35686;&#12290;</title><link>https://arxiv.org/abs/2404.01679</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20107;&#20214;&#26816;&#27979;&#29992;&#20110;&#30123;&#24773;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Event Detection from Social Media for Epidemic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01679
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#27969;&#34892;&#30149;&#30456;&#20851;&#20107;&#20214;&#30340;&#26694;&#26550;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#20107;&#20214;&#26816;&#27979;&#25216;&#26415;&#22312;COVID-19&#27969;&#34892;&#30149;&#21644;&#20854;&#20182;&#26410;&#35265;&#27969;&#34892;&#30149;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#39044;&#35686;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#26159;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#24179;&#21488;&#65292;&#25552;&#20379;&#26377;&#20851;&#31038;&#20250;&#36235;&#21183;&#21644;&#20107;&#20214;&#30340;&#21450;&#26102;&#26356;&#26032;&#12290;&#20851;&#20110;&#27969;&#34892;&#30149;&#30456;&#20851;&#20107;&#20214;&#30340;&#35752;&#35770;&#65292;&#22914;&#24863;&#26579;&#12289;&#30151;&#29366;&#21644;&#31038;&#20250;&#20114;&#21160;&#65292;&#23545;&#20110;&#22312;&#27969;&#34892;&#30149;&#29190;&#21457;&#26399;&#38388;&#21046;&#23450;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21019;&#21033;&#29992;&#20107;&#20214;&#26816;&#27979;&#65288;ED&#65289;&#26469;&#26356;&#22909;&#22320;&#20934;&#22791;&#21644;&#25552;&#21069;&#39044;&#35686;&#20219;&#20309;&#21363;&#23558;&#21040;&#26469;&#30340;&#27969;&#34892;&#30149;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#19982;&#27969;&#34892;&#30149;&#30456;&#20851;&#30340;&#20107;&#20214;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#25324;&#19971;&#31181;&#19982;&#30142;&#30149;&#26080;&#20851;&#30340;&#20107;&#20214;&#31867;&#22411;&#30340;&#27969;&#34892;&#30149;&#20107;&#20214;&#26412;&#20307;&#35770;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SPEED&#30340;Twitter&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20154;&#24037;&#27880;&#37322;&#30340;&#37325;&#28857;&#20851;&#27880;COVID-19&#27969;&#34892;&#30149;&#30340;&#20107;&#20214;&#12290;&#23454;&#39564;&#25581;&#31034;&#20102;&#22312;COVID&#36895;&#24230;&#19978;&#35757;&#32451;&#30340;ED&#27169;&#22411;&#22914;&#20309;&#26377;&#25928;&#22320;&#26816;&#27979;&#20986;&#20851;&#20110;&#19977;&#31181;&#26410;&#35265;&#27969;&#34892;&#30149;&#65288;&#29492;&#30168;&#12289;&#23528;&#21345;&#30149;&#27602;&#21644;&#30331;&#38761;&#28909;&#65289;&#30340;&#27969;&#34892;&#30149;&#20107;&#20214;&#65307;&#32780;&#22312;&#29616;&#26377;ED&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21017;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25253;&#36947;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01679v1 Announce Type: new  Abstract: Social media is an easy-to-access platform providing timely updates about societal trends and events. Discussions regarding epidemic-related events such as infections, symptoms, and social interactions can be crucial for informing policymaking during epidemic outbreaks. In our work, we pioneer exploiting Event Detection (ED) for better preparedness and early warnings of any upcoming epidemic by developing a framework to extract and analyze epidemic-related events from social media posts. To this end, we curate an epidemic event ontology comprising seven disease-agnostic event types and construct a Twitter dataset SPEED with human-annotated events focused on the COVID-19 pandemic. Experimentation reveals how ED models trained on COVID-based SPEED can effectively detect epidemic events for three unseen epidemics of Monkeypox, Zika, and Dengue; while models trained on existing ED datasets fail miserably. Furthermore, we show that reporting 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GFaiR&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#35777;&#26126;&#25554;&#20837;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#20102;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;</title><link>https://arxiv.org/abs/2404.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#24402;&#32467;&#21453;&#39539;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#36890;&#29992;&#19988;&#21487;&#38752;&#30340;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01677
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GFaiR&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#35777;&#26126;&#25554;&#20837;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#20102;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#26377;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#21487;&#27867;&#21270;&#19988;&#21487;&#38752;&#25512;&#29702;&#22120;&#65288;GFaiR&#65289;&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01677v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks. However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language. This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system's completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#35328;&#22330;&#26223;&#19979;LLMs&#35780;&#20272;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;10&#31181;&#35821;&#35328;&#30340;&#31934;&#24515;&#31574;&#21010;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2404.01667</link><description>&lt;p&gt;
METAL: &#36808;&#21521;&#22810;&#35821;&#35328;&#20803;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
METAL: Towards Multilingual Meta-Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01667
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#35328;&#22330;&#26223;&#19979;LLMs&#35780;&#20272;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;10&#31181;&#35821;&#35328;&#30340;&#31934;&#24515;&#31574;&#21010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#30340;&#31934;&#24230;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21033;&#29992;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#35328;&#22330;&#26223;&#19979;LLMs&#35780;&#20272;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;10&#31181;&#35821;&#35328;&#65292;&#21253;&#21547;&#20102;&#21407;&#29983;&#35828;&#35805;&#32773;&#23545;&#25688;&#35201;&#20219;&#21153;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01667v1 Announce Type: new  Abstract: With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evalua
&lt;/p&gt;</description></item><item><title>CMAT&#26694;&#26550;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;</title><link>https://arxiv.org/abs/2404.01663</link><description>&lt;p&gt;
CMAT: &#29992;&#20110;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01663
&lt;/p&gt;
&lt;p&gt;
CMAT&#26694;&#26550;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LLMs&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#25805;&#20316;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#36755;&#20837;&#26469;&#20934;&#30830;&#24341;&#23548;&#23545;&#35805;&#27969;&#31243;&#65292;&#26234;&#33021;&#20307;&#35843;&#25972;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#28041;&#21450;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#35843;&#25972;&#65292;&#20197;&#26356;&#22909;&#22320;&#21709;&#24212;&#36825;&#31181;&#24341;&#23548;&#12290;&#38024;&#23545;&#36825;&#19968;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Collaborative Multi-Agent Tuning&#65288;CMAT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26681;&#25454;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#26469;&#22686;&#24378;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#23398;&#20064;&#21644;&#23454;&#26102;&#36866;&#24212;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01663v1 Announce Type: new  Abstract: Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this resear
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#26085;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#32553;&#23567;&#38750;&#33521;&#35821;&#31038;&#21306;&#20013;&#30340;AI&#35775;&#38382;&#24046;&#36317;&#65292;&#20419;&#36827;AI&#27665;&#20027;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.01657</link><description>&lt;p&gt;
&#21457;&#24067;&#38024;&#23545;&#26085;&#35821;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Release of Pre-Trained Models for the Japanese Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01657
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#26085;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#32553;&#23567;&#38750;&#33521;&#35821;&#31038;&#21306;&#20013;&#30340;AI&#35775;&#38382;&#24046;&#36317;&#65292;&#20419;&#36827;AI&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01657v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;: AI&#27665;&#20027;&#21270;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26222;&#36890;&#20154;&#21487;&#20197;&#21033;&#29992;AI&#25216;&#26415;&#30340;&#19990;&#30028;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35768;&#22810;&#30740;&#31350;&#26426;&#26500;&#24050;&#32463;&#35797;&#22270;&#35753;&#20182;&#20204;&#30340;&#32467;&#26524;&#23545;&#20844;&#20247;&#21487;&#21450;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#28508;&#21147;&#65292;&#23427;&#20204;&#30340;&#21457;&#24067;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21457;&#24067;&#30340;&#27169;&#22411;&#19987;&#38376;&#38024;&#23545;&#33521;&#35821;&#65292;&#22240;&#27492;&#65292;&#22312;&#38750;&#33521;&#35821;&#31038;&#21306;&#20013;&#65292;AI&#27665;&#20027;&#21270;&#23384;&#22312;&#26126;&#26174;&#28382;&#21518;&#12290;&#20026;&#20102;&#32553;&#23567;AI&#35775;&#38382;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#29992;&#26085;&#35821;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#12289;&#23545;&#27604;&#35821;&#35328;&#21644;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#12289;&#31283;&#23450;&#25193;&#25955;&#21644;&#38544;&#34255;&#21333;&#20803;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#65288;HuBERT&#65289;&#12290;&#36890;&#36807;&#25552;&#20379;&#36825;&#20123;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#33258;&#30001;&#22320;&#19982;&#31526;&#21512;&#26085;&#26412;&#25991;&#21270;&#20215;&#20540;&#35266;&#30340;AI&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#30830;&#20445;&#26085;&#26412;&#25991;&#21270;&#30340;&#36523;&#20221;&#65292;&#20174;&#32780;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01657v1 Announce Type: cross  Abstract: AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#27867;&#21270;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#25361;&#25112;&#22312;&#20110;&#38405;&#35835;&#22120;&#36807;&#24230;&#20381;&#36182;&#35760;&#24518;&#22806;&#37096;&#35821;&#26009;&#24211;&#30340;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01652</link><description>&lt;p&gt;
&#36890;&#36807;&#32531;&#35299;&#19978;&#19979;&#25991;&#35760;&#24518;&#23454;&#29616;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#27867;&#21270;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#25361;&#25112;&#22312;&#20110;&#38405;&#35835;&#22120;&#36807;&#24230;&#20381;&#36182;&#35760;&#24518;&#22806;&#37096;&#35821;&#26009;&#24211;&#30340;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OpenQA&#65289;&#26088;&#22312;&#21033;&#29992;&#22806;&#37096;&#22823;&#35268;&#27169;&#30693;&#35782;&#35821;&#26009;&#24211;&#22238;&#31572;&#20107;&#23454;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#30693;&#35782;&#24182;&#38750;&#38745;&#24577;&#30340;&#65307;&#23427;&#19981;&#26029;&#26356;&#26032;&#21644;&#28436;&#21464;&#12290;&#36825;&#31181;&#30693;&#35782;&#30340;&#21160;&#24577;&#29305;&#24615;&#20026;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#30340;&#27169;&#22411;&#38656;&#35201;&#19981;&#26029;&#36866;&#24212;&#26368;&#26032;&#20449;&#24687;&#65292;&#20197;&#30830;&#20445;&#31572;&#26696;&#20445;&#25345;&#20934;&#30830;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#24320;&#25918;&#22495;&#38382;&#31572;&#27169;&#22411;&#33021;&#22815;&#22810;&#22909;&#22320;&#36716;&#31227;&#21040;&#23436;&#20840;&#26032;&#30340;&#30693;&#35782;&#39046;&#22495;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;QA&#27169;&#22411;&#22312;&#20004;&#31181;&#20855;&#20307;&#24773;&#26223;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;1&#65289;&#36866;&#24212;&#30456;&#21516;&#30693;&#35782;&#35821;&#26009;&#24211;&#30340;&#26356;&#26032;&#29256;&#26412;&#65307;2&#65289;&#36716;&#25442;&#21040;&#23436;&#20840;&#19981;&#21516;&#30340;&#30693;&#35782;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24320;&#25918;&#22495;&#38382;&#31572;&#27169;&#22411;&#30340;&#27867;&#21270;&#25361;&#25112;&#28304;&#33258;&#38405;&#35835;&#22120;&#36807;&#24230;&#20381;&#36182;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#35760;&#24518;&#30693;&#35782;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01652v1 Announce Type: cross  Abstract: Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus. However, real-world knowledge is not static; it updates and evolves continually. Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate. In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains. In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains. We observe that the generalization challenges of OpenQA models stem from the reader's over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generaliz
&lt;/p&gt;</description></item><item><title>NLP &#31995;&#32479;&#26080;&#27861;&#21306;&#20998;&#20351;&#29992;&#21644;&#25552;&#21450;&#65292;&#23545;&#20110;&#22788;&#29702;&#22312;&#32447;&#25239;&#35758;&#35328;&#35770;&#33267;&#20851;&#37325;&#35201;&#65292;&#25945;&#23548;&#36825;&#31181;&#21306;&#20998;&#26377;&#21161;&#20110;&#20943;&#23569;&#34394;&#20551;&#20449;&#24687;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#23457;&#26597;&#38169;&#35823;</title><link>https://arxiv.org/abs/2404.01651</link><description>&lt;p&gt;
&#26080;&#27861;&#21306;&#20998;&#20351;&#29992;&#21644;&#25552;&#21450;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20250;&#25233;&#21046;&#25239;&#35758;&#35328;&#35770;&#65292;&#32780;&#25945;&#23548;&#36825;&#20010;&#21306;&#20998;&#26377;&#21161;&#20110;&#25913;&#21892;
&lt;/p&gt;
&lt;p&gt;
NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01651
&lt;/p&gt;
&lt;p&gt;
NLP &#31995;&#32479;&#26080;&#27861;&#21306;&#20998;&#20351;&#29992;&#21644;&#25552;&#21450;&#65292;&#23545;&#20110;&#22788;&#29702;&#22312;&#32447;&#25239;&#35758;&#35328;&#35770;&#33267;&#20851;&#37325;&#35201;&#65292;&#25945;&#23548;&#36825;&#31181;&#21306;&#20998;&#26377;&#21161;&#20110;&#20943;&#23569;&#34394;&#20551;&#20449;&#24687;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#23457;&#26597;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20351;&#29992;&#35789;&#35821;&#20256;&#36798;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#19982;&#8220;&#25552;&#21450;&#8221;&#35789;&#35821;&#24341;&#36848;&#20182;&#20154;&#30340;&#35805;&#35821;&#25110;&#25351;&#20986;&#35789;&#35821;&#30340;&#23646;&#24615;&#26159;&#26377;&#25152;&#21306;&#21035;&#30340;&#12290;&#22312;&#36825;&#37324;&#25105;&#20204;&#23637;&#31034;&#65292;&#35745;&#31639;&#27169;&#25311;&#36825;&#31181;&#20351;&#29992;-&#25552;&#21450;&#21306;&#21035;&#23545;&#20110;&#22788;&#29702;&#32447;&#19978;&#25239;&#35758;&#35328;&#35770;&#33267;&#20851;&#37325;&#35201;&#12290;&#39539;&#26021;&#26377;&#38382;&#39064;&#20869;&#23481;&#30340;&#25239;&#35758;&#35328;&#35770;&#36890;&#24120;&#25552;&#21450;&#26377;&#23475;&#35821;&#35328;&#20294;&#26412;&#36523;&#24182;&#19981;&#26377;&#23475;&#65288;&#20363;&#22914;&#65292;&#31216;&#30123;&#33495;&#21361;&#38505;&#24182;&#19981;&#31561;&#21516;&#20110;&#23545;&#26576;&#20154;&#34920;&#31034;&#21453;&#23545;&#31216;&#21628;&#30123;&#33495;&#21361;&#38505;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#21363;&#20351;&#26159;&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#26080;&#27861;&#21306;&#20998;&#20351;&#29992;&#21644;&#25552;&#21450;&#65292;&#32780;&#36825;&#20010;&#22833;&#36133;&#20250;&#24310;&#20280;&#21040;&#20004;&#20010;&#20851;&#38190;&#30340;&#19979;&#28216;&#20219;&#21153;&#65306;&#34394;&#20551;&#20449;&#24687;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#23548;&#33268;&#23545;&#25239;&#35758;&#35328;&#35770;&#30340;&#23457;&#26597;&#12290;&#25105;&#20204;&#24341;&#20837;&#25552;&#31034;&#32531;&#35299;&#25514;&#26045;&#26469;&#25945;&#23548;&#20351;&#29992;-&#25552;&#21450;&#21306;&#21035;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#20943;&#23569;&#20102;&#36825;&#20123;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#20351;&#29992;-&#25552;&#21450;&#21306;&#21035;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;CSS&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01651v1 Announce Type: new  Abstract: The use of words to convey speaker's intent is traditionally distinguished from the `mention' of words for quoting what someone said, or pointing out properties of a word. Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online. Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous). We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and hate speech detection, resulting in censorship of counterspeech. We introduce prompting mitigations that teach the use-mention distinction, and show they reduce these errors. Our work highlights the importance of the use-mention distinction for NLP and CSS and offers 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#21512;&#23454;&#20307;&#25551;&#36848;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;&#30340;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.01626</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#23454;&#20307;&#35299;&#30721;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Entity Disambiguation via Fusion Entity Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01626
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#21512;&#23454;&#20307;&#25551;&#36848;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;&#30340;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#28040;&#27495;&#65288;ED&#65289;&#26159;&#23558;&#27169;&#31946;&#23454;&#20307;&#30340;&#25552;&#21450;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#25351;&#20195;&#23454;&#20307;&#30340;&#36807;&#31243;&#65292;&#22312;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#22312;&#26631;&#20934;&#21270;&#30340;ZELDA&#22522;&#20934;&#19979;&#23637;&#31034;&#20986;&#27604;&#20998;&#31867;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;&#26041;&#27861;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#19988;&#29983;&#25104;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23454;&#20307;&#25551;&#36848;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#32780;&#36825;&#20123;&#25551;&#36848;&#21487;&#33021;&#21253;&#21547;&#21306;&#20998;&#30456;&#20284;&#23454;&#20307;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#65292;&#20197;&#26356;&#35814;&#32454;&#30340;&#23454;&#20307;&#25551;&#36848;&#26469;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;&#12290;&#32473;&#23450;&#25991;&#26412;&#21644;&#20505;&#36873;&#23454;&#20307;&#65292;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26412;&#19982;&#27599;&#20010;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20026;&#27599;&#20010;&#23454;&#20307;&#20505;&#36873;&#20135;&#29983;&#34920;&#31034;&#12290;&#35299;&#30721;&#22120;&#38543;&#21518;&#23558;&#23454;&#20307;&#20505;&#36873;&#30340;&#34920;&#31034;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36873;&#25321;&#27491;&#30830;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01626v1 Announce Type: new  Abstract: Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark. Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked. We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity. Our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#65292;&#23454;&#29616;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#65292;&#19988;&#30456;&#27604;&#20808;&#21069;&#31995;&#32479;&#21462;&#24471;10%&#30340;Recall@1&#32477;&#23545;&#25913;&#36827;</title><link>https://arxiv.org/abs/2404.01616</link><description>&lt;p&gt;
&#23558;LLMs&#36716;&#21270;&#20026;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01616
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#65292;&#23454;&#29616;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#65292;&#19988;&#30456;&#27604;&#20808;&#21069;&#31995;&#32479;&#21462;&#24471;10%&#30340;Recall@1&#32477;&#23545;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22312;&#20165;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#36229;&#20986;&#20102;&#20855;&#26377;&#37197;&#23545;&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#33539;&#22260;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#65288;DE&#65289;&#30340;&#26816;&#32034;&#31995;&#32479;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#25237;&#24433;&#21040;&#30456;&#21516;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#24182;&#22312;&#26816;&#32034;&#21644;&#21452;&#35821;&#25991;&#26412;&#25366;&#25496;&#20013;&#23637;&#31034;&#20102;&#25104;&#21151;&#12290;&#20026;&#20102;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#19981;&#38656;&#35201;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;LLM&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#26469;&#21305;&#37197;&#26816;&#32034;&#35757;&#32451;&#26399;&#38388;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;LLM-based&#26816;&#32034;&#31995;&#32479;&#33021;&#22815;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#23613;&#31649;&#21482;&#22312;21&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20248;&#20110;&#20808;&#21069;&#19987;&#38376;&#22312;&#25152;&#26377;102&#31181;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#31995;&#32479;&#12290;&#22312;&#36825;&#20123;&#35821;&#35328;&#20013;&#65292;&#25105;&#20204;&#22312;Recall@1&#19978;&#23454;&#29616;&#20102;10&#65285;&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01616v1 Announce Type: new  Abstract: Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29436;&#20154;&#28216;&#25103;&#27169;&#25311;&#24179;&#21488;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35266;&#28857;&#39046;&#23548;&#20316;&#29992;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.01602</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29436;&#20154;&#28216;&#25103;&#20013;&#30340;&#33333;&#25163;&#65311;&#35780;&#20272;&#20854;&#35266;&#28857;&#24341;&#39046;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29436;&#20154;&#28216;&#25103;&#27169;&#25311;&#24179;&#21488;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35266;&#28857;&#39046;&#23548;&#20316;&#29992;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#38590;&#24536;&#30340;&#25112;&#30053;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;LLM&#20195;&#29702;&#25152;&#23637;&#31034;&#30340;&#35266;&#28857;&#39046;&#23548;&#21147;&#30340;&#37325;&#35201;&#24615;&#34987;&#24573;&#35270;&#20102;&#65292;&#32780;&#36825;&#23545;&#20110;&#22810;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#35774;&#32622;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#29436;&#20154;&#28216;&#25103;&#20316;&#20026;&#27169;&#25311;&#24179;&#21488;&#65292;&#35780;&#20272;LLMs&#30340;&#35266;&#28857;&#24341;&#39046;&#20316;&#29992;&#12290;&#35813;&#28216;&#25103;&#20013;&#26377;&#35686;&#38271;&#35282;&#33394;&#65292;&#36127;&#36131;&#24635;&#32467;&#35770;&#25454;&#24182;&#25512;&#33616;&#20915;&#31574;&#36873;&#39033;&#65292;&#22240;&#27492;&#21487;&#20316;&#20026;&#35266;&#28857;&#39046;&#34966;&#30340;&#21487;&#20449;&#20195;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#35686;&#38271;&#35282;&#33394;&#30340;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#22522;&#20110;&#35266;&#28857;&#39046;&#34966;&#20851;&#38190;&#29305;&#24449;&#30340;&#26032;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#31532;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#34913;&#37327;&#35266;&#28857;&#39046;&#34966;&#30340;&#21487;&#38752;&#24615;&#65292;&#31532;&#20108;&#20010;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01602v1 Announce Type: cross  Abstract: Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game features the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics for evaluation based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#28304;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#30495;&#23454;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#30149;&#29702;&#24615;&#32959;&#30244;-&#28107;&#24052;&#32467;-&#36716;&#31227;&#65288;pTNM&#65289;&#20998;&#26399;&#20449;&#24687;</title><link>https://arxiv.org/abs/2404.01589</link><description>&lt;p&gt;
&#20351;&#29992;&#24320;&#28304;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30284;&#30151;&#20998;&#26399;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Cancer Stage with Open-Source Clinical Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#28304;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#30495;&#23454;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#30149;&#29702;&#24615;&#32959;&#30244;-&#28107;&#24052;&#32467;-&#36716;&#31227;&#65288;pTNM&#65289;&#20998;&#26399;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#20998;&#26399;&#20998;&#31867;&#23545;&#20110;&#20026;&#32959;&#30244;&#23398;&#24739;&#32773;&#21046;&#23450;&#27835;&#30103;&#21644;&#25252;&#29702;&#31649;&#29702;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#20998;&#26399;&#20449;&#24687;&#36890;&#24120;&#20197;&#38750;&#32467;&#26500;&#21270;&#24418;&#24335;&#21253;&#21547;&#22312;&#20020;&#24202;&#12289;&#30149;&#29702;&#23398;&#12289;&#25918;&#23556;&#23398;&#21644;&#20854;&#20182;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#20013;&#65292;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#35299;&#26512;&#21644;&#33719;&#21462;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20123;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#20808;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20934;&#22791;&#36215;&#26469;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#28304;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#20174;&#30495;&#23454;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#30149;&#29702;&#24615;&#32959;&#30244;-&#28107;&#24052;&#32467;-&#36716;&#31227;&#65288;pTNM&#65289;&#20998;&#26399;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#27604;&#36739;&#20102;LLMs&#21644;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;LLMs&#22312;&#32959;&#30244;&#65288;T&#65289;&#20998;&#31867;&#26041;&#38754;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#36866;&#24403;&#37319;&#29992;&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#20204;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01589v1 Announce Type: cross  Abstract: Cancer stage classification is important for making treatment and care management plans for oncology patients. Information on staging is often included in unstructured form in clinical, pathology, radiology and other free-text reports in the electronic health record system, requiring extensive work to parse and obtain. To facilitate the extraction of this information, previous NLP approaches rely on labeled training datasets, which are labor-intensive to prepare. In this study, we demonstrate that without any labeled training data, open-source clinical large language models (LLMs) can extract pathologic tumor-node-metastasis (pTNM) staging information from real-world pathology reports. Our experiments compare LLMs and a BERT-based model fine-tuned using the labeled data. Our findings suggest that while LLMs still exhibit subpar performance in Tumor (T) classification, with the appropriate adoption of prompting strategies, they can achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24187;&#35273;&#22810;&#26679;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#20943;&#23569;&#20102;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2404.01588</link><description>&lt;p&gt;
&#22522;&#20110;&#24187;&#35273;&#22810;&#26679;&#24615;&#30340;&#25991;&#26412;&#25688;&#35201;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hallucination Diversity-Aware Active Learning for Text Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24187;&#35273;&#22810;&#26679;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#20943;&#23569;&#20102;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#29983;&#25104;&#24187;&#35273;&#36755;&#20986;&#30340;&#20542;&#21521;&#65292;&#21363;&#22312;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#25110;&#19981;&#25903;&#25345;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#20943;&#36731;&#24187;&#35273;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#26469;&#35782;&#21035;&#21644;&#32416;&#27491;LLMs&#36755;&#20986;&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#19987;&#27880;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#24187;&#35273;&#65292;&#20363;&#22914;&#23454;&#20307;&#25110;&#26631;&#35760;&#38169;&#35823;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;LLMs&#36755;&#20986;&#20013;&#23637;&#31034;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#24187;&#35273;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLMs&#24187;&#35273;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#38477;&#20302;&#20102;&#23545;&#24187;&#35273;&#25152;&#38656;&#30340;&#26114;&#36149;&#20154;&#31867;&#27880;&#37322;&#12290;&#36890;&#36807;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#34913;&#37327;&#35821;&#20041;&#26694;&#26550;&#12289;&#35758;&#35770;&#21644;&#20869;&#23481;&#21487;&#39564;&#35777;&#24615;&#38169;&#35823;&#20013;&#30340;&#32454;&#31890;&#24230;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HAllucination Diversity-Aware Sampling&#65288;HADAS&#65289;&#26469;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#24187;&#35273;&#65292;&#20197;&#20379;LLM&#24494;&#35843;&#30340;&#20027;&#21160;&#23398;&#20064;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01588v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#39640;&#27700;&#24179;&#25220;&#34989;&#26816;&#27979;&#30740;&#31350;&#25968;&#25454;&#38598;&#32570;&#22833;&#30340;&#31354;&#30333;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#24322;</title><link>https://arxiv.org/abs/2404.01582</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#22686;&#24378;&#30340;&#20316;&#19994;&#25220;&#34989;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#39640;&#27700;&#24179;&#25220;&#34989;&#26816;&#27979;&#30740;&#31350;&#25968;&#25454;&#38598;&#32570;&#22833;&#30340;&#31354;&#30333;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25220;&#34989;&#26816;&#27979;&#20219;&#21153;&#26159;&#19968;&#39033;&#24120;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#32473;&#23450;&#25991;&#26412;&#26159;&#21542;&#21253;&#21547;&#20174;&#20854;&#20182;&#25991;&#26412;&#20013;&#25220;&#34989;&#25110;&#22797;&#21046;&#30340;&#20869;&#23481;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#26816;&#27979;&#39640;&#27700;&#24179;&#30340;&#25220;&#34989;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#20135;&#29983;&#20102;32,927&#23545;&#25991;&#26412;&#25220;&#34989;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#25220;&#34989;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;98.86&#65285;&#12289;98.90&#65285;&#12289;98.86&#65285;&#21644;0.9888&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#28436;&#31034;&#24179;&#21488;&#65292;&#20801;&#35768;&#29992;&#25143;&#19978;&#20256;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01582v1 Announce Type: cross  Abstract: Text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts. In existing research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets. In this paper, we propose a plagiarized text data generation method based on GPT-3.5, which produces 32,927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods, bridging the gap in this part of research. Meanwhile, we propose a plagiarism identification method based on Faiss with BERT with high efficiency and high accuracy. Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86\%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively. At the end, we also provide a user-friendly demo platform that allows users to upload a text 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#26367;&#25442;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#20026;&#21516;&#20041;&#35789;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2404.01569</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#31181;&#23454;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models Using Contrast Sets: An Experimental Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01569
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#26367;&#25442;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#20026;&#21516;&#20041;&#35789;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#22810;&#20010;&#36755;&#20837;&#25991;&#26412;&#20998;&#31867;&#30340;&#20219;&#21153;&#20013;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#24230;&#37327;&#34987;&#24191;&#27867;&#24212;&#29992;&#20316;&#20026;&#38169;&#35823;&#24230;&#37327;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#35813;&#24230;&#37327;&#22312;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#35821;&#21477;&#34164;&#28085;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#28041;&#21450;&#33258;&#21160;&#23558;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#26367;&#25442;&#20026;&#23427;&#20204;&#30340;&#21516;&#20041;&#35789;&#65292;&#20197;&#20445;&#30041;&#21477;&#23376;&#30340;&#21407;&#22987;&#21547;&#20041;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;&#25105;&#20204;&#20351;&#29992;ELECTRA-small&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#35813;&#27169;&#22411;&#22312;&#20256;&#32479;&#30340;SNLI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;89.9%&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#23545;&#27604;&#38598;&#19978;&#26174;&#31034;&#20986;&#20102;72.5%&#30340;&#20934;&#30830;&#24230;&#65292;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01569v1 Announce Type: cross  Abstract: In the domain of Natural Language Inference (NLI), especially in tasks involving the classification of multiple input texts, the Cross-Entropy Loss metric is widely employed as a standard for error measurement. However, this metric falls short in effectively evaluating a model's capacity to understand language entailments. In this study, we introduce an innovative technique for generating a contrast set for the Stanford Natural Language Inference (SNLI) dataset. Our strategy involves the automated substitution of verbs, adverbs, and adjectives with their synonyms to preserve the original meaning of sentences. This method aims to assess whether a model's performance is based on genuine language comprehension or simply on pattern recognition. We conducted our analysis using the ELECTRA-small model. The model achieved an accuracy of 89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5% on our contrast set, indicati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35774;&#22791;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36719;&#20214;API&#20989;&#25968;&#35843;&#29992;&#30340;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#23545;API&#32467;&#26500;&#21644;&#35821;&#27861;&#30340;&#29702;&#35299;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;API&#20989;&#25968;&#35843;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01549</link><description>&lt;p&gt;
Octopus: &#36719;&#20214;API&#20989;&#25968;&#35843;&#29992;&#30340;&#35774;&#22791;&#31471;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Octopus: On-device language model for function calling of software APIs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35774;&#22791;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36719;&#20214;API&#20989;&#25968;&#35843;&#29992;&#30340;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#23545;API&#32467;&#26500;&#21644;&#35821;&#27861;&#30340;&#29702;&#35299;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;API&#20989;&#25968;&#35843;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#25991;&#26412;&#22788;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26088;&#22312;&#21033;&#29992;&#35774;&#22791;&#31471;LLMs&#35843;&#29992;&#36719;&#20214;API&#30340;&#26032;&#31574;&#30053;&#12290;&#25105;&#20204;&#31934;&#24515;&#32534;&#21046;&#20102;&#20174;&#36719;&#20214;API&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20855;&#26377;2B&#12289;3B&#21644;7B&#21442;&#25968;&#23481;&#37327;&#30340;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#36719;&#20214;API&#20132;&#20114;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#25913;&#36827;&#27169;&#22411;&#23545;API&#32467;&#26500;&#21644;&#35821;&#27861;&#30340;&#29702;&#35299;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;API&#20989;&#25968;&#35843;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26465;&#20214;&#23631;&#34109;&#8221;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#36755;&#20986;&#31526;&#21512;&#26399;&#26395;&#30340;&#26684;&#24335;&#65292;&#24182;&#20943;&#23569;&#38169;&#35823;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;API&#20132;&#20114;&#20013;&#25928;&#26524;&#30340;&#26032;&#22522;&#20934;&#65292;&#24182;&#20026;&#38543;&#21518;&#30340;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01549v1 Announce Type: new  Abstract: In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) play a crucial role due to their advanced text processing and generation abilities. This study introduces a new strategy aimed at harnessing on-device LLMs in invoking software APIs. We meticulously compile a dataset derived from software API documentation and apply fine-tuning to LLMs with capacities of 2B, 3B and 7B parameters, specifically to enhance their proficiency in software API interactions. Our approach concentrates on refining the models' grasp of API structures and syntax, significantly enhancing the accuracy of API function calls. Additionally, we propose \textit{conditional masking} techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds. We also propose a novel benchmark designed to evaluate the effectiveness of LLMs in API interactions, establishing a foundation for subsequent resea
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#30340;&#31574;&#30053;&#65292;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#25968;&#23383;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.01536</link><description>&lt;p&gt;
&#25918;&#32622;&#38170;&#28857;&#65306;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#32473;&#25968;&#23383;&#35821;&#20041;&#19978;&#30340;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Laying Anchors: Semantically Priming Numerals in Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01536
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#30340;&#31574;&#30053;&#65292;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#25968;&#23383;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22823;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31649;&#32447;&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#26410;&#33021;&#27491;&#30830;&#32534;&#30721;&#25968;&#23383;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#25968;&#23383;&#29702;&#35299;&#30340;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20219;&#20309;&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#26469;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#25968;&#23383;&#26631;&#35760;&#30340;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#25968;&#20540;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#65292;&#23545;&#39046;&#22495;&#20869;&#65288;&#24050;&#35265;&#65289;&#21644;&#39046;&#22495;&#22806;&#65288;&#26410;&#35265;&#65289;&#30340;&#25968;&#23383;&#37117;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23454;&#35777;&#35780;&#20272;&#25193;&#23637;&#21040;&#20174;1&#21040;10&#20159;&#30340;&#25968;&#23383;&#33539;&#22260;&#65292;&#27604;&#20197;&#24448;&#30456;&#21516;&#31867;&#22411;&#30740;&#31350;&#30340;&#33539;&#22260;&#24191;&#24471;&#22810;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#24471;&#30340;&#23884;&#20837;&#21521;&#25968;&#23398;&#19978;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01536v1 Announce Type: cross  Abstract: Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#33258;&#21160;&#22238;&#24402;&#20107;&#20214;&#26102;&#38388;&#22270;&#29983;&#25104;&#30340;&#26465;&#20214;&#38598;&#21512;&#29983;&#25104;&#38382;&#39064;&#30340;&#38598;&#21512;&#23545;&#40784;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#21270;&#22270;&#21644;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#24207;&#21015;&#19981;&#21305;&#37197;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.01532</link><description>&lt;p&gt;
&#33258;&#21160;&#22238;&#24402;&#20107;&#20214;&#26102;&#38388;&#22270;&#29983;&#25104;&#30340;&#38598;&#21512;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#33258;&#21160;&#22238;&#24402;&#20107;&#20214;&#26102;&#38388;&#22270;&#29983;&#25104;&#30340;&#26465;&#20214;&#38598;&#21512;&#29983;&#25104;&#38382;&#39064;&#30340;&#38598;&#21512;&#23545;&#40784;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#21270;&#22270;&#21644;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#24207;&#21015;&#19981;&#21305;&#37197;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33258;&#22238;&#24402;&#29983;&#25104;&#32447;&#24615;&#21270;&#22270;&#65292;&#29992;&#20110;&#26500;&#24314;&#20107;&#20214;&#26102;&#38388;&#22270;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23548;&#33268;&#27425;&#20248;&#22270;&#29983;&#25104;&#65292;&#22240;&#20026;&#32447;&#24615;&#21270;&#22270;&#34920;&#29616;&#20986;&#38598;&#21512;&#29305;&#24449;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#21017;&#25353;&#39034;&#24207;&#22788;&#29702;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#37325;&#26032;&#26500;&#24605;&#20102;&#20219;&#21153;&#65292;&#23558;&#20854;&#20316;&#20026;&#26465;&#20214;&#38598;&#21512;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#21512;&#23545;&#40784;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01532v1 Announce Type: new  Abstract: Event temporal graphs have been shown as convenient and effective representations of complex temporal relations between events in text. Recent studies, which employ pre-trained language models to auto-regressively generate linearised graphs for constructing event temporal graphs, have shown promising results. However, these methods have often led to suboptimal graph generation as the linearised graphs exhibit set characteristics which are instead treated sequentially by language models. This discrepancy stems from the conventional text generation objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences. To address these challenges, we reframe the task as a conditional set generation problem, proposing a Set-aligning Framework tailored for the effective utilisation of Large Language Models (LLMs). The framework incorporates data augmentations and set-property regularis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#20197;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#65292;&#24212;&#29992;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#36866;&#37197;&#22120;&#26694;&#26550;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01490</link><description>&lt;p&gt;
AAdaM&#22312;SemEval-2024&#20219;&#21153;1&#20013;&#30340;&#34920;&#29616;&#65306;&#22810;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#30340;&#22686;&#24378;&#19982;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01490
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#20197;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#65292;&#24212;&#29992;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#36866;&#37197;&#22120;&#26694;&#26550;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;1&#24320;&#21457;&#30340;&#31995;&#32479;&#65306;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#12290;&#35813;&#20849;&#20139;&#20219;&#21153;&#26088;&#22312;&#34913;&#37327;&#21477;&#23376;&#23545;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#24212;&#29992;&#20110;&#26410;&#26631;&#35760;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#20197;&#24357;&#21512;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#36866;&#24212;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#27169;&#22411;&#35757;&#32451;&#26041;&#38754;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23436;&#20840;&#24494;&#35843;&#21644;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#35843;&#25972;&#65292;&#24182;&#37319;&#29992;&#36866;&#37197;&#22120;&#26694;&#26550;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#38646;-shot&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#22312;&#20849;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#23376;&#20219;&#21153;A&#65288;&#30417;&#30563;&#23398;&#20064;&#65289;&#21644;&#23376;&#20219;&#21153;C&#65288;&#36328;&#35821;&#35328;&#36716;&#31227;&#65289;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#25490;&#21517;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01490v1 Announce Type: new  Abstract: This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian Languages. The shared task aims at measuring the semantic textual relatedness between pairs of sentences, with a focus on a range of under-represented languages. In this work, we propose using machine translation for data augmentation to address the low-resource challenge of limited training data. Moreover, we apply task-adaptive pre-training on unlabeled task data to bridge the gap between pre-training and task adaptation. For model training, we investigate both full fine-tuning and adapter-based tuning, and adopt the adapter framework for effective zero-shot cross-lingual transfer. We achieve competitive results in the shared task: our system performs the best among all ranked teams in both subtask A (supervised learning) and subtask C (cross-lingual transfer).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#26694;&#26550;&#20998;&#26512;&#33267;&#22810;&#35821;&#35328;&#29615;&#22659;&#65292;&#36890;&#36807;&#20247;&#21253;&#21019;&#24314;&#25968;&#25454;&#38598;&#24182;&#32467;&#21512;&#20854;&#20182;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20247;&#21253;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01481</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#35821;&#35328;&#26032;&#38395;&#26694;&#26550;&#20998;&#26512;&#25193;&#23637;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Scaling Up Multilingual News Framing Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#26694;&#26550;&#20998;&#26512;&#33267;&#22810;&#35821;&#35328;&#29615;&#22659;&#65292;&#36890;&#36807;&#20247;&#21253;&#21019;&#24314;&#25968;&#25454;&#38598;&#24182;&#32467;&#21512;&#20854;&#20182;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20247;&#21253;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#26694;&#26550;&#26159;&#25351;&#26377;&#30446;&#30340;&#22320;&#36873;&#25321;&#21644;&#21576;&#29616;&#25919;&#27835;&#38382;&#39064;&#29305;&#23450;&#26041;&#38754;&#20197;&#22609;&#36896;&#20844;&#20247;&#33286;&#35770;&#30340;&#30740;&#31350;&#12290;&#23613;&#31649;&#19982;&#20840;&#29699;&#20960;&#20046;&#25152;&#26377;&#31038;&#20250;&#37117;&#30456;&#20851;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#38598;&#21644;&#20854;&#20182;&#36164;&#28304;&#65292;&#30740;&#31350;&#21463;&#21040;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#36890;&#36807;&#20247;&#21253;&#36827;&#34892;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#21487;&#33021;&#24615;&#65292;&#21033;&#29992;&#38750;&#19987;&#23478;&#27880;&#37322;&#32773;&#24320;&#21457;&#35757;&#32451;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#23558;&#26694;&#26550;&#20998;&#26512;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#29615;&#22659;&#65288;12&#31181;&#31867;&#22411;&#22810;&#26679;&#30340;&#35821;&#35328;&#65289;&#12290;&#25105;&#20204;&#36824;&#22312;&#23391;&#21152;&#25289;&#35821;&#21644;&#33889;&#33796;&#29273;&#35821;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;&#31227;&#27665;&#21644;&#21516;&#24615;&#23130;&#23035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22312;&#25105;&#20204;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#31995;&#32479;&#65292;&#32467;&#21512;&#20854;&#20182;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#22686;&#21152;&#20102;5.32&#20010;&#30334;&#20998;&#28857;&#65292;&#34920;&#26126;&#20247;&#21253;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01481v1 Announce Type: new  Abstract: Media framing is the study of strategically selecting and presenting specific aspects of political issues to shape public opinion. Despite its relevance to almost all societies around the world, research has been limited due to the lack of available datasets and other resources. This study explores the possibility of dataset creation through crowdsourcing, utilizing non-expert annotators to develop training corpora. We first extend framing analysis beyond English news to a multilingual context (12 typologically diverse languages) through automatic translation. We also present a novel benchmark in Bengali and Portuguese on the immigration and same-sex marriage domains. Additionally, we show that a system trained on our crowd-sourced dataset, combined with other existing ones, leads to a 5.32 percentage point increase from the baseline, showing that crowdsourcing is a viable option. Last, we study the performance of large language models (
&lt;/p&gt;</description></item><item><title>TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.01476</link><description>&lt;p&gt;
TraveLER&#65306;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TraveLER: A Multi-LMM Agent Framework for Video Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01476
&lt;/p&gt;
&lt;p&gt;
TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#20197;&#24103;&#20026;&#21333;&#20301;&#36827;&#34892;&#22788;&#29702;&#12290;&#34429;&#28982;&#22522;&#20110;&#22270;&#20687;&#30340;&#35270;&#39057;&#26041;&#27861;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#30340;&#23616;&#38480;&#26159;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20102;&#22914;&#20309;&#36873;&#25321;&#20851;&#38190;&#26102;&#38388;&#25139;&#65292;&#24182;&#19988;&#26080;&#27861;&#22312;&#30830;&#23450;&#38169;&#35823;&#26102;&#38388;&#25139;&#26102;&#36827;&#34892;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#25552;&#21462;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#32454;&#33410;&#65292;&#32780;&#26159;&#25552;&#20379;&#24103;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#23427;&#27839;&#30528;&#35270;&#39057;&#36827;&#34892;&#31227;&#21160;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#20174;&#20851;&#38190;&#24103;&#25910;&#38598;&#30456;&#20851;&#20449;&#24687;&#65292;&#30452;&#21040;&#33719;&#24471;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TraveLER&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21046;&#23450;&#8220;&#36941;&#21382;&#8221;&#35270;&#39057;&#35745;&#21010;&#30340;&#27169;&#22411;&#65292;&#35810;&#38382;&#20851;&#20110;&#21333;&#20010;&#24103;&#30340;&#38382;&#39064;&#20197;&#8220;&#23450;&#20301;&#8221;&#24182;&#23384;&#20648;&#20851;&#38190;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01476v1 Announce Type: cross  Abstract: Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key info
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#21644;&#20854;&#26368;&#20808;&#36827;&#30340;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;MQM&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#35774;&#32622;&#21487;&#38752;&#20154;&#31867;&#35780;&#20272;&#20197;&#24471;&#20986;&#31283;&#23450;&#32467;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#38024;&#23545;&#35774;&#35745;&#21487;&#22797;&#21046;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#30340;&#20855;&#20307;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2404.01474</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#23450;&#25490;&#21517;&#27010;&#29575;&#23547;&#25214;&#21487;&#22797;&#21046;&#30340;&#20154;&#31867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Finding Replicable Human Evaluations via Stable Ranking Probability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#21644;&#20854;&#26368;&#20808;&#36827;&#30340;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;MQM&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#35774;&#32622;&#21487;&#38752;&#20154;&#31867;&#35780;&#20272;&#20197;&#24471;&#20986;&#31283;&#23450;&#32467;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#38024;&#23545;&#35774;&#35745;&#21487;&#22797;&#21046;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#30340;&#20855;&#20307;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#20154;&#31867;&#35780;&#20272;&#23545;&#20110;&#25104;&#21151;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23454;&#29616;&#21487;&#38752;&#20154;&#31867;&#35780;&#20272;&#21364;&#26497;&#20026;&#22256;&#38590;&#12290;&#31283;&#23450;&#24615;&#23545;&#20110;&#36890;&#36807;&#36136;&#37327;&#23545;&#31995;&#32479;&#36827;&#34892;&#25490;&#21517;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#37325;&#22797;&#35780;&#20272;&#20013;&#31995;&#32479;&#30340;&#19968;&#33268;&#25490;&#21517;&#19981;&#20165;&#20165;&#26159;&#21487;&#21462;&#30340;&#65292;&#32780;&#19988;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#32570;&#20047;&#36825;&#19968;&#28857;&#65292;&#20415;&#26080;&#27861;&#20026;&#29228;&#22369;&#25110;&#20135;&#21697;&#25512;&#20986;&#20915;&#31574;&#25552;&#20379;&#21487;&#38752;&#22522;&#30784;&#12290;&#26412;&#25991;&#20197;&#26426;&#22120;&#32763;&#35793;&#21450;&#20854;&#26368;&#20808;&#36827;&#30340;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;MQM&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#22914;&#20309;&#35774;&#31435;&#21487;&#38752;&#30340;&#20154;&#31867;&#35780;&#20272;&#20197;&#24471;&#20986;&#31283;&#23450;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#37197;&#32473;&#35780;&#20272;&#32773;&#30340;&#39033;&#30446;&#37197;&#32622;&#12289;&#27599;&#20010;&#39033;&#30446;&#30340;&#35780;&#20998;&#27425;&#25968;&#20197;&#21450;&#24471;&#20998;&#24402;&#19968;&#21270;&#30340;&#26368;&#20339;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#35774;&#35745;&#21487;&#22797;&#21046;&#30340;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#30340;&#20855;&#20307;&#24314;&#35758;&#12290;&#25105;&#20204;&#36824;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#27573;&#32763;&#35793;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01474v1 Announce Type: new  Abstract: Reliable human evaluation is critical to the development of successful natural language generation models, but achieving it is notoriously difficult. Stability is a crucial requirement when ranking systems by quality: consistent ranking of systems across repeated evaluations is not just desirable, but essential. Without it, there is no reliable foundation for hill-climbing or product launch decisions. In this paper, we use machine translation and its state-of-the-art human evaluation framework, MQM, as a case study to understand how to set up reliable human evaluations that yield stable conclusions. We investigate the optimal configurations for item allocation to raters, number of ratings per item, and score normalization. Our study on two language pairs provides concrete recommendations for designing replicable human evaluation studies. We also collect and release the largest publicly available dataset of multi-segment translations rate
&lt;/p&gt;</description></item><item><title>OpenChemIE&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#21453;&#24212;&#25968;&#25454;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#20449;&#24687;&#20197;&#21450;&#20351;&#29992;&#19987;&#38376;&#31070;&#32463;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25991;&#26723;&#32423;&#21035;&#30340;&#21453;&#24212;&#25968;&#25454;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2404.01462</link><description>&lt;p&gt;
OpenChemIE&#65306;&#29992;&#20110;&#21270;&#23398;&#25991;&#29486;&#20449;&#24687;&#25552;&#21462;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
OpenChemIE: An Information Extraction Toolkit For Chemistry Literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01462
&lt;/p&gt;
&lt;p&gt;
OpenChemIE&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#21453;&#24212;&#25968;&#25454;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#20449;&#24687;&#20197;&#21450;&#20351;&#29992;&#19987;&#38376;&#31070;&#32463;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25991;&#26723;&#32423;&#21035;&#30340;&#21453;&#24212;&#25968;&#25454;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01462v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#23545;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#21270;&#23398;&#30340;&#26368;&#26032;&#21453;&#24212;&#25968;&#25454;&#24211;&#33267;&#20851;&#37325;&#35201;&#12290;&#23436;&#25972;&#30340;&#20449;&#24687;&#25552;&#21462;&#38656;&#35201;&#32467;&#21512;&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#30740;&#31350;&#20174;&#21333;&#19968;&#26041;&#24335;&#25552;&#21462;&#21453;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenChemIE&#26469;&#35299;&#20915;&#36825;&#19968;&#22797;&#26434;&#25361;&#25112;&#65292;&#23454;&#29616;&#22312;&#25991;&#26723;&#32423;&#21035;&#25552;&#21462;&#21453;&#24212;&#25968;&#25454;&#12290;OpenChemIE&#20998;&#20004;&#27493;&#35299;&#20915;&#38382;&#39064;&#65306;&#20174;&#21508;&#20010;&#26041;&#24335;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#28982;&#21518;&#25972;&#21512;&#32467;&#26524;&#24471;&#21040;&#26368;&#32456;&#30340;&#21453;&#24212;&#21015;&#34920;&#12290;&#23545;&#20110;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#37319;&#29992;&#19987;&#38376;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#22788;&#29702;&#21270;&#23398;&#20449;&#24687;&#25552;&#21462;&#30340;&#29305;&#23450;&#20219;&#21153;&#65292;&#27604;&#22914;&#20174;&#25991;&#26412;&#25110;&#22270;&#20687;&#20013;&#35299;&#26512;&#20998;&#23376;&#25110;&#21453;&#24212;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#21270;&#23398;&#30456;&#20851;&#30340;&#31639;&#27861;&#25972;&#21512;&#36825;&#20123;&#27169;&#22359;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#31934;&#32454;&#21270;&#21453;&#24212;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01462v1 Announce Type: cross  Abstract: Information extraction from chemistry literature is vital for constructing up-to-date reaction databases for data-driven chemistry. Complete extraction requires combining information across text, tables, and figures, whereas prior work has mainly investigated extracting reactions from single modalities. In this paper, we present OpenChemIE to address this complex challenge and enable the extraction of reaction data at the document level. OpenChemIE approaches the problem in two steps: extracting relevant information from individual modalities and then integrating the results to obtain a final list of reactions. For the first step, we employ specialized neural models that each address a specific task for chemistry information extraction, such as parsing molecules or reactions from text or figures. We then integrate the information from these modules using chemistry-informed algorithms, allowing for the extraction of fine-grained reactio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#21019;&#24314;&#20102;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;</title><link>https://arxiv.org/abs/2404.01461</link><description>&lt;p&gt;
&#35831;&#30495;&#27491;&#30340;&#29747;&#36798;&#31449;&#20986;&#26469;...&#38754;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#65311;&#22312;LLMs&#20013;&#23457;&#35270;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;
&lt;/p&gt;
&lt;p&gt;
Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#21019;&#24314;&#20102;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#23637;&#29616;&#20986;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#21487;&#33021;&#20250;&#23481;&#26131;&#21463;&#21040;&#20154;&#31867;&#20915;&#31574;&#20013;&#30340;&#19968;&#31181;&#24120;&#35265;&#35748;&#30693;&#38519;&#38449;&#24433;&#21709;&#65292;&#21363;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#12290;&#36825;&#26159;&#24515;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#26681;&#25454;&#20107;&#20214;&#19982;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#21407;&#22411;&#25110;&#20856;&#22411;&#20363;&#23376;&#30340;&#30456;&#20284;&#31243;&#24230;&#26469;&#21028;&#26029;&#20107;&#20214;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#20107;&#23454;&#25110;&#32479;&#35745;&#35777;&#25454;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#23545;LLM&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;REHEAT&#65288;Representativeness Heuristic AI Testing&#65289;&#65292;&#19968;&#20010;&#21253;&#21547;&#28085;&#30422;&#20845;&#31181;&#24120;&#35265;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#31867;&#22411;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;&#24212;&#29992;&#20110;REHEAT&#30340;&#22235;&#20010;LLMs&#37117;&#34920;&#29616;&#20986;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#27493;&#39588;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01461v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#26102;&#38388;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#37325;&#28857;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#26102;&#38388;&#20851;&#31995;&#19978;&#23384;&#22312;&#20559;&#21521;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01453</link><description>&lt;p&gt;
&#25581;&#31034;LLMs&#22312;&#26102;&#38388;&#25968;&#25454;&#19978;&#30340;&#20998;&#27495;&#24402;&#32435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unveiling Divergent Inductive Biases of LLMs on Temporal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#26102;&#38388;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#37325;&#28857;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#26102;&#38388;&#20851;&#31995;&#19978;&#23384;&#22312;&#20559;&#21521;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#33258;&#28982;&#35821;&#35328;&#20107;&#20214;&#30340;&#24494;&#22937;&#32454;&#33410;&#38656;&#35201;&#23545;&#26102;&#38388;&#21160;&#24577;&#36827;&#34892;&#24494;&#22937;&#29702;&#35299;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#25968;&#25454;&#20013;&#36776;&#21035;&#27169;&#24335;&#21644;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#24471;&#24456;&#29087;&#32451;&#65292;&#20294;&#23427;&#20204;&#23545;&#26102;&#38388;&#21160;&#24577;&#30340;&#20869;&#22312;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#22312;LLMs&#20013;&#32454;&#33268;&#25506;&#32034;&#36825;&#20123;&#22266;&#26377;&#25361;&#25112;&#65292;&#29305;&#21035;&#24378;&#35843;&#35780;&#20272;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#26102;&#38388;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#37319;&#29992;&#38382;&#31572;&#65288;QA&#65289;&#26684;&#24335;&#21644;&#25991;&#26412;&#34164;&#28085;&#65288;TE&#65289;&#26684;&#24335;&#20004;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31867;&#22411;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#28145;&#20837;&#25506;&#31350;&#20102;&#38544;&#24335;&#21644;&#26174;&#24335;&#20107;&#20214;&#12290;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#19968;&#20123;&#26174;&#33879;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;GPT-3.5&#21644;GPT-4&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20542;&#21521;&#20110;&#29305;&#23450;&#26102;&#38388;&#20851;&#31995;&#30340;&#20559;&#35265;&#28014;&#20986;&#27700;&#38754;&#65292;&#20854;&#20013;&#22312;QA&#26684;&#24335;&#20013;&#65292;GPT-3.5&#22312;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#38754;&#22343;&#34920;&#29616;&#20986;&#23545;"AFTER"&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01453v1 Announce Type: cross  Abstract: Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics. Despite the adeptness of Large Language Models (LLMs) in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge. This research meticulously explores these intrinsic challenges within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis of temporal data. Employing two distinct prompt types, namely Question Answering (QA) format and Textual Entailment (TE) format, our analysis probes into both implicit and explicit events. The findings underscore noteworthy trends, revealing disparities in the performance of GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships come to light, with GPT-3.5 demonstrating a preference for "AFTER'' in the QA format for both implicit and explicit
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32467;&#21512;&#20026;&#20225;&#19994;&#25552;&#20379;&#20102;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#35821;&#20041;&#20016;&#23500;&#30340;&#26041;&#24335;&#26469;&#32452;&#32455;&#21644;&#29702;&#35299;&#25968;&#25454;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#31181;&#32452;&#21512;&#24102;&#26469;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#28085;&#30422;&#20102;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#12289;&#25512;&#29702;&#20197;&#21450;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;NLP&#20219;&#21153;&#19977;&#20010;&#26680;&#24515;&#39046;&#22495;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.01443</link><description>&lt;p&gt;
&#20225;&#19994;&#24212;&#29992;&#26696;&#20363;&#20013;&#30693;&#35782;&#22270;&#35889;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Enterprise Use Cases Combining Knowledge Graphs and Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01443
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32467;&#21512;&#20026;&#20225;&#19994;&#25552;&#20379;&#20102;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#35821;&#20041;&#20016;&#23500;&#30340;&#26041;&#24335;&#26469;&#32452;&#32455;&#21644;&#29702;&#35299;&#25968;&#25454;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#31181;&#32452;&#21512;&#24102;&#26469;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#28085;&#30422;&#20102;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#12289;&#25512;&#29702;&#20197;&#21450;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;NLP&#20219;&#21153;&#19977;&#20010;&#26680;&#24515;&#39046;&#22495;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#31649;&#29702;&#26159;&#24403;&#20170;&#25968;&#23383;&#19990;&#30028;&#20013;&#20225;&#19994;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#38543;&#30528;&#29983;&#25104;&#21644;&#25910;&#38598;&#30340;&#25968;&#25454;&#37327;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#35821;&#20041;&#20016;&#23500;&#30340;&#26041;&#24335;&#26469;&#32452;&#32455;&#21644;&#29702;&#35299;&#25968;&#25454;&#12290;&#26412;&#25991;&#22312;&#26368;&#36817;&#23545;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#30740;&#31350;&#25991;&#29486;&#36827;&#34892;&#35843;&#26597;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#22522;&#20110;&#20225;&#19994;&#32972;&#26223;&#19979;&#36873;&#23450;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#27492;&#31867;&#32452;&#21512;&#24102;&#26469;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#12289;&#25512;&#29702;&#20197;&#21450;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;NLP&#20219;&#21153;&#19977;&#20010;&#26680;&#24515;&#39046;&#22495;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#38500;&#20102;&#35299;&#37322;&#21019;&#26032;&#30340;&#20225;&#19994;&#24212;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#30340;&#25104;&#29087;&#24230;&#65292;&#24182;&#26368;&#21518;&#23637;&#26395;&#20102;&#26410;&#26469;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01443v1 Announce Type: new  Abstract: Knowledge management is a critical challenge for enterprises in today's digital world, as the volume and complexity of data being generated and collected continue to grow incessantly. Knowledge graphs (KG) emerged as a promising solution to this problem by providing a flexible, scalable, and semantically rich way to organize and make sense of data. This paper builds upon a recent survey of the research literature on combining KGs and Natural Language Processing (NLP). Based on selected application scenarios from enterprise context, we discuss synergies that result from such a combination. We cover various approaches from the three core areas of KG construction, reasoning as well as KG-based NLP tasks. In addition to explaining innovative enterprise use cases, we assess their maturity in terms of practical applicability and conclude with an outlook on emergent application areas for the future.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22312;&#32447;&#25991;&#26412;&#28040;&#24687;&#20013;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;&#25152;&#34920;&#36798;&#24773;&#24863;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#65292;&#33410;&#30465;&#20102;&#23453;&#36149;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.01439</link><description>&lt;p&gt;
&#20174;&#25551;&#36848;&#20013;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#21019;&#24314;&#34920;&#24773;&#31526;&#21495;&#35789;&#24211;
&lt;/p&gt;
&lt;p&gt;
Creating emoji lexica from unsupervised sentiment analysis of their descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22312;&#32447;&#25991;&#26412;&#28040;&#24687;&#20013;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;&#25152;&#34920;&#36798;&#24773;&#24863;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#65292;&#33410;&#30465;&#20102;&#23453;&#36149;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#19978;&#23186;&#20307;&#65292;&#22914;&#21338;&#23458;&#21644;&#31038;&#20132;&#32593;&#32476;&#32593;&#31449;&#65292;&#29983;&#25104;&#20102;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20379;&#20998;&#26512;&#20010;&#20154;&#21644;&#32452;&#32455;&#30340;&#35266;&#28857;&#21644;&#24773;&#24863;&#20043;&#29992;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#24050;&#32463;&#19981;&#33021;&#24456;&#22909;&#22320;&#37327;&#21270;&#36825;&#20123;&#35266;&#28857;&#30340;&#26497;&#24615;&#24230;&#37327;&#65292;&#22240;&#27492;&#38656;&#35201;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#34920;&#24773;&#31526;&#21495;&#25152;&#34920;&#36798;&#30340;&#24773;&#24863;&#24471;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#22235;&#24180;&#38388;&#65292;&#31526;&#21495;&#30340;&#20351;&#29992;&#37327;&#28608;&#22686;&#12290;&#22914;&#20170;&#65292;Twitter&#20013;&#27599;&#22825;&#20250;&#34987;&#36755;&#20837;&#22823;&#32422;&#20004;&#30334;&#20159;&#20010;&#31526;&#21495;&#65292;&#24182;&#19988;&#27599;&#20010;&#26032;&#30340;Unicode&#29256;&#26412;&#37117;&#20250;&#22686;&#21152;&#26032;&#30340;&#34920;&#24773;&#31526;&#21495;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22312;&#32447;&#25991;&#26412;&#28040;&#24687;&#65288;&#22914;&#25512;&#25991;&#65289;&#20013;&#34920;&#24773;&#31526;&#21495;&#34920;&#36798;&#30340;&#24773;&#24863;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20154;&#24037;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#65292;&#20197;&#27492;&#33410;&#30465;&#23453;&#36149;&#30340;&#26102;&#38388;&#29992;&#20110;&#20854;&#20182;&#20998;&#26512;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#33258;&#21160;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#24773;&#31526;&#21495;&#24773;&#24863;&#35789;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01439v1 Announce Type: cross  Abstract: Online media, such as blogs and social networking sites, generate massive volumes of unstructured data of great interest to analyze the opinions and sentiments of individuals and organizations. Novel approaches beyond Natural Language Processing are necessary to quantify these opinions with polarity metrics. So far, the sentiment expressed by emojis has received little attention. The use of symbols, however, has boomed in the past four years. About twenty billion are typed in Twitter nowadays, and new emojis keep appearing in each new Unicode version, making them increasingly relevant to sentiment analysis tasks. This has motivated us to propose a novel approach to predict the sentiments expressed by emojis in online textual messages, such as tweets, that does not require human effort to manually annotate data and saves valuable time for other analysis tasks. For this purpose, we automatically constructed a novel emoji sentiment lexico
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#20027;&#35201;&#28304;&#20110;&#19981;&#21516;&#27169;&#22411;&#30340;&#22266;&#26377;&#20301;&#32622;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01430</link><description>&lt;p&gt;
&#38477;&#20302;LLMs&#20013;&#20301;&#32622;&#20559;&#24046;&#30340;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#20027;&#35201;&#28304;&#20110;&#19981;&#21516;&#27169;&#22411;&#30340;&#22266;&#26377;&#20301;&#32622;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22686;&#24378;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#28041;&#21450;&#20174;&#22806;&#37096;&#25968;&#25454;&#23384;&#20648;&#24211;&#26816;&#32034;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#19968;&#36827;&#23637;&#23588;&#20026;&#20851;&#38190;&#65292;&#22240;&#20026;&#21487;&#33021;&#28041;&#21450;&#38271;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;LLMs&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#65292;&#34920;&#26126;&#20854;&#24615;&#33021;&#20250;&#26681;&#25454;&#36755;&#20837;&#24207;&#21015;&#20013;&#26377;&#29992;&#20449;&#24687;&#30340;&#20301;&#32622;&#32780;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#20301;&#32622;&#20559;&#24046;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#28304;&#20110;&#19981;&#21516;&#27169;&#22411;&#30340;&#22266;&#26377;&#20301;&#32622;&#20559;&#22909;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20165;&#20165;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#20811;&#26381;&#20301;&#32622;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PAPEFT&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#21547;&#19968;&#20010;&#25968;&#25454;&#22686;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01430v1 Announce Type: cross  Abstract: Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.01413</link><description>&lt;p&gt;
&#27169;&#22411;&#23849;&#28291;&#26159;&#21542;&#19981;&#21487;&#36991;&#20813;&#65311;&#36890;&#36807;&#32047;&#31215;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#25171;&#30772;&#36882;&#24402;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#19978;&#30340;&#39044;&#35757;&#32451;&#65292;&#19968;&#20010;&#21450;&#26102;&#30340;&#38382;&#39064;&#28014;&#20986;&#27700;&#38754;&#65306;&#24403;&#36825;&#20123;&#27169;&#22411;&#34987;&#35757;&#32451;&#22312;&#23427;&#20204;&#33258;&#24049;&#29983;&#25104;&#30340;&#36755;&#20986;&#19978;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#26368;&#36817;&#23545;&#27169;&#22411;&#25968;&#25454;&#21453;&#39304;&#24490;&#29615;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#65292;&#21363;&#24615;&#33021;&#38543;&#30528;&#27599;&#27425;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#36880;&#28176;&#19979;&#38477;&#65292;&#30452;&#21040;&#26368;&#26032;&#30340;&#27169;&#22411;&#21464;&#24471;&#26080;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20960;&#31687;&#30740;&#31350;&#27169;&#22411;&#23849;&#28291;&#30340;&#35770;&#25991;&#37117;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#65292;&#26032;&#25968;&#25454;&#20250;&#21462;&#20195;&#26087;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#20551;&#35774;&#25968;&#25454;&#20250;&#38543;&#26102;&#38388;&#32047;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;&#31215;&#32047;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#35299;&#26512;&#21487;&#22788;&#29702;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#31995;&#21015;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#20808;&#21069;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22914;&#26524;&#25968;&#25454;&#34987;&#26367;&#25442;&#65292;&#27979;&#35797;&#35823;&#24046;&#20250;&#38543;&#30528;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#27425;&#25968;&#32447;&#24615;&#22686;&#21152;&#65307;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20010;&#30740;&#31350;&#25506;&#35752;&#20102;&#25968;&#25454;&#36880;&#28176;&#32047;&#31215;&#30340;&#24773;&#20917;&#19979;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01413v1 Announce Type: cross  Abstract: The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;SR$_{\text{LLM}}$&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#20840;&#38754;&#30340;&#23433;&#20840;&#39118;&#38505;&#20998;&#31867;&#27861;&#21644;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#36890;&#36807;&#25351;&#20196;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26377;&#25928;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01399</link><description>&lt;p&gt;
&#24320;&#21457;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; - &#19968;&#20010;&#20840;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Developing Safe and Responsible Large Language Models -- A Comprehensive Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;SR$_{\text{LLM}}$&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#20840;&#38754;&#30340;&#23433;&#20840;&#39118;&#38505;&#20998;&#31867;&#27861;&#21644;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#36890;&#36807;&#25351;&#20196;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26377;&#25928;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20154;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#24615;&#21644;&#39118;&#38505;&#26085;&#30410;&#20851;&#27880;&#65292;&#21457;&#23637;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SR$_{\text{LLM}}$&#65289;&#65292;&#36825;&#20010;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLM&#26469;&#22686;&#24378;&#35821;&#35328;&#29983;&#25104;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;LLM&#23433;&#20840;&#39118;&#38505;&#20998;&#31867;&#27861;&#65292;&#24182;&#21033;&#29992;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19982;&#36825;&#31181;&#20998;&#31867;&#27861;&#30456;&#19968;&#33268;&#12290;SR$_{\text{LLM}}$&#26088;&#22312;&#35782;&#21035;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#20869;&#23481;&#24182;&#20135;&#29983;&#33391;&#24615;&#21464;&#21270;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#19981;&#20165;&#26377;&#25928;&#22320;&#22686;&#24378;&#23433;&#20840;&#24615;&#65292;&#32780;&#19988;&#36164;&#28304;&#39640;&#25928;&#19988;&#26131;&#20110;&#35843;&#25972;&#12290;&#22312;&#25105;&#20204;&#23545;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#19987;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#23433;&#20840;&#20869;&#23481;&#29983;&#25104;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#26045;&#23433;&#20840;&#25514;&#26045;&#21518;&#65292;&#20986;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01399v1 Announce Type: new  Abstract: Given the growing concerns around the safety and risks of Large Language Models (LLMs), it is essential to develop methods for mitigating these issues. We introduce Safe and Responsible Large Language Model (SR$_{\text{LLM}}$) , a model designed to enhance the safety of language generation using LLMs. Our approach incorporates a comprehensive LLM safety risk taxonomy and utilizes a dataset annotated by experts that align with this taxonomy. SR$_{\text{LLM}}$ is designed to identify potentially unsafe content and produce benign variations. It employs instruction-based and parameter-efficient fine-tuning methods, making the model not only effective in enhancing safety but also resource-efficient and straightforward to adjust. Through our testing on five benchmark datasets and two proprietary datasets, we observed notable reductions in the generation of unsafe content. Moreover, following the implementation of safety measures, there was a s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01365</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM
&lt;/p&gt;
&lt;p&gt;
Prompt-prompted Mixture of Experts for Efficient LLM Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23454;&#29992;&#24615;&#65292;&#23427;&#20204;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#19968;&#20123;&#26041;&#27861;&#65292;&#22914;&#20462;&#21098;&#25110;&#26500;&#24314;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#65292;&#26088;&#22312;&#21033;&#29992;transformer&#21069;&#39304;&#65288;FF&#65289;&#22359;&#20013;&#30340;&#31232;&#30095;&#24615;&#65292;&#20197;&#25552;&#39640;&#36895;&#24230;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#21644;&#19981;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GRIFFIN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;MoE&#65292;&#23427;&#22312;&#24207;&#21015;&#32423;&#21035;&#20026;&#19981;&#21516;&#38750;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#22823;&#37327;LLMs&#36873;&#25321;&#29420;&#29305;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;&#36825;&#26159;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20851;&#38190;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#32463;&#36807;&#35757;&#32451;&#30340;LLMs&#22312;&#24207;&#21015;&#20013;&#33258;&#28982;&#20135;&#29983;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;FF&#28608;&#27963;&#27169;&#24335;&#65292;&#36825;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01365v1 Announce Type: cross  Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which
&lt;/p&gt;</description></item><item><title>LLM Attributor&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#26041;&#24335;&#29992;&#20110;&#23558;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#32467;&#26524;&#24402;&#22240;&#21040;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#24110;&#21161;&#29992;&#25143;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#12289;&#22686;&#24378;&#21487;&#20449;&#24230;&#65292;&#24182;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2404.01361</link><description>&lt;p&gt;
LLM Attributor: &#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24402;&#22240;&#29992;&#20110;LLM&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM Attributor: Interactive Visual Attribution for LLM Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01361
&lt;/p&gt;
&lt;p&gt;
LLM Attributor&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#26041;&#24335;&#29992;&#20110;&#23558;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#32467;&#26524;&#24402;&#22240;&#21040;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#24110;&#21161;&#29992;&#25143;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#12289;&#22686;&#24378;&#21487;&#20449;&#24230;&#65292;&#24182;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#20854;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20984;&#26174;&#20102;&#20102;&#35299;&#25991;&#26412;&#29983;&#25104;&#32972;&#21518;&#21407;&#22240;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM Attributor&#65292;&#19968;&#20010;&#25552;&#20379;LLM&#25991;&#26412;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#20132;&#20114;&#21487;&#35270;&#21270;&#30340;Python&#24211;&#12290;&#25105;&#20204;&#30340;&#24211;&#20026;&#24555;&#36895;&#23558;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#24402;&#22240;&#21040;&#35757;&#32451;&#25968;&#25454;&#28857;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#24335;&#65292;&#20197;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#12289;&#22686;&#24378;&#20854;&#21487;&#20449;&#24230;&#65292;&#24182;&#23558;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#24037;&#20855;&#30340;&#35270;&#35273;&#21644;&#20132;&#20114;&#35774;&#35745;&#65292;&#24182;&#24378;&#35843;LLaMA2&#27169;&#22411;&#30340;&#20351;&#29992;&#22330;&#26223;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65306;&#20851;&#20110;&#26368;&#36817;&#28798;&#38590;&#21644;&#37329;&#34701;&#30456;&#20851;&#38382;&#31572;&#23545;&#30340;&#22312;&#32447;&#25991;&#31456;&#12290;&#30001;&#20110;LLM Attributor&#23545;&#35745;&#31639;&#31508;&#35760;&#26412;&#30340;&#24191;&#27867;&#25903;&#25345;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#23558;&#20854;&#25972;&#21512;&#21040;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01361v1 Announce Type: cross  Abstract: While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#19982;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30456;&#20851;&#30340;21&#31181;&#28508;&#22312;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#26410;&#25253;&#21578;ASEs&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01358</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#21457;&#29616;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01358
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#19982;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30456;&#20851;&#30340;21&#31181;&#28508;&#22312;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#26410;&#25253;&#21578;ASEs&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#65288;ASEs&#65289;&#22312;FDA&#25209;&#20934;&#21518;&#34987;&#21457;&#29616;&#65292;&#23545;&#24739;&#32773;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#21450;&#26102;&#21457;&#29616;&#34987;&#24573;&#35270;&#30340;ASEs&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#24050;&#21457;&#34920;&#30340;&#20020;&#24202;&#30740;&#31350;&#12289;&#21046;&#36896;&#21830;&#25253;&#21578;&#21644;ChatGPT&#31561;&#22823;&#37327;&#20844;&#24320;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19982;&#32925;&#32032;&#26679;&#32957;1&#21463;&#20307;&#28608;&#21160;&#21058;&#65288;GLP-1 RA&#65289;&#30456;&#20851;&#30340;ASEs&#65292;&#36825;&#19968;&#24066;&#22330;&#39044;&#35745;&#21040;2030&#24180;&#23558;&#21576;&#25351;&#25968;&#22686;&#38271;&#33267;1335&#20159;&#32654;&#20803;&#12290;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#26816;&#27979;&#20986;FDA&#25209;&#20934;&#26102;&#34987;&#24573;&#35270;&#30340;21&#31181;&#28508;&#22312;ASEs&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#24443;&#24213;&#25913;&#21464;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#30456;&#20851;&#26410;&#25253;&#21578;&#30340;ASEs&#30340;&#26816;&#27979;&#65292;&#21033;&#29992;&#21069;&#27839;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#37322;&#25918;&#31038;&#20132;&#23186;&#20307;&#30340;&#21147;&#37327;&#26469;&#25903;&#25345;&#30417;&#31649;&#26426;&#26500;&#21644;&#21046;&#36896;&#21830;&#22312;&#24066;&#22330;&#19978;&#22686;&#21152;&#26032;&#33647;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01358v1 Announce Type: cross  Abstract: Adverse side effects (ASEs) of drugs, revealed after FDA approval, pose a threat to patient safety. To promptly detect overlooked ASEs, we developed a digital health methodology capable of analyzing massive public data from social media, published clinical research, manufacturers' reports, and ChatGPT. We uncovered ASEs associated with the glucagon-like peptide 1 receptor agonists (GLP-1 RA), a market expected to grow exponentially to $133.5 billion USD by 2030. Using a Named Entity Recognition (NER) model, our method successfully detected 21 potential ASEs overlooked upon FDA approval, including irritability and numbness. Our data-analytic approach revolutionizes the detection of unreported ASEs associated with newly deployed drugs, leveraging cutting-edge AI-driven social media analytics. It can increase the safety of new drugs in the marketplace by unlocking the power of social media to support regulators and manufacturers in the ra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#21830;&#19994;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.01353</link><description>&lt;p&gt;
&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;LLM&#39640;&#25928;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficiently Distilling LLMs for Edge Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01353
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#21830;&#19994;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;LLMs&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#20855;&#26377;&#24456;&#22823;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#23427;&#36171;&#20104;&#20102;&#20197;&#22266;&#23450;&#25104;&#26412;&#20135;&#29983;&#19981;&#21516;&#22823;&#23567;/&#24310;&#36831;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#21830;&#19994;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#34429;&#28982;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#23545;&#21387;&#32553;&#20855;&#26377;&#30456;&#24403;&#30340;&#25269;&#25239;&#21147;&#65292;&#20294;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#35299;&#30721;&#22120;&#36827;&#34892;&#20999;&#29255;&#20197;&#22823;&#24133;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01353v1 Announce Type: cross  Abstract: Supernet training of LLMs is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain high-quality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#20559;&#35265;&#22240;&#32032;&#30340;&#20998;&#26512;&#12289;&#20844;&#24179;&#24230;&#37327;&#21644;&#29616;&#26377;&#31639;&#27861;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2404.01349</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#19968;&#20010;&#20998;&#31867;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fairness in Large Language Models: A Taxonomic Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#20559;&#35265;&#22240;&#32032;&#30340;&#20998;&#26512;&#12289;&#20844;&#24179;&#24230;&#37327;&#21644;&#29616;&#26377;&#31639;&#27861;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31639;&#27861;&#32570;&#20047;&#20844;&#24179;&#24615;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#38024;&#23545;&#26576;&#20123;&#31038;&#21306;&#65292;&#29305;&#21035;&#26159;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#20419;&#20351;&#23545;&#20844;&#24179;&#30340;LLMs&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#12290;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#30456;&#21453;&#65292;&#22312;LLMs&#20013;&#30340;&#20844;&#24179;&#24615;&#28041;&#21450;&#29420;&#29305;&#30340;&#32972;&#26223;&#12289;&#20998;&#31867;&#27861;&#21644;&#23454;&#29616;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#35813;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#20844;&#24179;LLMs&#30340;&#29616;&#26377;&#25991;&#29486;&#30740;&#31350;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;LLMs&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#25509;&#30528;&#20998;&#26512;&#20102;&#23548;&#33268;LLMs&#20559;&#35265;&#30340;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#35752;&#35770;&#20102;LLMs&#20013;&#30340;&#20844;&#24179;&#27010;&#24565;&#65292;&#24635;&#32467;&#20102;&#35780;&#20272;LLMs&#20559;&#35265;&#30340;&#25351;&#26631;&#21644;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01349v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65292;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#35782;&#21035;&#34394;&#20551;&#26032;&#38395;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;99.16%</title><link>https://arxiv.org/abs/2404.01345</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;&#23391;&#21152;&#25289;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent Units and Deep Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01345
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65292;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#35782;&#21035;&#34394;&#20551;&#26032;&#38395;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;99.16%
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#30340;&#20852;&#36215;&#20351;&#24471;&#38656;&#35201;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#21464;&#24471;&#36234;&#21457;&#37325;&#35201;&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#33521;&#35821;&#35821;&#35328;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#35821;&#36825;&#31181;&#34987;&#35748;&#20026;&#19981;&#22826;&#37325;&#35201;&#30340;&#35821;&#35328;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422;50,000&#26465;&#26032;&#38395;&#39033;&#30446;&#30340;&#23436;&#25972;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12289;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#28151;&#21512;&#26550;&#26500;&#12290;&#23545;&#20110;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21033;&#29992;&#19968;&#31995;&#21015;&#26377;&#29992;&#25351;&#26631;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;F1&#24471;&#20998;&#21644;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#24212;&#29992;&#31243;&#24207;&#26469;&#23436;&#25104;&#36825;&#39033;&#24037;&#20316;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35797;&#39564;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35782;&#21035;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#34394;&#20551;&#26032;&#38395;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#21452;&#21521;GRU&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;&#24778;&#20154;&#30340;99.16%&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#35782;&#21035;&#34394;&#20551;&#26032;&#38395;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01345v1 Announce Type: new  Abstract: The rise of fake news has made the need for effective detection methods, including in languages other than English, increasingly important. The study aims to address the challenges of Bangla which is considered a less important language. To this end, a complete dataset containing about 50,000 news items is proposed. Several deep learning models have been tested on this dataset, including the bidirectional gated recurrent unit (GRU), the long short-term memory (LSTM), the 1D convolutional neural network (CNN), and hybrid architectures. For this research, we assessed the efficacy of the model utilizing a range of useful measures, including recall, precision, F1 score, and accuracy. This was done by employing a big application. We carry out comprehensive trials to show the effectiveness of these models in identifying bogus news in Bangla, with the Bidirectional GRU model having a stunning accuracy of 99.16%. Our analysis highlights the impo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#31867;&#20284;&#23454;&#20363;&#30340;&#30693;&#35782;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#26032;&#39062;&#25216;&#26415;&#26469;&#22686;&#24378;&#27861;&#24459;&#25991;&#20214;&#20462;&#36766;&#35282;&#33394;&#26631;&#27880;&#24615;&#33021;&#65292;&#22312;&#23439;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.01344</link><description>&lt;p&gt;
&#27880;&#24847;&#20320;&#30340;&#37051;&#23621;&#65306;&#21033;&#29992;&#31867;&#20284;&#23454;&#20363;&#36827;&#34892;&#27861;&#24459;&#25991;&#20214;&#30340;&#20462;&#36766;&#35282;&#33394;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Mind Your Neighbours: Leveraging Analogous Instances for Rhetorical Role Labeling for Legal Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01344
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#31867;&#20284;&#23454;&#20363;&#30340;&#30693;&#35782;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#26032;&#39062;&#25216;&#26415;&#26469;&#22686;&#24378;&#27861;&#24459;&#25991;&#20214;&#20462;&#36766;&#35282;&#33394;&#26631;&#27880;&#24615;&#33021;&#65292;&#22312;&#23439;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#21028;&#20915;&#30340;&#20462;&#36766;&#35282;&#33394;&#26631;&#27880;&#23545;&#20110;&#21508;&#31181;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#26696;&#20363;&#24635;&#32467;&#12289;&#35821;&#20041;&#25628;&#32034;&#21644;&#35770;&#28857;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#20174;&#19978;&#19979;&#25991;&#25512;&#26029;&#21477;&#23376;&#35282;&#33394;&#12289;&#30456;&#20114;&#20851;&#32852;&#30340;&#35282;&#33394;&#12289;&#26377;&#38480;&#30340;&#24102;&#26631;&#27880;&#25968;&#25454;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#26032;&#39062;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#23454;&#20363;&#65288;&#37051;&#23621;&#65289;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#20462;&#36766;&#35282;&#33394;&#26631;&#27880;&#24615;&#33021;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#25512;&#29702;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23439;F1&#20998;&#25968;&#19978;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;&#23545;&#20110;&#22522;&#20110;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22686;&#24378;&#26631;&#31614;&#39044;&#27979;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#25554;&#20540;&#25216;&#26415;&#12290;&#32780;&#22312;&#22522;&#20110;&#35757;&#32451;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#21407;&#22411;&#23398;&#20064;&#19982;&#25105;&#20204;&#30340;&#26032;&#39062;&#35770;&#36848;&#24863;&#30693;&#23545;&#27604;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30452;&#25509;&#22312;&#23884;&#20837;&#31354;&#38388;&#19978;&#36827;&#34892;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#36801;&#31227;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01344v1 Announce Type: new  Abstract: Rhetorical Role Labeling (RRL) of legal judgments is essential for various tasks, such as case summarization, semantic search and argument mining. However, it presents challenges such as inferring sentence roles from context, interrelated roles, limited annotated data, and label imbalance. This study introduces novel techniques to enhance RRL performance by leveraging knowledge from semantically similar instances (neighbours). We explore inference-based and training-based approaches, achieving remarkable improvements in challenging macro-F1 scores. For inference-based methods, we explore interpolation techniques that bolster label predictions without re-training. While in training-based methods, we integrate prototypical learning with our novel discourse-aware contrastive method that work directly on embedding spaces. Additionally, we assess the cross-domain applicability of our methods, demonstrating their effectiveness in transferring 
&lt;/p&gt;</description></item><item><title>CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01343</link><description>&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01343
&lt;/p&gt;
&lt;p&gt;
CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#21644;&#36719;&#20214;&#24179;&#21488;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#20351;&#29992;&#20687;GPT-3.5&#12289;GPT-4&#12289;GLM-3&#21644;LLaMa-2&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23458;&#25143;&#26381;&#21153;&#30340;&#32842;&#22825;&#36741;&#21161;&#25110;&#25512;&#29702;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#23458;&#25143;&#26381;&#21153;&#27169;&#22411;&#22312;&#19982;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#30340;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#24182;&#19988;&#32570;&#20047;&#26377;&#25928;&#26381;&#21153;&#25152;&#38656;&#30340;&#25805;&#20316;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#65288;CHat with custOmer Profile in existing System&#65289;&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#65306;&#65288;1&#65289;&#39640;&#25928;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#20197;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#25110;&#25353;&#29031;&#29616;&#26377;&#25351;&#21335;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#65307;&#65288;2&#65289;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#22312;&#31995;&#32479;&#20013;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#65307;&#65288;3&#65289;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01343v1 Announce Type: cross  Abstract: Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverag
&lt;/p&gt;</description></item><item><title>DiffAgent&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#24037;&#20855;&#65292;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;API&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;DABench&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01342</link><description>&lt;p&gt;
DiffAgent&#65306;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;API&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01342
&lt;/p&gt;
&lt;p&gt;
DiffAgent&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#24037;&#20855;&#65292;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;API&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;DABench&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#22312;&#23398;&#26415;&#30740;&#31350;&#20197;&#21450;&#20854;&#20182;&#39046;&#22495;&#25214;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24037;&#20855;&#20351;&#29992;&#30740;&#31350;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24341;&#20837;&#20102;DiffAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#35774;&#35745;&#29992;&#20110;&#36890;&#36807;API&#35843;&#29992;&#22312;&#20960;&#31186;&#38047;&#20869;&#36827;&#34892;&#20934;&#30830;&#36873;&#25321;&#30340;LLM&#20195;&#29702;&#12290;DiffAgent&#21033;&#29992;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;SFTA&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#31934;&#30830;&#22320;&#23558;T2I API&#21709;&#24212;&#19982;&#29992;&#25143;&#36755;&#20837;&#23545;&#40784;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;DiffAgent&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DABench&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;T2I API&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01342v1 Announce Type: cross  Abstract: Text-to-image (T2I) generative models have attracted significant attention and found extensive applications within and beyond academic research. For example, the Civitai community, a platform for T2I innovation, currently hosts an impressive array of 74,492 distinct models. However, this diversity presents a formidable challenge in selecting the most appropriate model and parameters, a process that typically requires numerous trials. Drawing inspiration from the tool usage research of large language models (LLMs), we introduce DiffAgent, an LLM agent designed to screen the accurate selection in seconds via API calls. DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences. To train and evaluate DiffAgent's capabilities, we present DABench, a comprehensive dataset encompassing an extensive range of T2I APIs from the community. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#65292;&#36890;&#36807;&#38646;-shot&#35774;&#32622;&#20013;&#24341;&#20837;&#20154;&#31867;&#24773;&#24863;&#21644;&#19981;&#27969;&#30021;&#29305;&#24449;&#65292;&#20351;&#24471;&#31995;&#32479;&#33021;&#26356;&#22909;&#22320;&#27169;&#20223;&#20154;&#31867;&#35821;&#38899;&#65292;&#20419;&#36827;&#26356;&#33258;&#28982;&#30340;&#29992;&#25143;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2404.01339</link><description>&lt;p&gt;
&#36890;&#36807;&#38646;-shot&#24773;&#32490;&#21644;&#19981;&#27969;&#30021;&#29983;&#25104;&#23454;&#29616;&#20154;&#24615;&#21270;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01339
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#65292;&#36890;&#36807;&#38646;-shot&#35774;&#32622;&#20013;&#24341;&#20837;&#20154;&#31867;&#24773;&#24863;&#21644;&#19981;&#27969;&#30021;&#29305;&#24449;&#65292;&#20351;&#24471;&#31995;&#32479;&#33021;&#26356;&#22909;&#22320;&#27169;&#20223;&#20154;&#31867;&#35821;&#38899;&#65292;&#20419;&#36827;&#26356;&#33258;&#28982;&#30340;&#29992;&#25143;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#23545;&#35805;&#31995;&#32479;&#24448;&#24448;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#23616;&#38480;&#24615;&#65306;&#23427;&#20204;&#30340;&#22238;&#24212;&#32570;&#20047;&#20154;&#31867;&#20132;&#20114;&#30340;&#24773;&#24863;&#28145;&#24230;&#21644;&#19981;&#27969;&#30021;&#29305;&#24449;&#12290;&#36825;&#31181;&#32570;&#22833;&#22312;&#29992;&#25143;&#23547;&#27714;&#26356;&#20010;&#24615;&#21270;&#21644;&#26377;&#20849;&#24773;&#30340;&#20114;&#21160;&#26102;&#23588;&#20026;&#26126;&#26174;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#23427;&#20204;&#26174;&#24471;&#26426;&#26800;&#21270;&#65292;&#38590;&#20197;&#24341;&#36215;&#20154;&#31867;&#29992;&#25143;&#30340;&#20849;&#40483;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30528;&#25163;&#20110;&#20154;&#24615;&#21270;&#26426;&#22120;&#36890;&#20449;&#30340;&#26053;&#31243;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#19981;&#20165;&#29702;&#35299;&#32780;&#19988;&#20849;&#40483;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35821;&#38899;&#21512;&#25104;&#27969;&#31243;&#12290;&#22312;&#36825;&#19968;&#26694;&#26550;&#20869;&#65292;&#19968;&#31181;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#24341;&#20837;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#24773;&#24863;&#21644;&#35821;&#35328;&#32010;&#20081;&#12290;&#36825;&#20123;&#22797;&#26434;&#24615;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26399;&#38388;&#26080;&#32541;&#38598;&#25104;&#21040;&#29983;&#25104;&#25991;&#26412;&#20013;&#65292;&#20351;&#31995;&#32479;&#26356;&#22909;&#22320;&#27169;&#20223;&#20154;&#31867;&#35821;&#38899;&#27169;&#24335;&#65292;&#20419;&#36827;&#26356;&#30452;&#35266;&#21644;&#33258;&#28982;&#30340;&#29992;&#25143;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01339v1 Announce Type: cross  Abstract: Contemporary conversational systems often present a significant limitation: their responses lack the emotional depth and disfluent characteristic of human interactions. This absence becomes particularly noticeable when users seek more personalized and empathetic interactions. Consequently, this makes them seem mechanical and less relatable to human users. Recognizing this gap, we embarked on a journey to humanize machine communication, to ensure AI systems not only comprehend but also resonate. To address this shortcoming, we have designed an innovative speech synthesis pipeline. Within this framework, a cutting-edge language model introduces both human-like emotion and disfluencies in a zero-shot setting. These intricacies are seamlessly integrated into the generated text by the language model during text generation, allowing the system to mirror human speech patterns better, promoting more intuitive and natural user interactions. The
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;Latent Dirichlet Allocation (LDA)&#36827;&#34892;&#30456;&#20851;&#30340;&#20027;&#39064;&#24314;&#27169;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#26816;&#27979;&#36130;&#32463;&#20107;&#20214;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12289;&#39044;&#27979;&#21644;&#39044;&#27979;</title><link>https://arxiv.org/abs/2404.01338</link><description>&lt;p&gt;
&#36890;&#36807;Latent Dirichlet Allocation&#20027;&#39064;&#24314;&#27169;&#33258;&#21160;&#26816;&#27979;&#36130;&#32463;&#26032;&#38395;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12289;&#39044;&#27979;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;Latent Dirichlet Allocation (LDA)&#36827;&#34892;&#30456;&#20851;&#30340;&#20027;&#39064;&#24314;&#27169;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#26816;&#27979;&#36130;&#32463;&#20107;&#20214;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12289;&#39044;&#27979;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01338v1&#36890;&#21578;&#31867;&#22411;:&#26032;&#25688;&#35201;:&#37329;&#34701;&#26032;&#38395;&#26159;&#19968;&#31181;&#38750;&#32467;&#26500;&#21270;&#30340;&#20449;&#24687;&#28304;&#65292;&#21487;&#20197;&#24320;&#37319;&#20197;&#20174;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#29992;&#20110;&#24066;&#22330;&#31579;&#36873;&#24212;&#29992;&#12290;&#20174;&#25345;&#32493;&#30340;&#37329;&#34701;&#26032;&#38395;&#27969;&#20013;&#25163;&#21160;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#26159;&#32321;&#29712;&#30340;&#65292;&#36229;&#20986;&#20102;&#35768;&#22810;&#25237;&#36164;&#32773;&#30340;&#25216;&#33021;&#33539;&#22260;&#65292;&#20182;&#20204;&#26368;&#22810;&#21482;&#33021;&#20851;&#27880;&#20960;&#20010;&#26469;&#28304;&#21644;&#20316;&#32773;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23545;&#37329;&#34701;&#26032;&#38395;&#30340;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#30456;&#20851;&#25991;&#26412;&#65292;&#24182;&#22312;&#35813;&#25991;&#26412;&#20013;&#36827;&#34892;&#39044;&#27979;&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#36890;&#36807;&#32771;&#34385;&#35805;&#35821;&#23618;&#38754;&#19978;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#24577;&#24615;&#65292;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#26816;&#27979;&#30456;&#20851;&#30340;&#36130;&#32463;&#20107;&#20214;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#20998;&#21106;&#20197;&#23558;&#30456;&#20851;&#25991;&#26412;&#24402;&#20026;&#19968;&#32452;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24212;&#29992;&#20849;&#25351;&#35299;&#26512;&#26469;&#21457;&#29616;&#27573;&#33853;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;Latent Dirichlet Allocation (LDA)&#36827;&#34892;&#30456;&#20851;&#20027;&#39064;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01338v1 Announce Type: new  Abstract: Financial news items are unstructured sources of information that can be mined to extract knowledge for market screening applications. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevant text and, within that text, forecasts and predictions. We propose a novel Natural Language Processing (NLP) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment the text to group together closely related text. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant topic modelling with Latent Dirichlet Allocation (LDA) to separ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22312;&#37329;&#34701;&#26032;&#38395;&#20013;&#26816;&#27979;&#31687;&#31456;&#32423;&#21035;&#30340;&#20851;&#38190;&#22768;&#26126;&#30340;&#26102;&#38388;&#24615;&#65292;&#20197;&#20998;&#26512;&#21477;&#27861;&#21644;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#65292;&#21306;&#20998;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#26377;&#20215;&#20540;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01337</link><description>&lt;p&gt;
&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#37329;&#34701;&#26032;&#38395;&#20013;&#26816;&#27979;&#31687;&#31456;&#32423;&#21035;&#30340;&#26102;&#38388;&#24615;
&lt;/p&gt;
&lt;p&gt;
Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01337
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22312;&#37329;&#34701;&#26032;&#38395;&#20013;&#26816;&#27979;&#31687;&#31456;&#32423;&#21035;&#30340;&#20851;&#38190;&#22768;&#26126;&#30340;&#26102;&#38388;&#24615;&#65292;&#20197;&#20998;&#26512;&#21477;&#27861;&#21644;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#65292;&#21306;&#20998;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#26377;&#20215;&#20540;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Finance-related news, such as Bloomberg News, CNN Business, and Forbes, provide valuable real data for market screening systems. Experts in these news articles not only provide technical analyses but also share opinions considering political, sociological, and cultural factors. We propose a novel system that utilizes Natural Language Processing and Machine Learning techniques to detect the temporality of key statements in finance-related news at the discourse level, aiming to differentiate between context information and valuable predictions by analyzing syntactic and semantic dependencies.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01337v1 Announce Type: new  Abstract: Finance-related news such as Bloomberg News, CNN Business and Forbes are valuable sources of real data for market screening systems. In news, an expert shares opinions beyond plain technical analyses that include context such as political, sociological and cultural factors. In the same text, the expert often discusses the performance of different assets. Some key statements are mere descriptions of past events while others are predictions. Therefore, understanding the temporality of the key statements in a text is essential to separate context information from valuable predictions. We propose a novel system to detect the temporality of finance-related news at discourse level that combines Natural Language Processing and Machine Learning techniques, and exploits sophisticated features such as syntactic and semantic dependencies. More specifically, we seek to extract the dominant tenses of the main statements, which may be either explicit 
&lt;/p&gt;</description></item><item><title>FineFake &#25968;&#25454;&#38598;&#20026;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#25552;&#20379;&#20102;&#30693;&#35782;&#22686;&#24378;&#65292;&#21253;&#21547;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#35206;&#30422;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2404.01336</link><description>&lt;p&gt;
FineFake&#65306;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#30693;&#35782;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01336
&lt;/p&gt;
&lt;p&gt;
FineFake &#25968;&#25454;&#38598;&#20026;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#25552;&#20379;&#20102;&#30693;&#35782;&#22686;&#24378;&#65292;&#21253;&#21547;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#35206;&#30422;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#35780;&#20272;&#26032;&#38395;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#36890;&#24120;&#20165;&#20851;&#27880;&#21333;&#19968;&#35821;&#20041;&#20027;&#39064;&#30340;&#26032;&#38395;&#25110;&#26469;&#33258;&#21333;&#19968;&#24179;&#21488;&#30340;&#26032;&#38395;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#30495;&#23454;&#22330;&#26223;&#20013;&#22810;&#39046;&#22495;&#26032;&#38395;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#20102;&#35299;&#19981;&#21516;&#39046;&#22495;&#30340;&#20551;&#26032;&#38395;&#65292;&#22806;&#37096;&#30693;&#35782;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#25552;&#20379;&#31934;&#30830;&#35777;&#25454;&#24182;&#25581;&#31034;&#21046;&#36896;&#20551;&#26032;&#38395;&#30340;&#22810;&#26679;&#28508;&#22312;&#31574;&#30053;&#65292;&#32780;&#36825;&#20063;&#26159;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#25152;&#24573;&#30053;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;FineFake&#30340;&#26032;&#22411;&#22810;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#32454;&#31890;&#24230;&#27880;&#37322;&#12290;FineFake&#28085;&#30422;&#20102;&#26469;&#33258;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#30340;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#27599;&#20010;&#26032;&#38395;&#39033;&#30446;&#37117;&#21253;&#21547;&#22810;&#27169;&#24577;&#20869;&#23481;&#12289;&#28508;&#22312;&#31038;&#20132;&#32972;&#26223;&#12289;&#21322;&#33258;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01336v1 Announce Type: cross  Abstract: Existing benchmarks for fake news detection have significantly contributed to the advancement of models in assessing the authenticity of news content. However, these benchmarks typically focus solely on news pertaining to a single semantic topic or originating from a single platform, thereby failing to capture the diversity of multi-domain news in real scenarios. In order to understand fake news across various domains, the external knowledge and fine-grained annotations are indispensable to provide precise evidence and uncover the diverse underlying strategies for fabrication, which are also ignored by existing benchmarks. To address this gap, we introduce a novel multi-domain knowledge-enhanced benchmark with fine-grained annotations, named \textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six semantic topics and eight platforms. Each news item is enriched with multi-modal content, potential social context, semi-man
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20197;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.01334</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#22686;&#24378;NER&#25968;&#25454;&#38598;&#65306;&#36808;&#21521;&#33258;&#21160;&#21270;&#21644;&#31934;&#32454;&#21270;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20197;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#29992;&#20110;&#20026;NER&#27169;&#22411;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#38754;&#20020;&#30528;&#39640;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26088;&#22312;&#25913;&#21892;&#25163;&#21160;&#27880;&#37322;&#20013;&#22266;&#26377;&#30340;&#22122;&#38899;&#65292;&#22914;&#36951;&#28431;&#65292;&#20174;&#32780;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#20197;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37319;&#29992;&#26631;&#31614;&#28151;&#21512;&#31574;&#30053;&#65292;&#23427;&#35299;&#20915;&#20102;LLM-based&#27880;&#37322;&#20013;&#36935;&#21040;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#36825;&#31181;&#26041;&#27861;&#19968;&#30452;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#27880;&#37322;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;co
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01334v1 Announce Type: new  Abstract: In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under co
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2404.01332</link><description>&lt;p&gt;
&#31561;&#31561;&#65292;&#36825;&#37117;&#26159;&#20196;&#29260;&#22122;&#38899;&#65311;&#19968;&#30452;&#23601;&#26159;&#21527;&#65306;&#21033;&#29992; Shapley &#20540;&#35299;&#37322; LLM &#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01332
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#36807;&#31243;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#24066;&#22330;&#30740;&#31350;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#20998;&#26512;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#26174;&#33879;&#24046;&#24322;&#26263;&#31034;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#36807;&#31243;&#22312;&#36215;&#20316;&#29992;&#65292;&#20197;&#21450;LLMs&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20154;&#31867;&#20027;&#20307;&#30340;&#26367;&#20195;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#24182;&#37327;&#21270;&#27599;&#20010;&#25552;&#31034;&#32452;&#20214;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#30456;&#23545;&#36129;&#29486;&#12290;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;--&#19968;&#20010;&#31163;&#25955;&#36873;&#25321;&#23454;&#39564;&#21644;&#19968;&#20010;&#35748;&#30693;&#20559;&#35265;&#35843;&#26597;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#26041;&#27861;&#22914;&#20309;&#25581;&#31034;&#25105;&#20204;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#21363;LLM&#20915;&#31574;&#21463;&#21040;&#30340;&#24433;&#21709;&#20005;&#37325;&#20559;&#21521;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01332v1 Announce Type: cross  Abstract: The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications-a discrete choice experiment and an investigation of cognitive biases-we demonstrate how the Shapley value method can uncover what we term "token noise" effects, a phenomenon where LLM decisions are disproportionately influenced by 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#20102;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#12289;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#21644;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.01331</link><description>&lt;p&gt;
LLaVA-Gemma&#65306;&#21033;&#29992;&#32039;&#20945;&#30340;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01331
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#20102;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#12289;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#21644;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27969;&#34892;&#30340;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;MMFM&#65289;&#12290;&#29305;&#21035;&#20540;&#24471;&#20851;&#27880;&#30340;&#26159;2B&#21442;&#25968;&#30340;Gemma&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#26500;&#24314;&#21151;&#33021;&#24378;&#22823;&#30340;&#23567;&#35268;&#27169;MMFM&#30340;&#26426;&#20250;&#12290;&#19982;&#35813;&#39046;&#22495;&#20854;&#20182;&#35770;&#25991;&#30340;&#21457;&#29616;&#19968;&#33268;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21435;&#38500;&#19977;&#31181;&#35774;&#35745;&#29305;&#24615;&#30340;&#24433;&#21709;&#65306;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#65292;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#65292;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;LLaVA-Gemma&#30340;&#32467;&#26524;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20013;&#31561;&#24615;&#33021;&#65292;&#20294;&#26410;&#33021;&#36229;&#36234;&#24403;&#21069;&#30456;&#23545;&#22823;&#23567;&#30340;SOTA&#27169;&#22411;&#12290;&#24615;&#33021;&#30340;&#26356;&#35814;&#32454;&#20998;&#26512;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#25928;&#26524;&#65306;&#36339;&#36807;&#39044;&#35757;&#32451;&#24448;&#24448;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#26356;&#22823;&#30340;&#35270;&#35273;&#27169;&#22411;&#26377;&#26102;&#20250;&#25552;&#39640;&#24615;&#33021;&#65292;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#25928;&#26524;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#35757;&#32451;&#37197;&#26041;&#65292;&#20195;&#30721;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01331v1 Announce Type: cross  Abstract: We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code an
&lt;/p&gt;</description></item><item><title>EBER chatbot&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#32769;&#24180;&#20154;&#25968;&#23383;&#40511;&#27807;&#30340;&#23089;&#20048;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20854;"&#26234;&#33021;&#30005;&#21488;"&#27010;&#24565;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#21644;&#38656;&#27714;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2404.01327</link><description>&lt;p&gt;
&#26080;&#25277;&#35937;&#33021;&#21147;&#30340;&#32769;&#24180;&#20154;&#25968;&#23383;&#21253;&#23481;&#23089;&#20048;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01327
&lt;/p&gt;
&lt;p&gt;
EBER chatbot&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#32769;&#24180;&#20154;&#25968;&#23383;&#40511;&#27807;&#30340;&#23089;&#20048;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20854;"&#26234;&#33021;&#30005;&#21488;"&#27010;&#24565;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#21644;&#38656;&#27714;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20801;&#35768;&#21019;&#24314;&#23545;&#35805;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#24179;&#21488;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#22823;&#20247;&#24066;&#22330;&#39046;&#22495;&#20173;&#28982;&#36807;&#20110;&#19981;&#25104;&#29087;&#65292;&#26080;&#27861;&#25903;&#25345;&#20196;&#20154;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#20294;&#23545;&#35805;&#24335;&#30028;&#38754;&#24050;&#32463;&#22312;&#20020;&#26102;&#24212;&#29992;&#20013;&#25214;&#21040;&#20102;&#33258;&#24049;&#30340;&#20301;&#32622;&#65292;&#27604;&#22914;&#30005;&#35805;&#20013;&#24515;&#21644;&#22312;&#32447;&#36141;&#29289;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23558;&#20854;&#24212;&#29992;&#20110;&#32769;&#24180;&#20154;&#30340;&#31038;&#20250;&#21253;&#23481;&#65292;&#32769;&#24180;&#20154;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#25968;&#23383;&#40511;&#27807;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#32769;&#24180;&#20154;&#36890;&#36807;&#20256;&#32479;&#23186;&#20307;&#22914;&#30005;&#35270;&#21644;&#24191;&#25773;&#26469;&#20943;&#36731;&#23396;&#29420;&#24863;&#65292;&#36825;&#20123;&#23186;&#20307;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#38506;&#20276;&#24863;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26088;&#22312;&#20943;&#23569;&#32769;&#24180;&#20154;&#25968;&#23383;&#40511;&#27807;&#30340;EBER&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;EBER&#20250;&#22312;&#21518;&#21488;&#38405;&#35835;&#26032;&#38395;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#35843;&#25972;&#22238;&#22797;&#12290;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#8220;&#26234;&#33021;&#30005;&#21488;&#8221;&#27010;&#24565;&#65292;&#26681;&#25454;&#35813;&#27010;&#24565;&#65292;&#19981;&#26159;&#31616;&#21270;&#25968;&#23383;&#20449;&#24687;&#31995;&#32479;&#20197;&#20351;&#20854;&#26131;&#20110;&#35775;&#38382;&#65292;&#32780;&#26159;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#21644;&#38656;&#27714;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01327v1 Announce Type: cross  Abstract: Current language processing technologies allow the creation of conversational chatbot platforms. Even though artificial intelligence is still too immature to support satisfactory user experience in many mass market domains, conversational interfaces have found their way into ad hoc applications such as call centres and online shopping assistants. However, they have not been applied so far to social inclusion of elderly people, who are particularly vulnerable to the digital divide. Many of them relieve their loneliness with traditional media such as TV and radio, which are known to create a feeling of companionship. In this paper we present the EBER chatbot, designed to reduce the digital gap for the elderly. EBER reads news in the background and adapts its responses to the user's mood. Its novelty lies in the concept of "intelligent radio", according to which, instead of simplifying a digital information system to make it accessible to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#20855;&#26377;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;LLMs&#30340;&#21457;&#23637;&#21382;&#31243;&#12289;transformer-based &#26550;&#26500;&#30340;&#36827;&#23637;&#20197;&#21450;&#27880;&#24847;&#26426;&#21046;&#30340;&#20316;&#29992;</title><link>https://arxiv.org/abs/2404.01322</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#19982;&#35270;&#35273;&#27169;&#22411;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Multi-Modal Large Language and Vision Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#20855;&#26377;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;LLMs&#30340;&#21457;&#23637;&#21382;&#31243;&#12289;transformer-based &#26550;&#26500;&#30340;&#36827;&#23637;&#20197;&#21450;&#27880;&#24847;&#26426;&#21046;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#25104;&#20026;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#28966;&#28857;&#65292;&#20854;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#21463;&#21040;&#20102;&#25512;&#21160;&#12290;&#26356;&#36817;&#26399;&#65292;LLMs&#34987;&#25193;&#23637;&#20026;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#65292;&#23558;&#20854;&#33021;&#21147;&#25193;&#23637;&#21040;&#22788;&#29702;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#38500;&#20102;&#25991;&#26412;&#12290;&#36825;&#25171;&#24320;&#20102;&#35832;&#22914;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#12289;&#22270;&#20687;&#23383;&#24149;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#31561;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#23558;LLM&#19982;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#21518;&#26399;&#35843;&#25972;&#65292;&#25110;&#32773;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;MM-LLM&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20855;&#26377;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#24403;&#21069;LLMs&#20197;&#21450;&#26368;&#26032;&#30340;MM-LLMs&#30340;&#29616;&#29366;&#12290;&#23427;&#28085;&#30422;&#20102;LLMs&#30340;&#21382;&#21490;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#30001;transformer-based &#26550;&#26500;&#22914;OpenAI&#30340;GPT&#31995;&#21015;&#21644;Google&#30340;BERT&#25552;&#20379;&#30340;&#36827;&#23637;&#65292;&#20197;&#21450;&#27880;&#24847;&#26426;&#21046;&#22312;&#22686;&#24378;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01322v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have recently emerged as a focal point of research and application, driven by their unprecedented ability to understand and generate text with human-like quality. Even more recently, LLMs have been extended into multi-modal large language models (MM-LLMs) which extends their capabilities to deal with image, video and audio information, in addition to text. This opens up applications like text-to-video generation, image captioning, text-to-speech, and more and is achieved either by retro-fitting an LLM with multi-modal capabilities, or building a MM-LLM from scratch. This paper provides an extensive review of the current state of those LLMs with multi-modal capabilities as well as the very recent MM-LLMs. It covers the historical development of LLMs especially the advances enabled by transformer-based architectures like OpenAI's GPT series and Google's BERT, as well as the role of attention mechanisms in enh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#36890;&#36807;&#26234;&#33021;&#23398;&#20064;&#29575;&#20998;&#24067;&#21462;&#24471;&#20102;&#27604;&#24179;&#22374;&#23398;&#20064;&#29575;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;GLUE&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2404.01317</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#26234;&#33021;&#23398;&#20064;&#29575;&#20998;&#24067;&#20197;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#36890;&#36807;&#26234;&#33021;&#23398;&#20064;&#29575;&#20998;&#24067;&#21462;&#24471;&#20102;&#27604;&#24179;&#22374;&#23398;&#20064;&#29575;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;GLUE&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23545;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#31070;&#32463;&#32593;&#32476;&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#36136;&#30097;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23545;&#25972;&#20010;&#32593;&#32476;&#37319;&#29992;&#30456;&#21516;&#23398;&#20064;&#29575;&#30340;&#24494;&#35843;&#24120;&#35265;&#20570;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#65292;&#25214;&#21040;&#20102;&#27604;&#24179;&#22374;&#23398;&#20064;&#29575;&#26356;&#22909;&#30340;&#23398;&#20064;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#32467;&#21512;&#36825;&#20123;&#23398;&#20064;&#29575;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;GLUE&#25968;&#25454;&#38598;&#20013;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#39564;&#35777;&#20102;&#36825;&#20123;&#23398;&#20064;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01317v1 Announce Type: cross  Abstract: Pretraining language models on large text corpora is a common practice in natural language processing. Fine-tuning of these models is then performed to achieve the best results on a variety of tasks. In this paper, we investigate the problem of catastrophic forgetting in transformer neural networks and question the common practice of fine-tuning with a flat learning rate for the entire network in this context. We perform a hyperparameter optimization process to find learning rate distributions that are better than a flat learning rate. We combine the learning rate distributions thus found and show that they generalize to better performance with respect to the problem of catastrophic forgetting. We validate these learning rate distributions with a variety of NLP benchmarks from the GLUE dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#31232;&#30095;&#26041;&#27861;&#65292;&#25506;&#32034;&#31867;&#20284;&#20110;&#29983;&#29289;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181; NLP &#20219;&#21153;&#37117;&#34920;&#29616;&#20986;&#33394;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#31232;&#30095;&#24615;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.01306</link><description>&lt;p&gt;
NeuroPrune&#65306;&#19968;&#31181;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25299;&#25169;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#31232;&#30095;&#26041;&#27861;&#65292;&#25506;&#32034;&#31867;&#20284;&#20110;&#29983;&#29289;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181; NLP &#20219;&#21153;&#37117;&#34920;&#29616;&#20986;&#33394;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#31232;&#30095;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; Transformer &#30340;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#32780;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21464;&#24471;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#26114;&#36149;&#30340;&#35757;&#32451;&#20197;&#21450;&#25512;&#29702;&#20173;&#28982;&#26159;&#23427;&#20204;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#22312;&#27169;&#22411;&#26550;&#26500;&#30340;&#21508;&#20010;&#23618;&#27425;&#24378;&#21046;&#24341;&#20837;&#31232;&#30095;&#24615;&#24050;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#35299;&#20915;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#20294;&#31232;&#30095;&#24615;&#23545;&#32593;&#32476;&#25299;&#25169;&#30340;&#24433;&#21709;&#20173;&#23384;&#22312;&#26029;&#35010;&#12290;&#21463;&#22823;&#33041;&#31070;&#32463;&#32593;&#32476;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#25299;&#25169;&#30340;&#35270;&#35282;&#25506;&#32034;&#31232;&#30095;&#24615;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#29983;&#29289;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#26426;&#21046;&#65292;&#22914;&#20248;&#20808;&#38468;&#30528;&#21644;&#20887;&#20313;&#31361;&#35302;&#20462;&#21098;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#31232;&#30095;&#24615;&#26041;&#27861;&#22312;&#36328;&#36234;&#20998;&#31867;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65289;&#21644;&#29983;&#25104;&#65288;&#25688;&#35201;&#12289;&#26426;&#22120;&#32763;&#35793;&#65289;&#30340;&#21508;&#31181; NLP &#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#19988;&#39640;&#25928;&#65292;&#23613;&#31649; o
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01306v1 Announce Type: cross  Abstract: Transformer-based Language Models have become ubiquitous in Natural Language Processing (NLP) due to their impressive performance on various tasks. However, expensive training as well as inference remains a significant impediment to their widespread applicability. While enforcing sparsity at various levels of the model architecture has found promise in addressing scaling and efficiency issues, there remains a disconnect between how sparsity affects network topology. Inspired by brain neuronal networks, we explore sparsity approaches through the lens of network topology. Specifically, we exploit mechanisms seen in biological networks, such as preferential attachment and redundant synapse pruning, and show that principled, model-agnostic sparsity approaches are performant and efficient across diverse NLP tasks, spanning both classification (such as natural language inference) and generation (summarization, machine translation), despite o
&lt;/p&gt;</description></item><item><title>IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01266</link><description>&lt;p&gt;
IsoBench&#65306;&#22522;&#20110;&#21516;&#26500;&#34920;&#31034;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01266
&lt;/p&gt;
&lt;p&gt;
IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#20165;&#25991;&#26412;&#25110;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#21516;&#26102;&#25552;&#31034;&#26102;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#26159;&#21542;&#20250;&#26681;&#25454;&#36755;&#20837;&#26041;&#24335;&#32780;&#25913;&#21464;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\textbf{IsoBench}$&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#30340;&#38382;&#39064;: &#25968;&#23398;&#12289;&#31185;&#23398;&#12289;&#31639;&#27861;&#21644;&#28216;&#25103;&#12290;&#27599;&#20010;&#31034;&#20363;&#21576;&#29616;&#20102;&#22810;&#20010;&#36755;&#20837;&#30340;&#21516;&#26500;&#34920;&#31034;&#65292;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#25968;&#23398;&#23637;&#31034;&#12290;IsoBench&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#21453;&#39304;&#65292;&#20197;&#35786;&#26029;&#30001;&#34920;&#31034;&#24418;&#24335;&#36896;&#25104;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#30456;&#21516;&#38382;&#39064;&#19978;&#65292;&#27169;&#22411;&#19968;&#36143;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;&#26368;&#31361;&#20986;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;IsoBench&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;Claude-3 Opus&#22312;&#25552;&#20379;&#22270;&#20687;&#32780;&#19981;&#26159;&#25991;&#26412;&#26102;&#24615;&#33021;&#19979;&#38477;28.7&#20998;&#65307;&#21516;&#26679;&#65292;GPT-4 Turbo&#24615;&#33021;&#19979;&#38477;18.7&#20998;&#65292;Gemini Pro&#19979;&#38477;14.9&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01266v1 Announce Type: new  Abstract: Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32852;&#37030;&#22810;&#27169;&#24335;&#21407;&#22411;&#65288;Fed-MP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26032;&#29992;&#25143;&#25552;&#20986;&#30340;&#28041;&#21450;&#20219;&#24847;&#26410;&#30693;&#31867;&#21035;&#30340;&#26597;&#35810;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01232</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#27169;&#24335;&#21407;&#22411;&#30340;&#24320;&#25918;&#35789;&#27719;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Federated Learning with Multimodal Prototyping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32852;&#37030;&#22810;&#27169;&#24335;&#21407;&#22411;&#65288;Fed-MP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26032;&#29992;&#25143;&#25552;&#20986;&#30340;&#28041;&#21450;&#20219;&#24847;&#26410;&#30693;&#31867;&#21035;&#30340;&#26597;&#35810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#35757;&#32451;&#26631;&#31614;&#31354;&#38388;&#21644;&#27979;&#35797;&#26631;&#31614;&#31354;&#38388;&#26159;&#30456;&#21516;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#22826;&#29702;&#24819;&#21270;&#20102;&#12290;&#26032;&#29992;&#25143;&#21487;&#33021;&#25552;&#20986;&#28041;&#21450;&#26469;&#33258;&#26410;&#35265;&#31867;&#21035;&#25968;&#25454;&#30340;&#26597;&#35810;&#65292;&#36825;&#20123;&#24320;&#25918;&#35789;&#27719;&#26597;&#35810;&#23558;&#30452;&#25509;&#23548;&#33268;&#36825;&#31181;FL&#31995;&#32479;&#30340;&#32570;&#38519;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#20851;&#27880;FL&#20013;&#23578;&#26410;&#24320;&#21457;&#30340;&#24320;&#25918;&#35789;&#27719;&#25361;&#25112;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#20110;&#19968;&#20010;&#26032;&#29992;&#25143;&#65292;&#20840;&#23616;&#26381;&#21153;&#22120;&#24212;&#35813;&#29702;&#35299;&#22905;/&#20182;&#30340;&#26597;&#35810;&#28041;&#21450;&#20219;&#24847;&#26410;&#30693;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;FL&#29615;&#22659;&#20013;VLMs&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32852;&#37030;&#22810;&#27169;&#24335;&#21407;&#22411;&#65288;Fed-MP&#65289;&#12290;Fed-MP&#26681;&#25454;&#36731;&#37327;&#32423;&#23458;&#25143;&#31471;&#27531;&#24046;&#33258;&#36866;&#24212;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#26681;&#25454;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#27169;&#24335;&#21407;&#22411;&#26426;&#21046;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01232v1 Announce Type: new  Abstract: Existing federated learning (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained vision-language models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechan
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#22312;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00600</link><description>&lt;p&gt;
AI&#27861;&#24459;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65306;&#24403;&#20851;&#38190;&#38382;&#39064;&#21644;&#38544;&#31169;&#24433;&#21709;&#38656;&#35201;&#20154;&#31867;&#21644;&#36947;&#24503;&#30417;&#30563;&#26102;
&lt;/p&gt;
&lt;p&gt;
AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00600
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#22312;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26085;&#30410;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#20351;&#24471;&#26377;&#24517;&#35201;&#23545;&#23427;&#20204;&#22312;&#38544;&#31169;&#12289;&#20010;&#20154;&#25968;&#25454;&#20445;&#25252;&#20197;&#21450;&#36947;&#24503;&#23618;&#38754;&#65292;&#23588;&#20854;&#26159;&#23545;&#26368;&#33030;&#24369;&#21644;&#26368;&#24369;&#21183;&#32676;&#20307;&#21487;&#33021;&#20135;&#29983;&#30340;&#39118;&#38505;&#21644;&#24433;&#21709;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#23545;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00600v1 Announce Type: cross  Abstract: The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#38376;&#30340;&#24773;&#32490;&#32534;&#30721;&#22120;&#21644;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#25552;&#21319;&#24773;&#32490;&#29702;&#35299;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#31454;&#20105;&#24615;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00511</link><description>&lt;p&gt;
MIPS&#22312;SemEval-2024&#20219;&#21153;3&#20013;&#30340;&#34920;&#29616;&#65306;&#20351;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#32490;-&#21407;&#22240;&#23545;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#38376;&#30340;&#24773;&#32490;&#32534;&#30721;&#22120;&#21644;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#25552;&#21319;&#24773;&#32490;&#29702;&#35299;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#31454;&#20105;&#24615;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval 2024&#20219;&#21153;3&#30340;&#23376;&#20219;&#21153;2&#20013;&#20851;&#20110;&#23545;&#35805;&#20013;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#20998;&#26512;&#30340;&#33719;&#22870;&#25552;&#20132;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#25552;&#21462;&#65288;MER-MCE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#19987;&#38376;&#30340;&#24773;&#32490;&#32534;&#30721;&#22120;&#25972;&#21512;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#19977;&#31181;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#25552;&#21319;&#24773;&#32490;&#29702;&#35299;&#21644;&#22240;&#26524;&#25512;&#29702;&#65292;&#20351;&#33258;&#24049;&#33073;&#39062;&#32780;&#20986;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#20102;&#25105;&#20204;&#22810;&#27169;&#24577;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#25552;&#20132;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#21152;&#26435;F1&#20998;&#25968;&#20026;0.3435&#65292;&#22312;0.0339&#20043;&#21518;&#25490;&#21517;&#31532;&#19968;&#30340;&#22242;&#38431;&#65292;&#20165;&#22312;0.0025&#20043;&#21518;&#25490;&#21517;&#31532;&#20108;&#12290;&#39033;&#30446;&#38142;&#25509;&#65306;https://github.com/MIPS-COLT/MER-MCE.git
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00511v1 Announce Type: new  Abstract: This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: https://github.com/MIPS-COLT/MER-MCE.git
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.19837</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Concept-based Analysis of Neural Networks via Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#21270;&#20998;&#26512;&#38750;&#24120;&#21487;&#21462;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#34920;&#36798;&#35270;&#35273;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#20197;&#21450;&#32570;&#20047;&#39640;&#25928;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#36825;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#12289;&#35270;&#35273;&#35821;&#35328;&#12289;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#20854;&#21487;&#20197;&#25512;&#29702;&#35270;&#35273;&#27169;&#22411;&#30340;&#36879;&#38236;&#12290;VLMs&#24050;&#32463;&#22312;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#27492;&#38544;&#24335;&#22320;&#20102;&#35299;&#25551;&#36848;&#36825;&#20123;&#22270;&#20687;&#30340;&#39640;&#23618;&#27425;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{Con}_{\texttt{spec}}$&#30340;&#36923;&#36753;&#35268;&#33539;&#35821;&#35328;&#65292;&#26088;&#22312;&#20415;&#20110;&#25353;&#29031;&#36825;&#20123;&#27010;&#24565;&#32534;&#20889;&#35268;&#33539;&#12290;&#20026;&#20102;&#23450;&#20041;&#21644;&#24418;&#24335;&#21270;&#26816;&#26597;$\texttt{Con}_{\texttt{spec}}$&#35268;&#33539;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;VLM&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#21644;&#39640;&#25928;&#26816;&#26597;&#35270;&#35273;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23646;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
&lt;/p&gt;</description></item><item><title>KazParC&#26159;&#19968;&#20010;&#36328;&#21704;&#33832;&#20811;&#35821;&#12289;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;371,902&#20010;&#24179;&#34892;&#21477;&#23376;&#65292;&#36824;&#24320;&#21457;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;Tilmash&#12290;</title><link>https://arxiv.org/abs/2403.19399</link><description>&lt;p&gt;
KazParC&#65306;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#21704;&#33832;&#20811;&#24179;&#34892;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
KazParC: Kazakh Parallel Corpus for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19399
&lt;/p&gt;
&lt;p&gt;
KazParC&#26159;&#19968;&#20010;&#36328;&#21704;&#33832;&#20811;&#35821;&#12289;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;371,902&#20010;&#24179;&#34892;&#21477;&#23376;&#65292;&#36824;&#24320;&#21457;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;Tilmash&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;KazParC&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#36328;&#21704;&#33832;&#20811;&#35821;&#12289;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#32780;&#35774;&#35745;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#20316;&#20026;&#20854;&#31867;&#21035;&#20013;&#39318;&#20010;&#20063;&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#35821;&#26009;&#24211;&#65292;KazParC&#21253;&#21547;&#20102;371,902&#20010;&#24179;&#34892;&#21477;&#23376;&#30340;&#38598;&#21512;&#65292;&#28085;&#30422;&#19981;&#21516;&#39046;&#22495;&#65292;&#24182;&#22312;&#20154;&#31867;&#35793;&#32773;&#30340;&#21327;&#21161;&#19979;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24037;&#20316;&#36824;&#25193;&#23637;&#21040;&#20102;&#21457;&#23637;&#19968;&#20010;&#34987;&#26165;&#31216;&#20026;Tilmash&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;Tilmash&#30340;&#34920;&#29616;&#19982;&#24037;&#19994;&#24040;&#22836;&#65292;&#22914;Google&#32763;&#35793;&#21644;Yandex&#32763;&#35793;&#65292;&#22312;&#26631;&#20934;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;BLEU&#21644;chrF&#65289;&#30340;&#22522;&#30784;&#19978;&#30456;&#23218;&#32654;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#36229;&#36234;&#12290;KazParC&#21644;Tilmash&#22343;&#21487;&#36890;&#36807;&#25105;&#20204;&#30340;GitHub&#23384;&#20648;&#24211;&#20197;&#30693;&#35782;&#20849;&#20139;&#32626;&#21517;4.0&#22269;&#38469;&#35768;&#21487;&#65288;CC BY 4.0&#65289;&#20844;&#24320;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19399v1 Announce Type: new  Abstract: We introduce KazParC, a parallel corpus designed for machine translation across Kazakh, English, Russian, and Turkish. The first and largest publicly available corpus of its kind, KazParC contains a collection of 371,902 parallel sentences covering different domains and developed with the assistance of human translators. Our research efforts also extend to the development of a neural machine translation model nicknamed Tilmash. Remarkably, the performance of Tilmash is on par with, and in certain instances, surpasses that of industry giants, such as Google Translate and Yandex Translate, as measured by standard evaluation metrics, such as BLEU and chrF. Both KazParC and Tilmash are openly available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#20197;&#35299;&#20915;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#30693;&#35782;&#21644;&#26080;&#27861;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19275</link><description>&lt;p&gt;
&#30693;&#35782;&#36793;&#30028;&#19982;&#35282;&#33394;&#21160;&#24577;&#22609;&#36896;&#26356;&#22909;&#30340;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#20197;&#35299;&#20915;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#30693;&#35782;&#21644;&#26080;&#27861;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20010;&#24615;&#21270;&#21644;&#25311;&#20154;&#21270;&#20195;&#29702;&#22312;&#31038;&#20132;&#32593;&#32476;&#27169;&#25311;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20316;&#21697;&#20013;&#20173;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#19981;&#33021;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#23545;&#24403;&#21069;&#34892;&#20026;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#21644;&#25311;&#20154;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#12290;&#23545;&#20110;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#25105;&#20204;&#28155;&#21152;&#22806;&#37096;&#30693;&#35782;&#28304;&#24182;&#23558;&#20854;&#19982;&#20195;&#29702;&#30340;&#35282;&#33394;&#20449;&#24687;&#21305;&#37197;&#65292;&#20174;&#32780;&#36171;&#20104;&#20195;&#29702;&#20010;&#24615;&#21270;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#23545;&#20110;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#65292;&#25105;&#20204;&#20351;&#29992;&#24403;&#21069;&#34892;&#20026;&#20449;&#24687;&#20869;&#37096;&#26816;&#32034;&#20195;&#29702;&#30340;&#35282;&#33394;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#23545;&#24403;&#21069;&#34892;&#20026;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19275v1 Announce Type: cross  Abstract: Constructing personalized and anthropomorphic agents holds significant importance in the simulation of social networks. However, there are still two key problems in existing works: the agent possesses world knowledge that does not belong to its personas, and it cannot eliminate the interference of diverse persona information on current actions, which reduces the personalization and anthropomorphism of the agent. To solve the above problems, we construct the social media agent based on personalized knowledge and dynamic persona information. For personalized knowledge, we add external knowledge sources and match them with the persona information of agents, thereby giving the agent personalized world knowledge. For dynamic persona information, we use current action information to internally retrieve the persona information of the agent, thereby reducing the interference of diverse persona information on the current action. To make the age
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#20219;&#21153;&#28041;&#21450;14&#31181;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#26088;&#22312;&#32771;&#23519;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18933</link><description>&lt;p&gt;
SemEval&#20219;&#21153;1&#65306;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18933
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#20219;&#21153;&#28041;&#21450;14&#31181;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#26088;&#22312;&#32771;&#23519;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#20849;&#20139;&#20219;&#21153;&#12290;&#32780;&#20808;&#21069;&#30340;&#20849;&#20139;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21017;&#35843;&#26597;&#20102;&#36328;&#36234;14&#31181;&#35821;&#35328;&#65288;&#21253;&#25324;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#23612;&#35821;&#12289;&#22522;&#23612;&#20122;&#40065;&#23433;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#30340;&#26356;&#24191;&#27867;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#35821;&#35328;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#31995;&#65292;&#24182;&#20027;&#35201;&#22312;&#38750;&#27954;&#21644;&#20122;&#27954;&#22320;&#21306;&#20351;&#29992;&#65292;&#36825;&#20123;&#22320;&#21306;&#30340;&#29305;&#28857;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#30456;&#23545;&#26377;&#38480;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#37117;&#26159;&#19968;&#20010;&#19982;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#21477;&#23545;&#65292;&#35813;&#20998;&#25968;&#34920;&#31034;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#31243;&#24230;&#12290;&#21442;&#19982;&#31995;&#32479;&#34987;&#35201;&#27714;&#22312;&#19977;&#20010;&#20027;&#35201;&#36712;&#36947;&#20013;&#30340;14&#31181;&#35821;&#35328;&#20013;&#25353;&#23427;&#20204;&#22312;&#24847;&#20041;&#19978;&#30340;&#25509;&#36817;&#31243;&#24230;&#65288;&#21363;&#23427;&#20204;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#31243;&#24230;&#65289;&#23545;&#21477;&#23545;&#36827;&#34892;&#25490;&#21517;&#65306;(a) &#30417;&#30563;&#65292;(b) &#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18933v1 Announce Type: new  Abstract: We present the first shared task on Semantic Textual Relatedness (STR). While earlier shared tasks primarily focused on semantic similarity, we instead investigate the broader phenomenon of semantic relatedness across 14 languages: Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by the relatively limited availability of NLP resources. Each instance in the datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. Participating systems were asked to rank sentence pairs by their closeness in meaning (i.e., their degree of semantic relatedness) in the 14 languages in three main tracks: (a) supervised, (b) unsupervi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18802</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form factuality in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#24320;&#25918;&#24615;&#20027;&#39064;&#30340;&#20107;&#23454;&#24615;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#29983;&#25104;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#23545;&#27169;&#22411;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;LongFact&#30340;&#25552;&#31034;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#22218;&#25324;38&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Search-Augmented Factuality Evaluator&#65288;SAFE&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#22120;&#12290;SAFE&#21033;&#29992;LLM&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#29420;&#30340;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#20197;&#21450;&#30830;&#23450;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#24471;&#21040;&#25628;&#32034;&#32467;&#26524;&#25903;&#25345;&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#35780;&#20272;&#27599;&#20010;&#20107;&#23454;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#23558;F1&#20998;&#25968;&#25193;&#23637;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#22238;&#24212;&#20013;&#25903;&#25345;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#65288;&#31934;&#24230;&#65289;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#35843;&#30740;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#29983;&#21644;&#25945;&#24072;&#36741;&#21161;&#12289;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.18105</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#65306;&#35843;&#30740;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Education: A Survey and Outlook
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18105
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#35843;&#30740;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#29983;&#21644;&#25945;&#24072;&#36741;&#21161;&#12289;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#25945;&#32946;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#31687;&#35843;&#30740;&#35770;&#25991;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#65292;&#28085;&#30422;&#20102;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#36741;&#21161;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#27599;&#20010;&#35270;&#35282;&#20013;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#25972;&#29702;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#25945;&#32946;&#20013;&#37096;&#32626;LLMs&#25152;&#28041;&#21450;&#30340;&#39118;&#38505;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#65292;&#31361;&#20986;&#20102;&#28508;&#22312;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#26088;&#22312;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#20840;&#38754;&#30340;&#25216;&#26415;&#22270;&#26223;&#65292;&#20197;&#21033;&#29992;LLMs&#30340;&#21147;&#37327;&#65292;&#24443;&#24213;&#25913;&#38761;&#25945;&#32946;&#23454;&#36341;&#65292;&#24182;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#23398;&#20064;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18105v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20197;&#20943;&#23569;NLP&#27169;&#22411;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.17860</link><description>&lt;p&gt;
&#25506;&#31350;LLMs&#20316;&#20026;&#30446;&#26631;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#26469;&#28304;&#65292;&#20197;&#20943;&#23569;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17860
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20197;&#20943;&#23569;NLP&#27169;&#22411;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#32463;&#36807;&#20248;&#21270;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#26102;&#65292;&#24120;&#24120;&#23384;&#22312;&#39640;&#32622;&#20449;&#24230;&#38169;&#35823;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20316;&#20026;&#35299;&#20915;NLP&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#20135;&#29983;&#39640;&#32622;&#20449;&#24230;&#38169;&#35823;&#39044;&#27979;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#30001;LLMs&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#36890;&#36807;&#30456;&#21516;&#36807;&#31243;&#33719;&#24471;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#38169;&#35823;&#65292;&#20154;&#31867;&#25110;LLMs&#25552;&#20379;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20197;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#28982;&#21518;&#29992;&#20110;&#25193;&#23637;&#35757;&#32451;&#38598;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20943;&#23569;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17860v1 Announce Type: new  Abstract: Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#27604;&#36739;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#30340;&#33041;&#27963;&#21160;&#26144;&#23556;&#26469;&#25351;&#23548;&#25991;&#26412;&#37325;&#24314;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#38388;&#25509;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17516</link><description>&lt;p&gt;
MapGuide: &#20174;&#33041;&#27963;&#21160;&#20013;&#37325;&#24314;&#36830;&#32493;&#35821;&#35328;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#27604;&#36739;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#30340;&#33041;&#27963;&#21160;&#26144;&#23556;&#26469;&#25351;&#23548;&#25991;&#26412;&#37325;&#24314;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#38388;&#25509;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#27963;&#21160;&#20013;&#35299;&#30721;&#36830;&#32493;&#35821;&#35328;&#26159;&#19968;&#39033;&#33392;&#24040;&#20294;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36825;&#23545;&#20110;&#24110;&#21161;&#35821;&#35328;&#27531;&#38556;&#20154;&#22763;&#36890;&#36807;&#33041;&#20449;&#21495;&#36827;&#34892;&#27807;&#36890;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23558;&#20174;&#33041;&#27963;&#21160;&#26144;&#23556;&#30340;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#21521;&#23548;&#25991;&#26412;&#37325;&#24314;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;METEOR&#20998;&#25968;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;77%&#21644;54%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17516v1 Announce Type: cross  Abstract: Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to text. The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17407</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26412;&#21040;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#30340;&#20934;&#30830;&#36716;&#24405;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#38899;&#38901;&#23398;&#21644;&#35821;&#22659;&#30456;&#20851;&#30340;&#38899;&#21464;&#12290;&#23545;&#20110;&#21306;&#22495;&#23391;&#21152;&#25289;&#26041;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26041;&#35328;&#30340;&#26631;&#20934;&#25340;&#20889;&#32422;&#23450;&#12289;&#24403;&#22320;&#21644;&#22806;&#35821;&#22312;&#36825;&#20123;&#22320;&#21306;&#20013;&#27969;&#34892;&#30340;&#35789;&#27719;&#20197;&#21450;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#38899;&#38901;&#22810;&#26679;&#24615;&#65292;&#36825;&#19968;&#25361;&#25112;&#29978;&#33267;&#26356;&#20026;&#20005;&#23803;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#35206;&#30422;&#23391;&#21152;&#25289;&#22269;&#20845;&#20010;&#22320;&#21306;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#8220;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#8221;&#65288;DGT&#65289;&#25216;&#26415;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#29983;&#25104;IPA&#36716;&#24405;&#20043;&#21069;&#21521;&#27169;&#22411;&#25552;&#20379;&#26377;&#20851;&#36755;&#20837;&#25991;&#26412;&#30340;&#21306;&#22495;&#26041;&#35328;&#25110;&#8220;&#22320;&#21306;&#8221;&#30340;&#26126;&#30830;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22312;&#36755;&#20837;&#24207;&#21015;&#21069;&#28155;&#21152;&#19968;&#20010;&#22320;&#21306;&#26631;&#35760;&#26469;&#23454;&#29616;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCoder&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#24341;&#23548;&#65292;&#36845;&#20195;&#22320;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;</title><link>https://arxiv.org/abs/2403.16792</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#36845;&#20195;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCoder&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#24341;&#23548;&#65292;&#36845;&#20195;&#22320;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#24212;&#29992;&#21040;&#29616;&#23454;&#39033;&#30446;&#20013;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#21487;&#33021;&#23384;&#22312;API&#20351;&#29992;&#12289;&#31867;&#12289;&#25968;&#25454;&#32467;&#26500;&#38169;&#35823;&#25110;&#32570;&#23569;&#39033;&#30446;&#29305;&#23450;&#20449;&#24687;&#12290;&#37492;&#20110;&#22823;&#37096;&#20998;&#39033;&#30446;&#29305;&#23450;&#19978;&#19979;&#25991;&#26080;&#27861;&#36866;&#24212;LLMs&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#24517;&#39035;&#25214;&#21040;&#35753;&#27169;&#22411;&#33021;&#22815;&#25506;&#32034;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProCoder&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#35793;&#22120;&#21453;&#39304;&#24341;&#23548;&#65292;&#36845;&#20195;&#22320;&#25913;&#36827;&#39033;&#30446;&#32423;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ProCoder&#39318;&#20808;&#21033;&#29992;&#32534;&#35793;&#22120;&#25216;&#26415;&#35782;&#21035;&#29983;&#25104;&#30340;&#20195;&#30721;&#19982;&#39033;&#30446;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#20043;&#22788;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20174;&#20195;&#30721;&#24211;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#36845;&#20195;&#22320;&#23545;&#40784;&#21644;&#20462;&#22797;&#35782;&#21035;&#20986;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;ProCoder&#19982;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16792v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable progress in automated code generation. Yet, incorporating LLM-based code generation into real-life software projects poses challenges, as the generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. To this end, this paper puts forward a novel approach, termed ProCoder, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback. In particular, ProCoder first leverages compiler techniques to identify a mismatch between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate ProCoder with two representative L
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#26681;&#25454;&#20196;&#29260;&#30340;&#29109;&#35843;&#25972;&#20854;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13485</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#25991;&#26412;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Entropy-based Text Watermarking Detection Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13485
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#26681;&#25454;&#20196;&#29260;&#30340;&#29109;&#35843;&#25972;&#20854;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#33021;&#22815;&#23884;&#20837;&#38544;&#34255;&#29305;&#24449;&#21040;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20197;&#20415;&#21518;&#32493;&#26816;&#27979;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;LLMs&#34987;&#35823;&#29992;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#39640;&#29109;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20302;&#29109;&#24773;&#20917;&#19979;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#27700;&#21360;&#26816;&#27979;&#36807;&#31243;&#20013;&#24212;&#20840;&#38754;&#32771;&#34385;&#20196;&#29260;&#29109;&#30340;&#24433;&#21709;&#65292;&#21363;&#24212;&#26681;&#25454;&#20854;&#29109;&#35843;&#25972;&#27599;&#20010;&#20196;&#29260;&#30340;&#37325;&#37327;&#65292;&#32780;&#19981;&#26159;&#20687;&#20197;&#21069;&#30340;&#26041;&#27861;&#20013;&#23558;&#25152;&#26377;&#20196;&#29260;&#30340;&#37325;&#37327;&#35774;&#32622;&#20026;&#30456;&#21516;&#20540;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#65288;EWD&#65289;&#65292;&#22312;&#27700;&#21360;&#26816;&#27979;&#36807;&#31243;&#20013;&#36171;&#20104;&#39640;&#29109;&#20196;&#29260;&#26356;&#39640;&#30340;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#36807;&#31243;&#26080;&#38656;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13485v1 Announce Type: new  Abstract: Currently, text watermarking algorithms for large language models (LLMs) can embed hidden features to texts generated by LLMs to facilitate subsequent detection, thus alleviating the problem of misuse of LLMs. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we proposed that the influence of token entropy should be fully considered in the watermark detection process, that is, the weight of each token should be adjusted according to its entropy during watermark detection, rather than setting the weight of all tokens to the same value as in previous methods. Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives higher-entropy tokens higher weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13002</link><description>&lt;p&gt;
AutoTRIZ&#65306;&#21033;&#29992;TRIZ&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#21019;&#24847;
&lt;/p&gt;
&lt;p&gt;
AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#21019;&#26032;&#32773;&#22312;&#24320;&#21457;&#24605;&#32500;&#26041;&#27861;&#26041;&#38754;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#27604;&#22914;&#24418;&#24577;&#20998;&#26512;&#21644;&#31867;&#27604;&#35774;&#35745;&#65292;&#20197;&#36741;&#21161;&#24037;&#31243;&#35774;&#35745;&#21019;&#24847;&#65292;&#35299;&#20915;&#38382;&#39064;&#21644;&#25512;&#21160;&#21019;&#26032;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;TRIZ&#20316;&#20026;&#26368;&#33879;&#21517;&#30340;&#26041;&#27861;&#33073;&#39062;&#32780;&#20986;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31995;&#32479;&#21270;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;TRIZ&#36164;&#28304;&#21644;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23545;&#29992;&#25143;&#30693;&#35782;&#12289;&#32463;&#39564;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20381;&#36182;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#24191;&#27867;&#30693;&#35782;&#21644;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;AutoTRIZ&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30683;&#30462;&#26816;&#27979;&#21644;&#27604;&#36739;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#23454;&#39564;&#26469;&#35777;&#26126;&#24182;&#35780;&#20272;AutoTRIZ&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13002v1 Announce Type: cross  Abstract: Researchers and innovators have made enormous efforts in developing ideation methods, such as morphological analysis and design-by-analogy, to aid engineering design ideation for problem solving and innovation. Among these, TRIZ stands out as the most well-known approach, widely applied for systematic innovation. However, the complexity of TRIZ resources and concepts, coupled with its reliance on users' knowledge, experience, and reasoning capabilities, limits its practicability. This paper proposes AutoTRIZ, an artificial ideation tool that leverages large language models (LLMs) to automate and enhance the TRIZ methodology. By leveraging the broad knowledge and advanced reasoning capabilities of LLMs, AutoTRIZ offers a novel approach to design automation and interpretable ideation with artificial intelligence. We demonstrate and evaluate the effectiveness of AutoTRIZ through consistency experiments in contradiction detection and compa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.11904</link><description>&lt;p&gt;
CICLe: &#36866;&#24212;&#19978;&#19979;&#25991;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#27745;&#26579;&#25110;&#25530;&#20551;&#39135;&#21697;&#23545;&#20154;&#31867;&#20581;&#24247;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#22312;&#32473;&#23450;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#32593;&#32476;&#25991;&#26412;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#33258;&#21160;&#26816;&#27979;&#36825;&#31181;&#39118;&#38505;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;7,546&#20010;&#25551;&#36848;&#20844;&#20849;&#39135;&#21697;&#21484;&#22238;&#20844;&#21578;&#30340;&#30701;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25991;&#26412;&#37117;&#32463;&#36807;&#25163;&#21160;&#26631;&#35760;&#65292;&#20998;&#20026;&#20004;&#20010;&#31890;&#24230;&#32423;&#21035;&#65288;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#21484;&#22238;&#23545;&#24212;&#30340;&#39135;&#21697;&#20135;&#21697;&#21644;&#21361;&#23475;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#24182;&#23545;&#26420;&#32032;&#12289;&#20256;&#32479;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;tf-idf&#34920;&#31034;&#30340;&#36923;&#36753;&#22238;&#24402;&#22312;&#25903;&#25345;&#36739;&#20302;&#30340;&#31867;&#21035;&#19978;&#20248;&#20110;RoBERTa&#21644;XLM-R&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#19982;&#26222;&#36890;&#25552;&#31034;&#30456;&#27604;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#32763;&#35793;&#25216;&#26415;&#65292;&#27604;&#36739;&#20116;&#31181;&#20013;&#38388;&#35821;&#35328;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#25351;&#26631;&#35780;&#20272;&#32763;&#35793;&#20013;&#38544;&#21547;&#30340;&#24615;&#21035;&#20559;&#35265;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.11896</link><description>&lt;p&gt;
&#35843;&#26597;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26631;&#35760;&#21644;&#39537;&#21160;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Investigating Markers and Drivers of Gender Bias in Machine Translations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11896
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#32763;&#35793;&#25216;&#26415;&#65292;&#27604;&#36739;&#20116;&#31181;&#20013;&#38388;&#35821;&#35328;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#25351;&#26631;&#35780;&#20272;&#32763;&#35793;&#20013;&#38544;&#21547;&#30340;&#24615;&#21035;&#20559;&#35265;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38544;&#21547;&#24615;&#21035;&#20559;&#35265;&#26159;&#19968;&#20010;&#26377;&#20805;&#20998;&#25991;&#29486;&#25903;&#25345;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#24341;&#20837;&#24615;&#21035;&#21487;&#33021;&#20250;&#24310;&#32493;&#29616;&#23454;&#19990;&#30028;&#30340;&#20559;&#35265;&#12290;&#26377;&#20123;LLMs&#20351;&#29992;&#21551;&#21457;&#24335;&#25110;&#21518;&#22788;&#29702;&#26469;&#25513;&#30422;&#36825;&#31181;&#20559;&#35265;&#65292;&#20351;&#35843;&#26597;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#30740;&#31350;LLMs&#20013;&#30340;&#20559;&#35265;&#65292;&#20351;&#29992;DeepL&#32763;&#35793;API&#26469;&#35843;&#26597;&#37325;&#22797;&#32763;&#35793;&#19968;&#32452;56&#20010;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#26102;&#25152;&#23637;&#29616;&#30340;&#20559;&#35265;&#12290;&#27599;&#20010;&#38472;&#36848;&#20197; 'she' &#24320;&#22987;&#65292;&#24182;&#39318;&#20808;&#32763;&#35793;&#20026;&#19968;&#20010; '&#26080;&#24615;&#21035;' &#20013;&#38388;&#35821;&#35328;&#65292;&#28982;&#21518;&#20877;&#32763;&#35793;&#22238;&#33521;&#35821;&#65307;&#28982;&#21518;&#25105;&#20204;&#26816;&#26597;&#20102;&#21453;&#21521;&#32763;&#35793;&#25991;&#26412;&#20013;&#30340;&#20195;&#35789;&#36873;&#25321;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65306;&#65288;1&#65289;&#27604;&#36739;&#20102;&#20116;&#31181;&#20013;&#38388;&#35821;&#35328;&#65288;&#33452;&#20848;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#29233;&#27801;&#23612;&#20122;&#35821;&#12289;&#22303;&#32819;&#20854;&#35821;&#21644;&#21256;&#29273;&#21033;&#35821;&#65289;&#30340;&#32467;&#26524;&#65307;&#65288;2&#65289;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#37325;&#22797;&#32763;&#35793;&#20013;&#25152;&#26263;&#31034;&#30340;&#24615;&#21035;&#21464;&#21270;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11896v1 Announce Type: new  Abstract: Implicit gender bias in Large Language Models (LLMs) is a well-documented problem, and implications of gender introduced into automatic translations can perpetuate real-world biases. However, some LLMs use heuristics or post-processing to mask such bias, making investigation difficult. Here, we examine bias in LLMss via back-translation, using the DeepL translation API to investigate the bias evinced when repeatedly translating a set of 56 Software Engineering tasks used in a previous study. Each statement starts with 'she', and is translated first into a 'genderless' intermediate language then back into English; we then examine pronoun- choice in the back-translated texts. We expand prior research in the following ways: (1) by comparing results across five intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and Hungarian; (2) by proposing a novel metric for assessing the variation in gender implied in the repeated tran
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24182;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.10576</link><description>&lt;p&gt;
&#24573;&#30053;&#25105;&#20294;&#19981;&#35201;&#26367;&#20195;&#25105;&#65306;&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10576
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24182;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32593;&#32476;&#23433;&#20840;&#20449;&#24687;&#36890;&#24120;&#25216;&#26415;&#22797;&#26434;&#19988;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20256;&#36882;&#65292;&#20351;&#24471;&#33258;&#21160;&#21270;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#38024;&#23545;&#28041;&#21450;&#39640;&#24230;&#19987;&#19994;&#30693;&#35782;&#30340;&#25991;&#26412;&#39046;&#22495;&#65292;&#22522;&#20110;&#39046;&#22495;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#19968;&#30452;&#26159;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#23433;&#20840;&#25991;&#26412;&#36890;&#24120;&#21253;&#21547;&#38750;&#35821;&#35328;&#20803;&#32032;&#65288;&#22914;URL&#21644;&#21704;&#24076;&#20540;&#65289;&#65292;&#36825;&#21487;&#33021;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#19981;&#36866;&#29992;&#12290;&#20808;&#21069;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#24037;&#20316;&#20013;&#65292;&#24050;&#23558;&#27492;&#31867;&#25991;&#26412;&#35270;&#20026;&#22122;&#38899;&#36827;&#34892;&#31227;&#38500;&#25110;&#36807;&#28388;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#21644;&#25506;&#27979;&#20219;&#21153;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#65288;&#36873;&#25321;&#24615;MLM&#21644;&#32852;&#21512;&#35757;&#32451;NLE&#26631;&#35760;&#20998;&#31867;&#65289;&#20248;&#20110;&#24120;&#29992;&#30340;&#26367;&#25442;&#38750;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10576v1 Announce Type: cross  Abstract: Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We propose different pretraining methodologies and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy (selective MLM and jointly training NLE token classification) outperforms the commonly taken approach of replacing non-l
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.09516</link><description>&lt;p&gt;
&#21033;&#29992;&#20856;&#22411;&#34920;&#31034;&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#32780;&#19981;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09516
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#36890;&#24120;&#38656;&#35201;&#35782;&#21035;&#19982;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30456;&#20851;&#32852;&#30340;&#31038;&#20250;&#32676;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DAFair&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#38382;&#39064;&#12290;&#19982;&#20381;&#36182;&#26174;&#24335;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#27492;&#31867;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20154;&#21475;&#32479;&#35745;&#20856;&#22411;&#25991;&#26412;&#65292;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#20943;&#36731;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#21644;&#20004;&#20010;&#27169;&#22411;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#19981;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#27880;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20248;&#20110;&#24120;&#35265;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;</title><link>https://arxiv.org/abs/2403.09057</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Continued Pretrained LLM Approach for Automatic Medical Note Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#27491;&#22312;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#26368;&#24378;&#22823;&#30340;LLM&#23545;&#20110;&#22823;&#22810;&#25968;&#39046;&#22495;&#29305;&#23450;&#22330;&#26223;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36830;&#32493;&#35757;&#32451;&#30340;130&#20159;&#21442;&#25968; Llama2-basd LLM&#65292;&#19987;&#20026;&#21307;&#30103;&#23545;&#35805;&#32780;&#35774;&#35745;&#65292;&#24182;&#22312;&#33258;&#21160;&#35760;&#24405;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;PubMedQA&#20013;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;76.6&#65285;&#65292;&#22312;&#24635;&#32467;&#21307;&#30103;&#23545;&#35805;&#20026;SOAP&#31508;&#35760;&#26041;&#38754;&#19982;GPT-4&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#26041;&#38754;&#36229;&#36807;&#20102;GPT-4&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09057v1 Announce Type: cross  Abstract: LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#24320;&#21457;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#35299;&#20915;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04798</link><description>&lt;p&gt;
JMI&#22312;SemEval 2024&#20219;&#21153;3&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;GPT&#21644;instruction-tuned Llama&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#30340;&#20004;&#27493;&#27861;
&lt;/p&gt;
&lt;p&gt;
JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#24320;&#21457;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#35299;&#20915;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#65306;&#8220;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31454;&#36187;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#26377;&#25928;&#25429;&#25417;&#20154;&#31867;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#38656;&#35201;&#25972;&#21512;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#31561;&#22810;&#31181;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22810;&#26679;&#24615;&#27169;&#24577;&#30340;&#22797;&#26434;&#24615;&#32473;&#24320;&#21457;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#23454;&#29616;&#20013;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#22312;&#26041;&#27861;1&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30340;Llama 2&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#21644;&#21407;&#22240;&#39044;&#27979;&#30340;instruction-tuning&#12290;&#22312;&#26041;&#27861;2&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4V&#36827;&#34892;&#20250;&#35805;&#32423;&#35270;&#39057;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;GPT 3.5&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#27880;&#37322;&#23545;&#35805;&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33719;&#24471;&#20102;&#31532;4&#21517;&#65292;&#31995;&#32479;&#28040;&#34701;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04798v1 Announce Type: new  Abstract: This paper presents our system development for SemEval-2024 Task 3: "The Competition of Multimodal Emotion Cause Analysis in Conversations". Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient multimodal emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experime
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2403.04483</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#21151;&#33021;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;GraphInstruct
&lt;/p&gt;
&lt;p&gt;
GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36890;&#29992;&#33021;&#21147;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22270;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#29702;&#35299;&#22270;&#25968;&#25454;&#23545;&#20110;&#25512;&#36827;&#36890;&#29992;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#21644;&#22686;&#24378;LLMs&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#20840;&#38754;&#21253;&#25324;21&#20010;&#32463;&#20856;&#22270;&#25512;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#22810;&#26679;&#30340;&#22270;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#35814;&#32454;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;&#22522;&#20110;GraphInstruct&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#39640;&#25928;&#30340;&#25351;&#23548;&#35843;&#25972;&#26500;&#24314;&#20102;GraphLM&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;LLM&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#25513;&#30721;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GraphLM+&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#22686;&#24378;LLMs&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20808;&#39537;&#24615;&#21162;&#21147;&#20043;&#19968;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04483v1 Announce Type: new  Abstract: Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demons
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.02090</link><description>&lt;p&gt;
&#24314;&#27169;&#22810;&#27169;&#24577;&#31038;&#20132;&#20114;&#21160;&#65306;&#20855;&#26377;&#23494;&#38598;&#23545;&#40784;&#34920;&#31034;&#30340;&#26032;&#25361;&#25112;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28041;&#21450;&#35328;&#35821;&#21644;&#38750;&#35328;&#35821;&#32447;&#32034;&#30340;&#31038;&#20132;&#20114;&#21160;&#23545;&#26377;&#25928;&#35299;&#37322;&#31038;&#20132;&#24773;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#22810;&#27169;&#24577;&#31038;&#20132;&#32447;&#32034;&#30340;&#20808;&#21069;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20154;&#34892;&#20026;&#19978;&#65292;&#25110;&#20381;&#36182;&#20110;&#19982;&#22810;&#26041;&#29615;&#22659;&#20013;&#30340;&#35805;&#35821;&#23494;&#20999;&#23545;&#40784;&#30340;&#25972;&#20307;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#20204;&#22312;&#24314;&#27169;&#22810;&#26041;&#20114;&#21160;&#30340;&#22797;&#26434;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20197;&#24314;&#27169;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65306;&#35805;&#35821;&#30446;&#26631;&#35782;&#21035;&#12289;&#20195;&#35789;&#25351;&#20195;&#28040;&#35299;&#21644;&#25552;&#21450;&#29609;&#23478;&#39044;&#27979;&#12290;&#25105;&#20204;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#20013;&#30340;&#36825;&#20123;&#26032;&#25361;&#25112;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#19982;&#20854;&#23545;&#24212;&#30340;&#35805;&#35821;&#21516;&#27493;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#65292;&#36825;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02090v1 Announce Type: cross  Abstract: Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18700</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning to Compress Prompt in Natural Language Formats
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#22788;&#29702;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#38271;&#19978;&#19979;&#25991;&#12289;&#25512;&#29702;&#36895;&#24230;&#24930;&#20197;&#21450;&#35745;&#31639;&#32467;&#26524;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;&#37096;&#32626;&#20855;&#26377;&#31934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#19978;&#19979;&#25991;&#30340;LLMs&#26377;&#21161;&#20110;&#29992;&#25143;&#26356;&#26377;&#25928;&#21644;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#20316;&#21697;&#20381;&#36182;&#23558;&#38271;&#25552;&#31034;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#36719;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36719;&#25552;&#31034;&#21387;&#32553;&#22312;&#19981;&#21516;LLM&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;API&#30340;LLMs&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20197;LLM&#21487;&#36716;&#31227;&#24615;&#30340;&#24418;&#24335;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#12290;&#36825;&#24102;&#26469;&#20004;&#20010;&#25361;&#25112;&#65306;(i) &#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#25552;&#31034;&#19981;&#20860;&#23481;&#21453;&#21521;&#20256;&#25773;&#65292;(ii) NL&#25552;&#31034;&#22312;&#26045;&#21152;&#38271;&#24230;&#32422;&#26463;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18700v1 Announce Type: cross  Abstract: Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framewor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#20010;&#22270;&#29255;&#20013;&#65292;&#20197;&#25552;&#21319;GPT-4V&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#26469;&#21462;&#24471;&#22810;&#20010;&#20248;&#28857;</title><link>https://arxiv.org/abs/2402.17971</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#25630;&#23450;&#65306;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#22270;&#29255;&#20869;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
All in a Single Image: Large Multimodal Models are In-Image Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#20010;&#22270;&#29255;&#20013;&#65292;&#20197;&#25552;&#21319;GPT-4V&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#26469;&#21462;&#24471;&#22810;&#20010;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#24352;&#22270;&#29255;&#20013;&#65292;&#20197;&#22686;&#24378;GPT-4V&#30340;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#20381;&#36182;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25110;&#23558;&#35270;&#35273;&#36755;&#20837;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;I$^2$L&#23558;&#25152;&#26377;&#20449;&#24687;&#25972;&#21512;&#21040;&#19968;&#24352;&#22270;&#29255;&#20013;&#65292;&#20027;&#35201;&#21033;&#29992;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#36991;&#20813;&#20102;&#23545;&#22797;&#26434;&#22270;&#20687;&#30340;&#19981;&#20934;&#30830;&#25991;&#26412;&#25551;&#36848;&#65292;&#25552;&#20379;&#20102;&#22312;&#23450;&#20301;&#28436;&#31034;&#31034;&#20363;&#26102;&#30340;&#28789;&#27963;&#24615;&#65292;&#20943;&#23569;&#20102;&#36755;&#20837;&#36127;&#25285;&#65292;&#24182;&#36890;&#36807;&#28040;&#38500;&#23545;&#22810;&#20010;&#22270;&#29255;&#21644;&#20887;&#38271;&#25991;&#26412;&#30340;&#38656;&#27714;&#26469;&#36991;&#20813;&#36229;&#36807;&#36755;&#20837;&#38480;&#21046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32467;&#21512;&#19981;&#21516;ICL&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#32473;&#23450;&#20219;&#21153;&#20013;&#25968;&#25454;&#31034;&#20363;&#30340;&#36866;&#24403;ICL&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;MathVi&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17971v1 Announce Type: cross  Abstract: This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVi
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#36136;&#37327;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20013;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#30340;&#39318;&#35201;&#20219;&#21153;&#65292;&#20294;&#31649;&#29702;&#32773;&#38388;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#32570;&#20047;&#20849;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.16611</link><description>&lt;p&gt;
&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#32972;&#21518;&#30340;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Understanding the Dataset Practitioners Behind Large Language Model Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16611
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36136;&#37327;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20013;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#30340;&#39318;&#35201;&#20219;&#21153;&#65292;&#20294;&#31649;&#29702;&#32773;&#38388;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#32570;&#20047;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21464;&#24471;&#36234;&#26469;&#36234;&#20808;&#36827;&#21644;&#26377;&#24433;&#21709;&#21147;&#65292;&#23457;&#35270;&#23427;&#20204;&#20381;&#36182;&#21644;&#20135;&#29983;&#30340;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#30340;&#24037;&#20316;&#20869;&#23481;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#35895;&#27468;&#36129;&#29486;LLM&#24320;&#21457;&#22242;&#38431;&#36131;&#20219;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#23450;&#20041;&#20102;&#8220;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#8221;&#30340;&#35282;&#33394;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#31649;&#29702;&#32773;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65288;N=10&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#36136;&#37327;&#26159;&#39318;&#35201;&#20219;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#65292;&#31649;&#29702;&#32773;&#35201;&#20040;&#20973;&#30452;&#35273;&#65292;&#35201;&#20040;&#32534;&#20889;&#33258;&#23450;&#20041;&#35780;&#20272;&#36923;&#36753;&#12290;&#31649;&#29702;&#32773;&#20043;&#38388;&#23545;&#25968;&#25454;&#36136;&#37327;&#30340;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#32570;&#20047;&#20849;&#35782;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#28508;&#22312;&#21407;&#22240;&#21644;&#23454;&#29616;&#19968;&#33268;&#24615;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16611v1 Announce Type: new  Abstract: As large language models (LLMs) become more advanced and impactful, it is increasingly important to scrutinize the data that they rely upon and produce. What is it to be a dataset practitioner doing this work? We approach this in two parts: first, we define the role of "dataset practitioner" by performing a retrospective analysis on the responsibilities of teams contributing to LLM development at Google. Then, we conduct semi-structured interviews with a cross-section of these practitioners (N=10). We find that data quality is the top priority. To evaluate data quality, practitioners either rely on their own intuition or write custom evaluation logic. There is a lack of consensus across practitioners on what quality is and how to evaluate it. We discuss potential reasons for this phenomenon and opportunities for alignment.
&lt;/p&gt;</description></item><item><title>Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14320</link><description>&lt;p&gt;
Triad: &#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14320
&lt;/p&gt;
&lt;p&gt;
Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#31572;&#30693;&#35782;&#24211;&#20013;&#38382;&#39064;&#30340;&#36816;&#29992;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#23454;&#29616;KBQA&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#20197;&#20219;&#21153;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triad&#65292;&#19968;&#20010;&#21033;&#29992;&#20855;&#26377;&#19977;&#20010;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#36827;&#34892;KBQA&#20219;&#21153;&#12290;&#20195;&#29702;&#34987;&#20998;&#37197;&#19977;&#20010;&#35282;&#33394;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;KBQA&#23376;&#20219;&#21153;&#65306;&#20316;&#20026;&#25484;&#25569;&#21508;&#31181;&#23376;&#20219;&#21153;&#30340;&#36890;&#25165;&#65292;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#32773;&#30340;&#20915;&#31574;&#32773;&#65292;&#20197;&#21450;&#20316;&#20026;&#22238;&#31572;&#24102;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#30340;&#39038;&#38382;&#12290;&#25105;&#20204;&#30340;KBQA&#26694;&#26550;&#22312;&#22235;&#20010;&#38454;&#27573;&#20013;&#25191;&#34892;&#65292;&#28041;&#21450;&#20195;&#29702;&#30340;&#22810;&#37325;&#35282;&#33394;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;OccCANINE&#24037;&#20855;&#65292;&#25105;&#20204;&#25104;&#21151;&#25171;&#30772;&#20102;HISCO&#38556;&#30861;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#32844;&#19994;&#26631;&#20934;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#31616;&#21270;&#20102;&#23545;&#32844;&#19994;&#25551;&#36848;&#30340;&#22788;&#29702;&#21644;&#20998;&#31867;&#36807;&#31243;&#65292;&#20026;&#32463;&#27982;&#23398;&#12289;&#32463;&#27982;&#21382;&#21490;&#31561;&#39046;&#22495;&#30340;&#32844;&#19994;&#32467;&#26500;&#20998;&#26512;&#25552;&#20379;&#20102;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.13604</link><description>&lt;p&gt;
&#25171;&#30772;HISCO&#38556;&#30861;&#65306;&#20351;&#29992;OccCANINE&#36827;&#34892;&#33258;&#21160;&#32844;&#19994;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13604
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;OccCANINE&#24037;&#20855;&#65292;&#25105;&#20204;&#25104;&#21151;&#25171;&#30772;&#20102;HISCO&#38556;&#30861;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#32844;&#19994;&#26631;&#20934;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#31616;&#21270;&#20102;&#23545;&#32844;&#19994;&#25551;&#36848;&#30340;&#22788;&#29702;&#21644;&#20998;&#31867;&#36807;&#31243;&#65292;&#20026;&#32463;&#27982;&#23398;&#12289;&#32463;&#27982;&#21382;&#21490;&#31561;&#39046;&#22495;&#30340;&#32844;&#19994;&#32467;&#26500;&#20998;&#26512;&#25552;&#20379;&#20102;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#24037;&#20855;OccCANINE&#65292;&#21487;&#33258;&#21160;&#23558;&#32844;&#19994;&#25551;&#36848;&#36716;&#25442;&#20026;HISCO&#20998;&#31867;&#31995;&#32479;&#12290;&#22788;&#29702;&#21644;&#20998;&#31867;&#32844;&#19994;&#25551;&#36848;&#28041;&#21450;&#30340;&#25163;&#21160;&#24037;&#20316;&#23481;&#26131;&#20986;&#38169;&#12289;&#32321;&#29712;&#19988;&#32791;&#26102;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;CANINE&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20960;&#31186;&#38047;&#21040;&#20960;&#20998;&#38047;&#20869;&#33258;&#21160;&#23436;&#25104;&#27492;&#36807;&#31243;&#65292;&#32780;&#20197;&#21069;&#38656;&#35201;&#25968;&#22825;&#29978;&#33267;&#25968;&#21608;&#12290;&#35813;&#27169;&#22411;&#22312;&#26469;&#33258;22&#20010;&#19981;&#21516;&#26469;&#28304;&#36129;&#29486;&#30340;13&#31181;&#35821;&#35328;&#20013;&#30340;1400&#19975;&#23545;&#32844;&#19994;&#25551;&#36848;&#21644;HISCO&#20195;&#30721;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#20934;&#30830;&#29575;&#22343;&#36229;&#36807;90%&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#31361;&#30772;&#20102;&#35937;&#24449;&#24615;HISCO&#38556;&#30861;&#65292;&#24182;&#20351;&#36825;&#20123;&#25968;&#25454;&#21487;&#20379;&#32463;&#27982;&#23398;&#12289;&#32463;&#27982;&#21382;&#21490;&#21644;&#21508;&#31181;&#30456;&#20851;&#23398;&#31185;&#20013;&#30340;&#32844;&#19994;&#32467;&#26500;&#20998;&#26512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13604v1 Announce Type: new  Abstract: This paper introduces a new tool, OccCANINE, to automatically transform occupational descriptions into the HISCO classification system. The manual work involved in processing and classifying occupational descriptions is error-prone, tedious, and time-consuming. We finetune a preexisting language model (CANINE) to do this automatically thereby performing in seconds and minutes what previously took days and weeks. The model is trained on 14 million pairs of occupational descriptions and HISCO codes in 13 different languages contributed by 22 different sources. Our approach is shown to have accuracy, recall and precision above 90 percent. Our tool breaks the metaphorical HISCO barrier and makes this data readily available for analysis of occupational structures with broad applicability in economics, economic history and various related disciplines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12997</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#20877;&#25490;&#24207;&#65306;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#24323;&#26435;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#65288;NIR&#65289;&#24050;&#32463;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;IR&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22833;&#36133;&#20173;&#28982;&#39057;&#32321;&#21457;&#29983;&#65292;&#36890;&#24120;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26080;&#27861;&#26816;&#32034;&#19982;&#29992;&#25143;&#26597;&#35810;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29305;&#21035;&#24378;&#35843;&#20877;&#25490;&#24207;&#38454;&#27573;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21327;&#35758;&#65292;&#29992;&#20110;&#22312;&#40657;&#21283;&#23376;&#22330;&#26223;&#20013;&#35780;&#20272;&#24323;&#26435;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#22797;&#21046;&#21644;&#24323;&#26435;&#23454;&#26045;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#20419;&#36827;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>FEUDA&#26159;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#35760;&#21644;&#26631;&#35760;&#30340;&#31034;&#20363;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#22522;&#30784;&#30340;&#20998;&#31867;&#26694;&#26550;&#20013;&#25506;&#32034;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#33539;&#20363;&#12290;</title><link>https://arxiv.org/abs/2401.17514</link><description>&lt;p&gt;
FEUDA&#65306;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17514
&lt;/p&gt;
&lt;p&gt;
FEUDA&#26159;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#35760;&#21644;&#26631;&#35760;&#30340;&#31034;&#20363;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#22522;&#30784;&#30340;&#20998;&#31867;&#26694;&#26550;&#20013;&#25506;&#32034;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#33539;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#20998;&#25903;&#21033;&#29992;&#26469;&#33258;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#23398;&#20064;&#36866;&#24212;&#30340;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#40723;&#21169;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#20998;&#31867;&#26694;&#26550;&#20013;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25110;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#24517;&#35201;&#24615;&#20173;&#19981;&#28165;&#26970;&#65292;&#20854;&#20013;&#19968;&#20010;&#36755;&#20837;&#31034;&#20363;&#30001;&#27169;&#26495;&#20462;&#25913;&#21518;&#65292;&#20877;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20013;&#29983;&#25104;&#19968;&#20010;&#26631;&#31614;&#23383;&#31526;&#20018;&#12290;&#20026;&#20102;&#30740;&#31350;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#36825;&#31181;&#26032;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;FEUDA&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#65292;&#22312;&#26410;&#26631;&#35760;&#21644;&#26631;&#35760;&#30340;&#31034;&#20363;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;LM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31532;&#19968;&#20010;&#20219;&#21153;&#36890;&#36807;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#22312;&#20004;&#20010;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26412;&#19978;&#35757;&#32451;LM&#65292;&#31532;&#20108;&#20010;&#20219;&#21153;&#20351;&#29992;&#28304;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#25351;&#20196;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation. However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training. The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string. To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks. Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for c
&lt;/p&gt;</description></item><item><title>&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#21487;&#20197;&#20316;&#20026;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23384;&#22312;&#23545;&#27969;&#34892;&#24086;&#23376;&#20559;&#35265;&#36739;&#39640;&#12289;&#24773;&#24863;&#26356;&#31215;&#26497;&#20197;&#21450;&#24573;&#35270;&#25919;&#27835;&#12289;&#33394;&#24773;&#21644;&#31895;&#20439;&#24086;&#23376;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.15479</link><description>&lt;p&gt;
&#24212;&#23545;&#21518;API&#22256;&#22659;&#65306;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#21576;&#29616;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#20559;&#35265;&#35266;
&lt;/p&gt;
&lt;p&gt;
Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15479
&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#21487;&#20197;&#20316;&#20026;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23384;&#22312;&#23545;&#27969;&#34892;&#24086;&#23376;&#20559;&#35265;&#36739;&#39640;&#12289;&#24773;&#24863;&#26356;&#31215;&#26497;&#20197;&#21450;&#24573;&#35270;&#25919;&#27835;&#12289;&#33394;&#24773;&#21644;&#31895;&#20439;&#24086;&#23376;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20572;&#27490;&#35775;&#38382;&#31038;&#20132;&#23186;&#20307;API&#30340;&#20915;&#23450;&#23545;&#20114;&#32852;&#32593;&#30740;&#31350;&#21644;&#25972;&#20010;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#20135;&#29983;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#36825;&#31181;&#23545;&#25968;&#25454;&#30340;&#35775;&#38382;&#32570;&#20047;&#24050;&#34987;&#31216;&#20026;&#20114;&#32852;&#32593;&#30740;&#31350;&#30340;&#21518;API&#26102;&#20195;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#27969;&#34892;&#30340;&#25628;&#32034;&#24341;&#25806;&#26377;&#33021;&#21147;&#29228;&#21462;&#12289;&#25429;&#33719;&#21644;&#23637;&#31034;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#22312;&#20854;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;(SERP)&#19978;&#65292;&#22914;&#26524;&#25552;&#20379;&#36866;&#24403;&#30340;&#25628;&#32034;&#26597;&#35810;&#65292;&#21487;&#33021;&#20250;&#20026;&#36825;&#19968;&#22256;&#22659;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#24403;&#21069;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;SERP&#26159;&#21542;&#25552;&#20379;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#23436;&#25972;&#21644;&#26080;&#20559;&#35265;&#26679;&#26412;&#65311; SERP&#26159;&#21542;&#26159;&#30452;&#25509;API&#35775;&#38382;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#65288;Google&#65289;SERP&#32467;&#26524;&#21644;&#26469;&#33258;Reddit&#21644;Twitter/X&#30340;&#38750;&#21462;&#26679;&#25968;&#25454;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SERP&#32467;&#26524;&#22312;&#25903;&#25345;&#27969;&#34892;&#24086;&#23376;&#26041;&#38754;&#23384;&#22312;&#39640;&#24230;&#20559;&#35265;&#65307;&#21453;&#23545;&#25919;&#27835;&#12289;&#33394;&#24773;&#21644;&#31895;&#20439;&#24086;&#23376;&#65307;&#22312;&#24773;&#24863;&#19978;&#26356;&#20026;&#31215;&#26497;&#65307;&#24182;&#26377;&#22823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15479v2 Announce Type: replace-cross  Abstract: Recent decisions to discontinue access to social media APIs are having detrimental effects on Internet research and the field of computational social science as a whole. This lack of access to data has been dubbed the Post-API era of Internet research. Fortunately, popular search engines have the means to crawl, capture, and surface social media data on their Search Engine Results Pages (SERP) if provided the proper search query, and may provide a solution to this dilemma. In the present work we ask: does SERP provide a complete and unbiased sample of social media data? Is SERP a viable alternative to direct API-access? To answer these questions, we perform a comparative analysis between (Google) SERP results and nonsampled data from Reddit and Twitter/X. We find that SERP results are highly biased in favor of popular posts; against political, pornographic, and vulgar posts; are more positive in their sentiment; and have large 
&lt;/p&gt;</description></item><item><title>TeleChat&#26159;&#19968;&#20010;&#21253;&#21547;30&#20159;&#12289;70&#20159;&#21644;120&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#38598;&#65292;&#26088;&#22312;&#25552;&#20379;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19982;&#20154;&#31867;&#21916;&#22909;&#30456;&#19968;&#33268;&#30340;&#24494;&#35843;&#32842;&#22825;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;TeleChat&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.03804</link><description>&lt;p&gt;
TeleChat&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
TeleChat Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03804
&lt;/p&gt;
&lt;p&gt;
TeleChat&#26159;&#19968;&#20010;&#21253;&#21547;30&#20159;&#12289;70&#20159;&#21644;120&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#38598;&#65292;&#26088;&#22312;&#25552;&#20379;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19982;&#20154;&#31867;&#21916;&#22909;&#30456;&#19968;&#33268;&#30340;&#24494;&#35843;&#32842;&#22825;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;TeleChat&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TeleChat&#65292;&#36825;&#26159;&#30001;30&#20159;&#12289;70&#20159;&#21644;120&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32452;&#25104;&#30340;&#21512;&#38598;&#12290;&#23427;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#19982;&#20154;&#31867;&#21916;&#22909;&#30456;&#19968;&#33268;&#30340;&#24494;&#35843;&#32842;&#22825;&#27169;&#22411;&#12290;TeleChat&#26368;&#21021;&#22312;&#21253;&#21547;&#33521;&#35821;&#21644;&#20013;&#25991;&#35821;&#35328;&#30340;&#22810;&#26679;&#25991;&#26412;&#38598;&#21512;&#20013;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#25968;&#19975;&#20159;&#30340;&#26631;&#35760;&#12290;&#38543;&#21518;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#24494;&#35843;&#20197;&#19982;&#20154;&#31867;&#21916;&#22909;&#30456;&#19968;&#33268;&#65292;&#36981;&#24490;&#25105;&#20204;&#25551;&#36848;&#30340;&#35814;&#32454;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;TeleChat&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#35821;&#35328;&#29702;&#35299;&#12289;&#25968;&#23398;&#12289;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;TeleChat&#22312;&#21508;&#31181;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#30456;&#20284;&#35268;&#27169;&#30340;&#21487;&#27604;&#24615;&#33021;&#12290;&#20026;&#25903;&#25345;&#26410;&#26469;&#21033;&#29992;&#35813;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03804v2 Announce Type: replace-cross  Abstract: In this technical report, we present TeleChat, a collection of large language models (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, including trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#20174;&#20301;&#32622;&#32534;&#30721;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#29992;&#20110;&#22686;&#24378;Transformer&#38271;&#24230;&#22806;&#25512;&#33021;&#21147;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32477;&#23545;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#20197;&#21450;&#22522;&#20110;&#23427;&#20204;&#30340;&#22806;&#25512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.17044</link><description>&lt;p&gt;
Transformer&#38271;&#24230;&#22806;&#25512;&#65306;&#20174;&#20301;&#32622;&#32534;&#30721;&#30340;&#35282;&#24230;&#36827;&#34892;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#20174;&#20301;&#32622;&#32534;&#30721;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#29992;&#20110;&#22686;&#24378;Transformer&#38271;&#24230;&#22806;&#25512;&#33021;&#21147;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32477;&#23545;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#20197;&#21450;&#22522;&#20110;&#23427;&#20204;&#30340;&#22806;&#25512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#33258;&#35806;&#29983;&#20197;&#26469;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#25472;&#36215;&#20102;&#19968;&#32929;&#39118;&#26292;&#12290;&#24314;&#31435;&#22312;&#20854;&#22522;&#30784;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21253;&#25324;&#36825;&#20123;&#24378;&#22823;&#30340;LLMs&#22312;&#20869;&#30340;&#25152;&#26377;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#37117;&#21463;&#21046;&#20110;&#39044;&#35774;&#30340;&#38271;&#24230;&#38480;&#21046;&#65292;&#24456;&#38590;&#20174;&#30701;&#35757;&#32451;&#24207;&#21015;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#25512;&#26029;&#24207;&#21015;&#65292;&#21363;&#23427;&#20204;&#26080;&#27861;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#26041;&#27861;&#26469;&#22686;&#24378;Transformer&#30340;&#38271;&#24230;&#22806;&#25512;&#33021;&#21147;&#65292;&#20854;&#20013;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#34987;&#35748;&#20026;&#26159;&#20027;&#35201;&#22240;&#32032;&#12290; &#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20174;PE&#30340;&#35282;&#24230;&#20197;&#32479;&#19968;&#31526;&#21495;&#20171;&#32461;&#20102;&#36825;&#20123;&#20851;&#20110;&#38271;&#24230;&#22806;&#25512;&#30340;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#21487;&#22806;&#25512;&#30340;PE&#65292;&#21253;&#25324;&#32477;&#23545;&#21644;&#30456;&#23545;PE&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#23427;&#20204;&#30340;&#22806;&#25512;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#20301;&#32622;&#25554;&#20540;&#21644;&#38543;&#26426;&#21270;&#20301;&#32622;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17044v3 Announce Type: replace  Abstract: Transformer has taken the field of natural language processing (NLP) by storm since its birth. Further, Large language models (LLMs) built upon it have captured worldwide attention due to its superior abilities. Nevertheless, all Transformer-based models including these powerful LLMs suffer from a preset length limit and can hardly generalize from short training sequences to longer inference ones, namely, they can not perform length extrapolation. Hence, a plethora of methods have been proposed to enhance length extrapolation of Transformer, in which the positional encoding (PE) is recognized as the major factor. In this survey, we present these advances towards length extrapolation in a unified notation from the perspective of PE. Specifically, we first introduce extrapolatable PEs, including absolute and relative PEs. Then, we dive into extrapolation methods based on them, covering position interpolation and randomized position met
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.07751</link><description>&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65306;&#38656;&#27714;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Human Language Models: A Need and the Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07751
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#20013;&#24515;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#36827;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#23558;&#20154;&#31867;&#21644;&#31038;&#20250;&#22240;&#32032;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#24050;&#32463;&#20005;&#37325;&#20381;&#36182;&#20110;LLM&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#24182;&#27809;&#26377;&#23545;&#20316;&#32773;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#30495;&#27491;&#29702;&#35299;&#20154;&#31867;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#26356;&#22909;&#22320;&#23558;&#20154;&#31867;&#32972;&#26223;&#25972;&#21512;&#21040;LLM&#20013;&#12290;&#36825;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#35774;&#35745;&#32771;&#34385;&#21644;&#25361;&#25112;&#65292;&#28041;&#21450;&#21040;&#35201;&#25429;&#25417;&#21738;&#20123;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#23427;&#20204;&#20197;&#21450;&#35201;&#37319;&#29992;&#20309;&#31181;&#24314;&#27169;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#20174;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#31185;&#23398;&#30340;&#27010;&#24565;&#20986;&#21457;&#65292;&#25903;&#25345;&#19977;&#20010;&#31435;&#22330;&#26469;&#21019;&#24314;&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65288;LHLMs&#65289;&#65306;&#39318;&#20808;&#65292;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24212;&#21253;&#25324;&#20154;&#31867;&#32972;&#26223;&#12290;&#20854;&#27425;&#65292;LHLMs&#24212;&#35813;&#24847;&#35782;&#21040;&#20154;&#19981;&#20165;&#20165;&#26159;&#20182;&#20204;&#25152;&#23646;&#30340;&#32676;&#20307;&#12290;&#31532;&#19977;&#65292;LHLMs&#24212;&#35813;&#33021;&#22815;&#32771;&#34385;&#21040;&#20154;&#31867;&#32972;&#26223;&#30340;&#21160;&#24577;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07751v2 Announce Type: replace-cross  Abstract: As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#24418;&#24335;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#26412;&#20307;&#35770;&#65292;&#29992;OWL&#32534;&#20889;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#34920;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2312.05209</link><description>&lt;p&gt;
HALO&#65306;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#31034;&#21644;&#20998;&#31867;&#24187;&#35273;&#30340;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#24418;&#24335;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#26412;&#20307;&#35770;&#65292;&#29992;OWL&#32534;&#20889;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#34920;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05209v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20026;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21040;&#30693;&#35782;&#21457;&#29616;&#21644;&#25968;&#25454;&#25366;&#25496;&#31561;&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#20363;&#22914;&#34394;&#26500;&#20449;&#24687;&#25110;&#8220;&#24187;&#35273;&#8221;&#65292;&#20197;&#21450;&#22312;&#30475;&#20284;&#31616;&#21333;&#38382;&#39064;&#19978;&#29359;&#38169;&#35823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30001;&#20110;&#20687;ChatGPT&#36825;&#26679;&#30340;&#27169;&#22411;&#22791;&#21463;&#27426;&#36814;&#65292;&#23398;&#26415;&#30028;&#21644;&#20844;&#27665;&#31185;&#23398;&#23478;&#37117;&#35760;&#24405;&#20102;&#22810;&#31181;&#31867;&#22411;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#24187;&#35273;&#12290;&#23613;&#31649;&#24050;&#26377;&#30456;&#20851;&#24037;&#20316;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#19968;&#31181;&#24418;&#24335;&#27169;&#22411;&#26469;&#32454;&#33268;&#25551;&#36848;&#21644;&#34920;&#31034;&#36825;&#20123;&#24187;&#35273;&#65288;&#24102;&#26377;&#30456;&#20851;&#20803;&#25968;&#25454;&#65289;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;Hallucination Ontology&#25110;HALO&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;HALO&#26159;&#19968;&#31181;&#27491;&#24335;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#26412;&#20307;&#35770;&#65292;&#29992;OWL&#32534;&#20889;&#65292;&#30446;&#21069;&#25903;&#25345;&#20845;&#31181;&#24050;&#30693;&#30340;&#24187;&#35273;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05209v2 Announce Type: replace  Abstract: Recent progress in generative AI, including large language models (LLMs) like ChatGPT, has opened up significant opportunities in fields ranging from natural language processing to knowledge discovery and data mining. However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems. Because of the popularity of models like ChatGPT, both academic scholars and citizen scientists have documented hallucinations of several different types and severity. Despite this body of work, a formal model for describing and representing these hallucinations (with relevant meta-data) at a fine-grained level, is still lacking. In this paper, we address this gap by presenting the Hallucination Ontology or HALO, a formal, extensible ontology written in OWL that currently offers support for six different types of hallucinations known to 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#25110;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#23545;&#25968;&#23398;&#23478;&#30340;&#28508;&#22312;&#24110;&#21161;&#21644;&#25913;&#21464;&#24037;&#20316;&#26041;&#24335;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2312.04556</link><description>&lt;p&gt;
&#25968;&#23398;&#23478;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Mathematicians
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04556
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#25110;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#23545;&#25968;&#23398;&#23478;&#30340;&#28508;&#22312;&#24110;&#21161;&#21644;&#25913;&#21464;&#24037;&#20316;&#26041;&#24335;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#25110;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#23545;&#35768;&#22810;&#32844;&#19994;&#26469;&#35828;&#65292;LLMs&#20195;&#34920;&#19968;&#31181;&#26080;&#20215;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#21152;&#24555;&#24037;&#20316;&#36895;&#24230;&#24182;&#25552;&#39640;&#24037;&#20316;&#36136;&#37327;&#12290;&#26412;&#25991;&#35752;&#35770;&#23427;&#20204;&#22312;&#24110;&#21161;&#19987;&#19994;&#25968;&#23398;&#23478;&#26041;&#38754;&#30340;&#20316;&#29992;&#31243;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#25152;&#26377;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;&#30340;&#25968;&#23398;&#25551;&#36848;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#38382;&#39064;&#65292;&#24182;&#25253;&#21578;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#25913;&#21464;&#25968;&#23398;&#23478;&#24037;&#20316;&#26041;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04556v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) such as ChatGPT have received immense interest for their general-purpose language understanding and, in particular, their ability to generate high-quality text or computer code. For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work. In this note, we discuss to what extent they can aid professional mathematicians. We first provide a mathematical description of the transformer model used in all modern language models. Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models. Finally, we shed light on the potential of LLMs to change how mathematicians work.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#30340;LLM&#25552;&#31034;&#25216;&#26415;In-Context Sampling&#65288;ICS&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;ICL&#25552;&#31034;&#36755;&#20837;&#30340;&#26500;&#24314;&#26469;&#29983;&#25104;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;ICS&#21487;&#20197;&#25345;&#32493;&#22686;&#24378;LLM&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#25968;&#25454;&#30456;&#20284;&#24615;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2311.09782</link><description>&lt;p&gt;
&#26356;&#22810;&#26679;&#26412;&#36824;&#26159;&#26356;&#22810;&#25552;&#31034;&#65311;&#25506;&#32034;LLM&#23569;&#26679;&#26412;&#25552;&#31034;&#24037;&#31243;&#20013;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
More Samples or More Prompts? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#30340;LLM&#25552;&#31034;&#25216;&#26415;In-Context Sampling&#65288;ICS&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;ICL&#25552;&#31034;&#36755;&#20837;&#30340;&#26500;&#24314;&#26469;&#29983;&#25104;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;ICS&#21487;&#20197;&#25345;&#32493;&#22686;&#24378;LLM&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#25968;&#25454;&#30456;&#20284;&#24615;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#20851;&#20110;LLM&#25552;&#31034;&#25216;&#26415;&#30340;&#24037;&#20316;&#21482;&#20851;&#27880;&#22914;&#20309;&#22312;&#21333;&#20010;&#25552;&#31034;&#36755;&#20837;&#20013;&#36873;&#25321;&#26356;&#22909;&#30340;&#25968;&#25454;&#26679;&#26412;&#38598;&#65288;&#19978;&#19979;&#25991;&#23398;&#20064;&#25110;ICL&#65289;&#65292;&#20294;&#20026;&#20160;&#20040;&#25105;&#20204;&#19981;&#33021;&#35774;&#35745;&#24182;&#21033;&#29992;&#22810;&#20010;&#25552;&#31034;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#30340;LLM&#25552;&#31034;&#25216;&#26415;In-Context Sampling&#65288;ICS&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;ICL&#25552;&#31034;&#36755;&#20837;&#30340;&#26500;&#24314;&#26469;&#29983;&#25104;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#22312;&#22235;&#20010;NLI&#25968;&#25454;&#38598;&#65288;e-SNLI&#12289;Multi-NLI&#12289;ANLI&#21644;Contract-NLI&#65289;&#21644;&#19968;&#20010;QA&#25968;&#25454;&#38598;&#65288;CommonsenseQA&#65289;&#19978;&#65292;&#19982;&#19977;&#20010;&#24320;&#28304;LLM&#65288;FlanT5-XL&#12289;Mistral-7B&#21644;Mixtral-8x7B&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;ICS&#21487;&#20197;&#25345;&#32493;&#22686;&#24378;LLM&#30340;&#24615;&#33021;&#12290;&#19982;&#19977;&#31181;&#22522;&#20110;&#25968;&#25454;&#30456;&#20284;&#24615;&#30340;ICS&#31574;&#30053;&#30340;&#28145;&#20837;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;LLM&#30340;&#24615;&#33021;&#65292;&#20026;&#26032;&#30340;&#20294;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25237;&#19979;&#20809;&#33426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09782v2 Announce Type: replace  Abstract: While most existing works on LLM prompting techniques focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can not we design and leverage multiple prompts together to further improve the LLM's performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, and Mixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance LLMs' performance. An in-depth evaluation with three data similarity-based ICS strategies suggests that these strategies can further elevate LLM's performance, which sheds light on a new yet promising future research direction.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26684;&#27979;&#37327;&#24037;&#20855;&#19978;&#30340;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#25506;&#35752;&#24403;&#21069;&#25552;&#31034;&#26041;&#24335;&#26159;&#21542;&#23548;&#33268;&#19968;&#33268;&#19988;&#31283;&#20581;&#30340;&#21709;&#24212;</title><link>https://arxiv.org/abs/2311.09718</link><description>&lt;p&gt;
&#20320;&#19981;&#38656;&#35201;&#36827;&#34892;&#20154;&#26684;&#27979;&#35797;&#26469;&#30693;&#36947;&#36825;&#20123;&#27169;&#22411;&#26159;&#19981;&#21487;&#38752;&#30340;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#27979;&#37327;&#24037;&#20855;&#19978;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09718
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26684;&#27979;&#37327;&#24037;&#20855;&#19978;&#30340;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#25506;&#35752;&#24403;&#21069;&#25552;&#31034;&#26041;&#24335;&#26159;&#21542;&#23548;&#33268;&#19968;&#33268;&#19988;&#31283;&#20581;&#30340;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#20351;&#20854;&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#20026;&#20102;&#27491;&#30830;&#29702;&#35299;LLMs&#30340;&#23646;&#24615;&#21644;&#22266;&#26377;&#20154;&#26684;&#65292;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#20102;&#28041;&#21450;&#20351;&#29992;&#25552;&#31034;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#25552;&#31034;&#37319;&#29992;&#38382;&#39064;&#24418;&#24335;&#35810;&#38382;LLMs&#20851;&#20110;&#29305;&#23450;&#35266;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35880;&#24910;&#36864;&#19968;&#27493;&#65292;&#26816;&#26597;&#24403;&#21069;&#25552;&#31034;LLMs&#30340;&#26684;&#24335;&#26159;&#21542;&#20197;&#19968;&#33268;&#19988;&#31283;&#20581;&#30340;&#26041;&#24335;&#24341;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;693&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;115&#20010;&#20154;&#26684;&#36724;&#19978;&#30340;39&#31181;&#19981;&#21516;&#20154;&#26684;&#27979;&#37327;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#21253;&#21547;&#24494;&#23567;&#21464;&#21270;&#30340;&#25552;&#31034;&#65292;&#24182;&#26816;&#26597;LLMs&#29983;&#25104;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25552;&#31034;&#21464;&#21270;&#20197;&#26816;&#26597;&#23427;&#20204;&#22312;&#20869;&#23481;&#32423;&#21035;&#21464;&#21270;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#65292;&#20363;&#22914;&#26356;&#25913;&#21709;&#24212;&#36873;&#39033;&#30340;&#39034;&#24207;&#25110;&#21542;&#23450;&#35813;&#35821;&#21477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09718v2 Announce Type: replace-cross  Abstract: The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting LLMs elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLMs' capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experimen
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#26032;&#35821;&#35328;&#27169;&#22411;&#36827;&#23637;&#30340;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#25925;&#20107;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2311.09648</link><description>&lt;p&gt;
&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#26159;&#35745;&#31639;&#25925;&#20107;&#29702;&#35299;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Event Causality Is Key to Computational Story Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09648
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#26032;&#35821;&#35328;&#27169;&#22411;&#36827;&#23637;&#30340;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#25925;&#20107;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#34920;&#26126;&#65292;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#20026;&#25925;&#20107;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#25925;&#20107;&#29702;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24456;&#23569;&#20351;&#29992;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#21487;&#38752;&#22320;&#35782;&#21035;&#24320;&#25918;&#19990;&#30028;&#22240;&#26524;&#20107;&#20214;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;&#20511;&#21161;&#36817;&#26399;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#35782;&#21035;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#25925;&#20107;&#29702;&#35299;&#26041;&#38754;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;COPES&#25968;&#25454;&#38598;&#65288;Wang&#31561;&#65292;2023&#24180;&#65289;&#30340;&#22240;&#26524;&#20107;&#20214;&#20851;&#31995;&#35782;&#21035;&#26041;&#38754;&#36798;&#21040;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#22312;&#19979;&#28216;&#25925;&#20107;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#20013;&#65292;&#35782;&#21035;&#30340;&#22240;&#26524;&#20851;&#31995;&#23548;&#33268;&#19982;&#20154;&#31867;&#35780;&#32423;&#30340;&#30456;&#20851;&#24615;&#25552;&#39640;&#20102;3.6-16.6%&#12290;&#22312;&#22810;&#27169;&#24335;&#25925;&#20107;&#35270;&#39057;-&#25991;&#26412;&#23545;&#40784;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;Clip&#20934;&#30830;&#24615;&#25552;&#39640;4.1-10.9%&#20197;&#21450;&#21477;&#23376;IoU&#25552;&#39640;4.2-13.5%&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09648v2 Announce Type: replace  Abstract: Cognitive science and symbolic AI research suggest that event causality provides vital information for story understanding. However, machine learning systems for story understanding rarely employ event causality, partially due to the lack of methods that reliably identify open-world causal event relations. Leveraging recent progress in large language models, we present the first method for event causality identification that leads to material improvements in computational story understanding. Our technique sets a new state of the art on the COPES dataset (Wang et al., 2023) for causal event relation identification. Further, in the downstream story quality evaluation task, the identified causal relations lead to 3.6-16.6% relative improvement on correlation with human ratings. In the multimodal story video-text alignment task, we attain 4.1-10.9% increase on Clip Accuracy and 4.2-13.5% increase on Sentence IoU. The findings indicate s
&lt;/p&gt;</description></item><item><title>&#23558;$k$NN&#26816;&#32034;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#21487;&#20197;&#38477;&#20302;&#22256;&#24785;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#23384;&#20648;&#25104;&#26412;&#38477;&#20302;&#36229;&#36807;25&#20493;&#12290;</title><link>https://arxiv.org/abs/2311.09615</link><description>&lt;p&gt;
&#20851;&#20110;&#26816;&#32034;&#22686;&#24378;&#21644;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Retrieval Augmentation and the Limitations of Language Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09615
&lt;/p&gt;
&lt;p&gt;
&#23558;$k$NN&#26816;&#32034;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#21487;&#20197;&#38477;&#20302;&#22256;&#24785;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#23384;&#20648;&#25104;&#26412;&#38477;&#20302;&#36229;&#36807;25&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;$k$-nearest neighbors ($k$NN)&#26816;&#32034;&#30456;&#32467;&#21512;&#21487;&#20197;&#38477;&#20302;&#20854;&#22256;&#24785;&#24230;&#65292;&#23613;&#31649;&#20854;&#32972;&#21518;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25490;&#38500;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#19968;&#20010;&#21487;&#33021;&#24615;-&#8220;softmax&#29942;&#39048;&#8221;&#12290;&#25105;&#20204;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;LM&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#39069;&#22806;&#38750;&#22240;&#26524;&#30456;&#20851;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21363;&#20351;&#23545;&#20110;GPT-3.5 Turbo&#26469;&#35828;&#65292;&#36825;&#39033;&#20219;&#21153;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;GPT-2&#21644;Mistral 7B&#65292;$k$NN&#26816;&#32034;&#22686;&#24378;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22987;&#32456;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20351;$k$NN&#26816;&#32034;&#26356;&#26131;&#29992;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#27169;&#22411;&#23558;&#25968;&#25454;&#23384;&#20648;&#38190;&#26144;&#23556;&#21040;&#20540;&#65292;&#20316;&#20026;&#20256;&#32479;&#26816;&#32034;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#36825;&#26679;&#21487;&#20197;&#23558;&#23384;&#20648;&#25104;&#26412;&#38477;&#20302;&#36229;&#36807;25&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09615v2 Announce Type: replace  Abstract: Augmenting a language model (LM) with $k$-nearest neighbors ($k$NN) retrieval on its training data alone can decrease its perplexity, though the underlying reasons for this remain elusive. In this work, we rule out one previously posited possibility -- the "softmax bottleneck." We then create a new dataset to evaluate LM generalization ability in the setting where training data contains additional information that is not causally relevant. This task is challenging even for GPT-3.5 Turbo. We show that, for both GPT-2 and Mistral 7B, $k$NN retrieval augmentation consistently improves performance in this setting. Finally, to make $k$NN retrieval more accessible, we propose using a multi-layer perceptron model that maps datastore keys to values as a drop-in replacement for traditional retrieval. This reduces storage costs by over 25x.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rationale Distillation&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#24182;OCR&#24037;&#20855;&#36755;&#20986;&#12289;LLMs&#21644;&#26356;&#22823;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20013;&#38388;&#8220;rationales&#8221;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#26469;&#39044;&#27979;rationales&#21644;&#31572;&#26696;&#65292;&#20197;&#23454;&#29616;&#23545;&#35270;&#35273;&#25991;&#26723;&#30340;&#39640;&#25928;&#31471;&#21040;&#31471;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2311.09612</link><description>&lt;p&gt;
&#20351;&#29992;Rationale Distillation&#23454;&#29616;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Efficient End-to-End Visual Document Understanding with Rationale Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09612
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rationale Distillation&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#24182;OCR&#24037;&#20855;&#36755;&#20986;&#12289;LLMs&#21644;&#26356;&#22823;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20013;&#38388;&#8220;rationales&#8221;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#26469;&#39044;&#27979;rationales&#21644;&#31572;&#26696;&#65292;&#20197;&#23454;&#29616;&#23545;&#35270;&#35273;&#25991;&#26723;&#30340;&#39640;&#25928;&#31471;&#21040;&#31471;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35270;&#35273;&#35821;&#22659;&#38656;&#35201;&#35299;&#37322;&#22797;&#26434;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#24067;&#23616;&#12290;&#39044;&#22788;&#29702;&#24037;&#20855;&#65292;&#22914;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#65292;&#21487;&#20197;&#23558;&#25991;&#26723;&#22270;&#20687;&#36755;&#20837;&#26144;&#23556;&#21040;&#25991;&#26412;&#26631;&#35760;&#65292;&#28982;&#21518;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#23545;&#25991;&#26412;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#39640;&#35745;&#31639;&#21644;&#24037;&#31243;&#22797;&#26434;&#24615;&#12290;&#23567;&#22411;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#27169;&#22411;&#33021;&#21542;&#36890;&#36807;&#31867;&#20284;&#30340;&#35782;&#21035;&#21644;&#25512;&#29702;&#27493;&#39588;&#20934;&#30830;&#29702;&#35299;&#35270;&#35273;&#25991;&#26723;&#65311;&#25105;&#20204;&#25552;&#20986;Rationale Distillation&#65288;RD&#65289;&#65292;&#23427;&#23558;OCR&#24037;&#20855;&#12289;LLMs&#21644;&#26356;&#22823;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#36755;&#20986;&#20316;&#20026;&#20013;&#38388;&#8220;rationales&#8221;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#26469;&#39044;&#27979;rationales&#21644;&#31572;&#26696;&#12290;&#22312;&#20195;&#34920;&#20449;&#24687;&#22270;&#34920;&#12289;&#25195;&#25551;&#25991;&#26723;&#21644;&#22270;&#34920;&#30340;&#19977;&#20010;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30340;Pix2Struct&#65288;282M&#21442;&#25968;&#65289;&#23398;&#29983;&#27169;&#22411;&#22312;&#32463;&#36807;RD&#24494;&#35843;&#21518;&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;4-5%&#30340;&#32477;&#23545;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09612v2 Announce Type: replace-cross  Abstract: Understanding visually situated language requires interpreting complex layouts of textual and visual elements. Pre-processing tools, such as optical character recognition (OCR), can map document image inputs to textual tokens, then large language models (LLMs) can reason over text. However, such methods have high computational and engineering complexity. Can small pretrained image-to-text models accurately understand visual documents through similar recognition and reasoning steps instead? We propose Rationale Distillation (RD), which incorporates the outputs of OCR tools, LLMs, and larger multimodal models as intermediate "rationales", and trains a small student model to predict both rationales and answers. On three visual document understanding benchmarks representing infographics, scanned documents, and figures, our Pix2Struct (282M parameters) student model finetuned with RD outperforms the base model by 4-5% absolute accur
&lt;/p&gt;</description></item><item><title>&#35299;&#37322;&#26684;&#24335;&#23545;&#20154;&#31867;&#21453;&#39304;&#25928;&#26524;&#21644;&#29992;&#25143;&#24863;&#30693;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#30740;&#31350;&#20998;&#26512;&#22914;&#20309;&#21576;&#29616;&#27169;&#22411;&#21709;&#24212;&#20197;&#20415;&#26356;&#26131;&#25509;&#21463;&#29992;&#25143;&#32416;&#27491;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2311.09558</link><description>&lt;p&gt;
&#22914;&#26524;&#20320;&#29992;&#19981;&#21516;&#26041;&#24335;&#34920;&#36798;&#20250;&#24590;&#26679;&#65306;&#35299;&#37322;&#26684;&#24335;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#21453;&#39304;&#25928;&#26524;&#21644;&#29992;&#25143;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09558
&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26684;&#24335;&#23545;&#20154;&#31867;&#21453;&#39304;&#25928;&#26524;&#21644;&#29992;&#25143;&#24863;&#30693;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#30740;&#31350;&#20998;&#26512;&#22914;&#20309;&#21576;&#29616;&#27169;&#22411;&#21709;&#24212;&#20197;&#20415;&#26356;&#26131;&#25509;&#21463;&#29992;&#25143;&#32416;&#27491;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;NLP&#27169;&#22411;&#30340;&#26368;&#32456;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#21453;&#39304;&#23545;&#20110;&#25913;&#36827;&#27169;&#22411;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#21521;&#29992;&#25143;&#21576;&#29616;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#20351;&#20854;&#26368;&#26131;&#20110;&#25509;&#21463;&#26469;&#33258;&#29992;&#25143;&#21453;&#39304;&#30340;&#26356;&#27491;&#65311;&#27492;&#22806;&#65292;&#29992;&#25143;&#30475;&#37325;&#29702;&#35299;&#21644;&#20449;&#20219;&#21709;&#24212;&#30340;&#21738;&#20123;&#23646;&#24615;&#65311;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#30001;QA&#27169;&#22411;&#29983;&#25104;&#30340;&#25903;&#25345;&#20854;&#31572;&#26696;&#30340;&#29702;&#30001;&#65288;&#25110;&#35299;&#37322;&#65289;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#23558;QA&#27169;&#22411;&#20998;&#35299;&#20026;&#39318;&#20808;&#26681;&#25454;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#25552;&#21462;&#20013;&#38388;&#21407;&#22240;&#65292;&#28982;&#21518;&#20165;&#20351;&#29992;&#35813;&#21407;&#22240;&#26469;&#22238;&#31572;&#38382;&#39064;&#30340;&#27169;&#22411;&#12290;&#21407;&#22240;&#27010;&#36848;&#20102;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#36825;&#20123;&#21407;&#22240;&#30340;&#21508;&#31181;&#26684;&#24335;&#65292;&#26681;&#25454;&#24863;&#20852;&#36259;&#30340;&#26126;&#30830;&#23450;&#20041;&#30340;&#23646;&#24615;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25552;&#31034;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#25277;&#26679;&#65292;&#28982;&#21518;&#36827;&#34892;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21521;&#29992;&#25143;&#23637;&#31034;&#19981;&#27491;&#30830;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09558v2 Announce Type: replace  Abstract: Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales (or explanations) generated by QA models to support their answers. We specifically consider decomposed QA models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample rationales from language models using few-shot prompting for two datasets, and then perform two user studies. First, we present users with incorre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24694;&#24847;&#28436;&#31034;&#22312;&#20843;&#20010;&#26041;&#38754;&#23545;&#24320;&#28304;LLMs&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#25932;&#23545;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;advCoU&#65292;&#20197;&#25581;&#31034;&#23427;&#20204;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09447</link><description>&lt;p&gt;
&#24320;&#28304;LLMs&#30340;&#21487;&#20449;&#24230;&#26377;&#22810;&#39640;&#65311;&#23545;&#24694;&#24847;&#28436;&#31034;&#19979;&#30340;&#35780;&#20272;&#26174;&#31034;&#23427;&#20204;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24694;&#24847;&#28436;&#31034;&#22312;&#20843;&#20010;&#26041;&#38754;&#23545;&#24320;&#28304;LLMs&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#25932;&#23545;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;advCoU&#65292;&#20197;&#25581;&#31034;&#23427;&#20204;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09447v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;&#25688;&#35201;&#65306;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26174;&#33879;&#25512;&#21160;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#20102;&#35299;&#26377;&#38480;&#12290;&#22312;&#35268;&#27169;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#65292;&#32780;&#27809;&#26377;&#36275;&#22815;&#30340;&#21487;&#20449;&#24230;&#21487;&#33021;&#20250;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#65292;&#31361;&#20986;&#20102;&#21450;&#26102;&#21457;&#29616;&#36825;&#20123;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#23545;&#24320;&#28304;LLMs&#22312;&#21487;&#20449;&#24230;&#19978;&#36827;&#34892;&#20102;&#25932;&#23545;&#35780;&#20272;&#65292;&#36328;&#36275;&#27602;&#24615;&#12289;&#38472;&#35268;&#20439;&#22871;&#12289;&#20262;&#29702;&#12289;&#24187;&#35273;&#12289;&#20844;&#24179;&#24615;&#12289;&#35844;&#23194;&#12289;&#38544;&#31169;&#20197;&#21450;&#23545;&#25239;&#24615;&#28436;&#31034;&#30340;&#20843;&#20010;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;advCoU&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#23637;&#30340;Chain of Utterances&#65288;CoU&#65289;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#21152;&#20837;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#28436;&#31034;&#26469;&#25915;&#20987;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#28085;&#30422;&#20102;&#26368;&#36817;&#21644;&#20195;&#34920;&#24615;&#31995;&#21015;&#30340;&#24320;&#28304;LLMs&#65292;&#21253;&#25324;Vicuna&#12289;MPT&#12289;Falcon&#12289;Mistral&#21644;Llama 2&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09447v2 Announce Type: replace-cross  Abstract: The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical
&lt;/p&gt;</description></item><item><title>LLMRefine&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21453;&#39304;&#27169;&#22411;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#20301;&#32570;&#38519;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#38271;&#31687;&#38382;&#31572;&#21644;&#20027;&#39064;&#24635;&#32467;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2311.09336</link><description>&lt;p&gt;
LLMRefine&#65306;&#36890;&#36807;&#32454;&#31890;&#24230;&#21487;&#25805;&#20316;&#21453;&#39304;&#31934;&#30830;&#23450;&#20301;&#21644;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLMRefine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09336
&lt;/p&gt;
&lt;p&gt;
LLMRefine&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21453;&#39304;&#27169;&#22411;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#20301;&#32570;&#38519;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#38271;&#31687;&#38382;&#31572;&#21644;&#20027;&#39064;&#24635;&#32467;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMRefine&#65292;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#25512;&#29702;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;LLM&#30340;&#36755;&#20986;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#23398;&#20064;&#30340;&#32454;&#31890;&#24230;&#21453;&#39304;&#27169;&#22411;&#26469;&#20934;&#30830;&#23450;&#20301;&#32570;&#38519;&#65292;&#24182;&#24341;&#23548;LLM&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;LLM&#20316;&#20026;&#32534;&#36753;&#24314;&#35758;&#65292;LLMRefine&#36890;&#36807;&#27169;&#25311;&#36864;&#28779;&#25628;&#32034;&#26080;&#32570;&#38519;&#25991;&#26412;&#65292;&#26435;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#65292;&#38271;&#31687;&#38382;&#31572;&#65288;QA&#65289;&#21644;&#20027;&#39064;&#24635;&#32467;&#12290;LLMRefine&#22312;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#19978;&#19968;&#36143;&#34920;&#29616;&#20248;&#24322;&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#39640;&#36798;1.7 MetricX&#28857;&#30340;&#25913;&#36827;&#65292;&#22312;ASQA&#19978;&#20026;8.1 ROUGE-L&#65292;&#22312;&#20027;&#39064;&#24635;&#32467;&#19978;&#20026;2.2 ROUGE-L&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09336v2 Announce Type: replace  Abstract: Recent large language models (LLM) are leveraging human feedback to improve their generation quality. However, human feedback is costly to obtain, especially during inference. In this work, we propose LLMRefine, an inference time optimization method to refine LLM's output. The core idea is to use a learned fine-grained feedback model to pinpoint defects and guide LLM to refine them iteratively. Using original LLM as a proposal of edits, LLMRefine searches for defect-less text via simulated annealing, trading off the exploration and exploitation. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA), and topical summarization. LLMRefine consistently outperforms all baseline approaches, achieving improvements up to 1.7 MetricX points on translation tasks, 8.1 ROUGE-L on ASQA, 2.2 ROUGE-L on topical summarization.
&lt;/p&gt;</description></item><item><title>&#31038;&#21306;&#22411;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20801;&#35768;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#33647;&#29289;&#30456;&#20851;&#34892;&#20026;&#65292;&#22312;2500&#20010;&#38463;&#29255;&#31867;&#33647;&#29289;&#24086;&#23376;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26631;&#35760;&#20845;&#31181;&#19981;&#21516;&#38454;&#27573;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29255;&#27573;&#32423;&#25688;&#35201;&#35299;&#37322;&#22312;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#22312;&#30417;&#30563;&#12289;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;&#20960;&#31181;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2311.09066</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#31038;&#21306;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#35782;&#21035;&#33258;&#25105;&#25259;&#38706;&#30340;&#20351;&#29992;&#12289;&#28389;&#29992;&#21644;&#25104;&#30270;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09066
&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#22411;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20801;&#35768;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#33647;&#29289;&#30456;&#20851;&#34892;&#20026;&#65292;&#22312;2500&#20010;&#38463;&#29255;&#31867;&#33647;&#29289;&#24086;&#23376;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26631;&#35760;&#20845;&#31181;&#19981;&#21516;&#38454;&#27573;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29255;&#27573;&#32423;&#25688;&#35201;&#35299;&#37322;&#22312;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#22312;&#30417;&#30563;&#12289;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;&#20960;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#32654;&#22269;&#26377;50&#19975;&#22810;&#20154;&#27515;&#20110;&#28041;&#21450;&#22788;&#26041;&#33647;&#21644;&#38750;&#27861;&#38463;&#29255;&#31867;&#33647;&#29289;&#30340;&#36807;&#37327;&#20351;&#29992;&#65292;&#36896;&#25104;&#20102;&#22269;&#23478;&#20844;&#20849;&#21355;&#29983;&#32039;&#24613;&#24773;&#20917;&#12290; &#21307;&#30103;&#20174;&#19994;&#32773;&#38656;&#35201;&#24378;&#22823;&#19988;&#21450;&#26102;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#22788;&#20110;&#39118;&#38505;&#20043;&#20013;&#30340;&#24739;&#32773;&#12290;&#31038;&#21306;&#22411;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65288;&#22914;Reddit&#65289;&#20801;&#35768;&#29992;&#25143;&#33258;&#34892;&#25259;&#38706;&#65292;&#35752;&#35770;&#19968;&#33324;&#24773;&#20917;&#19979;&#25935;&#24863;&#30340;&#19982;&#33647;&#29289;&#30456;&#20851;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#23376;&#31038;&#21306;&#30340;2500&#20010;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#30456;&#20851;&#30340;&#24086;&#23376;&#65292;&#26631;&#35760;&#26377;&#20845;&#31181;&#19981;&#21516;&#30340;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38454;&#27573;&#65306;&#21307;&#30103;&#20351;&#29992;&#12289;&#28389;&#29992;&#12289;&#25104;&#30270;&#12289;&#24247;&#22797;&#12289;&#22797;&#21457;&#12289;&#19981;&#20351;&#29992;&#12290;&#23545;&#20110;&#27599;&#20010;&#24086;&#23376;&#65292;&#25105;&#20204;&#27880;&#37322;&#20102;&#22522;&#20110;&#29255;&#27573;&#32423;&#30340;&#25688;&#35201;&#35299;&#37322;&#65292;&#24182;&#22312;&#27880;&#37322;&#36136;&#37327;&#21644;&#27169;&#22411;&#24320;&#21457;&#20013;&#37325;&#28857;&#30740;&#31350;&#23427;&#20204;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#30417;&#30563;&#12289;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#20102;&#35782;&#21035;&#21508;&#20010;&#38454;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09066v2 Announce Type: replace  Abstract: In the last decade, the United States has lost more than 500,000 people from an overdose involving prescription and illicit opioids making it a national public health emergency (USDHHS, 2017). Medical practitioners require robust and timely tools that can effectively identify at-risk patients. Community-based social media platforms such as Reddit allow self-disclosure for users to discuss otherwise sensitive drug-related behaviors. We present a moderate size corpus of 2500 opioid-related posts from various subreddits labeled with six different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;LLM&#20013;&#23450;&#20301;&#26041;&#27861;&#23545;&#35760;&#24518;&#25968;&#25454;&#30340;&#23450;&#20301;&#33021;&#21147;&#65292;&#21457;&#29616;&#26469;&#33258;&#32593;&#32476;&#20462;&#21098;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#25152;&#26377;&#35780;&#20272;&#26041;&#27861;&#22343;&#23637;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#23450;&#20301;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09060</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#23450;&#20301;&#26041;&#27861;&#30495;&#30340;&#21487;&#20197;&#23450;&#20301;&#35760;&#24518;&#25968;&#25454;&#21527;&#65311; &#20004;&#20010;&#22522;&#20934;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;LLM&#20013;&#23450;&#20301;&#26041;&#27861;&#23545;&#35760;&#24518;&#25968;&#25454;&#30340;&#23450;&#20301;&#33021;&#21147;&#65292;&#21457;&#29616;&#26469;&#33258;&#32593;&#32476;&#20462;&#21098;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#25152;&#26377;&#35780;&#20272;&#26041;&#27861;&#22343;&#23637;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#20013;&#30340;&#26412;&#22320;&#21270;&#27010;&#24565;&#32463;&#24120;&#20986;&#29616;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65307;&#28982;&#32780;&#65292;&#26412;&#22320;&#21270;&#26041;&#27861;&#20174;&#26410;&#24471;&#21040;&#31995;&#32479;&#21644;&#30452;&#25509;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#22522;&#20934;&#65292;&#35780;&#20272;&#26412;&#22320;&#21270;&#26041;&#27861;&#23450;&#20301;LLM&#32452;&#20214;&#36127;&#36131;&#35760;&#24518;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;INJ&#22522;&#20934;&#20013;&#65292;&#25105;&#20204;&#20027;&#21160;&#23558;&#26032;&#20449;&#24687;&#27880;&#20837;&#21040;LLM&#26435;&#37325;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#20013;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#35780;&#20272;&#26412;&#22320;&#21270;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#35782;&#21035;&#36825;&#20123;&#8220;&#22320;&#38754;&#30495;&#30456;&#8221;&#26435;&#37325;&#12290;&#22312;&#25105;&#20204;&#30340;DEL&#22522;&#20934;&#20013;&#65292;&#36890;&#36807;&#27979;&#37327;&#21024;&#38500;&#24050;&#35782;&#21035;&#31070;&#32463;&#20803;&#20250;&#21024;&#38500;&#24050;&#32463;&#39044;&#35757;&#32451;&#24207;&#21015;&#30340;&#37327;&#65292;&#26469;&#35780;&#20272;&#26412;&#22320;&#21270;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#30340;&#20004;&#20010;&#22522;&#20934;&#20135;&#29983;&#20102;&#20116;&#31181;&#26412;&#22320;&#21270;&#26041;&#27861;&#30340;&#19968;&#33268;&#25490;&#21517;&#12290;&#20174;&#32593;&#32476;&#20462;&#21098;&#20013;&#25913;&#32534;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#25152;&#26377;&#35780;&#20272;&#26041;&#27861;&#22343;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#23450;&#20301;&#33021;&#21147;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21363;&#20351;&#26159;&#25104;&#21151;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09060v2 Announce Type: replace  Abstract: The concept of localization in LLMs is often mentioned in prior work; however, methods for localization have never been systematically and directly evaluated. We propose two complementary benchmarks that evaluate the ability of localization methods to pinpoint LLM components responsible for memorized data. In our INJ benchmark, we actively inject a piece of new information into a small subset of LLM weights, enabling us to directly evaluate whether localization methods can identify these "ground truth" weights. In our DEL benchmark, we evaluate localization by measuring how much dropping out identified neurons deletes a memorized pretrained sequence. Despite their different perspectives, our two benchmarks yield consistent rankings of five localization methods. Methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability. On the other hand, even successful methods
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#32467;&#26500;&#21270;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#35760;&#24518;&#12289;&#25991;&#26412;&#25991;&#26723;&#30340;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#30693;&#35782;&#22270;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#24320;&#25918;&#22495;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.08505</link><description>&lt;p&gt;
&#21322;&#32467;&#26500;&#21270;&#24605;&#32500;&#38142;&#65306;&#25972;&#21512;&#22810;&#28304;&#30693;&#35782;&#20197;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#32467;&#26500;&#21270;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#35760;&#24518;&#12289;&#25991;&#26412;&#25991;&#26723;&#30340;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#30693;&#35782;&#22270;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#24320;&#25918;&#22495;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#25972;&#21512;&#26469;&#33258;&#19977;&#20010;&#26469;&#28304;&#30340;&#30693;&#35782;&#65306;&#27169;&#22411;&#30340;&#21442;&#25968;&#35760;&#24518;&#12289;&#22806;&#37096;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#22806;&#37096;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#32467;&#26500;&#21270;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32541;&#25972;&#21512;&#27169;&#22411;&#30340;&#21442;&#25968;&#35760;&#24518;&#12289;&#25991;&#26412;&#25991;&#26723;&#30340;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#30693;&#35782;&#22270;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;&#24320;&#25918;&#22495;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#31034;&#26041;&#27861;&#26126;&#26174;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#38656;&#35201;&#24494;&#35843;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08505v2 Announce Type: replace  Abstract: An important open question in the use of large language models for knowledge-intensive tasks is how to effectively integrate knowledge from three sources: the model's parametric memory, external structured knowledge, and external unstructured knowledge. Most existing prompting methods either rely on one or two of these sources, or require repeatedly invoking large language models to generate similar or identical content. In this work, we overcome these limitations by introducing a novel semi-structured prompting approach that seamlessly integrates the model's parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs. Experimental results on open-domain multi-hop question answering datasets demonstrate that our prompting method significantly surpasses existing techniques, even exceeding those that require fine-tuning.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;IPAPACK&#35821;&#26009;&#24211;&#21644;&#25552;&#20986;CLAP-IPA&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26410;&#30693;&#35821;&#35328;&#30340;&#24378;&#22823;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;95&#31181;&#26410;&#30693;&#35821;&#35328;&#19978;&#23637;&#31034;&#20102;&#24320;&#25918;&#35789;&#27719;&#21305;&#37197;&#21644;&#24378;&#21046;&#23545;&#40784;&#30340;&#25928;&#26524;</title><link>https://arxiv.org/abs/2311.08323</link><description>&lt;p&gt;
IPA&#30340;&#21619;&#36947;&#65306;&#23454;&#29616;&#20219;&#20309;&#35821;&#35328;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#21644;&#24378;&#21046;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
The taste of IPA: Towards open-vocabulary keyword spotting and forced alignment in any language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08323
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;IPAPACK&#35821;&#26009;&#24211;&#21644;&#25552;&#20986;CLAP-IPA&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26410;&#30693;&#35821;&#35328;&#30340;&#24378;&#22823;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;95&#31181;&#26410;&#30693;&#35821;&#35328;&#19978;&#23637;&#31034;&#20102;&#24320;&#25918;&#35789;&#27719;&#21305;&#37197;&#21644;&#24378;&#21046;&#23545;&#40784;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#35821;&#38899;&#22788;&#29702;&#30340;&#22522;&#20110;&#38899;&#32032;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#26410;&#30693;&#35821;&#35328;&#19978;&#23454;&#29616;&#24378;&#22823;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;IPAPACK&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;115&#22810;&#31181;&#35821;&#35328;&#30340;&#38899;&#32032;&#36716;&#24405;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#30001;&#35821;&#35328;&#23398;&#23478;&#36827;&#34892;&#36873;&#25321;&#24615;&#26816;&#26597;&#12290;&#22522;&#20110;IPAPACK&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLAP-IPA&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#38899;&#32032;&#35821;&#38899;&#23545;&#27604;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#35821;&#38899;&#20449;&#21495;&#21644;&#38899;&#32032;&#24207;&#21015;&#20043;&#38388;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#21305;&#37197;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;95&#31181;&#26410;&#30693;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#26174;&#31034;&#20986;&#36328;&#35821;&#35328;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;&#35757;&#32451;&#20986;&#29616;&#20102;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#21495;&#20043;&#38388;&#30340;&#26102;&#38388;&#23545;&#40784;&#65292;&#20351;&#24471;&#22312;&#26410;&#30693;&#35821;&#35328;&#20013;&#23454;&#29616;&#20102;&#38646;-shot&#24378;&#21046;&#23545;&#40784;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#24378;&#21046;&#23545;&#40784;&#22120;IPA-ALIGNER&#65292;&#36890;&#36807;&#29992;Forward-Sum&#25439;&#22833;&#24494;&#35843;CLAP-IPA&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#38899;&#32032;&#21040;&#38899;&#39057;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08323v2 Announce Type: replace  Abstract: In this project, we demonstrate that phoneme-based models for speech processing can achieve strong crosslinguistic generalizability to unseen languages. We curated the IPAPACK, a massively multilingual speech corpora with phonemic transcriptions, encompassing more than 115 languages from diverse language families, selectively checked by linguists. Based on the IPAPACK, we propose CLAP-IPA, a multi-lingual phoneme-speech contrastive embedding model capable of open-vocabulary matching between arbitrary speech signals and phonemic sequences. The proposed model was tested on 95 unseen languages, showing strong generalizability across languages. Temporal alignments between phonemes and speech signals also emerged from contrastive training, enabling zeroshot forced alignment in unseen languages. We further introduced a neural forced aligner IPA-ALIGNER by finetuning CLAP-IPA with the Forward-Sum loss to learn better phone-to-audio alignmen
&lt;/p&gt;</description></item><item><title>&#28779;&#23665;&#27169;&#22411;&#36890;&#36807;&#33258;&#21453;&#39304;&#24341;&#23548;&#20462;&#35746;&#30340;&#26041;&#24335;&#65292;&#26377;&#25928;&#20943;&#23569;&#22810;&#27169;&#24577;&#24187;&#35273;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#39033;&#35780;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2311.07362</link><description>&lt;p&gt;
&#28779;&#23665;&#65306;&#36890;&#36807;&#33258;&#21453;&#39304;&#24341;&#23548;&#20462;&#35746;&#20943;&#23569;&#22810;&#27169;&#24335;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07362
&lt;/p&gt;
&lt;p&gt;
&#28779;&#23665;&#27169;&#22411;&#36890;&#36807;&#33258;&#21453;&#39304;&#24341;&#23548;&#20462;&#35746;&#30340;&#26041;&#24335;&#65292;&#26377;&#25928;&#20943;&#23569;&#22810;&#27169;&#24577;&#24187;&#35273;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#39033;&#35780;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23384;&#22312;&#22810;&#27169;&#24577;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#25552;&#20379;&#19982;&#32473;&#23450;&#35270;&#35273;&#20449;&#24687;&#19981;&#31526;&#30340;&#38169;&#35823;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25512;&#27979;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#35270;&#35273;&#32534;&#30721;&#22120;&#26410;&#33021;&#27491;&#30830;&#22320;&#19982;&#22270;&#20687;&#23545;&#40784;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21453;&#39304;&#20316;&#20026;&#35270;&#35273;&#32447;&#32034;&#30340;&#26032;&#26041;&#27861;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28779;&#23665;&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21453;&#39304;&#24341;&#23548;&#20462;&#35746;&#27169;&#22411;&#12290;&#28779;&#23665;&#26681;&#25454;&#25552;&#20379;&#30340;&#35270;&#35273;&#20449;&#24687;&#20026;&#20854;&#21021;&#22987;&#21709;&#24212;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65292;&#24182;&#21033;&#29992;&#27492;&#21453;&#39304;&#26469;&#33258;&#25105;&#20462;&#35746;&#20854;&#21021;&#22987;&#21709;&#24212;&#12290;&#28779;&#23665;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#22810;&#27169;&#24577;&#24187;&#35273;&#65292;&#24182;&#22312;MMHal-Bench&#12289;POPE&#21644;GAVIE&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#23427;&#36824;&#25552;&#39640;&#20102;&#19968;&#33324;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#24182;&#22312;MM-Vet&#21644;MMBench&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28779;&#23665;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07362v3 Announce Type: replace  Abstract: Large multimodal models suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination is due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through qualitative analysis, we show that Volcano's fe
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.06899</link><description>&lt;p&gt;
&#28779;&#28976;: &#35780;&#20272;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#22865;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Flames: Benchmarking Value Alignment of Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06899
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#22320;&#21306;&#30340;&#24191;&#27867;&#24212;&#29992;&#24378;&#35843;&#20102;&#35780;&#20272;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#30340;&#36843;&#20999;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#26377;&#25928;&#22320;&#25581;&#31034;LLMs&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#23613;&#31649;&#35768;&#22810;&#27169;&#22411;&#22312;&#36825;&#20123;&#35780;&#20272;&#20013;&#24471;&#20998;&#24456;&#39640;&#65292;&#19988;&#8220;&#21517;&#21015;&#21069;&#33541;&#8221;&#65292;&#20294;&#22312;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#28145;&#23618;&#22865;&#21512;&#24615;&#21644;&#23454;&#29616;&#30495;&#27491;&#26080;&#23475;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#65292;&#20197;&#21450;&#19968;&#20010;&#25972;&#21512;&#20102;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#22914;&#21644;&#35856;&#30340;&#29420;&#29305;&#36947;&#24503;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#21253;&#21547;&#22797;&#26434;&#24773;&#22659;&#21644;&#22823;&#22810;&#24102;&#26377;&#38544;&#21547;&#24694;&#24847;&#30340;&#30772;&#35299;&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#36890;&#36807;&#23545;17&#20010;&#20027;&#27969;LLMs&#36827;&#34892;&#25552;&#31034;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27169;&#22411;&#30340;&#22238;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
&lt;/p&gt;</description></item><item><title>Lewis's Signaling Game&#20316;&#20026;beta-VAE&#65292;&#36890;&#36807;&#37325;&#26032;&#21046;&#23450;&#30446;&#26631;&#20989;&#25968;&#20026;ELBO&#65292;&#38416;&#26126;&#20102; emergent languages&#30340;&#20808;&#39564;&#20998;&#24067;&#23545;&#20110;&#20854;&#32479;&#35745;&#29305;&#24615;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#35789;&#38271;&#21644;&#20998;&#27573;&#30340;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.04453</link><description>&lt;p&gt;
Lewis's Signaling Game&#20316;&#20026;beta-VAE&#29992;&#20110;&#33258;&#28982;&#35789;&#38271;&#21644;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04453
&lt;/p&gt;
&lt;p&gt;
Lewis's Signaling Game&#20316;&#20026;beta-VAE&#65292;&#36890;&#36807;&#37325;&#26032;&#21046;&#23450;&#30446;&#26631;&#20989;&#25968;&#20026;ELBO&#65292;&#38416;&#26126;&#20102; emergent languages&#30340;&#20808;&#39564;&#20998;&#24067;&#23545;&#20110;&#20854;&#32479;&#35745;&#29305;&#24615;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#35789;&#38271;&#21644;&#20998;&#27573;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#36827;&#21270;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#19968;&#20010;&#23376;&#23398;&#31185;&#65292; emergent communication (EC) &#30740;&#31350;&#36890;&#20449;&#21327;&#35758;&#65292;&#21363; emergent languages&#65292;&#22312;&#20195;&#29702;&#36890;&#20449;&#30340;&#27169;&#25311;&#20013;&#20986;&#29616;&#12290; EC&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#26159;&#20135;&#29983;&#19982;&#33258;&#28982;&#35821;&#35328;&#20849;&#20139;&#32479;&#35745;&#29305;&#24615;&#30340;&#35821;&#35328;&#12290; &#26412;&#25991;&#23558; Lewis's signaling game&#65292;EC&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#35774;&#32622;&#65292;&#37325;&#26032;&#35299;&#37322;&#20026; beta-VAE&#65292;&#24182;&#23558;&#20854;&#30446;&#26631;&#20989;&#25968;&#37325;&#26032;&#21046;&#23450;&#20026;ELBO&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#38416;&#26126; emergent languages&#30340;&#20808;&#39564;&#20998;&#24067;&#30340;&#23384;&#22312;&#65292;&#24182;&#23637;&#31034;&#20808;&#39564;&#30340;&#36873;&#25321;&#21487;&#20197;&#24433;&#21709;&#20854;&#32479;&#35745;&#29305;&#24615;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35789;&#38271;&#21644;&#20998;&#27573;&#30340;&#23646;&#24615;&#65292;&#21363;&#32553;&#20889;&#30340; Zipf's law (ZLA) &#21644;&#21704;&#37324;&#26031;&#30340; articulation scheme (HAS)&#12290; &#25454;&#25253;&#36947;&#65292;&#22312;&#20351;&#29992;&#24120;&#35268;&#30446;&#26631;&#26102;&#65292; emergent languages&#24182;&#19981;&#36970;&#36825;&#20123;&#35268;&#24459;&#12290; &#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#36873;&#25321;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04453v2 Announce Type: replace  Abstract: As a sub-discipline of evolutionary and computational linguistics, emergent communication (EC) studies communication protocols, called emergent languages, arising in simulations where agents communicate. A key goal of EC is to give rise to languages that share statistical properties with natural languages. In this paper, we reinterpret Lewis's signaling game, a frequently used setting in EC, as beta-VAE and reformulate its objective function as ELBO. Consequently, we clarify the existence of prior distributions of emergent languages and show that the choice of the priors can influence their statistical properties. Specifically, we address the properties of word lengths and segmentation, known as Zipf's law of abbreviation (ZLA) and Harris's articulation scheme (HAS), respectively. It has been reported that the emergent languages do not follow them when using the conventional objective. We experimentally demonstrate that by selecting 
&lt;/p&gt;</description></item><item><title>&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#27867;&#21270;&#24615;&#21407;&#21017;&#25351;&#23548;&#19979;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20869;&#37096;&#39564;&#35777;&#30340;&#37325;&#35201;&#24615;&#21450;&#24433;&#21709;&#27867;&#21270;&#30340;&#22240;&#32032;&#36827;&#34892;&#30740;&#31350;&#20998;&#26512;</title><link>https://arxiv.org/abs/2311.03663</link><description>&lt;p&gt;
&#20020;&#24202;&#30740;&#31350;&#21407;&#21017;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#27867;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Principles from Clinical Research for NLP Model Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03663
&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#27867;&#21270;&#24615;&#21407;&#21017;&#25351;&#23548;&#19979;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20869;&#37096;&#39564;&#35777;&#30340;&#37325;&#35201;&#24615;&#21450;&#24433;&#21709;&#27867;&#21270;&#30340;&#22240;&#32032;&#36827;&#34892;&#30740;&#31350;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#31038;&#21306;&#36890;&#24120;&#20381;&#36182;&#27169;&#22411;&#22312;&#20445;&#30041;&#27979;&#35797;&#38598;&#19978;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23448;&#26041;&#27979;&#35797;&#38598;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#19979;&#38477;&#36890;&#24120;&#24402;&#22240;&#20110;&#8220;&#38750;&#20998;&#24067;&#8221;&#25928;&#24212;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#27867;&#21270;&#24615;&#30340;&#22522;&#30784;&#24182;&#30740;&#31350;&#20102;&#24433;&#21709;&#23427;&#30340;&#22240;&#32032;&#65292;&#38416;&#36848;&#20102;&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#25945;&#35757;&#12290;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#65292;&#27867;&#21270;&#24615;&#26159;&#19968;&#31181;&#20381;&#36182;&#20110;&#23454;&#39564;&#30340;&#20869;&#37096;&#39564;&#35777;&#26469;&#30830;&#20445;&#23545;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#21463;&#25511;&#27979;&#37327;&#20197;&#21450;&#32467;&#26524;&#23545;&#26356;&#24191;&#27867;&#20154;&#32676;&#30340;&#22806;&#37096;&#39564;&#35777;&#25110;&#21487;&#20256;&#36755;&#24615;&#30340;&#25512;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#27604;&#22914;&#22312;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#20869;&#37096;&#39564;&#35777;&#65292;&#20174;&#32780;&#23545;&#27867;&#21270;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#26500;&#24314;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#38656;&#35201;&#30830;&#20445;&#20869;&#37096;&#39564;&#35777;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03663v3 Announce Type: replace  Abstract: The NLP community typically relies on performance of a model on a held-out test set to assess generalization. Performance drops observed in datasets outside of official test sets are generally attributed to "out-of-distribution" effects. Here, we explore the foundations of generalizability and study the factors that affect it, articulating lessons from clinical studies. In clinical research, generalizability is an act of reasoning that depends on (a) internal validity of experiments to ensure controlled measurement of cause and effect, and (b) external validity or transportability of the results to the wider population. We demonstrate how learning spurious correlations, such as the distance between entities in relation extraction tasks, can affect a model's internal validity and in turn adversely impact generalization. We, therefore, present the need to ensure internal validity when building machine learning models in NLP. Our recomm
&lt;/p&gt;</description></item><item><title>TopicGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#19982;&#20154;&#31867;&#20998;&#31867;&#26356;&#31526;&#21512;&#30340;&#24182;&#19988;&#21487;&#35299;&#37322;&#30340;&#20027;&#39064;&#65292;&#30456;&#27604;&#31454;&#20105;&#26041;&#27861;&#22312;&#35856;&#27874;&#32431;&#24230;&#19978;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2311.01449</link><description>&lt;p&gt;
TopicGPT: &#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TopicGPT: A Prompt-based Topic Modeling Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.01449
&lt;/p&gt;
&lt;p&gt;
TopicGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#19982;&#20154;&#31867;&#20998;&#31867;&#26356;&#31526;&#21512;&#30340;&#24182;&#19988;&#21487;&#35299;&#37322;&#30340;&#20027;&#39064;&#65292;&#30456;&#27604;&#31454;&#20105;&#26041;&#27861;&#22312;&#35856;&#27874;&#32431;&#24230;&#19978;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#20256;&#32479;&#30340;&#20027;&#39064;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;LDA&#65289;&#23558;&#20027;&#39064;&#34920;&#31034;&#20026;&#35789;&#34955;&#65292;&#36890;&#24120;&#38656;&#35201;&#8220;&#38405;&#35835;&#33590;&#21494;&#8221;&#26469;&#35299;&#37322;&#65307;&#21478;&#22806;&#65292;&#23427;&#20204;&#23545;&#20110;&#29992;&#25143;&#25511;&#21046;&#29983;&#25104;&#20027;&#39064;&#30340;&#26684;&#24335;&#21644;&#29305;&#23450;&#24615;&#30340;&#31243;&#24230;&#24456;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TopicGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25581;&#31034;&#25991;&#26412;&#38598;&#21512;&#20013;&#30340;&#28508;&#22312;&#20027;&#39064;&#12290;&#19982;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#65292;TopicGPT&#20135;&#29983;&#30340;&#20027;&#39064;&#26356;&#31526;&#21512;&#20154;&#31867;&#20998;&#31867;&#65306;&#19982;&#26368;&#24378;&#22522;&#32447;&#30340;0.64&#30456;&#27604;&#65292;&#23427;&#22312;&#19982;&#20154;&#24037;&#26631;&#35760;&#30340;&#32500;&#22522;&#30334;&#31185;&#20027;&#39064;&#27604;&#23545;&#20013;&#36798;&#21040;&#20102;0.74&#30340;&#35856;&#27874;&#32431;&#24230;&#12290;&#23427;&#30340;&#20027;&#39064;&#20063;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#21462;&#20195;&#20102;&#21547;&#31946;&#30340;&#35789;&#34955;&#65292;&#36716;&#32780;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26631;&#31614;&#21644;&#30456;&#20851;&#30340;&#33258;&#30001;&#24418;&#24335;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#38750;&#24120;&#36866;&#24212;&#65292;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#32422;&#26463;&#21644;&#25511;&#21046;&#20027;&#39064;&#30340;&#29305;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.01449v2 Announce Type: replace  Abstract: Topic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require "reading the tea leaves" to interpret; additionally, they offer users minimal control over the formatting and specificity of resulting topics. To tackle these issues, we introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics in a text collection. TopicGPT produces topics that align better with human categorizations compared to competing methods: it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline. Its topics are also interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions. Moreover, the framework is highly adaptable, allowing users to specify constraints and m
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#30446;&#26631;&#23884;&#20837;&#22312;&#36830;&#32493;&#36755;&#20986;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#38750;&#29702;&#24615;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#22312;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#24182;&#19988;&#23545;&#32597;&#35265;&#35789;&#25928;&#26524;&#26368;&#20026;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2310.20620</link><description>&lt;p&gt;
&#38543;&#26426;&#30446;&#26631;&#23884;&#20837;&#23545;&#36830;&#32493;&#36755;&#20986;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#38750;&#29702;&#24615;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20620
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#30446;&#26631;&#23884;&#20837;&#22312;&#36830;&#32493;&#36755;&#20986;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#38750;&#29702;&#24615;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#22312;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#24182;&#19988;&#23545;&#32597;&#35265;&#35789;&#25928;&#26524;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#36755;&#20986;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;CoNMT&#65289;&#23558;&#31163;&#25955;&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#38382;&#39064;&#26367;&#25442;&#20026;&#23884;&#20837;&#39044;&#27979;&#12290;&#30446;&#26631;&#23884;&#20837;&#31354;&#38388;&#30340;&#35821;&#20041;&#32467;&#26500;&#65288;&#21363;&#30456;&#20851;&#35789;&#20043;&#38388;&#30340;&#25509;&#36817;&#31243;&#24230;&#65289;&#22312;&#30452;&#35273;&#19978;&#34987;&#35748;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25361;&#25112;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#24182;&#23637;&#31034;&#23436;&#20840;&#38543;&#26426;&#30340;&#36755;&#20986;&#23884;&#20837;&#21487;&#20197;&#32988;&#36807;&#36153;&#21147;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#19968;&#20196;&#20154;&#24778;&#35766;&#30340;&#25928;&#26524;&#23545;&#20110;&#32597;&#35265;&#35789;&#26368;&#20026;&#26174;&#33879;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#23884;&#20837;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#65292;&#32467;&#21512;&#19981;&#21516;&#26631;&#35760;&#30340;&#38543;&#26426;&#21644;&#39044;&#35757;&#32451;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.20620v2 Announce Type: replace  Abstract: Continuous-output neural machine translation (CoNMT) replaces the discrete next-word prediction problem with an embedding prediction. The semantic structure of the target embedding space (i.e., closeness of related words) is intuitively believed to be crucial. We challenge this assumption and show that completely random output embeddings can outperform laboriously pretrained ones, especially on larger datasets. Further investigation shows this surprising effect is strongest for rare words, due to the geometry of their embeddings. We shed further light on this finding by designing a mixed strategy that combines random and pre-trained embeddings for different tokens.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#20449;&#24687;&#20316;&#20026;&#39044;&#38450;&#24615;&#28548;&#28165;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20943;&#23569;&#19981;&#20805;&#20998;&#24615;&#65292;&#24182;&#31616;&#21270;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2310.05861</link><description>&lt;p&gt;
&#37325;&#26032;&#34920;&#36848;&#12289;&#22686;&#24378;&#12289;&#25512;&#29702;&#65306;&#35270;&#35273;&#38382;&#39064;&#30340;&#35270;&#35273;&#22522;&#30784;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05861
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#20449;&#24687;&#20316;&#20026;&#39044;&#38450;&#24615;&#28548;&#28165;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20943;&#23569;&#19981;&#20805;&#20998;&#24615;&#65292;&#24182;&#31616;&#21270;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#21487;&#20197;&#22312;&#38646;&#33267;&#23569;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#65292;&#21363;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#12290;&#23613;&#31649;&#36825;&#20855;&#26377;&#24040;&#22823;&#30340;&#20248;&#21183;&#65292;&#27604;&#22914;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#25110;&#33258;&#23450;&#20041;&#26550;&#26500;&#65292;&#20294;&#22914;&#20309;&#23558;&#36755;&#20837;&#21576;&#29616;&#32473;LVLM&#20250;&#23545;&#38646;&#21442;&#32771;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#20197;&#19981;&#20805;&#20998;&#26041;&#24335;&#34920;&#36798;&#30340;&#36755;&#20837;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#31572;&#26696;&#65292;&#21407;&#22240;&#21253;&#25324;&#32570;&#22833;&#35270;&#35273;&#20449;&#24687;&#12289;&#22797;&#26434;&#30340;&#38544;&#21547;&#25512;&#29702;&#25110;&#35821;&#35328;&#27495;&#20041;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#20449;&#24687;&#20316;&#20026;&#39044;&#38450;&#24615;&#28548;&#28165;&#65292;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#20943;&#23569;&#19981;&#20805;&#20998;&#24615;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20363;&#22914;&#36890;&#36807;&#23450;&#20301;&#23545;&#35937;&#21644;&#28040;&#38500;&#24341;&#29992;&#27495;&#20041;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;VQA&#35774;&#32622;&#20013;&#65292;&#25913;&#21464;&#38382;&#39064;&#30340;&#26500;&#24605;&#26041;&#24335;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05861v2 Announce Type: replace-cross  Abstract: An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To th
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2310.05746</link><description>&lt;p&gt;
&#35753;&#34892;&#21160;&#32988;&#20110;&#38596;&#36777;&#65306;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#20013;&#30340;&#25112;&#30053;&#35268;&#21010;&#19982;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05746
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28982;&#32780;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#22522;&#20934;&#12290;&#35780;&#20272;&#36825;&#19968;&#28857;&#38656;&#35201;&#27979;&#35797;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#30340;&#29615;&#22659;&#65292;&#36825;&#31181;&#29615;&#22659;&#38656;&#35201;&#22312;&#21160;&#24577;&#30340;&#31454;&#20105;&#22330;&#26223;&#20013;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AucArena&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25293;&#21334;&#30340;&#26032;&#39062;&#35780;&#20272;&#22871;&#20214;&#65292;&#36873;&#25321;&#36825;&#20010;&#35774;&#32622;&#26159;&#22240;&#20026;&#23427;&#38750;&#24120;&#19981;&#21487;&#39044;&#27979;&#65292;&#28041;&#21450;&#19982;&#36164;&#28304;&#21644;&#39118;&#38505;&#31649;&#29702;&#30456;&#20851;&#30340;&#35768;&#22810;&#25216;&#33021;&#65292;&#21516;&#26102;&#20063;&#26131;&#20110;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLM&#39537;&#21160;&#31454;&#26631;&#20195;&#29702;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#20182;&#20204;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;LLM&#20855;&#26377;&#25293;&#21334;&#21442;&#19982;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#22914;&#39044;&#31639;&#31649;&#29702;&#21644;&#30446;&#26631;&#36981;&#20174;&#65292;&#36825;&#20123;&#25216;&#33021;&#20250;&#38543;&#30528;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#25913;&#36827;&#32780;&#25552;&#39640;&#12290;&#36825;&#31361;&#20986;&#20102;LLM&#22312;&#24314;&#27169;&#31454;&#25216;&#32972;&#26223;&#19979;&#30340;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05746v2 Announce Type: replace-cross  Abstract: Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25237;&#24433;&#26041;&#27861;CLaP&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#32763;&#35793;&#26631;&#31614;&#65292;&#30830;&#20445;&#26356;&#20934;&#30830;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#36328;&#35821;&#35328;&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2309.08943</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#32467;&#26500;&#25552;&#21462;&#30340;&#24773;&#22659;&#26631;&#31614;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Contextual Label Projection for Cross-Lingual Structure Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25237;&#24433;&#26041;&#27861;CLaP&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#32763;&#35793;&#26631;&#31614;&#65292;&#30830;&#20445;&#26356;&#20934;&#30830;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#36328;&#35821;&#35328;&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#25237;&#24433;&#28041;&#21450;&#21516;&#26102;&#33719;&#21462;&#32763;&#35793;&#26631;&#31614;&#21644;&#25991;&#26412;&#65292;&#23545;&#20110;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#20419;&#36827;&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#26631;&#31614;&#25237;&#24433;&#26102;&#36890;&#24120;&#36890;&#36807;&#20559;&#29233;&#31616;&#21270;&#26631;&#31614;&#32763;&#35793;&#25110;&#20165;&#20381;&#36182;&#20110;&#21333;&#35789;&#32423;&#21035;&#23545;&#40784;&#26469;&#29306;&#29298;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#31614;&#25237;&#24433;&#26041;&#27861;CLaP&#65292;&#23558;&#25991;&#26412;&#32763;&#35793;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#24182;&#21033;&#29992;&#24050;&#32763;&#35793;&#25991;&#26412;&#20316;&#20026;&#19978;&#19979;&#25991;&#23545;&#26631;&#31614;&#36827;&#34892;&#24773;&#22659;&#32763;&#35793;&#65292;&#30830;&#20445;&#32763;&#35793;&#26631;&#31614;&#30340;&#26356;&#22909;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#22810;&#35821;&#35328;&#33021;&#21147;&#30340;&#25351;&#23548;&#35843;&#26657;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25105;&#20204;&#30340;&#24773;&#22659;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#25351;&#23548;&#22312;&#32763;&#35793;&#25991;&#26412;&#20013;&#23384;&#22312;&#32763;&#35793;&#26631;&#31614;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#22312;39&#31181;&#35821;&#35328;&#19978;&#36890;&#36807;&#38646;-shot&#36328;&#35821;&#35328;&#36716;&#31227;&#23545;CLaP&#19982;&#20854;&#20182;&#26631;&#31614;&#25237;&#24433;&#25216;&#26415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.08943v2 Announce Type: replace  Abstract: Label projection, which involves obtaining translated labels and texts jointly, is essential for leveraging machine translation to facilitate cross-lingual transfer in structured prediction tasks. Prior research exploring label projection often compromise translation accuracy by favoring simplified label translation or relying solely on word-level alignments. In this paper, we introduce a novel label projection approach, CLaP, which translates text to the target language and performs contextual translation on the labels using the translated text as the context, ensuring better accuracy for the translated labels. We leverage instruction-tuned language models with multilingual capabilities as our contextual translator, imposing the constraint of the presence of translated labels in the translated text via instructions. We benchmark CLaP with other label projection techniques on zero-shot cross-lingual transfer across 39 languages on tw
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;ValuePrism&#65292;&#19968;&#20010;&#21253;&#21547;218k&#20010;&#20215;&#20540;&#35266;&#12289;&#26435;&#21033;&#21644;&#20041;&#21153;&#65292;&#24182;&#19982;31k&#20154;&#31867;&#20070;&#20889;&#24773;&#22659;&#30456;&#32852;&#31995;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2309.00779</link><description>&lt;p&gt;
&#20215;&#20540;&#19975;&#33457;&#31570;&#65306;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#22810;&#20803;&#21270;&#20154;&#31867;&#20215;&#20540;&#35266;&#12289;&#26435;&#21033;&#21644;&#20041;&#21153;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00779
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;ValuePrism&#65292;&#19968;&#20010;&#21253;&#21547;218k&#20010;&#20215;&#20540;&#35266;&#12289;&#26435;&#21033;&#21644;&#20041;&#21153;&#65292;&#24182;&#19982;31k&#20154;&#31867;&#20070;&#20889;&#24773;&#22659;&#30456;&#32852;&#31995;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#20110;&#20154;&#31867;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20215;&#20540;&#35266;&#22810;&#20803;&#20027;&#20041;&#35748;&#20026;&#65292;&#21487;&#20197;&#21516;&#26102;&#23384;&#22312;&#22810;&#31181;&#27491;&#30830;&#30340;&#20215;&#20540;&#35266;&#65288;&#20363;&#22914;&#65292;&#22312;&#32771;&#34385;&#26159;&#21542;&#23545;&#26379;&#21451;&#25746;&#35854;&#20197;&#20445;&#25252;&#20182;&#20204;&#30340;&#24863;&#21463;&#26102;&#65292;&#22914;&#20309;&#24179;&#34913;&#35802;&#23454;&#21644;&#21451;&#35850;&#65311;&#65289;&#12290;&#20316;&#20026;&#32479;&#35745;&#23398;&#20064;&#32773;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#40664;&#35748;&#36866;&#24212;&#24179;&#22343;&#20540;&#65292;&#24573;&#30053;&#20102;&#36825;&#20123;&#28508;&#22312;&#30340;&#19981;&#21487;&#31616;&#21270;&#30340;&#20215;&#20540;&#20914;&#31361;&#12290;&#20026;&#20102;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#20215;&#20540;&#22810;&#20803;&#20027;&#20041;&#65292;&#39318;&#35201;&#25361;&#25112;&#26159;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#27169;&#25311;&#22810;&#20803;&#21270;&#20154;&#31867;&#20215;&#20540;&#35266;&#12289;&#26435;&#21033;&#21644;&#20041;&#21153;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#20114;&#21160;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00779v2 Announce Type: replace-cross  Abstract: Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction.   We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgroun
&lt;/p&gt;</description></item><item><title>PANDA&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;PoT&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26377;&#25928;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2208.10160</link><description>&lt;p&gt;
PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation
&lt;/p&gt;
&lt;p&gt;
PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.10160
&lt;/p&gt;
&lt;p&gt;
PANDA&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;PoT&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26377;&#25928;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#36716;&#31227;&#65288;PoT&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25552;&#31034;&#24494;&#35843;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#25552;&#31034;&#21021;&#22987;&#21270;&#20026;&#22312;&#31867;&#20284;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#29616;&#26377;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26222;&#36890;&#30340;PoT&#26041;&#27861;&#36890;&#24120;&#20250;&#36798;&#21040;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#65288;i&#65289;PoT&#23545;&#28304;-&#30446;&#26631;&#23545;&#30340;&#30456;&#20284;&#24615;&#25935;&#24863;&#65292;&#24182;&#19988;&#65288;ii&#65289;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30452;&#25509;&#24494;&#35843;&#20351;&#29992;&#28304;&#25552;&#31034;&#21021;&#22987;&#21270;&#30340;&#25552;&#31034;&#21487;&#33021;&#23548;&#33268;&#36951;&#24536;&#20174;&#28304;&#20219;&#21153;&#23398;&#21040;&#30340;&#26377;&#29992;&#30340;&#36890;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#20934;&#30830;&#39044;&#27979;&#25552;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#65288;&#20851;&#20110;&#65288;i&#65289;&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26377;&#25928;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#30340;&#26032;&#39062;PoT&#26041;&#27861;&#65288;&#21363;PANDA&#65289;&#65288;&#20851;&#20110;&#65288;ii&#65289;&#65289;&#12290;&#23545;&#20110;21&#20010;&#28304;&#21644;9&#20010;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;189&#31181;&#32452;&#21512;&#65292;&#22312;5&#20010;&#35268;&#27169;&#30340;PLM&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#31995;&#32479;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#33021;&#22815;&#24456;&#22909;&#22320;&#39044;&#27979;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.10160v2 Announce Type: replace  Abstract: Prompt Transfer (PoT) is a recently-proposed approach to improve prompt-tuning, by initializing the target prompt with the existing prompt trained on similar source tasks. However, such a vanilla PoT approach usually achieves sub-optimal performance, as (i) the PoT is sensitive to the similarity of source-target pair and (ii) directly fine-tuning the prompt initialized with source prompt on target task might lead to forgetting of the useful general knowledge learned from source task. To tackle these issues, we propose a new metric to accurately predict the prompt transferability (regarding (i)), and a novel PoT approach (namely PANDA) that leverages the knowledge distillation technique to alleviate the knowledge forgetting effectively (regarding (ii)). Extensive and systematic experiments on 189 combinations of 21 source and 9 target datasets across 5 scales of PLMs demonstrate that: 1) our proposed metric works well to predict the p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#21487;&#27604;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#35789;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#33258;&#21160;&#26415;&#35821;&#35782;&#21035;&#26694;&#26550;&#65292;&#23558;&#33521;&#35821;&#28040;&#36153;&#32773;&#20581;&#24247;&#35789;&#27719;&#25193;&#23637;&#20026;&#36328;&#35821;&#35328;&#35789;&#27719;&#12290;</title><link>https://arxiv.org/abs/2206.11612</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#21487;&#27604;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#35789;&#23884;&#20837;&#26500;&#24314;&#36328;&#35821;&#35328;&#28040;&#36153;&#32773;&#20581;&#24247;&#35789;&#27719;
&lt;/p&gt;
&lt;p&gt;
Constructing Cross-lingual Consumer Health Vocabulary with Word-Embedding from Comparable User Generated Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.11612
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#21487;&#27604;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#35789;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#33258;&#21160;&#26415;&#35821;&#35782;&#21035;&#26694;&#26550;&#65292;&#23558;&#33521;&#35821;&#28040;&#36153;&#32773;&#20581;&#24247;&#35789;&#27719;&#25193;&#23637;&#20026;&#36328;&#35821;&#35328;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#65288;OHC&#65289;&#26159;&#26222;&#36890;&#20154;&#20998;&#20139;&#20581;&#24247;&#20449;&#24687;&#30340;&#20027;&#35201;&#28192;&#36947;&#12290;&#20026;&#20102;&#20998;&#26512;OHC&#20013;&#28040;&#36153;&#32773;&#29983;&#25104;&#20869;&#23481;&#65288;HCGC&#65289;&#65292;&#35782;&#21035;&#26222;&#36890;&#20154;&#20351;&#29992;&#30340;&#21475;&#22836;&#21307;&#23398;&#34920;&#36798;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#24320;&#25918;&#33719;&#21462;&#21644;&#21327;&#20316;&#30340;&#28040;&#36153;&#32773;&#20581;&#24247;&#35789;&#27719;&#65288;OAC CHV&#65289;&#26159;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#21463;&#25511;&#35789;&#27719;&#12290;&#28982;&#32780;&#65292;OAC CHV&#20165;&#22312;&#33521;&#35821;&#20013;&#21487;&#29992;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#33258;&#21160;&#26415;&#35821;&#35782;&#21035;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#33521;&#35821;CHV&#25193;&#23637;&#20026;&#36328;&#35821;&#35328;CHV&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38656;&#35201;&#19968;&#20010;&#33521;&#35821;HCGC&#35821;&#26009;&#24211;&#21644;&#19968;&#20010;&#38750;&#33521;&#35821;&#65288;&#26412;&#30740;&#31350;&#20013;&#20026;&#20013;&#25991;&#65289;HCGC&#35821;&#26009;&#24211;&#20316;&#20026;&#36755;&#20837;&#12290;&#20351;&#29992;skip-gram&#31639;&#27861;&#30830;&#23450;&#20102;&#20004;&#20010;&#21333;&#35821;&#35789;&#21521;&#37327;&#31354;&#38388;&#65292;&#20351;&#24471;&#27599;&#20010;&#31354;&#38388;&#32534;&#30721;&#20102;&#35821;&#35328;&#20869;&#26222;&#36890;&#20154;&#20043;&#38388;&#30340;&#24120;&#35265;&#35789;&#20851;&#32852;&#12290;&#26681;&#25454;&#31561;&#36317;&#20551;&#35774;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#26144;&#23556;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.11612v2 Announce Type: replace  Abstract: The online health community (OHC) is the primary channel for laypeople to share health information. To analyze the health consumer-generated content (HCGC) from the OHCs, identifying the colloquial medical expressions used by laypeople is a critical challenge. The open-access and collaborative consumer health vocabulary (OAC CHV) is the controlled vocabulary for addressing such a challenge. Nevertheless, OAC CHV is only available in English, limiting its applicability to other languages. This research proposes a cross-lingual automatic term recognition framework for extending the English CHV into a cross-lingual one. Our framework requires an English HCGC corpus and a non-English (i.e., Chinese in this study) HCGC corpus as inputs. Two monolingual word vector spaces are determined using the skip-gram algorithm so that each space encodes common word associations from laypeople within a language. Based on the isometry assumption, the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MM-&#20132;&#38169;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#21644;&#22810;&#22270;&#20687;&#29305;&#24449;&#21516;&#27493;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#25429;&#25417;&#22270;&#20687;&#32454;&#33410;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10208</link><description>&lt;p&gt;
MM-&#20132;&#38169;&#30340;&#65306;&#36890;&#36807;&#22810;&#27169;&#24335;&#29305;&#24449;&#21516;&#27493;&#22120;&#36827;&#34892;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer. (arXiv:2401.10208v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MM-&#20132;&#38169;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#21644;&#22810;&#22270;&#20687;&#29305;&#24449;&#21516;&#27493;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#25429;&#25417;&#22270;&#20687;&#32454;&#33410;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24320;&#21457;&#20855;&#26377;&#30740;&#31350;&#21644;&#23454;&#38469;&#20215;&#20540;&#12290;&#23427;&#35201;&#27714;&#27169;&#22411;&#29702;&#35299;&#20132;&#38169;&#30340;&#24207;&#21015;&#65292;&#24182;&#38543;&#21518;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21463;&#21040;&#20102;&#22266;&#23450;&#25968;&#37327;&#30340;&#35270;&#35273;&#26631;&#35760;&#19981;&#33021;&#26377;&#25928;&#25429;&#25417;&#22270;&#20687;&#32454;&#33410;&#30340;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#22312;&#22810;&#22270;&#20687;&#22330;&#26223;&#20013;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MM-&#20132;&#38169;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#21644;&#22810;&#22270;&#20687;&#29305;&#24449;&#21516;&#27493;&#22120;&#27169;&#22359;&#65292;&#20801;&#35768;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30452;&#25509;&#35775;&#38382;&#20808;&#21069;&#19978;&#19979;&#25991;&#20013;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#29305;&#24449;&#12290;MM-&#20132;&#38169;&#22312;&#37197;&#23545;&#21644;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#36890;&#36807;&#19968;&#38454;&#27573;&#30340;&#30417;&#30563;&#24494;&#35843;&#26469;&#36827;&#19968;&#27493;&#25913;&#21892;&#20854;&#36981;&#24490;&#22797;&#26434;&#22810;&#27169;&#24577;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;MM-&#20132;&#38169;&#22312;&#22270;&#20687;&#20462;&#22797;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in rec
&lt;/p&gt;</description></item><item><title>CASA&#26159;&#19968;&#20010;&#22240;&#26524;&#39537;&#21160;&#30340;&#35770;&#35777;&#20805;&#20998;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#21069;&#25552;&#21644;&#32467;&#35770;&#19981;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#27880;&#20837;&#21069;&#25552;&#20107;&#20214;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#19981;&#36275;&#30340;&#35770;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.05249</link><description>&lt;p&gt;
CASA: &#22240;&#26524;&#39537;&#21160;&#30340;&#35770;&#35777;&#20805;&#20998;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CASA: Causality-driven Argument Sufficiency Assessment. (arXiv:2401.05249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05249
&lt;/p&gt;
&lt;p&gt;
CASA&#26159;&#19968;&#20010;&#22240;&#26524;&#39537;&#21160;&#30340;&#35770;&#35777;&#20805;&#20998;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#21069;&#25552;&#21644;&#32467;&#35770;&#19981;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#27880;&#20837;&#21069;&#25552;&#20107;&#20214;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#19981;&#36275;&#30340;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#20805;&#20998;&#24615;&#35780;&#20272;&#20219;&#21153;&#26088;&#22312;&#30830;&#23450;&#19968;&#20010;&#32473;&#23450;&#35770;&#35777;&#30340;&#21069;&#25552;&#26159;&#21542;&#25903;&#25345;&#20854;&#32467;&#35770;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23545;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#22120;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#25968;&#25454;&#26159;&#36153;&#21147;&#30340;&#65292;&#32780;&#19988;&#30001;&#20110;&#20027;&#35266;&#26631;&#20934;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#26631;&#27880;&#24448;&#24448;&#20063;&#19981;&#19968;&#33268;&#12290;&#21463;&#22240;&#26524;&#25991;&#29486;&#20013;&#30340;&#20805;&#20998;&#27010;&#29575;&#65288;PS&#65289;&#23450;&#20041;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CASA&#65292;&#19968;&#20010;&#38646;&#23556;&#22240;&#26524;&#39537;&#21160;&#30340;&#35770;&#35777;&#20805;&#20998;&#24615;&#35780;&#20272;&#26694;&#26550;&#12290;PS&#34913;&#37327;&#30340;&#26159;&#24403;&#21069;&#25552;&#20107;&#20214;&#21644;&#32467;&#35770;&#20107;&#20214;&#37117;&#19981;&#23384;&#22312;&#26102;&#65292;&#24341;&#20837;&#21069;&#25552;&#20107;&#20214;&#26159;&#21542;&#20250;&#23548;&#33268;&#32467;&#35770;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#20272;&#35745;&#36825;&#20010;&#27010;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19982;&#21069;&#25552;&#21644;&#32467;&#35770;&#19981;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#27880;&#20837;&#21069;&#25552;&#20107;&#20214;&#23545;&#23427;&#20204;&#36827;&#34892;&#20462;&#25913;&#12290;&#22312;&#20004;&#20010;&#36923;&#36753;&#35884;&#35823;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CASA&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#19981;&#36275;&#30340;&#35770;&#35777;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;CASA&#37096;&#32626;&#22312;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion. To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the probability of sufficiency (PS) definition in the causal literature, we propose CASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion, when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion, and revise them by injecting the premise event. Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#25506;&#31350;&#25913;&#21464;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21363;&#20351;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#31572;&#26696;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#35831;&#27714;&#20197;XML&#26684;&#24335;&#36820;&#22238;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.03729</link><description>&lt;p&gt;
&#25913;&#21464;&#25552;&#31034;&#30340;&#34676;&#34678;&#25928;&#24212;&#65306;&#24494;&#23567;&#30340;&#21464;&#21270;&#21644;&#36234;&#29425;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance. (arXiv:2401.03729v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#25506;&#31350;&#25913;&#21464;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21363;&#20351;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#31572;&#26696;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#35831;&#27714;&#20197;XML&#26684;&#24335;&#36820;&#22238;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23545;&#22810;&#20010;&#39046;&#22495;&#21644;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#21521;LLM&#25552;&#38382;&#25110;&#8220;&#25552;&#31034;&#8221;&#65292;&#23454;&#36341;&#32773;&#33021;&#22815;&#24555;&#36895;&#33719;&#24471;&#20219;&#24847;&#20219;&#21153;&#30340;&#21709;&#24212;&#12290;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#26159;&#21542;&#21464;&#21270;&#20250;&#24433;&#21709;LLM&#30340;&#26368;&#32456;&#20915;&#31574;&#65311;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#26368;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#30340;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;LLM&#25913;&#21464;&#20854;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;XML&#20013;&#35831;&#27714;&#21709;&#24212;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#21487;&#33021;&#23545;&#30001;LLMs&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are regularly being used to label data across many domains and for myriad tasks. By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task. This prompting is done through a series of decisions by the practitioner, from simple wording of the prompt, to requesting the output in a certain data format, to jailbreaking in the case of prompts that address more sensitive topics. In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM? We answer this using a series of prompt variations across a variety of text classification tasks. We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.15548</link><description>&lt;p&gt;
YAYI-UIE: &#19968;&#20010;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction. (arXiv:2312.15548v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38590;&#28857;&#22312;&#20110;&#22788;&#29702;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#27169;&#24335;&#21644;&#24322;&#26500;&#25968;&#25454;&#32467;&#26500;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#32479;&#19968;&#24314;&#27169;&#19981;&#21516;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#22312;&#38500;&#20102;&#33521;&#35821;&#20197;&#22806;&#30340;&#20013;&#25991;&#35821;&#35328;&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#25903;&#25345;&#20013;&#25991;&#21644;&#33521;&#25991;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26377;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of the information extraction task lies in dealing with the task-specific label schemas and heterogeneous data structures. Recent work has proposed methods based on large language models to uniformly model different information extraction tasks. However, these existing methods are deficient in their information extraction capabilities for Chinese languages other than English. In this paper, we propose an end-to-end chat-enhanced instruction tuning framework for universal information extraction (YAYI-UIE), which supports both Chinese and English. Specifically, we utilize dialogue data and information extraction data to enhance the information extraction performance jointly. Experimental results show that our proposed framework achieves state-of-the-art performance on Chinese datasets while also achieving comparable performance on English datasets under both supervised settings and zero-shot settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.11482</link><description>&lt;p&gt;
AGI&#31995;&#32479;&#30340;&#20803;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#25552;&#31034;(meta prompting)&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#37325;&#26032;&#22609;&#36896;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#21033;&#29992;&#12290;&#22522;&#20110;&#31867;&#22411;&#29702;&#35770;&#21644;&#33539;&#30068;&#35770;&#65292;&#20803;&#25552;&#31034;&#27880;&#37325;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#20197;&#20869;&#23481;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20803;&#25552;&#31034;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#19982;&#23569;&#26679;&#26412;&#25552;&#31034;(few-shot prompting)&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#20851;&#27880;&#23558;&#20803;&#25552;&#31034;&#25193;&#23637;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#23637;&#31034;&#22914;&#20309;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#20998;&#25104;&#36739;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20196;&#29260;&#25928;&#29575;&#65292;&#24182;&#20351;&#38382;&#39064;&#27714;&#35299;&#30340;&#27604;&#36739;&#26356;&#21152;&#20844;&#24179;&#65292;&#23588;&#20854;&#26159;&#19982;&#23569;&#26679;&#26412;&#31034;&#20363;&#26041;&#27861;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#25552;&#31034;&#20219;&#21153;&#65292;&#20801;&#35768;LLMs&#20197;&#36845;&#20195;&#30340;&#20803;&#32534;&#31243;&#24418;&#24335;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#21457;&#29616;&#23427;&#20204;&#26174;&#31034;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#30340;&#27169;&#24335;&#65292;&#36827;&#19968;&#27493;&#35777;&#23454;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#36328;&#27169;&#24577;&#20851;&#32852;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.16781</link><description>&lt;p&gt;
Kiki&#36824;&#26159;Bouba&#65311;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#35937;&#24449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Kiki or Bouba? Sound Symbolism in Vision-and-Language Models. (arXiv:2310.16781v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#21457;&#29616;&#23427;&#20204;&#26174;&#31034;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#30340;&#27169;&#24335;&#65292;&#36827;&#19968;&#27493;&#35777;&#23454;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#36328;&#27169;&#24577;&#20851;&#32852;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#31867;&#35821;&#35328;&#20013;&#30340;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#26144;&#23556;&#34987;&#35748;&#20026;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38543;&#26426;&#30340;&#65292;&#20294;&#35748;&#30693;&#31185;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#21644;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#29305;&#23450;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#23384;&#22312;&#38750;&#24179;&#20961;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#22768;&#38899;&#35937;&#24449;&#24615;&#12290;&#22312;&#35768;&#22810;&#24847;&#20041;&#32500;&#24230;&#20013;&#65292;&#22768;&#38899;&#35937;&#24449;&#24615;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#20851;&#32852;&#26041;&#38754;&#23588;&#20026;&#26174;&#33879;&#21644;&#20805;&#20998;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#26159;&#21542;&#22312;CLIP&#21644;Stable Diffusion&#31561;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20307;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#30693;&#35782;&#25506;&#27979;&#26469;&#35843;&#26597;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#25105;&#20204;&#21457;&#29616;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#30830;&#23454;&#26174;&#31034;&#20102;&#36825;&#31181;&#27169;&#24335;&#65292;&#19982;&#24515;&#29702;&#35821;&#35328;&#23398;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;kiki-bouba&#25928;&#24212;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#35745;&#31639;&#24037;&#20855;&#26469;&#23637;&#31034;&#22768;&#38899;&#35937;&#24449;&#24615;&#24182;&#29702;&#35299;&#20854;&#26412;&#36136;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25628;&#32034;&#24341;&#25806;&#22312;&#24110;&#21161;&#29992;&#25143;&#20107;&#23454;&#26680;&#26597;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29992;&#25143;&#38405;&#35835;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#27604;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#26356;&#39640;&#25928;&#65292;&#20294;&#24403;&#35299;&#37322;&#38169;&#35823;&#26102;&#65292;&#29992;&#25143;&#23481;&#26131;&#36807;&#24230;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#20943;&#23569;&#36807;&#24230;&#20381;&#36182;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#20449;&#24687;&#36827;&#34892;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12558</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#20154;&#31867;&#39564;&#35777;&#30495;&#23454;&#24615;&#8212;&#8212;&#38500;&#38750;&#23427;&#20204;&#20196;&#20154;&#20449;&#26381;&#22320;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong. (arXiv:2310.12558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25628;&#32034;&#24341;&#25806;&#22312;&#24110;&#21161;&#29992;&#25143;&#20107;&#23454;&#26680;&#26597;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29992;&#25143;&#38405;&#35835;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#27604;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#26356;&#39640;&#25928;&#65292;&#20294;&#24403;&#35299;&#37322;&#38169;&#35823;&#26102;&#65292;&#29992;&#25143;&#23481;&#26131;&#36807;&#24230;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#20943;&#23569;&#36807;&#24230;&#20381;&#36182;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#20449;&#24687;&#36827;&#34892;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#33719;&#21462;&#32593;&#32476;&#19978;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#30495;&#23454;&#24615;&#21644;&#20107;&#23454;&#24615;&#22791;&#21463;&#20851;&#27880;&#12290;&#20026;&#20102;&#24110;&#21161;&#29992;&#25143;&#20570;&#20986;&#27491;&#30830;&#30340;&#20449;&#24687;&#20915;&#31574;&#65292;LLMs&#19981;&#20165;&#24212;&#25552;&#20379;&#20449;&#24687;&#65292;&#36824;&#24212;&#24110;&#21161;&#29992;&#25143;&#20107;&#23454;&#26680;&#26597;&#12290;&#26412;&#25991;&#36890;&#36807;&#19982;80&#21517;&#20247;&#21253;&#24037;&#20316;&#32773;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25628;&#32034;&#24341;&#25806;&#65288;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65289;&#22312;&#24110;&#21161;&#20154;&#31867;&#29992;&#25143;&#20107;&#23454;&#26680;&#26597;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#24341;&#23548;LLMs&#39564;&#35777;&#32473;&#23450;&#30340;&#22768;&#26126;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#37322;&#12290;&#19982;&#20351;&#29992;&#20934;&#30830;&#29575;&#30456;&#20284;&#30340;&#25628;&#32034;&#24341;&#25806;&#30456;&#27604;&#65292;&#38405;&#35835;LLM&#30340;&#35299;&#37322;&#30340;&#29992;&#25143;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#24403;&#35299;&#37322;&#38169;&#35823;&#26102;&#65292;&#20182;&#20204;&#24448;&#24448;&#36807;&#24230;&#20381;&#36182;LLMs&#12290;&#20026;&#20102;&#20943;&#23569;&#23545;LLMs&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#25105;&#20204;&#35201;&#27714;LLMs&#25552;&#20379;&#23545;&#27604;&#20449;&#24687;&#65292;&#35299;&#37322;&#20026;&#20160;&#20040;&#22768;&#26126;&#20026;&#30495;&#21644;&#20026;&#20551;&#65292;&#24182;&#23558;&#20004;&#26041;&#38754;&#30340;&#35299;&#37322;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;&#36825;&#31181;&#23545;&#27604;&#35299;&#37322;&#20943;&#36731;&#20102;&#29992;&#25143;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they're getting, LLMs should not only provide but also help users fact-check information. In this paper, we conduct experiments with 80 crowdworkers in total to compare language models with search engines (information retrieval systems) at facilitating fact-checking by human users. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than using search engines with similar accuracy. However, they tend to over-rely the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users' over-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#65292;&#37327;&#21270;&#20102;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#30340;&#23384;&#20648;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#36890;&#29992;LLMs&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;LLMs&#37117;&#20542;&#21521;&#20110;&#36814;&#21512;&#29992;&#25143;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.11722</link><description>&lt;p&gt;
&#37327;&#21270;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#65306;&#19968;&#39033;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis. (arXiv:2310.11722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#65292;&#37327;&#21270;&#20102;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#30340;&#23384;&#20648;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#36890;&#29992;LLMs&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;LLMs&#37117;&#20542;&#21521;&#20110;&#36814;&#21512;&#29992;&#25143;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#30452;&#25509;&#21644;&#39640;&#25928;&#22320;&#25552;&#20379;&#29992;&#25143;&#30340;&#33258;&#35786;&#26029;&#24314;&#35758;&#65292;&#20174;&#32780;&#38761;&#26032;&#29992;&#25143;&#33258;&#35786;&#26029;&#30340;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;GPT-4&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#25110;&#20854;&#36890;&#36807;&#21307;&#23398;&#32771;&#35797;&#30340;&#33021;&#21147;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#37327;&#21270;&#23384;&#20648;&#22312;LLMs&#35760;&#24518;&#20013;&#30340;&#20581;&#24247;&#30456;&#20851;&#21407;&#23376;&#30693;&#35782;&#30340;&#31243;&#24230;&#65292;&#32780;&#36825;&#26159;LLMs&#25552;&#20379;&#26356;&#20934;&#30830;&#24314;&#35758;&#30340;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;&#29992;&#25143;&#33258;&#35786;&#26029;&#26597;&#35810;&#20013;&#26368;&#24120;&#35265;&#30340;&#21407;&#23376;&#30693;&#35782;&#31867;&#22411;&#65292;&#20849;17&#31181;&#21407;&#23376;&#31867;&#22411;&#21644;14048&#26465;&#21407;&#23376;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36890;&#29992;&#21644;&#19987;&#19994;LLMs&#22312;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#65292;&#36890;&#29992;LLMs&#30340;&#34920;&#29616;&#20248;&#20110;&#19987;&#19994;LLMs&#12290;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#65292;&#36890;&#29992;&#21644;&#19987;&#19994;LLMs&#37117;&#26159;&#39532;&#23617;&#31934;&#65292;&#21363;&#22312;&#28041;&#21450;&#29992;&#25143;&#35201;&#27714;&#26102;&#24635;&#26159;&#36814;&#21512;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the potential to revolutionize the way users self-diagnose through search engines by offering direct and efficient suggestions. Recent studies primarily focused on the quality of LLMs evaluated by GPT-4 or their ability to pass medical exams, no studies have quantified the extent of health-related atomic knowledge stored in LLMs' memory, which is the basis of LLMs to provide more factual suggestions. In this paper, we first constructed a benchmark, including the most common types of atomic knowledge in user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces of atomic knowledge. Then, we evaluated both generic and specialized LLMs on the benchmark. The experimental results showcased that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability. Error analysis revealed that both generic and specialized LLMs are sycophantic, e.g., always catering to users' claims when it comes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06404</link><description>&lt;p&gt;
Hexa: &#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#65288;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#35760;&#24518;&#26816;&#32034;&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#23545;&#35805;&#21709;&#24212;&#30456;&#27604;&#65292;&#36825;&#20123;&#27493;&#39588;&#30340;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#22240;&#20026;&#22312;&#26222;&#36890;&#23545;&#35805;&#20013;&#26080;&#27861;&#35266;&#23519;&#21040;&#23427;&#20204;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#22810;&#26679;&#24615;&#30340;&#33258;&#20030;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#33258;&#25105;&#25552;&#21319;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#20013;&#38388;&#21644;&#26368;&#32456;&#22238;&#31572;&#26041;&#38754;&#25913;&#21892;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; TEMPO&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#20010;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#23558;&#22797;&#26434;&#20132;&#20114;&#20998;&#35299;&#21644;&#24341;&#20837;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#26469;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.04948</link><description>&lt;p&gt;
TEMPO: &#22522;&#20110;&#25552;&#31034;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. (arXiv:2310.04948v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; TEMPO&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#20010;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#23558;&#22797;&#26434;&#20132;&#20114;&#20998;&#35299;&#21644;&#24341;&#20837;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#26469;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#22312;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#26368;&#22909;&#30340;&#26550;&#26500;&#22312;&#19981;&#21516;&#24212;&#29992;&#21644;&#39046;&#22495;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;(GPT)&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25506;&#32034;&#26159;&#21542;GPT&#31867;&#22411;&#30340;&#26550;&#26500;&#21487;&#20197;&#23545;&#26102;&#38388;&#24207;&#21015;&#20135;&#29983;&#26377;&#25928;&#30340;&#24433;&#21709;&#65292;&#25429;&#25417;&#20854;&#20869;&#22312;&#21160;&#24577;&#23646;&#24615;&#24182;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEMPO&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#31181;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;(i) &#23545;&#36235;&#21183;&#12289;&#23395;&#33410;&#21644;&#27531;&#24046;&#25104;&#20998;&#22797;&#26434;&#20132;&#20114;&#30340;&#20998;&#35299;&#65307;&#21644;(ii) &#25552;&#20986;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#20197;&#20415;&#20110;&#38750;&#20998;&#24067;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-
&lt;/p&gt;</description></item><item><title>LatticeGen&#26159;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22122;&#22768;&#28151;&#21512;&#24182;&#38544;&#34255;&#22312;&#26684;&#23376;&#20013;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LatticeGen&#33021;&#22815;&#22312;&#38754;&#23545;&#24378;&#25915;&#20987;&#26102;&#25104;&#21151;&#20445;&#25252;&#30495;&#23454;&#29983;&#25104;&#65292;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#12290;</title><link>http://arxiv.org/abs/2309.17157</link><description>&lt;p&gt;
LatticeGen: &#19968;&#31181;&#22312;&#20113;&#19978;&#36827;&#34892;&#38544;&#31169;&#24863;&#30693;&#29983;&#25104;&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#38544;&#34255;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#26684;&#23376;&#20013;
&lt;/p&gt;
&lt;p&gt;
LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17157
&lt;/p&gt;
&lt;p&gt;
LatticeGen&#26159;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22122;&#22768;&#28151;&#21512;&#24182;&#38544;&#34255;&#22312;&#26684;&#23376;&#20013;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LatticeGen&#33021;&#22815;&#22312;&#38754;&#23545;&#24378;&#25915;&#20987;&#26102;&#25104;&#21151;&#20445;&#25252;&#30495;&#23454;&#29983;&#25104;&#65292;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#29992;&#25143;-&#26381;&#21153;&#22120;&#20132;&#20114;&#27169;&#24335;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25552;&#31034;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#26381;&#21153;&#22120;&#23436;&#20840;&#25511;&#21046;&#30528;&#29983;&#25104;&#36807;&#31243;&#65292;&#36825;&#20351;&#24471;&#24819;&#35201;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#20445;&#30041;&#32473;&#33258;&#24049;&#30340;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LatticeGen&#65292;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#26381;&#21153;&#22120;&#20173;&#28982;&#22788;&#29702;&#22823;&#37096;&#20998;&#35745;&#31639;&#20219;&#21153;&#65292;&#32780;&#29992;&#25143;&#25511;&#21046;&#37319;&#26679;&#25805;&#20316;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#29992;&#25143;&#23558;&#30495;&#23454;&#29983;&#25104;&#24207;&#21015;&#19982;&#22122;&#22768;&#26631;&#35760;&#28151;&#21512;&#65292;&#24182;&#38544;&#34255;&#22312;&#19968;&#20010;&#24102;&#22122;&#22768;&#30340;&#26684;&#23376;&#20013;&#12290;&#32771;&#34385;&#21040;&#26469;&#33258;&#20551;&#35774;&#24694;&#24847;&#26381;&#21153;&#22120;&#30340;&#28508;&#22312;&#25915;&#20987;&#20197;&#21450;&#29992;&#25143;&#22914;&#20309;&#36827;&#34892;&#38450;&#24481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#22797;&#27874;&#26463;&#25628;&#32034;&#25915;&#20987;&#21644;&#28151;&#21512;&#22122;&#22768;&#26041;&#26696;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;LatticeGen&#24212;&#29992;&#20110;&#20445;&#25252;&#25552;&#31034;&#21644;&#29983;&#25104;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#24102;&#22122;&#22768;&#30340;&#26684;&#23376;&#20250;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;LatticeGen&#25104;&#21151;&#22320;&#22312;&#24378;&#25915;&#20987;&#19979;&#26174;&#33879;&#20445;&#25252;&#20102;&#30495;&#23454;&#29983;&#25104;&#65288;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current user-server interaction paradigm of prompted generation with large language models (LLM) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. Considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15028</link><description>&lt;p&gt;
&#35753;PPO&#21464;&#24471;&#26356;&#22909;&#65306;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;Proximal Policy Optimization (PPO)&#65292;&#22240;&#27492;&#21487;&#20197;&#35748;&#20026;&#25512;&#29702;&#26102;&#38388;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#22914;Monte-Carlo Tree Search (MCTS) &#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;PPO&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35299;&#30721;&#25991;&#26412;&#26102;&#65292;&#19981;&#35201;&#20002;&#24323;&#20540;&#32593;&#32476;&#65292;&#21363;PPO&#35757;&#32451;&#26102;&#29992;&#20110;&#35780;&#20272;&#37096;&#20998;&#36755;&#20986;&#24207;&#21015;&#30340;&#21103;&#20135;&#21697;&#65292;&#32780;&#26159;&#23558;&#20854;&#19982;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PPO-MCTS&#30340;&#26032;&#39062;&#30340;&#20540;&#23548;&#21521;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;PPO&#30340;&#20540;&#32593;&#32476;&#19982;&#25512;&#29702;&#26102;&#38388;&#20135;&#29983;&#30340;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#19982;&#22522;&#20110;MCTS&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#30340;&#35780;&#20998;&#26426;&#21046;&#30340;&#22522;&#26412;&#19981;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PPO-MCTS&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(AceGPT)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#26469;&#22521;&#20859;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AceGPT&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26159;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.12053</link><description>&lt;p&gt;
AceGPT&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#22320;&#21270;&#20026;&#38463;&#25289;&#20271;&#25991;
&lt;/p&gt;
&lt;p&gt;
AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(AceGPT)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#26469;&#22521;&#20859;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AceGPT&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26159;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21457;&#36866;&#29992;&#20110;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36843;&#20999;&#38656;&#27714;&#21644;&#26041;&#27861;&#35770;&#65292;&#38463;&#25289;&#20271;&#25991;&#20855;&#26377;&#29420;&#29305;&#30340;&#25991;&#21270;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#30446;&#21069;&#30340;&#20027;&#27969;&#27169;&#22411;&#22914;ChatGPT&#24182;&#26410;&#20805;&#20998;&#35299;&#20915;&#12290;&#22312;&#32771;&#34385;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#26412;&#22320;&#20215;&#20540;&#35266;&#26102;&#36824;&#23384;&#22312;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25171;&#21253;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#36827;&#19968;&#27493;&#20351;&#29992;&#38463;&#25289;&#20271;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#12289;&#20351;&#29992;&#26412;&#22320;&#38463;&#25289;&#20271;&#25351;&#20196;&#21644;&#38463;&#25289;&#20271;&#35821;GPT-4&#22238;&#24212;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;(SFT)&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#26412;&#22320;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#25935;&#24863;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;(RLAIF)&#12290;&#30446;&#26631;&#26159;&#35757;&#32451;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#19982;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;LLM&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#22810;&#26679;&#21270;&#30340;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#24471;&#21040;&#30340;&#21517;&#20026;AceGPT&#30340;&#38463;&#25289;&#20271;&#25991;LLM&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the imperative need and methodology for developing a localized Large Language Model (LLM) tailored for Arabic, a language with unique cultural characteristics that are not adequately addressed by current mainstream models like ChatGPT. Key concerns additionally arise when considering cultural sensitivity and local values. To this end, the paper outlines a packaged solution, including further pre-training with Arabic texts, supervised fine-tuning (SFT) using native Arabic instructions and GPT-4 responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using a reward model that is sensitive to local culture and values. The objective is to train culturally aware and value-aligned Arabic LLMs that can serve the diverse application-specific needs of Arabic-speaking communities.  Extensive evaluations demonstrated that the resulting LLM called `\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including instruction-following benchmark (i.e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDE&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#26469;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#33021;&#28040;&#38500;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.08832</link><description>&lt;p&gt;
SLIDE: &#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#36827;&#34892;&#26080;&#21442;&#32771;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window. (arXiv:2309.08832v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDE&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#26469;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#33021;&#28040;&#38500;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#36890;&#24120;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#20248;&#20110;&#20165;&#33021;&#35775;&#38382;&#28304;&#35821;&#35328;&#21644;&#31995;&#32479;&#36755;&#20986;&#30340;&#36136;&#37327;&#20272;&#35745;&#24230;&#37327;&#12290;&#36825;&#24182;&#19981;&#22855;&#24618;&#65292;&#22240;&#20026;&#21442;&#32771;&#33021;&#22815;&#28040;&#38500;&#28304;&#35821;&#35328;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#29992;&#39069;&#22806;&#30340;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#26377;&#25928;&#22320;&#26367;&#20195;&#21442;&#32771;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;SLIDE&#65288;SLiding Document Evaluator&#65289;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#28369;&#21160;&#31383;&#21475;&#22312;&#27599;&#20010;&#27979;&#35797;&#38598;&#20013;&#30340;&#25991;&#26723;&#19978;&#25805;&#20316;&#65292;&#23558;&#27599;&#20010;&#22359;&#36755;&#20837;&#21040;&#26410;&#20462;&#25913;&#30340;&#29616;&#25104;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SLIDE&#22312;&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#25104;&#23545;&#27604;&#36739;&#19978;&#36739;&#21477;&#23376;&#32423;&#21035;&#22522;&#32447;&#26174;&#33879;&#25552;&#39640;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#28040;&#38500;&#20102;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reference-based metrics that operate at the sentence level typically outperform quality estimation metrics, which have access only to the source and system output. This is unsurprising, since references resolve ambiguities that may be present in the source. We investigate whether additional source context can effectively substitute for a reference. We present a metric, SLIDE (SLiding Document Evaluator), which operates on blocks of sentences using a window that slides over each document in the test set, feeding each chunk into an unmodified, off-the-shelf quality estimation model. We find that SLIDE obtains significantly higher pairwise system accuracy than its sentence-level baseline, in some cases even eliminating the gap with reference-base metrics. This suggests that source context may provide the same information as a human reference.
&lt;/p&gt;</description></item><item><title>MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08730</link><description>&lt;p&gt;
MusiLingo&#65306;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#38899;&#20048;&#23383;&#24149;&#21644;&#26597;&#35810;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. (arXiv:2309.08730v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08730
&lt;/p&gt;
&lt;p&gt;
MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#22810;&#27169;&#24577;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#25991;&#26412;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;&#34701;&#21512;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MusiLingo&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#21644;&#38899;&#20048;&#30456;&#20851;&#26597;&#35810;&#21709;&#24212;&#30340;&#26032;&#31995;&#32479;&#12290;MusiLingo&#20351;&#29992;&#19968;&#20010;&#25237;&#24433;&#23618;&#26469;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#38899;&#20048;&#38899;&#39057;&#27169;&#22411;MERT&#21644;&#20923;&#32467;&#30340;LLaMA&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#20048;&#34920;&#31034;&#65292;&#23454;&#29616;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#25351;&#23548;&#24615;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#38382;&#31572;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#25105;&#20204;&#20174;MusicCaps&#21019;&#24314;&#20102;MusicInstruct&#65288;MI&#65289;&#25968;&#25454;&#38598;&#65292;&#19987;&#20026;&#24320;&#25918;&#24335;&#38899;&#20048;&#26597;&#35810;&#32780;&#35774;&#35745;&#12290;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#23427;&#22312;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#32452;&#32455;&#38899;&#20048;&#30456;&#20851;&#38382;&#31572;&#23545;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#30340;&#25968;&#25454;&#38598;&#22312;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ExpertQA&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#24773;&#26223;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#31561;&#26041;&#38754;&#26469;&#30830;&#20445;&#25552;&#20379;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#30740;&#31350;&#36824;&#25910;&#38598;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#38382;&#39064;&#24182;&#35201;&#27714;&#20182;&#20204;&#35780;&#20272;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#19981;&#20250;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.07852</link><description>&lt;p&gt;
ExpertQA: &#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
ExpertQA: Expert-Curated Questions and Attributed Answers. (arXiv:2309.07852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ExpertQA&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#24773;&#26223;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#31561;&#26041;&#38754;&#26469;&#30830;&#20445;&#25552;&#20379;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#30740;&#31350;&#36824;&#25910;&#38598;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#38382;&#39064;&#24182;&#35201;&#27714;&#20182;&#20204;&#35780;&#20272;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#19981;&#20250;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#34987;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#25152;&#37319;&#29992;&#65292;&#30830;&#20445;&#23427;&#20204;&#25552;&#20379;&#22522;&#20110;&#21487;&#39564;&#35777;&#26469;&#28304;&#30340;&#20107;&#23454;&#20934;&#30830;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#32844;&#19994;&#20013;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#29305;&#21035;&#36866;&#29992;&#20110;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#22240;&#20026;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#39118;&#38505;&#36739;&#39640;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20107;&#23454;&#24615;&#21644;&#24402;&#22240;&#26041;&#38754;&#65292;&#24182;&#26410;&#19987;&#27880;&#20110;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#24773;&#26223;&#20013;&#30340;&#36825;&#20123;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39046;&#22495;&#19987;&#23478;&#32435;&#20837;&#20854;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#30740;&#31350;&#65292;&#20998;&#26512;&#26469;&#33258;&#20960;&#20010;&#31995;&#32479;&#30340;&#21709;&#24212;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20808;&#20174;32&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;484&#21517;&#21442;&#19982;&#32773;&#20013;&#25910;&#38598;&#30001;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#65292;&#28982;&#21518;&#35201;&#27714;&#36825;&#20123;&#19987;&#23478;&#35780;&#20272;&#23545;&#20182;&#20204;&#33258;&#24049;&#38382;&#39064;&#30340;&#20135;&#29983;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#36824;&#35201;&#27714;&#19987;&#23478;&#20462;&#25913;&#20135;&#29983;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models are adapted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study &amp; professions. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying factuality and attribution has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we present an evaluation study analyzing various axes of factuality and attribution provided in responses from a few systems, by bringing domain experts in the loop. Specifically, we first collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. We also ask experts to revise answers produce
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#35777;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65292;&#24182;&#19988;ALPACA&#22312;&#20445;&#30041;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.08747</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. (arXiv:2308.08747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08747
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#35777;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65292;&#24182;&#19988;ALPACA&#22312;&#20445;&#30041;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#29616;&#35937;&#65292;&#24403;&#27169;&#22411;&#23398;&#20064;&#26032;&#20449;&#24687;&#26102;&#65292;&#23427;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#25506;&#31350;LLMs&#22312;&#25345;&#32493;&#24494;&#35843;&#20013;&#26159;&#21542;&#23384;&#22312;CF&#26159;&#24456;&#26377;&#24847;&#20041;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#39046;&#22495;&#30693;&#35782;&#12289;&#25512;&#29702;&#21644;&#38405;&#35835;&#29702;&#35299;&#30340;&#35282;&#24230;&#23545;LLMs&#30340;&#36951;&#24536;&#29616;&#35937;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;1b&#21040;7b&#30340;&#33539;&#22260;&#20869;&#65292;LLMs&#26222;&#36941;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#19988;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;mT0&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;BLOOMZ&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65288;&#22914;&#24615;&#21035;&#20559;&#35265;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;LLAMA&#30456;&#27604;&#65292;ALPACA&#22312;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information as it learns new information. As large language models (LLMs) have shown excellent performance, it is interesting to uncover whether CF exists in the continual fine-tuning of LLMs. In this study, we empirically evaluate the forgetting phenomenon in LLMs' knowledge, from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments demonstrate that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b. Furthermore, as the scale increases, the severity of forgetting also intensifies. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ suffers less forgetting and maintains more knowledge. We also observe that LLMs can mitigate language bias (e.g. gender bias) during continual fine-tuning. Moreover, we find that ALPACA can maintain more knowledge and capacity compared with LLAMA du
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15780</link><description>&lt;p&gt;
LLM-Rec: &#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;LLM-Rec&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65306;&#65288;1&#65289;&#22522;&#30784;&#24341;&#23548;&#65292;&#65288;2&#65289;&#25512;&#33616;&#39537;&#21160;&#24341;&#23548;&#65292;&#65288;3&#65289;&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#65292;&#21644;&#65288;4&#65289;&#25512;&#33616;&#39537;&#21160;+&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#21407;&#22987;&#20869;&#23481;&#25551;&#36848;&#19982;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#36755;&#20837;&#25991;&#26412;&#32467;&#21512;&#36215;&#26469;&#65292;&#37319;&#29992;&#36825;&#20123;&#24341;&#23548;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#35780;&#20272;&#21327;&#35758;&#65292;&#21475;&#36848;&#32773;&#25805;&#20316;&#65292;&#36890;&#36807;&#21475;&#36848;&#20219;&#21153;&#26631;&#31614;&#26469;&#26816;&#26597;&#27169;&#22411;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#20197;&#21450;&#35206;&#30422;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#36827;&#34892;&#25351;&#31034;&#36981;&#24490;&#12290;</title><link>http://arxiv.org/abs/2307.10558</link><description>&lt;p&gt;
&#36890;&#36807;&#21475;&#36848;&#26041;&#24335;&#25805;&#20316;&#36827;&#34892;&#25351;&#31034;&#36981;&#24490;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Instruction-following Evaluation through Verbalizer Manipulation. (arXiv:2307.10558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#35780;&#20272;&#21327;&#35758;&#65292;&#21475;&#36848;&#32773;&#25805;&#20316;&#65292;&#36890;&#36807;&#21475;&#36848;&#20219;&#21153;&#26631;&#31614;&#26469;&#26816;&#26597;&#27169;&#22411;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#20197;&#21450;&#35206;&#30422;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#36827;&#34892;&#25351;&#31034;&#36981;&#24490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35843;&#25972;&#25351;&#20196;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20934;&#30830;&#35780;&#20272;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#19982;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#30340;&#20869;&#23481;&#30456;&#21563;&#21512;&#30340;&#24120;&#35265;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#25351;&#20196;&#30340;&#22238;&#24212;&#33021;&#21147;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#24378;&#22823;&#30340;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25351;&#31034;&#36981;&#24490;&#35780;&#20272;&#21327;&#35758;&#65292;&#31216;&#20026;&#21475;&#36848;&#32773;&#25805;&#20316;&#12290;&#23427;&#35201;&#27714;&#27169;&#22411;&#29992;&#19982;&#27169;&#22411;&#20808;&#39564;&#30693;&#35782;&#19981;&#21516;&#31243;&#24230;&#21563;&#21512;&#30340;&#21333;&#35789;&#21475;&#36848;&#20219;&#21153;&#26631;&#31614;&#65292;&#20174;&#39640;&#24230;&#21563;&#21512;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#31215;&#26497;&#24773;&#32490;&#36755;&#20986;&#8220;&#31215;&#26497;&#8221;&#65289;&#21040;&#26368;&#23569;&#21563;&#21512;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#31215;&#26497;&#24773;&#32490;&#36755;&#20986;&#8220;&#28040;&#26497;&#8221;&#65289;&#12290;&#21475;&#36848;&#32773;&#25805;&#20316;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#31867;&#22522;&#20934;&#26080;&#32541;&#38598;&#25104;&#65292;&#20197;&#26816;&#26597;&#27169;&#22411;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#20381;&#36182;&#31243;&#24230;&#20197;&#21450;&#35206;&#30422;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#36827;&#34892;&#25351;&#31034;&#36981;&#24490;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned models have shown remarkable success in various natural language processing tasks, accurately evaluating their ability to follow instructions remains challenging. Existing benchmarks primarily focus on common instructions that align well with what the model learned during training. However, proficiency in responding to these instructions does not necessarily imply strong ability in instruction following. In this paper, we propose a novel instruction-following evaluation protocol called verbalizer manipulation. It instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting ``postive'' for positive sentiment), to minimally aligned (e.g., outputting ``negative'' for positive sentiment). Verbalizer manipulation can be seamlessly integrated with any classification benchmark to examine the model's reliance on priors and its ability to override them to accurately 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06077</link><description>&lt;p&gt;
&#35270;&#35273;&#35789;&#27719;&#25551;&#36848;&#25552;&#21319;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Visually-Grounded Descriptions Improve Zero-Shot Image Classification. (arXiv:2306.06077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#22914;CLIP&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#20219;&#21153;&#65288;&#20363;&#22914;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;ZSIC&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#20307;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#31867;&#21035;&#25551;&#36848;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#31890;&#24230;&#21644;&#26631;&#31614;&#27495;&#20041;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;V-GLOSS&#65306;Visual Glosses&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#26469;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;ZSIC&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;ImageNet&#21644;STL-10&#65289;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#26469;&#23637;&#31034;V-GLOSS&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;V-GLOSS&#29983;&#25104;&#30340;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20854;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-vision models like CLIP have made significant progress in zero-shot vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive class descriptions remains a major challenge. Existing approaches suffer from granularity and label ambiguity issues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel method leveraging modern language models and semantic knowledge bases to produce visually-grounded class descriptions. We demonstrate V-GLOSS's effectiveness by achieving state-of-the-art results on benchmark ZSIC datasets including ImageNet and STL-10. In addition, we introduce a silver dataset with class descriptions generated by V-GLOSS, and show its usefulness for vision tasks. We make available our code and dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#20363;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#30340;&#25968;&#25454;&#38598;BorderLines&#21644;&#20960;&#20010;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.14610</link><description>&lt;p&gt;
&#36825;&#29255;&#22303;&#22320;&#26159;&#20320;&#25105;&#30340;&#22303;&#22320;&#65306;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models. (arXiv:2305.14610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#20363;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#30340;&#25968;&#25454;&#38598;BorderLines&#21644;&#20960;&#20010;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#8212;&#8212;&#21363;&#26681;&#25454;&#35821;&#35328;&#29615;&#22659;&#25253;&#36947;&#19981;&#21516;&#30340;&#22320;&#32536;&#25919;&#27835;&#30693;&#35782;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#34987;&#24191;&#27867;&#20105;&#35758;&#30340;&#21335;&#27801;&#32676;&#23707;&#65292;&#22914;&#26524;&#29992;&#20013;&#25991;&#38382;&#65292;LM&#26159;&#21542;&#26356;&#26377;&#21487;&#33021;&#35828;&#23427;&#20204;&#23646;&#20110;&#20013;&#22269;&#65292;&#32780;&#22914;&#26524;&#29992;&#22612;&#21152;&#27931;&#35821;&#38382;&#65292;&#21017;&#26356;&#26377;&#21487;&#33021;&#35828;&#23427;&#20204;&#23646;&#20110;&#33778;&#24459;&#23486;&#65311;&#20026;&#20102;&#35780;&#20272;&#26159;&#21542;&#23384;&#22312;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#32500;&#22522;&#30334;&#31185;&#19978;&#25910;&#38598;&#20102;&#19968;&#32452;&#39046;&#22303;&#20105;&#31471;&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#39046;&#22303;&#19982;&#19968;&#32452;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;BorderLines&#65292;&#23427;&#21253;&#25324;250&#20010;&#39046;&#22303;&#21644;45&#31181;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#38598;&#25552;&#20132;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#25552;&#20986;&#30340;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#20013;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#12290;&#36825;&#20123;&#25351;&#26631;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#30340;&#22238;&#31572;&#20197;&#21450;&#23454;&#38469;&#30340;&#22320;&#32536;&#25919;&#27835;&#24773;&#20917;&#12290;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#36328;&#35821;&#35328;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the notion of geopolitical bias -- a tendency to report different geopolitical knowledge depending on the linguistic context. As a case study, we consider territorial disputes between countries. For example, for the widely contested Spratly Islands, would an LM be more likely to say they belong to China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To evaluate if such biases exist, we first collect a dataset of territorial disputes from Wikipedia, then associate each territory with a set of multilingual, multiple-choice questions. This dataset, termed BorderLines, consists of 250 territories with questions in 45 languages. We pose these question sets to language models, and analyze geopolitical bias in their responses through several proposed quantitative metrics. The metrics compare between responses in different question languages as well as to the actual geopolitical situation. The phenomenon of geopolitical bias is a uniquely cross-lingual evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.02547</link><description>&lt;p&gt;
PersonaLLM: &#25506;&#31350;GPT-3.5&#34920;&#36798;&#20010;&#24615;&#29305;&#24449;&#21644;&#24615;&#21035;&#24046;&#24322;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#24182;&#19988;&#30740;&#31350;&#34920;&#26126;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#19981;&#21516;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20010;&#24615;&#21270;LLM&#30340;&#34892;&#20026;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#12289;&#19968;&#33268;&#22320;&#21453;&#26144;&#26576;&#20123;&#20154;&#26684;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM personas&#65292;&#24182;&#20351;&#29992;GPT-3.5&#65288;text-davinci-003&#65289;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;LLM&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;320&#20010;LLM personas&#65288;&#27599;&#31181;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#26377;5&#20010;&#22899;&#24615;&#21644;5&#20010;&#30007;&#24615;&#65289;&#65292;&#24182;&#25552;&#31034;&#20182;&#20204;&#23436;&#25104;&#32463;&#20856;&#30340;44&#39033;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#65292;&#28982;&#21518;&#25776;&#20889;&#19968;&#20010;&#20851;&#20110;&#20182;&#20204;&#31461;&#24180;&#30340;800&#23383;&#25925;&#20107;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM personas&#30340;&#33258;&#25105;&#25253;&#21578;&#30340;BFI&#20998;&#25968;&#19982;&#20182;&#20204;&#20998;&#37197;&#30340;&#20154;&#26684;&#31867;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20960;&#20010;&#26032;&#26041;&#21521;&#65292;&#21253;&#25324;&#39118;&#26684;&#21270;MT&#12289;&#20132;&#20114;&#24335;MT&#21644;&#22522;&#20110;&#32763;&#35793;&#35760;&#24518;&#30340;MT&#65292;&#24182;&#35752;&#35770;&#20102;&#38544;&#31169;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.01181</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#26032;&#36235;&#21183;&#65306;&#20197;ChatGPT&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT. (arXiv:2305.01181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20960;&#20010;&#26032;&#26041;&#21521;&#65292;&#21253;&#25324;&#39118;&#26684;&#21270;MT&#12289;&#20132;&#20114;&#24335;MT&#21644;&#22522;&#20110;&#32763;&#35793;&#35760;&#24518;&#30340;MT&#65292;&#24182;&#35752;&#35770;&#20102;&#38544;&#31169;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#21160;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;GPT-3&#21644;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#21518;&#12290;&#36825;&#20026;&#20351;&#29992;LLMs&#30340;MT&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#20351;&#29992;LLMs&#30340;MT&#26041;&#21521;&#65292;&#21253;&#25324;&#39118;&#26684;&#21270;MT&#12289;&#20132;&#20114;&#24335;MT&#21644;&#22522;&#20110;&#32763;&#35793;&#35760;&#24518;&#30340;MT&#65292;&#20197;&#21450;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#26032;&#35780;&#20272;&#33539;&#20363;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#30340;MT&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#39118;&#38505;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#20197;&#19978;&#25552;&#21040;&#30340;&#26032;&#26041;&#21521;&#30340;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#21521;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;MT&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation (MT) has made significant progress in recent years using deep learning, especially after the emergence of large language models (LLMs) such as GPT-3 and ChatGPT. This brings new challenges and opportunities for MT using LLMs. In this paper, we brainstorm some interesting directions for MT using LLMs, including stylized MT, interactive MT, and Translation Memory-based MT, as well as a new evaluation paradigm using LLMs. We also discuss the privacy concerns in MT using LLMs and a basic privacy-preserving method to mitigate such risks. To illustrate the potential of our proposed directions, we present several examples for the new directions mentioned above, demonstrating the feasibility of the proposed directions and highlight the opportunities and challenges for future research in MT using LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#20004;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#32416;&#27491;&#21021;&#23398;&#32773;&#22312;&#36923;&#36753;&#35821;&#35328;&#36716;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06186</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#23398;&#32773;&#30340;&#65288;&#38750;&#65289;&#24418;&#24335;&#21270;&#21644;&#33258;&#28982;&#35770;&#35777;&#32451;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using large language models for (de-)formalization and natural argumentation exercises for beginner's students. (arXiv:2304.06186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#20004;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#32416;&#27491;&#21021;&#23398;&#32773;&#22312;&#36923;&#36753;&#35821;&#35328;&#36716;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#20010;&#31995;&#32479;&#65292;&#20351;&#29992;&#25991;&#26412;&#36798;&#33452;&#22855;-003&#65292;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#32416;&#27491;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#19982;&#21629;&#39064;&#36923;&#36753;&#35821;&#35328;&#21644;&#19968;&#38454;&#35859;&#35789;&#36923;&#36753;&#35821;&#35328;&#20043;&#38388;&#36716;&#21270;&#30340;&#32451;&#20064;; &#21644;&#65288;ii&#65289;&#22312;&#38750;&#25968;&#23398;&#22330;&#26223;&#19979;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#20889;&#31616;&#21333;&#35770;&#28857;&#30340;&#32451;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe two systems that use text-davinci-003, a large language model, for the automatized correction of (i) exercises in translating back and forth between natural language and the languages of propositional logic and first-order predicate logic and (ii) exercises in writing simple arguments in natural language in non-mathematical scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;Diproche&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17513</link><description>&lt;p&gt;
&#36890;&#36807;GPT-3&#33258;&#21160;&#24418;&#24335;&#21270;&#25552;&#39640;Diproche CNL&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Improving the Diproche CNL through autoformalization via GPT-3. (arXiv:2303.17513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;Diproche&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Diproche&#31995;&#32479;&#26159;&#19968;&#27454;&#38024;&#23545;&#24503;&#35821;&#25511;&#21046;&#35821;&#35328;&#29255;&#27573;&#30340;&#33258;&#21160;&#21270;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#26088;&#22312;&#29992;&#20110;&#25945;&#23398;&#24212;&#29992;&#65292;&#22312;&#24341;&#23548;&#23398;&#29983;&#36827;&#34892;&#35777;&#26126;&#26102;&#20351;&#29992;&#12290;&#35813;&#31995;&#32479;&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#20351;&#29992;&#19968;&#31181;&#25511;&#21046;&#33258;&#28982;&#35821;&#35328;&#65292;&#20854;Prolog&#24418;&#24335;&#21270;&#20363;&#31243;&#24050;&#32463;&#32534;&#20889;&#22909;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;Diproche&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Diproche system is an automated proof checker for texts written in a controlled fragment of German, designed for didactical applications in classes introducing students to proofs for the first time. The first version of the system used a controlled natural language for which a Prolog formalization routine was written. In this paper, we explore the possibility of prompting large language models for autoformalization in the context of Diproche, with encouraging first results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#31070;&#32463;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65288;DTG&#65289;&#39046;&#22495;&#30340;&#21019;&#26032;&#65292;&#23558;&#20854;&#19982;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65288;NLG&#65289;&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20102;&#20851;&#27880;&#31995;&#32479;&#35821;&#35328;&#33021;&#21147;&#21644;&#20844;&#24179;&#12289;&#20844;&#27491;&#30340;DTG&#30740;&#31350;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2207.12571</link><description>&lt;p&gt;
&#31070;&#32463;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#30340;&#21019;&#26032;&#65306;&#32508;&#36848;&#65288;arXiv&#65306;2207.12571v2 [cs.CL] &#26356;&#26032;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Innovations in Neural Data-to-text Generation: A Survey. (arXiv:2207.12571v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#31070;&#32463;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65288;DTG&#65289;&#39046;&#22495;&#30340;&#21019;&#26032;&#65292;&#23558;&#20854;&#19982;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65288;NLG&#65289;&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20102;&#20851;&#27880;&#31995;&#32479;&#35821;&#35328;&#33021;&#21147;&#21644;&#20844;&#24179;&#12289;&#20844;&#27491;&#30340;DTG&#30740;&#31350;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#38388;&#65292;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20852;&#36215;&#22312;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65288;DTG&#65289;&#39046;&#22495;&#21516;&#26679;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#21019;&#26032;&#12290;&#26412;&#32508;&#36848;&#36890;&#36807;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#31070;&#32463;DTG&#33539;&#24335;&#30340;&#26041;&#27861;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#21327;&#35758;&#36827;&#34892;&#20102;&#32508;&#21512;&#24615;&#30340;&#27010;&#36848;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#39046;&#22495;&#20013;&#21306;&#20998;&#20986;DTG&#65292;&#21253;&#25324;&#23545;&#25991;&#29486;&#30340;&#26368;&#26032;&#32508;&#21512;&#21644;&#25216;&#26415;&#37319;&#29992;&#38454;&#27573;&#30340;&#37325;&#28857;&#20171;&#32461;&#65292;&#24378;&#35843;&#20102;&#19981;&#20165;&#20851;&#27880;&#35774;&#35745;&#20855;&#26377;&#35821;&#35328;&#33021;&#21147;&#30340;&#31995;&#32479;&#65292;&#32780;&#19988;&#36824;&#35201;&#20851;&#27880;&#20307;&#29616;&#20844;&#24179;&#21644;&#20844; accountability &#30340;&#31995;&#32479;&#30340;DTG&#30740;&#31350;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural boom that has sparked natural language processing (NLP) research through the last decade has similarly led to significant innovations in data-to-text generation (DTG). This survey offers a consolidated view into the neural DTG paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for DTG research that not only focus on the design of linguistically capable systems but also systems that exhibit fairness and accountability.
&lt;/p&gt;</description></item></channel></rss>