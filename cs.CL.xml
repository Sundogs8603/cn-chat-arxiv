<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>MambaByte&#26159;&#19968;&#31181;&#26080;&#26631;&#35760;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23383;&#33410;&#32423;&#21035;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;MambaByte&#22312;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13660</link><description>&lt;p&gt;
MambaByte: &#26080;&#26631;&#35760;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaByte: Token-free Selective State Space Model. (arXiv:2401.13660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13660
&lt;/p&gt;
&lt;p&gt;
MambaByte&#26159;&#19968;&#31181;&#26080;&#26631;&#35760;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23383;&#33410;&#32423;&#21035;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;MambaByte&#22312;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#35760;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#21407;&#22987;&#23383;&#33410;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#23376;&#35789;&#26631;&#35760;&#21270;&#30340;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#25805;&#20316;&#23383;&#33410;&#20250;&#23548;&#33268;&#24207;&#21015;&#38271;&#24230;&#26174;&#33879;&#22686;&#21152;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;MambaByte&#65292;&#23427;&#26159;&#22522;&#20110;&#23383;&#33410;&#24207;&#21015;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#26080;&#26631;&#35760;&#36866;&#24212;Mamba&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#23383;&#33410;&#32423;&#27169;&#22411;&#30456;&#27604;&#65292;MambaByte&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;MambaByte&#22312;&#24615;&#33021;&#19978;&#19982;&#29978;&#33267;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38271;&#24230;&#30340;&#32447;&#24615;&#25193;&#23637;&#65292;MambaByte&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#33719;&#24471;&#20102;&#24555;&#36895;&#24615;&#33021;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;Transformer&#21017;&#27809;&#26377;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;MambaByte&#22312;&#23454;&#29616;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.
&lt;/p&gt;</description></item><item><title>VisualWebArena&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#23545;&#20195;&#29702;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#35201;&#27714;&#20195;&#29702;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#26469;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.13649</link><description>&lt;p&gt;
VisualWebArena: &#22312;&#30495;&#23454;&#35270;&#35273;Web&#20219;&#21153;&#19978;&#35780;&#20272;&#22810;&#27169;&#24577;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. (arXiv:2401.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13649
&lt;/p&gt;
&lt;p&gt;
VisualWebArena&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#23545;&#20195;&#29702;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#35201;&#27714;&#20195;&#29702;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#26469;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#35745;&#21010;&#12289;&#25512;&#29702;&#21644;&#25191;&#34892;&#21160;&#20316;&#30340;&#33258;&#20027;&#20195;&#29702;&#20026;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#25991;&#26412;&#30340;&#20195;&#29702;&#65292;&#22312;&#25928;&#26524;&#19978;&#24573;&#35270;&#20102;&#35768;&#22810;&#38656;&#35201;&#35270;&#35273;&#20449;&#24687;&#25165;&#33021;&#26377;&#25928;&#35299;&#20915;&#30340;&#33258;&#28982;&#20219;&#21153;&#12290;&#37492;&#20110;&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#30028;&#38754;&#26159;&#20026;&#20154;&#31867;&#24863;&#30693;&#32780;&#35774;&#35745;&#30340;&#65292;&#35270;&#35273;&#20449;&#24687;&#24448;&#24448;&#20197;&#25991;&#26412;&#25968;&#25454;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#30340;&#26041;&#24335;&#22686;&#24378;&#25991;&#26412;&#25968;&#25454;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VisualWebArena&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;VisualWebArena&#21253;&#25324;&#19968;&#32452;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#22522;&#20110;Web&#30340;&#20219;&#21153;&#65292;&#35780;&#20272;&#33258;&#20027;&#22810;&#27169;&#24577;&#20195;&#29702;&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#25191;&#34892;&#65292;&#20195;&#29702;&#38656;&#35201;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#20197;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an ext
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;&#22122;&#22768;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#21644;&#36801;&#31227;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2401.13621</link><description>&lt;p&gt;
DenoSent&#65306;&#29992;&#20110;&#33258;&#25105;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#21435;&#22122;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning. (arXiv:2401.13621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;&#22122;&#22768;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#21644;&#36801;&#31227;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#23548;&#30528;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#25289;&#36817;&#30456;&#20284;&#30340;&#21477;&#23376;&#34920;&#31034;&#24182;&#23558;&#19981;&#30456;&#20284;&#30340;&#21477;&#23376;&#34920;&#31034;&#25512;&#24320;&#26469;&#35268;&#33539;&#34920;&#31034;&#31354;&#38388;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20363;&#22914;&#65292;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#26469;&#35828;&#65292;&#23398;&#20064;&#32454;&#31890;&#24230;&#35821;&#20041;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#20174;&#21477;&#38388;&#30340;&#35282;&#24230;&#23398;&#20064;&#65292;&#21363;&#65292;&#23427;&#20204;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#33258;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#22122;&#30446;&#26631;&#65292;&#23427;&#32487;&#25215;&#20102;&#21478;&#19968;&#20010;&#35270;&#35282;&#65292;&#21363;&#21477;&#20869;&#30340;&#35270;&#35282;&#12290;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#29983;&#25104;&#24102;&#26377;&#22122;&#22768;&#30340;&#21477;&#23376;&#65292;&#28982;&#21518;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#20854;&#24674;&#22797;&#20026;&#21407;&#22987;&#24418;&#24335;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#21644;&#24191;&#27867;&#30340;&#36801;&#31227;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive-learning-based methods have dominated sentence representation learning. These methods regularize the representation space by pulling similar sentence representations closer and pushing away the dissimilar ones and have been proven effective in various NLP tasks, e.g., semantic textual similarity (STS) tasks. However, it is challenging for these methods to learn fine-grained semantics as they only learn from the inter-sentence perspective, i.e., their supervision signal comes from the relationship between data samples. In this work, we propose a novel denoising objective that inherits from another perspective, i.e., the intra-sentence perspective. By introducing both discrete and continuous noise, we generate noisy sentences and then train our model to restore them to their original form. Our empirical evaluations demonstrate that this approach delivers competitive results on both semantic textual similarity (STS) and a wide range of transfer tasks, standing up well in compa
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#25903;&#25345;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;MM-LLMs&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#25972;&#29702;&#20102;&#29616;&#26377;&#30340;MM-LLMs&#21450;&#20854;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.13601</link><description>&lt;p&gt;
MM-LLMs: &#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
MM-LLMs: Recent Advances in MultiModal Large Language Models. (arXiv:2401.13601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13601
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#25903;&#25345;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;MM-LLMs&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#25972;&#29702;&#20102;&#29616;&#26377;&#30340;MM-LLMs&#21450;&#20854;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#19968;&#24180;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;LLMs&#23545;&#22810;&#27169;&#36755;&#20837;&#25110;&#36755;&#20986;&#30340;&#25903;&#25345;&#12290;&#36825;&#20123;&#32467;&#26524;&#27169;&#22411;&#19981;&#20165;&#20445;&#30041;&#20102;LLMs&#22266;&#26377;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#36824;&#36171;&#20104;&#20102;&#21508;&#31181;&#22810;&#27169;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;MM-LLMs&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#27969;&#31243;&#30340;&#19968;&#33324;&#35774;&#35745;&#26041;&#26696;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;26&#31181;&#29616;&#26377;&#30340;MM-LLMs&#65292;&#27599;&#31181;&#37117;&#20197;&#20854;&#20855;&#20307;&#30340;&#20844;&#24335;&#20026;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;MM-LLMs&#22312;&#20027;&#27969;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#25552;&#39640;MM-LLMs&#25928;&#21147;&#30340;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MM-LLMs&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#36824;&#20026;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#25552;&#20379;&#20102;&#23454;&#26102;&#36861;&#36394;&#32593;&#31449;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#22815;&#20419;&#36827;&#23545;MM-LLMs&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this surv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#25991;&#26723;&#32423;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;ZeroDocRTE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;LLMs&#20013;&#26816;&#32034;&#21644;&#21435;&#22122;&#30693;&#35782;&#26469;&#29983;&#25104;&#26631;&#35760;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#33719;&#21462;&#26032;&#20851;&#31995;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.13598</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#25351;&#23548;&#22312;LLMs&#20013;&#36827;&#34892;&#30693;&#35782;&#26816;&#32034;&#21644;&#21435;&#22122;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#25991;&#26723;&#32423;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction. (arXiv:2401.13598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#25991;&#26723;&#32423;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;ZeroDocRTE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;LLMs&#20013;&#26816;&#32034;&#21644;&#21435;&#22122;&#30693;&#35782;&#26469;&#29983;&#25104;&#26631;&#35760;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#33719;&#21462;&#26032;&#20851;&#31995;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;DocRTE&#65289;&#26159;&#20449;&#24687;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#25991;&#26723;&#20013;&#21516;&#26102;&#25552;&#21462;&#24102;&#26377;&#35821;&#20041;&#20851;&#31995;&#30340;&#23454;&#20307;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#23436;&#25972;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#27880;&#37322;&#26032;&#20986;&#29616;&#30340;&#20851;&#31995;&#30340;&#25968;&#25454;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;LLaMA&#65292;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#21551;&#21457;&#25105;&#20204;&#25506;&#32034;&#19968;&#31181;&#29992;&#20110;&#33719;&#21462;&#20855;&#26377;&#26032;&#20851;&#31995;&#30340;&#33258;&#21160;&#26631;&#35760;&#25991;&#26723;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#25991;&#26723;&#32423;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;ZeroDocRTE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;LLMs&#20013;&#26816;&#32034;&#21644;&#21435;&#22122;&#30693;&#35782;&#26469;&#29983;&#25104;&#26631;&#35760;&#25968;&#25454;&#65292;&#31216;&#20026;GenRDK&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#25552;&#31034;&#38142;&#20197;&#25351;&#23548;ChatGPT&#36880;&#27493;&#29983;&#25104;&#26631;&#35760;&#30340;&#38271;&#25991;&#26412;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#31574;&#30053;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#35814;&#23613;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#32039;&#20945;&#30340;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;QA&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#29305;&#23450;&#38382;&#31572;&#20219;&#21153;&#20013;&#19982;GPT&#21464;&#20307;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#36807;&#31243;&#24615;&#25991;&#26412;&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#27493;&#39588;&#21644;&#25972;&#20010;&#27969;&#31243;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#24182;&#20197;&#22270;&#33410;&#28857;&#20026;&#26465;&#20214;&#65292;&#22312;&#35814;&#23613;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#19979;&#33258;&#21160;&#29983;&#25104;QA&#23545;&#12290;</title><link>http://arxiv.org/abs/2401.13594</link><description>&lt;p&gt;
&#29992;&#20110;&#31243;&#24207;&#24615;&#38382;&#31572;&#30340;&#22270;&#24418;&#24341;&#23548;&#38382;&#39064;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Guided Question Answer Generation for Procedural Question-Answering. (arXiv:2401.13594v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#35814;&#23613;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#32039;&#20945;&#30340;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;QA&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#29305;&#23450;&#38382;&#31572;&#20219;&#21153;&#20013;&#19982;GPT&#21464;&#20307;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#36807;&#31243;&#24615;&#25991;&#26412;&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#27493;&#39588;&#21644;&#25972;&#20010;&#27969;&#31243;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#24182;&#20197;&#22270;&#33410;&#28857;&#20026;&#26465;&#20214;&#65292;&#22312;&#35814;&#23613;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#19979;&#33258;&#21160;&#29983;&#25104;QA&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#38382;&#31572;(QA)&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#35814;&#23613;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#32039;&#20945;&#30340;&#12289;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;QA&#27169;&#22411;&#65292;&#24182;&#19982;GPT&#21464;&#20307;&#27169;&#22411;&#31454;&#20105;&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#25903;&#25345;&#26159;&#19968;&#31181;&#20174;&#36807;&#31243;&#25991;&#26412;&#20013;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#30340;&#26032;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22823;&#37327;&#30340;&#25991;&#26412;&#25351;&#20196;&#65292;&#24182;&#20135;&#29983;&#35814;&#23613;&#30340;&#39046;&#22495;&#20869;QA&#35757;&#32451;&#25968;&#25454;&#12290;&#30446;&#21069;&#30340;QA&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20250;&#20135;&#29983;&#24418;&#24335;&#33391;&#22909;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#65292;&#20294;&#20854;&#38750;&#35814;&#23613;&#24615;&#19981;&#21033;&#20110;&#35757;&#32451;QA&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#36807;&#31243;&#24615;&#25991;&#26412;&#30340;&#39640;&#24230;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#23558;&#27599;&#20010;&#27493;&#39588;&#21644;&#25972;&#20010;&#27969;&#31243;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#24182;&#20197;&#22270;&#33410;&#28857;&#20026;&#26465;&#20214;&#65292;&#33258;&#21160;&#20197;&#35814;&#23613;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#29983;&#25104;QA&#23545;&#12290;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65306;1) &#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#23567;&#22411;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#31454;&#20105;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on task-specific question answering (QA). To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants. The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data. While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model. In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs. We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner. Comprehensive evaluations of our method show that: 1) small models trained with our 
&lt;/p&gt;</description></item><item><title>&#22312;&#23454;&#38469;&#21307;&#30103;&#39046;&#22495;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26356;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25104;&#20154;&#37325;&#30151;&#25252;&#29702;&#21307;&#23398;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.13588</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#25104;&#20154;&#37325;&#30151;&#25252;&#29702;&#30005;&#23376;&#30149;&#21382;&#20013;&#25552;&#21462;&#35821;&#20041;&#27010;&#24565;&#30340;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes. (arXiv:2401.13588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13588
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#21307;&#30103;&#39046;&#22495;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26356;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25104;&#20154;&#37325;&#30151;&#25252;&#29702;&#21307;&#23398;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#35813;&#30740;&#31350;&#23558;&#28966;&#28857;&#36716;&#21521;&#20102;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20256;&#32479;&#30340;&#38382;&#31572;&#20219;&#21153;&#35780;&#20272;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36825;&#31181;&#24046;&#36317;&#20984;&#26174;&#20102;&#22312;&#23454;&#38469;&#21307;&#30103;&#39046;&#22495;&#20013;&#23545;LLMs&#36827;&#34892;&#26356;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#31995;&#32479;&#21270;&#21644;&#26131;&#20110;&#29702;&#35299;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21253;&#25324;&#20020;&#24202;&#21307;&#29983;&#27880;&#37322;&#21644;&#35009;&#23450;&#65292;&#35780;&#20272;LLMs&#22312;&#25104;&#20154;&#37325;&#30151;&#25252;&#29702;&#21307;&#23398;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of healthcare has increasingly turned its focus towards Large Language Models (LLMs) due to their remarkable performance. However, their performance in actual clinical applications has been underexplored. Traditional evaluations based on question-answering tasks don't fully capture the nuanced contexts. This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings. Objective: We sought to evaluate the performance of LLMs in the complex clinical context of adult critical care medicine using systematic and comprehensible analytic methods, including clinician annotation and adjudication. Methods: We investigated the performance of three general LLMs in understanding and processing real-world clinical notes. Concepts from 150 clinical notes were identified by MetaMap and then labeled by 9 clinicians. Each LLM's proficiency was evaluated by identifying the temporality and negation of these concepts using different prompts for an
&lt;/p&gt;</description></item><item><title>LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13586</link><description>&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#25552;&#31034;&#26435;&#37325;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13586
&lt;/p&gt;
&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#22411;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#22914;&#20309;&#24433;&#21709;&#22312;&#25351;&#20196;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;7B&#22823;&#23567;&#30340;LLaMA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#37325;&#29616;&#20102;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Alpaca&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;LLaMA 1&#21644;LLaMA 2&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;PLW&#20043;&#38388;&#23384;&#22312;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#22312;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19981;&#21463;PLW&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Mistral 7B&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#36827;&#23637;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#23545;&#25552;&#21319;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#27604;&#20102;&#20854;&#22312;Tatabahasa&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13565</link><description>&lt;p&gt;
&#22522;&#20110;Mistral&#30340;&#22823;&#35268;&#27169;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#21319;&#26412;&#22320;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding. (arXiv:2401.13565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Mistral 7B&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39532;&#26469;&#35199;&#20122;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#36827;&#23637;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#23545;&#25552;&#21319;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#27604;&#20102;&#20854;&#22312;Tatabahasa&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Mistral 7B&#30340;&#39044;&#35757;&#32451;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#20351;&#29992;&#20102;32.6GB&#30340;&#25968;&#25454;&#38598;&#65292;&#30456;&#24403;&#20110;11&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#24067;&#20102;&#19978;&#19979;&#25991;&#38271;&#24230;&#20026;4096&#21644;32768&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;16384&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39532;&#26469;&#35199;&#20122;Mistral&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#25193;&#23637;&#19978;&#19979;&#25991;&#38271;&#24230;&#23545;Mistral 7B&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#19987;&#38376;&#35843;&#25972;&#20102;16384&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#25429;&#25417;&#24494;&#22937;&#35821;&#35328;&#32454;&#33410;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#23545;&#27604;&#20102;&#39532;&#26469;&#35199;&#20122;Mistral&#19982;ChatGPT3.5&#21644;Claude 2&#31561;&#33879;&#21517;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21576;&#29616;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#34920;&#26126;&#39532;&#26469;&#35199;&#20122;Mistral&#22312;Tatabahasa&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present significant advancements in the pretraining of Mistral 7B, a large-scale language model, using a dataset of 32.6 GB, equivalent to 1.1 billion tokens. We explore the impact of extending the context length, releasing models with context lengths of 4096 and 32768 tokens, and further refining performance with a specialized 16384 context length instruction-tuned model, we called it Malaysian Mistral.  Our experiments demonstrate the efficacy of continue pretraining and the influence of extended context lengths on Mistral 7B's language understanding capabilities. Additionally, we release a model specifically tuned with a 16384 context length instruction, showcasing its potential for capturing nuanced language intricacies.  Furthermore, our research contributes to the benchmarking of Malaysian Mistral against prominent language models, including ChatGPT3.5 and Claude 2. We present compelling results indicating Malaysian Mistral's superior performance on Tatabahasa (
&lt;/p&gt;</description></item><item><title>SpeechGPT-Gen&#26159;&#19968;&#20010;8&#20159;&#21442;&#25968;&#30340;&#35821;&#38899;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Chain-of-Information Generation&#26041;&#27861;&#26469;&#35299;&#32806;&#35821;&#20041;&#21644;&#24863;&#30693;&#20449;&#24687;&#65292;&#22312;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13527</link><description>&lt;p&gt;
SpeechGPT-Gen: &#32553;&#25918;&#20449;&#24687;&#38142;&#35821;&#38899;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation. (arXiv:2401.13527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13527
&lt;/p&gt;
&lt;p&gt;
SpeechGPT-Gen&#26159;&#19968;&#20010;8&#20159;&#21442;&#25968;&#30340;&#35821;&#38899;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Chain-of-Information Generation&#26041;&#27861;&#26469;&#35299;&#32806;&#35821;&#20041;&#21644;&#24863;&#30693;&#20449;&#24687;&#65292;&#22312;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20973;&#20511;&#26377;&#25928;&#30340;&#35821;&#38899;&#24314;&#27169;&#65292;&#24403;&#21069;&#30340;&#35821;&#38899;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLLMs&#65289;&#22312;&#19978;&#19979;&#25991;&#35821;&#38899;&#29983;&#25104;&#21644;&#23545;&#26410;&#35265;&#36807;&#30340;&#35828;&#35805;&#20154;&#30340;&#39640;&#25928;&#27867;&#21270;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20449;&#24687;&#24314;&#27169;&#36807;&#31243;&#21463;&#21040;&#19968;&#23450;&#20887;&#20313;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#35821;&#38899;&#29983;&#25104;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#38142;&#29983;&#25104;&#65288;CoIG&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#32806;&#22823;&#35268;&#27169;&#35821;&#38899;&#29983;&#25104;&#20013;&#30340;&#35821;&#20041;&#21644;&#24863;&#30693;&#20449;&#24687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SpeechGPT-Gen&#65292;&#19968;&#20010;8&#20159;&#21442;&#25968;&#30340;SLLM&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#35821;&#20041;&#21644;&#24863;&#30693;&#20449;&#24687;&#24314;&#27169;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#20449;&#24687;&#24314;&#27169;&#65292;&#20197;&#21450;&#19968;&#20010;&#20351;&#29992;&#27969;&#21305;&#37197;&#36827;&#34892;&#24863;&#30693;&#20449;&#24687;&#24314;&#27169;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23558;&#35821;&#20041;&#20449;&#24687;&#27880;&#20837;&#20808;&#39564;&#20998;&#24067;&#20197;&#22686;&#24378;&#27969;&#21305;&#37197;&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate th
&lt;/p&gt;</description></item><item><title>GPT-3.5&#34987;&#29992;&#20110;&#29983;&#25104;&#21644;&#26631;&#27880;&#21307;&#30103;&#25991;&#20214;&#20197;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#23545;ICD-10&#20195;&#30721;&#30340;&#32534;&#30721;&#24615;&#33021;&#33391;&#22909;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#25991;&#20214;&#22312;&#20020;&#24202;&#21487;&#25509;&#21463;&#24615;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#35748;&#21487;&#12290;</title><link>http://arxiv.org/abs/2401.13512</link><description>&lt;p&gt;
GPT-3.5&#33021;&#21542;&#29983;&#25104;&#21644;&#26631;&#27880;&#20986;&#38498;&#25688;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GPT-3.5 Generate and Code Discharge Summaries?. (arXiv:2401.13512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13512
&lt;/p&gt;
&lt;p&gt;
GPT-3.5&#34987;&#29992;&#20110;&#29983;&#25104;&#21644;&#26631;&#27880;&#21307;&#30103;&#25991;&#20214;&#20197;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#23545;ICD-10&#20195;&#30721;&#30340;&#32534;&#30721;&#24615;&#33021;&#33391;&#22909;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#25991;&#20214;&#22312;&#20020;&#24202;&#21487;&#25509;&#21463;&#24615;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#35748;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#25506;&#31350;GPT-3.5&#22312;&#29983;&#25104;&#21644;&#26631;&#27880;&#20855;&#26377;ICD-10&#20195;&#30721;&#30340;&#21307;&#30103;&#25991;&#20214;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#26631;&#31614;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#21033;&#29992;GPT-3.5&#22522;&#20110;MIMIC-IV&#25968;&#25454;&#38598;&#20013;&#32597;&#35265;&#65288;&#29983;&#25104;&#65289;&#20195;&#30721;&#30340;ICD-10&#20195;&#30721;&#25551;&#36848;&#21015;&#34920;&#29983;&#25104;&#21644;&#26631;&#27880;&#20102;9,606&#20221;&#20986;&#38498;&#25688;&#35201;&#12290;&#23558;&#20854;&#19982;&#22522;&#32447;&#35757;&#32451;&#38598;&#32467;&#21512;&#65292;&#24418;&#25104;&#19968;&#20010;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;&#20351;&#29992;&#31070;&#32463;&#32534;&#30721;&#27169;&#22411;&#22312;&#22522;&#32447;&#21644;&#22686;&#24378;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;MIMIC-IV&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20840;&#20195;&#30721;&#38598;&#12289;&#29983;&#25104;&#20195;&#30721;&#21450;&#20854;&#25152;&#23646;&#20195;&#30721;&#26063;&#30340;&#24494;&#35266;&#21644;&#23439;&#35266;F1&#24471;&#20998;&#12290;&#37319;&#29992;&#24369;&#23618;&#27425;&#28151;&#28102;&#30697;&#38453;&#26469;&#30830;&#23450;&#21518;&#38754;&#20004;&#20010;&#20195;&#30721;&#38598;&#20013;&#30340;&#20195;&#30721;&#26063;&#20869;&#21644;&#20195;&#30721;&#26063;&#22806;&#30340;&#32534;&#30721;&#38169;&#35823;&#12290;&#23545;GPT-3.5&#30340;&#32534;&#30721;&#24615;&#33021;&#36827;&#34892;&#20102;&#33258;&#34892;&#29983;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;MIMIC-IV&#25968;&#25454;&#30340;&#35780;&#20272;&#12290;&#20020;&#24202;&#19987;&#19994;&#20154;&#21592;&#23545;&#29983;&#25104;&#30340;&#25991;&#20214;&#36827;&#34892;&#20102;&#20020;&#24202;&#21487;&#25509;&#21463;&#24615;&#35780;&#20272;&#12290;&#32467;&#26524;&#21644;&#32467;&#35770;&#65306;&#22686;&#24378;&#24494;&#23567;
&lt;/p&gt;
&lt;p&gt;
Objective: To investigate GPT-3.5 in generating and coding medical documents with ICD-10 codes for data augmentation on low-resources labels.  Materials and Methods: Employing GPT-3.5 we generated and coded 9,606 discharge summaries based on lists of ICD-10 code descriptions of patients with infrequent (generation) codes within the MIMIC-IV dataset. Combined with the baseline training set, this formed an augmented training set. Neural coding models were trained on baseline and augmented data and evaluated on a MIMIC-IV test set. We report micro- and macro-F1 scores on the full codeset, generation codes, and their families. Weak Hierarchical Confusion Matrices were employed to determine within-family and outside-of-family coding errors in the latter codesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided self-generated data and real MIMIC-IV data. Clinical professionals evaluated the clinical acceptability of the generated documents.  Results: Augmentation slight
&lt;/p&gt;</description></item><item><title>AI&#24605;&#24819;&#23545;&#20010;&#20307;&#21019;&#36896;&#21147;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13481</link><description>&lt;p&gt;
AI&#24605;&#24819;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24605;&#24819;&#30340;&#21019;&#36896;&#21147;&#12289;&#22810;&#26679;&#24615;&#21644;&#36827;&#21270;&#65306;&#26469;&#33258;&#19968;&#20010;&#22823;&#35268;&#27169;&#21160;&#24577;&#23454;&#39564;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment. (arXiv:2401.13481v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13481
&lt;/p&gt;
&lt;p&gt;
AI&#24605;&#24819;&#23545;&#20010;&#20307;&#21019;&#36896;&#21147;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#25509;&#35302;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#35266;&#30475;&#21040;AI&#29983;&#25104;&#30340;&#24605;&#24819;&#23558;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24605;&#24819;&#65311;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23454;&#39564;&#65288;800+&#21442;&#19982;&#32773;&#65292;40+&#20010;&#22269;&#23478;&#65289;&#65292;&#21442;&#19982;&#32773;&#35266;&#30475;&#20102;&#26469;&#33258;ChatGPT&#25110;&#20043;&#21069;&#23454;&#39564;&#21442;&#19982;&#32773;&#30340;&#21019;&#24847;&#24605;&#24819;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#21019;&#24847;&#24605;&#32771;&#12290;&#25105;&#20204;&#21464;&#21270;&#20102;AI&#29983;&#25104;&#31034;&#20363;&#30340;&#25968;&#37327;&#65288;&#26080;&#12289;&#20302;&#12289;&#39640;&#26333;&#20809;&#65289;&#20197;&#21450;&#31034;&#20363;&#26159;&#21542;&#26631;&#35760;&#20026;&#8220;AI&#8221;&#65288;&#25259;&#38706;&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#23454;&#39564;&#35774;&#35745; - &#22312;&#21516;&#19968;&#23454;&#39564;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;&#20043;&#21069;&#21442;&#19982;&#32773;&#30340;&#24605;&#24819;&#20316;&#20026;&#26410;&#26469;&#21442;&#19982;&#32773;&#30340;&#21050;&#28608; - &#27169;&#25311;&#20102;&#25991;&#21270;&#21019;&#36896;&#30340;&#30456;&#20114;&#20381;&#36182;&#36807;&#31243;&#65306;&#21019;&#36896;&#24615;&#24605;&#24819;&#24314;&#31435;&#22312;&#20043;&#21069;&#30340;&#24605;&#24819;&#22522;&#30784;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25429;&#25417;&#21040;&#20102;LLM&#8220;&#22312;&#25991;&#21270;&#24490;&#29615;&#20013;&#8221;&#30340;&#22797;&#21512;&#25928;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;AI&#26333;&#20809;&#65288;&#20294;&#19981;&#26159;&#20302;AI&#26333;&#20809;&#65289;&#24182;&#27809;&#26377;&#24433;&#21709;&#20010;&#20154;&#24605;&#24819;&#30340;&#21019;&#36896;&#21147;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#24179;&#22343;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;AI&#20351;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#32047;&#31215;&#25928;&#24212;&#22686;&#24378;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- mimics the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made 
&lt;/p&gt;</description></item><item><title>SciMMIR&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#36890;&#36807;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.13478</link><description>&lt;p&gt;
SciMMIR:&#31185;&#23398;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#30340;&#22522;&#20934;&#35780;&#27979;
&lt;/p&gt;
&lt;p&gt;
SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval. (arXiv:2401.13478v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13478
&lt;/p&gt;
&lt;p&gt;
SciMMIR&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#36890;&#36807;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#65288;MMIR&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#36328;&#27169;&#24577;&#23545;&#40784;&#30740;&#31350;&#65292;&#22312;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#31185;&#23398;&#39046;&#22495;&#20869;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#30340;MMIR&#24615;&#33021;&#30340;&#24403;&#21069;&#22522;&#20934;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#23398;&#26415;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#22270;&#34920;&#21644;&#34920;&#26684;&#22270;&#20687;&#36890;&#24120;&#19981;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21033;&#29992;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#31185;&#23398;MMIR&#65288;SciMMIR&#65289;&#22522;&#20934;&#65292;&#20197;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;&#20102;530K&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#20174;&#31185;&#23398;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#36825;&#20123;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#26469;&#33258;&#20110;&#20855;&#26377;&#35814;&#32454;&#26631;&#39064;&#30340;&#31185;&#23398;&#25991;&#26723;&#20013;&#30340;&#22270;&#34920;&#21644;&#34920;&#26684;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20004;&#32423;&#23376;&#38598;-&#23376;&#31867;&#21035;&#23618;&#27425;&#27880;&#37322;&#23545;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#20197;&#20419;&#36827;&#23545;&#22522;&#20934;&#27169;&#22411;&#30340;&#26356;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations o
&lt;/p&gt;</description></item><item><title>SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2401.13463</link><description>&lt;p&gt;
SpeechDPR: &#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering. (arXiv:2401.13463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13463
&lt;/p&gt;
&lt;p&gt;
SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#38382;&#31572;(SQA)&#26159;&#26426;&#22120;&#36890;&#36807;&#22312;&#32473;&#23450;&#21475;&#35821;&#27573;&#33853;&#20013;&#25214;&#21040;&#31572;&#26696;&#33539;&#22260;&#26469;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#36807;&#21435;&#30340;SQA&#26041;&#27861;&#27809;&#26377;&#20351;&#29992;ASR&#65292;&#20197;&#36991;&#20813;&#35782;&#21035;&#38169;&#35823;&#21644;&#35789;&#27719;&#22806;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#24320;&#25918;&#39046;&#22495;SQA(openSQA)&#38382;&#39064;&#20013;&#65292;&#26426;&#22120;&#38656;&#35201;&#39318;&#20808;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#29992;&#20110;openSQA&#38382;&#39064;&#26816;&#32034;&#32452;&#20214;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;SpeechDPR&#12290;SpeechDPR&#36890;&#36807;&#20174;&#26080;&#30417;&#30563;ASR(UASR)&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;(TDR)&#30340;&#32423;&#32852;&#27169;&#22411;&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#23398;&#20064;&#21477;&#23376;&#32423;&#35821;&#20041;&#34920;&#31034;&#12290;&#19981;&#38656;&#35201;&#25163;&#21160;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#32423;&#32852;&#30340;UASR&#21644;TDR&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#26174;&#33879;&#25552;&#39640;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robus
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13444</link><description>&lt;p&gt;
&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65306;&#22522;&#20110;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption. (arXiv:2401.13444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#23427;&#20204;&#30340;&#30693;&#35782;&#38754;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#24403;&#38754;&#23545;&#19981;&#29087;&#24713;&#30340;&#26597;&#35810;&#26102;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23558;LLMs&#35270;&#20026;&#20027;&#35201;&#30340;&#20915;&#31574;&#32773;&#65292;&#23545;&#20854;&#33021;&#21147;&#25552;&#20986;&#20102;&#36739;&#39640;&#30340;&#35201;&#27714;&#12290;&#23545;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#30340;LLMs&#26469;&#35828;&#65292;&#36825;&#26159;&#19981;&#22826;&#21512;&#36866;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#20026;&#26680;&#24515;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65288;CGPE&#65289;&#65292;&#23427;&#23558;&#30693;&#35782;&#24211;&#19982;LLMs&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#35201;&#27714;&#36739;&#20302;&#12290;&#21463;&#20154;&#31867;&#25163;&#21160;&#26816;&#32034;&#30693;&#35782;&#30340;&#26041;&#27861;&#21551;&#21457;&#65292;CGPE&#21033;&#29992;&#38382;&#39064;&#20013;&#30340;&#20449;&#24687;&#20316;&#20026;&#32447;&#32034;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#30693;&#35782;&#24211;&#20013;&#25152;&#38656;&#30340;&#30693;&#35782;&#36335;&#24452;&#12290;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CGPE&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#29992;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#24615;&#33021;&#36739;&#24046;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, large language models (LLMs) have showcased remarkable capabilities. However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries. While integrating knowledge graphs with LLMs has been explored, existing approaches treat LLMs as primary decision-makers, imposing high demands on their capabilities. This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance. In this paper, we introduce a Clue-Guided Path Exploration framework (CGPE) that efficiently merges a knowledge base with an LLM, placing less stringent requirements on the model's capabilities. Inspired by the method humans use to manually retrieve knowledge, CGPE employs information from the question as clues to systematically explore the required knowledge path within the knowledge base. Experiments on open-source datasets reveal that CGPE outperforms previous methods and is highly applicable to LLMs w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#20998;&#31867;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31616;&#21270;&#20572;&#29992;&#35789;&#25552;&#21462;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#28151;&#21512;&#32479;&#35745;&#21644;&#35821;&#35328;&#23398;&#26041;&#27861;&#21019;&#24314;&#20840;&#38754;&#30340;&#20572;&#29992;&#35789;&#21015;&#34920;&#65292;&#25552;&#39640;&#20102;&#38750;&#27954;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.13398</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#21487;&#20197;&#22686;&#24378;&#39046;&#22495;&#26080;&#20851;&#30340;&#20572;&#29992;&#35789;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Text Categorization Can Enhance Domain-Agnostic Stopword Extraction. (arXiv:2401.13398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#20998;&#31867;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31616;&#21270;&#20572;&#29992;&#35789;&#25552;&#21462;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#28151;&#21512;&#32479;&#35745;&#21644;&#35821;&#35328;&#23398;&#26041;&#27861;&#21019;&#24314;&#20840;&#38754;&#30340;&#20572;&#29992;&#35789;&#21015;&#34920;&#65292;&#25552;&#39640;&#20102;&#38750;&#27954;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#20998;&#31867;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31616;&#21270;&#20572;&#29992;&#35789;&#25552;&#21462;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#21253;&#25324;&#27861;&#35821;&#22312;&#20869;&#30340;9&#31181;&#38750;&#27954;&#35821;&#35328;&#12290;&#36890;&#36807;&#21033;&#29992;MasakhaNEWS&#12289;African Stopwords Project&#21644;MasakhaPOS&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#25991;&#26412;&#20998;&#31867;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#22823;&#22810;&#25968;&#32771;&#23519;&#35821;&#35328;&#20013;&#36229;&#36807;80&#65285;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#20572;&#29992;&#35789;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#21464;&#24322;&#23548;&#33268;&#26576;&#20123;&#35821;&#35328;&#30340;&#35782;&#21035;&#29575;&#36739;&#20302;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#36229;&#36807;40&#65285;&#30340;&#20572;&#29992;&#35789;&#22312;&#21508;&#31867;&#26032;&#38395;&#20013;&#37117;&#26159;&#20849;&#21516;&#30340;&#65292;&#20294;&#23569;&#20110;15&#65285;&#30340;&#20572;&#29992;&#35789;&#26159;&#26576;&#19968;&#31867;&#21035;&#29420;&#26377;&#30340;&#12290;&#19981;&#24120;&#35265;&#30340;&#20572;&#29992;&#35789;&#22686;&#21152;&#20102;&#25991;&#26412;&#30340;&#28145;&#24230;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#34987;&#20998;&#31867;&#20026;&#20572;&#29992;&#35789;&#21017;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#32467;&#21512;&#32479;&#35745;&#21644;&#35821;&#35328;&#23398;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#20840;&#38754;&#30340;&#20572;&#29992;&#35789;&#21015;&#34920;&#65292;&#20984;&#26174;&#20102;&#25105;&#20204;&#28151;&#21512;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#39640;&#20102;&#38750;&#27954;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27700;&#24179;&#65292;&#24182;&#24378;&#35843;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the role of text categorization in streamlining stopword extraction in natural language processing (NLP), specifically focusing on nine African languages alongside French. By leveraging the MasakhaNEWS, African Stopwords Project, and MasakhaPOS datasets, our findings emphasize that text categorization effectively identifies domain-agnostic stopwords with over 80% detection success rate for most examined languages. Nevertheless, linguistic variances result in lower detection rates for certain languages. Interestingly, we find that while over 40% of stopwords are common across news categories, less than 15% are unique to a single category. Uncommon stopwords add depth to text but their classification as stopwords depends on context. Therefore combining statistical and linguistic approaches creates comprehensive stopword lists, highlighting the value of our hybrid method. This research enhances NLP for African languages and underscores the importance of text catego
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;InstructDoc&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;30&#20010;&#20855;&#26377;&#32479;&#19968;&#25351;&#20196;&#30340;VDU&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#25991;&#26723;&#38405;&#35835;&#21644;&#29702;&#35299;&#27169;&#22411;InstructDr&#26469;&#25552;&#39640;VDU&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;InstructDr&#21487;&#20197;&#36890;&#36807;&#32473;&#23450;&#30340;&#25351;&#20196;&#26377;&#25928;&#36866;&#24212;&#26032;&#30340;VDU&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#20855;&#20307;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;LLMs&#21644;ChatGPT&#12290;</title><link>http://arxiv.org/abs/2401.13313</link><description>&lt;p&gt;
InstructDoc&#65306;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#25351;&#20196;&#23454;&#29616;&#23545;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions. (arXiv:2401.13313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;InstructDoc&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;30&#20010;&#20855;&#26377;&#32479;&#19968;&#25351;&#20196;&#30340;VDU&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#25991;&#26723;&#38405;&#35835;&#21644;&#29702;&#35299;&#27169;&#22411;InstructDr&#26469;&#25552;&#39640;VDU&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;InstructDr&#21487;&#20197;&#36890;&#36807;&#32473;&#23450;&#30340;&#25351;&#20196;&#26377;&#25928;&#36866;&#24212;&#26032;&#30340;VDU&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#20855;&#20307;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;LLMs&#21644;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#20196;&#22312;&#30495;&#23454;&#19990;&#30028;&#25991;&#26723;&#19978;&#23436;&#25104;&#21508;&#31181;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#65288;VDU&#65289;&#20219;&#21153;&#65288;&#20363;&#22914;&#38382;&#31572;&#21644;&#20449;&#24687;&#25552;&#21462;&#65289;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructDoc&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;30&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;VDU&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#25910;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#20855;&#26377;&#32479;&#19968;&#26684;&#24335;&#30340;&#22810;&#26679;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;12&#20010;&#19981;&#21516;&#20219;&#21153;&#21644;&#21253;&#25324;&#24320;&#25918;&#25991;&#26723;&#31867;&#22411;/&#26684;&#24335;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#22312;VDU&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#25991;&#26723;&#38405;&#35835;&#19982;&#29702;&#35299;&#27169;&#22411;InstructDr&#65292;&#23427;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#26725;&#25509;&#27169;&#22359;&#36830;&#25509;&#25991;&#26723;&#22270;&#20687;&#12289;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;InstructDr&#33021;&#22815;&#36890;&#36807;&#32473;&#23450;&#30340;&#25351;&#20196;&#26377;&#25928;&#36866;&#24212;&#26032;&#30340;VDU&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#20855;&#20307;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;LLMs&#21644;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of completing various visual document understanding (VDU) tasks, e.g., question answering and information extraction, on real-world documents through human-written instructions. To this end, we propose InstructDoc, the first large-scale collection of 30 publicly available VDU datasets, each with diverse instructions in a unified format, which covers a wide range of 12 tasks and includes open document types/formats. Furthermore, to enhance the generalization performance on VDU tasks, we design a new instruction-based document reading and understanding model, InstructDr, that connects document images, image encoders, and large language models (LLMs) through a trainable bridging module. Experiments demonstrate that InstructDr can effectively adapt to new VDU datasets, tasks, and domains via given instructions and outperforms existing multimodal LLMs and ChatGPT without specific training.
&lt;/p&gt;</description></item><item><title>MaLA-500&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35774;&#35745;&#29992;&#20110;&#35206;&#30422;534&#31181;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#22312;LLaMA 2&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.13303</link><description>&lt;p&gt;
MaLA-500: &#22823;&#35268;&#27169;&#35821;&#35328;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MaLA-500: Massive Language Adaptation of Large Language Models. (arXiv:2401.13303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13303
&lt;/p&gt;
&lt;p&gt;
MaLA-500&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35774;&#35745;&#29992;&#20110;&#35206;&#30422;534&#31181;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#22312;LLaMA 2&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20027;&#35201;&#35774;&#35745;&#38024;&#23545;&#33521;&#35821;&#25110;&#19968;&#23567;&#37096;&#20998;&#35821;&#35328;&#65292;&#20854;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25928;&#26524;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MaLA-500&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35774;&#35745;&#29992;&#20110;&#35206;&#30422;534&#31181;&#35821;&#35328;&#12290;&#20026;&#20102;&#35757;&#32451;MaLA-500&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#35789;&#27719;&#25193;&#23637;&#21644;&#22312;LLaMA 2&#19978;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;SIB-200&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MaLA-500&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#32467;&#26524;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#25105;&#20204;&#22312;https://huggingface.co/MaLA-LM&#19978;&#21457;&#24067;&#20102;MaLA-500&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have advanced the state of the art in natural language processing. However, their predominant design for English or a limited set of languages creates a substantial gap in their effectiveness for low-resource languages. To bridge this gap, we introduce MaLA-500, a novel large language model designed to cover an extensive range of 534 languages. To train MaLA-500, we employ vocabulary extension and continued pretraining on LLaMA 2 with Glot500-c. Our experiments on SIB-200 show that MaLA-500 achieves state-of-the-art in-context learning results. We release MaLA-500 at https://huggingface.co/MaLA-LM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#36777;&#35770;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#25512;&#29702;&#26377;&#23475;&#21644;&#26080;&#23475;&#31435;&#22330;&#20043;&#38388;&#30340;&#30456;&#20114;&#30683;&#30462;&#29702;&#30001;&#65292;&#29983;&#25104;&#21487;&#35835;&#30340;&#35299;&#37322;&#65292;&#25552;&#21319;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.13298</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#36777;&#35770;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models. (arXiv:2401.13298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#36777;&#35770;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#25512;&#29702;&#26377;&#23475;&#21644;&#26080;&#23475;&#31435;&#22330;&#20043;&#38388;&#30340;&#30456;&#20114;&#30683;&#30462;&#29702;&#30001;&#65292;&#29983;&#25104;&#21487;&#35835;&#30340;&#35299;&#37322;&#65292;&#25552;&#21319;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#26102;&#20195;&#20805;&#26021;&#30528;&#20114;&#32852;&#32593;&#27169;&#22240;&#65292;&#38656;&#35201;&#26126;&#30830;&#25484;&#25569;&#21644;&#26377;&#25928;&#35782;&#21035;&#26377;&#23475;&#27169;&#22240;&#12290;&#30001;&#20110;&#27169;&#22240;&#20013;&#34164;&#21547;&#30340;&#38544;&#21547;&#21547;&#20041;&#19981;&#33021;&#36890;&#36807;&#34920;&#38754;&#25991;&#26412;&#21644;&#22270;&#20687;&#26126;&#30830;&#20256;&#36798;&#65292;&#36825;&#19968;&#20219;&#21153;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#26041;&#27861;&#26410;&#25552;&#20379;&#21487;&#35835;&#30340;&#35299;&#37322;&#20197;&#25581;&#31034;&#36825;&#31181;&#38544;&#21547;&#21547;&#20041;&#20197;&#25903;&#25345;&#20854;&#26816;&#27979;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26377;&#23475;&#27169;&#22240;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#29702;&#26377;&#23475;&#21644;&#26080;&#23475;&#31435;&#22330;&#20043;&#38388;&#30340;&#30456;&#20114;&#30683;&#30462;&#29702;&#30001;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#36777;&#35770;&#65292;&#29983;&#25104;&#22522;&#20110;&#30683;&#30462;&#35770;&#25454;&#30340;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31934;&#35843;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36777;&#35770;&#35009;&#21028;&#26469;&#25512;&#26029;&#26377;&#23475;&#24615;&#65292;&#20197;&#20419;&#36827;&#26377;&#23475;&#21644;&#26080;&#23475;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The age of social media is flooded with Internet memes, necessitating a clear grasp and effective identification of harmful ones. This task presents a significant challenge due to the implicit meaning embedded in memes, which is not explicitly conveyed through the surface text and image. However, existing harmful meme detection methods do not present readable explanations that unveil such implicit meaning to support their detection decisions. In this paper, we propose an explainable approach to detect harmful memes, achieved through reasoning over conflicting rationales from both harmless and harmful positions. Specifically, inspired by the powerful capacity of Large Language Models (LLMs) on text generation and reasoning, we first elicit multimodal debate between LLMs to generate the explanations derived from the contradictory arguments. Then we propose to fine-tune a small language model as the debate judge for harmfulness inference, to facilitate multimodal fusion between the harmfu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;"I don't know"&#65288;Idk&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;AI&#21161;&#25163;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2401.13275</link><description>&lt;p&gt;
AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;?
&lt;/p&gt;
&lt;p&gt;
Can AI Assistants Know What They Don't Know?. (arXiv:2401.13275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;"I don't know"&#65288;Idk&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;AI&#21161;&#25163;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;AI&#21161;&#25163;&#22312;&#23545;&#35805;&#12289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#32534;&#20889;&#20195;&#30721;&#21644;&#20351;&#29992;&#24037;&#20855;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#28145;&#20837;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#22312;&#38754;&#23545;&#26576;&#20123;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65289;&#26102;&#20173;&#28982;&#20250;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#12290;AI&#21161;&#25163;&#30340;&#36825;&#31181;&#19981;&#30495;&#23454;&#22238;&#31572;&#21487;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36896;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;AI&#21161;&#25163;&#25298;&#32477;&#22238;&#31572;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#38382;&#39064;&#26159;&#20943;&#23569;&#24187;&#35273;&#21644;&#20351;&#21161;&#25163;&#30495;&#23454;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#8220;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#21161;&#25163;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;&#8220;I don't know&#8221;(Idk)&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#38382;&#39064;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#21161;&#25163;&#19982;&#20854;&#30456;&#24212;&#30340;Idk&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools. Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications. We believe that an AI assistant's refusal to answer questions it does not know is a crucial method for reducing hallucinations and making the assistant truthful. Therefore, in this paper, we ask the question "Can AI assistants know what they don't know and express them through natural language?" To answer this question, we construct a model-specific "I don't know" (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets. Then we align the assistant with its corresponding I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MF-AED-AEC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#12289;ASR&#38169;&#35823;&#26816;&#27979;&#21644;ASR&#38169;&#35823;&#20462;&#27491;&#65292;&#36890;&#36807;&#22686;&#24378;ASR&#25991;&#26412;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#26469;&#25913;&#36827;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13260</link><description>&lt;p&gt;
MF-AED-AEC: &#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#12289;ASR&#38169;&#35823;&#26816;&#27979;&#21644;ASR&#38169;&#35823;&#20462;&#27491;&#23454;&#29616;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, ASR Error Detection, and ASR Error Correction. (arXiv:2401.13260v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MF-AED-AEC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#12289;ASR&#38169;&#35823;&#26816;&#27979;&#21644;ASR&#38169;&#35823;&#20462;&#27491;&#65292;&#36890;&#36807;&#22686;&#24378;ASR&#25991;&#26412;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#26469;&#25913;&#36827;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#20013;&#26222;&#36941;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#25972;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#20449;&#24687;&#26469;&#20840;&#38754;&#35782;&#21035;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#65292;&#20854;&#20013;&#25991;&#26412;&#36890;&#24120;&#36890;&#36807;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#33719;&#21462;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#65292;&#25991;&#26412;&#27169;&#24577;&#20013;&#30340;ASR&#38169;&#35823;&#20250;&#20351;SER&#30340;&#24615;&#33021;&#21464;&#24046;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#36741;&#21161;&#30340;ASR&#38169;&#35823;&#26816;&#27979;&#20219;&#21153;&#26469;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;ASR&#20551;&#35774;&#20013;&#27599;&#20010;&#35789;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#25913;&#36827;&#28508;&#21147;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#35299;&#20915;&#25991;&#26412;&#20013;&#35821;&#20041;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#22266;&#26377;&#24322;&#36136;&#24615;&#23548;&#33268;&#23427;&#20204;&#34920;&#31034;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#20351;&#23427;&#20204;&#30340;&#34701;&#21512;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#36741;&#21161;&#20219;&#21153;&#65292;ASR&#38169;&#35823;&#26816;&#27979;&#65288;AED&#65289;&#21644;ASR&#38169;&#35823;&#20462;&#27491;&#65288;AEC&#65289;&#65292;&#20197;&#22686;&#24378;ASR&#25991;&#26412;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent approach in speech emotion recognition (SER) involves integrating both audio and textual information to comprehensively identify the speaker's emotion, with the text generally obtained through automatic speech recognition (ASR). An essential issue of this approach is that ASR errors from the text modality can worsen the performance of SER. Previous studies have proposed using an auxiliary ASR error detection task to adaptively assign weights of each word in ASR hypotheses. However, this approach has limited improvement potential because it does not address the coherence of semantic information in the text. Additionally, the inherent heterogeneity of different modalities leads to distribution gaps between their representations, making their fusion challenging. Therefore, in this paper, we incorporate two auxiliary tasks, ASR error detection (AED) and ASR error correction (AEC), to enhance the semantic coherence of ASR text, and further introduce a novel multi-modal fusion 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.13256</link><description>&lt;p&gt;
UniMS-RAG: &#29992;&#20110;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems. (arXiv:2401.13256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#28041;&#21450;&#21040;&#22810;&#20010;&#20449;&#24687;&#28304;&#26102;&#65292;&#20010;&#24615;&#21270;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#21521;&#24448;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35745;&#21010;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#22797;&#20013;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30456;&#21516;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#33539;&#24335;&#23558;&#36825;&#19977;&#20010;&#23376;&#20219;&#21153;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#30340;&#20196;&#29260;&#65292;&#21363;&#34892;&#21160;&#20196;&#29260;&#21644;&#35780;&#20272;&#20196;&#29260;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#24182;&#35780;&#20272;&#20851;&#32852;&#24615;&#12290;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34892;&#21160;&#20196;&#29260;&#26377;&#21161;&#20110;&#19982;&#21508;&#31181;&#30693;&#35782;&#28304;&#36827;&#34892;&#20132;&#20114;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20854;&#19978;&#19979;&#25991;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their
&lt;/p&gt;</description></item><item><title>SEER&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#26469;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13246</link><description>&lt;p&gt;
SEER: &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning. (arXiv:2401.13246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13246
&lt;/p&gt;
&lt;p&gt;
SEER&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#26469;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38416;&#26126;&#20174;&#38382;&#39064;&#21040;&#31572;&#26696;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#35299;&#37322;&#26159;&#26681;&#26412;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#26174;&#33879;&#22686;&#24378;&#20102;&#38382;&#31572;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#32467;&#26500;&#21270;&#35299;&#37322;&#35201;&#27714;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#25512;&#29702;&#65292;&#36825;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#22312;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#21333;&#27493;&#25512;&#29702;&#65292;&#24573;&#35270;&#27493;&#39588;&#20043;&#38388;&#30340;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#32467;&#26500;&#21270;&#20851;&#31995;&#65292;&#38459;&#30861;&#20102;RL&#22312;&#32467;&#26500;&#21270;&#25512;&#29702;&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEER&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#65292;&#20197;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#20934;&#30830;&#25551;&#36848;&#20102;&#32467;&#26500;&#21270;&#25512;&#29702;&#20013;&#22266;&#26377;&#30340;&#20998;&#23618;&#21644;&#20998;&#25903;&#32467;&#26500;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#29366;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Elucidating the reasoning process with structured explanations from question to answer is fundamentally crucial, as it significantly enhances the interpretability and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricate structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Meanwhile, existing reinforcement learning (RL)-based methods overlook the structured relationships, impeding RL's potential in structured reasoning. In this paper, we propose SEER, a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between states. We also introduce a fine-grained reward function
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20248;&#21270;&#20154;&#31867;&#27880;&#37322;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#26041;&#27861;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#32780;&#35813;&#26041;&#27861;&#23558;&#32771;&#34385;&#36825;&#20123;&#22240;&#32032;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36873;&#25321;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13229</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#21040;&#26377;&#20449;&#24687;&#36873;&#25321;&#25968;&#25454;&#65306;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#20248;&#21270;&#20154;&#31867;&#27880;&#37322;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning. (arXiv:2401.13229v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13229
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20248;&#21270;&#20154;&#31867;&#27880;&#37322;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#26041;&#27861;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#32780;&#35813;&#26041;&#27861;&#23558;&#32771;&#34385;&#36825;&#20123;&#22240;&#32032;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36873;&#25321;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#19968;&#31181;&#36873;&#25321;&#26159;&#20351;&#29992;&#20247;&#21253;&#24179;&#21488;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#20247;&#21253;&#24341;&#20837;&#20102;&#19982;&#27880;&#37322;&#32773;&#30340;&#32463;&#39564;&#12289;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#31181;&#36873;&#25321;&#26159;&#20351;&#29992;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#20294;&#19982;&#23569;&#26679;&#26412;&#25110;&#23436;&#20840;&#30417;&#30563;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#38646;&#26679;&#26412;&#26041;&#27861;&#26377;&#20854;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#21160;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#25968;&#25454;&#20005;&#37325;&#21463;&#38480;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#36866;&#24212;&#12290;&#22240;&#27492;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20154;&#31867;&#38543;&#26426;&#27880;&#37322;&#19968;&#32452;&#25968;&#25454;&#28857;&#26469;&#26500;&#24314;&#21021;&#22987;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#25277;&#26679;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23427;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#24403;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#26102;&#65292;&#24773;&#20917;&#26356;&#21152;&#31967;&#31957;&#65292;&#22240;&#20026;&#38543;&#26426;&#25277;&#26679;&#20542;&#21521;&#20110;&#20005;&#37325;&#20559;&#21521;&#22810;&#25968;&#31867;&#21035;&#65292;&#23548;&#33268;&#36807;&#22810;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in Natural Language Processing is obtaining annotated data for supervised learning. An option is the use of crowdsourcing platforms for data annotation. However, crowdsourcing introduces issues related to the annotator's experience, consistency, and biases. An alternative is to use zero-shot methods, which in turn have limitations compared to their few-shot or fully supervised counterparts. Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data. The most common approaches therefore involve the human itself randomly annotating a set of datapoints to build initial datasets. But randomly sampling data to be annotated is often inefficient as it ignores the characteristics of the data and the specific needs of the model. The situation worsens when working with imbalanced datasets, as random sampling tends to heavily bias towards the majority classes, leading to excessive annotated data. To
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13227</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22270;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#20449;&#24687;&#32473;&#36825;&#19968;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;LPNL&#65288;Link Prediction via Natural Language&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#30340;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#22270;&#32454;&#33410;&#30340;&#21019;&#26032;&#25552;&#31034;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37319;&#26679;&#27969;&#31243;&#65292;&#20174;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#25511;&#21046;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#22312;&#39044;&#23450;&#38480;&#21046;&#20869;&#65292;&#35299;&#20915;&#20102;&#20449;&#24687;&#36807;&#36733;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#22823;&#22411;&#20844;&#20849;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;LPNL&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21508;&#31181;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
&lt;/p&gt;</description></item><item><title>TAT-LLM&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#32780;&#20026;&#20102;&#24212;&#23545;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TAT-LLM&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#36739;&#23567;LLM&#12290;</title><link>http://arxiv.org/abs/2401.13223</link><description>&lt;p&gt;
TAT-LLM: &#19968;&#31181;&#38024;&#23545;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. (arXiv:2401.13223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13223
&lt;/p&gt;
&lt;p&gt;
TAT-LLM&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#32780;&#20026;&#20102;&#24212;&#23545;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TAT-LLM&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#36739;&#23567;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#38382;&#31572;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;Web&#19978;&#38750;&#24120;&#24120;&#35265;&#65288;&#22914;SEC&#25991;&#20214;&#65289;&#65292;&#36890;&#24120;&#38656;&#35201;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#22810;&#27493;&#39588;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38754;&#21521;&#34920;&#26684;&#21644;&#25991;&#26412;&#38382;&#31572;&#30340;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#25277;&#35937;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#19977;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#24182;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20221;&#25351;&#20196;&#26469;&#23454;&#20363;&#21270;&#35813;&#27969;&#27700;&#32447;&#24182;&#39564;&#35777;GPT-4&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#20687;GPT-4&#36825;&#26679;&#30340;&#22312;&#32447;LLM&#23384;&#22312;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#21508;&#31181;&#25361;&#25112;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#24320;&#21457;&#36739;&#23567;&#30340;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29616;&#26377;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#33258;&#21160;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;LLaMA 2&#36827;&#34892;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;TAT-LLM&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-w
&lt;/p&gt;</description></item><item><title>ULTRA&#26159;&#19968;&#31181;&#23618;&#32423;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#20013;&#36827;&#34892;&#32463;&#27982;&#39640;&#25928;&#30340;&#22788;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#21644;&#20505;&#36873;&#35770;&#35777;&#38598;&#21512;&#30340;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13218</link><description>&lt;p&gt;
ULTRA:&#36890;&#36807;&#23618;&#32423;&#24314;&#27169;&#21644;&#36880;&#23545;&#20248;&#21270;&#37322;&#25918;LLMs&#22312;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
ULTRA: Unleash LLMs' Potential for Event Argument Extraction through Hierarchical Modeling and Pair-wise Refinement. (arXiv:2401.13218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13218
&lt;/p&gt;
&lt;p&gt;
ULTRA&#26159;&#19968;&#31181;&#23618;&#32423;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#20013;&#36827;&#34892;&#32463;&#27982;&#39640;&#25928;&#30340;&#22788;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#21644;&#20505;&#36873;&#35770;&#35777;&#38598;&#21512;&#30340;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20107;&#20214;&#22312;&#35805;&#35821;&#20013;&#36827;&#34892;&#32467;&#26500;&#21270;&#25552;&#21462;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20132;&#27969;&#27169;&#24335;&#21644;&#34892;&#20026;&#36235;&#21183;&#12290;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65288;EAE&#65289;&#26159;&#20107;&#20214;&#20013;&#24515;&#29702;&#35299;&#30340;&#26680;&#24515;&#20219;&#21153;&#65292;&#20854;&#20219;&#21153;&#26159;&#20026;&#32473;&#23450;&#20107;&#20214;&#35782;&#21035;&#29305;&#23450;&#35282;&#33394;&#30340;&#25991;&#26412;&#33539;&#22260;&#65288;&#21363;&#35770;&#35777;&#65289;&#12290;&#25991;&#26723;&#32423;EAE&#65288;DocEAE&#65289;&#20391;&#37325;&#20110;&#25955;&#24067;&#22312;&#25972;&#20010;&#25991;&#26723;&#20013;&#30340;&#35770;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65292;&#20363;&#22914;Flan-UL2&#65289;&#22312;DocEAE&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ULTRA&#65292;&#19968;&#31181;&#23618;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#21152;&#32463;&#27982;&#39640;&#25928;&#22320;&#25552;&#21462;&#20107;&#20214;&#35770;&#35777;&#65292;&#20174;&#32780;&#22312;&#26041;&#27861;&#20013;&#21482;&#38656;&#35201;&#23569;&#20110;50&#20010;&#27880;&#37322;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35775;&#38382;&#26114;&#36149;&#30340;API&#31471;&#28857;&#12290;&#27492;&#22806;&#65292;&#23427;&#32531;&#35299;&#20102;LLMs&#22266;&#26377;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;ULTRA&#39318;&#20808;&#39034;&#24207;&#38405;&#35835;&#25991;&#26723;&#30340;&#25991;&#26412;&#22359;&#20197;&#29983;&#25104;&#20505;&#36873;&#35770;&#35777;&#38598;&#21512;&#65292;&#38543;&#21518;&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#23398;&#20064;&#25918;&#24323;&#38750;&#30456;&#20851;&#30340;&#20505;&#36873;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Structural extraction of events within discourse is critical since it avails a deeper understanding of communication patterns and behavior trends. Event argument extraction (EAE), at the core of event-centric understanding, is the task of identifying role-specific text spans (i.e., arguments) for a given event. Document-level EAE (DocEAE) focuses on arguments that are scattered across an entire document. In this work, we explore the capabilities of open source Large Language Models (LLMs), i.e., Flan-UL2, for the DocEAE task. To this end, we propose ULTRA, a hierarchical framework that extracts event arguments more cost-effectively -- the method needs as few as 50 annotations and doesn't require hitting costly API endpoints. Further, it alleviates the positional bias issue intrinsic to LLMs. ULTRA first sequentially reads text chunks of a document to generate a candidate argument set, upon which ULTRA learns to drop non-pertinent candidates through self-refinement. We further introduce
&lt;/p&gt;</description></item><item><title>MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13201</link><description>&lt;p&gt;
MLLMReID: &#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13201
&lt;/p&gt;
&lt;p&gt;
MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20154;&#29289;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;&#22914;&#20309;&#23558;&#23427;&#20204;&#36866;&#24212;&#20110;ReID&#20219;&#21153;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#24819;&#27861;&#26159;&#20351;&#29992;ReID&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;ReID&#30340;&#20027;&#24178;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#20004;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20026;ReID&#35774;&#35745;&#25351;&#20196;&#26102;&#65292;MLLM&#21487;&#33021;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#25351;&#20196;&#65292;&#32780;&#35774;&#35745;&#21508;&#31181;&#25351;&#20196;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#25104;&#26412;&#12290;&#65288;2&#65289;LLM&#30340;&#28508;&#22312;&#22270;&#20687;&#29305;&#24449;&#21521;&#37327;&#27809;&#26377;&#21442;&#19982;&#25439;&#22833;&#35745;&#31639;&#12290;&#25351;&#20196;&#23398;&#20064;&#65292;&#23545;&#40784;&#22270;&#20687;-&#25991;&#26412;&#29305;&#24449;&#65292;&#23548;&#33268;&#38388;&#25509;&#20248;&#21270;&#21644;&#23398;&#20064;&#30446;&#26631;&#19981;&#20805;&#20998;&#21033;&#29992;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#20154;&#29289;&#29305;&#24449;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MLLMReID&#65306;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;ReID&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#20849;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instru
&lt;/p&gt;</description></item><item><title>AgentBoard&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;&#20998;&#26512;&#35780;&#20272;LLM&#26234;&#33021;&#20307;&#32780;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#36718;&#20132;&#20114;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#12290;</title><link>http://arxiv.org/abs/2401.13178</link><description>&lt;p&gt;
AgentBoard: &#19968;&#31181;&#22810;&#36718;LLM&#26234;&#33021;&#20307;&#30340;&#20998;&#26512;&#35780;&#20272;&#26495;
&lt;/p&gt;
&lt;p&gt;
AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. (arXiv:2401.13178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13178
&lt;/p&gt;
&lt;p&gt;
AgentBoard&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;&#20998;&#26512;&#35780;&#20272;LLM&#26234;&#33021;&#20307;&#32780;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#36718;&#20132;&#20114;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#26234;&#33021;&#20307;&#23545;&#20110;&#29702;&#35299;&#20854;&#33021;&#21147;&#24182;&#20419;&#36827;&#20854;&#34701;&#20837;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36807;&#31243;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#22312;&#32479;&#19968;&#26694;&#26550;&#20869;&#23545;&#26234;&#33021;&#20307;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#29305;&#21035;&#26159;&#22312;&#32500;&#25252;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#21644;&#30830;&#20445;&#22810;&#36718;&#20132;&#20114;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26694;&#26550;&#20027;&#35201;&#20851;&#27880;&#26368;&#32456;&#25104;&#21151;&#29575;&#65292;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#35265;&#35299;&#24456;&#23569;&#65292;&#26080;&#27861;&#28145;&#20837;&#29702;&#35299;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AgentBoard&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#32508;&#21512;&#22522;&#20934;&#21644;&#20276;&#38543;&#30340;&#24320;&#28304;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;LLM&#26234;&#33021;&#20307;&#30340;&#20998;&#26512;&#35780;&#20272;&#32780;&#35774;&#35745;&#12290;AgentBoard&#25552;&#20379;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#65292;&#25429;&#25417;&#36880;&#27493;&#30340;&#36827;&#23637;&#65292;&#20197;&#21450;&#19968;&#20010;&#32508;&#21512;&#30340;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20855;&#26377;&#26131;&#20110;&#35780;&#20272;&#21644;&#20998;&#26512;&#27169;&#22411;&#33021;&#21147;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assess
&lt;/p&gt;</description></item><item><title>CFMatch&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23558;&#33258;&#21160;&#31572;&#26696;&#31561;&#20215;&#35780;&#20272;&#19982;&#20154;&#24037;&#19987;&#23478;&#21028;&#26029;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#21335;&#24182;&#24341;&#20837;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#36731;&#37327;&#32423;&#30340;&#21028;&#21035;&#24335;AE&#20998;&#31867;&#22120;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#24403;&#21069;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13170</link><description>&lt;p&gt;
CFMatch: &#23558;&#33258;&#21160;&#31572;&#26696;&#31561;&#20215;&#35780;&#20272;&#19982;&#20154;&#24037;&#19987;&#23478;&#21028;&#26029;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering. (arXiv:2401.13170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13170
&lt;/p&gt;
&lt;p&gt;
CFMatch&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23558;&#33258;&#21160;&#31572;&#26696;&#31561;&#20215;&#35780;&#20272;&#19982;&#20154;&#24037;&#19987;&#23478;&#21028;&#26029;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#21335;&#24182;&#24341;&#20837;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#36731;&#37327;&#32423;&#30340;&#21028;&#21035;&#24335;AE&#20998;&#31867;&#22120;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#24403;&#21069;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#30340;&#24773;&#20917;&#19979;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#38382;&#31572;&#31034;&#20363;&#65292;&#24403;&#21069;&#29992;&#20110;&#30830;&#23450;&#31572;&#26696;&#31561;&#20215;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26356;&#20887;&#38271;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#31572;&#26696;&#12290;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#65306;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#21487;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#24037;&#35780;&#21028;&#21592;&#30456;&#20851;&#32852;&#65292;&#20294;&#36825;&#20010;&#20219;&#21153;&#21482;&#22312;&#26377;&#38480;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21363;&#20351;&#21487;&#29992;&#65292;&#23545;&#27169;&#22411;&#30340;&#26356;&#26032;&#20063;&#26377;&#38480;&#65292;&#22240;&#20026;LLM&#36807;&#22823;&#19988;&#24448;&#24448;&#26114;&#36149;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#19968;&#33268;&#30340;&#25351;&#21335;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#25351;&#21335;&#29992;&#20110;&#20174;&#19987;&#19994;&#20154;&#24037;&#38382;&#31572;&#27604;&#36187;&#20013;&#37319;&#32435;&#26426;&#22120;&#38382;&#31572;&#22312;&#31572;&#26696;&#31561;&#20215;&#24615;&#35780;&#20272;&#26041;&#38754;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#20934;&#35780;&#20272;&#21644;&#19968;&#31181;&#26356;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#36731;&#37327;&#32423;&#30340;&#21028;&#21035;&#24335;AE&#20998;&#31867;&#22120;&#21305;&#37197;&#26041;&#27861;&#65288;CFMatch&#65292;&#22823;&#23567;&#23567;&#20110;1MB&#65289;&#65292;&#32463;&#36807;&#35757;&#32451;&#21644;&#39564;&#35777;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence (AE) often do not align with human judgments, particularly more verbose, free-form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big: LLM-based scorers can correlate better with human judges, but this task has only been tested on limited QA datasets, and even when available, update of the model is limited because LLMs are large and often expensive. We rectify both of these issues by providing clear and consistent guidelines for evaluating AE in machine QA adopted from professional human QA contests. We also introduce a combination of standard evaluation and a more efficient, robust, and lightweight discriminate AE classifier-based matching method (CFMatch, smaller than 1 MB), trained and validated to more accurately evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#30456;&#20851;&#38169;&#35823;&#65292;&#20197;&#23391;&#21152;&#25289;&#35821;&#20026;&#20363;&#65292;&#35752;&#35770;&#20102;&#24615;&#21035;&#30340;&#20551;&#35774;&#21644;&#25512;&#26029;&#65292;&#20197;&#21450;&#36825;&#20123;&#38169;&#35823;&#23548;&#33268;&#30340;&#21518;&#27542;&#27665;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#25552;&#21319;&#35821;&#35328;&#22320;&#20301;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.13165</link><description>&lt;p&gt;
&#22312;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;&#26426;&#22120;&#32763;&#35793;&#20013;&#31216;&#21628;&#38169;&#35823;&#21644;&#24615;&#21035;&#20551;&#35774;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Misgendering and Assuming Gender in Machine Translation when Working with Low-Resource Languages. (arXiv:2401.13165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#30456;&#20851;&#38169;&#35823;&#65292;&#20197;&#23391;&#21152;&#25289;&#35821;&#20026;&#20363;&#65292;&#35752;&#35770;&#20102;&#24615;&#21035;&#30340;&#20551;&#35774;&#21644;&#25512;&#26029;&#65292;&#20197;&#21450;&#36825;&#20123;&#38169;&#35823;&#23548;&#33268;&#30340;&#21518;&#27542;&#27665;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#25552;&#21319;&#35821;&#35328;&#22320;&#20301;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#35770;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#30456;&#20851;&#38169;&#35823;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#37322;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#31181;&#35821;&#35328;&#23618;&#32423;&#30340;&#31038;&#20250;&#21644;&#35745;&#31639;&#22240;&#32032;&#30340;&#19981;&#21487;&#20998;&#21106;&#24615;&#12290;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#27597;&#35821;&#23391;&#21152;&#25289;&#35821;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#28304;&#25991;&#26412;&#20013;&#27809;&#26377;&#25552;&#20379;&#30456;&#24212;&#20449;&#24687;&#26102;&#65292;&#24615;&#21035;&#22914;&#20309;&#34987;&#20551;&#35774;&#21644;&#25512;&#26029;&#20986;&#26469;&#65292;&#24182;&#20197;&#39640;&#36164;&#28304;&#33521;&#35821;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#38169;&#35823;&#23548;&#33268;&#35821;&#35328;&#28040;&#22833;&#21644;&#34920;&#24449;&#20260;&#23475;&#30340;&#21518;&#27542;&#27665;&#21644;&#31038;&#20250;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;&#25552;&#21319;&#35821;&#35328;&#22320;&#20301;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#36171;&#20104;&#35821;&#35328;&#26356;&#22810;&#30340;&#26435;&#23041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter focuses on gender-related errors in machine translation (MT) in the context of low-resource languages. We begin by explaining what low-resource languages are, examining the inseparable social and computational factors that create such linguistic hierarchies. We demonstrate through a case study of our mother tongue Bengali, a global language spoken by almost 300 million people but still classified as low-resource, how gender is assumed and inferred in translations to and from the high(est)-resource English when no such information is provided in source texts. We discuss the postcolonial and societal impacts of such errors leading to linguistic erasure and representational harms, and conclude by discussing potential solutions towards uplifting languages by providing them more agency in MT conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;SpacTor-T5&#65292;&#32467;&#21512;&#20102;&#36328;&#24230;&#30772;&#22351;&#21644;&#26367;&#25442;&#35789;&#27719;&#26816;&#27979;&#30340;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;&#20004;&#38454;&#27573;&#35838;&#31243;&#34920;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SpacTor-T5&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;SC&#39044;&#35757;&#32451;&#30456;&#21516;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#21644;&#24635;FLOP&#12290;</title><link>http://arxiv.org/abs/2401.13160</link><description>&lt;p&gt;
SpacTor-T5&#65306;&#20351;&#29992;&#36328;&#24230;&#30772;&#22351;&#21644;&#26367;&#25442;&#35789;&#27719;&#26816;&#27979;&#36827;&#34892;T5&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection. (arXiv:2401.13160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;SpacTor-T5&#65292;&#32467;&#21512;&#20102;&#36328;&#24230;&#30772;&#22351;&#21644;&#26367;&#25442;&#35789;&#27719;&#26816;&#27979;&#30340;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;&#20004;&#38454;&#27573;&#35838;&#31243;&#34920;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SpacTor-T5&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;SC&#39044;&#35757;&#32451;&#30456;&#21516;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#21644;&#24635;FLOP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26497;&#20854;&#32791;&#36153;&#36164;&#28304;&#19988;&#32463;&#24120;&#20302;&#25928;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#35757;&#32451;&#25991;&#26412;&#24207;&#21015;&#20013;&#25152;&#34164;&#21547;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SpacTor&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21253;&#25324;(1)&#23558;&#36328;&#24230;&#30772;&#22351;(SC)&#21644;&#26367;&#25442;&#35789;&#27719;&#26816;&#27979;(RTD)&#32467;&#21512;&#30340;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#21644;(2)&#19968;&#20010;&#20004;&#38454;&#27573;&#35838;&#31243;&#34920;&#65292;&#39318;&#20808;&#22312;&#21021;&#22987;&#30340;$\tau$&#36845;&#20195;&#20013;&#20248;&#21270;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#28982;&#21518;&#36807;&#28193;&#21040;&#26631;&#20934;&#30340;SC&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#28151;&#21512;&#30446;&#26631;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#19982;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#36827;&#24230;&#34920;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#23545;&#27492;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#23545;&#21508;&#31181;NLP&#20219;&#21153;&#36827;&#34892;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;(T5)&#30340;&#23454;&#39564;&#20013;&#65292;SpacTor-T5&#19982;&#26631;&#20934;&#30340;SC&#39044;&#35757;&#32451;&#20855;&#26377;&#30456;&#21516;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#39044;&#35757;&#32451;&#36845;&#20195;&#20943;&#23569;50%&#65292;&#24635;FLOP&#20943;&#23569;40%&#12290;&#21478;&#22806;&#65292;&#22312;&#32473;&#23450;&#30456;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;SpacTor&#20855;&#26377;&#27604;&#26631;&#20934;SC&#39044;&#35757;&#32451;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training large language models is known to be extremely resource intensive and often times inefficient, under-utilizing the information encapsulated in the training text sequences. In this paper, we present SpacTor, a new training procedure consisting of (1) a hybrid objective combining span corruption (SC) and token replacement detection (RTD), and (2) a two-stage curriculum that optimizes the hybrid objective over the initial $\tau$ iterations, then transitions to standard SC loss. We show empirically that the effectiveness of the hybrid objective is tied to the two-stage pre-training schedule, and provide extensive analysis on why this is the case. In our experiments with encoder-decoder architectures (T5) on a variety of NLP tasks, SpacTor-T5 yields the same downstream performance as standard SC pre-training, while enabling a 50% reduction in pre-training iterations and 40% reduction in total FLOPs. Alternatively, given the same amount of computing budget, we find that SpacTor 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;ASR&#20013;&#22686;&#24378;&#30340;&#26412;&#22320;&#20559;&#32622;&#21644;&#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#30340;&#37319;&#26679;&#31574;&#30053;&#24182;&#24341;&#20837;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#65292;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#21644;&#31232;&#26377;&#35789;&#35780;&#20272;&#19978;&#24179;&#22343;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#25913;&#21892;&#20102;25.84%&#12290;</title><link>http://arxiv.org/abs/2401.13146</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;ASR&#20013;&#22686;&#24378;&#30340;&#26412;&#22320;&#20559;&#32622;&#21644;&#37319;&#26679;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Locality enhanced dynamic biasing and sampling strategies for contextual ASR. (arXiv:2401.13146v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;ASR&#20013;&#22686;&#24378;&#30340;&#26412;&#22320;&#20559;&#32622;&#21644;&#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#30340;&#37319;&#26679;&#31574;&#30053;&#24182;&#24341;&#20837;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#65292;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#21644;&#31232;&#26377;&#35789;&#35780;&#20272;&#19978;&#24179;&#22343;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#25913;&#21892;&#20102;25.84%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#22312;&#35782;&#21035;&#26102;&#38388;&#21464;&#21270;&#30340;&#31232;&#26377;&#30701;&#35821;&#26102;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#19978;&#19979;&#25991;&#20559;&#32622;&#65288;CB&#65289;&#27169;&#22359;&#21487;&#20197;&#23558;ASR&#27169;&#22411;&#20559;&#21521;&#20110;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#30701;&#35821;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#20174;&#22823;&#37327;&#30701;&#35821;&#20013;&#36873;&#25321;&#19968;&#32452;&#20559;&#32622;&#30701;&#35821;&#12290;&#26412;&#25991;&#39318;&#20808;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#22270;&#34920;&#25581;&#31034;&#20102;CB&#22312;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#20043;&#38388;&#30340;&#20559;&#32622;&#23884;&#20837;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#23616;&#37096;&#21270;&#21040;&#26368;&#36817;&#30340;&#30456;&#37051;&#24103;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;CB&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#21644;&#31232;&#26377;&#35789;&#35780;&#20272;&#19978;&#24179;&#22343;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#25913;&#21892;&#20102;25.84%&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) still face challenges when recognizing time-variant rare-phrases. Contextual biasing (CB) modules bias ASR model towards such contextually-relevant phrases. During training, a list of biasing phrases are selected from a large pool of phrases following a sampling strategy. In this work we firstly analyse different sampling strategies to provide insights into the training of CB for ASR with correlation plots between the bias embeddings among various training stages. Secondly, we introduce a neighbourhood attention (NA) that localizes self attention (SA) to the nearest neighbouring frames to further refine the CB output. The results show that this proposed approach provides on average a 25.84% relative WER improvement on LibriSpeech sets and rare-word evaluation compared to the baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;LLMs&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#20197;&#21450;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#65292;LLMs&#23545;&#24694;&#24847;&#25552;&#31034;&#30340;&#21709;&#24212;&#23384;&#22312;&#24046;&#24322;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#21709;&#24212;&#26356;&#23481;&#26131;&#20135;&#29983;&#19981;&#23433;&#20840;&#12289;&#19981;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#31561;&#26041;&#27861;&#23545;&#25968;&#25454;&#36827;&#34892;&#35843;&#33410;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13136</link><description>&lt;p&gt;
&#35821;&#35328;&#38556;&#30861;&#65306;&#21078;&#26512;LLMs&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts. (arXiv:2401.13136v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;LLMs&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#20197;&#21450;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#65292;LLMs&#23545;&#24694;&#24847;&#25552;&#31034;&#30340;&#21709;&#24212;&#23384;&#22312;&#24046;&#24322;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#21709;&#24212;&#26356;&#23481;&#26131;&#20135;&#29983;&#19981;&#23433;&#20840;&#12289;&#19981;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#31561;&#26041;&#27861;&#23545;&#25968;&#25454;&#36827;&#34892;&#35843;&#33410;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20840;&#29699;&#31038;&#21306;&#30340;&#24433;&#21709;&#19981;&#26029;&#25193;&#22823;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#25361;&#25112;&#23545;&#40784;&#30740;&#31350;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#30340;&#21464;&#21270;&#65292;&#24182;&#35752;&#35770;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#23545;&#21516;&#19968;&#32452;&#24694;&#24847;&#25552;&#31034;&#30340;&#21709;&#24212;&#24773;&#20917;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;&#24403;&#24694;&#24847;&#25552;&#31034;&#29992;&#20302;&#36164;&#28304;&#35821;&#35328;&#32534;&#20889;&#26102;&#65292;LLMs&#24448;&#24448;&#26356;&#23481;&#26131;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#21709;&#24212;&#65292;&#65288;2&#65289;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#26356;&#23481;&#26131;&#29983;&#25104;&#19982;&#24694;&#24847;&#25552;&#31034;&#26080;&#20851;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#29702;&#35299;&#36896;&#25104;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#25110;&#26377;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#36827;&#34892;&#25351;&#23548;&#35843;&#33410;&#23545;HH-RLHF&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#34429;&#28982;&#20351;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#65292;&#20294;&#36890;&#36807;&#35757;&#32451;&#20173;&#20250;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages, we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, traini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;transformer-based&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#23612;&#26085;&#21033;&#20122;&#20154;&#23545;COVID-19&#30123;&#33495;&#30340;&#25509;&#21463;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#21457;&#29616;&#22823;&#22810;&#25968;&#25512;&#25991;&#34920;&#36798;&#20013;&#31435;&#24773;&#24863;&#65292;&#27809;&#26377;&#23545;&#29305;&#23450;&#30123;&#33495;&#31867;&#22411;&#30340;&#24378;&#28872;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.13133</link><description>&lt;p&gt;
&#22312;&#23612;&#26085;&#21033;&#20122;&#32593;&#32476;&#31354;&#38388;&#20998;&#26512;COVID-19&#30123;&#33495;&#24773;&#24863;&#65306;&#26469;&#33258;&#25163;&#21160;&#27880;&#37322;&#30340;Twitter&#25968;&#25454;&#38598;&#30340;&#27934;&#23519;&#12290;&#65288;arXiv:2401.13133v1[cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Analyzing COVID-19 Vaccination Sentiments in Nigerian Cyberspace: Insights from a Manually Annotated Twitter Dataset. (arXiv:2401.13133v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;transformer-based&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#23612;&#26085;&#21033;&#20122;&#20154;&#23545;COVID-19&#30123;&#33495;&#30340;&#25509;&#21463;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#21457;&#29616;&#22823;&#22810;&#25968;&#25512;&#25991;&#34920;&#36798;&#20013;&#31435;&#24773;&#24863;&#65292;&#27809;&#26377;&#23545;&#29305;&#23450;&#30123;&#33495;&#31867;&#22411;&#30340;&#24378;&#28872;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25239;&#20987;COVID-19&#22823;&#27969;&#34892;&#26041;&#38754;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#26368;&#21021;&#37319;&#29992;&#20102;&#21508;&#31181;&#39044;&#38450;&#25514;&#26045;&#65292;&#22914;&#23553;&#38145;&#12289;&#31038;&#20132;&#36317;&#31163;&#21644;&#20351;&#29992;&#21475;&#32617;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#21508;&#31181;&#30123;&#33495;&#65292;&#20197;&#24110;&#21161;&#39044;&#38450;&#25110;&#20943;&#36731;COVID-19&#24863;&#26579;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#23613;&#31649;&#36825;&#20123;&#39044;&#38450;&#25514;&#26045;&#21644;&#30123;&#33495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65288;&#22914;Twitter&#65289;&#19978;&#23384;&#22312;&#19968;&#20123;&#20105;&#35758;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#30740;&#31350;&#23612;&#26085;&#21033;&#20122;&#20154;&#23545;&#30123;&#33495;&#30340;&#25509;&#21463;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#25235;&#21462;&#22810;&#35821;&#35328;&#25512;&#25991;&#20351;&#29992;&#30456;&#20851;&#30340;&#26631;&#31614;&#21644;&#20851;&#38190;&#23383;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#25512;&#25991;&#34920;&#36798;&#20102;&#20851;&#20110;COVID-19&#30123;&#33495;&#30340;&#20013;&#31435;&#24773;&#24863;&#65292;&#19968;&#20123;&#20154;&#34920;&#36798;&#20102;&#31215;&#26497;&#30340;&#35266;&#28857;&#65292;&#24182;&#19988;&#27809;&#26377;&#23545;&#29305;&#23450;&#30123;&#33495;&#31867;&#22411;&#30340;&#24378;&#28872;&#20559;&#22909;&#65292;&#23613;&#31649;Moderna&#31245;&#24494;&#24471;&#21040;&#20102;&#26356;&#22810;&#30340;&#27491;&#38754;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous successes have been achieved in combating the COVID-19 pandemic, initially using various precautionary measures like lockdowns, social distancing, and the use of face masks. More recently, various vaccinations have been developed to aid in the prevention or reduction of the severity of the COVID-19 infection. Despite the effectiveness of the precautionary measures and the vaccines, there are several controversies that are massively shared on social media platforms like Twitter. In this paper, we explore the use of state-of-the-art transformer-based language models to study people's acceptance of vaccines in Nigeria. We developed a novel dataset by crawling multi-lingual tweets using relevant hashtags and keywords. Our analysis and visualizations revealed that most tweets expressed neutral sentiments about COVID-19 vaccines, with some individuals expressing positive views, and there was no strong preference for specific vaccine types, although Moderna received slightly more pos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#31181;&#23376;&#24341;&#23548;&#19979;&#23545;&#32454;&#31890;&#24230;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#21010;&#20998;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#29992;&#26080;&#26631;&#27880;&#35821;&#26009;&#24211;&#25214;&#21040;&#26356;&#22810;&#23454;&#20307;&#26469;&#20016;&#23500;&#30417;&#30563;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21010;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.13129</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#31181;&#23376;&#24341;&#23548;&#19979;&#23545;&#32454;&#31890;&#24230;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Seed-Guided Fine-Grained Entity Typing in Science and Engineering Domains. (arXiv:2401.13129v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#31181;&#23376;&#24341;&#23548;&#19979;&#23545;&#32454;&#31890;&#24230;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#21010;&#20998;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#29992;&#26080;&#26631;&#27880;&#35821;&#26009;&#24211;&#25214;&#21040;&#26356;&#22810;&#23454;&#20307;&#26469;&#20016;&#23500;&#30417;&#30563;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#23545;&#25991;&#26412;&#29255;&#27573;&#20013;&#30340;&#23454;&#20307;&#25552;&#20379;&#31867;&#22411;&#21010;&#20998;&#26159;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#26469;&#25191;&#34892;&#23454;&#20307;&#31867;&#22411;&#21010;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65288;&#20363;&#22914;&#36719;&#20214;&#24037;&#31243;&#21644;&#23433;&#20840;&#39046;&#22495;&#65289;&#20013;&#25910;&#38598;&#27492;&#31867;&#25968;&#25454;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#65292;&#26356;&#19981;&#29992;&#25552;&#36825;&#20123;&#27169;&#22411;&#22914;&#26524;&#38656;&#35201;&#24212;&#29992;&#20110;&#20445;&#23494;&#25968;&#25454;&#38598;&#26102;&#65292;&#35757;&#32451;&#21644;&#25512;&#26029;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#31181;&#23376;&#24341;&#23548;&#19979;&#23545;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#36827;&#34892;&#21010;&#20998;&#30340;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#20197;&#23454;&#20307;&#30340;&#21517;&#31216;&#21644;&#19968;&#20123;&#31181;&#23376;&#23454;&#20307;&#20316;&#20026;&#21807;&#19968;&#30340;&#30417;&#30563;&#65292;&#24182;&#26088;&#22312;&#23558;&#26032;&#30340;&#23454;&#20307;&#25552;&#21450;&#20998;&#31867;&#20026;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#22411;&#65288;&#21363;&#27809;&#26377;&#31181;&#23376;&#23454;&#20307;&#30340;&#31867;&#22411;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEType&#65292;&#39318;&#20808;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#26080;&#26631;&#27880;&#35821;&#26009;&#24211;&#25214;&#21040;&#27599;&#20010;&#24050;&#30693;&#31867;&#22411;&#30340;&#26356;&#22810;&#23454;&#20307;&#26469;&#20016;&#23500;&#24369;&#30417;&#30563;&#20449;&#24687;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#24102;&#26377;&#20004;&#23618;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#23545;&#23454;&#20307;&#36827;&#34892;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately typing entity mentions from text segments is a fundamental task for various natural language processing applications. Many previous approaches rely on massive human-annotated data to perform entity typing. Nevertheless, collecting such data in highly specialized science and engineering domains (e.g., software engineering and security) can be time-consuming and costly, without mentioning the domain gaps between training and inference data if the model needs to be applied to confidential datasets. In this paper, we study the task of seed-guided fine-grained entity typing in science and engineering domains, which takes the name and a few seed entities for each entity type as the only supervision and aims to classify new entity mentions into both seen and unseen types (i.e., those without seed entities). To solve this problem, we propose SEType which first enriches the weak supervision by finding more entities for each seen type from an unlabeled corpus using the contextualized 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#36136;&#37327;&#38382;&#39064;&#65292;&#21457;&#29616;&#26631;&#35760;&#21270;&#19981;&#21487;&#38752;&#12289;&#20559;&#35265;&#20197;&#21450;&#20449;&#24687;&#36136;&#37327;&#19979;&#38477;&#21487;&#33021;&#23548;&#33268;&#24187;&#35273;&#12289;&#25423;&#36896;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#20225;&#19994;&#20915;&#31574;&#20135;&#29983;&#38169;&#35823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13086</link><description>&lt;p&gt;
&#21521;&#21487;&#20449;&#36182;&#30340;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;&#65306;&#25506;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Towards Trustable Language Models: Investigating Information Quality of Large Language Models. (arXiv:2401.13086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13086
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#36136;&#37327;&#38382;&#39064;&#65292;&#21457;&#29616;&#26631;&#35760;&#21270;&#19981;&#21487;&#38752;&#12289;&#20559;&#35265;&#20197;&#21450;&#20449;&#24687;&#36136;&#37327;&#19979;&#38477;&#21487;&#33021;&#23548;&#33268;&#24187;&#35273;&#12289;&#25423;&#36896;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#20225;&#19994;&#20915;&#31574;&#20135;&#29983;&#38169;&#35823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#36805;&#36895;&#29983;&#25104;&#22823;&#37327;&#20449;&#24687;&#65292;&#29992;&#25143;&#36234;&#26469;&#36234;&#20381;&#36182;&#21644;&#20449;&#20219;&#36825;&#20123;&#25968;&#25454;&#12290;&#23613;&#31649;LLM&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20449;&#24687;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24182;&#19981;&#23436;&#20840;&#21487;&#20449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26631;&#35760;&#21270;&#19981;&#21487;&#38752;&#12289;&#23384;&#22312;&#20559;&#35265;&#65292;&#23548;&#33268;&#20449;&#24687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20449;&#24687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;LLM&#21487;&#33021;&#20250;&#20135;&#29983;&#24187;&#35273;&#12289;&#25423;&#36896;&#20449;&#24687;&#12290;&#19981;&#21487;&#38752;&#30340;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#20225;&#19994;&#20570;&#20986;&#38169;&#35823;&#20915;&#31574;&#65292;&#24433;&#21709;&#32463;&#27982;&#27963;&#21160;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#30340;&#26032;&#39062;&#25968;&#23398;&#20449;&#24687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#20998;&#26512;&#21644;&#31361;&#20986;&#20102;&#20449;&#24687;&#36136;&#37327;&#25361;&#25112;&#65292;&#20197;&#31995;&#32479;&#22320;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) are generating information at a rapid pace, requiring users to increasingly rely and trust the data. Despite remarkable advances of LLM, Information generated by LLM is not completely trustworthy, due to challenges in information quality. Specifically, integrity of Information quality decreases due to unreliable, biased, tokenization during pre-training of LLM. Moreover, due to decreased information quality issues, has led towards hallucination, fabricated information. Unreliable information can lead towards flawed decisions in businesses, which impacts economic activity. In this work, we introduce novel mathematical information quality evaluation of LLM, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;Easy Data Augmentation&#12289;Back Translation&#12289;Paraphrasing&#12289;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#25193;&#23637;&#31561;&#25216;&#26415;&#12290;&#36890;&#36807;&#20108;&#20803;&#21644;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#22522;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13085</link><description>&lt;p&gt;
IndiText Boost: &#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
IndiText Boost: Text Augmentation for Low Resource India Languages. (arXiv:2401.13085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;Easy Data Augmentation&#12289;Back Translation&#12289;Paraphrasing&#12289;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#25193;&#23637;&#31561;&#25216;&#26415;&#12290;&#36890;&#36807;&#20108;&#20803;&#21644;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#22522;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22686;&#24378;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#26377;&#21161;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#32463;&#23545;&#33521;&#35821;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#24037;&#20316;&#65292;&#32780;&#22312;&#21360;&#24230;&#35821;&#35328;&#26041;&#38754;&#21364;&#20570;&#24471;&#24456;&#23569;&#12290;&#36825;&#19982;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#30340;&#20107;&#23454;&#30456;&#21453;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#23454;&#26045;Easy Data Augmentation&#12289;Back Translation&#12289;Paraphrasing&#12289;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#25193;&#23637;&#31561;&#25216;&#26415;&#65292;&#29992;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;6&#31181;&#21360;&#24230;&#35821;&#35328;&#65306;&#20449;&#24503;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#21644;&#26805;&#35821;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#31867;&#20284;&#30340;&#24037;&#20316;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20108;&#20803;&#21644;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#65292;&#20197;&#20351;&#25105;&#20204;&#30340;&#32467;&#26524;&#26356;&#20855;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#22522;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text Augmentation is an important task for low-resource languages. It helps deal with the problem of data scarcity. A data augmentation strategy is used to deal with the problem of data scarcity. Through the years, much work has been done on data augmentation for the English language. In contrast, very less work has been done on Indian languages. This is contrary to the fact that data augmentation is used to deal with data scarcity. In this work, we focus on implementing techniques like Easy Data Augmentation, Back Translation, Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for text classification on different languages. We focus on 6 Indian languages namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to our knowledge, no such work exists for text augmentation on Indian languages. We carry out binary as well as multi-class text classification to make our results more comparable. We get surprising results as basic data augmentation techniq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#12298;&#21476;&#20848;&#32463;&#38382;&#31572;2023&#12299;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25237;&#31080;&#38598;&#25104;&#26469;&#25552;&#39640;&#39044;&#27979;&#31283;&#23450;&#24615;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;Transformer&#27169;&#22411;&#21644;&#38408;&#20540;&#26426;&#21046;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#38544;&#34255;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13060</link><description>&lt;p&gt;
TCE&#22312;&#12298;&#21476;&#20848;&#32463;&#38382;&#31572;2023&#12299;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;: &#20302;&#36164;&#28304;&#22686;&#24378;Transformer-&#22522;&#20110;&#38598;&#25104;&#26041;&#27861;&#30340;&#21476;&#20848;&#32463;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur'anic QA. (arXiv:2401.13060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#12298;&#21476;&#20848;&#32463;&#38382;&#31572;2023&#12299;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25237;&#31080;&#38598;&#25104;&#26469;&#25552;&#39640;&#39044;&#27979;&#31283;&#23450;&#24615;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;Transformer&#27169;&#22411;&#21644;&#38408;&#20540;&#26426;&#21046;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#38544;&#34255;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#12298;&#21476;&#20848;&#32463;&#38382;&#31572;2023&#12299;&#20849;&#20139;&#20219;&#21153;A&#21644;B&#20013;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#36164;&#28304;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20381;&#38752;&#36801;&#31227;&#23398;&#20064;&#21644;&#25237;&#31080;&#38598;&#25104;&#26469;&#25552;&#39640;&#22312;&#22810;&#27425;&#36816;&#34892;&#20013;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#23398;&#20064;&#26426;&#21046;&#65292;&#38024;&#23545;&#20004;&#20010;&#20219;&#21153;&#20351;&#29992;&#19968;&#31995;&#21015;&#30340;&#38463;&#25289;&#20271;&#35821;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#12290;&#20026;&#20102;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#38408;&#20540;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#38544;&#34255;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#22823;&#22823;&#36229;&#36807;&#22522;&#20934;&#24615;&#33021;&#65292;&#20219;&#21153;A&#30340;MAP&#24471;&#20998;&#20026;25.05%&#65292;&#20219;&#21153;B&#30340;&#23616;&#37096;&#24179;&#22343;&#20934;&#30830;&#29575;(pAP)&#20026;57.11%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present our approach to tackle Qur'an QA 2023 shared tasks A and B. To address the challenge of low-resourced training data, we rely on transfer learning together with a voting ensemble to improve prediction stability across multiple runs. Additionally, we employ different architectures and learning mechanisms for a range of Arabic pre-trained transformer-based models for both tasks. To identify unanswerable questions, we propose using a thresholding mechanism. Our top-performing systems greatly surpass the baseline performance on the hidden split, achieving a MAP score of 25.05% for task A and a partial Average Precision (pAP) of 57.11% for task B.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#21307;&#23398;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20197;&#39592;&#20851;&#33410;&#28814;&#31649;&#29702;&#20026;&#20363;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#39046;&#22495;&#29305;&#23450;&#30340;&#39592;&#20851;&#33410;&#28814;&#31649;&#29702;&#27169;&#22411;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19987;&#38376;&#27169;&#22411;&#34920;&#29616;&#26377;&#26126;&#26174;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.12998</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#21307;&#23398;&#20013;&#30340;&#34920;&#29616;&#65306;&#20197;DocOA&#20026;&#20363;&#30340;&#39592;&#20851;&#33410;&#28814;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Enhancing Large Language Models Performance in Domain-specific Medicine: Osteoarthritis Management with DocOA. (arXiv:2401.12998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#21307;&#23398;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20197;&#39592;&#20851;&#33410;&#28814;&#31649;&#29702;&#20026;&#20363;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#39046;&#22495;&#29305;&#23450;&#30340;&#39592;&#20851;&#33410;&#28814;&#31649;&#29702;&#27169;&#22411;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19987;&#38376;&#27169;&#22411;&#34920;&#29616;&#26377;&#26126;&#26174;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29305;&#23450;&#39046;&#22495;&#21307;&#23398;&#20013;&#30340;&#25928;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#31649;&#29702;&#39592;&#20851;&#33410;&#28814;&#65288;OA&#65289;&#31561;&#22797;&#26434;&#30142;&#30149;&#26041;&#38754;&#65292;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#35780;&#20272;&#21644;&#25552;&#21319;LLMs&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20020;&#24202;&#33021;&#21147;&#65292;&#20197;&#39592;&#20851;&#33410;&#28814;&#65288;OA&#65289;&#31649;&#29702;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#35780;&#20272;LLMs&#22312;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#30495;&#23454;&#20020;&#24202;&#22330;&#26223;&#20013;&#30340;&#20020;&#24202;&#24212;&#29992;&#20043;&#38388;&#30340;&#34920;&#29616;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;OA&#31649;&#29702;&#30340;&#19987;&#38376;LLM&#65292;&#21517;&#20026;DocOA&#65292;&#23427;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#25351;&#20196;&#25552;&#31034;&#12290;&#36890;&#36807;&#23458;&#35266;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;GPT-3.5&#12289;GPT-4&#21644;&#19987;&#38376;&#21161;&#25163;DocOA&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#29992;LLMs&#22914;GPT-3.5&#21644;GPT-4&#22312;OA&#31649;&#29702;&#36825;&#31181;&#19987;&#38376;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;DocOA&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficacy of large language models (LLMs) in domain-specific medicine, particularly for managing complex diseases such as osteoarthritis (OA), remains largely unexplored. This study focused on evaluating and enhancing the clinical capabilities of LLMs in specific domains, using osteoarthritis (OA) management as a case study. A domain specific benchmark framework was developed, which evaluate LLMs across a spectrum from domain-specific knowledge to clinical applications in real-world clinical scenarios. DocOA, a specialized LLM tailored for OA management that integrates retrieval-augmented generation (RAG) and instruction prompts, was developed. The study compared the performance of GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less effective in the specialized domain of OA management, particularly in providing personalized treatment recommendations. However, DocOA showed signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#29983;&#25104;&#29305;&#24449;&#30340;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#39044;&#33976;&#39311;&#21644;&#21387;&#32553;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#24341;&#20837;&#36974;&#34109;&#29983;&#25104;&#30340;&#25945;&#24072;-&#23398;&#29983;&#29305;&#24449;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12997</link><description>&lt;p&gt;
&#22522;&#20110;&#36974;&#34109;&#29983;&#25104;&#29305;&#24449;&#26041;&#27861;&#30340;&#28176;&#36827;&#33976;&#39311;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion. (arXiv:2401.12997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#29983;&#25104;&#29305;&#24449;&#30340;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#39044;&#33976;&#39311;&#21644;&#21387;&#32553;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#24341;&#20837;&#36974;&#34109;&#29983;&#25104;&#30340;&#25945;&#24072;-&#23398;&#29983;&#29305;&#24449;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLM) &#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840; (KGC) &#27169;&#22411;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;PLM &#27169;&#22411;&#30340;&#22823;&#37327;&#21442;&#25968;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#23545;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#29983;&#25104;&#29305;&#24449;&#30340;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110; KGC &#20219;&#21153;&#65292;&#26088;&#22312;&#26174;&#33879;&#38477;&#20302;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545; PLM &#36827;&#34892;&#39044;&#33976;&#39311;&#65292;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#21387;&#32553; PLM &#32593;&#32476;&#24471;&#21040;&#22810;&#31561;&#32423;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#29305;&#24449;&#33976;&#39311;&#22312;&#25945;&#24072;&#27169;&#22411;&#20013;&#21482;&#26377;&#21333;&#19968;&#20449;&#24687;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25945;&#24072;-&#23398;&#29983;&#29305;&#24449;&#30340;&#36974;&#34109;&#29983;&#25104;&#65292;&#20854;&#20013;&#21253;&#21547;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#30340;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, knowledge graph completion (KGC) models based on pre-trained language model (PLM) have shown promising results. However, the large number of parameters and high computational cost of PLM models pose challenges for their application in downstream tasks. This paper proposes a progressive distillation method based on masked generation features for KGC task, aiming to significantly reduce the complexity of pre-trained models. Specifically, we perform pre-distillation on PLM to obtain high-quality teacher models, and compress the PLM network to obtain multi-grade student models. However, traditional feature distillation suffers from the limitation of having a single representation of information in teacher models. To solve this problem, we propose masked generation of teacher-student features, which contain richer representation information. Furthermore, there is a significant gap in representation ability between teacher and student. Therefore, we design a progressive dist
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32763;&#35793;&#20102;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21457;&#29616;&#20102;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#30340;&#36864;&#20237;&#20891;&#20154;&#12290;&#19982;&#20165;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#35782;&#21035;&#30340;&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#24739;&#32773;&#30456;&#27604;&#65292;&#36825;&#20123;&#24739;&#32773;&#20855;&#26377;&#19981;&#21516;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.12996</link><description>&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#35760;&#24405;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#20351;&#29992;&#35786;&#26029;&#20195;&#30721;&#23545;&#27604;&#21457;&#29616;&#23384;&#22312;&#38382;&#39064;&#30340;&#30103;&#25928;&#24615;&#40486;&#29255;&#20351;&#29992;&#30340;&#36864;&#20237;&#20891;&#20154;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Veterans with Problematic Opioid Use Identified through Natural Language Processing of Clinical Notes versus Using Diagnostic Codes. (arXiv:2401.12996v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12996
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#20102;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21457;&#29616;&#20102;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#30340;&#36864;&#20237;&#20891;&#20154;&#12290;&#19982;&#20165;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#35782;&#21035;&#30340;&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#24739;&#32773;&#30456;&#27604;&#65292;&#36825;&#20123;&#24739;&#32773;&#20855;&#26377;&#19981;&#21516;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26159;&#40486;&#29255;&#31867;&#33647;&#29289;&#30740;&#31350;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#40486;&#29255;&#31867;&#33647;&#29289;&#28389;&#29992;&#38590;&#20197;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#36827;&#34892;&#32534;&#30721;&#65292;&#20294;&#26159;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#21487;&#20197;&#35760;&#24405;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;1&#65289;&#20174;&#21508;&#31181;&#20020;&#24202;&#35760;&#24405;&#20013;&#35782;&#21035;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#65307;2&#65289;&#27604;&#36739;&#20165;&#36890;&#36807;&#20020;&#24202;&#35760;&#24405;&#35760;&#24405;&#23384;&#22312;&#38382;&#39064;&#40486;&#29255;&#20351;&#29992;&#30340;&#24739;&#32773;&#19982;&#20351;&#29992;ICD&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#35786;&#26029;&#20195;&#30721;&#30340;&#24739;&#32773;&#30340;&#29305;&#24449;&#12290;&#26448;&#26009;&#19982;&#26041;&#27861;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#65292;&#23545;&#26469;&#33258;&#20004;&#20010;&#36864;&#20237;&#20891;&#20154;&#20107;&#21153;&#25152;&#21306;&#22495;&#30340;&#24739;&#32773;&#38431;&#21015;&#65288;n=222,371&#65289;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#20998;&#26512;&#20197;&#35782;&#21035;&#23384;&#22312;&#38382;&#39064;&#30340;&#40486;&#29255;&#20351;&#29992;&#30340;&#24739;&#32773;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#19968;&#32452;ICD&#35786;&#26029;&#20195;&#30721;&#26469;&#35782;&#21035;&#26469;&#33258;&#30456;&#21516;&#38431;&#21015;&#30340;&#24739;&#26377;&#40486;&#29255;&#20351;&#29992;&#38556;&#30861;&#30340;&#24739;&#32773;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20165;&#36890;&#36807;NLP&#35782;&#21035;&#20986;&#30340;&#24739;&#32773;&#19982;&#36890;&#36807;&#35786;&#26029;&#20195;&#30721;&#35782;&#21035;&#20986;&#30340;&#24739;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Electronic health records (EHRs) are a data source for opioid research. Opioid use disorder is known to be under-coded as a diagnosis, yet problematic opioid use can be documented in clinical notes.  Objectives: Our goals were 1) to identify problematic opioid use from a full range of clinical notes; and 2) to compare the characteristics of patients identified as having problematic opioid use, exclusively documented in clinical notes, to those having documented ICD opioid use disorder diagnostic codes.  Materials and Methods: We developed and applied a natural language processing (NLP) tool to the clinical notes of a patient cohort (n=222,371) from two Veteran Affairs service regions to identify patients with problematic opioid use. We also used a set of ICD diagnostic codes to identify patients with opioid use disorder from the same cohort. We compared the demographic and clinical characteristics of patients identified only through NLP, to those of patients identified thro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20195;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#30340;&#22238;&#24212;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20174;&#23545;&#35805;&#20013;&#33719;&#21462;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#26469;&#22686;&#24378;&#22238;&#24212;&#29983;&#25104;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12995</link><description>&lt;p&gt;
&#21327;&#35843;&#20195;&#30721;&#28151;&#21512;&#23545;&#35805;&#65306;&#22312;&#23545;&#35805;&#20013;&#22522;&#20110;&#20154;&#26684;&#36741;&#21161;&#30340;&#20195;&#30721;&#28151;&#21512;&#22238;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Harmonizing Code-mixed Conversations: Personality-assisted Code-mixed Response Generation in Dialogues. (arXiv:2401.12995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20195;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#30340;&#22238;&#24212;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20174;&#23545;&#35805;&#20013;&#33719;&#21462;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#26469;&#22686;&#24378;&#22238;&#24212;&#29983;&#25104;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#28151;&#21512;&#26159;&#22312;&#19968;&#20010;&#23545;&#35805;&#20013;&#28151;&#21512;&#22810;&#31181;&#35821;&#35328;&#30340;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#22312;&#22238;&#24212;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25429;&#25417;&#20195;&#30721;&#28151;&#21512;&#30340;&#22797;&#26434;&#24615;&#34987;&#35777;&#26126;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20010;&#20307;&#30340;&#35828;&#35805;&#39118;&#26684;&#21644;&#25991;&#21270;&#32972;&#26223;&#20250;&#20135;&#29983;&#21508;&#31181;&#21508;&#26679;&#30340;&#21464;&#24322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20195;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#21033;&#29992;&#20174;&#23545;&#35805;&#20013;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#33719;&#21462;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#26469;&#22686;&#24378;&#22238;&#24212;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#25512;&#26029;&#20986;&#26469;&#30340;&#20154;&#26684;&#23646;&#24615;&#34987;&#26080;&#32541;&#22320;&#32534;&#32455;&#21040;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#26426;&#21046;PA3&#12290;&#23427;&#20351;&#29992;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#27880;&#24847;&#21147;&#20844;&#24335;&#26469;&#34701;&#21512;&#23545;&#35805;&#21644;&#20010;&#24615;&#20449;&#24687;&#12290;&#36825;&#31181;&#34701;&#21512;&#19981;&#20165;&#22686;&#24378;&#20102;&#29983;&#25104;&#22238;&#24212;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-mixing, the blending of multiple languages within a single conversation, introduces a distinctive challenge, particularly in the context of response generation. Capturing the intricacies of code-mixing proves to be a formidable task, given the wide-ranging variations influenced by individual speaking styles and cultural backgrounds. In this study, we explore response generation within code-mixed conversations. We introduce a novel approach centered on harnessing the Big Five personality traits acquired in an unsupervised manner from the conversations to bolster the performance of response generation. These inferred personality attributes are seamlessly woven into the fabric of the dialogue context, using a novel fusion mechanism, PA3. It uses an effective two-step attention formulation to fuse the dialogue and personality information. This fusion not only enhances the contextual relevance of generated responses but also elevates the overall performance of the model. Our experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20266;&#26631;&#31614;&#25216;&#26415;&#33258;&#21160;&#35780;&#20998;&#20020;&#24202;&#30149;&#20154;&#31508;&#35760;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#21516;&#26102;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#34920;&#26126;&#22312;&#20020;&#24202;&#31508;&#35760;&#35780;&#20272;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#30340;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2401.12994</link><description>&lt;p&gt;
&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20266;&#26631;&#31614;&#25216;&#26415;&#33258;&#21160;&#35780;&#20998;&#20020;&#24202;&#30149;&#20154;&#31508;&#35760;
&lt;/p&gt;
&lt;p&gt;
Automated Scoring of Clinical Patient Notes using Advanced NLP and Pseudo Labeling. (arXiv:2401.12994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20266;&#26631;&#31614;&#25216;&#26415;&#33258;&#21160;&#35780;&#20998;&#20020;&#24202;&#30149;&#20154;&#31508;&#35760;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#21516;&#26102;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#34920;&#26126;&#22312;&#20020;&#24202;&#31508;&#35760;&#35780;&#20272;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#30149;&#20154;&#31508;&#35760;&#23545;&#20110;&#35760;&#24405;&#30149;&#20154;&#20114;&#21160;&#12289;&#35786;&#26029;&#21644;&#27835;&#30103;&#35745;&#21010;&#22312;&#21307;&#30103;&#23454;&#36341;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#30830;&#20445;&#23545;&#36825;&#20123;&#31508;&#35760;&#30340;&#20934;&#30830;&#35780;&#20272;&#23545;&#20110;&#21307;&#23398;&#25945;&#32946;&#21644;&#35748;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35780;&#20272;&#22797;&#26434;&#19988;&#32791;&#26102;&#65292;&#24448;&#24448;&#23548;&#33268;&#35780;&#20272;&#32467;&#26524;&#30340;&#21487;&#21464;&#24615;&#21644;&#36164;&#28304;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#20855;&#20307;&#21253;&#25324;&#23553;&#25513;&#35821;&#35328;&#24314;&#27169;(MLM)&#39044;&#35757;&#32451;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#34920;&#26126;&#20020;&#24202;&#31508;&#35760;&#35780;&#20272;&#21487;&#33021;&#21457;&#29983;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical patient notes are critical for documenting patient interactions, diagnoses, and treatment plans in medical practice. Ensuring accurate evaluation of these notes is essential for medical education and certification. However, manual evaluation is complex and time-consuming, often resulting in variability and resource-intensive assessments. To tackle these challenges, this research introduces an approach leveraging state-of-the-art Natural Language Processing (NLP) techniques, specifically Masked Language Modeling (MLM) pretraining, and pseudo labeling. Our methodology enhances efficiency and effectiveness, significantly reducing training time without compromising performance. Experimental results showcase improved model performance, indicating a potential transformation in clinical note assessment.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#25991;&#26412;&#65292;&#33258;&#21160;&#31995;&#32479;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#21475;&#33108;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#23545;&#20110;&#24739;&#32773;&#30340;&#21450;&#26102;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.12993</link><description>&lt;p&gt;
&#36890;&#36807;&#24773;&#24863;&#20998;&#31867;&#26469;&#20272;&#35745;&#21475;&#33108;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Estimating the severity of dental and oral problems via sentiment classification over clinical reports. (arXiv:2401.12993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12993
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#25991;&#26412;&#65292;&#33258;&#21160;&#31995;&#32479;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#21475;&#33108;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#23545;&#20110;&#24739;&#32773;&#30340;&#21450;&#26102;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20316;&#32773;&#22312;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#20316;&#20026;&#19968;&#31181;&#35782;&#21035;&#25991;&#26412;&#26497;&#24615;&#30340;&#25216;&#26415;&#65292;&#22312;&#21307;&#23398;&#21644;&#29273;&#31185;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#21487;&#20197;&#24456;&#23454;&#29992;&#21644;&#26377;&#29992;&#12290;&#30446;&#21069;&#65292;&#30001;&#20110;&#24739;&#32773;&#23545;&#33258;&#36523;&#29366;&#20917;&#20102;&#35299;&#26377;&#38480;&#12289;&#38590;&#20197;&#25509;&#35302;&#19987;&#31185;&#21307;&#29983;&#25110;&#32773;&#23545;&#30142;&#30149;&#30340;&#24656;&#24807;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#27969;&#34892;&#30149;&#26465;&#20214;&#19979;&#65292;&#25509;&#25910;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#21672;&#35810;&#21307;&#29983;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#24310;&#36831;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#24310;&#36831;&#21487;&#33021;&#23545;&#24739;&#32773;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#65292;&#22240;&#27492;&#21450;&#26102;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20998;&#26512;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#25991;&#26412;&#65292;&#33258;&#21160;&#31995;&#32479;&#21487;&#20197;&#36890;&#30693;&#24739;&#32773;&#20854;&#30149;&#24773;&#24694;&#21270;&#31243;&#24230;&#65292;&#36825;&#22312;&#21450;&#26102;&#20915;&#31574;&#26041;&#38754;&#23558;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25910;&#38598;&#20102;&#26469;&#33258;Shiraz&#22823;&#23398;&#21307;&#23398;&#31185;&#23398;&#38498;&#30340;1,134&#20221;&#38181;&#26463;&#35745;&#31639;&#26426;&#26029;&#23618;&#25668;&#24433;&#65288;CBCT&#65289;&#29031;&#29255;&#25253;&#21578;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#26696;&#20363;&#37117;&#32463;&#36807;&#26816;&#26597;&#65292;&#30001;&#19987;&#23478;&#23545;&#27599;&#20010;&#25991;&#26723;&#19978;&#30340;&#24739;&#32773;&#29366;&#20917;&#26631;&#27880;&#20102;&#20005;&#37325;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing authors' sentiments in texts as a technique for identifying text polarity can be practical and useful in various fields, including medicine and dentistry. Currently, due to factors such as patients' limited knowledge about their condition, difficulties in accessing specialist doctors, or fear of illness, particularly in pandemic conditions, there might be a delay between receiving a radiology report and consulting a doctor. In some cases, this delay can pose significant risks to the patient, making timely decision-making crucial. Having an automatic system that can inform patients about the deterioration of their condition by analyzing the text of radiology reports could greatly impact timely decision-making. In this study, a dataset comprising 1,134 cone-beam computed tomography (CBCT) photo reports was collected from the Shiraz University of Medical Sciences. Each case was examined, and an expert labeled a severity level for the patient's condition on each document. After p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#26080;&#20851;&#30340;&#35821;&#21477;&#32423;&#35821;&#38899;&#32534;&#30721;&#23454;&#29616;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35821;&#35328;&#24179;&#34892;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#29992;&#20110;&#22810;&#35821;&#35328;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2401.12992</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#26080;&#20851;&#30340;&#35821;&#21477;&#32423;&#35821;&#38899;&#32534;&#30721;&#23454;&#29616;&#30340;&#35821;&#38899;&#23545;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
TranSentence: Speech-to-speech Translation via Language-agnostic Sentence-level Speech Encoding without Language-parallel Data. (arXiv:2401.12992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#26080;&#20851;&#30340;&#35821;&#21477;&#32423;&#35821;&#38899;&#32534;&#30721;&#23454;&#29616;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35821;&#35328;&#24179;&#34892;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#29992;&#20110;&#22810;&#35821;&#35328;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20256;&#32479;&#27169;&#22411;&#20173;&#38656;&#35201;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#24179;&#34892;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35821;&#35328;&#24179;&#34892;&#35821;&#38899;&#25968;&#25454;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#20102;&#19968;&#31181;&#35821;&#35328;&#26080;&#20851;&#30340;&#35821;&#21477;&#32423;&#35821;&#38899;&#32534;&#30721;&#65292;&#23427;&#21487;&#20197;&#25429;&#25417;&#21040;&#35821;&#38899;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#19981;&#21463;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#65292;&#22522;&#20110;&#20174;&#35821;&#35328;&#26080;&#20851;&#30340;&#21477;&#23376;&#32423;&#35821;&#38899;&#32534;&#30721;&#22120;&#20013;&#33719;&#24471;&#30340;&#32534;&#30721;&#23884;&#20837;&#29983;&#25104;&#35821;&#38899;&#12290;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26469;&#33258;&#28304;&#35821;&#35328;&#35821;&#38899;&#30340;&#35821;&#35328;&#26080;&#20851;&#35821;&#38899;&#23884;&#20837;&#22312;&#25512;&#29702;&#38454;&#27573;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#35821;&#38899;&#65292;&#23613;&#31649;&#25105;&#20204;&#20165;&#22312;&#30446;&#26631;&#35821;&#35328;&#30340;&#21333;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there has been significant advancement in the field of speech-to-speech translation, conventional models still require language-parallel speech data between the source and target languages for training. In this paper, we introduce TranSentence, a novel speech-to-speech translation without language-parallel speech data. To achieve this, we first adopt a language-agnostic sentence-level speech encoding that captures the semantic information of speech, irrespective of language. We then train our model to generate speech based on the encoded embedding obtained from a language-agnostic sentence-level speech encoder that is pre-trained with various languages. With this method, despite training exclusively on the target language's monolingual data, we can generate target language speech in the inference stage using language-agnostic speech embedding from the source language speech. Furthermore, we extend TranSentence to multilingual speech-to-speech translation. The experimental resu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#30340;&#36755;&#20986;&#25193;&#23637;&#21040;&#20165;&#38480;&#20110;&#38548;&#31163;&#20196;&#29260;&#21015;&#34920;&#20043;&#22806;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12990</link><description>&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#65306;&#36229;&#36234;&#20196;&#29260;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Topic Modelling: Going Beyond Token Outputs. (arXiv:2401.12990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12990
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#30340;&#36755;&#20986;&#25193;&#23637;&#21040;&#20165;&#38480;&#20110;&#38548;&#31163;&#20196;&#29260;&#21015;&#34920;&#20043;&#22806;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#19968;&#31181;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#22810;&#20010;&#25991;&#26723;&#20013;&#35782;&#21035;&#26174;&#33879;&#20027;&#39064;&#12290;&#36890;&#24120;&#36755;&#20986;&#26159;&#30001;&#24120;&#24120;&#20849;&#21516;&#20986;&#29616;&#22312;&#36825;&#20123;&#25991;&#26723;&#20013;&#30340;&#38548;&#31163;&#20196;&#29260;&#32452;&#25104;&#30340;&#20027;&#39064;&#38598;&#21512;&#12290;&#20174;&#20154;&#31867;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#36755;&#20986;&#21487;&#33021;&#19981;&#36275;&#20197;&#20805;&#20998;&#25512;&#26029;&#20027;&#39064;&#30340;&#21547;&#20041;&#65307;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#29702;&#35299;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#33258;&#21160;&#25193;&#23637;&#20027;&#39064;&#25551;&#36848;&#20197;&#22686;&#24378;&#20027;&#39064;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#21487;&#33021;&#19981;&#21487;&#29992;&#30340;&#22806;&#37096;&#35821;&#35328;&#36164;&#28304;&#65292;&#24182;&#38656;&#35201;&#20445;&#25345;&#26368;&#26032;&#20197;&#29983;&#25104;&#30456;&#20851;&#32467;&#26524;&#65292;&#24182;&#22312;&#35757;&#32451;&#25110;&#22788;&#29702;&#25968;&#25454;&#26102;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#30340;&#36755;&#20986;&#25193;&#23637;&#21040;&#20165;&#38480;&#20110;&#38548;&#31163;&#20196;&#29260;&#21015;&#34920;&#20043;&#22806;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic modelling is a text mining technique for identifying salient themes from a number of documents. The output is commonly a set of topics consisting of isolated tokens that often co-occur in such documents. Manual effort is often associated with interpreting a topic's description from such tokens. However, from a human's perspective, such outputs may not adequately provide enough information to infer the meaning of the topics; thus, their interpretability is often inaccurately understood. Although several studies have attempted to automatically extend topic descriptions as a means of enhancing the interpretation of topic models, they rely on external language sources that may become unavailable, must be kept up-to-date to generate relevant results, and present privacy issues when training on or processing data. This paper presents a novel approach towards extending the output of traditional topic modelling methods beyond a list of isolated tokens. This approach removes the dependenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30417;&#27979;&#26538;&#25903;&#26292;&#21147;&#20107;&#20214;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#22242;&#38431;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#27169;&#22411;&#35782;&#21035;&#24052;&#35199;&#30340;&#26538;&#25903;&#26292;&#21147;&#25253;&#21578;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#20154;&#26435;&#32452;&#32455;&#25910;&#38598;&#21253;&#21547;&#25152;&#38656;&#25968;&#25454;&#30340;&#20840;&#38754;&#25968;&#25454;&#24211;&#12290;</title><link>http://arxiv.org/abs/2401.12989</link><description>&lt;p&gt;
&#22312;&#20132;&#28779;&#20013;&#65306;&#35780;&#20272;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20247;&#21253;&#26538;&#25903;&#26292;&#21147;&#25253;&#21578;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Into the crossfire: evaluating the use of a language model to crowdsource gun violence reports. (arXiv:2401.12989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30417;&#27979;&#26538;&#25903;&#26292;&#21147;&#20107;&#20214;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#22242;&#38431;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#27169;&#22411;&#35782;&#21035;&#24052;&#35199;&#30340;&#26538;&#25903;&#26292;&#21147;&#25253;&#21578;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#20154;&#26435;&#32452;&#32455;&#25910;&#38598;&#21253;&#21547;&#25152;&#38656;&#25968;&#25454;&#30340;&#20840;&#38754;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26538;&#25903;&#26292;&#21147;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#20154;&#26435;&#38382;&#39064;&#65292;&#24433;&#21709;&#30528;&#31038;&#20250;&#30340;&#26041;&#26041;&#38754;&#38754;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21644;&#25945;&#32946;&#21040;&#24515;&#29702;&#23398;&#21644;&#32463;&#27982;&#23398;&#12290;&#21487;&#38752;&#30340;&#26538;&#25903;&#20107;&#20214;&#25968;&#25454;&#23545;&#20110;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#20844;&#20849;&#25919;&#31574;&#21644;&#24212;&#24613;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#25968;&#25454;&#24211;&#21644;&#38754;&#23545;&#38754;&#35843;&#26597;&#30340;&#39118;&#38505;&#38459;&#27490;&#20102;&#20154;&#26435;&#32452;&#32455;&#22312;&#22823;&#22810;&#25968;&#22269;&#23478;&#25910;&#38598;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19982;&#19968;&#23478;&#24052;&#35199;&#20154;&#26435;&#32452;&#32455;&#21512;&#20316;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#20197;&#24110;&#21161;&#30417;&#27979;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#29616;&#23454;&#19990;&#30028;&#26538;&#25903;&#20107;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;Twitter&#19978;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#26538;&#25903;&#26292;&#21147;&#25253;&#21578;&#21644;&#26222;&#36890;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#39640;&#36798;0.97&#30340;AUC&#20998;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#25972;&#21512;&#21040;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#24182;&#22312;&#23454;&#26102;&#24178;&#39044;&#20013;&#23545;&#20854;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30740;&#31350;&#24182;&#37319;&#35775;&#24052;&#35199;&#20998;&#26512;&#24072;&#65292;&#20182;&#20204;&#22312;&#25345;&#32493;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#20107;&#23454;&#26680;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gun violence is a pressing and growing human rights issue that affects nearly every dimension of the social fabric, from healthcare and education to psychology and the economy. Reliable data on firearm events is paramount to developing more effective public policy and emergency responses. However, the lack of comprehensive databases and the risks of in-person surveys prevent human rights organizations from collecting needed data in most countries. Here, we partner with a Brazilian human rights organization to conduct a systematic evaluation of language models to assist with monitoring real-world firearm events from social media data. We propose a fine-tuned BERT-based model trained on Twitter (now X) texts to distinguish gun violence reports from ordinary Portuguese texts. Our model achieves a high AUC score of 0.97. We then incorporate our model into a web application and test it in a live intervention. We study and interview Brazilian analysts who continuously fact-check social media
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#36827;&#34892;&#31934;&#31070;&#38556;&#30861;&#30340;&#26816;&#27979;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#25552;&#31034;&#21644;&#21307;&#30103;&#30693;&#35782;&#27880;&#20837;&#26469;&#35299;&#20915;&#25968;&#25454;&#25361;&#25112;&#65292;&#23454;&#29616;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.12988</link><description>&lt;p&gt;
&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#19982;&#21307;&#30103;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection. (arXiv:2401.12988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#36827;&#34892;&#31934;&#31070;&#38556;&#30861;&#30340;&#26816;&#27979;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#25552;&#31034;&#21644;&#21307;&#30103;&#30693;&#35782;&#27880;&#20837;&#26469;&#35299;&#20915;&#25968;&#25454;&#25361;&#25112;&#65292;&#23454;&#29616;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#24930;&#24615;&#30149;&#31649;&#29702;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20869;&#23481;&#26469;&#26816;&#27979;&#21508;&#31181;&#31934;&#31070;&#38556;&#30861;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#20840;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#27880;&#37322;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#27599;&#31181;&#30142;&#30149;&#30340;&#36153;&#26102;&#36153;&#21147;&#30340;&#25163;&#21160;&#36807;&#31243;&#65292;&#20197;&#21450;&#38656;&#35201;&#20026;&#27599;&#20010;&#38382;&#39064;&#35774;&#35745;&#19987;&#38376;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#25968;&#25454;&#39537;&#21160;&#24930;&#24615;&#30149;&#31649;&#29702;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65306;&#65288;1&#65289;&#24320;&#21457;&#20010;&#24615;&#21270;&#30340;&#25552;&#31034;&#26469;&#34920;&#31034;&#27599;&#20010;&#29992;&#25143;&#30340;&#29420;&#29305;&#24615;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23558;&#21307;&#30103;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#20026;&#24930;&#24615;&#30149;&#26816;&#27979;&#25552;&#20379;&#19978;&#19979;&#25991;&#65292;&#25351;&#23548;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#23454;&#29616;&#39044;&#27979;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#31181;&#31934;&#31070;&#38556;&#30861;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study harnesses state-of-the-art AI technology for chronic disease management, specifically in detecting various mental disorders through user-generated textual content. Existing studies typically rely on fully supervised machine learning, which presents challenges such as the labor-intensive manual process of annotating extensive training data for each disease and the need to design specialized deep learning architectures for each problem. To address such challenges, we propose a novel framework that leverages advanced AI techniques, including large language models and multi-prompt engineering. Specifically, we address two key technical challenges in data-driven chronic disease management: (1) developing personalized prompts to represent each user's uniqueness and (2) incorporating medical knowledge into prompts to provide context for chronic disease detection, instruct learning objectives, and operationalize prediction goals. We evaluate our method using four mental disorders, w
&lt;/p&gt;</description></item><item><title>TelME&#26159;&#19968;&#31181;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#20248;&#21270;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;MELD&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12987</link><description>&lt;p&gt;
TelME&#65306;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation. (arXiv:2401.12987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12987
&lt;/p&gt;
&lt;p&gt;
TelME&#26159;&#19968;&#31181;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#20248;&#21270;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;MELD&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#22312;&#20351;&#23545;&#35805;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22238;&#24212;&#29992;&#25143;&#35831;&#27714;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#21487;&#20197;&#36890;&#36807;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#31561;&#22810;&#31181;&#27169;&#24577;&#30340;&#34920;&#31034;&#36827;&#34892;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#35821;&#35328;&#27169;&#24577;&#23545;&#35782;&#21035;&#24773;&#32490;&#30340;&#36129;&#29486;&#36739;&#24369;&#65292;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#25945;&#24072;&#23548;&#21521;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65288;TelME&#65289;&#12290;TelME&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23558;&#20449;&#24687;&#20174;&#20316;&#20026;&#25945;&#24072;&#30340;&#35821;&#35328;&#27169;&#22411;&#20256;&#36882;&#32473;&#38750;&#35821;&#35328;&#30340;&#23398;&#29983;&#65292;&#20174;&#32780;&#20248;&#21270;&#20102;&#24369;&#27169;&#24577;&#30340;&#25928;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#31227;&#21160;&#34701;&#21512;&#26041;&#27861;&#23558;&#22810;&#27169;&#24577;&#29305;&#24449;&#32452;&#21512;&#36215;&#26469;&#65292;&#20854;&#20013;&#23398;&#29983;&#32593;&#32476;&#25903;&#25345;&#25945;&#24072;&#12290;TelME&#22312;MELD&#65288;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#23454;&#39564;&#35770;&#35777;&#20102;&#25105;&#20204;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue systems to effectively respond to user requests. The emotions in a conversation can be identified by the representations from various modalities, such as audio, visual, and text. However, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal knowledge distillation to transfer information from a language model acting as the teacher to the non-verbal students, thereby optimizing the efficacy of the weak modalities. We then combine multimodal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effectiveness of our components through additional expe
&lt;/p&gt;</description></item><item><title>&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;&#26041;&#27861;&#65288;CSAS&#65289;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#36755;&#20837;&#28436;&#21464;&#38382;&#39064;&#24211;&#65292;&#24182;&#22312;&#35843;&#26597;&#20013;&#36866;&#24212;&#26032;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#22312;&#25289;&#19969;&#35028;&#20449;&#24687;&#29615;&#22659;&#21644;&#35758;&#39064;&#37325;&#35201;&#24615;&#39046;&#22495;&#65292;&#33021;&#22815;&#35782;&#21035;&#38590;&#20197;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#36319;&#36394;&#30340;&#20027;&#24352;&#25110;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12986</link><description>&lt;p&gt;
&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Crowdsourced Adaptive Surveys. (arXiv:2401.12986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12986
&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;&#26041;&#27861;&#65288;CSAS&#65289;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#36755;&#20837;&#28436;&#21464;&#38382;&#39064;&#24211;&#65292;&#24182;&#22312;&#35843;&#26597;&#20013;&#36866;&#24212;&#26032;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#22312;&#25289;&#19969;&#35028;&#20449;&#24687;&#29615;&#22659;&#21644;&#35758;&#39064;&#37325;&#35201;&#24615;&#39046;&#22495;&#65292;&#33021;&#22815;&#35782;&#21035;&#38590;&#20197;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#36319;&#36394;&#30340;&#20027;&#24352;&#25110;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20247;&#33286;&#35770;&#35843;&#26597;&#23545;&#20110;&#27665;&#20027;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#20256;&#32479;&#35843;&#26597;&#26041;&#27861;&#26469;&#35828;&#65292;&#24555;&#36895;&#21464;&#21270;&#30340;&#20449;&#24687;&#29615;&#22659;&#21644;&#22312;&#23567;&#20247;&#31038;&#21306;&#20013;&#34913;&#37327;&#35266;&#28857;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;&#26041;&#27861;&#65288;CSAS&#65289;&#65292;&#23427;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#36827;&#23637;&#32467;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#38543;&#30528;&#29992;&#25143;&#36755;&#20837;&#19981;&#26029;&#28436;&#21464;&#30340;&#38382;&#39064;&#24211;&#12290;CSAS&#26041;&#27861;&#23558;&#21442;&#19982;&#32773;&#25552;&#20379;&#30340;&#24320;&#25918;&#24335;&#25991;&#26412;&#36716;&#25442;&#20026;Likert&#24335;&#39033;&#30446;&#65292;&#24182;&#24212;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#26469;&#30830;&#23450;&#24212;&#20248;&#20808;&#32771;&#34385;&#22312;&#35843;&#26597;&#20013;&#30340;&#29992;&#25143;&#25552;&#20379;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#33258;&#36866;&#24212;&#24615;&#20801;&#35768;&#25506;&#32034;&#26032;&#30340;&#35843;&#26597;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35843;&#26597;&#38271;&#24230;&#19978;&#26045;&#21152;&#26368;&#23567;&#30340;&#25104;&#26412;&#12290;&#22312;&#25289;&#19969;&#35028;&#20449;&#24687;&#29615;&#22659;&#21644;&#35758;&#39064;&#37325;&#35201;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;CSAS&#35782;&#21035;&#21487;&#33021;&#38590;&#20197;&#36890;&#36807;&#26631;&#20934;&#26041;&#27861;&#36319;&#36394;&#30340;&#20027;&#24352;&#25110;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#25552;&#20986; Conclusion by di&#30340;&#32467;&#26463;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public opinion surveys are vital for informing democratic decision-making, but responding to rapidly changing information environments and measuring beliefs within niche communities can be challenging for traditional survey methods. This paper introduces a crowdsourced adaptive survey methodology (CSAS) that unites advances in natural language processing and adaptive algorithms to generate question banks that evolve with user input. The CSAS method converts open-ended text provided by participants into Likert-style items and applies a multi-armed bandit algorithm to determine user-provided questions that should be prioritized in the survey. The method's adaptive nature allows for the exploration of new survey questions, while imposing minimal costs in survey length. Applications in the domains of Latino information environments and issue importance showcase CSAS's ability to identify claims or issues that might otherwise be difficult to track using standard approaches. I conclude by di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20154;&#24037;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#21644;&#32771;&#34385;&#24448;&#36820;&#27979;&#35797;&#30340;&#35774;&#32622;&#65292;&#35780;&#20272;&#20102;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#65288;SASs&#65289;&#30340;&#20559;&#35265;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#35780;&#20272;SASs&#26174;&#31034;&#20986;&#26356;&#22810;&#30340;&#20559;&#35265;&#65292;&#32780;&#24448;&#36820;&#27979;&#35797;&#21017;&#26356;&#30495;&#23454;&#22320;&#23637;&#31034;&#20102;SASs&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12985</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#21512;&#25104;&#27979;&#35797;&#25968;&#25454;&#20197;&#21450;&#24448;&#36820;&#27979;&#35797;&#23545;&#20559;&#35265;&#20998;&#26512;&#31995;&#32479;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Human v/s Synthetic Test Data and Round-tripping on Assessment of Sentiment Analysis Systems for Bias. (arXiv:2401.12985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20154;&#24037;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#21644;&#32771;&#34385;&#24448;&#36820;&#27979;&#35797;&#30340;&#35774;&#32622;&#65292;&#35780;&#20272;&#20102;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#65288;SASs&#65289;&#30340;&#20559;&#35265;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#35780;&#20272;SASs&#26174;&#31034;&#20986;&#26356;&#22810;&#30340;&#20559;&#35265;&#65292;&#32780;&#24448;&#36820;&#27979;&#35797;&#21017;&#26356;&#30495;&#23454;&#22320;&#23637;&#31034;&#20102;SASs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#65288;SASs&#65289;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24403;&#32473;&#23450;&#19968;&#27573;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#26102;&#65292;&#21487;&#20197;&#36755;&#20986;&#24773;&#24863;&#26497;&#24615;&#21644;&#24773;&#24863;&#24378;&#24230;&#12290;&#21644;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#19968;&#26679;&#65292;&#24403;SASs&#38754;&#20020;&#25968;&#25454;&#21464;&#21270;&#26102;&#65292;&#20063;&#20250;&#20986;&#29616;&#19981;&#31283;&#23450;&#30340;&#34892;&#20026;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;AI&#19982;&#20154;&#31867;&#21512;&#20316;&#26102;&#20986;&#29616;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#21040;&#20445;&#25252;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#65289;&#30340;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#23545;&#20854;&#23384;&#22312;&#20559;&#35265;&#30340;&#20449;&#20219;&#24863;&#20135;&#29983;&#30097;&#34385;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#35780;&#20272;SASs&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#25110;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#33521;&#25991;&#25968;&#25454;&#23545;&#20854;&#36827;&#34892;&#20559;&#35265;&#35780;&#32423;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#20154;&#24037;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#26469;&#25193;&#23637;&#36825;&#20010;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#24448;&#36820;&#27979;&#35797;&#30340;&#35774;&#32622;&#65292;&#21363;&#36890;&#36807;&#19968;&#20010;&#20013;&#38388;&#35821;&#35328;&#23558;&#25968;&#25454;&#20174;&#19968;&#31181;&#35821;&#35328;&#32763;&#35793;&#20026;&#30456;&#21516;&#35821;&#35328;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35774;&#32622;&#21487;&#20197;&#26356;&#30495;&#23454;&#22320;&#23637;&#31034;SASs&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#35780;&#20272;SASs&#26174;&#31034;&#20986;&#27604;&#21512;&#25104;&#25968;&#25454;&#26356;&#22810;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#20351;&#29992;&#35199;&#29677;&#29273;&#35821;&#21644;&#20025;&#40614;&#35821;&#36827;&#34892;&#24448;&#36820;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis Systems (SASs) are data-driven Artificial Intelligence (AI) systems that output polarity and emotional intensity when given a piece of text as input. Like other AIs, SASs are also known to have unstable behavior when subjected to changes in data which can make it problematic to trust out of concerns like bias when AI works with humans and data has protected attributes like gender, race, and age. Recently, an approach was introduced to assess SASs in a blackbox setting without training data or code, and rating them for bias using synthetic English data. We augment it by introducing two human-generated chatbot datasets and also consider a round-trip setting of translating the data from one language to the same through an intermediate language. We find that these settings show SASs performance in a more realistic light. Specifically, we find that rating SASs on the chatbot data showed more bias compared to the synthetic data, and round-tripping using Spanish and Danish 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#26800;&#24037;&#31243;&#39046;&#22495;&#20013;&#35299;&#20915;&#27010;&#24565;&#24615;&#38382;&#39064;&#30340;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#22312;&#21508;&#20010;&#21147;&#23398;&#20027;&#39064;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#21644;&#20154;&#31867;&#23545;&#29031;&#32452;&#65292;&#26174;&#31034;&#20986;&#28508;&#22312;&#30340;&#26410;&#26469;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.12983</link><description>&lt;p&gt;
&#22312;&#26426;&#26800;&#24037;&#31243;&#25945;&#32946;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20851;&#20110;&#20197;&#21147;&#23398;&#20026;&#37325;&#28857;&#30340;&#27010;&#24565;&#29702;&#35299;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding. (arXiv:2401.12983v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#26800;&#24037;&#31243;&#39046;&#22495;&#20013;&#35299;&#20915;&#27010;&#24565;&#24615;&#38382;&#39064;&#30340;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#22312;&#21508;&#20010;&#21147;&#23398;&#20027;&#39064;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#21644;&#20154;&#31867;&#23545;&#29031;&#32452;&#65292;&#26174;&#31034;&#20986;&#28508;&#22312;&#30340;&#26410;&#26469;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#26800;&#24037;&#31243;&#39046;&#22495;&#20013;&#35299;&#20915;&#27010;&#24565;&#24615;&#38382;&#39064;&#30340;&#33021;&#21147;&#36827;&#34892;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#21147;&#23398;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;126&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#32771;&#35797;&#26469;&#36827;&#34892;&#32771;&#23519;&#65292;&#28085;&#30422;&#20102;&#21147;&#23398;&#35838;&#31243;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#27969;&#20307;&#21147;&#23398;&#12289;&#26426;&#26800;&#25391;&#21160;&#12289;&#24037;&#31243;&#38745;&#21147;&#23398;&#21644;&#21160;&#21147;&#23398;&#12289;&#26448;&#26009;&#21147;&#23398;&#12289;&#24377;&#24615;&#29702;&#35770;&#21644;&#36830;&#32493;&#20171;&#36136;&#21147;&#23398;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;LLM&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;ChatGPT&#65288;GPT-3.5&#65289;&#12289;ChatGPT&#65288;GPT-4&#65289;&#21644;Claude&#65288;Claude-2.1&#65289;&#65292;&#24182;&#23558;&#20854;&#19982;&#20855;&#26377;&#25110;&#27809;&#26377;&#26426;&#26800;&#24037;&#31243;&#32972;&#26223;&#30340;&#24037;&#31243;&#25945;&#32844;&#21592;&#21644;&#23398;&#29983;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21508;&#31181;&#21147;&#23398;&#20027;&#39064;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#65292;&#38500;&#20102;&#36830;&#32493;&#20171;&#36136;&#21147;&#23398;&#22806;&#65292;GPT-4&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20004;&#20010;LLM&#21644;&#20154;&#31867;&#23545;&#29031;&#32452;&#12290;&#36825;&#34920;&#26126;GPT&#27169;&#22411;&#22312;&#22788;&#29702;&#31526;&#21495;&#35745;&#31639;&#21644;&#24352;&#37327;&#20998;&#26512;&#26041;&#38754;&#26377;&#28508;&#22312;&#30340;&#26410;&#26469;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study is a pioneering endeavor to investigate the capabilities of Large Language Models (LLMs) in addressing conceptual questions within the domain of mechanical engineering with a focus on mechanics. Our examination involves a manually crafted exam encompassing 126 multiple-choice questions, spanning various aspects of mechanics courses, including Fluid Mechanics, Mechanical Vibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of Elasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5), ChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against engineering faculties and students with or without mechanical engineering background. The findings reveal GPT-4's superior performance over the other two LLMs and human cohorts in answering questions across various mechanics topics, except for Continuum Mechanics. This signals the potential future improvements for GPT models in handling symbolic calculations and tensor analyses
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#23558;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#23618;&#27425;&#21270;&#22320;&#20998;&#20026;&#31934;&#32454;&#30340;&#31867;&#21035;&#21644;&#20855;&#20307;&#25216;&#26415;&#65292;&#29992;&#20197;&#35299;&#20915;&#29616;&#26377;&#32508;&#36848;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12982</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#65306;&#19968;&#39033;&#22238;&#39038;&#12289;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text Classification: A Review, Empirical, and Experimental Evaluation. (arXiv:2401.12982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#23558;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#23618;&#27425;&#21270;&#22320;&#20998;&#20026;&#31934;&#32454;&#30340;&#31867;&#21035;&#21644;&#20855;&#20307;&#25216;&#26415;&#65292;&#29992;&#20197;&#35299;&#20915;&#29616;&#26377;&#32508;&#36848;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#21644;&#24191;&#27867;&#22686;&#38271;&#20351;&#24471;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#25104;&#20026;&#24517;&#35201;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#32463;&#20856;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#30740;&#31350;&#20986;&#29616;&#20102;&#28608;&#22686;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#20173;&#28982;&#36843;&#20999;&#38656;&#35201;&#19968;&#20221;&#20840;&#38754;&#21644;&#26368;&#26032;&#30340;&#32508;&#36848;&#12290;&#29616;&#26377;&#30340;&#32508;&#36848;&#25991;&#31456;&#23558;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#20998;&#20026;&#24191;&#27867;&#30340;&#31867;&#21035;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#26080;&#20851;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#20197;&#21450;&#20351;&#29992;&#30456;&#21516;&#24230;&#37327;&#26631;&#20934;&#23545;&#20854;&#36136;&#37327;&#21644;&#34892;&#20026;&#36827;&#34892;&#38169;&#35823;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#23558;&#31639;&#27861;&#23618;&#27425;&#21270;&#22320;&#20998;&#20026;&#31934;&#32454;&#30340;&#31867;&#21035;&#21644;&#20855;&#20307;&#25216;&#26415;&#12290;&#35813;&#20998;&#31867;&#27861;&#21253;&#25324;&#26041;&#27861;&#23398;&#31867;&#21035;&#12289;&#26041;&#27861;&#23398;&#25216;&#26415;&#21644;&#26041;&#27861;&#23398;&#23376;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#20998;&#31867;&#27861;&#23545;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explosive and widespread growth of data necessitates the use of text classification to extract crucial information from vast amounts of data. Consequently, there has been a surge of research in both classical and deep learning text classification methods. Despite the numerous methods proposed in the literature, there is still a pressing need for a comprehensive and up-to-date survey. Existing survey papers categorize algorithms for text classification into broad classes, which can lead to the misclassification of unrelated algorithms and incorrect assessments of their qualities and behaviors using the same metrics. To address these limitations, our paper introduces a novel methodological taxonomy that classifies algorithms hierarchically into fine-grained classes and specific techniques. The taxonomy includes methodology categories, methodology techniques, and methodology sub-techniques. Our study is the first survey to utilize this methodological taxonomy for classifying algorithm
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#20351;&#29992;&#36890;&#29992;AI&#21270;&#36523;&#36827;&#34892;&#20132;&#20114;&#30340;&#28508;&#21147;&#21644;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#35789;&#20856;&#21644;&#25913;&#36827;&#26426;&#21046;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21307;&#30103;AI&#21270;&#36523;&#24212;&#29992;&#26694;&#26550;&#65292;&#25552;&#20379;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#22686;&#24378;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23545;&#35805;&#33021;&#21147;&#21644;&#20010;&#24615;&#29305;&#28857;&#65292;&#23454;&#29616;&#26356;&#22909;&#20154;&#24037;&#26234;&#33021;&#19982;&#24739;&#32773;&#20043;&#38388;&#20132;&#27969;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.12981</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#21270;&#36523;
&lt;/p&gt;
&lt;p&gt;
A General-purpose AI Avatar in Healthcare. (arXiv:2401.12981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12981
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#20351;&#29992;&#36890;&#29992;AI&#21270;&#36523;&#36827;&#34892;&#20132;&#20114;&#30340;&#28508;&#21147;&#21644;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#35789;&#20856;&#21644;&#25913;&#36827;&#26426;&#21046;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21307;&#30103;AI&#21270;&#36523;&#24212;&#29992;&#26694;&#26550;&#65292;&#25552;&#20379;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#22686;&#24378;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23545;&#35805;&#33021;&#21147;&#21644;&#20010;&#24615;&#29305;&#28857;&#65292;&#23454;&#29616;&#26356;&#22909;&#20154;&#24037;&#26234;&#33021;&#19982;&#24739;&#32773;&#20043;&#38388;&#20132;&#27969;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#21307;&#30103;&#34892;&#19994;&#20013;&#23453;&#36149;&#24037;&#20855;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20316;&#20026;&#23545;&#35805;&#20195;&#29702;&#25110;&#32842;&#22825;&#26426;&#22120;&#20154;&#26377;&#28508;&#21147;&#24110;&#21161;&#21307;&#29983;&#35786;&#26029;&#24739;&#32773;&#12289;&#26816;&#27979;&#30142;&#30149;&#30340;&#26089;&#26399;&#30151;&#29366;&#20197;&#21450;&#21521;&#24739;&#32773;&#25552;&#20379;&#20581;&#24247;&#24314;&#35758;&#12290;&#26412;&#25991;&#20851;&#27880;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#20316;&#29992;&#65292;&#25506;&#32034;&#20351;&#29992;&#21270;&#36523;&#26469;&#20351;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#20114;&#23545;&#24739;&#32773;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#19977;&#31867;&#21035;&#25552;&#31034;&#35789;&#20856;&#21644;&#25552;&#31034;&#25913;&#36827;&#26426;&#21046;&#65292;&#28436;&#31034;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#21270;&#36523;&#24212;&#29992;&#26694;&#26550;&#12290;&#24314;&#35758;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#23545;&#36890;&#29992;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#21019;&#24314;&#19981;&#21516;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#21270;&#36523;&#19982;&#29992;&#25143;&#35752;&#35770;&#21307;&#23398;&#38382;&#39064;&#12290;&#25552;&#31034;&#24037;&#31243;&#22686;&#24378;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23545;&#35805;&#33021;&#21147;&#21644;&#20154;&#26684;&#29305;&#28857;&#65292;&#22521;&#20859;&#20102;&#19982;&#24739;&#32773;&#26356;&#20855;&#20154;&#31867;&#20132;&#27969;&#26041;&#24335;&#30340;&#20114;&#21160;&#12290;&#24402;&#26681;&#32467;&#24213;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#23454;&#29616;&#26356;&#22909;&#20154;&#24037;&#26234;&#33021;&#19982;&#24739;&#32773;&#20043;&#38388;&#20132;&#27969;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in machine learning and natural language processing have led to the rapid development of artificial intelligence (AI) as a valuable tool in the healthcare industry. Using large language models (LLMs) as conversational agents or chatbots has the potential to assist doctors in diagnosing patients, detecting early symptoms of diseases, and providing health advice to patients. This paper focuses on the role of chatbots in healthcare and explores the use of avatars to make AI interactions more appealing to patients. A framework of a general-purpose AI avatar application is demonstrated by using a three-category prompt dictionary and prompt improvement mechanism. A two-phase approach is suggested to fine-tune a general-purpose AI language model and create different AI avatars to discuss medical issues with users. Prompt engineering enhances the chatbot's conversational abilities and personality traits, fostering a more human-like interaction with patients. Ultimately, the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;LSTM&#25216;&#26415;&#35782;&#21035;&#24052;&#35199;&#35686;&#26041;&#25253;&#21578;&#20013;&#22899;&#24615;&#35851;&#26432;&#21069;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#21463;&#23475;&#32773;&#34987;&#35851;&#26432;&#39118;&#38505;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#65292;&#20026;&#39044;&#38450;&#22899;&#24615;&#35851;&#26432;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.12980</link><description>&lt;p&gt;
&#22312;&#24052;&#35199;&#35686;&#26041;&#25253;&#21578;&#20013;&#35782;&#21035;&#22899;&#24615;&#35851;&#26432;&#21069;&#30340;&#39118;&#38505;&#27169;&#24335;&#65306;&#22522;&#20110;LSTM&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Identifying Risk Patterns in Brazilian Police Reports Preceding Femicides: A Long Short Term Memory (LSTM) Based Analysis. (arXiv:2401.12980v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12980
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;LSTM&#25216;&#26415;&#35782;&#21035;&#24052;&#35199;&#35686;&#26041;&#25253;&#21578;&#20013;&#22899;&#24615;&#35851;&#26432;&#21069;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#21463;&#23475;&#32773;&#34987;&#35851;&#26432;&#39118;&#38505;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#65292;&#20026;&#39044;&#38450;&#22899;&#24615;&#35851;&#26432;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22899;&#24615;&#35851;&#26432;&#26159;&#25351;&#30007;&#24615;&#20276;&#20387;&#25110;&#23478;&#24237;&#25104;&#21592;&#23545;&#22899;&#24615;&#21463;&#23475;&#32773;&#30340;&#26432;&#23475;&#65292;&#24182;&#19982;&#24615;&#21035;&#26292;&#21147;&#26377;&#20851;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#35851;&#26432;&#20043;&#21069;&#23384;&#22312;&#19968;&#31181;&#21152;&#21095;&#30340;&#26292;&#21147;&#27169;&#24335;&#65292;&#22914;&#26524;&#33021;&#35780;&#20272;&#21463;&#23475;&#32773;&#38754;&#20020;&#30340;&#21361;&#38505;&#31243;&#24230;&#65292;&#23601;&#26377;&#21487;&#33021;&#39044;&#38450;&#36825;&#31181;&#24773;&#20917;&#12290;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22522;&#20110;&#23545;&#26292;&#21147;&#34892;&#20026;&#30340;&#25551;&#36848;&#26469;&#39044;&#27979;&#39118;&#38505;&#27700;&#24179;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#25216;&#26415;&#26469;&#35782;&#21035;&#24052;&#35199;&#35686;&#26041;&#25253;&#21578;&#20013;&#22899;&#24615;&#35851;&#26432;&#21069;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#30446;&#26631;&#26159;&#23558;&#36825;&#20123;&#25253;&#21578;&#30340;&#20869;&#23481;&#20998;&#31867;&#20026;&#21463;&#23475;&#32773;&#34987;&#35851;&#26432;&#30340;&#36739;&#20302;&#39118;&#38505;&#25110;&#36739;&#39640;&#39118;&#38505;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;66%&#12290;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#21463;&#23475;&#32773;&#22312;&#19968;&#31995;&#21015;&#27169;&#24335;&#21270;&#20107;&#20214;&#20013;&#21487;&#33021;&#32463;&#21382;&#30340;&#19979;&#19968;&#27493;&#34892;&#21160;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23545;&#25105;&#20204;&#29702;&#35299;&#39044;&#38450;&#22899;&#24615;&#35851;&#26432;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Femicide refers to the killing of a female victim, often perpetrated by an intimate partner or family member, and is also associated with gender-based violence. Studies have shown that there is a pattern of escalating violence leading up to these killings, highlighting the potential for prevention if the level of danger to the victim can be assessed. Machine learning offers a promising approach to address this challenge by predicting risk levels based on textual descriptions of the violence. In this study, we employed the Long Short Term Memory (LSTM) technique to identify patterns of behavior in Brazilian police reports preceding femicides. Our first objective was to classify the content of these reports as indicating either a lower or higher risk of the victim being murdered, achieving an accuracy of 66%. In the second approach, we developed a model to predict the next action a victim might experience within a sequence of patterned events. Both approaches contribute to the understand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#33258;&#27880;&#24847;&#21147;&#22312;Transformer&#20013;&#26159;&#22266;&#26377;&#30340;&#21508;&#21521;&#24322;&#24615;&#29616;&#35937;&#65292;&#35813;&#29616;&#35937;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#19981;&#20165;&#38480;&#20110;&#38271;&#23614;&#20998;&#24067;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.12143</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#22312;Transformer&#20013;&#26159;&#22266;&#26377;&#30340;&#21508;&#21521;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Anisotropy Is Inherent to Self-Attention in Transformers. (arXiv:2401.12143v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#33258;&#27880;&#24847;&#21147;&#22312;Transformer&#20013;&#26159;&#22266;&#26377;&#30340;&#21508;&#21521;&#24322;&#24615;&#29616;&#35937;&#65292;&#35813;&#29616;&#35937;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#19981;&#20165;&#38480;&#20110;&#38271;&#23614;&#20998;&#24067;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#26159;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#26222;&#36941;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23427;&#20197;&#21508;&#21521;&#24322;&#24615;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#36825;&#26159;&#38544;&#34255;&#34920;&#31034;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#20351;&#20854;&#22312;&#35282;&#24230;&#36317;&#31163;&#65288;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#26041;&#38754;&#24847;&#22806;&#22320;&#24444;&#27492;&#38752;&#36817;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#21508;&#21521;&#24322;&#24615;&#26159;&#22312;&#38271;&#23614;&#20998;&#24067;&#30340;&#20196;&#29260;&#19978;&#20248;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#65292;&#21508;&#21521;&#24322;&#24615;&#20063;&#21487;&#20197;&#22312;&#20855;&#26377;&#29305;&#23450;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#24212;&#30452;&#25509;&#21463;&#21040;&#30456;&#21516;&#21518;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21508;&#21521;&#24322;&#24615;&#38382;&#39064;&#25193;&#23637;&#21040;&#22312;&#20854;&#20182;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;Transformer&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#34920;&#26126;&#65292;&#21508;&#21521;&#24322;&#24615;&#23454;&#38469;&#19978;&#26159;Transformer-based&#27169;&#22411;&#22266;&#26377;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations suggest that anisotropy is actually inherent to Transformers-based models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;COVID-19&#38376;&#35786;&#27835;&#30103;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11120</link><description>&lt;p&gt;
&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines. (arXiv:2401.11120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11120
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;COVID-19&#38376;&#35786;&#27835;&#30103;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25645;&#37197;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#65288;CPGs&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#65288;CDS&#65289;&#12290;&#28982;&#32780;&#65292;&#23558;CPGs&#32435;&#20837;LLMs&#30340;&#26041;&#27861;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#23558;CPGs&#32435;&#20837;LLMs&#65306;&#20108;&#20803;&#20915;&#31574;&#26641;&#65288;BDT&#65289;&#65292;&#31243;&#24207;&#36741;&#21161;&#22270;&#26500;&#24314;&#65288;PAGC&#65289;&#65292;&#20197;&#21450;&#24605;&#32500;&#38142;&#23569;&#26679;&#26412;&#25552;&#31034;&#65288;CoT-FSP&#65289;&#12290;&#20026;&#35780;&#20272;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#21512;&#25104;&#24739;&#32773;&#25551;&#36848;&#65292;&#24182;&#23545;&#30001;&#22235;&#20010;LLMs&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#65306;GPT-4&#65292;GPT-3.5 Turbo&#65292;LLaMA&#21644;PaLM 2&#12290;&#38646;&#26679;&#26412;&#25552;&#31034;&#65288;ZSP&#65289;&#34987;&#29992;&#20316;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#20197;COVID-19&#38376;&#35786;&#27835;&#30103;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#22235;&#20010;LLMs&#22312;&#22686;&#21152;&#20102;CPGs&#21518;&#30456;&#23545;&#20110;&#22522;&#32447;ZSP&#23637;&#29616;&#20102;&#25552;&#39640;&#30340;&#24615;&#33021;&#12290;BDT&#22312;&#33258;&#21160;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;CoT-FSP&#21644;PAGC&#12290;&#25152;&#26377;&#25552;&#20986;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background Large Language Models (LLMs), enhanced with Clinical Practice Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS). However, methods for incorporating CPGs into LLMs are not well studied. Methods We develop three distinct methods for incorporating CPGs into LLMs: Binary Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of the proposed methods, we create a set of synthetic patient descriptions and conduct both automatic and human evaluation of the responses generated by four LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was used as the baseline method. We focus on CDS for COVID-19 outpatient treatment as the case study. Results All four LLMs exhibit improved performance when enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP and PAGC in automatic evaluation. All of the proposed methods demonstrated high per
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.10841</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#21457;&#29616;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#32534;&#30721;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20167;&#24680;&#35328;&#35770;&#30340;&#34067;&#24310;&#32473;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24102;&#26469;&#20102;&#19968;&#20010;&#38590;&#39064;&#12290;&#19968;&#20010;&#29305;&#27530;&#30340;&#25361;&#25112;&#19982;&#20351;&#29992;&#32534;&#30721;&#35821;&#35328;&#30340;&#32676;&#20307;&#26377;&#20851;&#65292;&#36825;&#20123;&#32676;&#20307;&#26082;&#24819;&#20026;&#20854;&#29992;&#25143;&#21019;&#36896;&#24402;&#23646;&#24863;&#65292;&#21448;&#24819;&#22238;&#36991;&#26816;&#27979;&#12290;&#32534;&#30721;&#35821;&#35328;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20351;&#29992;&#26041;&#24335;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26041;&#27861;&#22312;&#22312;&#32447;&#21453;&#29369;&#22826;&#35328;&#35770;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#25235;&#21462;&#30340;&#24086;&#23376;&#65292;&#36890;&#24120;&#26159;&#26497;&#31471;&#20027;&#20041;&#29992;&#25143;&#20351;&#29992;&#30340;&#12290;&#24086;&#23376;&#26159;&#20351;&#29992;&#19982;&#20197;&#21069;&#24050;&#30693;&#30340;&#38024;&#23545;&#29369;&#22826;&#20154;&#30340;&#20167;&#24680;&#35328;&#35770;&#30456;&#20851;&#30340;&#31181;&#23376;&#34920;&#36798;&#24335;&#36827;&#34892;&#25235;&#21462;&#30340;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#24086;&#23376;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#22312;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#30340;&#39057;&#29575;&#12290;&#36807;&#28388;&#25481;&#35821;&#27861;&#19981;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#21644;&#20043;&#21069;&#36935;&#21040;&#36807;&#30340;&#34920;&#36798;&#24335;&#65292;&#20197;&#20415;&#20851;&#27880;&#26032;&#20986;&#29616;&#30340;&#33391;&#22909;&#24418;&#24335;&#30340;&#26415;&#35821;&#12290;&#28982;&#21518;&#36827;&#34892;&#20102;&#35821;&#20041;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20316;&#20026;&#23398;&#29983;&#23398;&#20064;&#25512;&#33616;&#30340;&#35299;&#37322;&#24037;&#20855;&#21644;&#25351;&#23548;&#12290;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#26469;&#35843;&#25511;LLM&#30340;&#29983;&#25104;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22312;&#19982;&#23398;&#29983;&#23545;&#35805;&#20013;&#25552;&#20379;&#35299;&#37322;&#21644;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.08517</link><description>&lt;p&gt;
&#25903;&#25345;&#23398;&#29983;&#20915;&#31574;&#30340;&#23398;&#20064;&#25512;&#33616;&#65306;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#23454;&#29616;&#23545;&#35805;&#35299;&#37322;&#21644;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring. (arXiv:2401.08517v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08517
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20316;&#20026;&#23398;&#29983;&#23398;&#20064;&#25512;&#33616;&#30340;&#35299;&#37322;&#24037;&#20855;&#21644;&#25351;&#23548;&#12290;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#26469;&#35843;&#25511;LLM&#30340;&#29983;&#25104;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22312;&#19982;&#23398;&#29983;&#23545;&#35805;&#20013;&#25552;&#20379;&#35299;&#37322;&#21644;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#23545;&#23398;&#20064;&#25512;&#33616;&#30340;&#20915;&#31574;&#19982;&#20854;&#29702;&#35299;&#25512;&#33616;&#21407;&#22240;&#30340;&#33021;&#21147;&#26159;&#19981;&#21487;&#20998;&#21106;&#30340;&#65307;&#20182;&#20204;&#33021;&#21542;&#26681;&#25454;&#36825;&#31181;&#29702;&#35299;&#36827;&#34892;&#20462;&#25913;&#12290;&#22312;&#21508;&#31181;&#35299;&#37322;&#24615;&#26041;&#27861;&#20013;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#20855;&#26377;&#19982;&#21516;&#34892;&#25110;&#23548;&#24072;&#35752;&#35770;&#31867;&#20284;&#30340;&#28508;&#21147;&#26469;&#19982;&#23398;&#29983;&#36827;&#34892;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#20197;&#21462;&#20195;&#20154;&#31867;&#23548;&#24072;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#23545;&#35805;&#30340;&#20013;&#20171;&#21644;&#35299;&#37322;&#30340;&#26377;&#38480;&#21644;&#21463;&#25511;&#29983;&#25104;&#30340;&#26469;&#28304;&#65292;&#20197;&#21033;&#29992;LLM&#30340;&#28508;&#21147;&#30340;&#21516;&#26102;&#20943;&#23569;&#20854;&#28508;&#22312;&#39118;&#38505;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25903;&#25345;&#23398;&#29983;&#29702;&#35299;&#23398;&#20064;&#36335;&#24452;&#25512;&#33616;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20316;&#20026;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#65292;&#36890;&#36807;&#23450;&#20041;&#20854;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#26469;&#35843;&#25511;LLM&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Student commitment towards a learning recommendation is not separable from their understanding of the reasons it was recommended to them; and their ability to modify it based on that understanding. Among explainability approaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with a peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an approach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of explanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed LLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge graph (KG) as a human-curated source of information, to regulate the LLM's output through defining its prompt's contex
&lt;/p&gt;</description></item><item><title>GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.08396</link><description>&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#32972;&#21518;&#30340;&#38544;&#34255;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08396
&lt;/p&gt;
&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;Vision&#21151;&#33021;&#30340;GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;GPT-4V&#22312;&#35299;&#20915;&#26032;&#33521;&#26684;&#20848;&#21307;&#23398;&#26434;&#24535;&#22270;&#20687;&#25361;&#25112;&#20013;&#30340;&#22270;&#20687;&#29702;&#35299;&#12289;&#21307;&#23398;&#30693;&#35782;&#22238;&#24518;&#21644;&#36880;&#27493;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21407;&#29702;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#23454;&#65292;GPT-4V&#22312;&#22810;&#39033;&#36873;&#25321;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#65288;88.0% vs. 77.0%&#65292;p=0.034&#65289;&#12290;GPT-4V&#22312;&#21307;&#29983;&#22238;&#31572;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#34920;&#29616;&#20986;&#36229;&#36807;80%&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4V&#22312;&#26368;&#32456;&#20570;&#20986;&#27491;&#30830;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#24120;&#25552;&#20379;&#26377;&#32570;&#38519;&#30340;&#25512;&#29702;&#65288;27.3%&#65289;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#22270;&#20687;&#29702;&#35299;&#65288;21.6%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks. However, these evaluations primarily focused on the accuracy of multi-choice questions alone. Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals. Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy. However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#20154;&#31867;&#20132;&#27969;&#32773;&#65292;&#25506;&#32034;&#20102;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35828;&#26381;LLMs&#36827;&#34892;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39118;&#38505;&#31867;&#21035;&#19978;&#22343;&#21462;&#24471;&#20102;&#36229;&#36807;92%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06373</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;Johnny&#35828;&#26381;LLMs&#36234;&#29425;&#65306;&#36890;&#36807;&#20154;&#24615;&#21270;LLMs&#37325;&#26032;&#24605;&#32771;&#23545;AI&#23433;&#20840;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. (arXiv:2401.06373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#20154;&#31867;&#20132;&#27969;&#32773;&#65292;&#25506;&#32034;&#20102;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35828;&#26381;LLMs&#36827;&#34892;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39118;&#38505;&#31867;&#21035;&#19978;&#22343;&#21462;&#24471;&#20102;&#36229;&#36807;92%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;AI&#23433;&#20840;&#30740;&#31350;&#23558;AI&#27169;&#22411;&#35270;&#20026;&#26426;&#22120;&#65292;&#24182;&#38598;&#20013;&#22312;&#30001;&#23433;&#20840;&#19987;&#23478;&#24320;&#21457;&#30340;&#22522;&#20110;&#31639;&#27861;&#30340;&#25915;&#20987;&#19978;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#31454;&#20105;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#38750;&#19987;&#23478;&#29992;&#25143;&#22312;&#26085;&#24120;&#20114;&#21160;&#20013;&#20063;&#21487;&#33021;&#20135;&#29983;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#23558;LLMs&#20316;&#20026;&#31867;&#20284;&#20154;&#31867;&#30340;&#20132;&#27969;&#32773;&#26469;&#36234;&#29425;&#65292;&#20197;&#25506;&#32034;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#34987;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#35828;&#26381;LLMs&#36234;&#29425;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#20960;&#21313;&#24180;&#30340;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#35828;&#26381;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#36825;&#20010;&#20998;&#31867;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#35828;&#26381;&#23545;&#25239;&#25552;&#31034;&#65288;PAP&#65289;&#26469;&#36234;&#29425;LLMs&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#25152;&#26377;&#39118;&#38505;&#31867;&#21035;&#19978;PAP&#22312;Llama 2-7b Chat&#12289;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#22312;10&#27425;&#35797;&#39564;&#20013;&#22343;&#36229;&#36807;92%&#65292;&#36229;&#36807;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#31639;&#27861;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#20197;&#37329;&#34701;&#39046;&#22495;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#21644;&#23433;&#20840;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#65292;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.02981</link><description>&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning and Utilization Methods of Domain-specific LLMs. (arXiv:2401.02981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#20197;&#37329;&#34701;&#39046;&#22495;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#21644;&#23433;&#20840;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#65292;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#20851;&#20110;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#24212;&#29992;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;LLM&#30340;&#36235;&#21183;&#12289;&#22522;&#30784;&#27169;&#22411;&#21644;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20197;&#37329;&#34701;&#34892;&#19994;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#20851;&#38190;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#38024;&#23545;&#37329;&#34701;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#65292;&#20197;&#21450;&#23433;&#20840;&#24615;&#21644;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;LLM&#24494;&#35843;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent releases of pre-trained Large Language Models (LLMs) have gained considerable traction, yet research on fine-tuning and employing domain-specific LLMs remains scarce. This study investigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs, foundational models, and methods for domain-specific pre-training. Focusing on the financial sector, it details dataset selection, preprocessing, model choice, and considerations crucial for LLM fine-tuning in finance. Addressing the unique characteristics of financial data, the study explores the construction of domain-specific vocabularies and considerations for security and regulatory compliance. In the practical application of LLM fine-tuning, the study outlines the procedure and implementation for generating domain-specific LLMs in finance. Various financial cases, including stock price prediction, sentiment analysis of financial news, automated document processing, research, information extract
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#27597;&#23156;&#20581;&#24247;&#39046;&#22495;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#65292;&#24182;&#36890;&#36807;&#35843;&#26597;&#21644;&#30740;&#35752;&#20250;&#30340;&#20114;&#21160;&#35752;&#35770;&#65292;&#20174;&#21463;&#24433;&#21709;&#32773;&#30340;&#22768;&#38899;&#20013;&#24635;&#32467;&#20986;&#32467;&#26524;&#65292;&#20026;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31561;&#24037;&#20855;&#30340;&#20581;&#24247;&#24212;&#29992;&#25552;&#20379;&#20262;&#29702;&#26694;&#26550;&#21644;&#25351;&#23548;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2312.11803</link><description>&lt;p&gt;
&#27597;&#23156;&#20581;&#24247;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65306;&#22312;LLMs&#26102;&#20195;&#30340;&#35266;&#28857;&#21644;&#25351;&#23548;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
NLP for Maternal Healthcare: Perspectives and Guiding Principles in the Age of LLMs. (arXiv:2312.11803v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#27597;&#23156;&#20581;&#24247;&#39046;&#22495;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#65292;&#24182;&#36890;&#36807;&#35843;&#26597;&#21644;&#30740;&#35752;&#20250;&#30340;&#20114;&#21160;&#35752;&#35770;&#65292;&#20174;&#21463;&#24433;&#21709;&#32773;&#30340;&#22768;&#38899;&#20013;&#24635;&#32467;&#20986;&#32467;&#26524;&#65292;&#20026;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31561;&#24037;&#20855;&#30340;&#20581;&#24247;&#24212;&#29992;&#25552;&#20379;&#20262;&#29702;&#26694;&#26550;&#21644;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36843;&#20999;&#38656;&#35201;&#20262;&#29702;&#26694;&#26550;&#26469;&#25351;&#23548;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31561;&#24037;&#20855;&#36827;&#34892;&#21307;&#30103;&#24212;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#30528;&#26435;&#21147;&#24179;&#34913;&#12289;&#31995;&#32479;&#24615;&#20581;&#24247;&#24046;&#36317;&#12289;&#21382;&#21490;&#19981;&#20844;&#21644;&#32463;&#27982;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;&#22312;&#30452;&#25509;&#20542;&#21548;&#26368;&#21463;&#24433;&#21709;&#32773;&#30340;&#22768;&#38899;&#30340;&#22522;&#30784;&#19978;&#65292;&#24182;&#20197;&#26576;&#19968;&#29305;&#23450;&#21307;&#30103;&#29615;&#22659;&#30340;&#26696;&#20363;&#30740;&#31350;&#20026;&#37325;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#22312;&#27597;&#23156;&#20581;&#24247;&#39046;&#22495;&#20013;&#20351;&#29992;NLP&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20026;&#26399;&#19968;&#22825;&#30340;&#30740;&#35752;&#20250;&#19978;&#65292;&#32452;&#32455;&#20102;&#19968;&#20010;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#28436;&#31034;&#30340;&#20114;&#21160;&#20250;&#35805;&#65292;&#24182;&#39069;&#22806;&#35843;&#26597;&#20102;30&#21517;&#21307;&#30103;&#24037;&#20316;&#32773;&#21644;30&#21517;&#20998;&#23081;&#32773;&#23545;&#20110;&#27597;&#23156;&#20581;&#24247;&#32972;&#26223;&#19979;NLP&#24037;&#20855;&#30340;&#20215;&#20540;&#12289;&#38656;&#27714;&#21644;&#24863;&#30693;&#12290;&#25105;&#20204;&#23545;&#35843;&#26597;&#32467;&#26524;&#21644;&#20114;&#21160;&#35752;&#35770;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#24635;&#32467;&#20986;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ethical frameworks for the use of natural language processing (NLP) are urgently needed to shape how large language models (LLMs) and similar tools are used for healthcare applications. Healthcare faces existing challenges including the balance of power in clinician-patient relationships, systemic health disparities, historical injustices, and economic constraints. Drawing directly from the voices of those most affected, and focusing on a case study of a specific healthcare setting, we propose a set of guiding principles for the use of NLP in maternal healthcare. We led an interactive session centered on an LLM-based chatbot demonstration during a full-day workshop with 39 participants, and additionally surveyed 30 healthcare workers and 30 birthing people about their values, needs, and perceptions of NLP tools in the context of maternal health. We conducted quantitative and qualitative analyses of the survey results and interactive discussions to consolidate our findings into a set of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#23454;&#29616;&#35821;&#35328;&#24314;&#27169;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#21644;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#30828;&#20214;&#65292;&#35813;&#26041;&#27861;&#26377;&#26395;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.09084</link><description>&lt;p&gt;
&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Language Modeling on a SpiNNaker 2 Neuromorphic Chip. (arXiv:2312.09084v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#23454;&#29616;&#35821;&#35328;&#24314;&#27169;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#21644;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#30828;&#20214;&#65292;&#35813;&#26041;&#27861;&#26377;&#26395;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#36805;&#36895;&#22686;&#38271;&#65292;&#25152;&#38656;&#30340;&#35745;&#31639;&#33021;&#21147;&#20063;&#22312;&#22686;&#21152;&#12290;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#30340;&#20107;&#20214;&#39537;&#21160;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#26174;&#33879;&#38477;&#20302;&#25512;&#29702;&#33021;&#32791;&#30340;&#28508;&#22312;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#21487;&#20197;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#32593;&#32476;&#65292;&#21253;&#25324;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNN)&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#20219;&#21153;&#24615;&#33021;&#29978;&#33267;&#19981;&#33021;&#19982;LSTM&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22240;&#27492;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#20284;&#20046;&#26159;&#19968;&#20010;&#36965;&#36828;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411; - &#20855;&#20307;&#26469;&#35828;&#26159;&#22522;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#21517;&#20026;EGRU&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#30340;SpiNNaker 2&#33455;&#29255;&#12290;SpiNNaker 2&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#20247;&#26680;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#65292;&#32780;EGRU&#26159;&#20026;&#20102;&#22312;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#39640;&#25928;&#21033;&#29992;&#36825;&#31181;&#30828;&#20214;&#32780;&#35774;&#35745;&#30340;&#12290;&#36825;&#20010;&#23454;&#29616;&#26631;&#24535;&#30528;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#30340;&#31532;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to scale in size rapidly, so too does the computational power required to run them. Event-based networks on neuromorphic devices offer a potential way to reduce energy consumption for inference significantly. However, to date, most event-based networks that can run on neuromorphic hardware, including spiking neural networks (SNNs), have not achieved task performance even on par with LSTM models for language modeling. As a result, language modeling on neuromorphic devices has seemed a distant prospect. In this work, we demonstrate the first-ever implementation of a language model on a neuromorphic device - specifically the SpiNNaker 2 chip based on a recently published event-based architecture called the EGRU. SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale asynchronous processing, while the EGRU is architected to leverage such hardware efficiently while maintaining competitive task performance. This implementation marks the firs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24230;&#35780;&#20272;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22870;&#21169;&#27169;&#22411;&#23545;&#20110;&#25552;&#31034;&#21644;&#21709;&#24212;&#30340;&#36716;&#31227;&#20855;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#21576;&#29616;&#20986;&#26032;&#39062;&#30340;&#26657;&#20934;&#27169;&#24335;&#21644;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#21516;&#26102;&#65292;&#23558;&#24120;&#29992;&#30340;OOD&#26816;&#27979;&#25216;&#26415;&#24341;&#20837;&#21040;&#22870;&#21169;&#27169;&#22411;&#35774;&#32622;&#20013;&#65292;&#29992;&#20110;&#26816;&#27979;&#20998;&#24067;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2311.14743</link><description>&lt;p&gt;
&#22522;&#20934;&#20998;&#26512;&#22870;&#21169;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#20934;&#30830;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v7 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24230;&#35780;&#20272;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22870;&#21169;&#27169;&#22411;&#23545;&#20110;&#25552;&#31034;&#21644;&#21709;&#24212;&#30340;&#36716;&#31227;&#20855;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#21576;&#29616;&#20986;&#26032;&#39062;&#30340;&#26657;&#20934;&#27169;&#24335;&#21644;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#21516;&#26102;&#65292;&#23558;&#24120;&#29992;&#30340;OOD&#26816;&#27979;&#25216;&#26415;&#24341;&#20837;&#21040;&#22870;&#21169;&#27169;&#22411;&#35774;&#32622;&#20013;&#65292;&#29992;&#20110;&#26816;&#27979;&#20998;&#24067;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#21644;&#24212;&#29992;&#12290;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#26469;&#25429;&#25417;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#28982;&#21518;&#29992;&#20110;&#23545;&#40784;LLM&#12290;&#36825;&#20123;&#22870;&#21169;&#27169;&#22411;&#36824;&#22312;&#25512;&#26029;&#26102;&#29992;&#20110;&#20272;&#35745;LLM&#21709;&#24212;&#19982;&#26399;&#26395;&#34892;&#20026;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#26469;&#34913;&#37327;&#36825;&#20123;&#22870;&#21169;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36890;&#36807;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24230;&#65288;&#21363;&#20934;&#30830;&#24615;&#21644;&#20449;&#24515;&#30340;&#21305;&#37197;&#31243;&#24230;&#65289;&#34913;&#37327;&#30340;&#22870;&#21169;&#27169;&#22411;&#24615;&#33021;&#22914;&#20309;&#21463;&#21040;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20110;OOD&#25552;&#31034;&#21644;&#21709;&#24212;&#32780;&#20135;&#29983;&#30340;&#26032;&#22411;&#26657;&#20934;&#27169;&#24335;&#21644;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#24182;&#19988;&#21457;&#29616;&#22870;&#21169;&#27169;&#22411;&#23545;&#21709;&#24212;&#30340;&#36716;&#31227;&#27604;&#25552;&#31034;&#26356;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#24120;&#29992;&#20110;&#20998;&#31867;&#30340;OOD&#26816;&#27979;&#25216;&#26415;&#36866;&#24212;&#21040;&#22870;&#21169;&#27169;&#22411;&#35774;&#32622;&#20013;&#65292;&#20197;&#26816;&#27979;&#36825;&#20123;&#25552;&#31034;&#21644;&#21709;&#24212;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models, specifically Large Language Models (LLMs), have lately gained wide-spread attention and adoption. Reinforcement Learning with Human Feedback (RLHF) involves training a reward model to capture desired behaviors, which is then used to align LLM's. These reward models are additionally used at inference-time to estimate LLM responses' adherence to those desired behaviors. However, there is little work measuring how robust these reward models are to distribution shifts. In this work, we evaluate how reward model performance measured via accuracy and calibration (i.e. alignment between accuracy and confidence) - is affected by distribution shift. We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts. Additionally, we adapt an OOD detection technique commonly used in classification to the reward model setting to detect these distribution shifts in prompts and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#32423;&#22768;&#26126;&#20013;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#24555;&#36895;&#35774;&#35745;&#21644;&#23454;&#26045;&#12290;</title><link>http://arxiv.org/abs/2310.08535</link><description>&lt;p&gt;
&#27491;&#24335;&#35268;&#23450;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#39640;&#32423;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Formally Specifying the High-Level Behavior of LLM-Based Agents. (arXiv:2310.08535v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#32423;&#22768;&#26126;&#20013;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#24555;&#36895;&#35774;&#35745;&#21644;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#30446;&#26631;&#39537;&#21160;&#22411;&#20195;&#29702;&#20154;&#24050;&#25104;&#20026;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#33719;&#24471;&#26114;&#36149;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#12290;&#30446;&#21069;&#65292;&#36825;&#31867;&#20195;&#29702;&#20154;&#30340;&#35774;&#35745;&#21644;&#23454;&#26045;&#26159;&#20020;&#26102;&#24615;&#30340;&#65292;&#22240;&#20026;LLM-based&#20195;&#29702;&#20154;&#21487;&#33021;&#24212;&#29992;&#20110;&#30340;&#21508;&#31181;&#20219;&#21153;&#30340;&#24191;&#27867;&#24615;&#36136;&#24847;&#21619;&#30528;&#19981;&#33021;&#26377;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#20195;&#29702;&#20154;&#35774;&#35745;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#31616;&#21270;&#20195;&#29702;&#20154;&#26500;&#24314;&#36807;&#31243;&#30340;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#26469;&#20943;&#36731;&#35774;&#35745;&#21644;&#23454;&#26045;&#26032;&#20195;&#29702;&#20154;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#30340;&#26694;&#26550;&#20801;&#35768;&#29992;&#25143;&#20197;&#39640;&#32423;&#22768;&#26126;&#30340;&#35268;&#33539;&#26041;&#24335;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#35268;&#33539;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20197;&#30830;&#20445;LLM&#20250;&#20135;&#29983;&#20855;&#26377;&#25152;&#38656;&#34892;&#20026;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#22768;&#26126;&#24615;&#26041;&#27861;&#65292;&#21363;&#25551;&#36848;&#34892;&#20026;&#32780;&#19981;&#32771;&#34385;&#22914;&#20309;&#23454;&#26045;&#25110;&#24378;&#21046;&#25191;&#34892;&#65292;&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Autonomous, goal-driven agents powered by LLMs have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic generation framework that simplifies the process of building agents. The framework we introduce allows the user to define desired agent behaviors in a high-level, declarative specification that is then used to construct a decoding monitor which guarantees the LLM will produce an output exhibiting the desired behavior. Our declarative approach, in which the behavior is described without concern for how it should be implemented or enforced, enables rapid design,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#20195;&#29702;&#36171;&#20104;&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#20581;&#24247;&#25252;&#29702;&#26381;&#21153;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21307;&#30103;&#24037;&#20855;&#65292;&#23454;&#29616;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#65292;&#24182;&#19982;&#22810;&#31181;&#29992;&#25143;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2310.02374</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65306;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Conversational Health Agents: A Personalized LLM-Powered Agent Framework. (arXiv:2310.02374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#20195;&#29702;&#36171;&#20104;&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#20581;&#24247;&#25252;&#29702;&#26381;&#21153;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21307;&#30103;&#24037;&#20855;&#65292;&#23454;&#29616;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#65292;&#24182;&#19982;&#22810;&#31181;&#29992;&#25143;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65288;CHAs&#65289;&#26159;&#19968;&#31181;&#20114;&#21160;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#36827;&#34892;&#20849;&#24773;&#23545;&#35805;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#22686;&#24378;&#20010;&#20154;&#20581;&#24247;&#25252;&#29702;&#26381;&#21153;&#12290;&#24403;&#21069;&#30340;CHAs&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31995;&#32479;&#65292;&#20027;&#35201;&#20851;&#27880;&#23545;&#35805;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#20840;&#38754;&#30340;&#20195;&#29702;&#33021;&#21147;&#12290;&#36825;&#21253;&#25324;&#20174;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#20840;&#22825;&#20505;&#25968;&#25454;&#25910;&#38598;&#28304;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#33719;&#21462;&#20010;&#20154;&#29992;&#25143;&#30340;&#20581;&#24247;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25972;&#21512;&#26368;&#26032;&#21457;&#24067;&#30340;&#20581;&#24247;&#35265;&#35299;&#65292;&#24182;&#19982;&#24050;&#24314;&#31435;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36830;&#25509;&#12290;&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#36171;&#20104;CHAs&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26469;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CHA&#24179;&#21488;&#30001;LLMs&#39537;&#21160;&#65292;&#26080;&#32541;&#38598;&#25104;&#20102;&#21307;&#30103;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#65292;&#24182;&#19982;&#21508;&#31181;&#29992;&#25143;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36827;&#34892;&#25509;&#21475;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#22788;&#29702;&#22797;&#26434;&#21307;&#30103;&#20219;&#21153;&#26041;&#38754;&#30340;&#29087;&#32451;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational Health Agents (CHAs) are interactive systems designed to enhance personal healthcare services by engaging in empathetic conversations and processing multimodal data. While current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation, they often lack comprehensive agent capabilities. This includes the ability to access personal user health data from wearables, 24/7 data collection sources, and electronic health records, as well as integrating the latest published health insights and connecting with established multimodal data analysis tools. We are developing a framework to empower CHAs by equipping them with critical thinking, knowledge acquisition, and problem-solving abilities. Our CHA platform, powered by LLMs, seamlessly integrates healthcare tools, enables multilingual and multimodal conversations, and interfaces with a variety of user data analysis tools. We illustrate its proficiency in handling complex healthcare tasks, s
&lt;/p&gt;</description></item><item><title>&#22534;&#26632;&#27880;&#24847;&#21147;&#20026;Transformers&#27169;&#22411;&#22788;&#29702;&#23618;&#27425;&#27169;&#24335;&#25552;&#20379;&#20102;&#33021;&#21147;&#65292;&#36890;&#36807;&#32467;&#21512;&#22534;&#26632;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#21644;&#35782;&#21035;&#20219;&#24847;&#28145;&#24230;&#30340;&#35821;&#27861;&#32467;&#26500;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#35299;&#26512;&#38590;&#24230;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.01749</link><description>&lt;p&gt;
&#22534;&#26632;&#27880;&#24847;&#21147;: &#25552;&#21319;Transformers&#23545;&#23618;&#27425;&#27169;&#24335;&#24314;&#27169;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns. (arXiv:2310.01749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01749
&lt;/p&gt;
&lt;p&gt;
&#22534;&#26632;&#27880;&#24847;&#21147;&#20026;Transformers&#27169;&#22411;&#22788;&#29702;&#23618;&#27425;&#27169;&#24335;&#25552;&#20379;&#20102;&#33021;&#21147;&#65292;&#36890;&#36807;&#32467;&#21512;&#22534;&#26632;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#21644;&#35782;&#21035;&#20219;&#24847;&#28145;&#24230;&#30340;&#35821;&#27861;&#32467;&#26500;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#35299;&#26512;&#38590;&#24230;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23588;&#20854;&#26159;&#32553;&#25918;&#28857;&#31215;&#27880;&#24847;&#21147;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24050;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#23545;&#20110;&#22788;&#29702;&#20219;&#24847;&#23884;&#22871;&#28145;&#24230;&#30340;&#23618;&#27425;&#27169;&#24335;&#27809;&#26377;&#26426;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#35782;&#21035;&#26576;&#20123;&#21477;&#27861;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22534;&#26632;&#27880;&#24847;&#21147;&#65306;&#19968;&#31181;&#32467;&#21512;&#20102;&#22534;&#26632;&#30340;&#27880;&#24847;&#21147;&#25805;&#20316;&#31526;&#65292;&#21463;&#21040;&#23427;&#20204;&#19982;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#65288;CFLs&#65289;&#30340;&#29702;&#35770;&#32852;&#31995;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22534;&#26632;&#27880;&#24847;&#21147;&#31867;&#20284;&#20110;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#20294;&#23427;&#20855;&#26377;&#19981;&#38656;&#35201;&#21477;&#27861;&#30417;&#30563;&#30340;&#35821;&#27861;&#28508;&#22312;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65306;&#19982;&#30830;&#23450;&#24615;&#19979;&#25512;&#33258;&#21160;&#26426;&#65288;PDAs&#65289;&#30456;&#20851;&#30340;&#19968;&#31181;&#65292;&#20197;&#21450;&#22522;&#20110;&#38750;&#30830;&#23450;&#24615;PDAs&#30340;&#19968;&#31181;&#65292;&#36825;&#20351;&#24471;transformers&#33021;&#22815;&#35782;&#21035;&#20219;&#24847;&#30340;CFLs&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#22534;&#26632;&#27880;&#24847;&#21147;&#30340;transformers&#22312;&#23398;&#20064;&#26631;&#20934;transformers&#38590;&#20197;&#24212;&#23545;&#30340;CFLs&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#22312;&#26368;&#22823;&#35299;&#26512;&#38590;&#24230;&#30340;CFL&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22534;&#26632;&#27880;&#24847;&#21147;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17249</link><description>&lt;p&gt;
&#25209;&#37327;&#26657;&#20934;&#65306;&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#39640;&#25928;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;LLM&#23384;&#22312;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#21508;&#31181;&#20559;&#35265;&#22240;&#32032;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#26684;&#24335;&#12289;&#36873;&#25321;&#24615;&#30340;&#34920;&#36798;&#26041;&#24335;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26657;&#20934;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#24182;&#24674;&#22797;LLM&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35266;&#28857;&#24182;&#25581;&#31034;&#20102;&#22833;&#36133;&#26696;&#20363;&#12290;&#21463;&#36825;&#20123;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25209;&#37327;&#36755;&#20837;&#20013;&#25511;&#21046;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#21508;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;BC&#26159;&#38646;-shot&#12289;&#20165;&#25512;&#29702;&#21644;&#39069;&#22806;&#25104;&#26412;&#21487;&#24573;&#30053;&#12290;&#22312;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;BC&#20197;&#23454;&#29616;&#20840;&#37096;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08565</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#33021;&#21542;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23450;&#21046;&#20026;&#31526;&#21512;&#32454;&#31890;&#24230;&#23646;&#24615;&#65288;&#22914;&#24418;&#24335;&#65289;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#33267;&#23569;&#19968;&#20123;&#24102;&#26377;&#23646;&#24615;&#27880;&#37322;&#30340;&#30417;&#30563;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#31232;&#32570;&#20173;&#28982;&#26159;&#23558;&#27492;&#23450;&#21046;&#33021;&#21147;&#26222;&#21450;&#21040;&#26356;&#24191;&#27867;&#35821;&#35328;&#33539;&#22260;&#65292;&#23588;&#20854;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#19968;&#20010;&#29942;&#39048;&#12290;&#37492;&#20110;&#26368;&#36817;&#22312;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20316;&#20026;&#23545;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#36827;&#34892;&#23646;&#24615;&#25511;&#21046;&#33021;&#21147;&#36801;&#31227;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;NLLB-200&#27169;&#22411;&#23545;&#23646;&#24615;&#25511;&#21046;&#22120;&#30340;&#36801;&#31227;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#20004;&#31181;&#33539;&#24335;&#26159;&#20114;&#34917;&#30340;&#65292;&#36890;&#36807;&#19968;&#33268;&#30340;&#25913;&#36827;&#26469;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customizing machine translation models to comply with fine-grained attributes such as formality has seen tremendous progress recently. However, current approaches mostly rely on at least some supervised data with attribute annotation. Data scarcity therefore remains a bottleneck to democratizing such customization possibilities to a wider range of languages, lower-resource ones in particular. Given recent progress in pretrained massively multilingual translation models, we use them as a foundation to transfer the attribute controlling capabilities to languages without supervised data. In this work, we present a comprehensive analysis of transferring attribute controllers based on a pretrained NLLB-200 model. We investigate both training- and inference-time control techniques under various data scenarios, and uncover their relative strengths and weaknesses in zero-shot performance and domain robustness. We show that both paradigms are complementary, as shown by consistent improvements o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#24037;&#31243;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#21322;&#32467;&#26500;&#21270;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#22686;&#24378;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#39564;&#35777;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08347</link><description>&lt;p&gt;
&#29983;&#25104;&#21322;&#32467;&#26500;&#21270;&#35299;&#37322;&#30340;&#22870;&#21169;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reward Engineering for Generating Semi-structured Explanation. (arXiv:2309.08347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#24037;&#31243;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#21322;&#32467;&#26500;&#21270;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#22686;&#24378;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#39564;&#35777;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#32467;&#26500;&#21270;&#35299;&#37322;&#25551;&#32472;&#20102;&#19968;&#20010;&#25512;&#29702;&#32773;&#30340;&#38544;&#24335;&#36807;&#31243;&#21644;&#26174;&#24335;&#34920;&#31034;&#12290;&#36825;&#31181;&#35299;&#37322;&#31361;&#20986;&#20102;&#22312;&#29305;&#23450;&#26597;&#35810;&#20013;&#21487;&#29992;&#20449;&#24687;&#22914;&#20309;&#19982;&#25512;&#29702;&#32773;&#20174;&#20869;&#37096;&#26435;&#37325;&#20135;&#29983;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#26368;&#36817;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#29983;&#25104;&#32467;&#26500;&#21270;&#35299;&#37322;&#20197;&#39564;&#35777;&#27169;&#22411;&#30495;&#27491;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23545;&#20110;&#35268;&#27169;&#19981;&#26159;&#24456;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#25512;&#29702;&#32773;&#34987;&#26399;&#26395;&#23558;&#39034;&#24207;&#30340;&#31572;&#26696;&#19982;&#20307;&#29616;&#27491;&#30830;&#23637;&#31034;&#21644;&#27491;&#30830;&#25512;&#29702;&#36807;&#31243;&#30340;&#32467;&#26500;&#21270;&#35299;&#37322;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#30417;&#30563;&#24494;&#35843;(SFT)&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#28982;&#21518;&#22312;&#24378;&#21270;&#23398;&#20064;(RL)&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#24037;&#31243;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#22870;&#21169;&#32858;&#21512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
Semi-structured explanation depicts the implicit process of a reasoner with an explicit representation. This explanation highlights how available information in a specific query is supplemented with information a reasoner produces from its internal weights towards generating an answer. Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify model's true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs, as the reasoner is expected to couple a sequential answer with a structured explanation which embodies both the correct presentation and the correct reasoning process. In this work, we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge, and then introduce a carefully crafted reward engineering method in reinforcement learning (RL) to better address this problem. We investigate multiple reward aggregation methods and provide a de
&lt;/p&gt;</description></item><item><title>PromptASR&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#25552;&#31034;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#35821;&#38899;&#36716;&#24405;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#21069;&#19968;&#35805;&#35821;&#30340;&#30495;&#23454;&#25991;&#26412;&#20316;&#20026;&#20869;&#23481;&#25552;&#31034;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;ASR&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#38405;&#35835;&#22270;&#20070;&#25968;&#25454;&#38598;&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;21.9&#65285;&#21644;6.8&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#37319;&#29992;&#21333;&#35789;&#32423;&#20559;&#32622;&#21015;&#34920;&#20316;&#20026;&#25552;&#31034;&#26469;&#25552;&#39640;&#23545;&#32597;&#35265;&#21333;&#35789;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#20351;&#29992;&#39069;&#22806;&#30340;&#26679;&#24335;&#25552;&#31034;&#26469;&#24341;&#23548;ASR&#31995;&#32479;&#36755;&#20986;&#19981;&#21516;&#39118;&#26684;&#30340;&#36716;&#24405;&#12290;</title><link>http://arxiv.org/abs/2309.07414</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;PromptASR
&lt;/p&gt;
&lt;p&gt;
PromptASR for contextualized ASR with controllable style. (arXiv:2309.07414v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07414
&lt;/p&gt;
&lt;p&gt;
PromptASR&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#25552;&#31034;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#35821;&#38899;&#36716;&#24405;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#21069;&#19968;&#35805;&#35821;&#30340;&#30495;&#23454;&#25991;&#26412;&#20316;&#20026;&#20869;&#23481;&#25552;&#31034;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;ASR&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#38405;&#35835;&#22270;&#20070;&#25968;&#25454;&#38598;&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;21.9&#65285;&#21644;6.8&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#37319;&#29992;&#21333;&#35789;&#32423;&#20559;&#32622;&#21015;&#34920;&#20316;&#20026;&#25552;&#31034;&#26469;&#25552;&#39640;&#23545;&#32597;&#35265;&#21333;&#35789;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#20351;&#29992;&#39069;&#22806;&#30340;&#26679;&#24335;&#25552;&#31034;&#26469;&#24341;&#23548;ASR&#31995;&#32479;&#36755;&#20986;&#19981;&#21516;&#39118;&#26684;&#30340;&#36716;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#65292;&#25552;&#31034;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#20027;&#39064;&#25110;&#36923;&#36753;&#20851;&#31995;&#31561;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptASR&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#25552;&#31034;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;E2E ASR&#65289;&#31995;&#32479;&#20013;&#65292;&#20197;&#23454;&#29616;&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#35821;&#38899;&#36716;&#24405;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#19987;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#20004;&#31181;&#27169;&#24577;&#30340;&#29305;&#24449;&#20132;&#20114;&#23558;&#32534;&#30721;&#27880;&#20837;&#21040;&#35821;&#38899;&#32534;&#30721;&#22120;&#20013;&#12290;&#24403;&#20351;&#29992;&#21069;&#38754;&#35805;&#35821;&#30340;&#30495;&#23454;&#25991;&#26412;&#20316;&#20026;&#20869;&#23481;&#25552;&#31034;&#26102;&#65292;&#19982;&#22522;&#32447;ASR&#31995;&#32479;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#38405;&#35835;&#22270;&#20070;&#25968;&#25454;&#38598;&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;21.9&#65285;&#21644;6.8&#65285;&#30340;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;&#31995;&#32479;&#36824;&#21487;&#20197;&#37319;&#29992;&#21333;&#35789;&#32423;&#20559;&#32622;&#21015;&#34920;&#20316;&#20026;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#23545;&#32597;&#35265;&#21333;&#35789;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#36824;&#21487;&#20197;&#32473;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#26679;&#24335;&#25552;&#31034;&#65292;&#24182;&#24341;&#23548;ASR&#31995;&#32479;&#36755;&#20986;&#19981;&#21516;&#39118;&#26684;&#30340;&#36716;&#24405;&#12290;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompts are crucial to large language models as they provide context information such as topic or logical relationships. Inspired by this, we propose PromptASR, a framework that integrates prompts in end-to-end automatic speech recognition (E2E ASR) systems to achieve contextualized ASR with controllable style of transcriptions. Specifically, a dedicated text encoder encodes the text prompts and the encodings are injected into the speech encoder by cross-attending the features from two modalities. When using the ground truth text from preceding utterances as content prompt, the proposed system achieves 21.9% and 6.8% relative word error rate reductions on a book reading dataset and an in-house dataset compared to a baseline ASR system. The system can also take word-level biasing lists as prompt to improve recognition accuracy on rare words. An additional style prompt can be given to the text encoder and guide the ASR system to output different styles of transcriptions. The code is avai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#35745;&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20248;&#21270;&#20559;&#22909;&#30340;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#32570;&#20047;&#22870;&#21169;&#27169;&#22411;&#21644;&#20174;&#26368;&#20248;&#31574;&#30053;&#37319;&#26679;&#20559;&#22909;&#23545;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06657</link><description>&lt;p&gt;
&#32479;&#35745;&#25298;&#32477;&#25277;&#26679;&#25913;&#36827;&#20102;&#20248;&#21270;&#20559;&#22909;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Rejection Sampling Improves Preference Optimization. (arXiv:2309.06657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#35745;&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20248;&#21270;&#20559;&#22909;&#30340;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#32570;&#20047;&#22870;&#21169;&#27169;&#22411;&#21644;&#20174;&#26368;&#20248;&#31574;&#30053;&#37319;&#26679;&#20559;&#22909;&#23545;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#36890;&#36807;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22914;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;&#26041;&#27861;&#22914;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#65288;SLiC&#65289;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#24050;&#32463;&#25104;&#20026;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;SLiC&#36890;&#36807;&#20351;&#29992;&#20174;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#31574;&#30053;&#20013;&#37319;&#26679;&#30340;&#24207;&#21015;&#23545;&#26469;&#20248;&#21270;&#20854;&#25439;&#22833;&#20989;&#25968;&#65292;&#32780;DPO&#30452;&#25509;&#26681;&#25454;&#20559;&#22909;&#25968;&#25454;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#26368;&#20248;&#31574;&#30053;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65288;MLE&#65289;&#38656;&#35201;&#20174;&#35813;&#31574;&#30053;&#20013;&#37319;&#26679;&#26631;&#35760;&#30340;&#20559;&#22909;&#23545;&#12290;DPO&#32570;&#20047;&#22870;&#21169;&#27169;&#22411;&#38480;&#21046;&#20854;&#20174;&#26368;&#20248;&#31574;&#30053;&#20013;&#37319;&#26679;&#20559;&#22909;&#23545;&#30340;&#33021;&#21147;&#65292;&#32780;SLiC&#21017;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. DPO's lack of a reward model constrains its ability to sample preference pairs from the optimal policy, and SLiC is restricted t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15812</link><description>&lt;p&gt;
&#36879;&#36807;&#20559;&#22909;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#33719;&#21462;&#65306;&#25581;&#31034;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#30340;&#23545;&#40784;&#25215;&#35834;&#28041;&#21450;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#21453;&#39304;&#12290;&#31264;&#23494;&#30340;&#21453;&#39304;&#27880;&#37322;&#33719;&#21462;&#21644;&#25972;&#21512;&#25104;&#26412;&#36739;&#39640;&#65292;&#32780;&#31232;&#30095;&#30340;&#21453;&#39304;&#21017;&#28041;&#21450;&#32467;&#26500;&#24615;&#35774;&#35745;&#36873;&#25321;&#65292;&#21363;&#35780;&#20998;&#65288;&#20363;&#22914;&#65292;&#22312;1-7&#30340;&#33539;&#22260;&#20869;&#23545;&#22238;&#31572;A&#36827;&#34892;&#35780;&#20998;&#65289;&#21644;&#25490;&#21517;&#65288;&#20363;&#22914;&#65292;&#22238;&#31572;A&#26159;&#21542;&#27604;&#22238;&#31572;B&#26356;&#22909;&#65311;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#23545;LLMs&#30340;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#22312;&#20154;&#31867;&#21644;AI&#27880;&#37322;&#32773;&#20013;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;60%&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#20998;&#26512;&#30830;&#23450;&#20102;&#35299;&#37322;&#36825;&#20010;&#29616;&#35937;&#30340;&#21508;&#31181;&#27880;&#37322;&#32773;&#20559;&#35265;&#26041;&#38754;&#65292;&#27604;&#22914;&#20154;&#31867;&#27880;&#37322;&#32773;&#26356;&#21916;&#27426;&#23494;&#38598;&#22238;&#31572;&#24182;&#22312;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#26356;&#38738;&#30544;&#20934;&#30830;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#23545;&#23545;&#40784;&#30340;LLMs&#30340;&#35780;&#20272;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#35780;&#20272;&#32467;&#26524;&#22240;&#20026;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
&lt;/p&gt;</description></item><item><title>CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12539</link><description>&lt;p&gt;
CALM: &#19968;&#31181;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12539
&lt;/p&gt;
&lt;p&gt;
CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#37327;&#21270;&#21644;&#27604;&#36739;&#23427;&#20204;&#22312;&#31038;&#20250;&#21644;&#20154;&#21475;&#23398;&#20559;&#35265;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#28508;&#22312;&#30340;&#21361;&#23475;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#20559;&#35265;&#27979;&#37327;&#25968;&#25454;&#38598;&#23545;&#20110;&#20154;&#24037;&#35774;&#35745;&#27169;&#26495;&#30340;&#25200;&#21160;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65288;CALM&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37327;&#21270;LMs&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#26032;&#38395;&#25991;&#31456;&#65289;&#30340;16&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#36807;&#28388;&#20986;224&#20010;&#27169;&#26495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;78,400&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#27169;&#26495;&#38271;&#24230;&#30340;&#21464;&#24322;&#31243;&#24230;&#31561;&#25351;&#26631;&#65292;&#27604;&#36739;CALM&#19982;&#20808;&#21069;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#32454;&#24494;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#22240;&#27492;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;20&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
&lt;/p&gt;</description></item><item><title>VELMA&#26159;&#19968;&#20010;&#21475;&#22836;&#21270;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20154;&#31867;&#20889;&#20316;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#30340;&#22320;&#26631;&#24182;&#32467;&#21512;CLIP&#26469;&#36827;&#34892;&#35270;&#35273;&#29615;&#22659;&#30340;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#22312;&#34903;&#26223;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2307.06082</link><description>&lt;p&gt;
VELMA: LLM&#26234;&#33021;&#20307;&#22312;&#34903;&#26223;&#20013;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#21475;&#22836;&#21270;&#20307;&#29616;
&lt;/p&gt;
&lt;p&gt;
VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06082
&lt;/p&gt;
&lt;p&gt;
VELMA&#26159;&#19968;&#20010;&#21475;&#22836;&#21270;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20154;&#31867;&#20889;&#20316;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#30340;&#22320;&#26631;&#24182;&#32467;&#21512;CLIP&#26469;&#36827;&#34892;&#35270;&#35273;&#29615;&#22659;&#30340;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#22312;&#34903;&#26223;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#22686;&#37327;&#20915;&#31574;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20197;&#20307;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20854;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20043;&#19968;&#26159;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;(VLN)&#65292;&#23427;&#38656;&#35201;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20197;&#21450;&#31354;&#38388;&#21644;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20010;&#20307;&#29616;&#26234;&#33021;&#20307;&#38656;&#35201;&#22312;&#34903;&#26223;&#31561;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#35266;&#23519;&#22522;&#30784;&#19978;&#20934;&#30830;&#29702;&#35299;&#23548;&#33322;&#25351;&#20196;&#12290;&#23613;&#31649;LLM&#22312;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#23427;&#20204;&#19982;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#36830;&#25509;&#36215;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VELMA&#65292;&#19968;&#31181;&#20351;&#29992;&#36712;&#36857;&#21644;&#35270;&#35273;&#29615;&#22659;&#35266;&#23519;&#30340;&#21475;&#22836;&#21270;&#20316;&#20026;&#19979;&#19968;&#27493;&#25805;&#20316;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;LLM&#26234;&#33021;&#20307;&#12290;&#35270;&#35273;&#20449;&#24687;&#36890;&#36807;&#19968;&#20010;&#27969;&#31243;&#36827;&#34892;&#21475;&#22836;&#21270;&#65292;&#35813;&#27969;&#31243;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#25552;&#21462;&#22320;&#26631;&#65292;&#24182;&#20351;&#29992;CLIP&#26469;&#30830;&#23450;&#23427;&#20204;&#22312;&#24403;&#21069;&#20840;&#26223;&#35270;&#22270;&#20013;&#30340;&#21487;&#35265;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SynGen&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#27861;&#20998;&#26512;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#30340;&#23545;&#40784;&#26469;&#35299;&#20915;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#20307;&#21644;&#35270;&#35273;&#23646;&#24615;&#20043;&#38388;&#38169;&#35823;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08877</link><description>&lt;p&gt;
&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#32465;&#23450;&#65306;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#23545;&#40784;&#22686;&#24378;&#23646;&#24615;&#23545;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment. (arXiv:2306.08877v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SynGen&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#27861;&#20998;&#26512;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#30340;&#23545;&#40784;&#26469;&#35299;&#20915;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#20307;&#21644;&#35270;&#35273;&#23646;&#24615;&#20043;&#38388;&#38169;&#35823;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24120;&#24120;&#22312;&#23454;&#20307;&#21644;&#20854;&#35270;&#35273;&#23646;&#24615;&#20043;&#38388;&#29983;&#25104;&#38169;&#35823;&#30340;&#20851;&#32852;&#12290;&#36825;&#21453;&#26144;&#20102;&#22312;&#25552;&#31034;&#20013;&#30340;&#23454;&#20307;&#21644;&#20462;&#39280;&#31526;&#30340;&#35821;&#35328;&#32465;&#23450;&#20197;&#21450;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30456;&#24212;&#20803;&#32032;&#30340;&#35270;&#35273;&#32465;&#23450;&#20043;&#38388;&#30340;&#26144;&#23556;&#21463;&#21040;&#25439;&#23475;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#31867;&#20284;&#8220;&#19968;&#20010;&#31881;&#33394;&#30340;&#21521;&#26085;&#33909;&#21644;&#19968;&#20010;&#40644;&#33394;&#30340;&#28779;&#28872;&#40479;&#8221;&#30340;&#26597;&#35810;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#20135;&#29983;&#19968;&#24352;&#40644;&#33394;&#30340;&#21521;&#26085;&#33909;&#21644;&#19968;&#21482;&#31881;&#33394;&#30340;&#28779;&#28872;&#40479;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SynGen&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#23545;&#25552;&#31034;&#36827;&#34892;&#21477;&#27861;&#20998;&#26512;&#20197;&#35782;&#21035;&#23454;&#20307;&#21644;&#23427;&#20204;&#30340;&#20462;&#39280;&#31526;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#26032;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#40723;&#21169;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#19982;&#35821;&#35328;&#32465;&#23450;&#30340;&#35821;&#27861;&#19968;&#33268;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#40723;&#21169;&#23454;&#20307;&#21644;&#20854;&#20462;&#39280;&#31526;&#30340;&#27880;&#24847;&#21147;&#22270;&#20043;&#38388;&#26377;&#22823;&#37327;&#30340;&#37325;&#21472;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#23454;&#20307;&#21644;&#20462;&#39280;&#31526;&#35789;&#20043;&#38388;&#30340;&#37325;&#21472;&#24456;&#23567;&#12290;&#25439;&#22833;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#12290;&#23545;&#19977;&#20010;&#25552;&#31034;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#20154;&#24037;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models often generate incorrect associations between entities and their visual attributes. This reflects an impaired mapping between linguistic binding of entities and modifiers in the prompt and visual binding of the corresponding elements in the generated image. As one notable example, a query like "a pink sunflower and a yellow flamingo" may incorrectly produce an image of a yellow sunflower and a pink flamingo. To remedy this issue, we propose SynGen, an approach which first syntactically analyses the prompt to identify entities and their modifiers, and then uses a novel loss function that encourages the cross-attention maps to agree with the linguistic binding reflected by the syntax. Specifically, we encourage large overlap between attention maps of entities and their modifiers, and small overlap with other entities and modifier words. The loss is optimized during inference, without retraining or fine-tuning the model. Human evaluation on three d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28526;&#27969;&#20013;&#65292;&#36824;&#24212;&#25512;&#24191;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20197; StackOverflow &#20026;&#20363;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03268</link><description>&lt;p&gt;
&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;&#30456;&#27604;&#19968;&#38149;&#31909;&#24335;&#27169;&#22411;&#65292;&#21315;&#19975;&#19981;&#35201;&#35753;&#39046;&#22495;&#30340;&#20379;&#32473;&#19981;&#36275;&#21463;&#21040;&#27874;&#21450;
&lt;/p&gt;
&lt;p&gt;
Stack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models. (arXiv:2306.03268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28526;&#27969;&#20013;&#65292;&#36824;&#24212;&#25512;&#24191;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20197; StackOverflow &#20026;&#20363;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;OpenAI&#30340;GPT&#31995;&#21015;&#65289;&#20026;NLP&#21644;&#36719;&#20214;&#24037;&#31243;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36861;&#27714;&#22823;&#32780;&#20840;&#30340;&#28526;&#27969;&#24212;&#35813;&#19982;&#38024;&#23545;&#29305;&#23450;&#30446;&#30340;&#12289;&#35268;&#27169;&#36866;&#20013;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#26412;&#25991;&#20197;StackOverflow&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#39564;&#35777;&#22256;&#24785;&#24230;&#21644;&#36801;&#31227;&#23398;&#20064;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained neural language models have brought immense progress to both NLP and software engineering. Models in OpenAI's GPT series now dwarf Google's BERT and Meta's RoBERTa, which previously set new benchmarks on a wide range of NLP applications. These models are trained on massive corpora of heterogeneous data from web crawls, which enables them to learn general language patterns and semantic relationships. However, the largest models are both expensive to train and deploy and are often closed-source, so we lack access to their data and design decisions. We argue that this trend towards large, general-purpose models should be complemented with single-purpose, more modestly sized pre-trained models. In this work, we take StackOverflow (SO) as a domain example in which large volumes of rich aligned code and text data is available. We adopt standard practices for pre-training large language models, including using a very large context size (2,048 tokens), batch size (0.5M tokens
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#20013;&#65292;&#35201;&#20351;&#29992;&#22810;&#20010;&#26381;&#21153;&#22120;&#36149;&#37325;&#30340;GPU&#23548;&#33268;&#26174;&#33879;&#30340;&#25104;&#26412;&#38556;&#30861;&#65292;OWQ&#25552;&#20986;&#30340;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#36136;&#37327;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#36825;&#31181;&#38480;&#21046;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#28608;&#27963;&#31163;&#32676;&#20540;&#26469;&#30830;&#23450;&#26435;&#20540;&#37327;&#21270;&#35823;&#24046;&#30340;&#22240;&#32032;&#65292;&#24182;&#20026;&#26131;&#21463;&#25915;&#20987;&#30340;&#26435;&#37325;&#20998;&#37197;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#19982;OPTQ&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.02272</link><description>&lt;p&gt;
OWQ&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20013;&#28608;&#27963;&#31163;&#32676;&#20540;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
OWQ: Lessons learned from activation outliers for weight quantization in large language models. (arXiv:2306.02272v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02272
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#20013;&#65292;&#35201;&#20351;&#29992;&#22810;&#20010;&#26381;&#21153;&#22120;&#36149;&#37325;&#30340;GPU&#23548;&#33268;&#26174;&#33879;&#30340;&#25104;&#26412;&#38556;&#30861;&#65292;OWQ&#25552;&#20986;&#30340;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#36136;&#37327;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#36825;&#31181;&#38480;&#21046;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#28608;&#27963;&#31163;&#32676;&#20540;&#26469;&#30830;&#23450;&#26435;&#20540;&#37327;&#21270;&#35823;&#24046;&#30340;&#22240;&#32032;&#65292;&#24182;&#20026;&#26131;&#21463;&#25915;&#20987;&#30340;&#26435;&#37325;&#20998;&#37197;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#19982;OPTQ&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#21644;&#23569;&#37327;&#30340;&#31034;&#20363;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#21497;&#30340;&#32467;&#26524;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24040;&#22823;&#30340;&#23610;&#23544;&#35201;&#27714;&#29978;&#33267;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#26381;&#21153;&#22120;&#32423;&#30340;GPU&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#25104;&#26412;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#26469;&#37327;&#21270;&#26435;&#37325;&#65292;&#20943;&#23569;&#36136;&#37327;&#25439;&#22833;&#12290;&#34429;&#28982;&#24050;&#30693;&#28608;&#27963;&#31163;&#32676;&#20540;&#22312;&#28608;&#27963;&#37327;&#21270;&#20013;&#23384;&#22312;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;&#32771;&#34385;&#28608;&#27963;&#31163;&#32676;&#20540;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#23548;&#33268;&#26435;&#37325;&#37327;&#21270;&#35823;&#24046;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#21517;&#20026;Outlier-Aware Weight Quantization (OWQ)&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#26435;&#37325;&#24182;&#20026;&#23427;&#20204;&#20998;&#37197;&#39640;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;OWQ&#29983;&#25104;&#30340;3.01&#20301;&#27169;&#22411;&#20855;&#26377;&#19982;OPTQ&#29983;&#25104;&#30340;4&#20301;&#27169;&#22411;&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with hundreds of billions of parameters show impressive results across various language tasks using simple prompt tuning and few-shot examples, without the need for task-specific fine-tuning. However, their enormous size requires multiple server-grade GPUs even for inference, creating a significant cost barrier. To address this limitation, we introduce a novel post-training quantization method for weights with minimal quality degradation. While activation outliers are known to be problematic in activation quantization, our theoretical analysis suggests that we can identify factors contributing to weight quantization errors by considering activation outliers. We propose an innovative PTQ scheme called outlier-aware weight quantization (OWQ), which identifies vulnerable weights and allocates high-precision to them. Our extensive experiments demonstrate that the 3.01-bit models produced by OWQ exhibit comparable quality to the 4-bit models generated by OPTQ.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#38646;-shot&#25490;&#21517;&#33021;&#21147;&#65292;&#20294;&#22312;&#24863;&#30693;&#21382;&#21490;&#20114;&#21160;&#39034;&#24207;&#21644;&#21463;&#21040;&#20559;&#35265;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#25552;&#31034;&#21644;&#24341;&#23548;&#31574;&#30053;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08845</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;-shot&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Zero-Shot Rankers for Recommender Systems. (arXiv:2305.08845v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08845
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#38646;-shot&#25490;&#21517;&#33021;&#21147;&#65292;&#20294;&#22312;&#24863;&#30693;&#21382;&#21490;&#20114;&#21160;&#39034;&#24207;&#21644;&#21463;&#21040;&#20559;&#35265;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#25552;&#31034;&#21644;&#24341;&#23548;&#31574;&#30053;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-4&#65289;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#65292;&#21253;&#25324;&#28508;&#21147;&#25509;&#36817;&#25512;&#33616;&#20219;&#21153;&#12290;&#22312;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#19978;&#65292;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#27169;&#22411;&#30340;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with thes
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#35268;&#27169;&#19978;&#30340;&#35299;&#37322;&#24615;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#25628;&#32034;&#21040;&#35299;&#37322;&#24615;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;Alpaca&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.08809</link><description>&lt;p&gt;
&#35268;&#27169;&#19978;&#30340;&#35299;&#37322;&#24615;&#65306;&#22312;Alpaca&#20013;&#35782;&#21035;&#22240;&#26524;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpretability at Scale: Identifying Causal Mechanisms in Alpaca. (arXiv:2305.08809v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08809
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#35268;&#27169;&#19978;&#30340;&#35299;&#37322;&#24615;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#25628;&#32034;&#21040;&#35299;&#37322;&#24615;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;Alpaca&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;AI&#23433;&#20840;&#32780;&#35328;&#65292;&#33719;&#24471;&#22823;&#22411;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#35299;&#37322;&#26159;&#19968;&#20010;&#32039;&#24613;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#25105;&#20204;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#33021;&#22815;&#24544;&#23454;&#20110;&#27169;&#22411;&#34892;&#20026;&#24213;&#23618;&#30340;&#22240;&#26524;&#21160;&#21147;&#23398;&#65292;&#19988;&#33021;&#22815;&#22312;&#26410;&#35265;&#36755;&#20837;&#19978;&#20855;&#26377;&#40065;&#26834;&#27867;&#21270;&#24615;&#12290;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#28176;&#21464;&#19979;&#38477;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#19968;&#31181;&#22240;&#26524;&#25277;&#35937;&#29702;&#35770;&#65292;&#24050;&#32463;&#21457;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#31639;&#27861;&#21644;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#32454;&#35843;&#30340;&#23567;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#23436;&#32654;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29992;&#23398;&#20064;&#24471;&#21040;&#30340;&#21442;&#25968;&#26469;&#26367;&#25442;&#21097;&#20313;&#30340;&#34542;&#21147;&#25628;&#32034;&#27493;&#39588;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;DAS&#65292;&#36825;&#31181;&#26041;&#27861;&#25105;&#20204;&#31216;&#20043;&#20026;&#26080;&#36793;&#30028;DAS&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#39640;&#25928;&#22320;&#25628;&#32034;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#21516;&#26102;&#23427;&#20204;&#36981;&#24490;&#25351;&#20196;&#12290;&#25105;&#20204;&#23558;&#26080;&#36793;&#30028;DAS&#24212;&#29992;&#20110;Alpaca&#27169;&#22411;&#65288;7B&#21442;&#25968;&#65289;&#65292;&#23427;&#21487;&#20197;&#24555;&#36895;&#35299;&#20915;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#12290;&#36890;&#36807;&#26080;&#36793;&#30028;DAS&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that has uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#25104;&#36890;&#29992;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#30340;&#32454;&#33410;&#21487;&#33021;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#20316;&#32773;&#23545;COGS&#22522;&#20934;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#36275;&#22815;&#30340;&#25484;&#25569;&#12290;&#20316;&#32773;&#36824;&#24378;&#35843;&#20102;&#35774;&#35745;&#33021;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#35821;&#20041;&#30340;LF&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13716</link><description>&lt;p&gt;
ReCOGS: &#19968;&#20010;&#36923;&#36753;&#24418;&#24335;&#30340;&#32454;&#33410;&#22914;&#20309;&#24433;&#21709;&#35821;&#20041;&#35299;&#37322;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation. (arXiv:2303.13716v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#25104;&#36890;&#29992;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#30340;&#32454;&#33410;&#21487;&#33021;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#20316;&#32773;&#23545;COGS&#22522;&#20934;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#36275;&#22815;&#30340;&#25484;&#25569;&#12290;&#20316;&#32773;&#36824;&#24378;&#35843;&#20102;&#35774;&#35745;&#33021;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#35821;&#20041;&#30340;LF&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#36890;&#29992;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#35745;&#31639;&#26032;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#20294;&#26159;&#29992;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#39044;&#27979;&#26469;&#25805;&#20316;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#25285;&#24551;&#65292;&#21363;&#25152;&#36873;&#25321;&#30340;LF&#30340;&#35821;&#20041;&#26080;&#20851;&#30340;&#32454;&#33410;&#21487;&#33021;&#20250;&#22609;&#36896;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;COGS&#22522;&#20934;&#65288;Kim&#21644;Linzen&#65292;2020&#65289;&#23454;&#29616;&#20102;&#36825;&#19968;&#20851;&#27880;&#28857;&#12290;COGS&#25552;&#20986;&#20102;&#30475;&#36215;&#26469;&#23545;&#29616;&#26377;&#27169;&#22411;&#26469;&#35828;&#19981;&#21487;&#33021;&#30340;&#36890;&#29992;&#20998;&#21106;&#65292;&#36825;&#21487;&#33021;&#34987;&#35270;&#20026;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#25511;&#35785;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#36127;&#38754;&#32467;&#26524;&#36319;COGS LFs&#30340;&#32454;&#33410;&#30456;&#20851;&#12290;&#23558;&#36825;&#20123;LF&#36716;&#25442;&#20026;&#35821;&#20041;&#31561;&#25928;&#30340;LF&#65292;&#24182;&#20998;&#35299;&#20986;&#19982;&#35821;&#20041;&#35299;&#37322;&#26080;&#20851;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#26159;&#22522;&#32447;&#27169;&#22411;&#20063;&#33021;&#33719;&#24471;&#36275;&#22815;&#30340;&#25484;&#25569;&#12290;&#26368;&#36817;&#30340;COGS LFs&#26080;&#21464;&#37327;&#32763;&#35793;&#34920;&#26126;&#20102;&#31867;&#20284;&#30340;&#32467;&#35770;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#26684;&#24335;&#19981;&#26159;&#35821;&#20041;&#31561;&#25928;&#30340;&#65307;&#23427;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19968;&#20123;COGS&#30340;&#21547;&#20041;&#12290;&#36825;&#20123;&#21457;&#29616;&#20419;&#36827;&#25105;&#20204;&#23545;&#24403;&#21069;&#30340;&#21512;&#25104;&#36890;&#29992;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#24378;&#35843;&#35774;&#35745;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#35821;&#20041;&#30340;LF&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization benchmarks seek to assess whether models can accurately compute meanings for novel sentences, but operationalize this in terms of logical form (LF) prediction. This raises the concern that semantically irrelevant details of the chosen LFs could shape model performance. We argue that this concern is realized for the COGS benchmark (Kim and Linzen, 2020). COGS poses generalization splits that appear impossible for present-day models, which could be taken as an indictment of those models. However, we show that the negative results trace to incidental features of COGS LFs. Converting these LFs to semantically equivalent ones and factoring out capabilities unrelated to semantic interpretation, we find that even baseline models get traction. A recent variable-free translation of COGS LFs suggests similar conclusions, but we observe this format is not semantically equivalent; it is incapable of accurately representing some COGS meanings. These findings inform our 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#21487;&#25511;&#23454;&#39564;&#35777;&#26126;&#36716;&#31227;&#23398;&#20064;&#30340;&#22256;&#38590;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#20197;&#20174;&#21477;&#27861;&#39118;&#26684;&#30340;&#36716;&#21464;&#20013;&#24674;&#22797;&#65292;&#20294;&#26080;&#27861;&#20174;&#35789;&#27719;&#19981;&#23545;&#40784;&#21644;&#23884;&#20837;&#30697;&#38453;&#37325;&#26032;&#21021;&#22987;&#21270;&#20013;&#24674;&#22797;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#22823;&#37327;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2202.12312</link><description>&lt;p&gt;
Oolong: &#29992;&#21487;&#25511;&#23454;&#39564;&#35777;&#26126;&#36716;&#31227;&#23398;&#20064;&#30340;&#22256;&#38590;&#24615;
&lt;/p&gt;
&lt;p&gt;
Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies. (arXiv:2202.12312v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#21487;&#25511;&#23454;&#39564;&#35777;&#26126;&#36716;&#31227;&#23398;&#20064;&#30340;&#22256;&#38590;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#20197;&#20174;&#21477;&#27861;&#39118;&#26684;&#30340;&#36716;&#21464;&#20013;&#24674;&#22797;&#65292;&#20294;&#26080;&#27861;&#20174;&#35789;&#27719;&#19981;&#23545;&#40784;&#21644;&#23884;&#20837;&#30697;&#38453;&#37325;&#26032;&#21021;&#22987;&#21270;&#20013;&#24674;&#22797;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#22823;&#37327;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36716;&#31227;&#21040;&#26032;&#30340;&#35821;&#35328;&#26102;&#65292;&#26377;&#35768;&#22810;&#21464;&#21270;&#22240;&#32032;&#21516;&#26102;&#21457;&#29983;&#12290;&#20026;&#20102;&#35299;&#20132;&#21449;&#35821;&#35328;&#21464;&#20307;&#30340;&#19981;&#21516;&#22240;&#32032;&#65288;&#22914;&#21477;&#27861;&#30456;&#20284;&#24615;&#21644;&#35789;&#27719;&#30456;&#20284;&#24615;&#65289;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#25511;&#21046;&#30340;&#36716;&#31227;&#30740;&#31350;&#65306;&#25105;&#20204;&#31995;&#32479;&#22320;&#36716;&#25442;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#35821;&#35328;&#65292;&#36880;&#20010;&#25913;&#21464;&#19968;&#31181;&#20132;&#21449;&#35821;&#35328;&#21464;&#20307;&#30340;&#22240;&#32032;&#65292;&#28982;&#21518;&#27979;&#37327;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21518;&#32493;&#24615;&#33021;&#20013;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#21477;&#27861;&#39118;&#26684;&#30340;&#36716;&#21464;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#24674;&#22797;&#65292;&#20294;&#26080;&#27861;&#20174;&#35789;&#27719;&#19981;&#23545;&#40784;&#21644;&#23884;&#20837;&#30697;&#38453;&#37325;&#26032;&#21021;&#22987;&#21270;&#20013;&#24674;&#22797;&#65292;&#21363;&#20351;&#22312;1500&#19975;&#20196;&#29260;&#30340;&#32487;&#32493;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#12290;%&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#20302;&#25968;&#25454;&#33539;&#22260;&#20869;&#36716;&#31227;&#21040;&#20855;&#26377;&#19981;&#23545;&#40784;&#35789;&#27719;&#30340;&#25968;&#25454;&#38598;&#20013;&#24456;&#38590;&#24674;&#22797;&#12290;&#27492;&#22806;&#65292;&#36716;&#31227;&#35821;&#35328;&#20013;&#30340;&#39640;&#36136;&#37327;&#20998;&#35789;&#22120;&#20063;&#19981;&#20250;&#20351;&#35789;&#27719;&#23545;&#40784;&#21464;&#24471;&#26356;&#23481;&#26131;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#21449;&#35821;&#35328;&#36716;&#31227;&#22240;&#32032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
When we transfer a pretrained language model to a new language, there are many axes of variation that change at once. To disentangle the impact of different factors like syntactic similarity and vocabulary similarity, we propose a set of controlled transfer studies: we systematically transform the language of the GLUE benchmark, altering one axis of crosslingual variation at a time, and then measure the resulting drops in a pretrained model's downstream performance. We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens. %On the other hand, transferring to a dataset with an unaligned vocabulary is extremely hard to recover from in the low-data regime. Moreover, good-quality tokenizers in the transfer language do not make vocabulary alignment easier. Our experiments provide insights into the factors of cross-lingual transfer that rese
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21477;&#23376;&#25552;&#21462;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#29305;&#24449;&#20998;&#24067;&#21644;&#38477;&#20302;&#25688;&#35201;&#21477;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#35813;&#31574;&#30053;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25277;&#21462;&#24335;&#25688;&#35201;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.03203</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30417;&#30563;&#25277;&#21462;&#24335;&#25688;&#35201;&#26041;&#27861;&#30340;&#21477;&#23376;&#25552;&#21462;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A New Sentence Extraction Strategy for Unsupervised Extractive Summarization Methods. (arXiv:2112.03203v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21477;&#23376;&#25552;&#21462;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#29305;&#24449;&#20998;&#24067;&#21644;&#38477;&#20302;&#25688;&#35201;&#21477;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#35813;&#31574;&#30053;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25277;&#21462;&#24335;&#25688;&#35201;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#20877;&#27425;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#21069;&#22823;&#37096;&#20998;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#37117;&#26159;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#33719;&#21462;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#25277;&#21462;&#24335;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#32479;&#19968;&#30340;&#26694;&#26550;&#25551;&#36848;&#20102;&#26080;&#30417;&#30563;&#25277;&#21462;&#26041;&#27861;&#12290;&#20026;&#20102;&#25913;&#21892;&#29305;&#24449;&#20998;&#24067;&#24182;&#38477;&#20302;&#25688;&#35201;&#21477;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21477;&#23376;&#25552;&#21462;&#31574;&#30053;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25277;&#21462;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31574;&#30053;&#30830;&#23454;&#26377;&#25928;&#24182;&#31526;&#21512;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, text summarization methods have attracted much attention again thanks to the researches on neural network models. Most of the current text summarization methods based on neural network models are supervised methods which need large-scale datasets. However, large-scale datasets are difficult to obtain in practical applications. In this paper, we model the task of extractive text summarization methods from the perspective of Information Theory, and then describe the unsupervised extractive methods with a uniform framework. To improve the feature distribution and to decrease the mutual information of summarization sentences, we propose a new sentence extraction strategy which can be applied to existing unsupervised extractive methods. Experiments are carried out on different datasets, and results show that our strategy is indeed effective and in line with expectations.
&lt;/p&gt;</description></item></channel></rss>