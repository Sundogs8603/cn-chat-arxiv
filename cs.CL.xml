<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#24341;&#20837;RWQ-Elo&#35780;&#32423;&#31995;&#32479;&#65292;&#22312;&#26032;&#22522;&#20934;&#8220;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#8221;&#19978;&#23545;24&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20004;&#20154;&#31454;&#20105;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#22312;&#35821;&#20041;&#29702;&#35299;&#35780;&#20272;&#20013;&#30340;&#28508;&#22312;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2403.07872</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#37325;&#26032;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Rethinking Generative Large Language Model Evaluation for Semantic Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07872
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;RWQ-Elo&#35780;&#32423;&#31995;&#32479;&#65292;&#22312;&#26032;&#22522;&#20934;&#8220;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#8221;&#19978;&#23545;24&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20004;&#20154;&#31454;&#20105;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#22312;&#35821;&#20041;&#29702;&#35299;&#35780;&#20272;&#20013;&#30340;&#28508;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#22797;&#26434;&#30340;&#21151;&#33021;&#65292;&#20294;&#22312;&#26377;&#25928;&#35780;&#20272;&#26041;&#38754;&#20173;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#39318;&#20808;&#37325;&#26032;&#23457;&#35270;&#20027;&#27969;&#30340;&#35780;&#20272;&#26041;&#27861; - &#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#30452;&#35266;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;11&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;24&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#31361;&#20986;&#26174;&#31034;&#20102;MCQA&#30340;&#22810;&#20010;&#28508;&#22312;&#32570;&#28857;&#65292;&#20363;&#22914;MCQA&#35780;&#20272;&#19982;&#23454;&#38469;&#24773;&#26223;&#20013;&#24320;&#25918;&#24335;&#22238;&#31572;&#30340;&#29983;&#25104;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;RWQ-Elo&#35780;&#32423;&#31995;&#32479;&#65292;&#21033;&#29992;GPT-4&#12289;GPT-3.5&#12289;Google-Gemini-Pro&#21644;LLaMA-1/-2&#31561;24&#20010;LLMs&#65292;&#37319;&#29992;&#20108;&#20154;&#31454;&#20105;&#26684;&#24335;&#65292;&#20854;&#20013;GPT-4&#20316;&#20026;&#35009;&#21028;&#12290;&#27599;&#20010;LLM&#38543;&#21518;&#25910;&#21040;&#19968;&#20010;Elo&#35780;&#32423;&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#20986;&#20110;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#8221;&#65288;RWQ&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#21253;&#21547;20,772&#20010;&#30495;&#23454;&#29992;&#25143;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07872v1 Announce Type: new  Abstract: Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called ``Real-world questions'' (RWQ), comprising 20,772 authentic user inquirie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861; GORA &#21644;&#19968;&#31181;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861; SORA&#65292;&#29992;&#20197;&#35299;&#20915;&#27169;&#22411;&#32534;&#36753;&#20013;&#30340;&#38544;&#34255;&#31354;&#38388;&#20013;&#30340;&#28063;&#28458;&#25928;&#24212;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07825</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#20013;&#30340;&#36951;&#28431;&#20043;&#22788;&#65306;&#28145;&#20837;&#25506;&#35752;&#27169;&#22411;&#32534;&#36753;&#24102;&#26469;&#30340;&#38544;&#34255;&#25439;&#23475;
&lt;/p&gt;
&lt;p&gt;
The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861; GORA &#21644;&#19968;&#31181;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861; SORA&#65292;&#29992;&#20197;&#35299;&#20915;&#27169;&#22411;&#32534;&#36753;&#20013;&#30340;&#38544;&#34255;&#31354;&#38388;&#20013;&#30340;&#28063;&#28458;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#21331;&#36234;&#30340;&#25928;&#26524;&#24443;&#24213;&#25913;&#21464;&#20102;&#35768;&#22810;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#20197;&#20462;&#25913;&#36807;&#26102;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#20851;&#38190;&#24615;&#24037;&#20316;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#19968;&#20010;&#31216;&#20026;&#8220;&#38544;&#34255;&#31354;&#38388;&#20013;&#30340;&#28063;&#28458;&#25928;&#24212;&#8221;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#31181;&#25928;&#24212;&#34429;&#28982;&#38590;&#20197;&#26816;&#27979;&#65292;&#20294;&#21364;&#20250;&#26174;&#33879;&#38459;&#30861;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#30340;&#25928;&#26524;&#65292;&#24182;&#24694;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#22522;&#20110;&#22270;&#24418;&#29305;&#24322;&#20540;&#20851;&#31995;&#30340;&#35780;&#20272;(GORA)&#65292;&#26469;&#24212;&#23545;&#36825;&#19968;&#31185;&#23398;&#25361;&#25112;&#65292;&#37327;&#21270;&#35780;&#20272;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#21644;&#32534;&#36753;&#30340;&#21518;&#32493;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26088;&#22312;&#20943;&#36731;&#36825;&#31181;&#28063;&#28458;&#25928;&#24212;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#8212;&#8212;&#36873;&#25321;&#24615;&#24322;&#24120;&#20540;&#37325;&#26032;&#32534;&#36753;&#26041;&#27861;(SORA)&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#25581;&#31034;&#20102;&#38544;&#34255;&#31354;&#38388;&#20013;&#30340;&#28063;&#28458;&#25928;&#24212;&#22312;&#25152;&#26377;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20013;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;G
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07825v1 Announce Type: new  Abstract: Large Language Models have revolutionized numerous tasks with their remarkable efficacy.However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space. This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing. Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect. Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods. However, our proposed methods, G
&lt;/p&gt;</description></item><item><title>BTX&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20855;&#22791;&#22810;&#20010;&#19987;&#19994;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31181;&#23376;&#27169;&#22411;&#30340;&#20998;&#25903;&#22521;&#35757;&#25104;&#19987;&#23478;&#65292;&#28982;&#21518;&#22312;Mixture-of-Expert&#23618;&#20013;&#23558;&#23427;&#20204;&#27719;&#38598;&#20026;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;MoE&#24494;&#35843;&#38454;&#27573;&#23398;&#20064;&#22522;&#20110;&#26631;&#35760;&#30340;&#36335;&#30001;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.07816</link><description>&lt;p&gt;
Branch-Train-MiX: &#23558;&#19987;&#23478;LLMs&#28151;&#21512;&#21040;&#28151;&#21512;&#19987;&#23478;LLM&#20013;
&lt;/p&gt;
&lt;p&gt;
Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07816
&lt;/p&gt;
&lt;p&gt;
BTX&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20855;&#22791;&#22810;&#20010;&#19987;&#19994;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31181;&#23376;&#27169;&#22411;&#30340;&#20998;&#25903;&#22521;&#35757;&#25104;&#19987;&#23478;&#65292;&#28982;&#21518;&#22312;Mixture-of-Expert&#23618;&#20013;&#23558;&#23427;&#20204;&#27719;&#38598;&#20026;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;MoE&#24494;&#35843;&#38454;&#27573;&#23398;&#20064;&#22522;&#20110;&#26631;&#35760;&#30340;&#36335;&#30001;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20855;&#22791;&#22810;&#20010;&#19987;&#19994;&#39046;&#22495;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#20363;&#22914;&#32534;&#30721;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#19990;&#30028;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;Branch-Train-MiX&#65288;BTX&#65289;&#65292;&#20174;&#19968;&#20010;&#31181;&#23376;&#27169;&#22411;&#24320;&#22987;&#65292;&#23558;&#20854;&#20998;&#25903;&#35757;&#32451;&#25104;&#19987;&#23478;&#65292;&#20197;&#39640;&#21534;&#21520;&#37327;&#21644;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#23604;&#23596;&#24182;&#34892;&#26041;&#24335;&#12290;&#22312;&#21508;&#20010;&#19987;&#23478;&#24322;&#27493;&#35757;&#32451;&#21518;&#65292;BTX&#23558;&#23427;&#20204;&#20316;&#20026;&#19987;&#23478;&#22312;Mixture-of-Expert&#65288;MoE&#65289;&#23618;&#20013;&#27719;&#38598;&#20854;&#21069;&#39304;&#21442;&#25968;&#65292;&#24182;&#24179;&#22343;&#20854;&#20182;&#21442;&#25968;&#65292;&#38543;&#21518;&#26159;&#19968;&#20010;MoE&#24494;&#35843;&#38454;&#27573;&#26469;&#23398;&#20064;&#22522;&#20110;&#26631;&#35760;&#30340;&#36335;&#30001;&#12290;BTX&#27010;&#25324;&#20102;&#20004;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#21363;Branch-Train-Merge&#26041;&#27861;&#65292;&#23427;&#27809;&#26377;MoE&#24494;&#35843;&#38454;&#27573;&#26469;&#23398;&#20064;&#36335;&#30001;&#65292;&#20197;&#21450;&#31232;&#30095;&#21319;&#32423;&#65292;&#23427;&#30465;&#30053;&#20102;&#24322;&#27493;&#35757;&#32451;&#19987;&#23478;&#30340;&#38454;&#27573;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;BTX&#23454;&#29616;&#20102;&#26368;&#20339;&#31934;&#24230;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07816v1 Announce Type: cross  Abstract: We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.
&lt;/p&gt;</description></item><item><title>pyvene&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#25903;&#25345;&#22312;PyTorch&#27169;&#22411;&#19978;&#36827;&#34892;&#21487;&#23450;&#21046;&#30340;&#24178;&#39044;&#65292;&#25552;&#20379;&#32479;&#19968;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#24182;&#19982;&#20182;&#20154;&#20998;&#20139;&#32463;&#36807;&#24178;&#39044;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.07809</link><description>&lt;p&gt;
pyvene: &#36890;&#36807;&#24178;&#39044;&#26469;&#29702;&#35299;&#21644;&#25913;&#36827;PyTorch&#27169;&#22411;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
pyvene: A Library for Understanding and Improving PyTorch Models via Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07809
&lt;/p&gt;
&lt;p&gt;
pyvene&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#25903;&#25345;&#22312;PyTorch&#27169;&#22411;&#19978;&#36827;&#34892;&#21487;&#23450;&#21046;&#30340;&#24178;&#39044;&#65292;&#25552;&#20379;&#32479;&#19968;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#24182;&#19982;&#20182;&#20154;&#20998;&#20139;&#32463;&#36807;&#24178;&#39044;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#30340;&#24178;&#39044;&#26159;&#20154;&#24037;&#26234;&#33021;&#35768;&#22810;&#39046;&#22495;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#21253;&#25324;&#27169;&#22411;&#32534;&#36753;&#12289;&#25511;&#21046;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;pyvene&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#25903;&#25345;&#22312;&#21508;&#31181;&#19981;&#21516;PyTorch&#27169;&#22359;&#19978;&#36827;&#34892;&#21487;&#23450;&#21046;&#30340;&#24178;&#39044;&#12290;Pyvene&#25903;&#25345;&#22797;&#26434;&#30340;&#24178;&#39044;&#26041;&#26696;&#65292;&#20855;&#26377;&#30452;&#35266;&#30340;&#37197;&#32622;&#26684;&#24335;&#65292;&#24182;&#19988;&#20854;&#24178;&#39044;&#21487;&#20197;&#26159;&#38745;&#24577;&#30340;&#25110;&#21253;&#21547;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;pyvene&#22914;&#20309;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#24178;&#39044;&#24182;&#19982;&#20182;&#20154;&#20849;&#20139;&#32463;&#36807;&#24178;&#39044;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#25277;&#35937;&#21644;&#30693;&#35782;&#23450;&#20301;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#35813;&#24211;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;Python Package Index&#65288;PyPI&#65289;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#24211;&#65292;&#24182;&#22312;https://github.com/stanfordnlp/pyvene &#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#25991;&#26723;&#21644;&#25945;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07809v1 Announce Type: cross  Abstract: Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce $\textbf{pyvene}$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. $\textbf{pyvene}$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how $\textbf{pyvene}$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.07805</link><description>&lt;p&gt;
&#36229;&#36234;&#27515;&#35760;&#30828;&#32972;&#65306;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Memorization: The Challenge of Random Memory Access in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#21442;&#25968;&#20869;&#37096;&#30340;&#30693;&#35782;&#23384;&#20648;&#21644;&#20869;&#23384;&#35775;&#38382;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#26159;&#21542;&#33021;&#22815;&#39034;&#24207;&#25110;&#38543;&#26426;&#22320;&#35775;&#38382;&#20854;&#20869;&#23384;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20840;&#38754;&#32972;&#35829;&#12289;&#36873;&#25321;&#24615;&#32972;&#35829;&#21644;&#22522;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24773;&#26223;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LMs&#33021;&#22815;&#39034;&#24207;&#35775;&#38382;&#20854;&#20869;&#23384;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#35775;&#38382;&#24050;&#35760;&#24518;&#20869;&#23481;&#26102;&#36935;&#21040;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;LMs&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#36825;&#31181;&#24178;&#39044;&#24212;&#29992;&#20110;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36890;&#36807;&#32972;&#35829;&#26469;&#22686;&#24378;&#38543;&#26426;&#35775;&#38382;&#25216;&#26415;&#23545;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07805v1 Announce Type: cross  Abstract: Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in questi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#25191;&#34892;&#22810;&#20010;&#39034;&#24207;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#20248;&#20110;&#20256;&#32479;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.07794</link><description>&lt;p&gt;
&#20351;&#29992;&#39034;&#24207;&#25351;&#20196;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models with Sequential Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07794
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#25191;&#34892;&#22810;&#20010;&#39034;&#24207;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#20248;&#20110;&#20256;&#32479;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21333;&#20010;&#26597;&#35810;&#20013;&#36981;&#24490;&#19968;&#31995;&#21015;&#25351;&#20196;&#26102;&#24448;&#24448;&#20250;&#24573;&#30053;&#25110;&#35823;&#35299;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#38656;&#35201;&#22810;&#20010;&#20013;&#38388;&#27493;&#39588;&#30340;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#22810;&#35821;&#35328;&#65288;&#20808;&#32763;&#35793;&#20877;&#22238;&#31572;&#65289;&#21644;&#22810;&#27169;&#24577;&#65288;&#26631;&#39064;&#21518;&#22238;&#31572;&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#28304;LLMs&#65288;&#22914;LLaMA-2 70B&#21644;Mixtral-8x7B&#65289;&#30340;&#23454;&#35777;&#39564;&#35777;&#20102;&#36825;&#19968;&#28857;&#12290;&#38024;&#23545;&#24403;&#21069;&#25968;&#25454;&#20013;&#39034;&#24207;&#25351;&#20196;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#33258;&#21160;&#22686;&#21152;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#65292;&#20351;LLMs&#20855;&#22791;&#25191;&#34892;&#22810;&#20010;&#39034;&#24207;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#22312;&#25506;&#32034;&#29616;&#26377;&#25968;&#25454;&#38598;&#65288;&#22914;Alpaca&#65289;&#20013;&#25554;&#20837;&#25351;&#20196;&#24182;&#36827;&#34892;&#19968;&#31995;&#21015;&#20013;&#38388;&#20219;&#21153;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20256;&#32479;&#30340;&#25351;&#20196;&#24494;&#35843;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07794v1 Announce Type: new  Abstract: Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks. We empirically verify this with open-source LLMs as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential instructions in present-day data, we propose sequential instruction tuning, a simple yet effective strategy to automatically augment instruction tuning data and equip LLMs with the ability to execute multiple sequential instructions. After exploring interleaving instructions in existing datasets, such as Alpaca, with a wide range of intermediate tasks, we find that sequential instruction-tuned models consistently outperform the conventional instruction-tuned baselines in downstream tas
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07769</link><description>&lt;p&gt;
&#23558;&#31454;&#20105;&#36716;&#21270;&#20026;&#21512;&#20316;&#65306;&#22810;Agent&#31995;&#32479;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#20195;&#32452;&#32455;&#20013;&#30340;&#38761;&#21629;&#24615;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07769
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#65288;SMA&#65289;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35745;&#31639;&#23454;&#20307;&#30340;&#21160;&#24577;&#24433;&#21709;&#65292;&#20854;&#29305;&#28857;&#26159;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20316;&#20026;&#19968;&#31181;&#38761;&#26032;&#20154;&#31867;&#29992;&#25143;&#20132;&#20114;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#21033;&#29992;&#19987;&#38376;&#30340;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#20174;&#25805;&#20316;&#32452;&#32455;&#27969;&#31243;&#21040;&#22522;&#20110;&#24212;&#29992;&#30693;&#35782;&#21644;&#20154;&#30340;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#12290; &#20808;&#21069;&#30340;&#35843;&#26597;&#26174;&#31034;&#65292;&#22312;&#22788;&#29702;&#26032;&#25361;&#25112;&#21644;&#23454;&#29992;&#20219;&#21153;&#65288;&#22914;&#24341;&#21457;&#36923;&#36753;&#25512;&#29702;&#21644;&#38382;&#39064;&#35299;&#20915;&#65289;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#20195;&#29702;&#30340;&#33258;&#20027;&#26041;&#27861;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290; &#36824;&#32771;&#34385;&#21040;&#65292;&#20256;&#32479;&#25216;&#26415;&#65292;&#22914;&#28608;&#21457;&#24605;&#24819;&#38142;&#65292;&#38656;&#35201;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290; &#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24320;&#21457;&#30340;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07769v1 Announce Type: new  Abstract: This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct
&lt;/p&gt;</description></item><item><title>FineMath&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#23567;&#23398;&#25968;&#23398;&#20013;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#21010;&#20998;&#20026;17&#31867;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25163;&#21160;&#27880;&#37322;&#38590;&#24230;&#32423;&#21035;&#65292;&#23454;&#39564;&#35777;&#23454;&#22312;&#25968;&#23398;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.07747</link><description>&lt;p&gt;
FineMath&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07747
&lt;/p&gt;
&lt;p&gt;
FineMath&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#23567;&#23398;&#25968;&#23398;&#20013;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#21010;&#20998;&#20026;17&#31867;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25163;&#21160;&#27880;&#37322;&#38590;&#24230;&#32423;&#21035;&#65292;&#23454;&#39564;&#35777;&#23454;&#22312;&#25968;&#23398;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#31934;&#24515;&#31574;&#21010;&#28085;&#30422;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#21508;&#31181;&#25968;&#23398;&#27010;&#24565;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;FineMath&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;FineMath&#26088;&#22312;&#28085;&#30422;&#23567;&#23398;&#25968;&#23398;&#20013;&#25945;&#25480;&#30340;&#20027;&#35201;&#25968;&#23398;&#27010;&#24565;&#65292;&#36827;&#19968;&#27493;&#21010;&#20998;&#20026;17&#31867;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#28145;&#20837;&#20998;&#26512;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#25152;&#26377;17&#31867;&#25968;&#23398;&#38382;&#39064;&#22343;&#26681;&#25454;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25152;&#38656;&#30340;&#25512;&#29702;&#27493;&#39588;&#25968;&#37327;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#20854;&#38590;&#24230;&#32423;&#21035;&#12290;&#25105;&#20204;&#22312;FineMath&#19978;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#23398;&#26041;&#38754;&#20173;&#26377;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07747v1 Announce Type: cross  Abstract: To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SHROOM&#20849;&#20139;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;&#24187;&#35273;&#65292;&#20197;&#21450;&#21442;&#19982;&#32773;&#20351;&#29992;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.07726</link><description>&lt;p&gt;
SemEval-2024&#20849;&#20139;&#20219;&#21153;6: SHROOM&#65292;&#19968;&#20010;&#20851;&#20110;&#24187;&#35273;&#21450;&#30456;&#20851;&#21487;&#35266;&#23519;&#36807;&#24230;&#29983;&#25104;&#38169;&#35823;&#30340;&#20849;&#20139;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SHROOM&#20849;&#20139;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;&#24187;&#35273;&#65292;&#20197;&#21450;&#21442;&#19982;&#32773;&#20351;&#29992;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SHROOM&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#26816;&#27979;&#24187;&#35273;&#30340;&#20849;&#20139;&#20219;&#21153;&#65306;&#21363;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#30340;&#36755;&#20986;&#27969;&#30021;&#20294;&#19981;&#20934;&#30830;&#12290;&#36825;&#31181;&#36807;&#24230;&#29983;&#25104;&#30340;&#24773;&#20917;&#21487;&#33021;&#21361;&#21450;&#35768;&#22810;NLG&#24212;&#29992;&#65292;&#20854;&#20013;&#27491;&#30830;&#24615;&#24448;&#24448;&#33267;&#20851;&#37325;&#35201;&#12290;&#20849;&#20139;&#20219;&#21153;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4000&#20010;&#30001;5&#20010;&#26631;&#27880;&#32773;&#26631;&#35760;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#28085;&#30422;&#20102;3&#20010;NLP&#20219;&#21153;&#65306;&#26426;&#22120;&#32763;&#35793;&#12289;&#37322;&#20041;&#29983;&#25104;&#21644;&#23450;&#20041;&#24314;&#27169;&#12290; &#20849;&#20139;&#20219;&#21153;&#30001;58&#20010;&#19981;&#21516;&#29992;&#25143;&#32452;&#25104;&#30340;42&#25903;&#22242;&#38431;&#20849;&#21516;&#35299;&#20915;&#65292;&#20854;&#20013;27&#25903;&#36873;&#25321;&#25776;&#20889;&#31995;&#32479;&#25551;&#36848;&#35770;&#25991;&#65307;&#20182;&#20204;&#20849;&#25552;&#20132;&#20102;&#36229;&#36807;300&#20010;&#39044;&#27979;&#38598;&#22312;&#20849;&#20139;&#20219;&#21153;&#30340;&#20004;&#20010;&#36319;&#36394;&#19978;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#34987;&#22788;&#29702;&#30340;&#19968;&#20123;&#20851;&#38190;&#36235;&#21183;--&#35768;&#22810;&#21442;&#19982;&#32773;&#20381;&#36182;&#23569;&#25968;&#27169;&#22411;&#65292;&#24182;&#32463;&#24120;&#20381;&#36182;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#25110;&#38646;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07726v1 Announce Type: new  Abstract: This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate. Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical. The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling.   The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task. We observe a number of key trends in how this approach was tackled -- many participants rely on a handful of model, and often rely either on synthetic data for fine-tuning or zero-shot prompting strategies. While 
&lt;/p&gt;</description></item><item><title>StableToolBench&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#65292;&#36890;&#36807;&#32531;&#23384;&#31995;&#32479;&#12289;API&#27169;&#25311;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31283;&#23450;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07714</link><description>&lt;p&gt;
StableToolBench&#65306;&#38754;&#21521;&#22823;&#35268;&#27169;&#31283;&#23450;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20855;&#23398;&#20064;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07714
&lt;/p&gt;
&lt;p&gt;
StableToolBench&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#65292;&#36890;&#36807;&#32531;&#23384;&#31995;&#32479;&#12289;API&#27169;&#25311;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31283;&#23450;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#20351;&#20154;&#20204;&#25506;&#32034;&#24037;&#20855;&#23398;&#20064;&#65292;&#23558;LLMs&#19982;&#22806;&#37096;&#24037;&#20855;&#25972;&#21512;&#20197;&#35299;&#20915;&#21508;&#31181;&#29616;&#23454;&#25361;&#25112;&#12290;&#35780;&#20272;LLMs&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#38656;&#35201;&#22823;&#35268;&#27169;&#19988;&#31283;&#23450;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;ToolBench&#28436;&#21464;&#32780;&#26469;&#30340;StableToolBench&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#12290;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21253;&#21547;&#32531;&#23384;&#31995;&#32479;&#21644;API&#27169;&#25311;&#22120;&#65292;&#20114;&#34917;&#20943;&#36731;API&#29366;&#24577;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#31283;&#23450;&#30340;&#35780;&#20272;&#31995;&#32479;&#20351;&#29992;GPT-4&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#35774;&#35745;&#21487;&#35299;&#20915;&#30340;&#36890;&#36807;&#29575;&#21644;&#32988;&#29575;&#65292;&#20197;&#28040;&#38500;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07714v1 Announce Type: new  Abstract: Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#23545;&#27604;&#22870;&#21169;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#23545;&#22522;&#20934;&#30340;&#25913;&#21892;&#65292;&#24182;&#33021;&#26681;&#25454;&#20219;&#21153;&#30340;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.07708</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#22870;&#21169;&#25913;&#21892;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07708
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#23545;&#27604;&#22870;&#21169;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#23545;&#22522;&#20934;&#30340;&#25913;&#21892;&#65292;&#24182;&#33021;&#26681;&#25454;&#20219;&#21153;&#30340;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26159;&#29992;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;RLHF&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#26469;&#28304;&#30340;&#22122;&#22768;&#65292;&#20363;&#22914;&#20154;&#31867;&#26631;&#27880;&#38169;&#35823;&#65292;&#20351;&#24471;&#27969;&#31243;&#33030;&#24369;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23545;&#22870;&#21169;&#30340;&#24809;&#32602;&#39033;&#65292;&#21629;&#21517;&#20026;&#8220;&#23545;&#27604;&#22870;&#21169;&#8221;&#65292;&#26469;&#25552;&#39640;&#22870;&#21169;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#31163;&#32447;&#25277;&#26679;&#27493;&#39588;&#65292;&#33719;&#21462;&#29992;&#20316;&#22522;&#20934;&#35745;&#31639;&#30340;&#25552;&#31034;&#21709;&#24212;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#22522;&#20934;&#21709;&#24212;&#35745;&#31639;&#23545;&#27604;&#22870;&#21169;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;Proximal Policy Optimization&#65288;PPO&#65289;&#27493;&#39588;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#22870;&#21169;&#20351;&#24471;LLM&#33021;&#22815;&#24809;&#32602;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#20248;&#20110;&#22522;&#32447;&#30340;&#25913;&#36827;&#65292;&#26681;&#25454;&#20219;&#21153;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#19988;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07708v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and re
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#35780;&#35770;&#26469;&#23454;&#29616;&#35266;&#28857;&#25688;&#35201;&#30340;&#21435;&#20559;&#35265;&#21270;&#65292;&#36991;&#20813;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#39640;&#26114;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.07693</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#29992;&#20110;&#21435;&#20559;&#35265;&#35266;&#28857;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07693
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#35780;&#35770;&#26469;&#23454;&#29616;&#35266;&#28857;&#25688;&#35201;&#30340;&#21435;&#20559;&#35265;&#21270;&#65292;&#36991;&#20813;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#39640;&#26114;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#35266;&#28857;&#25688;&#35201;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;70&#65285;&#30340;&#35780;&#35770;&#26159;&#31215;&#26497;&#30340;&#65292;&#24403;&#21069;&#30340;&#35266;&#28857;&#25688;&#35201;&#26041;&#27861;&#22312;&#32473;&#23450;&#36127;&#38754;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#19981;&#24895;&#29983;&#25104;&#36127;&#38754;&#25688;&#35201;&#65292;&#36896;&#25104;&#24773;&#24863;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#24773;&#24863;&#20559;&#35265;&#65292;&#19968;&#20010;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39069;&#22806;&#30340;&#25968;&#25454;&#65292;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24773;&#24863;&#20998;&#24067;&#65292;&#32780;&#19981;&#36807;&#20998;&#20381;&#36182;&#29305;&#23450;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#38754;&#20020;&#20004;&#20010;&#32570;&#28857;&#65306;1&#65289;&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#38382;&#39064;&#25110;&#27602;&#24615;&#65307;2&#65289;&#26114;&#36149;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#35265;&#35266;&#28857;&#25688;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#27491;&#38754;&#25991;&#26412;&#33719;&#24471;&#20102;&#23567;&#35268;&#27169;&#21512;&#25104;&#30340;&#36127;&#38754;&#35780;&#35770;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#29983;&#25104;&#30340;&#20869;&#23481;&#35757;&#32451;&#19968;&#20010;&#35299;&#32806;&#37325;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07693v1 Announce Type: cross  Abstract: As more than 70$\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#22312;SFT&#36807;&#31243;&#20013;&#36890;&#36807;&#36731;&#24494;&#24809;&#32602;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;</title><link>https://arxiv.org/abs/2403.07691</link><description>&lt;p&gt;
&#20855;&#26377;&#36180;&#29575;&#27604;&#30340;&#26080;&#21442;&#32771;&#21333;&#20307;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reference-free Monolithic Preference Optimization with Odds Ratio
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#22312;SFT&#36807;&#31243;&#20013;&#36890;&#36807;&#36731;&#24494;&#24809;&#32602;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#35821;&#35328;&#27169;&#22411;&#20559;&#22909;&#23545;&#40784;&#31639;&#27861;&#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20173;&#28982;&#23545;&#20110;&#25104;&#21151;&#25910;&#25947;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20559;&#22909;&#23545;&#40784;&#30340;&#29615;&#22659;&#20013;SFT&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24378;&#35843;&#23545;&#20110;&#20559;&#22909;&#23545;&#40784;&#30340;SFT&#26469;&#35828;&#65292;&#23545;&#20110;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#26045;&#21152;&#36731;&#24494;&#24809;&#32602;&#23601;&#36275;&#22815;&#20102;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21019;&#26032;&#30340;&#26080;&#21442;&#32771;&#27169;&#22411;&#30340;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#25163;&#27573;&#35777;&#26126;&#65292;&#36180;&#29575;&#27604;&#26159;&#22312;125M&#33267;7B&#19981;&#21516;&#35268;&#27169;&#19979;&#36827;&#34892;SFT&#26102;&#23545;&#27604;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#39118;&#26684;&#30340;&#26126;&#26234;&#36873;&#25321;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;ORPO&#22312;&#20165;UltraFeedback&#19978;&#23545;Phi-2&#65288;2.7B&#65289;&#12289;Llama-2&#65288;7B&#65289;&#21644;Mistral&#65288;7B&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36229;&#36234;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07691v1 Announce Type: cross  Abstract: While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance o
&lt;/p&gt;</description></item><item><title>SATDAUG&#36825;&#19968;&#24179;&#34913;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#26088;&#22312;&#35299;&#20915;&#33258;&#35748;&#25216;&#26415;&#20538;&#35782;&#21035;&#21644;&#20998;&#31867;&#20013;&#29616;&#26377;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07690</link><description>&lt;p&gt;
SATDAUG -- &#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#33258;&#35748;&#25216;&#26415;&#20538;&#30340;&#24179;&#34913;&#21644;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted Technical Debt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07690
&lt;/p&gt;
&lt;p&gt;
SATDAUG&#36825;&#19968;&#24179;&#34913;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#26088;&#22312;&#35299;&#20915;&#33258;&#35748;&#25216;&#26415;&#20538;&#35782;&#21035;&#21644;&#20998;&#31867;&#20013;&#29616;&#26377;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35748;&#25216;&#26415;&#20538;(SATD)&#26159;&#25351;&#24320;&#21457;&#20154;&#21592;&#26126;&#30830;&#25215;&#35748;&#24182;&#35760;&#24405;&#20195;&#30721;&#24211;&#20013;&#23384;&#22312;&#30340;&#25216;&#26415;&#25463;&#24452;&#12289;&#21464;&#36890;&#26041;&#27861;&#25110;&#20020;&#26102;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#21508;&#31181;&#36719;&#20214;&#24320;&#21457;&#24037;&#20214;&#36827;&#34892;&#25163;&#21160;&#26631;&#35760;&#65292;&#21253;&#25324;&#28304;&#20195;&#30721;&#27880;&#37322;&#12289;&#38382;&#39064;&#36319;&#36394;&#22120;&#21644;&#25289;&#21462;&#35831;&#27714;&#37096;&#20998;&#30340;&#28040;&#24687;&#20197;&#21450;&#25552;&#20132;&#28040;&#24687;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#26088;&#22312;&#29992;&#20110;&#35757;&#32451;&#12289;&#35780;&#20272;&#12289;&#24615;&#33021;&#39564;&#35777;&#21644;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#35782;&#21035;SATD&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#23384;&#22312;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#30740;&#31350;&#20154;&#21592;&#26377;&#20852;&#36259;&#23545;SATD&#30340;&#29305;&#23450;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#26102;&#12290;&#20026;&#35299;&#20915;SATD&#35782;&#21035;&#65288;&#21363;&#19968;&#20010;&#23454;&#20363;&#26159;&#21542;&#20026;SATD&#65289;&#21644;&#20998;&#31867;&#65288;&#21363;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;SATD&#65289;&#30340;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24179;&#34913;&#21644;&#22686;&#24378;&#30340;SATD&#25968;&#25454;&#38598;&#65288;SATDAUG&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07690v1 Announce Type: cross  Abstract: Self-admitted technical debt (SATD) refers to a form of technical debt in which developers explicitly acknowledge and document the existence of technical shortcuts, workarounds, or temporary solutions within the codebase. Over recent years, researchers have manually labeled datasets derived from various software development artifacts: source code comments, messages from the issue tracker and pull request sections, and commit messages. These datasets are designed for training, evaluation, performance validation, and improvement of machine learning and deep learning models to accurately identify SATD instances. However, class imbalance poses a serious challenge across all the existing datasets, particularly when researchers are interested in categorizing the specific types of SATD. In order to address the scarcity of labeled data for SATD \textit{identification} (i.e., whether an instance is SATD or not) and \textit{categorization} (i.e.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#25968;&#25454;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#27880;&#37322;&#25104;&#26412;</title><link>https://arxiv.org/abs/2403.07687</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#27880;&#37322;&#65306;&#21033;&#29992;&#22320;&#29702;&#25968;&#25454;&#30456;&#20284;&#24615;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#27880;&#37322;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#25968;&#25454;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#27880;&#37322;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#22312;&#22320;&#29702;&#21644;&#32463;&#27982;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#24182;&#19981;&#23545;&#27599;&#20010;&#20154;&#37117;&#26377;&#25928;&#12290;&#22823;&#22810;&#25968;&#25968;&#25454;&#26469;&#33258;&#35199;&#26041;&#22269;&#23478;&#65292;&#23548;&#33268;&#23545;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#22269;&#23478;&#30340;&#32467;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#20174;&#36825;&#20123;&#22269;&#23478;&#25910;&#38598;&#26356;&#22810;&#25968;&#25454;&#65292;&#20294;&#27880;&#37322;&#25104;&#26412;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#38656;&#35201;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#28041;&#21450;&#25214;&#21040;&#20855;&#26377;&#26368;&#22823;&#35270;&#35273;&#24046;&#24322;&#30340;&#20027;&#39064;&#65288;&#29289;&#20307;&#21644;&#21160;&#20316;&#65289;&#22270;&#20687;&#30340;&#22269;&#23478;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23545;&#20110;&#36825;&#20123;&#20027;&#39064;&#22312;&#35270;&#35273;&#19978;&#26356;&#30456;&#20284;&#30340;&#22269;&#23478;&#65292;&#24182;&#34920;&#26126;&#21033;&#29992;&#36825;&#20123;&#22269;&#23478;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#27880;&#37322;&#25104;&#26412;&#26041;&#38754;&#33410;&#30465;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07687v1 Announce Type: cross  Abstract: Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that usin
&lt;/p&gt;</description></item><item><title>MoralBERT &#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#25991;&#26412;&#20013;&#36947;&#24503;&#24494;&#22937;&#20043;&#22788;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;Twitter&#12289;Reddit&#21644;Facebook&#30340;&#25968;&#25454;&#65292;&#25193;&#22823;&#20102;&#27169;&#22411;&#29702;&#35299;&#36947;&#24503;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07678</link><description>&lt;p&gt;
MoralBERT&#65306;&#26816;&#27979;&#31038;&#20250;&#35805;&#35821;&#20013;&#30340;&#36947;&#24503;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
MoralBERT: Detecting Moral Values in Social Discourse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07678
&lt;/p&gt;
&lt;p&gt;
MoralBERT &#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#25991;&#26412;&#20013;&#36947;&#24503;&#24494;&#22937;&#20043;&#22788;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;Twitter&#12289;Reddit&#21644;Facebook&#30340;&#25968;&#25454;&#65292;&#25193;&#22823;&#20102;&#27169;&#22411;&#29702;&#35299;&#36947;&#24503;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#24503;&#22312;&#25105;&#20204;&#24863;&#30693;&#20449;&#24687;&#12289;&#24433;&#21709;&#20915;&#31574;&#21644;&#21028;&#26029;&#36807;&#31243;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#21253;&#25324;&#30123;&#33495;&#25509;&#31181;&#12289;&#22549;&#32974;&#12289;&#31181;&#26063;&#20027;&#20041;&#21644;&#24615;&#21462;&#21521;&#22312;&#20869;&#30340;&#26377;&#20105;&#35758;&#35805;&#39064;&#24448;&#24448;&#24341;&#21457;&#30340;&#24847;&#35265;&#21644;&#24577;&#24230;&#24182;&#38750;&#20165;&#22522;&#20110;&#35777;&#25454;&#65292;&#32780;&#26356;&#22810;&#21453;&#26144;&#20102;&#36947;&#24503;&#19990;&#30028;&#35266;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#36947;&#24503;&#20215;&#20540;&#21487;&#20197;&#20174;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#20869;&#23481;&#20013;&#24471;&#21040;&#21028;&#26029;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#25429;&#25417;&#25991;&#26412;&#20013;&#36947;&#24503;&#24494;&#22937;&#20043;&#22788;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;MoralBERT&#12290;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#26469;&#28304;&#65288;Twitter&#12289;Reddit&#21644;Facebook&#65289;&#30340;&#24102;&#26377;&#27880;&#37322;&#30340;&#36947;&#24503;&#25968;&#25454;&#65292;&#28085;&#30422;&#21508;&#31181;&#31038;&#20250;&#30456;&#20851;&#20027;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#22823;&#20102;&#35821;&#35328;&#22810;&#26679;&#24615;&#65292;&#21487;&#33021;&#22686;&#24378;&#27169;&#22411;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#29702;&#35299;&#36947;&#24503;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#24182;&#23558;&#20854;&#19982;&#26631;&#20934;&#30340;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07678v1 Announce Type: new  Abstract: Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements. Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews. Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content. Here, we design a range of language representation models fine-tuned to capture exactly the moral nuances in text, called MoralBERT. We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics. This approach broadens linguistic diversity and potentially enhances the models' ability to comprehend morality in various contexts. We also explore a domain adaptation technique and compare it to the standard fine-tu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#19987;&#23478;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38024;&#23545;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#28608;&#27963;&#19981;&#21516;&#25968;&#37327;&#30340;&#19987;&#23478;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;Top-K&#36335;&#30001;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#21160;&#24577;&#36335;&#30001;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.07652</link><description>&lt;p&gt;
&#36739;&#22256;&#38590;&#30340;&#20219;&#21153;&#38656;&#35201;&#26356;&#22810;&#19987;&#23478;&#65306;MoE&#27169;&#22411;&#20013;&#30340;&#21160;&#24577;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Harder Tasks Need More Experts: Dynamic Routing in MoE Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07652
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#19987;&#23478;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38024;&#23545;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#28608;&#27963;&#19981;&#21516;&#25968;&#37327;&#30340;&#19987;&#23478;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;Top-K&#36335;&#30001;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#21160;&#24577;&#36335;&#30001;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#19987;&#23478;&#36873;&#25321;&#26694;&#26550;&#65292;&#29992;&#20110;Mixture of Experts&#65288;MoE&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#26681;&#25454;&#36755;&#20837;&#38590;&#24230;&#35843;&#25972;&#28608;&#27963;&#30340;&#19987;&#23478;&#25968;&#37327;&#65292;&#22686;&#24378;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;&#20381;&#36182;&#20110;&#22266;&#23450;Top-K&#36335;&#30001;&#30340;&#20256;&#32479;MoE&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#23545;&#27599;&#20010;&#36755;&#20837;&#30340;&#19987;&#23478;&#36873;&#25321;&#30340;&#32622;&#20449;&#27700;&#24179;&#21160;&#24577;&#36873;&#25321;&#19987;&#23478;&#12290;&#36825;&#20801;&#35768;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#28608;&#27963;&#26356;&#22810;&#30340;&#19987;&#23478;&#65292;&#23545;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#28608;&#27963;&#26356;&#23569;&#30340;&#19987;&#23478;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#21160;&#24577;&#36335;&#30001;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#19982;&#24120;&#35268;Top-2&#36335;&#30001;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#36827;0.7%&#30340;&#25928;&#26524;&#65292;&#19988;&#28608;&#27963;&#21442;&#25968;&#23569;&#20110;90%&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07652v1 Announce Type: cross  Abstract: In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#22686;&#24378;&#22686;&#24378;&#20010;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#28860;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20219;&#21153;&#20013;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2403.07581</link><description>&lt;p&gt;
LLMvsSmall &#27169;&#22411;&#65311;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#22686;&#24378;&#22686;&#24378;&#20010;&#24615;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#22686;&#24378;&#22686;&#24378;&#20010;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#28860;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20219;&#21153;&#20013;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26684;&#26816;&#27979;&#26088;&#22312;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#38544;&#34255;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#20174;&#33258;&#25105;&#25253;&#21578;&#38382;&#21367;&#20013;&#25910;&#38598;&#30340;&#22320;&#38754;&#30495;&#23454;&#20154;&#26684;&#29305;&#36136;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22312;&#26377;&#38480;&#20154;&#26684;&#26631;&#31614;&#30340;&#30417;&#30563;&#19979;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#30452;&#25509;&#23398;&#20064;&#24086;&#23376;&#29305;&#24449;&#12290;&#36825;&#23548;&#33268;&#24086;&#23376;&#29305;&#24449;&#36136;&#37327;&#36739;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23558;&#20154;&#26684;&#29305;&#36136;&#35270;&#20026;&#29420;&#28909;&#20998;&#31867;&#26631;&#31614;&#65292;&#24573;&#35270;&#20854;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25991;&#26412;&#22686;&#24378;&#22686;&#24378;&#20010;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25552;&#28860;LLM&#30340;&#30693;&#35782;&#20197;&#22686;&#24378;&#29992;&#20110;&#20010;&#24615;&#26816;&#27979;&#30340;&#23567;&#27169;&#22411;&#65292;&#21363;&#20351;LLM&#22312;&#35813;&#20219;&#21153;&#20013;&#22833;&#36133;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;LLM&#33021;&#22815;&#20174;&#35821;&#20041;&#12289;&#24773;&#24863;&#26041;&#38754;&#29983;&#25104;&#24086;&#23376;&#20998;&#26512;&#65288;&#22686;&#24378;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07581v1 Announce Type: new  Abstract: Personality detection aims to detect one's personality traits underlying in social media posts. One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires. Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance. In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them. In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even when the LLM fails in this task. Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#32858;&#38598;&#24615;&#35821;&#35328;isiXhosa&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#22788;&#29702;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;T2X&#21644;SSPG&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#35780;&#20272;&#26694;&#26550;&#65292;&#24110;&#21161;&#29992;&#25143;&#26356;&#20934;&#30830;&#25551;&#36848;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.07567</link><description>&lt;p&gt;
Triples-to-isiXhosa (T2X): &#24212;&#23545;&#20302;&#36164;&#28304;&#32858;&#38598;&#24615;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#32858;&#38598;&#24615;&#35821;&#35328;isiXhosa&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#22788;&#29702;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;T2X&#21644;SSPG&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#35780;&#20272;&#26694;&#26550;&#65292;&#24110;&#21161;&#29992;&#25143;&#26356;&#20934;&#30830;&#25551;&#36848;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25968;&#25454;&#21040;&#25991;&#26412;&#25968;&#25454;&#38598;&#26159;&#33521;&#25991;&#30340;&#65292;&#25152;&#20197;&#24314;&#27169;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#22256;&#38590;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;isiXhosa&#35821;&#35328;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#38382;&#39064;&#65292;&#35813;&#35821;&#35328;&#23646;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#19988;&#20026;&#32858;&#38598;&#24615;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;WebNLG&#23376;&#38598;&#30340;&#26032;&#25968;&#25454;&#38598;Triples-to-isiXhosa (T2X)&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#29615;&#22659;&#65292;&#23558;&#24314;&#27169;&#38656;&#27714;&#36716;&#21521;&#23376;&#35789;&#39537;&#21160;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#20026;T2X&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#34913;&#37327;&#29983;&#25104;&#30340;&#25991;&#26412;&#25551;&#36848;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20351;&#24471;T2X&#30340;&#26410;&#26469;&#29992;&#25143;&#33021;&#22815;&#36229;&#36234;&#34920;&#38754;&#32423;&#21035;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#22312;&#24314;&#27169;&#26041;&#38754;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31867;&#26041;&#27861; - &#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#19987;&#29992;&#25968;&#25454;&#21040;&#25991;&#26412;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19987;&#38376;&#38754;&#21521;&#32858;&#38598;&#24615;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#26550;&#26500;&#65292;&#21363;Subword Segmental Pointer Generator (SSPG)&#12290;&#23427;&#20849;&#21516;&#23398;&#20064;&#20998;&#21106;&#21333;&#35789;&#21644;&#22797;&#21046;&#23454;&#20307;&#65292;&#24182;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07567v1 Announce Type: new  Abstract: Most data-to-text datasets are for English, so the difficulties of modelling data-to-text for low-resource languages are largely unexplored. In this paper we tackle data-to-text for isiXhosa, which is low-resource and agglutinative. We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of WebNLG, which presents a new linguistic context that shifts modelling demands to subword-driven techniques. We also develop an evaluation framework for T2X that measures how accurately generated text describes the data. This enables future users of T2X to go beyond surface-level metrics in evaluation. On the modelling side we explore two classes of methods - dedicated data-to-text models trained from scratch and pretrained language models (PLMs). We propose a new dedicated architecture aimed at agglutinative data-to-text, the Subword Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy entities, and outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;SIFiD&#65288;&#24102;&#26377;&#36807;&#28388;&#25991;&#26723;&#30340;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25110;&#27979;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.07557</link><description>&lt;p&gt;
SIFiD&#65306;&#20351;&#29992;LLM&#37325;&#26032;&#35780;&#20272;&#25688;&#35201;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SIFiD: Reassess Summary Factual Inconsistency Detection with LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;SIFiD&#65288;&#24102;&#26377;&#36807;&#28388;&#25991;&#26723;&#30340;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25110;&#27979;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#25688;&#35201;&#19982;&#21407;&#22987;&#25991;&#26723;&#20043;&#38388;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#26816;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#21033;&#29992;&#23427;&#20204;&#20808;&#36827;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;&#23581;&#35797;&#34920;&#26126;&#65292;&#30001;&#20110;LLMs&#26377;&#38480;&#30340;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#21644;&#32570;&#20047;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#35770;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#21450;&#20256;&#32479;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#27604;&#36739;LLMs&#30340;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#34920;&#29616;&#65292;&#20197;&#25512;&#21160;&#22522;&#20110;LLM&#30340;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SIFiD&#65288;&#24102;&#26377;&#36807;&#28388;&#25991;&#26723;&#30340;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25110;&#27979;&#37327;&#25688;&#35201;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07557v1 Announce Type: new  Abstract: Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65288;TACS&#65289;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#30495;&#30456;&#26816;&#27979;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#26469;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#20135;&#29983;&#24187;&#35273;</title><link>https://arxiv.org/abs/2403.07556</link><description>&lt;p&gt;
&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65306;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07556
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65288;TACS&#65289;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#30495;&#30456;&#26816;&#27979;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#26469;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#29992;&#25143;&#25110;&#30693;&#35782;&#35770;&#35777;&#24037;&#20855;&#25552;&#20379;&#30340;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#65292;&#20174;&#32780;&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#20943;&#36731;LLMs&#34987;&#19981;&#30495;&#23454;&#20449;&#24687;&#35823;&#23548;&#24182;&#21033;&#29992;&#30693;&#35782;&#35770;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65288;TACS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#36755;&#20837;&#20013;&#23631;&#34109;&#19981;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#12290;TACS&#39318;&#20808;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#30495;&#30456;&#26816;&#27979;&#65292;&#21033;&#29992;LLM&#20869;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#26681;&#25454;&#27599;&#20010;&#20301;&#32622;&#30340;&#30495;&#23454;&#24615;&#26500;&#24314;&#30456;&#24212;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#65292;&#36873;&#25321;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#24182;&#20002;&#24323;&#19981;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25200;&#21160;&#36866;&#24212;&#29575;&#65292;&#20197;&#36827;&#19968;&#27493;&#30740;&#31350;LLMs&#25509;&#21463;&#30495;&#23454;&#20449;&#24687;&#21644;&#25269;&#21046;&#19981;&#30495;&#23454;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07556v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations. To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MAMMOTH&#24037;&#20855;&#21253;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22359;&#21270;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#35774;&#35745;&#23454;&#29616;&#20102;&#22312;&#35745;&#31639;&#38598;&#32676;&#19978;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.07544</link><description>&lt;p&gt;
MAMMOTH: &#36203;&#23572;&#36763;&#22522;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22359;&#21270;&#24320;&#25918;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MAMMOTH&#24037;&#20855;&#21253;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22359;&#21270;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#35774;&#35745;&#23454;&#29616;&#20102;&#22312;&#35745;&#31639;&#38598;&#32676;&#19978;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21333;&#20307;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27491;&#25509;&#36817;&#20854;&#22312;&#23610;&#23544;&#21644;&#20449;&#24687;&#22788;&#29702;&#26041;&#38754;&#30340;&#26497;&#38480;&#12290;&#36235;&#21183;&#26159;&#27169;&#22359;&#21270;&#65292;&#36825;&#26159;&#26397;&#30528;&#35774;&#35745;&#20855;&#26377;&#19987;&#38376;&#21151;&#33021;&#30340;&#36739;&#23567;&#23376;&#32593;&#32476;&#21644;&#32452;&#20214;&#30340;&#26041;&#21521;&#36808;&#20986;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MAMMOTH&#24037;&#20855;&#21253;&#65306;&#19968;&#20010;&#26088;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22359;&#21270;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26368;&#21021;&#28304;&#33258;OpenNMT-py&#65292;&#28982;&#21518;&#32463;&#36807;&#35843;&#25972;&#20197;&#30830;&#20445;&#22312;&#35745;&#31639;&#38598;&#32676;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;A100&#21644;V100 NVIDIA GPU&#38598;&#32676;&#19978;&#30340;&#25928;&#29575;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#29702;&#24565;&#21644;&#26410;&#26469;&#30340;&#35745;&#21010;&#12290;&#35813;&#24037;&#20855;&#21253;&#24050;&#22312;&#32593;&#19978;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07544v1 Announce Type: new  Abstract: NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled. The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality. In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular machine translation systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters. We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information. The toolkit is publicly available online.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07440</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#65306;&#19968;&#31181;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LPLMs&#65289;&#30340;&#24494;&#35843;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26377;&#25928;&#25511;&#21046;LPLMs&#30340;&#36755;&#20986;&#34892;&#20026;&#12290;&#26412;&#25991;&#21463;&#22823;&#33041;&#21151;&#33021;&#21463;&#20854;&#20960;&#20309;&#32467;&#26500;&#22609;&#36896;&#30340;&#21551;&#21457;&#65292;&#23558;&#36825;&#19968;&#24605;&#24819;&#34701;&#20837;LoRA&#25216;&#26415;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;COM2&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#20013;&#25277;&#26679;&#22810;&#36339;&#36923;&#36753;&#26597;&#35810;&#24182;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07398</link><description>&lt;p&gt;
&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#30340;&#22797;&#26434;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07398
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;COM2&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#20013;&#25277;&#26679;&#22810;&#36339;&#36923;&#36753;&#26597;&#35810;&#24182;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#24120;&#35782;&#25512;&#29702;&#38656;&#35201;&#20855;&#26377;&#25512;&#29702;&#20107;&#20214;&#20043;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25512;&#26029;&#22312;&#36825;&#31181;&#20851;&#31995;&#20043;&#19979;&#30340;&#38544;&#21547;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#31232;&#32570;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#23398;&#20250;&#20026;&#28041;&#21450;&#22797;&#26434;&#20107;&#20214;&#30456;&#20114;&#20316;&#29992;&#30340;&#32972;&#26223;&#21644;&#38382;&#39064;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COM2&#65288;COMplex COMmonsense&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20174;&#29616;&#26377;&#24120;&#35782;&#30693;&#35782;&#22270;&#65288;CSKG&#65289;&#20013;&#25277;&#26679;&#22810;&#36339;&#36923;&#36753;&#26597;&#35810;&#65288;&#20363;&#22914;&#65292;&#20107;&#20214;A&#21644;B&#30340;&#32852;&#21512;&#25928;&#26524;&#25110;&#22240;&#26524;&#20851;&#31995;&#65292;&#25110;&#20107;&#20214;C&#30340;&#25928;&#26524;&#30340;&#25928;&#26524;&#65289;&#65292;&#24182;&#21033;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#35268;&#21017;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20854;&#29992;&#22810;&#36873;&#21644;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#30340;&#24418;&#24335;&#34920;&#36798;&#20986;&#26469;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;COM2&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#38646;-shot&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#39046;&#22495;&#20869;&#36824;&#26159;&#39046;&#22495;&#22806;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07398v1 Announce Type: cross  Abstract: Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for
&lt;/p&gt;</description></item><item><title>S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07384</link><description>&lt;p&gt;
SmallToLarge (S2L): &#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#25552;&#20379;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07384
&lt;/p&gt;
&lt;p&gt;
S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#36873;&#25321;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#38454;&#27573;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20013;&#25913;&#21892;&#25968;&#25454;&#25928;&#29575;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#24494;&#35843;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;S2L&#65288;SmallToLarge&#65289;&#65292;&#23427;&#21033;&#29992;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#26469;&#25351;&#23548;&#26356;&#22823;&#27169;&#22411;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;S2L&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;SFT&#25968;&#25454;&#25928;&#29575;&#65292;&#23558;&#35757;&#32451;&#25968;&#25454;&#32553;&#20943;&#21040;&#21407;&#22987;MathInstruct&#25968;&#25454;&#38598;&#65288;Yue&#31561;&#20154;&#65292;2023&#65289;&#30340;&#20165;11%&#65292;&#20197;&#36798;&#21040;&#20840;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;6&#20010;&#39046;&#22495;&#20869;&#22806;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#24179;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#36873;&#25321;&#31639;&#27861;4.7%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#36873;&#25321;50K&#25968;&#25454;&#36827;&#34892;SFT&#65292;S2L&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07384v1 Announce Type: cross  Abstract: Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achi
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#21644;LLMs&#20013;&#20248;&#21270;&#36712;&#36857;&#30340;&#22797;&#26434;&#24615;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#21253;&#25324;&#26041;&#21521;&#25506;&#32034;&#21644;&#26041;&#21521;&#27491;&#21017;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.07379</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21644;LLMs&#20013;&#20248;&#21270;&#36712;&#36857;&#30340;&#29305;&#24449;&#65306;&#38271;&#24230;&#12289;&#25296;&#28857;&#21644;&#27515;&#32993;&#21516;
&lt;/p&gt;
&lt;p&gt;
Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07379
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#21644;LLMs&#20013;&#20248;&#21270;&#36712;&#36857;&#30340;&#22797;&#26434;&#24615;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#21253;&#25324;&#26041;&#21521;&#25506;&#32034;&#21644;&#26041;&#21521;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#20998;&#26512;&#20854;&#20248;&#21270;&#36712;&#36857;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#21442;&#25968;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#20851;&#20110;&#20248;&#21270;&#36712;&#36857;&#22797;&#26434;&#24615;&#30340;&#33258;&#28982;&#27010;&#24565;&#65292;&#26082;&#23450;&#24615;&#21448;&#23450;&#37327;&#22320;&#25581;&#31034;&#20102;&#21508;&#31181;&#20248;&#21270;&#36873;&#25321;&#65288;&#22914;&#21160;&#37327;&#12289;&#26435;&#37325;&#34928;&#20943;&#21644;&#25209;&#22823;&#23567;&#65289;&#20043;&#38388;&#25152;&#28041;&#21450;&#30340;&#20869;&#22312;&#24494;&#22937;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#26469;&#25552;&#20379;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26412;&#36136;&#30340;&#20851;&#38190;&#29305;&#24449;&#65306;&#20309;&#26102;&#39034;&#21033;&#36827;&#34892;&#65292;&#20309;&#26102;&#38519;&#20837;&#27515;&#32993;&#21516;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#36712;&#36857;&#35270;&#35282;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21160;&#37327;&#21644;&#26435;&#37325;&#34928;&#20943;&#20043;&#38388;&#20419;&#36827;&#26041;&#21521;&#25506;&#32034;&#30340;&#20132;&#32455;&#34892;&#20026;&#65292;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#34892;&#20026;&#30340;&#26041;&#21521;&#27491;&#21017;&#21270;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#35774;&#32622;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#20855;&#26377;&#26368;&#22810;120&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07379v1 Announce Type: cross  Abstract: We propose a fresh take on understanding the mechanisms of neural networks by analyzing the rich structure of parameters contained within their optimization trajectories. Towards this end, we introduce some natural notions of the complexity of optimization trajectories, both qualitative and quantitative, which reveal the inherent nuance and interplay involved between various optimization choices, such as momentum, weight decay, and batch size. We use them to provide key hallmarks about the nature of optimization in deep neural networks: when it goes right, and when it finds itself in a dead end. Further, thanks to our trajectory perspective, we uncover an intertwined behaviour of momentum and weight decay that promotes directional exploration, as well as a directional regularization behaviour of some others. We perform experiments over large-scale vision and language settings, including large language models (LLMs) with up to 12 billio
&lt;/p&gt;</description></item><item><title>SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07378</link><description>&lt;p&gt;
SVD-LLM: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#25130;&#26029;&#24863;&#30693;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07378
&lt;/p&gt;
&lt;p&gt;
SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21463;&#21040;&#20854;&#24222;&#22823;&#23610;&#23544;&#30340;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;LLM&#21387;&#32553;&#26041;&#27861;&#20197;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#12290;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20026;LLM&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#25130;&#26029;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#21387;&#32553;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;SVD&#25130;&#26029;&#21518;&#21097;&#20313;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#26032;&#32570;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD-LLM&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;SVD-LLM&#37319;&#29992;&#20102;&#19968;&#31181;&#25130;&#26029;&#24863;&#30693;&#30340;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;SVD-LLM&#37319;&#29992;&#19968;&#31181;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;SVD&#25130;&#26029;&#24341;&#36215;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24635;&#20849;11&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;m&#19978;&#35780;&#20272;&#20102;SVD-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NavCoT&#30340;&#26032;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#39046;&#22495;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07376</link><description>&lt;p&gt;
NavCoT: &#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#25552;&#21319;&#22522;&#20110;LLM&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NavCoT&#30340;&#26032;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#39046;&#22495;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;(VLN)&#20316;&#20026;&#20855;&#26377;&#37325;&#35201;&#30740;&#31350;&#20215;&#20540;&#30340;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#36523;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#31034;&#31359;&#36234;&#22797;&#26434;&#30340;3D&#29615;&#22659;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;VLN&#20013;&#25552;&#39640;&#23548;&#33322;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#22312;&#31163;&#32447;&#26041;&#24335;&#19979;&#30340;&#20351;&#29992;&#36890;&#24120;&#22312;VLN&#20219;&#21153;&#21644;LLM&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#36973;&#21463;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#23548;&#33322;&#24605;&#32500;&#38142;(NavCoT)&#30340;&#26032;&#22411;&#31574;&#30053;&#65292;&#25105;&#20204;&#36890;&#36807;&#23436;&#25104;&#39046;&#22495;&#20869;&#39640;&#25928;&#21442;&#25968;&#35757;&#32451;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#39046;&#22495;&#24046;&#36317;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;LLM&#34987;&#25552;&#31034;&#36890;&#36807;&#20316;&#20026;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#23548;&#33322;&#24605;&#32500;&#38142;&#65306;1)&#26681;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07376v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the
&lt;/p&gt;</description></item><item><title>KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.07350</link><description>&lt;p&gt;
KEBench: &#29992;&#20110;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07350
&lt;/p&gt;
&lt;p&gt;
KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#30446;&#21069;&#65292;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#30693;&#35782;&#32534;&#36753;&#30740;&#31350;&#24456;&#23569;&#12290;&#32534;&#36753;LVLMs&#38754;&#20020;&#30528;&#26377;&#25928;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65288;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30830;&#20445;&#20462;&#25913;&#36830;&#36143;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#12290;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#38752;&#24615;&#12289;&#23616;&#37096;&#24615;&#21644;&#19968;&#33324;&#24615;&#65289;&#29992;&#20110;&#34913;&#37327;LVLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#22312;&#35780;&#20272;&#20013;&#20351;&#29992;&#30340;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#19981;&#36275;&#65292;&#24182;&#19988;&#26080;&#27861;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#22320;&#21033;&#29992;&#19982;&#30456;&#20851;&#20869;&#23481;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;$\textbf{KEBench}$&#65292;&#24182;&#25193;&#23637;&#20102;&#26032;&#24230;&#37327;&#26631;&#20934;(&#21487;&#31227;&#26893;&#24615;)&#20197;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#20511;&#21161;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65292;&#25105;&#20204;&#30340;&#22270;&#20687;&#25968;&#25454;&#21576;&#29616;&#20986;&#26126;&#30830;&#30340;&#32473;&#23454;&#20307;&#26041;&#21521;&#24615;&#12290;&#36825;&#31181;&#26041;&#21521;&#24615;&#21487;&#20197;&#36827;&#19968;&#27493;&#29992;&#20110;&#25552;&#21462;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#30693;&#35782;&#21644;&#36827;&#34892;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 Announce Type: cross  Abstract: Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#37325;&#26032;&#24605;&#32771;ASTE&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#36234;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07342</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;ASTE:&#19968;&#31181;&#26497;&#31616;&#30340;&#26631;&#35760;&#26041;&#26696;&#19982;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07342
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#37325;&#26032;&#24605;&#32771;ASTE&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#36234;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Aspect Sentiment Triplet Extraction (ASTE) &#26159;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#30340;&#19968;&#20010;&#26032;&#20852;&#23376;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#30340;&#24773;&#24863;&#19977;&#20803;&#32452;&#12290;&#29616;&#26377;&#30340;ASTE&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#39069;&#22806;&#30340;&#32467;&#26500;&#25110;&#22806;&#37096;&#25968;&#25454;&#26469;&#22797;&#26434;&#21270;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20986;&#21487;&#27604;&#29978;&#33267;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26102;&#20195;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#20110;GPT 3.5&#21644;GPT 4&#30340;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#36824;&#20026;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#33539;&#24335;&#19979;&#25512;&#36827;ASTE&#25216;&#26415;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07342v1 Announce Type: cross  Abstract: Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of fine-grained sentiment analysis, aiming to extract structured sentiment triplets from unstructured textual data. Existing approaches to ASTE often complicate the task with additional structures or external data. In this research, we propose a novel tagging scheme and employ a contrastive learning approach to mitigate these challenges. The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead. Notably, even in the era of Large Language Models (LLMs), our method exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning scenarios. This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of large language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777;&#22312;&#31227;&#38500;&#20302;&#20301;&#23485;&#38480;&#21046;&#26102;&#65292;&#23545;&#20110;&#21508;&#31181;Transformer-based&#27169;&#22411;&#65292;&#25972;&#25968;&#26159;&#21542;&#36275;&#20197;&#28385;&#36275;&#25152;&#26377;GEMM&#38656;&#27714;&#65288;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#28014;&#28857;&#25968;&#30456;&#23218;&#32654;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2403.07339</link><description>&lt;p&gt;
IM-Unpack: &#20351;&#29992;&#20219;&#24847;&#20302;&#31934;&#24230;&#25972;&#25968;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777;&#22312;&#31227;&#38500;&#20302;&#20301;&#23485;&#38480;&#21046;&#26102;&#65292;&#23545;&#20110;&#21508;&#31181;Transformer-based&#27169;&#22411;&#65292;&#25972;&#25968;&#26159;&#21542;&#36275;&#20197;&#28385;&#36275;&#25152;&#26377;GEMM&#38656;&#27714;&#65288;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#28014;&#28857;&#25968;&#30456;&#23218;&#32654;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GEneral Matrix Multiply (GEMM)&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25805;&#20316;&#65292;&#23545;&#24212;&#20110;&#35745;&#31639;&#21344;&#27604;&#26368;&#22823;&#30340;&#37096;&#20998;&#12290;&#22240;&#27492;&#65292;&#25552;&#39640;&#20854;&#25928;&#29575;&#26159;&#19968;&#20010;&#27491;&#22312;&#36827;&#34892;&#30740;&#31350;&#30340;&#28909;&#38376;&#20027;&#39064;&#12290;&#19968;&#31181;&#27969;&#34892;&#30340;&#31574;&#30053;&#26159;&#20351;&#29992;&#20302;&#20301;&#23485;&#25972;&#25968;&#26469;&#36817;&#20284;&#30697;&#38453;&#20013;&#30340;&#21407;&#22987;&#26465;&#30446;&#12290;&#36825;&#26679;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#24120;&#24120;&#38656;&#35201;&#22797;&#26434;&#30340;&#25216;&#26415;&#26469;&#25511;&#21046;&#20135;&#29983;&#30340;&#33293;&#20837;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#39564;&#35777;&#24403;&#31227;&#38500;&#20302;&#20301;&#23485;&#38480;&#21046;&#26102;&#65292;&#23545;&#20110;&#21508;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#25972;&#25968;&#26159;&#21542;&#36275;&#22815;&#28385;&#36275;&#25152;&#26377;GEMMs&#30340;&#38656;&#27714; - &#26080;&#35770;&#26159;&#35757;&#32451;&#38454;&#27573;&#36824;&#26159;&#25512;&#26029;&#38454;&#27573;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#28014;&#28857;&#25968;&#23545;&#24212;&#39033;&#36798;&#21040;&#19968;&#33268;&#12290;&#26080;&#38656;&#22797;&#26434;&#25216;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#36935;&#21040;&#30340;&#22823;&#22810;&#25968;&#30697;&#38453;&#26465;&#30446;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#29992;&#20302;&#20301;&#23485;&#25972;&#25968;&#34920;&#31034;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#26465;&#30446;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07339v1 Announce Type: cross  Abstract: GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need -- for {\em both} training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\em low} bit-width integers, the existence of a few heavy hitter entries m
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;GPT&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25968;&#25454;&#38598;GRiD&#65292;&#29992;&#20110;&#35780;&#20272;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#21644;&#21306;&#20998;&#20154;&#31867;&#19982;ChatGPT&#29983;&#25104;&#21709;&#24212;&#12290;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#20998;&#26512;&#21644;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07321</link><description>&lt;p&gt;
GPT&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#65306;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#24352;&#37327;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07321
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;GPT&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25968;&#25454;&#38598;GRiD&#65292;&#29992;&#20110;&#35780;&#20272;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#21644;&#21306;&#20998;&#20154;&#31867;&#19982;ChatGPT&#29983;&#25104;&#21709;&#24212;&#12290;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#20998;&#26512;&#21644;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35832;&#22914;ChatGPT&#31561;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#22312;&#24212;&#29992;&#21644;&#26381;&#21153;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#26816;&#27979;&#20854;&#36755;&#20986;&#30340;&#40065;&#26834;&#20934;&#30830;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GPT Reddit&#25968;&#25454;&#38598;&#65288;GRiD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#39062;&#30340;GPT&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35782;&#21035;ChatGPT&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22522;&#20110;Reddit&#30340;&#21508;&#31181;&#19978;&#19979;&#25991;&#25552;&#31034;&#23545;&#65292;&#21253;&#25324;&#20154;&#20026;&#29983;&#25104;&#21644;ChatGPT&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#19978;&#19979;&#25991;&#22797;&#26434;&#24615;&#21644;&#21709;&#24212;&#36136;&#37327;&#12290;&#20026;&#23637;&#31034;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#20102;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#23427;&#20204;&#22312;&#21306;&#20998;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#21709;&#24212;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#36825;&#19968;&#25968;&#25454;&#38598;&#21487;&#29992;&#20316;&#35780;&#20272;&#21644;&#25512;&#36827;&#26816;&#27979;&#25216;&#26415;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07321v1 Announce Type: new  Abstract: As natural language models like ChatGPT become increasingly prevalent in applications and services, the need for robust and accurate methods to detect their output is of paramount importance. In this paper, we present GPT Reddit Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text detection dataset designed to assess the performance of detection models in identifying generated responses from ChatGPT. The dataset consists of a diverse collection of context-prompt pairs based on Reddit, with human-generated and ChatGPT-generated responses. We provide an analysis of the dataset's characteristics, including linguistic diversity, context complexity, and response quality. To showcase the dataset's utility, we benchmark several detection methods on it, demonstrating their efficacy in distinguishing between human and ChatGPT-generated responses. This dataset serves as a resource for evaluating and advancing detection te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07311</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;KG-LLM&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20998;&#26512;&#39046;&#22495;&#65292;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20869;&#22810;&#20010;&#38142;&#25509;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#38190;&#30340;NLP&#33539;&#20363;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;KG&#36716;&#25442;&#20026;CoT&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35782;&#21035;&#24182;&#23398;&#20064;&#23454;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20026;&#20102;&#23637;&#31034;KG-LLM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#24494;&#35843;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#38750;ICL&#21644;ICL&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20026;LLMs&#25552;&#20379;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;LLMs&#23545;&#40784;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#65292;&#20805;&#20998;&#37322;&#25918;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2403.07300</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#25511;&#21046;&#39044;&#35757;&#32451;LLMs&#36827;&#34892;&#24191;&#20041;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;LLMs&#23545;&#40784;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#65292;&#20805;&#20998;&#37322;&#25918;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26368;&#36817;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26377;&#38480;&#30340;&#26102;&#38388;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28608;&#22686;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;LLMs&#24341;&#20837;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#23558;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#22266;&#26377;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#26102;&#38388;&#24207;&#21015;&#23545;&#40784;&#26694;&#26550;&#65292;&#31216;&#20026;LLaTA&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#20013;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#36755;&#20837;&#26080;&#20851;&#38745;&#24577;&#30693;&#35782;&#21644;&#36755;&#20837;&#30456;&#20851;&#21160;&#24577;&#30693;&#35782;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#20026;&#39044;&#27979;&#27169;&#22411;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07300v1 Announce Type: cross  Abstract: Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with f
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CypherTalk&#30340;&#25104;&#26412;&#25928;&#30410;&#21644;&#33258;&#36866;&#24212;&#30340;LLM&#25671;&#26179;&#35843;&#25972;&#21644;&#24674;&#22797;&#26426;&#21046;&#65292;&#36890;&#36807;&#20248;&#21270;&#25671;&#26179;&#25805;&#20316;&#31526;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#22312;&#25104;&#26412;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#26435;&#34913;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07283</link><description>&lt;p&gt;
&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#21644;&#33258;&#36866;&#24212;&#30340;LLM&#25671;&#26179;&#21644;&#24674;&#22797;&#26426;&#21046;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07283
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CypherTalk&#30340;&#25104;&#26412;&#25928;&#30410;&#21644;&#33258;&#36866;&#24212;&#30340;LLM&#25671;&#26179;&#35843;&#25972;&#21644;&#24674;&#22797;&#26426;&#21046;&#65292;&#36890;&#36807;&#20248;&#21270;&#25671;&#26179;&#25805;&#20316;&#31526;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#22312;&#25104;&#26412;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#26435;&#34913;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#29992;&#25143;&#24076;&#26395;&#36890;&#36807;&#20113;&#26381;&#21153;&#24320;&#21457;&#21644;&#37096;&#32626;&#20182;&#20204;&#23450;&#21046;&#30340;LLMs&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20123;&#29305;&#23450;&#39046;&#22495;&#65292;&#20154;&#20204;&#20173;&#28982;&#20851;&#27880;&#25104;&#26412;&#12289;&#38544;&#31169;&#38382;&#39064;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CypherTalk&#30340;&#25104;&#26412;&#25928;&#30410;&#21644;&#33258;&#36866;&#24212;LLM&#25671;&#26179;&#35843;&#25972;&#21644;&#24674;&#22797;&#26426;&#21046;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#27700;&#24179;&#21644;&#22402;&#30452;&#25671;&#26179;&#25805;&#20316;&#31526;&#65292;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#19982;&#22522;&#20110;&#23494;&#30721;&#23398;&#25110;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#30340;LLM&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;CypherTalk&#26694;&#26550;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#20351;&#29992;&#20248;&#21270;&#25671;&#26179;&#25805;&#20316;&#31526;&#35774;&#32622;&#26102;&#23454;&#29616;&#21487;&#38752;&#30340;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#32771;&#34385;&#22312;LLM&#22330;&#26223;&#20013;&#25104;&#26412;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#26435;&#34913;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07283v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) gain great success in real-world applications, an increasing number of users are seeking to develop and deploy their customized LLMs through cloud services. Nonetheless, in some specific domains, there are still concerns regarding cost and trade-offs between privacy issues and accuracy. In this study, we introduce a cost-effective and self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With carefully designed horizontal and vertical shaking operators, we can achieve comparable accuracy results with SOTA privacy-preserving LLM schemes using Cryptography-based or Differential Privacy-based methods. Experiments also show that with the CypherTalk framework, users can achieve reliable accuracy when using optimized shaking operator settings. To our best knowledge, this is the first work that considers cost, and trade-off between model utility and privacy in LLM scenarios.
&lt;/p&gt;</description></item><item><title>&#23545;&#30693;&#35782;&#36861;&#36394;&#31639;&#27861;&#36827;&#34892;&#20102;&#35299;&#37322;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#20998;&#20026;&#36879;&#26126;&#27169;&#22411;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#38454;&#27573;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07279</link><description>&lt;p&gt;
&#23545;&#21487;&#35299;&#37322;&#30693;&#35782;&#36861;&#36394;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Explainable Knowledge Tracing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07279
&lt;/p&gt;
&lt;p&gt;
&#23545;&#30693;&#35782;&#36861;&#36394;&#31639;&#27861;&#36827;&#34892;&#20102;&#35299;&#37322;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#20998;&#20026;&#36879;&#26126;&#27169;&#22411;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#38454;&#27573;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39640;&#36136;&#37327;&#25945;&#32946;&#25968;&#25454;&#30340;&#38271;&#26399;&#31215;&#32047;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#30693;&#35782;&#36861;&#36394;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19968;&#20123;&#31639;&#27861;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#20219;&#38477;&#20302;&#65292;&#26234;&#33021;&#20915;&#31574;&#30340;&#25509;&#21463;&#24230;&#38477;&#20302;&#12290;&#22240;&#27492;&#65292;&#31639;&#27861;&#38656;&#35201;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#65292;&#29992;&#25143;&#38656;&#35201;&#20102;&#35299;&#20869;&#37096;&#25805;&#20316;&#26426;&#21046;&#24182;&#20026;&#20915;&#31574;&#25552;&#20379;&#21487;&#38752;&#35299;&#37322;&#12290;&#26412;&#25991;&#23545;&#21487;&#35299;&#37322;&#24615;KT&#31639;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#30693;&#35782;&#36861;&#36394;&#30340;&#27010;&#24565;&#21644;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#23558;&#21487;&#35299;&#37322;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#20998;&#20026;&#20004;&#31867;&#65306;&#36879;&#26126;&#27169;&#22411;&#21644;&#40657;&#30418;&#27169;&#22411;&#12290;&#25509;&#30528;&#65292;&#20174;&#8220;ante hoc&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#8221;&#12289;&#8220;post hoc&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#8221;&#21644;&#20854;&#20182;&#32500;&#24230;&#23457;&#26597;&#20102;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07279v1 Announce Type: new  Abstract: With the long term accumulation of high quality educational data, artificial intelligence has shown excellent performance in knowledge tracing. However, due to the lack of interpretability and transparency of some algorithms, this approach will result in reduced stakeholder trust and a decreased acceptance of intelligent decisions. Therefore, algorithms need to achieve high accuracy, and users need to understand the internal operating mechanism and provide reliable explanations for decisions. This paper thoroughly analyzes the interpretability of KT algorithms. First, the concepts and common methods of explainable artificial intelligence and knowledge tracing are introduced. Next, explainable knowledge tracing models are classified into two categories: transparent models and black box models. Then, the interpretable methods used are reviewed from three stages: ante hoc interpretable methods, post hoc interpretable methods, and other dime
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32852;&#21512;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#29983;&#25104;&#20114;&#21160;&#32773;&#30340;&#24120;&#35782;&#30693;&#35782;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07260</link><description>&lt;p&gt;
CKERC&#65306;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32852;&#21512;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32852;&#21512;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#29983;&#25104;&#20114;&#21160;&#32773;&#30340;&#24120;&#35782;&#30693;&#35782;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;(ERC)&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#23427;&#22312;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#20013;&#39044;&#27979;&#35805;&#35821;&#30340;&#24773;&#24863;&#12290;&#23427;&#39640;&#24230;&#20381;&#36182;&#20110;&#23545;&#35805;&#35821;&#22659;&#12289;&#35828;&#35805;&#32773;&#36523;&#20221;&#20449;&#24687;&#12289;&#22810;&#26041;&#23545;&#35805;&#22330;&#26223;&#31561;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;instructERC&#65289;&#20165;&#20165;&#35782;&#21035;&#35828;&#35805;&#32773;&#65292;&#24573;&#30053;&#20102;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#35828;&#35805;&#32773;&#32972;&#21518;&#30340;&#24120;&#35782;&#30693;&#35782;(&#21363;&#65292;&#21548;&#20247;&#30340;&#21453;&#24212;&#21644;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#31561;)&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#20197;&#28145;&#20837;&#25366;&#25496;&#35828;&#35805;&#32773;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32852;&#21512;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;&#26694;&#26550;&#65292;&#21363;CKERC&#12290;&#25105;&#20204;&#35774;&#35745;&#25552;&#31034;&#26469;&#29983;&#25104;&#22522;&#20110;&#21382;&#21490;&#35805;&#35821;&#30340;&#23545;&#35805;&#32773;&#24120;&#35782;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#21160;&#32773;&#24120;&#35782;&#35782;&#21035;&#20219;&#21153;&#36827;&#34892;LLM&#39044;&#35757;&#32451;&#65292;&#20197;&#24494;&#35843;&#35828;&#35805;&#32773;&#38544;&#21547;&#32447;&#32034;&#20449;&#24687;&#12290;&#36890;&#36807;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07260v1 Announce Type: new  Abstract: Emotion recognition in conversation (ERC) is a task which predicts the emotion of an utterance in the context of a conversation. It tightly depends on dialogue context, speaker identity information, multiparty dialogue scenario and so on. However, the state-of-the-art method (instructERC) solely identifying speaker, and ignores commonsense knowledge(i.e., reaction of the listeners and intention of the speaker, etc.) behind speakers during a conversation, which can deeply mine speaker information. To this end, we propose a novel joint large language models with commonsense knowledge framework for emotion recognition in conversation, namely CKERC.We design prompts to generate interlocutors' commonsense based on historical utterances with large language model. And we use the interlocutor commonsense identification task for LLM pre-training to fine-tune speaker implicit clues information.By solving above challenge, our method achieve state-o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Curry-DPO&#30340;&#26041;&#27861;&#65292;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#20013;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07230</link><description>&lt;p&gt;
Curry-DPO&#65306;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#25490;&#21517;&#20559;&#22909;&#22686;&#24378;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Curry-DPO: Enhancing Alignment using Curriculum Learning &amp; Ranked Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Curry-DPO&#30340;&#26041;&#27861;&#65292;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#20013;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;(&#36890;&#24120;&#26159;&#27599;&#20010;&#29992;&#25143;&#25552;&#31034;&#36873;&#25321;&#21644;&#25298;&#32477;&#30340;&#21709;&#24212;&#23545;)&#23558;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#20110;&#32473;&#23450;&#25552;&#31034;&#21487;&#33021;&#20250;&#23384;&#22312;&#22810;&#20010;&#21709;&#24212;&#65292;&#36825;&#20123;&#21709;&#24212;&#30340;&#36136;&#37327;&#30456;&#23545;&#20110;&#24444;&#27492;&#32780;&#35328;&#26377;&#25152;&#19981;&#21516;&#12290;&#26377;&#20102;&#36825;&#20123;&#22810;&#20010;&#21709;&#24212;&#30340;&#36136;&#37327;&#35780;&#32423;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36825;&#20123;&#21709;&#24212;&#20026;&#32473;&#23450;&#25552;&#31034;&#21019;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#31995;&#32479;&#22320;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#36827;&#34892;DPO&#35757;&#32451;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#26631;&#20934;&#23558;&#36825;&#20123;&#22810;&#20010;&#20559;&#22909;&#25968;&#25454;&#23545;&#20174;&#26131;&#21040;&#38590;(&#27169;&#25311;&#35838;&#31243;&#35757;&#32451;)&#25490;&#24207;&#12290;&#25105;&#20204;&#35814;&#32454;&#27604;&#36739;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Curry-DPO&#65292;&#22312;MTbench&#12289;Vicuna&#12289;Wiz&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#22686;&#24378;&#30340;&#24615;&#33021;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07230v1 Announce Type: cross  Abstract: Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, Wiz
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#26469;&#24314;&#31435;&#29702;&#35770;&#65292;&#20351;&#29992;&#35748;&#30693;&#39537;&#21160;&#35299;&#26512;&#22120;SPAWN&#29983;&#25104;&#37327;&#21270;&#21551;&#21160;&#39044;&#27979;&#24182;&#35780;&#20272;&#65292;&#24182;&#20197;&#31616;&#21270;&#30340;&#23450;&#35821;&#20174;&#21477;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#19968;&#20010;&#29702;&#35770;&#30340;&#21551;&#21160;&#39044;&#27979;&#19982;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#19968;&#33268;</title><link>https://arxiv.org/abs/2403.07202</link><description>&lt;p&gt;
&#20174;&#35748;&#30693;&#39537;&#21160;&#30340;&#35299;&#26512;&#22120;&#20013;&#39044;&#27979;&#32467;&#26500;&#21551;&#21160;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SPAWNing Structural Priming Predictions from a Cognitively Motivated Parser
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#26469;&#24314;&#31435;&#29702;&#35770;&#65292;&#20351;&#29992;&#35748;&#30693;&#39537;&#21160;&#35299;&#26512;&#22120;SPAWN&#29983;&#25104;&#37327;&#21270;&#21551;&#21160;&#39044;&#27979;&#24182;&#35780;&#20272;&#65292;&#24182;&#20197;&#31616;&#21270;&#30340;&#23450;&#35821;&#20174;&#21477;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#19968;&#20010;&#29702;&#35770;&#30340;&#21551;&#21160;&#39044;&#27979;&#19982;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21551;&#21160;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#33539;&#24335;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#21477;&#23376;&#34920;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#26469;&#24314;&#31435;&#29702;&#35770;&#65292;&#25551;&#36848;&#20154;&#31867;&#22788;&#29702;&#21477;&#23376;&#26102;&#26500;&#24314;&#30340;&#32467;&#26500;&#34920;&#24449;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#39537;&#21160;&#35299;&#26512;&#22120;SPAWN&#65292;&#26681;&#25454;&#29702;&#35770;&#21477;&#27861;&#29983;&#25104;&#37327;&#21270;&#21551;&#21160;&#39044;&#27979;&#65292;&#24182;&#29992;&#23454;&#35777;&#20154;&#31867;&#34892;&#20026;&#35780;&#20272;&#36825;&#20123;&#39044;&#27979;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#24212;&#29992;&#36825;&#19968;&#26694;&#26550;&#26469;&#30740;&#31350;&#33521;&#35821;&#20013;&#31616;&#21270;&#30340;&#23450;&#35821;&#20174;&#21477;&#34920;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;SPAWN&#20174;&#20004;&#20010;&#29702;&#35770;&#35299;&#37322;&#20013;&#29983;&#25104;&#21551;&#21160;&#39044;&#27979;&#65292;&#36825;&#20004;&#20010;&#35299;&#37322;&#23545;&#23450;&#35821;&#20174;&#21477;&#30340;&#32467;&#26500;&#20570;&#20986;&#20102;&#19981;&#21516;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#26377;&#19968;&#20010;&#29702;&#35770;&#65288;&#21442;&#19982;&#24335;-&#30456;&#20301;&#65289;&#30340;&#39044;&#27979;&#19982;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#19968;&#33268;&#65292;&#20174;&#32780;&#31361;&#20986;&#26174;&#31034;&#20986;&#21738;&#20123;&#23545;&#23450;&#35821;&#20174;&#21477;&#30340;&#20551;&#35774;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#20154;&#31867;&#21477;&#23376;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07202v1 Announce Type: new  Abstract: Structural priming is a widely used psycholinguistic paradigm to study human sentence representations. In this work we propose a framework for using empirical priming patterns to build a theory characterizing the structural representations humans construct when processing sentences. This framework uses a new cognitively motivated parser, SPAWN, to generate quantitative priming predictions from theoretical syntax and evaluate these predictions with empirical human behavior. As a case study, we apply this framework to study reduced relative clause representations in English. We use SPAWN to generate priming predictions from two theoretical accounts which make different assumptions about the structure of relative clauses. We find that the predictions from only one of these theories (Participial-Phase) align with empirical priming patterns, thus highlighting which assumptions about relative clause better capture human sentence representation
&lt;/p&gt;</description></item><item><title>CuentosIE&#25552;&#20379;&#20102;&#19968;&#22871;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#23507;&#35328;&#25925;&#20107;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#24037;&#20855;&#65292;&#26088;&#22312;&#25945;&#32946;&#29992;&#25143;&#24773;&#24863;&#30693;&#35782;&#65292;&#24182;&#30417;&#27979;&#20182;&#20204;&#30340;&#24773;&#24863;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.07193</link><description>&lt;p&gt;
CuentosIE&#65306;&#19968;&#20010;&#20851;&#20110;&#8220;&#23507;&#35328;&#23507;&#24847;&#8221;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#21542;&#26377;&#21161;&#20110;&#25945;&#25480;&#24773;&#21830;&#65311;
&lt;/p&gt;
&lt;p&gt;
CuentosIE: can a chatbot about "tales with a message" help to teach emotional intelligence?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07193
&lt;/p&gt;
&lt;p&gt;
CuentosIE&#25552;&#20379;&#20102;&#19968;&#22871;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#23507;&#35328;&#25925;&#20107;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#24037;&#20855;&#65292;&#26088;&#22312;&#25945;&#32946;&#29992;&#25143;&#24773;&#24863;&#30693;&#35782;&#65292;&#24182;&#30417;&#27979;&#20182;&#20204;&#30340;&#24773;&#24863;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CuentosIE&#65288;TalesEI&#65306;&#19968;&#20010;&#20851;&#20110;&#24773;&#21830;&#21457;&#23637;&#30340;&#23507;&#35328;&#23507;&#24847;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#24773;&#24863;&#30340;&#25945;&#32946;&#24615;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#20063;&#20026;&#25945;&#24072;&#21644;&#24515;&#29702;&#23398;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;CuentosIE&#32534;&#21046;&#30340;&#25351;&#26631;&#21644;&#25968;&#25454;&#26469;&#30417;&#27979;&#20182;&#20204;&#30340;&#23398;&#29983;/&#24739;&#32773;&#12290;&#36873;&#25321;&#8220;&#23507;&#35328;&#23507;&#24847;&#8221;&#30340;&#29702;&#30001;&#22312;&#20110;&#23427;&#20204;&#30340;&#31616;&#21333;&#24615;&#21644;&#26131;&#20110;&#29702;&#35299;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#36947;&#24503;&#25110;&#30456;&#20851;&#30340;&#38544;&#21947;&#12290; CuentosIE&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#36873;&#25321;&#12289;&#25910;&#38598;&#21644;&#20998;&#31867;&#19968;&#32452;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#25925;&#20107;&#65292;&#20197;&#21450;&#25552;&#20379;&#24037;&#20855;&#65288;&#25628;&#32034;&#12289;&#38405;&#35835;&#29702;&#35299;&#12289;&#32842;&#22825;&#12289;&#25512;&#33616;&#21644;&#20998;&#31867;&#65289;&#23545;&#25945;&#32946;&#29992;&#25143;&#24773;&#24863;&#24182;&#30417;&#27979;&#20182;&#20204;&#30340;&#24773;&#24863;&#21457;&#23637;&#37117;&#24456;&#26377;&#29992;&#12290;&#35813;&#24037;&#20855;&#30340;&#21021;&#27493;&#35780;&#20272;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#36825;&#32943;&#23450;&#20102;&#25991;&#31456;&#26631;&#39064;&#20013;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07193v1 Announce Type: cross  Abstract: In this article, we present CuentosIE (TalesEI: chatbot of tales with a message to develop Emotional Intelligence), an educational chatbot on emotions that also provides teachers and psychologists with a tool to monitor their students/patients through indicators and data compiled by CuentosIE. The use of "tales with a message" is justified by their simplicity and easy understanding, thanks to their moral or associated metaphors. The main contributions of CuentosIE are the selection, collection, and classification of a set of highly specialized tales, as well as the provision of tools (searching, reading comprehension, chatting, recommending, and classifying) that are useful for both educating users about emotions and monitoring their emotional development. The preliminary evaluation of the tool has obtained encouraging results, which provides an affirmative answer to the question posed in the title of the article.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;$(N,K)$-Puzzle&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07191</link><description>&lt;p&gt;
$\mathbf{(N,K)}$-Puzzle&#65306;&#19968;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25104;&#26412;&#25928;&#30410;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07191
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;$(N,K)$-Puzzle&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#26088;&#22312;&#25552;&#39640;&#35268;&#27169;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290; &#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#19988;&#26631;&#20934;&#21270;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#36825;&#20123;&#31639;&#27861;&#12290; &#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;24-Puzzle&#30340;&#19968;&#33324;&#21270;&#29256;&#26412;&#65306;$(N, K)$-Puzzle&#65292;&#25361;&#25112;&#35821;&#35328;&#27169;&#22411;&#20197;&#20351;&#29992;$N$&#20010;&#25972;&#25968;&#36798;&#21040;&#30446;&#26631;&#20540;$K$&#12290; &#25105;&#20204;&#35780;&#20272;&#20102;&#24050;&#24314;&#31435;&#30340;RL&#31639;&#27861;&#65288;&#22914;Proximal Policy Optimization&#65288;PPO&#65289;&#65289;&#65292;&#20197;&#21450;&#26032;&#39062;&#26041;&#27861;&#65288;&#22914;Identity Policy Optimization&#65288;IPO&#65289;&#21644;Direct Policy Optimization&#65288;DPO&#65289;&#65289;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07191v1 Announce Type: cross  Abstract: Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms. To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers. We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO).
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.07183</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#19978;&#30417;&#27979;AI&#20462;&#25913;&#30340;&#20869;&#23481;&#65306;AI&#20250;&#35758;&#21516;&#34892;&#35780;&#23457;&#20013;ChatGPT&#24433;&#21709;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#25991;&#26412;&#21487;&#33021;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22823;&#24133;&#20462;&#25913;&#25110;&#29983;&#25104;&#30340;&#37096;&#20998;&#27604;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26368;&#22823;&#20284;&#28982;&#27169;&#22411;&#21033;&#29992;&#19987;&#23478;&#25776;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#21442;&#32771;&#25991;&#26412;&#65292;&#20934;&#30830;&#39640;&#25928;&#22320;&#26816;&#26597;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#30495;&#23454;&#19990;&#30028;LLM&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;AI&#20250;&#35758;&#19978;&#31185;&#23398;&#21516;&#34892;&#35780;&#23457;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#21457;&#29983;&#22312;ChatGPT&#21457;&#24067;&#20043;&#21518;&#65292;&#21253;&#25324;ICLR 2024&#12289;NeurIPS 2023&#12289;CoRL 2023&#21644;EMNLP 2023&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20250;&#35758;&#25552;&#20132;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#26159;&#30001;LLMs&#22823;&#24133;&#20462;&#25913;&#30340;&#65292;&#21363;&#36229;&#20986;&#25340;&#20889;&#26816;&#26597;&#25110;&#23567;&#24133;&#26356;&#26032;&#30340;&#33539;&#22260;&#12290;&#29983;&#25104;&#25991;&#26412;&#20986;&#29616;&#30340;&#24773;&#20917;&#20026;&#29992;&#25143;&#34892;&#20026;&#25552;&#20379;&#20102;&#35265;&#35299;&#65306;&#22312;&#25253;&#21578;&#20449;&#24515;&#36739;&#20302;&#12289;&#22312;&#25130;&#27490;&#26085;&#26399;&#21069;&#25552;&#20132;&#30340;&#35780;&#35770;&#20197;&#21450;&#20174;&#35780;&#35770;&#20844;&#21496;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07183v1 Announce Type: cross  Abstract: We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from review
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;3M-Diffusion&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#12289;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.07179</link><description>&lt;p&gt;
3M-Diffusion&#65306;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#28508;&#22312;&#22810;&#27169;&#24577;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;3M-Diffusion&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#12289;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#20998;&#23376;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#29983;&#25104;&#19982;&#25991;&#26412;&#25551;&#36848;&#31934;&#30830;&#21305;&#37197;&#30340;&#20998;&#23376;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;3M-Diffusion&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07179v1 Announce Type: cross  Abstract: Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the late
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07175</link><description>&lt;p&gt;
&#37325;&#24314;ROME: &#35299;&#20915;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;Rank-One Model Editing (ROME)&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26377;&#19968;&#20123;&#20107;&#23454;&#34920;&#26126;&#35813;&#31639;&#27861;&#26080;&#27861;&#36827;&#34892;&#32534;&#36753;&#32780;&#19981;&#30772;&#22351;&#27169;&#22411;&#12290;&#36825;&#20123;&#32534;&#36753;&#20197;&#21069;&#34987;&#31216;&#20026;&#31105;&#29992;&#32534;&#36753;&#12290;&#36825;&#20123;&#31105;&#29992;&#32534;&#36753;&#20250;&#23548;&#33268;&#31435;&#21363;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#38480;&#21046;&#20102;ROME&#29992;&#20110;&#39034;&#24207;&#32534;&#36753;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20351;&#29992;CounterFact&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#36753;&#26102;&#65292;ROME&#20165;&#22312;&#27492;&#26102;&#21457;&#29983;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#22312;&#20351;&#29992;zsRE&#25968;&#25454;&#38598;&#26102;&#19981;&#20250;&#21457;&#29983;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#31105;&#29992;&#32534;&#36753;&#26159;ROME&#21407;&#22987;&#23454;&#29616;&#30340;&#20135;&#29289;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#31283;&#23450;&#30340;&#23454;&#29616;ROME&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;r-ROME&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#22312;&#20351;&#29992;ROME&#36827;&#34892;&#22823;&#35268;&#27169;&#39034;&#24207;&#32534;&#36753;&#26102;&#19981;&#20877;&#35266;&#23519;&#21040;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07175v1 Announce Type: cross  Abstract: Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME.
&lt;/p&gt;</description></item><item><title>Thought Graph&#26694;&#26550;&#36890;&#36807;&#22522;&#22240;&#38598;&#20998;&#26512;&#25581;&#31034;&#29983;&#29289;&#36807;&#31243;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#27880;&#37322;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07144</link><description>&lt;p&gt;
Thought Graph: &#29983;&#25104;&#29983;&#29289;&#25512;&#29702;&#24605;&#32500;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Thought Graph: Generating Thought Process for Biological Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07144
&lt;/p&gt;
&lt;p&gt;
Thought Graph&#26694;&#26550;&#36890;&#36807;&#22522;&#22240;&#38598;&#20998;&#26512;&#25581;&#31034;&#29983;&#29289;&#36807;&#31243;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#27880;&#37322;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Thought Graph&#20316;&#20026;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#25903;&#25345;&#22797;&#26434;&#25512;&#29702;&#65292;&#24182;&#20197;&#22522;&#22240;&#38598;&#20998;&#26512;&#20026;&#20363;&#25581;&#31034;&#29983;&#29289;&#36807;&#31243;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#28145;&#20837;&#29702;&#35299;&#22522;&#22240;&#38598;&#65292;&#26681;&#25454;&#19982;&#20154;&#31867;&#27880;&#37322;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;GSEA 40.28%&#21644;LLM&#22522;&#32447;5.38%&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29983;&#29289;&#36807;&#31243;&#21629;&#21517;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#23545;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#31934;&#20934;&#21307;&#23398;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07144v1 Announce Type: new  Abstract: We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28% and LLM baselines by 5.38% based on cosine similarity to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#38598;&#31934;&#28860;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#31934;&#28860;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#22797;&#26434;&#26550;&#26500;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07142</link><description>&lt;p&gt;
&#19968;&#20010;&#31867;&#21035;&#19968;&#20010;&#25552;&#31034;&#65306;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
One Category One Prompt: Dataset Distillation using Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07142
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#38598;&#31934;&#28860;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#31934;&#28860;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#22797;&#26434;&#26550;&#26500;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25152;&#38656;&#30340;&#22823;&#37327;&#25968;&#25454;&#23545;&#23384;&#20648;&#21644;&#20256;&#36755;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25968;&#25454;&#38598;&#31934;&#28860;&#24050;&#32463;&#25104;&#20026;&#23558;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#32452;&#20195;&#34920;&#24615;&#21512;&#25104;&#26679;&#26412;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#36890;&#24120;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#26356;&#22797;&#26434;&#26550;&#26500;&#26102;&#24456;&#38590;&#26377;&#25928;&#25193;&#23637;&#65292;&#36825;&#26159;&#30001;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#21033;&#29992;&#20998;&#31163;&#30340;&#20248;&#21270;&#26041;&#26696;&#23558;&#30693;&#35782;&#31934;&#28860;&#21644;&#25968;&#25454;&#38598;&#31934;&#28860;&#30456;&#32467;&#21512;&#65292;&#20197;&#25193;&#22823;&#25968;&#25454;&#38598;&#31934;&#28860;&#35268;&#27169;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#24191;&#27867;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#38656;&#35201;&#23384;&#20648;&#22686;&#24378;&#22270;&#20687;&#30340;&#36719;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;D3M&#65289;&#20316;&#20026;&#25968;&#25454;&#38598;&#31934;&#28860;&#30340;&#26032;&#33539;&#24335;&#65292;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07142v1 Announce Type: cross  Abstract: The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20174;&#22240;&#26524;&#22270;&#20013;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#22240;&#26524;&#25991;&#26412;&#25551;&#36848;&#22312;&#35757;&#32451;&#25968;&#25454;&#22686;&#21152;&#26102;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#22312;&#38646;&#28909;&#36523;&#35774;&#32622;&#19979;&#26356;&#38590;&#29983;&#25104;&#65292;&#29992;&#25143;&#21487;&#20197;&#26356;&#24555;&#37096;&#32626;&#26410;&#26469;&#24212;&#29992;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2403.07118</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21465;&#36848;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Narrating Causal Graphs with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20174;&#22240;&#26524;&#22270;&#20013;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#22240;&#26524;&#25991;&#26412;&#25551;&#36848;&#22312;&#35757;&#32451;&#25968;&#25454;&#22686;&#21152;&#26102;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#22312;&#38646;&#28909;&#36523;&#35774;&#32622;&#19979;&#26356;&#38590;&#29983;&#25104;&#65292;&#29992;&#25143;&#21487;&#20197;&#26356;&#24555;&#37096;&#32626;&#26410;&#26469;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20174;&#22270;&#24418;&#29983;&#25104;&#25991;&#26412;&#25551;&#36848;&#30340;&#24212;&#29992;&#20027;&#35201;&#38598;&#20013;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#65292;&#20854;&#36890;&#36807;&#20107;&#23454;&#36830;&#25509;&#27010;&#24565;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20174;&#22240;&#26524;&#22270;&#20013;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#26174;&#33879;&#27010;&#24565;&#34920;&#31034;&#20026;&#33410;&#28857;&#65292;&#22240;&#26524;&#20851;&#31995;&#36890;&#36807;&#26377;&#21521;&#12289;&#31867;&#22411;&#21270;&#36793;&#34920;&#31034;&#12290;&#36825;&#20123;&#22270;&#20013;&#32534;&#30721;&#30340;&#22240;&#26524;&#25512;&#29702;&#21487;&#20197;&#25903;&#25345;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#25110;&#33829;&#38144;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22240;&#26524;&#22270;&#25968;&#25454;&#38598;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#23454;&#35777;&#30740;&#31350;&#20102;&#22235;&#20010;GPT-3&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#20107;&#23454;&#30340;&#22270;&#24418;&#30456;&#27604;&#65292;&#34429;&#28982;&#22240;&#26524;&#25991;&#26412;&#25551;&#36848;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#25913;&#21892;&#65292;&#20294;&#22312;&#38646;&#28909;&#36523;&#35774;&#32622;&#19979;&#26356;&#38590;&#29983;&#25104;&#12290;&#32467;&#26524;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#29992;&#25143;&#21487;&#20197;&#26356;&#24555;&#22320;&#37096;&#32626;&#26410;&#26469;&#24212;&#29992;&#31243;&#24207;&#65292;&#22240;&#20026;&#24403;&#20165;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#35757;&#32451;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07118v1 Announce Type: new  Abstract: The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts. In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges. The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing. Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings. Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings. Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few example
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#24555;&#36895;&#36827;&#34892;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#30041;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2403.07088</link><description>&lt;p&gt;
SPA&#65306;&#38754;&#21521;&#20113;&#31471;&#21644;&#35774;&#22791;&#21327;&#20316;&#30340;&#35745;&#31639;&#21451;&#22909;&#22411;Seq2seq&#20010;&#24615;&#21270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07088
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#24555;&#36895;&#36827;&#34892;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#30041;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#38382;&#31572;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;&#28982;&#32780;&#65292;LLMs&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#22823;&#20869;&#23384;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24403;&#35757;&#32451;&#25110;&#39044;&#27979;&#36807;&#31243;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#24555;&#36895;&#35774;&#22791;&#19978;&#30340;&#25512;&#26029;&#21644;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#20445;&#25345;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LSTM&#30340;&#25991;&#26412;&#29983;&#25104;&#22312;&#21382;&#21490;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#35821;&#35328;&#20016;&#23500;&#12289;&#35821;&#22659;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07087</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;&#25991;&#26412;&#29983;&#25104;&#65306;&#23545;&#21382;&#21490;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LSTM-Based Text Generation: A Study on Historical Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LSTM&#30340;&#25991;&#26412;&#29983;&#25104;&#22312;&#21382;&#21490;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#35821;&#35328;&#20016;&#23500;&#12289;&#35821;&#22659;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#33678;&#22763;&#27604;&#20122;&#21644;&#23612;&#37319;&#30340;&#21382;&#21490;&#25968;&#25454;&#38598;&#12290;LSTMs&#20197;&#20854;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#32780;&#38395;&#21517;&#65292;&#36825;&#37324;&#24212;&#29992;&#23427;&#20204;&#26469;&#24314;&#27169;&#21382;&#21490;&#25991;&#26412;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#35821;&#35328;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;LSTM&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#35821;&#35328;&#20016;&#23500;&#19988;&#35821;&#22659;&#30456;&#20851;&#30340;&#25991;&#26412;&#65292;&#36824;&#33021;&#25552;&#20379;&#20851;&#20110;&#35821;&#35328;&#27169;&#24335;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#35265;&#35299;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#23612;&#37319;&#20316;&#21697;&#25991;&#26412;&#26102;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#29575;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#25439;&#22833;&#20540;&#21644;100&#27425;&#36845;&#20195;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20026;0.9521&#65292;&#34920;&#26126;&#20854;&#39640;&#20934;&#30830;&#24615;&#12290;&#27169;&#22411;&#30340;&#25439;&#22833;&#20026;0.2518&#65292;&#26174;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07087v1 Announce Type: cross  Abstract: This paper presents an exploration of Long Short-Term Memory (LSTM) networks in the realm of text generation, focusing on the utilization of historical datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical texts. The study demonstrates that LSTM-based models, when trained on historical datasets, can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time. The finding presents models that are highly accurate and efficient in predicting text from works of Nietzsche, with low loss values and a training time of 100 iterations. The accuracy of the model is 0.9521, indicating high accuracy. The loss of the model is 0.2518, indicating its effectiveness. The accuracy of the model in predicting text from the w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;</title><link>https://arxiv.org/abs/2403.07008</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20215;&#27491;&#30830;: &#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AutoEval Done Right: Using Synthetic Data for Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07008
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35780;&#20272;&#20351;&#29992;&#20154;&#24037;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#21487;&#20197;&#20351;&#29992;AI&#26631;&#35760;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#23569;&#27492;&#31867;&#30446;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#37327;&#65292;&#36825;&#19968;&#36807;&#31243;&#31216;&#20026;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#20559;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#19982;GPT-4&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07008v1 Announce Type: cross  Abstract: The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31639;&#27861;DOMINO&#65292;&#22312;&#29983;&#25104;&#25991;&#26412;&#36807;&#31243;&#20013;&#20197;&#23436;&#20840;&#22522;&#20110;&#23376;&#35789;&#23545;&#40784;&#30340;&#26041;&#24335;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#65292;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#24320;&#38144;&#24182;&#26377;&#26102;&#29978;&#33267;&#23454;&#29616;&#36817;2&#20493;&#36895;&#24230;&#25552;&#21319;&#65292;&#36828;&#36828;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06988</link><description>&lt;p&gt;
&#24341;&#23548;LLM&#36208;&#21521;&#27491;&#30830;&#20043;&#36335;&#65306;&#24555;&#36895;&#12289;&#38750;&#20405;&#20837;&#24335;&#21463;&#38480;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06988
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31639;&#27861;DOMINO&#65292;&#22312;&#29983;&#25104;&#25991;&#26412;&#36807;&#31243;&#20013;&#20197;&#23436;&#20840;&#22522;&#20110;&#23376;&#35789;&#23545;&#40784;&#30340;&#26041;&#24335;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#65292;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#24320;&#38144;&#24182;&#26377;&#26102;&#29978;&#33267;&#23454;&#29616;&#36817;2&#20493;&#36895;&#24230;&#25552;&#21319;&#65292;&#36828;&#36828;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#31526;&#21512;&#39044;&#26399;&#26684;&#24335;&#65292;&#21463;&#38480;&#35299;&#30721;&#25552;&#20986;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;&#20005;&#26684;&#30340;&#24418;&#24335;&#35821;&#35328;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#23637;&#31034;&#30340;&#65292;&#36825;&#31867;&#26041;&#27861;&#19981;&#20165;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20135;&#29983;&#24615;&#33021;&#24320;&#38144;&#65292;&#32780;&#19988;&#35768;&#22810;&#26041;&#27861;&#22914;&#26524;&#27809;&#26377;&#27491;&#30830;&#22320;&#23558;LLM&#23376;&#35789;&#35789;&#27719;&#19982;&#22806;&#37096;&#32422;&#26463;&#23545;&#40784;&#65292;&#21017;&#36824;&#20250;&#26174;&#33879;&#25439;&#23475;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31639;&#27861;DOMINO&#65292;&#21487;&#20197;&#20197;&#23436;&#20840;&#22522;&#20110;&#23376;&#35789;&#23545;&#40784;&#30340;&#26041;&#24335;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#65292;&#21516;&#26102;&#21033;&#29992;&#39044;&#35745;&#31639;&#21644;&#25512;&#27979;&#35299;&#30721;&#26469;&#23454;&#29616;&#20960;&#20046;&#38646;&#24320;&#38144;&#65292;&#26377;&#26102;&#29978;&#33267;&#27604;&#19981;&#21463;&#38480;&#21046;&#30340;&#35299;&#30721;&#24555;&#36817;2&#20493;&#65292;&#20174;&#32780;&#36828;&#36828;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06988v1 Announce Type: cross  Abstract: To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding proposes to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$\times$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25552;&#39640;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.06914</link><description>&lt;p&gt;
MEND&#65306;&#20803;&#28436;&#31034;&#33976;&#39311;&#29992;&#20110;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25552;&#39640;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#33021;&#21147;&#65292;&#20854;&#20013;LLM&#20026;&#32473;&#23450;&#30340;&#27979;&#35797;&#36755;&#20837;&#21644;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#23545;(&#28436;&#31034;)&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#28436;&#31034;&#30340;&#21152;&#20837;&#23548;&#33268;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#24320;&#38144;&#21576;&#20108;&#27425;&#22686;&#21152;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#23581;&#35797;&#23558;&#20887;&#38271;&#30340;&#28436;&#31034;&#33976;&#39311;&#25104;&#32039;&#20945;&#30340;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#29306;&#29298;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#23558;&#20219;&#20309;&#20887;&#38271;&#28436;&#31034;&#33976;&#39311;&#20026;&#21521;&#37327;&#65292;&#32780;&#26080;&#38656;&#20026;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#22686;&#24378;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;MEND&#20855;&#26377;&#33976;&#39311;&#28436;&#31034;&#30340;&#20803;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06914v1 Announce Type: cross  Abstract: Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrat
&lt;/p&gt;</description></item><item><title>CLIcK&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#32780;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.06412</link><description>&lt;p&gt;
CLIcK&#65306;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06412
&lt;/p&gt;
&lt;p&gt;
CLIcK&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#32780;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38024;&#23545;&#38889;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#32570;&#20047;&#27979;&#35797;&#24517;&#35201;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#35768;&#22810;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#32763;&#35793;&#20174;&#33521;&#35821;&#23545;&#24212;&#25968;&#25454;&#38598;&#20013;&#34893;&#29983;&#20986;&#26469;&#30340;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#35270;&#19981;&#21516;&#30340;&#25991;&#21270;&#32972;&#26223;&#12290;&#20165;&#26377;&#23569;&#25968;&#20174;&#38889;&#22269;&#25968;&#25454;&#28304;&#25429;&#25417;&#25991;&#21270;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#30340;&#20165;&#26377;&#20559;&#35265;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#29421;&#31364;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CLIcK&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#12290;CLIcK&#23558;&#20854;&#25968;&#25454;&#26469;&#28304;&#20110;&#38889;&#22269;&#23448;&#26041;&#32771;&#35797;&#21644;&#25945;&#31185;&#20070;&#65292;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#65288;&#35821;&#35328;&#21644;&#25991;&#21270;&#65289;&#19979;&#30340;11&#20010;&#31867;&#21035;&#12290;&#23545;&#20110;CLIcK&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#21738;&#20123;&#25991;&#21270;&#21644;&#35821;&#35328;&#30693;&#35782;&#30340;&#32454;&#31890;&#24230;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06412v1 Announce Type: new  Abstract: Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#38899;&#39057;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#32534;&#30721;&#20010;&#20307;&#29305;&#24449;&#21644;&#36890;&#29992;&#27169;&#24335;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#25968;&#25454;&#65292;&#29992;&#20110;&#35821;&#35328;&#20998;&#26512;&#21644;&#20020;&#24202;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.05820</link><description>&lt;p&gt;
&#19968;&#31181;&#38899;&#39057;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#23558;&#35821;&#38899;&#20449;&#21495;&#36716;&#25442;&#20026;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
An Audio-textual Diffusion Model For Converting Speech Signals Into Ultrasound Tongue Imaging Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05820
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#38899;&#39057;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#32534;&#30721;&#20010;&#20307;&#29305;&#24449;&#21644;&#36890;&#29992;&#27169;&#24335;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#25968;&#25454;&#65292;&#29992;&#20110;&#35821;&#35328;&#20998;&#26512;&#21644;&#20020;&#24202;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#21040;&#35821;&#38899;&#36870;&#21464;&#25442;&#65288;AAI&#65289;&#26088;&#22312;&#23558;&#38899;&#39057;&#36716;&#25442;&#20026;&#21457;&#38899;&#22120;&#23448;&#36816;&#21160;&#65292;&#22914;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#65288;UTI&#65289;&#25968;&#25454;&#12290;&#29616;&#26377;AAI&#26041;&#27861;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#20165;&#20351;&#29992;&#20010;&#24615;&#21270;&#38899;&#39057;&#20449;&#24687;&#26469;&#25512;&#23548;&#33292;&#22836;&#36816;&#21160;&#30340;&#19968;&#33324;&#27169;&#24335;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;UTI&#25968;&#25454;&#36136;&#37327;&#26377;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;UTI&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#30340;&#38899;&#39057;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;wav2vec 2.0&#23545;&#20010;&#20307;&#30456;&#20851;&#30340;&#33292;&#22836;&#36816;&#21160;&#32454;&#33410;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#20351;&#29992;BERT&#23545;&#19982;&#33292;&#22836;&#36816;&#21160;&#26222;&#36941;&#24615;&#30456;&#20851;&#30340;ASR&#36716;&#24405;&#36827;&#34892;&#32534;&#30721;&#12290;&#28982;&#21518;&#36890;&#36807;&#25193;&#25955;&#27169;&#22359;&#29983;&#25104;UTI&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#28165;&#26224;&#33292;&#22836;&#36718;&#24275;&#30340;&#39640;&#36136;&#37327;UTI&#25968;&#25454;&#65292;&#36825;&#23545;&#35821;&#35328;&#20998;&#26512;&#21644;&#20020;&#24202;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05820v1 Announce Type: cross  Abstract: Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator movements, such as ultrasound tongue imaging (UTI) data. An issue of existing AAI methods is only using the personalized acoustic information to derive the general patterns of tongue motions, and thus the quality of generated UTI data is limited. To address this issue, this paper proposes an audio-textual diffusion model for the UTI data generation task. In this model, the inherent acoustic characteristics of individuals related to the tongue motion details are encoded by using wav2vec 2.0, while the ASR transcriptions related to the universality of tongue motions are encoded by using BERT. UTI data are then generated by using a diffusion module. Experimental results showed that the proposed diffusion model could generate high-quality UTI data with clear tongue contour that is crucial for the linguistic analysis and clinical assessment. The project can be fou
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05326</link><description>&lt;p&gt;
ChatASU&#65306;&#21796;&#36215;LLM&#30340;&#21453;&#24605;&#65292;&#30495;&#27491;&#29702;&#35299;&#23545;&#35805;&#20013;&#30340;&#26041;&#38754;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#21160;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#38382;&#31572;&#21644;&#23545;&#35805;&#65289;&#20013;&#36827;&#34892;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ASU&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#24573;&#30053;&#20102;&#24847;&#35265;&#30446;&#26631;&#65288;&#21363;&#26041;&#38754;&#65289;&#30340;&#20849;&#25351;&#38382;&#39064;&#65292;&#32780;&#36825;&#31181;&#29616;&#35937;&#22312;&#20114;&#21160;&#22330;&#26223;&#29305;&#21035;&#26159;&#23545;&#35805;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#38480;&#21046;&#20102;ASU&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#23558;&#21508;&#31181;NLP&#20219;&#21153;&#19982;&#32842;&#22825;&#33539;&#24335;&#30456;&#32467;&#21512;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#39033;ChatASU&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#26041;&#38754;&#38142;&#25512;&#29702;&#65288;ACR&#65289;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#33258;&#21453;&#24605;&#26041;&#27861;&#65288;TSA&#65289;&#19982;ChatGLM&#20316;&#20026;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.04964</link><description>&lt;p&gt;
&#21578;&#35785;&#25105;&#23454;&#35805;&#65306;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Tell me the truth: A system to measure the trustworthiness of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#20174;2023&#24180;11&#26376;ChatGPT&#25512;&#20986;&#20197;&#26469;&#65292;&#22312;&#22823;&#22810;&#25968;&#26032;&#38395;&#20013;&#21344;&#25454;&#20102;&#37325;&#35201;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#19968;&#24180;&#22810;&#36807;&#21435;&#20102;&#65292;&#20844;&#21496;&#25269;&#35302;&#37319;&#29992;&#23427;&#20204;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20182;&#20204;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#32570;&#20047;&#20449;&#24515;&#12290;&#19968;&#39033;&#30001;Baymard&#65288;2023&#65289;&#36827;&#34892;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT-4 &#22312;&#35782;&#21035;&#32593;&#31449;&#21487;&#29992;&#24615;&#38382;&#39064;&#26102;&#26377;80.1%&#30340;&#20551;&#38451;&#24615;&#38169;&#35823;&#29575;&#12290;&#32780;&#12298;JAMA&#20799;&#31185;&#23398;&#12299;&#26434;&#24535;&#65288;JAMA Pediatrics&#65289;&#20110;2024&#24180;1&#26376;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT &#22312;&#35786;&#26029;&#20799;&#31185;&#21307;&#30103;&#26696;&#20363;&#26102;&#30340;&#20934;&#30830;&#29575;&#20026;17%&#65288;Barile et al., 2024&#65289;&#12290;&#37027;&#20040;&#65292;&#20309;&#20026;&#8220;&#20449;&#20219;&#8221;&#65311;&#20449;&#20219;&#26159;&#19968;&#20010;&#30456;&#23545;&#30340;&#12289;&#20027;&#35266;&#30340;&#26465;&#20214;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#21270;&#12289;&#39046;&#22495;&#21644;&#20010;&#20307;&#32780;&#21464;&#21270;&#12290;&#37027;&#20040;&#65292;&#22312;&#32473;&#23450;&#19968;&#20010;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#34913;&#37327;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#21602;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#34920;&#31034;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04964v1 Announce Type: new  Abstract: Large Language Models (LLM) have taken the front seat in most of the news since November 2023, when ChatGPT was introduced. After more than one year, one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems. In a study by (Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics found that ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile et al., 2024). But then, what is "trust"? Trust is a relative, subject condition that can change based on culture, domain, individuals. And then, given a domain, how can the trustworthiness of a system be measured? In this paper, I present a systematic approach to measure trustworthiness based on a predefined ground truth, represented as a knowledge graph of the domain. The approach is a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03102</link><description>&lt;p&gt;
&#8220;&#22312;&#23545;&#35805;&#20013;&#23398;&#20064;&#8221;&#65306;&#36890;&#36807;&#23545;&#35805;&#20013;&#23398;&#20064;&#23454;&#29616;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
"In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#33021;&#22815;&#29983;&#25104;&#19982;&#19981;&#21516;&#20154;&#35774;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#20010;&#20154;&#36164;&#26009;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#65292;&#36824;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;In-Dialogue Learning&#65288;IDL&#65289;&#65292;&#19968;&#31181;&#24494;&#35843;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#23545;&#35805;&#21382;&#21490;&#26469;&#21051;&#30011;&#20010;&#20154;&#35774;&#65292;&#20197;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IDL&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;BLEU&#21644;ROUGE&#20998;&#25968;&#20998;&#21035;&#22686;&#21152;&#20102;&#39640;&#36798;200%&#21644;247%&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03102v1 Announce Type: cross  Abstract: Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;TP-BERTa&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#21644;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#29305;&#24449;&#20540;&#19978;&#30340;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.01841</link><description>&lt;p&gt;
&#22312;&#34920;&#26684;&#39044;&#27979;&#19978;&#20248;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Making Pre-trained Language Models Great on Tabular Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01841
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;TP-BERTa&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#21644;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#29305;&#24449;&#20540;&#19978;&#30340;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21487;&#36801;&#31227;&#24615;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#31181;DNN&#30340;&#20248;&#21183;&#22312;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#65288;&#20363;&#22914;&#22238;&#24402;&#25110;&#20998;&#31867;&#20219;&#21153;&#65289;&#19978;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TP-BERTa&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#23558;&#26631;&#37327;&#25968;&#20540;&#29305;&#24449;&#20540;&#36716;&#25442;&#20026;&#31163;&#25955;&#24230;&#39640;&#12289;&#39640;&#32500;&#24230;&#30340;&#26631;&#35760;&#65292;&#24182;&#19988;&#19968;&#31181;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#25972;&#21512;&#20102;&#29305;&#24449;&#21517;&#31216;&#21644;&#25968;&#20540;&#29305;&#24449;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01841v1 Announce Type: new  Abstract: The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM model for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates fe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01548</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#34920;&#24449;&#30340;&#19978;&#19979;&#25991;&#38160;&#24230;&#20316;&#20026;&#35686;&#25253;&#65306;&#20943;&#23569;&#24187;&#35273;&#30340;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#23427;&#20204;&#20026;&#20160;&#20040;&#20250;&#29359;&#36825;&#20123;&#38169;&#35823;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20869;&#37096;&#34920;&#24449;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;LLM&#24187;&#35273;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#19982;&#24187;&#35273;&#30456;&#20851;&#30340;&#19968;&#20010;&#31361;&#20986;&#27169;&#24335;&#65306;&#27491;&#30830;&#30340;&#29983;&#25104;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#65292;&#32780;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#21017;&#27809;&#26377;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#19978;&#19979;&#25991;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#38160;&#24230;&#8221;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#20197;&#21046;&#23450;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#65292;&#22312;TruthfulQA&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;8.6&#28857;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24187;&#35273;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
&lt;/p&gt;</description></item><item><title>WanJuan-CC&#26159;&#19968;&#20010;&#23433;&#20840;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;Common Crawl&#25968;&#25454;&#24182;&#32463;&#36807;&#22810;&#39033;&#31579;&#36873;&#21644;&#36807;&#28388;&#27493;&#39588;&#24471;&#21040;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.19282</link><description>&lt;p&gt;
WanJuan-CC&#65306;&#19968;&#20010;&#23433;&#20840;&#19988;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19282
&lt;/p&gt;
&lt;p&gt;
WanJuan-CC&#26159;&#19968;&#20010;&#23433;&#20840;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;Common Crawl&#25968;&#25454;&#24182;&#32463;&#36807;&#22810;&#39033;&#31579;&#36873;&#21644;&#36807;&#28388;&#27493;&#39588;&#24471;&#21040;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; WanJuan-CC&#65292;&#36825;&#26159;&#19968;&#20010;&#23433;&#20840;&#19988;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#26469;&#28304;&#20110;Common Crawl&#25968;&#25454;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#20026;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27969;&#31243;&#26469;&#22788;&#29702;Common Crawl&#25968;&#25454;&#65292;&#21253;&#25324;&#25552;&#21462;&#12289;&#21551;&#21457;&#24335;&#35268;&#21017;&#36807;&#28388;&#12289;&#27169;&#31946;&#21435;&#37325;&#12289;&#20869;&#23481;&#23433;&#20840;&#36807;&#28388;&#21644;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#12290;&#20174;&#22823;&#32422;680&#20159;&#20010;&#21407;&#22987;&#33521;&#25991;&#25991;&#26723;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;22&#19975;&#20159;&#26631;&#35760;&#30340;&#23433;&#20840;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#36873;&#20986;&#20102;10&#19975;&#20159;&#26631;&#35760;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20316;&#20026;WanJuan-CC&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#24050;&#32463;&#24320;&#28304;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;3000&#20159;&#26631;&#35760;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19982;&#25968;&#25454;&#36136;&#37327;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#36866;&#24403;&#30340;&#25968;&#25454;&#12290;&#20026;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;WanJuan-CC&#35757;&#32451;&#20102;10&#20159;&#21442;&#25968;&#21644;30&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19282v1 Announce Type: new  Abstract: This paper presents WanJuan-CC, a safe and high-quality open-sourced English webtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which require vast amounts of high-quality data. A comprehensive process was designed to handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data according to their needs. To evaluate the quality and utility of the dataset, we trained 1B-parameter and 3B-parameter models using WanJuan-CC and ano
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>Finer&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#19978;&#30340;&#30701;&#26495;&#65292;&#23588;&#20854;&#26159;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#32454;&#33268;&#23646;&#24615;&#35299;&#37322;&#65292;&#23613;&#31649;&#20855;&#26377;&#29983;&#25104;&#39640;&#27700;&#24179;&#22270;&#20687;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16315</link><description>&lt;p&gt;
Finer: &#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30740;&#31350;&#21644;&#22686;&#24378;&#32454;&#31890;&#24230;&#35270;&#35273;&#27010;&#24565;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16315
&lt;/p&gt;
&lt;p&gt;
Finer&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#19978;&#30340;&#30701;&#26495;&#65292;&#23588;&#20854;&#26159;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#32454;&#33268;&#23646;&#24615;&#35299;&#37322;&#65292;&#23613;&#31649;&#20855;&#26377;&#29983;&#25104;&#39640;&#27700;&#24179;&#22270;&#20687;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#27169;&#22411;&#33021;&#22815;&#36731;&#26494;&#29983;&#25104;&#39640;&#27700;&#24179;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#31181;&#33021;&#21147;&#20027;&#35201;&#24402;&#22240;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20845;&#20010;&#19981;&#21516;&#22522;&#20934;&#35774;&#32622;&#19979;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#65288;FGVC&#65289;&#19978;&#30340;&#32570;&#38519;&#12290;&#26368;&#36817;&#30340;LVLMs&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22914;LLaVa-1.5&#65292;InstructBLIP&#21644;GPT-4V&#65292;&#22312;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#20005;&#37325;&#19979;&#38477;&#65292;&#20363;&#22914;&#65292;LLaVA-1.5&#22312;&#26031;&#22374;&#31119;&#29399;&#30340;EM&#24179;&#22343;&#19979;&#38477;&#20102;65.58&#65292;&#32780;&#19988;&#36824;&#38590;&#20197;&#26681;&#25454;&#20986;&#29616;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#27010;&#24565;&#29983;&#25104;&#20855;&#26377;&#35814;&#32454;&#23646;&#24615;&#30340;&#20934;&#30830;&#35299;&#37322;&#65292;&#23613;&#31649;&#23427;&#20204;&#26377;&#29983;&#25104;&#25972;&#20307;&#22270;&#20687;&#32423;&#25551;&#36848;&#30340;&#33021;&#21147;&#12290;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;LVLMs&#22312;&#32473;&#23450;&#25991;&#26412;&#26102;&#21576;&#29616;&#20986;&#27169;&#24577;&#24046;&#36317;&#65292;&#26174;&#31034;&#20986;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16315v1 Announce Type: cross  Abstract: Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given tex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#25991;&#26412;&#21040;SQL&#39046;&#22495;&#20013;&#30340;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#22823;&#37327;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12243</link><description>&lt;p&gt;
&#29702;&#35299;&#25991;&#26412;&#21040;SQL&#20013;&#22122;&#22768;&#30340;&#24433;&#21709;&#65306;&#23545;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12243
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#25991;&#26412;&#21040;SQL&#39046;&#22495;&#20013;&#30340;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#22823;&#37327;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Text-to-SQL&#28041;&#21450;&#23558;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#20026;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#65292;&#23545;&#20110;&#20351;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21487;&#20197;&#22312;&#27809;&#26377;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#24191;&#27867;&#35775;&#38382;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21407;&#22240;&#21253;&#25324;&#23384;&#22312;&#8220;&#22122;&#22768;&#8221;&#65292;&#22914;&#27169;&#31946;&#38382;&#39064;&#21644;&#35821;&#27861;&#38169;&#35823;&#12290;&#35813;&#30740;&#31350;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#22122;&#22768;&#30340;&#20998;&#24067;&#21644;&#31867;&#22411;&#20197;&#21450;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#34429;&#28982;BIRD-Bench&#26088;&#22312;&#27169;&#25311;&#33039;&#20081;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24211;&#20540;&#65292;&#20294;&#24182;&#26410;&#21253;&#21547;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#21644;&#38169;&#35823;&#12290;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#26222;&#36941;&#23384;&#22312;&#65292;&#36328;&#39046;&#22495;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#22122;&#22768;&#65292;&#24182;&#19988;&#22122;&#22768;&#31867;&#22411;&#20043;&#38388;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#23384;&#22312;&#19981;&#27491;&#30830;&#30340;&#26631;&#20934;SQL&#26597;&#35810;&#65292;&#36827;&#32780;&#29983;&#25104;&#19981;&#27491;&#30830;&#30340;&#26631;&#20934;&#31572;&#26696;&#65292;&#23545;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12243v1 Announce Type: new  Abstract: Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of 'noise,' such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the ben
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20581;&#24247;-LLM&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#26816;&#32034;&#22686;&#24378;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20581;&#24247;&#25253;&#21578;&#65292;&#35843;&#25972;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#21450;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#23478;&#35265;&#35299;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.00746</link><description>&lt;p&gt;
&#20581;&#24247;-LLM&#65306;&#20010;&#24615;&#21270;&#26816;&#32034;&#22686;&#24378;&#30340;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20581;&#24247;-LLM&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#26816;&#32034;&#22686;&#24378;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20581;&#24247;&#25253;&#21578;&#65292;&#35843;&#25972;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#21450;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#23478;&#35265;&#35299;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21355;&#29983;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26234;&#33021;&#21307;&#30103;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26234;&#33021;&#21307;&#30103;&#21463;&#38480;&#20110;&#38745;&#24577;&#25968;&#25454;&#21644;&#32479;&#19968;&#26631;&#20934;&#65292;&#26080;&#27861;&#23436;&#20840;&#19982;&#20010;&#20307;&#24773;&#20917;&#38598;&#25104;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#20854;&#20182;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#20581;&#24247;-LLM&#65292;&#23558;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#30456;&#32467;&#21512;&#12290;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20581;&#24247;&#25253;&#21578;&#25972;&#21512;&#21040;&#22823;&#27169;&#22411;&#20013;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#20219;&#21153;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#19994;&#30340;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#35843;&#25972;&#20581;&#24247;&#29305;&#24449;&#30340;&#26435;&#37325;&#24471;&#20998;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#21322;&#33258;&#21160;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#24182;&#25972;&#21512;&#19987;&#23478;&#35265;&#35299;&#20197;&#25552;&#39640;&#30142;&#30149;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease predic
&lt;/p&gt;</description></item><item><title>&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#21487;&#20197;&#20316;&#20026;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23384;&#22312;&#23545;&#27969;&#34892;&#24086;&#23376;&#20559;&#35265;&#36739;&#39640;&#12289;&#24773;&#24863;&#26356;&#31215;&#26497;&#20197;&#21450;&#24573;&#35270;&#25919;&#27835;&#12289;&#33394;&#24773;&#21644;&#31895;&#20439;&#24086;&#23376;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.15479</link><description>&lt;p&gt;
&#24212;&#23545;&#21518;API&#22256;&#22659;&#65306;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#21576;&#29616;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#20559;&#35265;&#35266;
&lt;/p&gt;
&lt;p&gt;
Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15479
&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#21487;&#20197;&#20316;&#20026;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23384;&#22312;&#23545;&#27969;&#34892;&#24086;&#23376;&#20559;&#35265;&#36739;&#39640;&#12289;&#24773;&#24863;&#26356;&#31215;&#26497;&#20197;&#21450;&#24573;&#35270;&#25919;&#27835;&#12289;&#33394;&#24773;&#21644;&#31895;&#20439;&#24086;&#23376;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20572;&#27490;&#35775;&#38382;&#31038;&#20132;&#23186;&#20307;API&#30340;&#20915;&#23450;&#23545;&#20114;&#32852;&#32593;&#30740;&#31350;&#21644;&#25972;&#20010;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#20135;&#29983;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#36825;&#31181;&#23545;&#25968;&#25454;&#30340;&#35775;&#38382;&#32570;&#20047;&#24050;&#34987;&#31216;&#20026;&#20114;&#32852;&#32593;&#30740;&#31350;&#30340;&#21518;API&#26102;&#20195;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#27969;&#34892;&#30340;&#25628;&#32034;&#24341;&#25806;&#26377;&#33021;&#21147;&#29228;&#21462;&#12289;&#25429;&#33719;&#21644;&#23637;&#31034;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#22312;&#20854;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;(SERP)&#19978;&#65292;&#22914;&#26524;&#25552;&#20379;&#36866;&#24403;&#30340;&#25628;&#32034;&#26597;&#35810;&#65292;&#21487;&#33021;&#20250;&#20026;&#36825;&#19968;&#22256;&#22659;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#24403;&#21069;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;SERP&#26159;&#21542;&#25552;&#20379;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#23436;&#25972;&#21644;&#26080;&#20559;&#35265;&#26679;&#26412;&#65311; SERP&#26159;&#21542;&#26159;&#30452;&#25509;API&#35775;&#38382;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#65288;Google&#65289;SERP&#32467;&#26524;&#21644;&#26469;&#33258;Reddit&#21644;Twitter/X&#30340;&#38750;&#21462;&#26679;&#25968;&#25454;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SERP&#32467;&#26524;&#22312;&#25903;&#25345;&#27969;&#34892;&#24086;&#23376;&#26041;&#38754;&#23384;&#22312;&#39640;&#24230;&#20559;&#35265;&#65307;&#21453;&#23545;&#25919;&#27835;&#12289;&#33394;&#24773;&#21644;&#31895;&#20439;&#24086;&#23376;&#65307;&#22312;&#24773;&#24863;&#19978;&#26356;&#20026;&#31215;&#26497;&#65307;&#24182;&#26377;&#22823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15479v2 Announce Type: replace-cross  Abstract: Recent decisions to discontinue access to social media APIs are having detrimental effects on Internet research and the field of computational social science as a whole. This lack of access to data has been dubbed the Post-API era of Internet research. Fortunately, popular search engines have the means to crawl, capture, and surface social media data on their Search Engine Results Pages (SERP) if provided the proper search query, and may provide a solution to this dilemma. In the present work we ask: does SERP provide a complete and unbiased sample of social media data? Is SERP a viable alternative to direct API-access? To answer these questions, we perform a comparative analysis between (Google) SERP results and nonsampled data from Reddit and Twitter/X. We find that SERP results are highly biased in favor of popular posts; against political, pornographic, and vulgar posts; are more positive in their sentiment; and have large 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31526;&#21495;&#36716;&#25442;&#20026;&#35821;&#35328;&#65288;S2L&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#36716;&#25442;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#20351;&#24471;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#31526;&#21495;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.11725</link><description>&lt;p&gt;
&#36890;&#36807;&#31526;&#21495;&#36716;&#25442;&#20026;&#35821;&#35328;&#35299;&#20915;&#31526;&#21495;&#30456;&#20851;&#38382;&#39064;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language Conversion for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11725
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31526;&#21495;&#36716;&#25442;&#20026;&#35821;&#35328;&#65288;S2L&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#36716;&#25442;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#20351;&#24471;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#31526;&#21495;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#65288;&#25110;&#26356;&#24191;&#20041;&#30340;&#65292;&#38750;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#34920;&#31034;&#65289;&#22914;&#25968;&#23383;&#24207;&#21015;&#12289;&#20998;&#23376;&#24335;&#20197;&#21450;&#34920;&#26684;&#36793;&#30028;&#31561;&#24191;&#27867;&#23384;&#22312;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#27604;&#22914;&#25277;&#35937;&#25512;&#29702;&#12289;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#20197;&#21450;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#12290;&#23613;&#31649;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#31526;&#21495;&#26041;&#38754;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#65292;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;&#31526;&#21495;&#34920;&#31034;&#19982;&#19968;&#33324;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#31526;&#21495;&#36716;&#25442;&#20026;&#35821;&#35328;&#65288;S2L&#65289;&#26041;&#27861;&#65292;&#20351;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#31526;&#21495;&#30456;&#20851;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;S2L&#39318;&#20808;&#23558;&#28041;&#21450;&#30340;&#31526;&#21495;&#36716;&#25442;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;LLMs&#25110;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#23454;&#29616;&#65292;&#28982;&#21518;&#36825;&#20123;&#22522;&#20110;&#35821;&#35328;&#30340;&#34920;&#31034;&#34987;&#25972;&#21512;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11725v2 Announce Type: replace  Abstract: Symbols (or more broadly, non-natural language textual representations) such as numerical sequences, molecular formulas, and table delimiters widely exist, playing important roles in various tasks such as abstract reasoning, chemical property prediction, and table question answering. Despite the impressive natural language comprehension capabilities of large language models (LLMs), their reasoning abilities for symbols remain inadequate, which could attributed to the difference between symbol representations and general natural languages. We propose symbol-to-language (S2L), a tuning-free method that enables large language models to solve symbol-related problems with information expressed in natural language. Specifically, S2L first converts the symbols involved to language-based representations, which can be implemented by prompting LLMs or leveraging external tools, then these language-based representations are integrated into the 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.16480</link><description>&lt;p&gt;
&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#29992;&#20110;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16480
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#20999;&#29255;&#22270;&#20687;&#26159;&#29992;&#20110;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#22522;&#30784;&#12290;&#25776;&#20889;&#30149;&#29702;&#25253;&#21578;&#23545;&#32463;&#39564;&#19981;&#36275;&#30340;&#30149;&#29702;&#23398;&#23478;&#26469;&#35828;&#26159;&#36153;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#24037;&#20316;&#37327;&#24182;&#25913;&#21892;&#20020;&#24202;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#32473;&#23450;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#22312;&#25968;&#25454;&#31471;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#26368;&#22823;&#30340;WSI-&#25991;&#26412;&#25968;&#25454;&#38598;&#65288;TCGA-PathoText&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#21644;&#28165;&#29702;TCGA&#20013;&#21465;&#36848;&#35786;&#26029;&#24187;&#28783;&#29255;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#25910;&#38598;&#20102;&#36817;1&#19975;&#23545;&#39640;&#36136;&#37327;&#30340;WSI-&#25991;&#26412;&#37197;&#23545;&#65292;&#20379;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#27169;&#22411;&#31471;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#20026;&#21315;&#20159;&#20687;&#32032;WSI&#29983;&#25104;&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#65288;MI-Gen&#65289;&#12290;&#25105;&#20204;&#22312;TCGA-PathoText&#30340;&#26368;&#22823;&#23376;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#27492;&#22806;&#65292;WSI-&#25991;&#26412;&#39044;&#27979;&#21487;&#34987;&#35270;&#20026;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16480v2 Announce Type: replace-cross  Abstract: Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#21475;&#30340;&#26032;&#26041;&#27861;&#26469;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;&#65292;&#21457;&#29616;LLM&#20195;&#29702;&#23384;&#22312;&#22266;&#26377;&#20559;&#35265;&#23548;&#33268;&#27169;&#25311;&#20195;&#29702;&#36235;&#21521;&#20110;&#31185;&#23398;&#29616;&#23454;&#19968;&#33268;&#30340;&#20849;&#35782;&#65292;&#20294;&#24341;&#20837;&#30830;&#35748;&#20559;&#35265;&#21518;&#35266;&#23519;&#21040;&#24847;&#35265;&#20998;&#35010;&#65292;&#31361;&#26174;&#20102;LLM&#20195;&#29702;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09618</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32593;&#32476;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Simulating Opinion Dynamics with Networks of LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#21475;&#30340;&#26032;&#26041;&#27861;&#26469;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;&#65292;&#21457;&#29616;LLM&#20195;&#29702;&#23384;&#22312;&#22266;&#26377;&#20559;&#35265;&#23548;&#33268;&#27169;&#25311;&#20195;&#29702;&#36235;&#21521;&#20110;&#31185;&#23398;&#29616;&#23454;&#19968;&#33268;&#30340;&#20849;&#35782;&#65292;&#20294;&#24341;&#20837;&#30830;&#35748;&#20559;&#35265;&#21518;&#35266;&#23519;&#21040;&#24847;&#35265;&#20998;&#35010;&#65292;&#31361;&#26174;&#20102;LLM&#20195;&#29702;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#27169;&#25311;&#20154;&#31867;&#24847;&#35265;&#21160;&#24577;&#23545;&#20110;&#29702;&#35299;&#21508;&#31181;&#31038;&#20250;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#26497;&#21270;&#21644;&#38169;&#35823;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#20110;&#27492;&#31867;&#27169;&#25311;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#65288;ABM&#65289;&#32463;&#24120;&#20250;&#36807;&#20998;&#31616;&#21270;&#20154;&#31867;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#21475;&#30340;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#20195;&#29702;&#23384;&#22312;&#19968;&#31181;&#23545;&#20135;&#29983;&#20934;&#30830;&#20449;&#24687;&#30340;&#24378;&#28872;&#22266;&#26377;&#20559;&#35265;&#65292;&#23548;&#33268;&#27169;&#25311;&#20195;&#29702;&#36235;&#21521;&#20110;&#19982;&#31185;&#23398;&#29616;&#23454;&#19968;&#33268;&#30340;&#20849;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20559;&#35265;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#31561;&#38382;&#39064;&#19978;&#25269;&#21046;&#20849;&#35782;&#35266;&#28857;&#30340;&#25928;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#35825;&#23548;&#30830;&#35748;&#20559;&#35265;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19982;&#29616;&#26377;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#21644;&#24847;&#35265;&#21160;&#24577;&#30740;&#31350;&#19968;&#33268;&#30340;&#24847;&#35265;&#20998;&#35010;&#12290;&#36825;&#20123;&#35265;&#35299;&#31361;&#26174;&#20102;LLM&#20195;&#29702;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#26465;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09618v2 Announce Type: replace-cross  Abstract: Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path
&lt;/p&gt;</description></item><item><title>BizBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#29702;&#36130;&#21153;&#38382;&#39064;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#20843;&#20010;&#37327;&#21270;&#25512;&#29702;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#36890;&#36807;&#31243;&#24207;&#21512;&#25104;&#23545;&#36130;&#21153;&#25968;&#25454;&#36827;&#34892;&#38382;&#31572;&#12290;</title><link>https://arxiv.org/abs/2311.06602</link><description>&lt;p&gt;
BizBench: &#19968;&#20010;&#29992;&#20110;&#21830;&#19994;&#21644;&#37329;&#34701;&#30340;&#37327;&#21270;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BizBench: A Quantitative Reasoning Benchmark for Business and Finance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06602
&lt;/p&gt;
&lt;p&gt;
BizBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#29702;&#36130;&#21153;&#38382;&#39064;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#20843;&#20010;&#37327;&#21270;&#25512;&#29702;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#36890;&#36807;&#31243;&#24207;&#21512;&#25104;&#23545;&#36130;&#21153;&#25968;&#25454;&#36827;&#34892;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#20851;&#20110;&#21830;&#19994;&#21644;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#39064;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#12289;&#20934;&#30830;&#24615;&#20197;&#21450;&#24191;&#27867;&#30340;&#25216;&#26415;&#30693;&#35782;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BizBench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#29702;&#29616;&#23454;&#36130;&#21153;&#38382;&#39064;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;BizBench&#21253;&#25324;&#20843;&#20010;&#37327;&#21270;&#25512;&#29702;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#36890;&#36807;&#31243;&#24207;&#21512;&#25104;&#23545;&#36130;&#21153;&#25968;&#25454;&#36827;&#34892;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#25105;&#20204;&#21253;&#21547;&#20102;&#19977;&#20010;&#22522;&#20110;&#37329;&#34701;&#20027;&#39064;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#20174;&#26032;&#25910;&#38598;&#21644;&#25193;&#20805;&#30340;QA&#25968;&#25454;&#20013;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#31163;&#20986;&#20102;&#37329;&#34701;QA&#25152;&#38656;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#38405;&#35835;&#36130;&#21153;&#25991;&#26412;&#21644;&#34920;&#26684;&#20197;&#25552;&#21462;&#20013;&#38388;&#20540;&#65292;&#29702;&#35299;&#35745;&#31639;&#22797;&#26434;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#37329;&#34701;&#27010;&#24565;&#21644;&#20844;&#24335;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#20219;&#21153;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#37329;&#34701;&#32972;&#26223;&#30693;&#35782;&#12289;&#35299;&#26512;&#36130;&#21153;&#25991;&#20214;&#30340;&#33021;&#21147;&#20197;&#21450;&#22788;&#29702;&#22797;&#26434;&#35299;&#20915;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06602v2 Announce Type: replace  Abstract: Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models' ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model's financial background knowledge, ability to parse financial documents, and capacity to s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Octavius&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;MoE&#21644;LoRA&#25216;&#26415;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35299;&#30721;&#22120;LoRA-MoE&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;2D&#21644;3D&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#32422;20%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.02684</link><description>&lt;p&gt;
Octavius&#65306;&#36890;&#36807;MoE&#20943;&#36731;MLLM&#20013;&#30340;&#20219;&#21153;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Octavius: Mitigating Task Interference in MLLMs via MoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02684
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Octavius&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;MoE&#21644;LoRA&#25216;&#26415;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35299;&#30721;&#22120;LoRA-MoE&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;2D&#21644;3D&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#32422;20%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#23558;&#23427;&#20204;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;&#38543;&#30528;&#24341;&#20837;&#26356;&#22810;&#30340;&#24418;&#24335;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#36127;&#38754;&#20914;&#31361;&#21644;&#24178;&#25200;&#21487;&#33021;&#23545;&#24615;&#33021;&#20135;&#29983;&#26356;&#20005;&#37325;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#36825;&#31181;&#29616;&#35937;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#34987;&#24573;&#35270;&#20102;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\mname &#30340;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#19982;Multimodal Large Language Models&#65288;MLLMs&#65289;&#19968;&#36215;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#20840;&#38754;&#30740;&#31350;&#21644;&#23454;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#21644;&#20195;&#34920;&#24615;PEFT&#25216;&#26415;&#20043;&#19968;&#65292;&#21363;LoRA&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#35299;&#30721;&#22120;&#65292;&#31216;&#20026;LoRA-MoE&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#65288;&#32422;20\%&#30340;&#25913;&#36827;&#65289;&#34920;&#26126;&#20102;&#25105;&#20204;&#35774;&#35745;&#22312;&#21508;&#31181;2D&#21644;3D&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#20195;&#30721;&#21644;&#30456;&#24212;&#25968;&#25454;&#38598;&#23558;&#24456;&#24555;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02684v1 Announce Type: cross  Abstract: Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called \mname, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, \emph{i.e.,} LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. The experimental results (about 20\% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and corresponding dataset will be available soon.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.02129</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Pitfalls of Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#24320;&#21457;&#32534;&#36753;LLMs&#20869;&#22312;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#19968;&#20010;&#38452;&#20113;&#24748;&#22312;&#22836;&#39030;&#19978; - &#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#35302;&#21457;&#34676;&#34678;&#25928;&#24212;&#65311;&#22240;&#20026;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#24341;&#20837;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#19982;LLMs&#30693;&#35782;&#32534;&#36753;&#30456;&#20851;&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#30693;&#35782;&#20914;&#31361;&#65306;&#32534;&#36753;&#36923;&#36753;&#20914;&#31361;&#30340;&#20107;&#23454;&#32452;&#21487;&#33021;&#20250;&#25918;&#22823;LLMs&#22266;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615; - &#36825;&#26159;&#20197;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#65288;2&#65289;&#30693;&#35782;&#25197;&#26354;&#65306;&#20026;&#20102;&#32534;&#36753;&#20107;&#23454;&#30693;&#35782;&#32780;&#26356;&#25913;&#21442;&#25968;&#21487;&#33021;&#20250;&#19981;&#21487;&#36870;&#22320;&#25197;&#26354;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2309.00770</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#19982;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bias and Fairness in Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#20154;&#20204;&#33021;&#22815;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#65292;&#36880;&#28176;&#34701;&#20837;&#35302;&#21450;&#25105;&#20204;&#31038;&#20132;&#39046;&#22495;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#12289;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#25972;&#21512;&#12289;&#24418;&#24335;&#21270;&#21644;&#25193;&#23637;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#20260;&#23475;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#20960;&#20010;&#23454;&#29616;LLMs&#20844;&#24179;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19977;&#20010;&#30452;&#35266;&#30340;&#20998;&#31867;&#20307;&#31995;&#32479;&#19968;&#20102;&#25991;&#29486;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#20559;&#35265;&#35780;&#20272;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21363;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#32531;&#35299;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2306.08543</link><description>&lt;p&gt;
MiniLLM&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MiniLLM: Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;KD&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#30333;&#30418;&#20998;&#31867;&#27169;&#22411;&#25110;&#35757;&#32451;&#23567;&#27169;&#22411;&#26469;&#27169;&#20223;&#22914;ChatGPT&#20043;&#31867;&#30340;&#40657;&#30418;&#27169;&#22411;API&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#30333;&#30418;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#23567;&#27169;&#22411;&#20013;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#38543;&#30528;&#24320;&#28304;LLMs&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#36825;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;KD&#26041;&#27861;&#65292;&#23558;LLMs&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08543v2 Announce Type: replace-cross  Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named Mi
&lt;/p&gt;</description></item><item><title>APOLLO&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#23383;&#24863;&#30693;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#21644;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#38271;&#31687;&#25968;&#23383;&#25512;&#29702;&#26694;&#26550;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2212.07249</link><description>&lt;p&gt;
APOLLO: &#19968;&#31181;&#29992;&#20110;&#38271;&#31687;&#25968;&#23383;&#25512;&#29702;&#30340;&#20248;&#21270;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.07249
&lt;/p&gt;
&lt;p&gt;
APOLLO&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#23383;&#24863;&#30693;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#21644;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#38271;&#31687;&#25968;&#23383;&#25512;&#29702;&#26694;&#26550;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36130;&#21153;&#20998;&#26512;&#20013;&#38271;&#31687;&#25968;&#23383;&#25512;&#29702;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#25512;&#29702;&#31243;&#24207;&#20197;&#35745;&#31639;&#32473;&#23450;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;APOLLO&#26469;&#25913;&#21892;&#38271;&#31687;&#25968;&#23383;&#25512;&#29702;&#26694;&#26550;&#65292;&#38024;&#23545;&#30456;&#20851;&#24615;&#36873;&#25321;&#22120;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25968;&#23383;&#24863;&#30693;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#21152;&#36776;&#21035;&#20851;&#38190;&#30340;&#25968;&#23383;&#20107;&#23454;&#12290;&#32780;&#23545;&#20110;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#30446;&#26631;&#31243;&#24207;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.07249v3 Announce Type: replace  Abstract: Long-form numerical reasoning in financial analysis aims to generate a reasoning program to calculate the correct answer for a given question. Previous work followed a retriever-generator framework, where the retriever selects key facts from a long-form document, and the generator generates a reasoning program based on retrieved facts. However, they treated all facts equally without considering the different contributions of facts with and without numbers. Meanwhile, the program consistency were ignored under supervised training, resulting in lower training accuracy and diversity. To solve these problems, we proposed APOLLO to improve the long-form numerical reasoning framework. For the retriever, we adopt a number-aware negative sampling strategy to enable the retriever to be more discriminative on key numerical facts. For the generator, we design consistency-based reinforcement learning and target program augmentation strategy base
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#30340;Consprompt&#32467;&#21512;&#20102;&#25552;&#31034;&#32534;&#30721;&#32593;&#32476;&#12289;&#23545;&#27604;&#37319;&#26679;&#27169;&#22359;&#21644;&#23545;&#27604;&#35780;&#20998;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#24046;&#24322;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#19981;&#21516;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#23454;&#20102;&#21033;&#29992;&#22810;&#32423;&#23545;&#27604;&#23398;&#20064;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2211.04118</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#26679;&#26412;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.04118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#30340;Consprompt&#32467;&#21512;&#20102;&#25552;&#31034;&#32534;&#30721;&#32593;&#32476;&#12289;&#23545;&#27604;&#37319;&#26679;&#27169;&#22359;&#21644;&#23545;&#27604;&#35780;&#20998;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#24046;&#24322;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#19981;&#21516;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#23454;&#20102;&#21033;&#29992;&#22810;&#32423;&#23545;&#27604;&#23398;&#20064;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24050;&#25104;&#20026;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#35821;&#35328;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#65292;&#25552;&#31034;&#35774;&#35745;&#20013;&#30340;&#32454;&#24494;&#21464;&#21270;&#24635;&#26159;&#23548;&#33268;&#32467;&#26524;&#24046;&#24322;&#24456;&#22823;&#65292;&#24182;&#19988;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#20063;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#26377;&#38480;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#21512;&#36866;&#30340;&#23545;&#27604;&#26679;&#26412;&#21644;&#22810;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#25552;&#31034;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#20102;&#25552;&#20986;&#30340;Consprompt&#19982;&#25552;&#31034;&#32534;&#30721;&#32593;&#32476;&#12289;&#23545;&#27604;&#37319;&#26679;&#27169;&#22359;&#21644;&#23545;&#27604;&#35780;&#20998;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#24046;&#24322;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#19981;&#21516;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#28040;&#34701;&#23454;&#39564;&#20063;&#35777;&#26126;&#20102;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#21033;&#29992;&#22810;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.04118v3 Announce Type: replace-cross  Abstract: The prompt has become an effective linguistic tool for utilizing pre-trained language models. However, in few-shot scenarios, subtle changes in the prompt design always make the result widely different, and the prompt learning methods also make it easy to overfit the limited samples. To alleviate this, we explore utilizing suitable contrastive samples and multi-degree contrastive learning methods to improve the robustness of the prompt representation. Therefore, the proposed Consprompt combined with the prompt encoding network, contrastive sampling modules, and contrastive scoring modules, is introduced to realize differential contrastive learning. Our results exhibit state-of-the-art performance in different few-shot settings, and the ablation experiments also certify the effectiveness of utilizing multi-degree contrastive learning in the prompt-based fine-tuning process.
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#30456;&#20851;&#38169;&#35823;&#65292;&#20197;&#23391;&#21152;&#25289;&#35821;&#20026;&#20363;&#65292;&#35752;&#35770;&#20102;&#24615;&#21035;&#30340;&#20551;&#35774;&#21644;&#25512;&#26029;&#65292;&#20197;&#21450;&#36825;&#20123;&#38169;&#35823;&#23548;&#33268;&#30340;&#21518;&#27542;&#27665;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#25552;&#21319;&#35821;&#35328;&#22320;&#20301;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.13165</link><description>&lt;p&gt;
&#22312;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;&#26426;&#22120;&#32763;&#35793;&#20013;&#31216;&#21628;&#38169;&#35823;&#21644;&#24615;&#21035;&#20551;&#35774;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Misgendering and Assuming Gender in Machine Translation when Working with Low-Resource Languages. (arXiv:2401.13165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#30456;&#20851;&#38169;&#35823;&#65292;&#20197;&#23391;&#21152;&#25289;&#35821;&#20026;&#20363;&#65292;&#35752;&#35770;&#20102;&#24615;&#21035;&#30340;&#20551;&#35774;&#21644;&#25512;&#26029;&#65292;&#20197;&#21450;&#36825;&#20123;&#38169;&#35823;&#23548;&#33268;&#30340;&#21518;&#27542;&#27665;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#25552;&#21319;&#35821;&#35328;&#22320;&#20301;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#35770;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#30456;&#20851;&#38169;&#35823;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#37322;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#31181;&#35821;&#35328;&#23618;&#32423;&#30340;&#31038;&#20250;&#21644;&#35745;&#31639;&#22240;&#32032;&#30340;&#19981;&#21487;&#20998;&#21106;&#24615;&#12290;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#27597;&#35821;&#23391;&#21152;&#25289;&#35821;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#28304;&#25991;&#26412;&#20013;&#27809;&#26377;&#25552;&#20379;&#30456;&#24212;&#20449;&#24687;&#26102;&#65292;&#24615;&#21035;&#22914;&#20309;&#34987;&#20551;&#35774;&#21644;&#25512;&#26029;&#20986;&#26469;&#65292;&#24182;&#20197;&#39640;&#36164;&#28304;&#33521;&#35821;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#38169;&#35823;&#23548;&#33268;&#35821;&#35328;&#28040;&#22833;&#21644;&#34920;&#24449;&#20260;&#23475;&#30340;&#21518;&#27542;&#27665;&#21644;&#31038;&#20250;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;&#25552;&#21319;&#35821;&#35328;&#22320;&#20301;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#36171;&#20104;&#35821;&#35328;&#26356;&#22810;&#30340;&#26435;&#23041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter focuses on gender-related errors in machine translation (MT) in the context of low-resource languages. We begin by explaining what low-resource languages are, examining the inseparable social and computational factors that create such linguistic hierarchies. We demonstrate through a case study of our mother tongue Bengali, a global language spoken by almost 300 million people but still classified as low-resource, how gender is assumed and inferred in translations to and from the high(est)-resource English when no such information is provided in source texts. We discuss the postcolonial and societal impacts of such errors leading to linguistic erasure and representational harms, and conclude by discussing potential solutions towards uplifting languages by providing them more agency in MT conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DistilWhisper&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#36827;&#34892;&#36731;&#37327;&#32423;&#27169;&#22359;&#21270;ASR&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#25104;&#21151;&#24357;&#21512;&#20102;&#22810;&#20219;&#21153;&#35821;&#38899;&#27169;&#22411;&#22312;&#23569;&#25968;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2311.01070</link><description>&lt;p&gt;
DistilWhisper&#65306;&#36890;&#36807;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#39640;&#25928;&#21387;&#32553;&#22810;&#20219;&#21153;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts. (arXiv:2311.01070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DistilWhisper&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#36827;&#34892;&#36731;&#37327;&#32423;&#27169;&#22359;&#21270;ASR&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#25104;&#21151;&#24357;&#21512;&#20102;&#22810;&#20219;&#21153;&#35821;&#38899;&#27169;&#22411;&#22312;&#23569;&#25968;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whisper&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#28085;&#30422;99&#31181;&#35821;&#35328;&#12290;&#23427;&#22312;&#20854;&#28085;&#30422;&#30340;&#37096;&#20998;&#35821;&#35328;&#20013;&#33719;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#32467;&#26524;&#65292;&#20294;&#22312;&#19968;&#20123;&#25968;&#37327;&#21487;&#35266;&#30340;&#23569;&#25968;&#35821;&#35328;&#20013;&#65292;&#35813;&#27169;&#22411;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#29256;&#26412;&#20013;&#34920;&#29616;&#26356;&#20026;&#20005;&#37325;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistilWhisper&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;ASR&#26041;&#38754;&#24357;&#21512;&#36825;&#20123;&#35821;&#35328;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#30041;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#20248;&#21183;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#20351;&#29992;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#23545;whisper-small&#36827;&#34892;&#36731;&#37327;&#32423;&#27169;&#22359;&#21270;ASR&#24494;&#35843;&#65292;&#24182;&#20174;whisper-large-v2&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#36825;&#31181;&#21452;&#37325;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20445;&#25345;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#25552;&#21319;ASR&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26631;&#20934;&#24494;&#35843;&#25110;LoRA&#36866;&#37197;&#22120;&#26356;&#26377;&#25928;&#65292;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still under-performs on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we propose DistilWhisper, an approach able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.06117</link><description>&lt;p&gt;
&#36864;&#21518;&#19968;&#27493;&#65306;&#36890;&#36807;&#25277;&#35937;&#21796;&#36215;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#36864;&#21518;&#25552;&#31034;&#8221;&#30340;&#31616;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#21253;&#21547;&#20855;&#20307;&#32454;&#33410;&#30340;&#23454;&#20363;&#20013;&#36827;&#34892;&#25277;&#35937;&#65292;&#24471;&#20986;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#12290;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#21644;&#21407;&#29702;&#26469;&#25351;&#23548;&#25512;&#29702;&#27493;&#39588;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#25512;&#29702;&#36335;&#24452;&#19978;&#26174;&#33879;&#25552;&#21319;&#20102;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;PaLM-2L&#27169;&#22411;&#36827;&#34892;&#20102;&#36864;&#21518;&#25552;&#31034;&#23454;&#39564;&#65292;&#22312;&#21253;&#25324;STEM&#12289;&#30693;&#35782;&#38382;&#31572;&#21644;&#22810;&#36339;&#25512;&#29702;&#22312;&#20869;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35266;&#23519;&#21040;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20363;&#22914;&#65292;&#22312;MMLU&#29289;&#29702;&#21644;&#21270;&#23398;&#20219;&#21153;&#19978;&#65292;&#36864;&#21518;&#25552;&#31034;&#21487;&#20197;&#23558;PaLM-2L&#30340;&#24615;&#33021;&#25552;&#21319;7%&#21644;11%&#65292;&#22312;TimeQA&#20219;&#21153;&#19978;&#25552;&#21319;27%&#65292;&#22312;MuSiQue&#20219;&#21153;&#19978;&#25552;&#21319;7%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
&lt;/p&gt;</description></item><item><title>TRAM&#26159;&#19968;&#31181;&#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#26469;&#25552;&#20379;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#23427;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20248;&#21270;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#21644;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32479;&#19968;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;TRAM&#22312;&#20445;&#25345;&#39044;&#35757;&#32451;&#32467;&#26500;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.03646</link><description>&lt;p&gt;
TRAM: &#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
TRAM: Bridging Trust Regions and Sharpness Aware Minimization. (arXiv:2310.03646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03646
&lt;/p&gt;
&lt;p&gt;
TRAM&#26159;&#19968;&#31181;&#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#26469;&#25552;&#20379;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#23427;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20248;&#21270;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#21644;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32479;&#19968;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;TRAM&#22312;&#20445;&#25345;&#39044;&#35757;&#32451;&#32467;&#26500;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#19981;&#26159;&#20851;&#27880;&#21442;&#25968;&#65292;&#32780;&#26159;&#32771;&#34385;&#21040;&#34920;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#24494;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#20026;&#20102;&#40723;&#21169;&#20445;&#30041;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25216;&#33021;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#39044;&#35757;&#32451;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20449;&#20219;&#21306;&#22495;&#36793;&#30028;&#22312;&#36825;&#20004;&#31181;&#20248;&#21270;&#34920;&#38754;&#19978;&#36890;&#30693;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#32479;&#19968;&#20102;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Trust Region Aware Minimization (TRAM)&#65292;&#19968;&#31181;&#20248;&#21270;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#30340;&#24494;&#35843;&#31639;&#27861;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#39044;&#20808;&#35757;&#32451;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;TRAM&#20248;&#20110;&#38160;&#24230;&#24863;&#30693;&#21644;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
By reducing the curvature of the loss surface in the parameter space, Sharpness-aware minimization (SAM) yields widespread robustness improvement under domain transfer. Instead of focusing on parameters, however, this work considers the transferability of representations as the optimization target for out-of-domain generalization in a fine-tuning setup. To encourage the retention of transferable representations, we consider trust region-based fine-tuning methods, which exploit task-specific skills without forgetting task-agnostic representations from pre-training. We unify parameter- and representation-space smoothing approaches by using trust region bounds to inform SAM-style regularizers on both of these optimization surfaces. We propose Trust Region Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat minima and smooth, informative representations without forgetting pre-trained structure. We find that TRAM outperforms both sharpness-aware and trust region-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#24102;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;(BLOOD),&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;Transformer&#32593;&#32476;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02832</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#23618;&#38388;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#36827;&#34892;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness. (arXiv:2310.02832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#24102;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;(BLOOD),&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;Transformer&#32593;&#32476;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#23545;&#20110;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#30001;&#20110;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#32773;&#24178;&#39044;&#35757;&#32451;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#20013;&#38388;&#23618;&#30340;&#21464;&#25442;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24102;&#22806;&#25968;&#25454;&#65288;BLOOD&#65289;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;BLOOD&#21033;&#29992;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#30340;&#23618;&#38388;&#34920;&#31034;&#21464;&#25442;&#30456;&#36739;&#20110;&#24102;&#22806;&#25968;&#25454;&#30340;&#21464;&#25442;&#26356;&#24179;&#28369;&#30340;&#20542;&#21521;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#22312;Transformer&#32593;&#32476;&#20013;&#32463;&#39564;&#35777;&#26126;&#30340;&#19968;&#20010;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;BLOOD&#19982;Transformer&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#36164;&#28304;&#38656;&#27714;&#30456;&#24403;&#30340;&#26041;&#27861;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#24403;&#23398;&#20064;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#26102;&#65292;&#24102;&#22806;&#25968;&#25454;&#30340;&#21464;&#25442;&#20250;&#20445;&#25345;&#20854;&#21407;&#22987;&#30340;&#38160;&#24230;&#65292;&#32780;&#38160;&#24230;&#20250;&#38543;&#30528;&#20219;&#21153;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective OOD detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in deep neural networks based on transformation smoothness between intermediate layers of a network (BLOOD), which is applicable to pre-trained models without access to training data. BLOOD utilizes the tendency of between-layer representation transformations of in-distribution (ID) data to be smoother than the corresponding transformations of OOD data, a property that we also demonstrate empirically for Transformer networks. We evaluate BLOOD on several text classification tasks with Transformer networks and demonstrate that it outperforms methods with comparable resource requirements. Our analysis also suggests that when learning simpler tasks, OOD data transformations maintain their original sharpness, whereas sharpness increases wit
&lt;/p&gt;</description></item><item><title>ReCAT&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;Transformer&#27169;&#22411;&#65292;&#20351;&#29992;&#36882;&#24402;&#32452;&#21512;&#21644;&#19978;&#19979;&#25991;&#20869;&#22806;&#23618;&#33021;&#22815;&#27169;&#25311;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.16319</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#32452;&#21512;&#22810;&#31890;&#24230;&#34920;&#31034;&#30340;Transformer&#22686;&#24378;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Augmenting transformers with recursively composed multi-grained representations. (arXiv:2309.16319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16319
&lt;/p&gt;
&lt;p&gt;
ReCAT&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;Transformer&#27169;&#22411;&#65292;&#20351;&#29992;&#36882;&#24402;&#32452;&#21512;&#21644;&#19978;&#19979;&#25991;&#20869;&#22806;&#23618;&#33021;&#22815;&#27169;&#25311;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReCAT&#30340;&#36882;&#24402;&#32452;&#21512;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#23398;&#20064;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#26126;&#30830;&#24314;&#27169;&#21407;&#22987;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#12290;&#29616;&#26377;&#30740;&#31350;&#38480;&#21046;&#25968;&#25454;&#36981;&#24490;&#23618;&#32423;&#26641;&#32467;&#26500;&#65292;&#22240;&#27492;&#32570;&#20047;&#36328;&#36317;&#36890;&#20449;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#20869;&#22806;(CIO)&#23618;&#65292;&#36890;&#36807;&#33258;&#24213;&#21521;&#19978;&#21644;&#33258;&#39030;&#21521;&#19979;&#30340;&#20256;&#36882;&#23398;&#20064;&#36328;&#24230;&#30340;&#19978;&#19979;&#25991;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#33258;&#24213;&#21521;&#19978;&#20256;&#36882;&#36890;&#36807;&#32452;&#21512;&#20302;&#32423;&#36328;&#24230;&#24418;&#25104;&#39640;&#32423;&#36328;&#24230;&#30340;&#34920;&#31034;&#65292;&#32780;&#33258;&#39030;&#21521;&#19979;&#20256;&#36882;&#21017;&#32467;&#21512;&#20102;&#36328;&#24230;&#20869;&#37096;&#21644;&#22806;&#37096;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#23884;&#20837;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#20043;&#38388;&#21472;&#21152;&#22810;&#20010;CIO&#23618;&#65292;ReCAT&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#36328;&#36317;&#20869;&#37096;&#21644;&#36328;&#36317;&#38388;&#30340;&#28145;&#23618;&#20132;&#20114;&#65292;&#20174;&#32780;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#23436;&#20840;&#19978;&#19979;&#25991;&#21270;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;CIO&#23618;&#21487;&#20197;&#36827;&#34892;&#32852;&#21512;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained
&lt;/p&gt;</description></item><item><title>InstructERC&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#21644;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#25913;&#38761;&#20102;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.11911</link><description>&lt;p&gt;
InstructERC&#65306;&#20511;&#21161;&#26816;&#32034;&#22810;&#20219;&#21153;LLMs&#26694;&#26550;&#25913;&#38761;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework. (arXiv:2309.11911v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11911
&lt;/p&gt;
&lt;p&gt;
InstructERC&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#21644;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#25913;&#38761;&#20102;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;(ERC)&#30340;&#21457;&#23637;&#19968;&#30452;&#21463;&#21040;&#31649;&#36947;&#35774;&#35745;&#22797;&#26434;&#24615;&#30340;&#38459;&#30861;&#65292;&#23548;&#33268;ERC&#27169;&#22411;&#24448;&#24448;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#23545;&#35805;&#27169;&#24335;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;InstructERC&#65292;&#23558;ERC&#20219;&#21153;&#20174;&#21028;&#21035;&#24335;&#26694;&#26550;&#36716;&#21270;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#12290;InstructERC&#26377;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;InstructERC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#21382;&#21490;&#23545;&#35805;&#20869;&#23481;&#12289;&#26631;&#31614;&#35821;&#21477;&#21644;&#24773;&#24863;&#39046;&#22495;&#28436;&#31034;&#19982;&#39640;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#25340;&#25509;&#65292;&#24110;&#21161;&#27169;&#22411;&#26126;&#30830;&#22320;&#38598;&#25104;&#22810;&#31890;&#24230;&#23545;&#35805;&#30417;&#30563;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#21363;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#24773;&#24863;&#39044;&#27979;&#20219;&#21153;&#65292;&#20197;&#38544;&#24335;&#22320;&#24314;&#27169;&#23545;&#35805;&#35282;&#33394;&#20851;&#31995;&#21644;&#26410;&#26469;&#23545;&#35805;&#24773;&#32490;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The development of emotion recognition in dialogue (ERC) has been consistently hindered by the complexity of pipeline designs, leading to ERC models that often overfit to specific datasets and dialogue patterns. In this study, we propose a novel approach, namely  InstructERC, to reformulates the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs) . InstructERC has two significant contributions: Firstly, InstructERC introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information by concatenating the historical dialog content, label statement, and emotional domain demonstrations with high semantic similarity. Furthermore, we introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. Our LLM-based
&lt;/p&gt;</description></item><item><title>MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10691</link><description>&lt;p&gt;
MINT: &#35780;&#20272;&#22312;&#19982;&#24037;&#20855;&#21644;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#20013;&#30340;LLMs&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10691
&lt;/p&gt;
&lt;p&gt;
MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#38656;&#35201;&#19982;&#29992;&#25143;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#65292;&#26377;&#26102;&#20505;&#36741;&#20197;&#22806;&#37096;&#24037;&#20855;&#30340;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#24120;&#24120;&#24378;&#35843;&#29992;&#21333;&#36718;&#20132;&#27969;&#30340;&#22522;&#20934;&#24615;&#33021;&#65292;&#24573;&#30053;&#20102;&#29992;&#25143;&#12289;LLMs&#21644;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#30340;&#32454;&#33268;&#20114;&#21160;&#65292;&#24182;&#20302;&#20272;&#20102;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#30095;&#24573;&#23548;&#33268;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#32467;&#26524;&#19982;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MINT&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#35780;&#20272;LLMs&#35299;&#20915;&#22810;&#36718;&#20132;&#20114;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;Python&#20195;&#30721;&#26469;&#35775;&#38382;&#24037;&#20855;&#65292;&#24182;&#25509;&#25910;&#30001;GPT-4&#27169;&#25311;&#30340;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#30340;&#24050;&#24314;&#31435;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#25512;&#29702;&#12289;&#32534;&#30721;&#21644;&#20915;&#31574;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11432</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30693;&#35782;&#30340;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#20915;&#31574;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#33719;&#21462;&#22823;&#37327;&#30340;&#32593;&#32476;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#30340;&#39640;&#28072;&#20852;&#36259;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#30340;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#36825;&#20123;&#30740;&#31350;&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#23545;&#33258;&#20027;&#20195;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26500;&#24314;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#28155;&#21152;&#25351;&#20196;&#26631;&#31614;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#36394;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#21644;&#33258;&#25105;&#31579;&#36873;&#29983;&#25104;&#35757;&#32451;&#31034;&#20363;&#65292;&#24182;&#19988;&#22312;Alpaca&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#20986;&#38750;&#24120;&#39640;&#25928;&#30340;&#33258;&#21160;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06259</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#30340;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment with Instruction Backtranslation. (arXiv:2308.06259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#28155;&#21152;&#25351;&#20196;&#26631;&#31614;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#36394;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#21644;&#33258;&#25105;&#31579;&#36873;&#29983;&#25104;&#35757;&#32451;&#31034;&#20363;&#65292;&#24182;&#19988;&#22312;Alpaca&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#20986;&#38750;&#24120;&#39640;&#25928;&#30340;&#33258;&#21160;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#20026;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#28155;&#21152;&#30456;&#24212;&#30340;&#25351;&#20196;&#26631;&#31614;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#36394;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#65292;&#23427;&#20174;&#22312;&#23569;&#37327;&#31181;&#23376;&#25968;&#25454;&#21644;&#32473;&#23450;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#24320;&#22987;&#12290;&#31181;&#23376;&#27169;&#22411;&#29992;&#20110;&#36890;&#36807;&#20026;&#32593;&#32476;&#25991;&#26723;&#29983;&#25104;&#25351;&#20196;&#25552;&#31034;&#65288;&#33258;&#25105;&#22686;&#24378;&#65289;&#26469;&#26500;&#24314;&#35757;&#32451;&#31034;&#20363;&#65292;&#28982;&#21518;&#20174;&#36825;&#20123;&#20505;&#36873;&#31034;&#20363;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#65288;&#33258;&#25105;&#31579;&#36873;&#65289;&#12290;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#24494;&#35843;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;&#20004;&#27425;&#36845;&#20195;&#26469;&#24494;&#35843;LLaMa&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;Alpaca&#25490;&#34892;&#27036;&#19978;&#20987;&#36133;&#20102;&#25152;&#26377;&#20854;&#20182;&#22522;&#20110;LLaMa&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#33976;&#39311;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#38750;&#24120;&#26377;&#25928;&#30340;&#33258;&#21160;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.09476</link><description>&lt;p&gt;
&#36807;&#24230;&#24605;&#32771;&#30495;&#30456;&#65306;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#22797;&#26434;&#27169;&#24335;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#37325;&#29616;&#19981;&#20934;&#30830;&#25110;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#30740;&#31350;&#26377;&#23475;&#30340;&#27169;&#20223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#30456;&#20851;&#29616;&#35937;&#65306;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#12290;&#31532;&#19968;&#20010;&#29616;&#35937;&#65292;&#36807;&#24230;&#24605;&#32771;&#65292;&#22312;&#32473;&#20986;&#27491;&#30830;&#19982;&#38169;&#35823;&#30340;&#23569;&#37327;&#31034;&#33539;&#26102;&#65292;&#25105;&#20204;&#20174;&#20013;&#38388;&#23618;&#35299;&#30721;&#39044;&#27979;&#12290;&#22312;&#26089;&#26399;&#23618;&#20013;&#65292;&#20004;&#31181;&#31034;&#33539;&#24341;&#36215;&#20102;&#30456;&#20284;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#20294;&#22312;&#26576;&#20010;&#8220;&#20851;&#38190;&#23618;&#8221;&#20043;&#21518;&#65292;&#32473;&#20986;&#38169;&#35823;&#31034;&#33539;&#30340;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#12290;&#31532;&#20108;&#20010;&#29616;&#35937;&#65292;&#38169;&#35823;&#24402;&#32435;&#22836;&#65292;&#21487;&#33021;&#26159;&#36807;&#24230;&#24605;&#32771;&#30340;&#19968;&#31181;&#26426;&#21046;&#24615;&#21407;&#22240;&#65306;&#36825;&#20123;&#26159;&#20301;&#20110;&#36739;&#26202;&#23618;&#30340;&#22836;&#37096;&#65292;&#23427;&#20204;&#20851;&#27880;&#24182;&#22797;&#21046;&#20808;&#21069;&#31034;&#33539;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20854;&#21066;&#24369;&#20250;&#20943;&#23569;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#19968;&#20010;&#30693;&#35782;&#28170;&#21338;&#20294;&#32463;&#39564;&#19981;&#36275;&#30340;LLM&#65292;&#33021;&#22815;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#65292;&#20294;&#22312;&#26576;&#20123;&#31867;&#22411;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2303.16421</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#30693;&#35782;&#28170;&#21338;&#20294;&#32463;&#39564;&#19981;&#36275;&#30340;&#38382;&#39064;&#27714;&#35299;&#22120;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24120;&#35782;&#38382;&#39064;&#30340;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models. (arXiv:2303.16421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16421
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#30693;&#35782;&#28170;&#21338;&#20294;&#32463;&#39564;&#19981;&#36275;&#30340;LLM&#65292;&#33021;&#22815;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#65292;&#20294;&#22312;&#26576;&#20123;&#31867;&#22411;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#22312;NLP&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35760;&#24518;&#12289;&#34920;&#36798;&#21644;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#33021;&#21147;&#19968;&#30452;&#26159;LLMs&#30340;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#30171;&#28857;&#12290;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#20197;&#19979;&#20960;&#28857;&#65306;&#65288;1&#65289;GPT&#33021;&#21542;&#26377;&#25928;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#65311;&#65288;2&#65289;GPT&#23545;&#24120;&#35782;&#30693;&#35782;&#26159;&#21542;&#31934;&#36890;&#65311;&#65288;3&#65289;GPT&#26159;&#21542;&#20102;&#35299;&#29992;&#20110;&#22238;&#31572;&#29305;&#23450;&#38382;&#39064;&#30340;&#24213;&#23618;&#24120;&#35782;&#30693;&#35782;&#65311;&#65288;4&#65289;GPT&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#24120;&#35782;&#22238;&#31572;&#38382;&#39064;&#65311;&#20026;&#20102;&#35780;&#20272;&#20197;&#19978;&#24120;&#35782;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#35780;&#20272;ChatGPT&#30340;&#24120;&#35782;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;(1) GPT&#22312;&#24120;&#35782;&#20219;&#21153;&#20013;&#33021;&#22815;&#33719;&#24471;&#33391;&#22909;&#30340;&#38382;&#31572;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#26576;&#20123;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;(2) ChatGPT&#20855;&#26377;&#23398;&#35782;&#28170;&#21338;&#65292;&#21487;&#20197;&#20351;&#29992;&#30693;&#35782;&#25552;&#31034;&#20934;&#30830;&#22320;&#20135;&#29983;&#22823;&#37096;&#20998;&#24120;&#35782;&#30693;&#35782;&#12290;(3)&#23613;&#31649;&#20855;&#26377;&#30693;&#35782;&#65292;ChatGPT&#26159;&#19968;&#20010;&#32570;&#20047;&#32463;&#39564;&#30340;&#24120;&#35782;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#26576;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT and GPT-4 have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point for LLMs. It remains unclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are GPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question? (4) Can GPTs effectively leverage commonsense for answering questions? To evaluate the above commonsense problems, we conduct a series of experiments to evaluate ChatGPT's commonsense abilities, and the experimental results show that: (1) GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cann
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2302.09582</link><description>&lt;p&gt;
&#35821;&#35328;&#29305;&#23450;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#23545;&#24773;&#32490;&#25512;&#26029;&#30340;&#22240;&#26524;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#32490;&#31185;&#23398;&#20013;&#65292;&#22914;&#20309;&#29702;&#35299;&#35821;&#35328;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#20173;&#28982;&#26159;&#19968;&#20010;&#20105;&#35758;&#30340;&#35805;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#35843;&#26597;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#20351;&#29992;&#25552;&#31034;&#25216;&#26415;&#65292;&#21457;&#29616;&#20102;14&#20010;&#24773;&#32490;&#27010;&#24565;&#30340;&#23646;&#24615;&#30001;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#32676;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#23646;&#24615;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#19982;&#38543;&#26426;&#25805;&#32437;&#30456;&#27604;&#65292;&#22823;&#22810;&#25968;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#34920;&#29616;&#20986;&#29616;&#20102;&#19979;&#38477;&#12290;&#23646;&#24615;&#29305;&#23450;&#30340;&#34920;&#29616;&#19979;&#38477;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#30340;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#24378;&#35843;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
&lt;/p&gt;</description></item></channel></rss>