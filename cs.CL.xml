<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>FineFake &#25968;&#25454;&#38598;&#20026;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#25552;&#20379;&#20102;&#30693;&#35782;&#22686;&#24378;&#65292;&#21253;&#21547;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#35206;&#30422;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2404.01336</link><description>&lt;p&gt;
FineFake&#65306;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#30693;&#35782;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01336
&lt;/p&gt;
&lt;p&gt;
FineFake &#25968;&#25454;&#38598;&#20026;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#25552;&#20379;&#20102;&#30693;&#35782;&#22686;&#24378;&#65292;&#21253;&#21547;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#35206;&#30422;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#35780;&#20272;&#26032;&#38395;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#36890;&#24120;&#20165;&#20851;&#27880;&#21333;&#19968;&#35821;&#20041;&#20027;&#39064;&#30340;&#26032;&#38395;&#25110;&#26469;&#33258;&#21333;&#19968;&#24179;&#21488;&#30340;&#26032;&#38395;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#30495;&#23454;&#22330;&#26223;&#20013;&#22810;&#39046;&#22495;&#26032;&#38395;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#20102;&#35299;&#19981;&#21516;&#39046;&#22495;&#30340;&#20551;&#26032;&#38395;&#65292;&#22806;&#37096;&#30693;&#35782;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#25552;&#20379;&#31934;&#30830;&#35777;&#25454;&#24182;&#25581;&#31034;&#21046;&#36896;&#20551;&#26032;&#38395;&#30340;&#22810;&#26679;&#28508;&#22312;&#31574;&#30053;&#65292;&#32780;&#36825;&#20063;&#26159;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#25152;&#24573;&#30053;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;FineFake&#30340;&#26032;&#22411;&#22810;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#32454;&#31890;&#24230;&#27880;&#37322;&#12290;FineFake&#28085;&#30422;&#20102;&#26469;&#33258;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#30340;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#27599;&#20010;&#26032;&#38395;&#39033;&#30446;&#37117;&#21253;&#21547;&#22810;&#27169;&#24577;&#20869;&#23481;&#12289;&#28508;&#22312;&#31038;&#20132;&#32972;&#26223;&#12289;&#21322;&#33258;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01336v1 Announce Type: cross  Abstract: Existing benchmarks for fake news detection have significantly contributed to the advancement of models in assessing the authenticity of news content. However, these benchmarks typically focus solely on news pertaining to a single semantic topic or originating from a single platform, thereby failing to capture the diversity of multi-domain news in real scenarios. In order to understand fake news across various domains, the external knowledge and fine-grained annotations are indispensable to provide precise evidence and uncover the diverse underlying strategies for fabrication, which are also ignored by existing benchmarks. To address this gap, we introduce a novel multi-domain knowledge-enhanced benchmark with fine-grained annotations, named \textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six semantic topics and eight platforms. Each news item is enriched with multi-modal content, potential social context, semi-man
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;2024&#24180;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37327;&#23376;&#35745;&#31639;&#24212;&#29992;&#65292;&#22312;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#20102;&#35832;&#22914;&#35789;&#23884;&#20837;&#12289;&#24207;&#21015;&#27169;&#22411;&#12289;&#27880;&#24847;&#21147;&#21644;&#35821;&#27861;&#20998;&#26512;&#31561;NLP&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#35774;&#35745;&#26469;&#22788;&#29702;&#25991;&#26412;&#32534;&#30721;&#65292;&#24182;&#25506;&#35752;&#20102;&#37327;&#23376;&#29702;&#35770;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#26234;&#33021;&#26159;&#20160;&#20040;&#65311;&#8221;&#31561;&#26680;&#24515;&#38382;&#39064;&#30340;&#20851;&#38190;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.19758</link><description>&lt;p&gt;
2024&#24180;&#30340;&#33258;&#28982;&#35821;&#35328;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#37327;&#23376;&#35745;&#31639;&#65306;QNLP&#20013;&#30340;&#30740;&#31350;&#35201;&#28857;&#21644;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Natural Language, AI, and Quantum Computing in 2024: Research Ingredients and Directions in QNLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;2024&#24180;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37327;&#23376;&#35745;&#31639;&#24212;&#29992;&#65292;&#22312;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#20102;&#35832;&#22914;&#35789;&#23884;&#20837;&#12289;&#24207;&#21015;&#27169;&#22411;&#12289;&#27880;&#24847;&#21147;&#21644;&#35821;&#27861;&#20998;&#26512;&#31561;NLP&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#35774;&#35745;&#26469;&#22788;&#29702;&#25991;&#26412;&#32534;&#30721;&#65292;&#24182;&#25506;&#35752;&#20102;&#37327;&#23376;&#29702;&#35770;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#26234;&#33021;&#26159;&#20160;&#20040;&#65311;&#8221;&#31561;&#26680;&#24515;&#38382;&#39064;&#30340;&#20851;&#38190;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19758v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25277;&#35937;&#65306;&#35821;&#35328;&#22788;&#29702;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#20851;&#38190;&#65292;&#21516;&#26102;&#37327;&#23376;&#35745;&#31639;&#20063;&#24320;&#22987;&#24212;&#29992;&#12290;&#36825;&#24341;&#36215;&#20102;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20986;&#29616;&#20102;&#20960;&#20010;&#26089;&#26399;&#25552;&#26696;&#21644;&#23454;&#39564;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;NLP&#30456;&#20851;&#25216;&#26415;&#65292;&#21253;&#25324;&#35789;&#23884;&#20837;&#12289;&#24207;&#21015;&#27169;&#22411;&#12289;&#27880;&#24847;&#21147;&#21644;&#35821;&#27861;&#20998;&#26512;&#26159;&#22914;&#20309;&#24212;&#29992;&#20110;&#37327;&#23376;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#32534;&#30721;&#30340;&#22522;&#26412;&#20219;&#21153;&#65288;&#22312;&#20869;&#23384;&#20013;&#34920;&#31034;&#19968;&#20010;&#23383;&#31526;&#20018;&#65289;&#65292;&#36825;&#22312;&#20197;&#21069;&#27809;&#26377;&#35814;&#32454;&#35752;&#35770;&#36807;&#12290;&#38500;&#20102;&#25512;&#21160;&#26032;&#25216;&#26415;&#65292;&#37327;&#23376;&#29702;&#35770;&#36824;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#26234;&#33021;&#26159;&#20160;&#20040;&#65311;&#8221;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20570;&#20986;&#20102;&#20851;&#38190;&#36129;&#29486;&#12290;&#38543;&#30528;&#36825;&#20123;&#38382;&#39064;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#21464;&#24471;&#24840;&#21457;&#32039;&#36843;&#65292;&#26412;&#25991;&#36824;&#32771;&#34385;&#20102;&#19968;&#20123;&#20107;&#23454;&#27010;&#24565;&#21270;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19758v1 Announce Type: cross  Abstract: Language processing is at the heart of current developments in artificial intelligence, and quantum computers are becoming available at the same time. This has led to great interest in quantum natural language processing, and several early proposals and experiments. This paper surveys the state of this area, showing how NLP-related techniques including word embeddings, sequential models, attention, and grammatical parsing have been used in quantum language processing. We introduce a new quantum design for the basic task of text encoding (representing a string of characters in memory), which has not been addressed in detail before.   As well as motivating new technologies, quantum theory has made key contributions to the challenging questions of 'What is uncertainty?' and 'What is intelligence?' As these questions are taking on fresh urgency with artificial systems, the paper also considers some of the ways facts are conceptualized and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#25351;&#23548;&#22810;&#27493;&#28436;&#31034;&#20013;&#38544;&#21547;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#25628;&#32034;&#65292;&#20197;&#21450;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#28436;&#31034;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#12290;</title><link>https://arxiv.org/abs/2403.17124</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#35745;&#21010;&#22522;&#20110;&#28436;&#31034;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#36827;&#34892;&#33853;&#23454;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Plans in Demonstrations Through Counterfactual Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#25351;&#23548;&#22810;&#27493;&#28436;&#31034;&#20013;&#38544;&#21547;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#25628;&#32034;&#65292;&#20197;&#21450;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#28436;&#31034;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#25512;&#29702;&#22522;&#20110;&#29289;&#29702;&#39046;&#22495;&#33853;&#23454;&#22312;&#20307;&#29616;&#26234;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#19987;&#27880;&#20110;&#30452;&#25509;&#21033;&#29992;LLMs&#22312;&#31526;&#21495;&#31354;&#38388;&#20869;&#35268;&#21010;&#65292;&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;LLMs&#25351;&#23548;&#20219;&#21153;&#32467;&#26500;&#30340;&#25628;&#32034;&#65292;&#38544;&#21547;&#22312;&#22810;&#27493;&#28436;&#31034;&#20013;&#30340;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#25805;&#32437;&#35268;&#21010;&#25991;&#29486;&#20013;&#30340;&#27169;&#24335;&#26063;&#30340;&#27010;&#24565;&#65292;&#23427;&#25353;&#29031;&#29305;&#23450;&#36816;&#21160;&#32422;&#26463;&#23558;&#26426;&#22120;&#20154;&#37197;&#32622;&#20998;&#32452;&#65292;&#20316;&#20026;LLM&#39640;&#32423;&#35821;&#35328;&#34920;&#31034;&#21644;&#26426;&#22120;&#20154;&#20302;&#32423;&#29289;&#29702;&#36712;&#36857;&#20043;&#38388;&#30340;&#25277;&#35937;&#23618;&#12290;&#36890;&#36807;&#29992;&#21512;&#25104;&#24178;&#25200;&#37325;&#26032;&#25773;&#25918;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#35206;&#30422;&#28436;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#39069;&#22806;&#29983;&#25104;&#25104;&#21151;&#25191;&#34892;&#20197;&#21450;&#26410;&#23436;&#25104;&#20219;&#21153;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#23398;&#20064;&#26694;&#26550;&#35757;&#32451;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17124v1 Announce Type: cross  Abstract: Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural networ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24046;&#20998;&#31169;&#26377;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Next-Token Prediction of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38544;&#31169;&#26085;&#30410;&#37325;&#35201;&#12290;DP-SGD&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#20197;&#19968;&#31181;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DP-SGD&#38656;&#35201;&#27604;SGD&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#36807;&#39640;&#20272;&#35745;&#23545;&#25163;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26356;&#29616;&#23454;&#30340;&#22330;&#26223;&#20551;&#35774;&#21482;&#26377;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;&#22312;&#36825;&#20123;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31169;&#26377;&#28151;&#21512;&#38598;&#21512;&#20998;&#24067;&#65288;PMixED&#65289;&#65306;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#27599;&#20010;&#36755;&#20986;&#20998;&#24067;&#20174;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;LLM&#38598;&#21512;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#28982;&#21518;&#23545;&#25237;&#24433;&#20998;&#24067;&#36827;&#34892;&#24179;&#22343;&#24182;&#20174;&#20013;&#25277;&#26679;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31169;&#26377;&#39044;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;DP-SGD&#26356;&#36731;&#37327;&#21270;&#65292;&#22240;&#20026;&#23427;&#19982;&#27169;&#22411;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15412</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#27979;&#37327;&#21644;&#24314;&#27169;&#8220;&#25991;&#21270;&#8221;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring and Modeling "Culture" in LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15412
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#23545;39&#31687;&#26368;&#26032;&#35770;&#25991;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27809;&#26377;&#19968;&#31687;&#30740;&#31350;&#23450;&#20041;&#8220;&#25991;&#21270;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#12289;&#22810;&#23618;&#38754;&#30340;&#27010;&#24565;&#65307;&#30456;&#21453;&#65292;&#23427;&#20204;&#22312;&#19968;&#20123;&#29305;&#21035;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#38754;&#31216;&#20026;&#25991;&#21270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#22312;&#20154;&#21475;&#32479;&#35745;&#12289;&#35821;&#20041;&#21644;&#35821;&#35328;&#25991;&#21270;&#20132;&#20114;&#20195;&#29702;&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#12290;&#25105;&#20204;&#36824;&#23545;&#37319;&#29992;&#30340;&#25506;&#26597;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#26377;&#8220;&#25991;&#21270;&#8221;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#22914;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#65292;&#34987;&#30740;&#31350;&#20102;&#65292;&#30041;&#19979;&#20102;&#20960;&#20010;&#20854;&#20182;&#26377;&#36259;&#19988;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22823;&#37327;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#65288;Hershcovich&#31561;&#20154;&#65292;2022&#65289;&#30340;&#26410;&#34987;&#25506;&#31350;&#12290;&#21478;&#22806;&#20004;&#20010;&#20851;&#38190;&#30340;&#31354;&#30333;&#26159;&#30446;&#21069;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24773;&#22659;&#24615;&#30340;&#32570;&#20047;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
&lt;/p&gt;</description></item><item><title>CHAI&#25552;&#20986;&#20102;Clustered Head Attention&#65288;CHAI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#32467;&#21512;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#37327;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;21.4&#65285;&#65292;&#25512;&#29702;&#26102;&#38388;&#24310;&#36831;&#38477;&#20302;1.73&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.08058</link><description>&lt;p&gt;
CHAI&#65306;&#39640;&#25928;LLM&#25512;&#29702;&#30340;&#32858;&#31867;&#22836;&#37096;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
CHAI: Clustered Head Attention for Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08058
&lt;/p&gt;
&lt;p&gt;
CHAI&#25552;&#20986;&#20102;Clustered Head Attention&#65288;CHAI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#32467;&#21512;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#37327;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;21.4&#65285;&#65292;&#25512;&#29702;&#26102;&#38388;&#24310;&#36831;&#38477;&#20302;1.73&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#25968;&#30334;&#20159;&#21442;&#25968;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#26102;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#26082;&#38656;&#35201;&#35745;&#31639;&#21448;&#38656;&#35201;&#20869;&#23384;&#65292;&#19968;&#20010;&#35831;&#27714;&#21487;&#33021;&#38656;&#35201;&#22810;&#20010;GPU&#21644;&#25968;&#21313;GB&#30340;&#20869;&#23384;&#12290;&#22810;&#22836;&#27880;&#24847;&#21147;&#26159;LLMs&#30340;&#20851;&#38190;&#32452;&#20214;&#20043;&#19968;&#65292;&#21487;&#20197;&#21344;LLMs&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;50%&#20197;&#19978;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21508;&#22836;&#20043;&#38388;&#23545;&#27880;&#24847;&#21147;&#30340;&#20851;&#27880;&#26377;&#24456;&#39640;&#30340;&#20887;&#20313;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Clustered Head Attention (CHAI)&#12290;CHAI&#22312;&#36816;&#34892;&#26102;&#23558;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#22836;&#37096;&#32467;&#21512;&#36827;&#34892;&#33258;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CHAI&#33021;&#22815;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;&#22810;&#36798;21.4%&#65292;&#25512;&#29702;&#26102;&#24310;&#36831;&#38477;&#20302;&#22810;&#36798;1.73&#20493;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#12290;CHAI&#23454;&#29616;&#20102;&#26368;&#22810;3.2%&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08058v1 Announce Type: cross  Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviat
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#26426;&#21046;&#8220;&#39034;&#24207;&#26500;&#36896;&#20943;&#23569;&#8221;&#65292;&#26088;&#22312;&#23454;&#29616;&#22810;&#25773;&#24182;&#21457;&#36890;&#20449;&#30340;&#30830;&#23450;&#24615;&#65292;&#25193;&#23637;&#20102;CCS&#30340;&#25216;&#26415;&#35774;&#32622;&#65292;&#35777;&#26126;&#20102;&#26500;&#36896;&#20943;&#23569;&#30340;&#27719;&#32858;&#23646;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#19968;&#20123;&#35821;&#27861;&#38480;&#21046;&#19979;&#36816;&#31639;&#31526;&#30340;&#32467;&#26500;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04618</link><description>&lt;p&gt;
&#26102;&#26631;CCS&#20013;&#30340;&#24378;&#20248;&#20808;&#32423;&#21644;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Strong Priority and Determinacy in Timed CCS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04618
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#26426;&#21046;&#8220;&#39034;&#24207;&#26500;&#36896;&#20943;&#23569;&#8221;&#65292;&#26088;&#22312;&#23454;&#29616;&#22810;&#25773;&#24182;&#21457;&#36890;&#20449;&#30340;&#30830;&#23450;&#24615;&#65292;&#25193;&#23637;&#20102;CCS&#30340;&#25216;&#26415;&#35774;&#32622;&#65292;&#35777;&#26126;&#20102;&#26500;&#36896;&#20943;&#23569;&#30340;&#27719;&#32858;&#23646;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#19968;&#20123;&#35821;&#27861;&#38480;&#21046;&#19979;&#36816;&#31639;&#31526;&#30340;&#32467;&#26500;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#20248;&#20808;&#32423;&#30340;&#32463;&#20856;&#36827;&#31243;&#20195;&#25968;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#39034;&#24207;&#26500;&#36896;&#20943;&#23569;&#8221;&#30340;&#26032;&#35843;&#24230;&#26426;&#21046;&#65292;&#26088;&#22312;&#25429;&#25417;&#21516;&#27493;&#32534;&#31243;&#30340;&#26412;&#36136;&#12290;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#30340;&#29420;&#29305;&#23646;&#24615;&#26159;&#36890;&#36807;&#26500;&#36896;&#23454;&#29616;&#22810;&#25773;&#24182;&#21457;&#36890;&#20449;&#30340;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#27169;&#25311;&#20855;&#26377;&#23545;&#32570;&#22833;&#21453;&#24212;&#30340;&#20849;&#20139;&#20869;&#23384;&#22810;&#32447;&#31243;&#65292;&#22240;&#20026;&#23427;&#26159;Esterel&#32534;&#31243;&#35821;&#35328;&#30340;&#26680;&#24515;&#12290;&#22312;&#36890;&#36807;&#26102;&#38047;&#21644;&#20248;&#20808;&#32423;&#25193;&#23637;&#30340;CCS&#30340;&#25216;&#26415;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#25105;&#20204;&#31216;&#20026;&#8220;&#32467;&#26500;&#36830;&#36143;&#8221;&#30340;&#22823;&#31867;&#36807;&#31243;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26500;&#36896;&#20943;&#23569;&#30340;&#27719;&#32858;&#23646;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#22312;&#19968;&#20123;&#31216;&#20026;&#8220;&#21487;&#26530;&#32445;&#8221;&#30340;&#35821;&#27861;&#38480;&#21046;&#19979;&#65292;&#21069;&#32512;&#12289;&#27714;&#21644;&#12289;&#24182;&#34892;&#32452;&#25104;&#12289;&#38480;&#21046;&#21644;&#38544;&#34255;&#30340;&#36816;&#31639;&#31526;&#20445;&#25345;&#32467;&#26500;&#36830;&#36143;&#12290;&#36825;&#28085;&#30422;&#20102;&#19968;&#20010;&#20005;&#26684;&#26356;&#22823;&#30340;&#36807;&#31243;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04618v1 Announce Type: cross  Abstract: Building on the classical theory of process algebra with priorities, we identify a new scheduling mechanism, called "sequentially constructive reduction" which is designed to capture the essence of synchronous programming. The distinctive property of this evaluation strategy is to achieve determinism-by-construction for multi-cast concurrent communication. In particular, it permits us to model shared memory multi-threading with reaction to absence as it lies at the core of the programming language Esterel. In the technical setting of CCS extended by clocks and priorities, we prove for a large class of processes, which we call "structurally coherent" the confluence property for constructive reductions. We further show that under some syntactic restrictions, called "pivotable" the operators of prefix, summation, parallel composition, restriction and hiding preserve structural coherence. This covers a strictly larger class of processes co
&lt;/p&gt;</description></item><item><title>ProMoAI&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#65292;&#25903;&#25345;&#20248;&#21270;&#24182;&#36890;&#36807;&#29992;&#25143;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#65292;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;AI&#39537;&#21160;&#30340;&#36807;&#31243;&#24314;&#27169;&#24037;&#20855;&#65292;&#38477;&#20302;&#20102;&#29992;&#25143;&#30340;&#25216;&#26415;&#38376;&#27099;&#12290;</title><link>https://arxiv.org/abs/2403.04327</link><description>&lt;p&gt;
ProMoAI&#65306;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#36827;&#34892;&#36807;&#31243;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ProMoAI: Process Modeling with Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04327
&lt;/p&gt;
&lt;p&gt;
ProMoAI&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#65292;&#25903;&#25345;&#20248;&#21270;&#24182;&#36890;&#36807;&#29992;&#25143;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#65292;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;AI&#39537;&#21160;&#30340;&#36807;&#31243;&#24314;&#27169;&#24037;&#20855;&#65292;&#38477;&#20302;&#20102;&#29992;&#25143;&#30340;&#25216;&#26415;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ProMoAI&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#33258;&#21160;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#20808;&#36827;&#30340;&#25552;&#31034;&#24037;&#31243;&#12289;&#38169;&#35823;&#22788;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#25216;&#26415;&#12290;&#38500;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;&#22797;&#26434;&#30340;&#36807;&#31243;&#27169;&#22411;&#22806;&#65292;ProMoAI&#36824;&#25903;&#25345;&#36807;&#31243;&#27169;&#22411;&#20248;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#19982;&#24037;&#20855;&#36827;&#34892;&#20132;&#20114;&#65292;&#28982;&#21518;&#29992;&#20110;&#25913;&#36827;&#36807;&#31243;&#27169;&#22411;&#12290;ProMoAI&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#20197;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#36807;&#31243;&#24314;&#27169;&#26041;&#27861;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#23545;&#27809;&#26377;&#28145;&#20837;&#25216;&#26415;&#30693;&#35782;&#30340;&#29992;&#25143;&#30340;&#20934;&#20837;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04327v1 Announce Type: cross  Abstract: ProMoAI is a novel tool that leverages Large Language Models (LLMs) to automatically generate process models from textual descriptions, incorporating advanced prompt engineering, error handling, and code generation techniques. Beyond automating the generation of complex process models, ProMoAI also supports process model optimization. Users can interact with the tool by providing feedback on the generated model, which is then used for refining the process model. ProMoAI utilizes the capabilities LLMs to offer a novel, AI-driven approach to process modeling, significantly reducing the barrier to entry for users without deep technical knowledge in process modeling.
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.02090</link><description>&lt;p&gt;
&#24314;&#27169;&#22810;&#27169;&#24577;&#31038;&#20132;&#20114;&#21160;&#65306;&#20855;&#26377;&#23494;&#38598;&#23545;&#40784;&#34920;&#31034;&#30340;&#26032;&#25361;&#25112;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28041;&#21450;&#35328;&#35821;&#21644;&#38750;&#35328;&#35821;&#32447;&#32034;&#30340;&#31038;&#20132;&#20114;&#21160;&#23545;&#26377;&#25928;&#35299;&#37322;&#31038;&#20132;&#24773;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#22810;&#27169;&#24577;&#31038;&#20132;&#32447;&#32034;&#30340;&#20808;&#21069;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20154;&#34892;&#20026;&#19978;&#65292;&#25110;&#20381;&#36182;&#20110;&#19982;&#22810;&#26041;&#29615;&#22659;&#20013;&#30340;&#35805;&#35821;&#23494;&#20999;&#23545;&#40784;&#30340;&#25972;&#20307;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#20204;&#22312;&#24314;&#27169;&#22810;&#26041;&#20114;&#21160;&#30340;&#22797;&#26434;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20197;&#24314;&#27169;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65306;&#35805;&#35821;&#30446;&#26631;&#35782;&#21035;&#12289;&#20195;&#35789;&#25351;&#20195;&#28040;&#35299;&#21644;&#25552;&#21450;&#29609;&#23478;&#39044;&#27979;&#12290;&#25105;&#20204;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#20013;&#30340;&#36825;&#20123;&#26032;&#25361;&#25112;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#19982;&#20854;&#23545;&#24212;&#30340;&#35805;&#35821;&#21516;&#27493;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#65292;&#36825;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02090v1 Announce Type: cross  Abstract: Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;Brilla AI&#22312;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#32570;&#20047;&#21512;&#26684;&#25945;&#24072;&#30340;&#38750;&#27954;&#25552;&#20379;&#20102;&#23398;&#20064;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.01699</link><description>&lt;p&gt;
Brilla AI: &#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#30340;&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;
&lt;/p&gt;
&lt;p&gt;
Brilla AI: AI Contestant for the National Science and Maths Quiz
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01699
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;Brilla AI&#22312;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#32570;&#20047;&#21512;&#26684;&#25945;&#24072;&#30340;&#38750;&#27954;&#25552;&#20379;&#20102;&#23398;&#20064;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#22823;&#38470;&#32570;&#20047;&#36275;&#22815;&#30340;&#21512;&#26684;&#25945;&#24072;&#65292;&#36825;&#38459;&#30861;&#20102;&#25552;&#20379;&#36275;&#22815;&#30340;&#23398;&#20064;&#25903;&#25345;&#12290;&#20154;&#24037;&#26234;&#33021;&#26377;&#21487;&#33021;&#22686;&#24378;&#26377;&#38480;&#25968;&#37327;&#25945;&#24072;&#30340;&#21162;&#21147;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#23398;&#20064;&#25104;&#26524;&#12290;&#26412;&#25991;&#25551;&#36848;&#24182;&#35780;&#20272;&#20102;NSMQ AI Grand Challenge&#30340;&#39318;&#35201;&#25104;&#26524;&#65292;&#35813;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#29616;&#23454;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27492;&#31867;&#20154;&#24037;&#26234;&#33021;&#65306;&#8220;&#24314;&#31435;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65292;&#21442;&#21152;&#21152;&#32435;&#30340;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#65288;NSMQ&#65289;&#65292;&#24182;&#33719;&#32988;&#8212;&#8212;&#22312;&#27604;&#36187;&#30340;&#25152;&#26377;&#36718;&#27425;&#21644;&#38454;&#27573;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20248;&#31168;&#30340;&#21442;&#36187;&#32773;&#8221;&#12290;NSMQ&#26159;&#21152;&#32435;&#30340;&#39640;&#20013;&#23398;&#29983;&#27599;&#24180;&#20030;&#34892;&#30340;&#29616;&#22330;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#65292;3&#38431;2&#21517;&#23398;&#29983;&#36890;&#36807;&#22238;&#31572;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#12289;&#29289;&#29702;&#21644;&#25968;&#23398;&#38382;&#39064;&#22312;5&#36718;&#27604;&#36187;&#20013;&#31454;&#20105;&#65292;&#36880;&#28176;&#26187;&#32423;&#33267;&#26368;&#32456;&#20896;&#20891;&#30340;&#38431;&#20237;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;Brilla AI&#65292;&#19968;&#20010;&#21442;&#21152;NSMQ&#31454;&#36187;&#30340;&#20154;&#24037;&#26234;&#33021;&#36873;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01699v1 Announce Type: cross  Abstract: The African continent lacks enough qualified teachers which hampers the provision of adequate learning support. An AI could potentially augment the efforts of the limited number of teachers, leading to better learning outcomes. Towards that end, this work describes and evaluates the first key output for the NSMQ AI Grand Challenge, which proposes a robust, real-world benchmark for such an AI: "Build an AI to compete live in Ghana's National Science and Maths Quiz (NSMQ) competition and win - performing better than the best contestants in all rounds and stages of the competition". The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. In this work, we built Brilla AI, an AI contestant that we de
&lt;/p&gt;</description></item><item><title>DMoERM&#39318;&#27425;&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30340;&#27010;&#24565;&#24341;&#20837;&#22870;&#21169;&#24314;&#27169;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#21452;&#23618;MoE RM&#65288;DMoERM&#65289;&#65292;&#36890;&#36807;&#31232;&#30095;&#30340;&#22806;&#23618;MoE&#21644;&#23494;&#38598;&#30340;&#20869;&#23618;MoE&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#20013;&#30340;&#22810;&#20219;&#21153;&#24178;&#25200;&#21644;&#25968;&#25454;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01197</link><description>&lt;p&gt;
DMoERM: &#26377;&#25928;&#22870;&#21169;&#24314;&#27169;&#30340;&#28151;&#21512;&#19987;&#23478;&#37197;&#26041;
&lt;/p&gt;
&lt;p&gt;
DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01197
&lt;/p&gt;
&lt;p&gt;
DMoERM&#39318;&#27425;&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30340;&#27010;&#24565;&#24341;&#20837;&#22870;&#21169;&#24314;&#27169;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#21452;&#23618;MoE RM&#65288;DMoERM&#65289;&#65292;&#36890;&#36807;&#31232;&#30095;&#30340;&#22806;&#23618;MoE&#21644;&#23494;&#38598;&#30340;&#20869;&#23618;MoE&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#20013;&#30340;&#22810;&#20219;&#21153;&#24178;&#25200;&#21644;&#25968;&#25454;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#30340;&#24615;&#33021;&#26159;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23545;&#40784;&#24494;&#35843;&#36807;&#31243;&#20013;&#25928;&#26524;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290; RM&#35757;&#32451;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#20351;&#29992;&#21508;&#31181;&#31867;&#21035;&#25968;&#25454;&#35757;&#32451;&#30456;&#21516;&#30340;RM&#21487;&#33021;&#23548;&#33268;&#20854;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#22810;&#20219;&#21153;&#24178;&#25200;&#30340;&#24433;&#21709;&#65307;2&#65289;&#20154;&#31867;&#27880;&#37322;&#30340;&#19968;&#33268;&#24615;&#29575;&#36890;&#24120;&#20165;&#20026;60%&#33267;75%&#65292;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#22823;&#37327;&#22122;&#22768;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30340;&#27010;&#24565;&#24341;&#20837;&#21040;RM&#39046;&#22495;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23618;MoE RM&#65288;DMoERM&#65289;&#12290; &#22806;&#23618;MoE&#26159;&#19968;&#31181;&#31232;&#30095;&#27169;&#22411;&#12290; &#23558;&#36755;&#20837;&#20998;&#31867;&#25104;&#20219;&#21153;&#31867;&#21035;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#36335;&#30001;&#21040;&#30456;&#24212;&#30340;&#20869;&#23618;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#20013;&#12290; &#20869;&#23618;MoE&#26159;&#19968;&#31181;&#23494;&#38598;&#27169;&#22411;&#12290; &#25105;&#20204;&#23558;&#29305;&#23450;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#33021;&#21147;&#32500;&#24230;&#65292;&#24182;&#20998;&#21035;&#22312;&#27599;&#20010;&#32500;&#24230;&#19978;&#23545;LoRA&#19987;&#23478;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01197v1 Announce Type: new  Abstract: The performance of the reward model (RM) is a critical factor in improving the effectiveness of the large language model (LLM) during alignment fine-tuning. There remain two challenges in RM training: 1) training the same RM using various categories of data may cause its generalization performance to suffer from multi-task disturbance, and 2) the human annotation consistency rate is generally only $60\%$ to $75\%$, causing training data to contain a lot of noise. To tackle these two challenges, we introduced the idea of Mixture-of-Experts (MoE) into the field of RM for the first time. We propose the Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After classifying an input into task categories, we route it to the corresponding inner layer task-specific model. The inner layer MoE is a dense model. We decompose the specific task into multiple capability dimensions and individually fine-tune a LoRA expert on each one. T
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#22312;ChatGPT&#20013;&#23558;&#32763;&#35793;&#31616;&#35201;&#21644;&#32763;&#35793;&#32773;/&#20316;&#32773;&#20154;&#29289;&#35282;&#33394;&#34701;&#20837;&#25552;&#31034;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#31867;&#20043;&#38388;&#30340;&#32763;&#35793;&#36890;&#20449;&#65292;&#20294;&#23545;&#20110;&#25913;&#21892;ChatGPT&#32763;&#35793;&#36136;&#37327;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.00127</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;ChatGPT&#36827;&#34892;&#32763;&#35793;&#65306;&#32763;&#35793;&#31616;&#35201;&#21644;&#20154;&#29289;&#35282;&#33394;&#25552;&#31034;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Prompting ChatGPT for Translation: A Comparative Analysis of Translation Brief and Persona Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00127
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#22312;ChatGPT&#20013;&#23558;&#32763;&#35793;&#31616;&#35201;&#21644;&#32763;&#35793;&#32773;/&#20316;&#32773;&#20154;&#29289;&#35282;&#33394;&#34701;&#20837;&#25552;&#31034;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#31867;&#20043;&#38388;&#30340;&#32763;&#35793;&#36890;&#20449;&#65292;&#20294;&#23545;&#20110;&#25913;&#21892;ChatGPT&#32763;&#35793;&#36136;&#37327;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#24050;&#26174;&#31034;&#20986;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#25552;&#31034;&#35774;&#35745;&#20013;&#34701;&#20837;&#32763;&#35793;&#27010;&#24565;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;ChatGPT&#20013;&#20026;&#32763;&#35793;&#20219;&#21153;&#35774;&#35745;&#25552;&#31034;&#26102;&#23558;&#32763;&#35793;&#31616;&#35201;&#27010;&#24565;&#21644;&#32763;&#35793;&#32773;&#21450;&#20316;&#32773;&#30340;&#20154;&#29289;&#35282;&#33394;&#32467;&#21512;&#36215;&#26469;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#26576;&#20123;&#20803;&#32032;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#32763;&#35793;&#36890;&#20449;&#65292;&#20294;&#23427;&#20204;&#22312;&#25552;&#39640;ChatGPT&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#26356;&#22810;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#32763;&#35793;&#29702;&#35770;&#23478;&#21644;&#23454;&#36341;&#32773;&#22914;&#20309;&#24320;&#21457;&#30446;&#21069;&#26681;&#26893;&#20110;&#20154;&#19982;&#20154;&#20132;&#27969;&#33539;&#24335;&#30340;&#27010;&#24565;&#24037;&#20855;&#38598;&#65292;&#20197;&#24212;&#29992;&#20110;&#28041;&#21450;&#20154;&#26426;&#20132;&#20114;&#30340;&#26032;&#20852;&#24037;&#20316;&#27969;&#20013;&#30340;&#32763;&#35793;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00127v1 Announce Type: new  Abstract: Prompt engineering in LLMs has shown potential for improving translation quality. However, the potential of incorporating translation concepts in prompt design remains largely underexplored. Against this backdrop, this paper discusses the effectiveness of incorporating the conceptual tool of translation brief and the personas of translator and author into prompt design for translation tasks in ChatGPT. Findings suggest that, although certain elements are constructive in facilitating human to human communication for translation tasks, their effectiveness is limited for improving translation quality in ChatGPT. This accentuates the need for more explorative research on how translation theorists and practitioners can develop the current set of conceptual tools rooted in the human to human communication paradigm for translation purposes in this emerging workflow involving human machine interaction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#27969;&#31243;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#25551;&#36848;&#24433;&#21709;&#21147;&#27963;&#21160;&#65292;&#36890;&#36807;&#22810;&#31181;&#25216;&#26415;&#22686;&#24378;&#27969;&#31243;&#24615;&#33021;&#65292;&#24182;&#22312;&#25991;&#26723;&#32423;&#21035;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#39044;&#27979;&#25928;&#26524;</title><link>https://arxiv.org/abs/2402.17151</link><description>&lt;p&gt;
&#23545;&#25991;&#26723;&#37096;&#20998;&#36827;&#34892;&#32858;&#31867;&#65306;&#20174;&#25991;&#26723;&#20013;&#26816;&#27979;&#21644;&#25551;&#36848;&#24433;&#21709;&#21147;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17151
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#27969;&#31243;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#25551;&#36848;&#24433;&#21709;&#21147;&#27963;&#21160;&#65292;&#36890;&#36807;&#22810;&#31181;&#25216;&#26415;&#22686;&#24378;&#27969;&#31243;&#24615;&#33021;&#65292;&#24182;&#22312;&#25991;&#26723;&#32423;&#21035;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#39044;&#27979;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#27979;&#21644;&#25551;&#36848;&#24433;&#21709;&#21147;&#27963;&#21160;&#12290;&#35813;&#26041;&#27861;&#23545;&#25991;&#26723;&#30340;&#37096;&#20998;&#36827;&#34892;&#32858;&#31867;&#65292;&#26816;&#27979;&#21487;&#33021;&#21453;&#26144;&#24433;&#21709;&#21147;&#27963;&#21160;&#30340;&#32858;&#31867;&#65292;&#28982;&#21518;&#36890;&#36807;&#23427;&#20204;&#19982;&#39640;&#24433;&#21709;&#21147;&#32858;&#31867;&#30340;&#20851;&#32852;&#26469;&#35782;&#21035;&#19982;&#24433;&#21709;&#21147;&#27963;&#21160;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#25991;&#26723;&#26159;&#21542;&#23646;&#20110;&#24433;&#21709;&#21147;&#27963;&#21160;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#30452;&#25509;&#25991;&#26723;&#32423;&#21035;&#20998;&#31867;&#21644;&#30452;&#25509;&#25991;&#26723;&#32423;&#21035;&#32858;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#39062;&#25216;&#26415;&#26469;&#22686;&#24378;&#25105;&#20204;&#30340;&#27969;&#31243;&#65292;&#21253;&#25324;&#20351;&#29992;&#29616;&#26377;&#30340;&#20107;&#20214;&#20107;&#23454;&#39044;&#27979;&#31995;&#32479;&#33719;&#21462;&#25991;&#26723;&#37096;&#20998;&#65292;&#24182;&#32858;&#21512;&#22810;&#20010;&#32858;&#31867;&#23454;&#39564;&#20197;&#25552;&#39640;&#32858;&#31867;&#21644;&#25991;&#26723;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#22312;&#32858;&#31867;&#30340;&#22522;&#30784;&#19978;&#23545;&#25991;&#26723;&#36827;&#34892;&#20998;&#31867;&#19981;&#20165;&#21487;&#20197;&#20934;&#30830;&#25552;&#21462;&#19982;&#24433;&#21709;&#21147;&#27963;&#21160;&#30456;&#20851;&#30340;&#25991;&#26723;&#37096;&#20998;&#65292;&#36824;&#21487;&#20197;&#25429;&#25417;&#21040;&#24433;&#21709;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17151v1 Announce Type: new  Abstract: We propose a novel clustering pipeline to detect and characterize influence campaigns from documents. This approach clusters parts of document, detects clusters that likely reflect an influence campaign, and then identifies documents linked to an influence campaign via their association with the high-influence clusters. Our approach outperforms both the direct document-level classification and the direct document-level clustering approach in predicting if a document is part of an influence campaign. We propose various novel techniques to enhance our pipeline, including using an existing event factuality prediction system to obtain document parts, and aggregating multiple clustering experiments to improve the performance of both cluster and document classification. Classifying documents on the top of clustering not only accurately extracts the parts of the documents that are relevant to influence campaigns, but also capture influence camp
&lt;/p&gt;</description></item><item><title>LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.16906</link><description>&lt;p&gt;
LDB&#65306;&#36890;&#36807;&#36880;&#27493;&#39564;&#35777;&#36816;&#34892;&#26102;&#25191;&#34892;&#26469;&#35843;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16906
&lt;/p&gt;
&lt;p&gt;
LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#20165;&#23558;&#21333;&#27425;&#20195;&#30721;&#29983;&#25104;&#65292;&#32780;&#19988;&#36824;&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;&#31243;&#24207;&#39564;&#35777;&#22120;&#25972;&#21512;&#21040;LLMs&#20013;&#65292;&#20197;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23558;&#29983;&#25104;&#30340;&#31243;&#24207;&#35270;&#20026;&#19981;&#21487;&#20998;&#21106;&#30340;&#23454;&#20307;&#65292;&#36825;&#23545;LLMs&#22312;&#35843;&#35797;&#31243;&#24207;&#26102;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#30340;&#36923;&#36753;&#27969;&#31243;&#21644;&#25968;&#25454;&#25805;&#20316;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#24320;&#21457;&#20154;&#21592;&#35843;&#35797;&#31243;&#24207;&#26102;&#65292;&#20182;&#20204;&#36890;&#24120;&#35774;&#32622;&#26029;&#28857;&#24182;&#26377;&#36873;&#25321;&#22320;&#26816;&#26597;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#12290;&#25191;&#34892;&#27969;&#21644;&#20013;&#38388;&#21464;&#37327;&#22312;&#35843;&#35797;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#25991;&#29486;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#22120;&#65288;LDB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;LLMs&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#23436;&#21892;&#20854;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
&lt;/p&gt;</description></item><item><title>InstructEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#31616;&#21333;&#25351;&#20196;&#20351;&#32534;&#36753;&#22120;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#32534;&#36753;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16123</link><description>&lt;p&gt;
InstructEdit&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
InstructEdit: Instruction-based Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16123
&lt;/p&gt;
&lt;p&gt;
InstructEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#31616;&#21333;&#25351;&#20196;&#20351;&#32534;&#36753;&#22120;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#32534;&#36753;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#32780;&#19981;&#20250;&#23545;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#28040;&#26497;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#36328;&#20219;&#21153;&#30340;&#36890;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#35774;&#35745;&#19968;&#20010;&#29420;&#29305;&#30340;&#32534;&#36753;&#22120;&#65292;&#36825;&#26174;&#33879;&#38459;&#30861;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#30693;&#35782;&#32534;&#36753;&#20013;&#30340;&#22810;&#20219;&#21153;&#27867;&#21270;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#31216;&#20026;InstructEdit&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25351;&#20196;&#20419;&#36827;&#32534;&#36753;&#22120;&#21516;&#26102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#21482;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#32534;&#36753;&#22120;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#26041;&#38754;&#34920;&#26126;&#65292;InstructEdit&#21487;&#20197;&#25552;&#39640;&#32534;&#36753;&#22120;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#22810;&#20219;&#21153;&#32534;&#36753;&#35774;&#32622;&#20013;&#24179;&#22343;&#25552;&#39640;&#21487;&#38752;&#24615;14.86%&#12290;&#27492;&#22806;&#65292;&#28041;&#21450;&#20445;&#30041;&#26410;&#35265;&#20219;&#21153;&#30340;&#23454;&#39564;&#35828;&#26126;&#65292;InstructEdi
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16123v1 Announce Type: cross  Abstract: Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.14846</link><description>&lt;p&gt;
&#22362;&#25345;&#20320;&#30340;&#35282;&#33394;&#65281;&#20010;&#20154;&#20215;&#20540;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stick to your Role! Stability of Personal Values Expressed in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#25110;&#24515;&#29702;&#38382;&#21367;&#30340;&#26631;&#20934;&#26041;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#25552;&#20379;&#35768;&#22810;&#26469;&#28304;&#20110;&#31867;&#20284;&#26368;&#23567;&#32972;&#26223;&#30340;&#19981;&#21516;&#26597;&#35810;&#65288;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#39640;&#24230;&#20381;&#36182;&#20110;&#32972;&#26223;&#65292;&#22240;&#27492;&#20174;&#36825;&#31181;&#26368;&#23567;&#32972;&#26223;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#21487;&#33021;&#23545;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#34892;&#20026;&#65288;&#22312;&#37027;&#37324;&#23427;&#23558;&#26292;&#38706;&#20110;&#35768;&#22810;&#26032;&#32972;&#26223;&#65289;&#30340;&#35828;&#26126;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20381;&#36182;&#20110;&#32972;&#26223;&#30340;&#29305;&#24615;&#24212;&#35813;&#20316;&#20026;LLM&#27604;&#36739;&#30340;&#21478;&#19968;&#20010;&#32500;&#24230;&#26469;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#32500;&#24230;&#65292;&#22914;&#35748;&#30693;&#33021;&#21147;&#12289;&#30693;&#35782;&#25110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#65288;&#27169;&#25311;&#23545;&#19981;&#21516;&#35805;&#39064;&#30340;&#23545;&#35805;&#65289;&#20215;&#20540;&#34920;&#36798;&#31283;&#23450;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#65289;&#21644;&#34892;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#27979;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#20116;&#20010;&#23478;&#26063;&#30340;19&#20010;&#24320;&#28304;LLM&#12290;&#20511;&#37492;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31561;&#32423;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Factiverse AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12147</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#35268;&#27169;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
End-to-end multilingual fact-checking at scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12147
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Factiverse AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;Factiverse AI&#27169;&#22411;&#22312;100&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#24615;&#22522;&#20934;&#27979;&#35797;&#23637;&#31034;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;GPT-4&#12289;GPT-3.5-Turbo&#21644;Mistral-7b&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12147v1 Announce Type: cross  Abstract: In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models. We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.
&lt;/p&gt;</description></item><item><title>DoRA&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20998;&#35299;&#20026;&#24133;&#24230;&#21644;&#26041;&#21521;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#23545;LLaMA&#65292;LLaVA&#21644;VL-B&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09353</link><description>&lt;p&gt;
DoRA: &#20998;&#35299;&#26435;&#37325;&#30340;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DoRA: Weight-Decomposed Low-Rank Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09353
&lt;/p&gt;
&lt;p&gt;
DoRA&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20998;&#35299;&#20026;&#24133;&#24230;&#21644;&#26041;&#21521;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#23545;LLaMA&#65292;LLaVA&#21644;VL-B&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#26041;&#27861;&#20013;&#65292;&#30001;&#20110;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;LoRA&#21450;&#20854;&#21464;&#31181;&#26041;&#27861;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#31934;&#24230;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#20998;&#35299;&#20998;&#26512;&#26041;&#27861;&#26469;&#30740;&#31350;FT&#21644;LoRA&#20043;&#38388;&#30340;&#20869;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#27169;&#25311;FT&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoRA&#30340;&#26435;&#37325;&#20998;&#35299;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#12290;DoRA&#23558;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20998;&#35299;&#20026;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24133;&#24230;&#21644;&#26041;&#21521;&#65292;&#24182;&#19988;&#20855;&#20307;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#65292;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;DoRA&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;LoRA&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#20219;&#20309;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;&#24494;&#35843;LLaMA&#65292;LLaVA&#21644;VL-B&#19978;&#65292;DoRA&#22987;&#32456;&#20248;&#20110;LoRA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;Amharic-LLaMA&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36890;&#36807;&#21019;&#24314;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08015</link><description>&lt;p&gt;
&#22686;&#24378;Amharic-LLaMA: &#25972;&#21512;&#29305;&#23450;&#20219;&#21153;&#19982;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;Amharic-LLaMA&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36890;&#36807;&#21019;&#24314;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#20013;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#22240;&#32570;&#20047;&#36164;&#28304;&#32780;&#34987;&#33853;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25972;&#21512;&#29305;&#23450;&#20219;&#21153;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;LLaMA-2-Amharic&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#38463;&#22982;&#21704;&#25289;&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;LLaMA-2-Amharic&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#27969;&#31243;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#36755;&#20986;&#65292;&#20197;&#20419;&#36827;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However, low-resource languages are left behind due to the unavailability of resources. In this work, we focus on enhancing the LLaMA-2-Amharic model by integrating task-specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction fine-tuning dataset and fine-tuned LLaMA-2-Amharic model. The fine-tuned model shows promising results in different NLP tasks. We open-source our dataset creation pipeline, instruction datasets, trained models, and evaluation outputs to promote language-specific studies on these models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25551;&#36848;&#20102;&#22312;&#38750;&#27954;&#24320;&#21457;&#21644;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#24037;&#20855;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;SuaCode&#23398;&#20064;&#32534;&#30721;&#24212;&#29992;&#12289;AutoGrad&#33258;&#21160;&#35780;&#20998;&#21644;&#21453;&#39304;&#24037;&#20855;&#12289;&#20195;&#30721;&#25220;&#34989;&#26816;&#27979;&#24037;&#20855;&#20197;&#21450;&#21452;&#35821;AI&#25945;&#24072;Kwame&#12290;&#36825;&#20123;&#24037;&#20855;&#26377;&#21161;&#20110;&#35299;&#20915;&#38750;&#27954;&#23398;&#29983;&#22312;&#25945;&#32946;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07397</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25512;&#36827;&#38750;&#27954;&#31185;&#23398;&#21644;&#35745;&#31639;&#25945;&#32946;&#65306;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Leveraging AI to Advance Science and Computing Education across Africa: Progress, Challenges, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07397
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25551;&#36848;&#20102;&#22312;&#38750;&#27954;&#24320;&#21457;&#21644;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#24037;&#20855;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;SuaCode&#23398;&#20064;&#32534;&#30721;&#24212;&#29992;&#12289;AutoGrad&#33258;&#21160;&#35780;&#20998;&#21644;&#21453;&#39304;&#24037;&#20855;&#12289;&#20195;&#30721;&#25220;&#34989;&#26816;&#27979;&#24037;&#20855;&#20197;&#21450;&#21452;&#35821;AI&#25945;&#24072;Kwame&#12290;&#36825;&#20123;&#24037;&#20855;&#26377;&#21161;&#20110;&#35299;&#20915;&#38750;&#27954;&#23398;&#29983;&#22312;&#25945;&#32946;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#27954;&#22823;&#38470;&#65292;&#23398;&#29983;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25945;&#32946;&#25361;&#25112;&#65292;&#21253;&#25324;&#33719;&#21462;&#35745;&#31639;&#26426;&#12289;&#32593;&#32476;&#36830;&#25509;&#12289;&#21487;&#38752;&#30005;&#21147;&#21644;&#21512;&#26684;&#25945;&#24072;&#31561;&#22522;&#26412;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#65288;&#22914;BERT&#21644;GPT-4&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#20419;&#36827;&#25945;&#32946;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#24448;&#24448;&#22312;&#35199;&#26041;&#25945;&#32946;&#29615;&#22659;&#20013;&#36827;&#34892;&#37096;&#32626;&#21644;&#35780;&#20272;&#65292;&#23545;&#38750;&#27954;&#23398;&#29983;&#38754;&#20020;&#30340;&#29420;&#29305;&#38656;&#27714;&#21644;&#25361;&#25112;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;&#38750;&#27954;&#24320;&#21457;&#21644;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#24037;&#20855;&#30340;&#24037;&#20316;&#65306;&#65288;1&#65289;SuaCode&#65292;&#19968;&#27454;AI&#21160;&#21147;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20351;&#38750;&#27954;&#20154;&#21487;&#20197;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#23398;&#20064;&#32534;&#31243;&#65292;&#65288;2&#65289;AutoGrad&#65292;&#29992;&#20110;&#22270;&#24418;&#21644;&#20132;&#20114;&#24335;&#32534;&#31243;&#20316;&#19994;&#30340;&#33258;&#21160;&#35780;&#20998;&#21644;&#21453;&#39304;&#24037;&#20855;&#65292;&#65288;3&#65289;&#19968;&#31181;&#20195;&#30721;&#25220;&#34989;&#26816;&#27979;&#24037;&#20855;&#65292;&#23637;&#31034;&#20102;&#25220;&#34989;&#30340;&#21487;&#35270;&#35777;&#25454;&#65292;&#65288;4&#65289;Kwame&#65292;&#19968;&#27454;&#21452;&#35821;&#30340;AI&#25945;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;
Across the African continent, students grapple with various educational challenges, including limited access to essential resources such as computers, internet connectivity, reliable electricity, and a shortage of qualified teachers. Despite these challenges, recent advances in AI such as BERT, and GPT-4 have demonstrated their potential for advancing education. Yet, these AI tools tend to be deployed and evaluated predominantly within the context of Western educational settings, with limited attention directed towards the unique needs and challenges faced by students in Africa. In this book chapter, we describe our works developing and deploying AI in Education tools in Africa: (1) SuaCode, an AI-powered app that enables Africans to learn to code using their smartphones, (2) AutoGrad, an automated grading, and feedback tool for graphical and interactive coding assignments, (3) a tool for code plagiarism detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI teac
&lt;/p&gt;</description></item><item><title>&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03469</link><description>&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#19982;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Preference-free Alignment Learning with Regularized Relevance Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03469
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20542;&#21521;&#20110;&#32473;&#38271;&#30340;&#19982;&#20027;&#39064;&#26080;&#20851;&#30340;&#22238;&#22797;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#32780;&#32473;&#30701;&#30340;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#22238;&#22797;&#36739;&#20302;&#20998;&#12290;&#22312;&#36825;&#19968;&#35266;&#23519;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26080;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#8220;&#30456;&#20851;&#24615;&#8221;&#20316;&#20026;&#23545;&#40784;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#36890;&#36807;&#26816;&#32034;&#24471;&#21040;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#30340;&#24433;&#21709;&#65292;&#21363;&#36807;&#24230;&#20248;&#21270;&#21040;&#19981;&#26399;&#26395;&#30340;&#25463;&#24452;&#19978;&#65292;&#24403;&#25105;&#20204;&#23558;&#35813;&#24471;&#20998;&#20316;&#20026;&#22870;&#21169;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#25972;&#21512;&#21040;&#24120;&#35268;&#30340;&#30456;&#20851;&#24615;&#20013;&#65292;&#20114;&#30456;&#27491;&#21017;&#21270;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#28151;&#21512;&#22870;&#21169;&#20989;&#25968;&#65306;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#65288;$R^3$&#65289;&#12290;$R^3$&#36890;&#36807;&#25552;&#20379;&#31283;&#20581;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;$R^3$&#19981;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
&lt;/p&gt;</description></item><item><title>DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03300</link><description>&lt;p&gt;
DeepSeekMath: &#23558;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25512;&#21521;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03300
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#30340;&#29305;&#24615;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSeekMath 7B&#65292;&#23427;&#22312;Common Crawl&#20013;&#33719;&#21462;&#20102;120B&#20010;&#19982;&#25968;&#23398;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#20195;&#30721;&#25968;&#25454;&#26469;&#32487;&#32493;&#39044;&#35757;&#32451;DeepSeek-Coder-Base-v1.5 7B&#12290;DeepSeekMath 7B&#22312;&#31454;&#36187;&#32423;&#21035;&#30340;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;51.7%&#30340;&#20998;&#25968;&#65292;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#24037;&#20855;&#21253;&#21644;&#25237;&#31080;&#25216;&#26415;&#65292;&#25509;&#36817;&#20102;Gemini-Ultra&#21644;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;DeepSeekMath 7B&#30340;&#33258;&#19968;&#33268;&#24615;&#22312;MATH&#19978;&#30340;64&#20010;&#26679;&#26412;&#20013;&#36798;&#21040;&#20102;60.9%&#30340;&#20998;&#25968;&#12290;DeepSeekMath&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#24402;&#22240;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#36873;&#25321;&#31649;&#36947;&#20805;&#20998;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;GRPO&#65289;&#65292;&#36825;&#26159;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#21487;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00711</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Explaining Text Classifiers with Counterfactual Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20854;&#20013;&#21453;&#20107;&#23454;&#26159;&#25351;&#38500;&#20102;&#19968;&#20010;&#20998;&#31867;&#29305;&#24449;&#20043;&#22806;&#65292;&#19982;&#30495;&#23454;&#35266;&#23519;&#23436;&#20840;&#30456;&#21516;&#30340;&#20551;&#35774;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#39046;&#22495;&#26500;&#24314;&#36825;&#31181;&#21453;&#20107;&#23454;&#23384;&#22312;&#29305;&#23450;&#25361;&#25112;&#65292;&#22240;&#20026;&#26576;&#20123;&#23646;&#24615;&#20540;&#21487;&#33021;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#20107;&#20214;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#24178;&#39044;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#24178;&#39044;&#26041;&#27861;&#26159;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;Pearl&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20013;&#23450;&#20041;&#30340;&#21453;&#20107;&#23454;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;&#30495;&#23454;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#26126;&#30830;&#30340;&#25991;&#26412;&#24178;&#39044;&#33719;&#24471;&#65289;&#21644;&#25105;&#20204;&#30340;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#30340;&#24178;&#39044;&#24471;&#21040;&#65289;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2311.14455</link><description>&lt;p&gt;
&#20174;&#34987;&#27602;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#26500;&#24314;&#30340;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Universal Jailbreak Backdoors from Poisoned Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#26377;&#29992;&#19988;&#26080;&#23475;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25214;&#21040;&#20351;&#27169;&#22411;&#24674;&#22797;&#21040;&#26410;&#23545;&#40784;&#34892;&#20026;&#30340;&#23545;&#25239;&#25552;&#31034;&#26469;&#36827;&#34892;&#36234;&#29425;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;RLHF&#35757;&#32451;&#25968;&#25454;&#23558;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#23884;&#20837;&#27169;&#22411;&#20013;&#12290;&#35813;&#21518;&#38376;&#23558;&#19968;&#20010;&#35302;&#21457;&#35789;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#8220;sudo&#21629;&#20196;&#8221;&#65306;&#22312;&#20219;&#20309;&#25552;&#31034;&#20013;&#28155;&#21152;&#35302;&#21457;&#35789;&#23558;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#65292;&#26080;&#38656;&#25628;&#32034;&#23545;&#25239;&#25552;&#31034;&#12290;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20808;&#21069;&#30740;&#31350;&#30340;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#26356;&#24378;&#22823;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#24120;&#35265;&#30340;&#21518;&#38376;&#25915;&#20987;&#25216;&#26415;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;RLHF&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#23545;&#20854;&#25152;&#22768;&#31216;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#24067;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;AI&#25945;&#36741;Kwame&#65292;&#38754;&#21521;&#35199;&#38750;&#31185;&#23398;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20102;&#23454;&#38469;&#37096;&#32626;&#21644;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;Kwame for Science&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#21644;&#38382;&#39064;&#25552;&#38382;&#37327;&#12290;</title><link>https://arxiv.org/abs/2302.10786</link><description>&lt;p&gt;
&#38754;&#21521;&#35199;&#38750;&#31185;&#23398;&#25945;&#32946;&#30340;AI&#25945;&#36741;Kwame&#30340;&#29616;&#23454;&#19990;&#30028;&#37096;&#32626;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.10786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;AI&#25945;&#36741;Kwame&#65292;&#38754;&#21521;&#35199;&#38750;&#31185;&#23398;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20102;&#23454;&#38469;&#37096;&#32626;&#21644;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;Kwame for Science&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#21644;&#38382;&#39064;&#25552;&#38382;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#25945;&#24072;&#19982;&#23398;&#29983;&#30340;&#27604;&#20363;&#39640;&#65292;&#36825;&#38480;&#21046;&#20102;&#23398;&#29983;&#20204;&#33719;&#21462;&#25945;&#32946;&#38382;&#39064;&#35299;&#31572;&#31561;&#23398;&#20064;&#25903;&#25345;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#23558;&#38754;&#21521;&#32534;&#30721;&#25945;&#32946;&#30340;AI&#25945;&#36741;Kwame&#25193;&#23637;&#20026;&#38754;&#21521;&#31185;&#23398;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#37096;&#32626;&#20026;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#12290;Kwame for Science&#36890;&#36807;&#25552;&#20379;&#26469;&#33258;&#31934;&#36873;&#30693;&#35782;&#26469;&#28304;&#30340;&#27573;&#33853;&#20197;&#21450;&#22522;&#20110;&#35199;&#38750;&#39640;&#32423;&#20013;&#23398;&#35777;&#20070;&#32771;&#35797;&#65288;WASSCE&#65289;&#30340;&#32508;&#21512;&#31185;&#23398;&#31185;&#30446;&#30340;&#30456;&#20851;&#36807;&#21435;&#30340;&#22269;&#23478;&#32771;&#35797;&#38382;&#39064;&#30340;&#31572;&#26696;&#26469;&#22238;&#31572;&#23398;&#29983;&#20204;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23398;&#29983;&#20204;&#36824;&#21487;&#20197;&#26597;&#30475;&#36807;&#21435;&#30340;&#22269;&#23478;&#32771;&#35797;&#38382;&#39064;&#21450;&#20854;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#24320;&#21457;&#30340;&#20027;&#39064;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#25353;&#24180;&#20221;&#12289;&#38382;&#39064;&#31867;&#22411;&#21644;&#20027;&#39064;&#30340;&#33258;&#21160;&#20998;&#31867;&#65288;91%&#38750;&#21152;&#26435;&#24179;&#22343;&#21484;&#22238;&#29575;&#65289;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#37096;&#32626;&#20102;Kwame for Science&#36229;&#36807;8&#20010;&#26376;&#65292;&#26377;&#26469;&#33258;32&#20010;&#22269;&#23478;&#65288;&#20854;&#20013;15&#20010;&#22312;&#38750;&#27954;&#65289;&#30340;750&#20010;&#29992;&#25143;&#65292;&#20849;&#25552;&#20986;&#20102;1.5K&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;87.2%&#30340;&#21069;&#19977;&#21517;&#38382;&#39064;&#20934;&#30830;&#29575;&#65288;n=109&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Africa has a high student-to-teacher ratio which limits students' access to teachers for learning support such as educational question answering. In this work, we extended Kwame, a bilingual AI teaching assistant for coding education, adapted it for science education, and deployed it as a web app. Kwame for Science provides passages from well-curated knowledge sources and related past national exam questions as answers to questions from students based on the Integrated Science subject of the West African Senior Secondary Certificate Examination (WASSCE). Furthermore, students can view past national exam questions along with their answers and filter by year, question type, and topics that were automatically categorized by a topic detection model which we developed (91% unweighted average recall). We deployed Kwame for Science in the real world over 8 months and had 750 users across 32 countries (15 in Africa) and 1.5K questions asked. Our evaluation showed an 87.2% top 3 accuracy (n=109
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;(Chat)GPT&#21644;BERT&#22312;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;(Chat)GPT&#30340;&#34920;&#29616;&#26126;&#26174;&#20302;&#20110;BERT&#65292;&#23588;&#20854;&#22312;&#38271;&#26399;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14040</link><description>&lt;p&gt;
(&#32842;&#22825;)GPT v BERT: &#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20043;&#40654;&#26126;&#30340;&#27491;&#20041;&#12290;(arXiv:2401.14040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection. (arXiv:2401.14040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;(Chat)GPT&#21644;BERT&#22312;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;(Chat)GPT&#30340;&#34920;&#29616;&#26126;&#26174;&#20302;&#20110;BERT&#65292;&#23588;&#20854;&#22312;&#38271;&#26399;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#21644;(Chat)GPT&#65292;&#20316;&#20026;&#20855;&#26377;&#35299;&#20915;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#30340;&#24040;&#22823;&#33021;&#21147;&#30340;&#35789;&#27719;&#36229;&#32423;&#33521;&#38596;&#32780;&#20986;&#29616;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#35821;&#20041;&#21464;&#21270;&#30340;&#26102;&#38388;&#24615;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#35299;&#20915;Word-in-Context (WiC)&#20219;&#21153;&#30340;&#20004;&#20010;&#21382;&#26102;&#24615;&#25193;&#23637;&#65306;TempoWiC&#21644;HistoWiC&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#65288;&#21644;GPT&#65289;3.5&#36825;&#26679;&#30340;&#26032;&#22411;&#21363;&#29992;&#25216;&#26415;&#19982;&#24403;&#21069;&#20316;&#20026;&#24314;&#27169;&#35821;&#20041;&#21464;&#21270;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#23478;&#26063;BERT&#20043;&#38388;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26159;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;(Chat)GPT&#30740;&#31350;&#35821;&#20041;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#30340;&#24615;&#33021;&#26174;&#33879;&#20302;&#20110;&#22522;&#30784;GPT&#29256;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;(Chat)GPT&#22312;&#26816;&#27979;&#38271;&#26399;&#21464;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#30053;&#20302;&#20110;BERT&#65292;&#20294;&#22312;&#30701;&#26399;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10862</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#20445;&#25252;: &#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#23545;&#40784;&#30340;LLMs&#30340;&#36234;&#29425;&#25269;&#25239;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#35825;&#20351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#21644;&#36829;&#27861;&#20869;&#23481;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21098;&#26525;LLM&#21442;&#25968;&#22810;&#36798;20&#65285;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#23427;&#20204;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#24182;&#19988;&#19981;&#25439;&#23475;&#20854;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21098;&#26525;&#21518;&#35266;&#23519;&#21040;&#30340;&#22686;&#24378;&#23433;&#20840;&#24615;&#19982;&#27169;&#22411;&#30340;&#21021;&#22987;&#23433;&#20840;&#35757;&#32451;&#27700;&#24179;&#30456;&#20851;&#65292;&#36825;&#26263;&#31034;&#21098;&#26525;&#30340;&#25928;&#26524;&#21487;&#33021;&#26356;&#26222;&#36941;&#65292;&#20063;&#21487;&#33021;&#36866;&#29992;&#20110;&#36229;&#20986;&#23433;&#20840;&#24615;&#33539;&#30068;&#30340;&#20854;&#20182;LLM&#34892;&#20026;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#12289;&#25554;&#20837;&#21040;&#21313;&#20010;&#19981;&#21516;&#36234;&#29425;&#25552;&#31034;&#20013;&#30340;225&#20010;&#26377;&#23475;&#20219;&#21153;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;LLMs&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#36234;&#29425;&#25552;&#31034;&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;LLaMA-2 Chat&#65292;Vicuna&#21644;Mistral Instruct&#65289;&#20855;&#26377;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intention Analysis Prompting (IAPrompt)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#24182;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06561</link><description>&lt;p&gt;
Intention Analysis Prompting&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#33391;&#22909;&#30340;&#36234;&#29425;&#38450;&#24481;&#32773;
&lt;/p&gt;
&lt;p&gt;
Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender. (arXiv:2401.06561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intention Analysis Prompting (IAPrompt)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#24182;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#38544;&#34109;&#21644;&#22797;&#26434;&#30340;&#36234;&#29425;&#25915;&#20987;&#26102;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#21363;Intention Analysis Prompting&#65288;IAPrompt&#65289;&#12290;&#20854;&#21407;&#29702;&#26159;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#35302;&#21457;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#65306;1&#65289;&#22522;&#26412;&#24847;&#22270;&#20998;&#26512;&#65292;2&#65289;&#19982;&#25919;&#31574;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;IAPrompt&#26159;&#19968;&#31181;&#20165;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#32780;&#19981;&#25439;&#23475;&#20854;&#26377;&#29992;&#24615;&#12290;&#22312;Vicuna&#12289;ChatGLM&#12289;MPT&#12289;DeepSeek&#21644;GPT-3.5&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;IAPrompt&#33021;&#22815;&#25345;&#32493;&#19988;&#26174;&#33879;&#22320;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#65288;&#24179;&#22343;&#25915;&#20987;&#25104;&#21151;&#29575;&#19979;&#38477;46.5%&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#22312;https://github.com/alph&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#33050;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting (IAPrompt). The principle behind is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harmfulness in response (averagely -46.5% attack success rate) and maintain the general helpfulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alph
&lt;/p&gt;</description></item><item><title>L3Cube-IndicNews&#26159;&#19968;&#20010;&#38754;&#21521;&#21360;&#24230;&#35821;&#31995;&#30340;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30701;&#26631;&#39064;&#12289;&#38271;&#25991;&#26723;&#21644;&#38271;&#27573;&#33853;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#23427;&#25552;&#20379;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#25991;&#31456;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02254</link><description>&lt;p&gt;
L3Cube-IndicNews&#65306;&#21360;&#24230;&#35821;&#31995;&#26032;&#38395;&#30701;&#25991;&#21644;&#38271;&#25991;&#20998;&#31867;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages. (arXiv:2401.02254v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02254
&lt;/p&gt;
&lt;p&gt;
L3Cube-IndicNews&#26159;&#19968;&#20010;&#38754;&#21521;&#21360;&#24230;&#35821;&#31995;&#30340;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30701;&#26631;&#39064;&#12289;&#38271;&#25991;&#26723;&#21644;&#38271;&#27573;&#33853;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#23427;&#25552;&#20379;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#25991;&#31456;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;L3Cube-IndicNews&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#20026;&#21360;&#24230;&#22320;&#21306;&#30340;&#21508;&#22823;&#26041;&#35328;&#35821;&#35328;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20851;&#27880;&#26032;&#38395;&#26631;&#39064;&#21644;&#25991;&#31456;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;10&#31181;&#20027;&#35201;&#30340;&#21360;&#24230;&#35821;&#35328;&#19978;&#65292;&#21253;&#25324;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#12289;&#21345;&#32435;&#36798;&#35821;&#12289;&#22885;&#37324;&#20122;&#35821;&#12289;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#21644;&#26049;&#36974;&#26222;&#35821;&#12290;&#27599;&#20010;&#26032;&#38395;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;L3Cube-IndicNews&#25552;&#20379;&#20102;3&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#25991;&#26723;&#38271;&#24230;&#36827;&#34892;&#20998;&#31867;&#65306;&#30701;&#26631;&#39064;&#20998;&#31867;&#65288;SHC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#26032;&#38395;&#26631;&#39064;&#21644;&#26032;&#38395;&#31867;&#21035;&#65292;&#38271;&#25991;&#26723;&#20998;&#31867;&#65288;LDC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#25972;&#20010;&#26032;&#38395;&#25991;&#31456;&#21644;&#26032;&#38395;&#31867;&#21035;&#65292;&#38271;&#27573;&#33853;&#20998;&#31867;&#65288;LPC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#26032;&#38395;&#30340;&#23376;&#25991;&#31456;&#21644;&#26032;&#38395;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;3&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#20445;&#25345;&#20102;&#19968;&#33268;&#30340;&#26631;&#31614;&#65292;&#20197;&#36827;&#34892;&#28145;&#20837;&#30340;&#22522;&#20110;&#38271;&#24230;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#25351;&#26631;&#23545;&#27599;&#20010;&#21360;&#24230;&#35821;&#35328;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce L3Cube-IndicNews, a multilingual text classification corpus aimed at curating a high-quality dataset for Indian regional languages, with a specific focus on news headlines and articles. We have centered our work on 10 prominent Indic languages, including Hindi, Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and Punjabi. Each of these news datasets comprises 10 or more classes of news articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle different document lengths that are classified as: Short Headlines Classification (SHC) dataset containing the news headline and news category, Long Document Classification (LDC) dataset containing the whole news article and the news category, and Long Paragraph Classification (LPC) containing sub-articles of the news and the news category. We maintain consistent labeling across all 3 datasets for in-depth length-based analysis. We evaluate each of these Indic language datasets using 4 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#21019;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#12290;</title><link>http://arxiv.org/abs/2310.08383</link><description>&lt;p&gt;
&#37325;&#26500;&#26448;&#26009;&#22235;&#38754;&#20307;&#65306;&#26448;&#26009;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Materials Tetrahedron: Challenges in Materials Information Extraction. (arXiv:2310.08383v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#21019;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#26448;&#26009;&#30340;&#21457;&#29616;&#24050;&#32463;&#26377;&#30528;&#25345;&#32493;&#20960;&#20010;&#19990;&#32426;&#20035;&#33267;&#26356;&#38271;&#26102;&#38388;&#30340;&#25512;&#21160;&#20154;&#31867;&#36827;&#27493;&#30340;&#21382;&#21490;&#12290;&#26448;&#26009;&#30340;&#34892;&#20026;&#21462;&#20915;&#20110;&#20854;&#32452;&#25104;&#12289;&#32467;&#26500;&#21644;&#24615;&#36136;&#65292;&#32780;&#36825;&#20123;&#21448;&#20381;&#36182;&#20110;&#20854;&#21152;&#24037;&#21644;&#27979;&#35797;&#26465;&#20214;&#12290;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21457;&#23637;&#20351;&#24471;&#20174;&#20986;&#29256;&#30340;&#25991;&#29486;&#20013;&#65288;&#22914;&#21516;&#34892;&#35780;&#23457;&#20986;&#29256;&#29289;&#12289;&#22270;&#20070;&#21644;&#19987;&#21033;&#65289;&#22823;&#35268;&#27169;&#25552;&#21462;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20449;&#24687;&#20197;&#19981;&#21516;&#30340;&#26684;&#24335;&#65288;&#22914;&#34920;&#26684;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#23637;&#29616;&#65292;&#24182;&#19988;&#22312;&#25253;&#21578;&#26679;&#24335;&#19978;&#32570;&#20047;&#19968;&#33268;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#12289;&#37327;&#21270;&#21644;&#35760;&#24405;&#20102;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#33258;&#21160;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#26399;&#21019;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20174;&#25991;&#26412;&#21644;&#34920;&#26684;&#20013;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20102;&#20960;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#30740;&#31350;&#33021;&#28608;&#21457;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovery of new materials has a documented history of propelling human progress for centuries and more. The behaviour of a material is a function of its composition, structure, and properties, which further depend on its processing and testing conditions. Recent developments in deep learning and natural language processing have enabled information extraction at scale from published literature such as peer-reviewed publications, books, and patents. However, this information is spread in multiple formats, such as tables, text, and images, and with little or no uniformity in reporting style giving rise to several machine learning challenges. Here, we discuss, quantify, and document these challenges in automated information extraction (IE) from materials science literature towards the creation of a large materials science knowledge base. Specifically, we focus on IE from text and tables and outline several challenges with examples. We hope the present work inspires researchers to address 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02980</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65306;&#20844;&#27491;&#27604;&#36739;&#38271;&#24207;&#21015;&#27169;&#22411;&#38656;&#35201;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#20123;&#26550;&#26500;&#65292;&#22914;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27604;Transformers&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#24615;&#36827;&#23637;&#20027;&#35201;&#26159;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#24207;&#21015;&#30340;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;&#20363;&#22914;Long Range Arena&#65289;&#19978;&#23637;&#31034;&#20986;&#26469;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#23548;&#33268;&#23545;&#26550;&#26500;&#20043;&#38388;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#24182;&#19988;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20165;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#25968;&#25454;&#65289;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Transformers&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#20043;&#38388;&#24471;&#21040;&#24456;&#23567;&#30340;&#24046;&#36317;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#19982;S4&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;PathX-256&#20219;&#21153;&#19978;&#23558;SSMs&#30340;&#26368;&#20339;&#25253;&#21578;&#32467;&#26524;&#25552;&#39640;&#20102;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23618;&#27425;&#34920;&#31034;&#65288;SSHR&#65289;&#20248;&#21270;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#27425;&#34920;&#31034;&#65292;&#25552;&#21462;&#20986;&#35821;&#35328;&#30456;&#20851;&#24103;&#21644;&#29305;&#23450;&#20869;&#23481;&#65292;&#24182;&#24341;&#23548;&#27169;&#22411;&#22312;&#26368;&#32456;&#23618;&#27425;&#33719;&#21462;&#26356;&#22810;&#20869;&#23481;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.16937</link><description>&lt;p&gt;
SSHR: &#21033;&#29992;&#33258;&#30417;&#30563;&#23618;&#27425;&#34920;&#31034;&#25552;&#39640;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition. (arXiv:2309.16937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23618;&#27425;&#34920;&#31034;&#65288;SSHR&#65289;&#20248;&#21270;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#27425;&#34920;&#31034;&#65292;&#25552;&#21462;&#20986;&#35821;&#35328;&#30456;&#20851;&#24103;&#21644;&#29305;&#23450;&#20869;&#23481;&#65292;&#24182;&#24341;&#23548;&#27169;&#22411;&#22312;&#26368;&#32456;&#23618;&#27425;&#33719;&#21462;&#26356;&#22810;&#20869;&#23481;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22240;&#20854;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#25193;&#23637;&#35821;&#35328;&#35206;&#30422;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#34429;&#28982;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22810;&#35821;&#31181;ASR&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SSL&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#27425;&#34920;&#31034;&#21487;&#33021;&#21253;&#21547;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#19981;&#21516;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23618;&#27425;&#34920;&#31034;&#65288;SSHR&#65289;&#26469;&#20248;&#21270;&#22810;&#35821;&#31181;ASR&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;SSL&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#27425;&#23545;&#20110;&#35821;&#35328;&#30456;&#20851;&#21644;&#20869;&#23481;&#30456;&#20851;&#20449;&#24687;&#30340;&#34920;&#36798;&#24773;&#20917;&#65292;&#21457;&#29616;&#20855;&#26377;&#26356;&#24378;&#30456;&#20851;&#24615;&#30340;&#23618;&#27425;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#20013;&#38388;&#23618;&#20013;&#25552;&#21462;&#35821;&#35328;&#30456;&#20851;&#24103;&#65292;&#24182;&#36890;&#36807;&#33258;&#27880;&#24847;&#26426;&#21046;&#24341;&#23548;&#29305;&#23450;&#20869;&#23481;&#30340;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;&#20132;&#21449;CTC&#26041;&#27861;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#26368;&#32456;&#23618;&#27425;&#33719;&#24471;&#26356;&#22810;&#20869;&#23481;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#65288;C&#65289;&#19978;&#35780;&#20272;&#20102;SSHR&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual automatic speech recognition (ASR) systems have garnered attention for their potential to extend language coverage globally. While self-supervised learning (SSL) has demonstrated its effectiveness in multilingual ASR, it is worth noting that the various layers' representations of SSL potentially contain distinct information that has not been fully leveraged. In this study, we propose a novel method that leverages self-supervised hierarchical representations (SSHR) to fine-tune multilingual ASR. We first analyze the different layers of the SSL model for language-related and content-related information, uncovering layers that show a stronger correlation. Then, we extract a language-related frame from correlated middle layers and guide specific content extraction through self-attention mechanisms. Additionally, we steer the model toward acquiring more content-related information in the final layers using our proposed Cross-CTC. We evaluate SSHR on two multilingual datasets, C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#29992;&#20110;&#23545;&#26032;&#38395;&#23186;&#20307;&#26469;&#28304;&#36827;&#34892;&#29305;&#24449;&#20998;&#26512;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#24418;&#20998;&#26512;&#27169;&#22411;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#27934;&#23519;&#21147;&#65292;&#21487;&#20197;&#24555;&#36895;&#26816;&#27979;&#34394;&#20551;&#21644;&#26377;&#20559;&#35265;&#30340;&#26032;&#38395;&#23186;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.07384</link><description>&lt;p&gt;
&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#29992;&#20110;&#23545;&#26032;&#38395;&#23186;&#20307;&#26469;&#28304;&#36827;&#34892;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Interactive Framework for Profiling News Media Sources. (arXiv:2309.07384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#29992;&#20110;&#23545;&#26032;&#38395;&#23186;&#20307;&#26469;&#28304;&#36827;&#34892;&#29305;&#24449;&#20998;&#26512;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#24418;&#20998;&#26512;&#27169;&#22411;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#27934;&#23519;&#21147;&#65292;&#21487;&#20197;&#24555;&#36895;&#26816;&#27979;&#34394;&#20551;&#21644;&#26377;&#20559;&#35265;&#30340;&#26032;&#38395;&#23186;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#20852;&#36215;&#23548;&#33268;&#20102;&#22823;&#37327;&#34394;&#20551;&#21644;&#26377;&#20559;&#35265;&#30340;&#26032;&#38395;&#20256;&#25773;&#65292;&#21363;&#20197;&#24433;&#21709;&#20449;&#20208;&#20026;&#30446;&#30340;&#30340;&#20869;&#23481;&#21457;&#24067;&#12290;&#34429;&#28982;&#26816;&#27979;&#21644;&#23545;&#20256;&#25773;&#36825;&#20123;&#26032;&#38395;&#30340;&#26469;&#28304;&#36827;&#34892;&#29305;&#24449;&#20998;&#26512;&#23545;&#20110;&#32500;&#25252;&#19968;&#20010;&#20581;&#24247;&#30340;&#31038;&#20250;&#24456;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#29992;&#20110;&#26032;&#38395;&#23186;&#20307;&#30340;&#29305;&#24449;&#20998;&#26512;&#65292;&#23558;&#22522;&#20110;&#22270;&#30340;&#26032;&#38395;&#23186;&#20307;&#29305;&#24449;&#20998;&#26512;&#27169;&#22411;&#12289;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#27934;&#23519;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#34920;&#24449;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#31038;&#20250;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#20165;&#38656;&#20116;&#27425;&#20154;&#31867;&#20132;&#20114;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#24555;&#36895;&#26816;&#27979;&#20986;&#34394;&#20551;&#21644;&#26377;&#20559;&#35265;&#30340;&#26032;&#38395;&#23186;&#20307;&#65292;&#29978;&#33267;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26032;&#38395;&#20107;&#20214;&#20986;&#29616;&#26102;&#65292;&#20854;&#20013;&#30340;&#27979;&#35797;&#25968;&#25454;&#26159;&#26410;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rise of social media has led to the spread of large amounts of fake and biased news, content published with the intent to sway beliefs. While detecting and profiling the sources that spread this news is important to maintain a healthy society, it is challenging for automated systems.  In this paper, we propose an interactive framework for news media profiling. It combines the strengths of graph based news media profiling models, Pre-trained Large Language Models, and human insight to characterize the social context on social media. Experimental results show that with as little as 5 human interactions, our framework can rapidly detect fake and biased news media, even in the most challenging settings of emerging news events, where test data is unseen.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15051</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#22312;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#24739;&#32773;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;TrialGPT&#65292;&#37319;&#29992;LLMs&#39044;&#27979;&#22522;&#20110;&#26631;&#20934;&#30340;&#21512;&#26684;&#24615;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#26469;&#23545;&#20505;&#36873;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#25490;&#21517;&#21644;&#25490;&#38500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;184&#21517;&#24739;&#32773;&#21644;18,238&#20010;&#27880;&#37322;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#38431;&#21015;&#19978;&#35780;&#20272;&#20102;TrialGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;TrialGPT&#22312;&#26631;&#20934;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#31532;&#20108;&#65292;TrialGPT&#30340;&#32508;&#21512;&#35797;&#39564;&#32423;&#21035;&#35780;&#20998;&#19982;&#19987;&#23478;&#26631;&#27880;&#30340;&#21512;&#26684;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.12114</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;ChatGPT&#12289;Flan-T5 UL2&#12289;Tk-Instruct&#21644;Alpaca&#8212;&#8212;&#22312;13&#20010;&#23454;&#38469;&#19990;&#30028;&#30340;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#38382;&#31572;&#65288;QA&#65289;&#12289;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#31561;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#30340;LLM&#24320;&#22987;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;QA&#20219;&#21153;&#34920;&#29616;&#24471;&#29305;&#21035;&#22909;&#65292;&#21363;&#20351;&#23427;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#34920;&#29616;&#20302;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#65288;&#22914;PubMedBERT&#65289;&#21487;&#20197;&#36798;&#21040;&#30340;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#27809;&#26377;&#19968;&#20010;LLM&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#30740;&#31350;&#37325;&#28857;&#36880;&#28176;&#36716;&#21521;&#31038;&#20250;&#24433;&#21709;&#12290;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#30340;&#36235;&#21183;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#27880;&#37325;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.10700</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22609;&#36896;&#24182;&#21463;&#21040;&#31038;&#20250;&#30340;&#24433;&#21709;&#65306;arXiv&#20986;&#29256;&#27169;&#24335;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large language models shape and are shaped by society: A survey of arXiv publication patterns. (arXiv:2307.10700v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10700
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#30740;&#31350;&#37325;&#28857;&#36880;&#28176;&#36716;&#21521;&#31038;&#20250;&#24433;&#21709;&#12290;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#30340;&#36235;&#21183;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#27880;&#37325;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#36817;&#24180;&#26469;&#21576;&#24613;&#21095;&#22686;&#21152;&#65292;&#36825;&#31181;&#21464;&#21270;&#23545;&#31185;&#23398;&#39046;&#22495;&#20135;&#29983;&#20102;&#25103;&#21095;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#35814;&#32454;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;CS&#21644;Stat arXiv&#19978;&#21457;&#24067;&#30340;388K&#31687;&#35770;&#25991;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;2023&#24180;&#19982;2018-2022&#24180;&#20043;&#38388;&#21457;&#34920;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;LLM&#35770;&#25991;&#30340;&#27604;&#20363;&#22686;&#21152;&#24773;&#20917;&#65292;&#24471;&#21040;&#20102;&#26368;&#22810;&#20851;&#27880;&#30340;&#19982;LLM&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#25776;&#20889;LLM&#35770;&#25991;&#30340;&#20316;&#32773;&#65292;&#20316;&#32773;&#30340;&#30740;&#31350;&#20027;&#39064;&#19982;&#32972;&#26223;&#30340;&#30456;&#20851;&#24615;&#65292;&#21306;&#20998;&#39640;&#34987;&#24341;&#29992;LLM&#35770;&#25991;&#30340;&#22240;&#32032;&#65292;&#20197;&#21450;&#22269;&#38469;&#21512;&#20316;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#30740;&#31350;&#36234;&#26469;&#36234;&#20851;&#27880;&#31038;&#20250;&#24433;&#21709;&#65306;&#22312;&#35745;&#31639;&#26426;&#19982;&#31038;&#20250;&#23376;arXiv&#19978;&#65292;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#27604;&#20363;&#22686;&#21152;&#20102;18&#20493;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#20542;&#21521;&#20110;&#20851;&#27880;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;LLM&#30740;&#31350;&#20063;&#21463;&#21040;&#31038;&#20250;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a steep recent increase in the number of large language model (LLM) papers, producing a dramatic shift in the scientific landscape which remains largely undocumented through bibliometric analysis. Here, we analyze 388K papers posted on the CS and Stat arXivs, focusing on changes in publication patterns in 2023 vs. 2018-2022. We analyze how the proportion of LLM papers is increasing; the LLM-related topics receiving the most attention; the authors writing LLM papers; how authors' research topics correlate with their backgrounds; the factors distinguishing highly cited LLM papers; and the patterns of international collaboration. We show that LLM research increasingly focuses on societal impacts: there has been an 18x increase in the proportion of LLM-related papers on the Computers and Society sub-arXiv, and authors newly publishing on LLMs are more likely to focus on applications and societal impacts than more experienced authors. LLM research is also shaped by social dyn
&lt;/p&gt;</description></item><item><title>MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.06281</link><description>&lt;p&gt;
MMBench: &#24744;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20840;&#33021;&#29699;&#21592;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06281
&lt;/p&gt;
&lt;p&gt;
MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#20449;&#24687;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#35780;&#20272;&#36825;&#20123;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#26410;&#26469;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;VQAv2&#25110;COCO Caption&#25552;&#20379;&#20102;&#23450;&#37327;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#20294;&#22312;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#21644;&#38750;&#40065;&#26834;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26368;&#36817;&#30340;&#20027;&#35266;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;OwlEval&#65292;&#36890;&#36807;&#25972;&#21512;&#20154;&#21147;&#36164;&#28304;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20294;&#19981;&#21487;&#25193;&#23637;&#24182;&#19988;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMBench&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;MMBench&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20027;&#35201;&#30001;&#20004;&#20010;&#20803;&#32032;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#20803;&#32032;&#26159;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#35780;&#20272;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#31867;&#20284;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluatio
&lt;/p&gt;</description></item></channel></rss>