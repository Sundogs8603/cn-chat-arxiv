<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16854</link><description>&lt;p&gt;
&#19968;&#20010;&#19987;&#23478;&#20215;&#20540;&#19968;&#20010;&#20195;&#24065;&#65306;&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16854
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#65288;Expert-Token-Routing&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#36890;&#29992;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLM&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#19987;&#23478;LLMs&#34920;&#31034;&#20026;&#20803;LLM&#35789;&#27719;&#20013;&#30340;&#29305;&#27530;&#19987;&#23478;&#20195;&#24065;&#12290;&#20803;LLM&#21487;&#20197;&#36335;&#30001;&#21040;&#19987;&#23478;LLM&#65292;&#23601;&#20687;&#29983;&#25104;&#26032;&#20195;&#24065;&#19968;&#26679;&#12290;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#19981;&#20165;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19987;&#23478;LLMs&#30340;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#12290;&#23427;&#36824;&#21487;&#20197;&#38544;&#34255;&#29992;&#25143;&#35270;&#35282;&#20013;&#30340;&#35814;&#32454;&#21327;&#20316;&#36807;&#31243;&#65292;&#20419;&#36827;&#20132;&#20114;&#23601;&#20687;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;LLM&#19968;&#26679;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#28085;&#30422;&#20845;&#20010;&#19981;&#21516;&#19987;&#23478;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#65292;&#23637;&#29616;&#20102;&#36890;&#36807;&#21327;&#21516;&#22810;&#20010;&#19987;&#23478;LLM&#26469;&#26500;&#24314;&#36890;&#29992;&#22411;LLM&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16854v1 Announce Type: cross  Abstract: We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user's perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#26426;&#21046;&#8220;&#39034;&#24207;&#26500;&#36896;&#20943;&#23569;&#8221;&#65292;&#26088;&#22312;&#23454;&#29616;&#22810;&#25773;&#24182;&#21457;&#36890;&#20449;&#30340;&#30830;&#23450;&#24615;&#65292;&#25193;&#23637;&#20102;CCS&#30340;&#25216;&#26415;&#35774;&#32622;&#65292;&#35777;&#26126;&#20102;&#26500;&#36896;&#20943;&#23569;&#30340;&#27719;&#32858;&#23646;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#19968;&#20123;&#35821;&#27861;&#38480;&#21046;&#19979;&#36816;&#31639;&#31526;&#30340;&#32467;&#26500;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04618</link><description>&lt;p&gt;
&#26102;&#26631;CCS&#20013;&#30340;&#24378;&#20248;&#20808;&#32423;&#21644;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Strong Priority and Determinacy in Timed CCS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04618
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#26426;&#21046;&#8220;&#39034;&#24207;&#26500;&#36896;&#20943;&#23569;&#8221;&#65292;&#26088;&#22312;&#23454;&#29616;&#22810;&#25773;&#24182;&#21457;&#36890;&#20449;&#30340;&#30830;&#23450;&#24615;&#65292;&#25193;&#23637;&#20102;CCS&#30340;&#25216;&#26415;&#35774;&#32622;&#65292;&#35777;&#26126;&#20102;&#26500;&#36896;&#20943;&#23569;&#30340;&#27719;&#32858;&#23646;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#19968;&#20123;&#35821;&#27861;&#38480;&#21046;&#19979;&#36816;&#31639;&#31526;&#30340;&#32467;&#26500;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#20248;&#20808;&#32423;&#30340;&#32463;&#20856;&#36827;&#31243;&#20195;&#25968;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#39034;&#24207;&#26500;&#36896;&#20943;&#23569;&#8221;&#30340;&#26032;&#35843;&#24230;&#26426;&#21046;&#65292;&#26088;&#22312;&#25429;&#25417;&#21516;&#27493;&#32534;&#31243;&#30340;&#26412;&#36136;&#12290;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#30340;&#29420;&#29305;&#23646;&#24615;&#26159;&#36890;&#36807;&#26500;&#36896;&#23454;&#29616;&#22810;&#25773;&#24182;&#21457;&#36890;&#20449;&#30340;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#27169;&#25311;&#20855;&#26377;&#23545;&#32570;&#22833;&#21453;&#24212;&#30340;&#20849;&#20139;&#20869;&#23384;&#22810;&#32447;&#31243;&#65292;&#22240;&#20026;&#23427;&#26159;Esterel&#32534;&#31243;&#35821;&#35328;&#30340;&#26680;&#24515;&#12290;&#22312;&#36890;&#36807;&#26102;&#38047;&#21644;&#20248;&#20808;&#32423;&#25193;&#23637;&#30340;CCS&#30340;&#25216;&#26415;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#25105;&#20204;&#31216;&#20026;&#8220;&#32467;&#26500;&#36830;&#36143;&#8221;&#30340;&#22823;&#31867;&#36807;&#31243;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26500;&#36896;&#20943;&#23569;&#30340;&#27719;&#32858;&#23646;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#22312;&#19968;&#20123;&#31216;&#20026;&#8220;&#21487;&#26530;&#32445;&#8221;&#30340;&#35821;&#27861;&#38480;&#21046;&#19979;&#65292;&#21069;&#32512;&#12289;&#27714;&#21644;&#12289;&#24182;&#34892;&#32452;&#25104;&#12289;&#38480;&#21046;&#21644;&#38544;&#34255;&#30340;&#36816;&#31639;&#31526;&#20445;&#25345;&#32467;&#26500;&#36830;&#36143;&#12290;&#36825;&#28085;&#30422;&#20102;&#19968;&#20010;&#20005;&#26684;&#26356;&#22823;&#30340;&#36807;&#31243;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04618v1 Announce Type: cross  Abstract: Building on the classical theory of process algebra with priorities, we identify a new scheduling mechanism, called "sequentially constructive reduction" which is designed to capture the essence of synchronous programming. The distinctive property of this evaluation strategy is to achieve determinism-by-construction for multi-cast concurrent communication. In particular, it permits us to model shared memory multi-threading with reaction to absence as it lies at the core of the programming language Esterel. In the technical setting of CCS extended by clocks and priorities, we prove for a large class of processes, which we call "structurally coherent" the confluence property for constructive reductions. We further show that under some syntactic restrictions, called "pivotable" the operators of prefix, summation, parallel composition, restriction and hiding preserve structural coherence. This covers a strictly larger class of processes co
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65292;&#22312;Battleship&#28216;&#25103;&#20013;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#21305;&#37197;&#30340;&#25928;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27169;&#22411;&#22914;&#20309;&#25351;&#23548;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.19471</link><description>&lt;p&gt;
&#20005;&#26684;&#30340;LIPS&#27785;&#27809;&#33328;&#33337;&#65306;&#22312;Battleship&#20013;&#20351;&#29992;&#35821;&#35328;&#20449;&#24687;&#31243;&#24207;&#25277;&#26679;&#25552;&#20986;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19471
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65292;&#22312;Battleship&#28216;&#25103;&#20013;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#21305;&#37197;&#30340;&#25928;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27169;&#22411;&#22914;&#20309;&#25351;&#23548;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#32467;&#21512;&#20102;&#25105;&#20204;&#23545;&#35821;&#35328;&#30340;&#25484;&#25569;&#21644;&#25105;&#20204;&#23545;&#20110;&#22312;&#26377;&#38480;&#35748;&#30693;&#36164;&#28304;&#24773;&#20917;&#19979;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#20154;&#20204;&#22914;&#20309;&#22312;&#24040;&#22823;&#20551;&#35774;&#31354;&#38388;&#20013;&#25552;&#20986;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#38382;&#39064;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#22312;&#22522;&#20110;&#25112;&#33328;&#28216;&#25103;Battleship&#30340;&#32463;&#20856;&#25552;&#38382;&#20219;&#21153;&#20013;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#35821;&#35328;&#20449;&#24687;&#31243;&#24207;&#25277;&#26679;&#65288;LIPS&#65289;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#31526;&#21495;&#31243;&#24207;&#65292;&#24182;&#35780;&#20272;&#20854;&#39044;&#26399;&#20449;&#24687;&#22686;&#30410;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#36164;&#28304;&#39044;&#31639;&#19979;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#33945;&#29305;&#21345;&#32599;&#20248;&#21270;&#31574;&#30053;&#20063;&#33021;&#20135;&#29983;&#21453;&#26144;&#20154;&#31867;&#22312;&#21508;&#31181;Battleship&#26827;&#30424;&#22330;&#26223;&#20013;&#34920;&#29616;&#30340;&#20016;&#23500;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20165;&#20351;&#29992;LLM&#30340;&#22522;&#32447;&#22312;&#23558;&#38382;&#39064;&#19982;&#26827;&#30424;&#29366;&#24577;&#32852;&#31995;&#36215;&#26469;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65307;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-4V&#24182;&#27809;&#26377;&#27604;&#26080;&#35270;&#35273;&#22522;&#32447;&#25552;&#20379;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36125;&#21494;&#26031;&#25552;&#38382;&#27169;&#22411;&#22914;&#20309;&#21487;&#33021;&#27169;&#25311;&#21644;&#25351;&#23548;&#20154;&#31867;&#30340;&#38382;&#38382;&#39064;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19471v1 Announce Type: cross  Abstract: Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can l
&lt;/p&gt;</description></item><item><title>&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11355</link><description>&lt;p&gt;
&#25913;&#21464;&#20102;&#20160;&#20040;&#65311;&#23558;&#34920;&#24449;&#24178;&#39044;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
What Changed? Converting Representational Interventions to Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11355
&lt;/p&gt;
&lt;p&gt;
&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#24449;&#31354;&#38388;&#30340;&#24178;&#39044;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#29992;&#26469;&#28040;&#38500;&#25110;&#25913;&#21464;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#32534;&#30721;&#65292;&#21019;&#24314;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24178;&#39044;&#25805;&#20316;&#22312;&#34920;&#31034;&#31354;&#38388;&#20869;&#65292;&#20934;&#30830;&#29702;&#35299;&#23427;&#20462;&#25913;&#20102;&#21738;&#20123;&#29305;&#24449;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23545;&#24212;&#20110;&#32473;&#23450;&#34920;&#31034;&#31354;&#38388;&#24178;&#39044;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#35299;&#37322;&#29992;&#20110;&#32534;&#30721;&#29305;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#29992;&#20110;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.10466</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#22120;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Zero-shot Dialogue State Tracker through Function Calling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#26085;&#30410;&#26222;&#36941;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#22312;&#19968;&#33324;&#24773;&#22659;&#20013;&#20855;&#26377;&#20808;&#36827;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#19981;&#20165;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#36824;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#20869;&#36827;&#34892;&#26377;&#25928;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#65288;DST&#65289;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#35299;&#20915;LLMs&#20013;&#30340;DST&#30340;&#26032;&#26041;&#27861;FnCTOD&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#20102;&#38646;-shot DST&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#24320;&#28304;&#25110;&#19987;&#26377;LLMs&#26102;&#37117;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20351;&#24471;&#21508;&#31181;7B&#25110;13B&#21442;&#25968;&#27169;&#22411;&#36229;&#36234;&#20102;&#20043;&#21069;&#30001;ChatGPT&#23454;&#29616;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#65288;SOTA&#65289;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#20987;&#36133;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10466v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13933</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Structured Probabilistic Coding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13933
&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#65292;&#29992;&#20110;&#20174;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#36755;&#20837;&#20013;&#23398;&#20064;&#32039;&#20945;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;SPC&#26159;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#27010;&#29575;&#32534;&#30721;&#25216;&#26415;&#65292;&#20855;&#26377;&#26469;&#33258;&#30446;&#26631;&#31354;&#38388;&#30340;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27010;&#29575;&#32534;&#30721;&#22312;&#19968;&#20010;&#27169;&#22359;&#20013;&#21516;&#26102;&#36827;&#34892;&#20449;&#24687;&#32534;&#30721;&#21644;&#20219;&#21153;&#39044;&#27979;&#65292;&#20197;&#26356;&#20805;&#20998;&#22320;&#21033;&#29992;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#26377;&#25928;&#20449;&#24687;&#12290;&#23427;&#20351;&#29992;&#36755;&#20986;&#31354;&#38388;&#30340;&#21464;&#20998;&#25512;&#26029;&#26469;&#20943;&#23569;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#25511;&#21046;&#27010;&#29575;&#34920;&#31034;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#65292;&#20197;&#20419;&#36827;&#31867;&#21035;&#20043;&#38388;&#30340;&#22343;&#21248;&#24615;&#12290;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#65292;SPC&#21487;&#20197;&#20445;&#25345;&#28508;&#22312;&#32534;&#30721;&#30340;&#39640;&#26031;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#27169;&#20223;&#20351;&#29992;&#35299;&#30721;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#27700;&#21360;&#30340;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.04469</link><description>&lt;p&gt;
&#35770;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#21487;&#23398;&#20064;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Learnability of Watermarks for Language Models. (arXiv:2312.04469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#27169;&#20223;&#20351;&#29992;&#35299;&#30721;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#27700;&#21360;&#30340;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#27700;&#21360;&#21487;&#20197;&#23454;&#29616;&#23545;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#32479;&#35745;&#26816;&#27979;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#36131;&#20219;&#37096;&#32626;&#20013;&#12290;&#29616;&#26377;&#30340;&#27700;&#21360;&#31574;&#30053;&#36890;&#36807;&#25913;&#21464;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#26469;&#25805;&#20316;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#23558;&#23545;&#27700;&#21360;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#23398;&#20064;&#24471;&#21040;&#30340;&#27700;&#21360;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#33021;&#33258;&#28982;&#29983;&#25104;&#24102;&#27700;&#21360;&#25991;&#26412;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#20351;&#24320;&#25918;&#27169;&#22411;&#20063;&#33021;&#20174;&#27700;&#21360;&#20013;&#21463;&#30410;&#12290;&#20854;&#27425;&#65292;&#22914;&#26524;&#27700;&#21360;&#29992;&#20110;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#26469;&#28304;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#20266;&#36896;&#27700;&#21360;&#24182;&#29983;&#25104;&#26377;&#23475;&#30340;&#24102;&#27700;&#21360;&#25991;&#26412;&#26469;&#25439;&#23475;&#21463;&#23475;&#27169;&#22411;&#30340;&#22768;&#35465;&#12290;&#20026;&#20102;&#30740;&#31350;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#34892;&#20026;&#31867;&#20284;&#20110;&#20351;&#29992;&#22522;&#20110;&#35299;&#30721;&#30340;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermarking of language model outputs enables statistical detection of model-generated text, which has many applications in the responsible deployment of language models. Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;</title><link>http://arxiv.org/abs/2310.07815</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#32034;&#24341;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26631;&#35782;&#31526;&#65288;ID&#65289;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#26088;&#22312;&#20445;&#30041;&#23545;&#35937;&#65288;&#22914;&#25991;&#26723;&#21644;&#39033;&#65289;&#20869;&#37096;&#30340;&#35821;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#23398;&#20064;&#35821;&#20041;ID&#65292;&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#33719;&#21462;&#23884;&#20837;&#65292;&#24182;&#26681;&#25454;&#23884;&#20837;&#26469;&#25512;&#23548;ID&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#19988;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#23884;&#20837;&#20998;&#24067;&#36890;&#24120;&#19982;&#35821;&#20041;&#32034;&#24341;&#25152;&#38656;&#30340;&#39044;&#26399;&#20998;&#24067;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#26082;&#33021;&#23398;&#20064;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#21448;&#33021;&#21516;&#26102;&#23398;&#20064;&#20854;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35821;&#20041;ID&#26159;&#31163;&#25955;&#21644;&#39034;&#24207;&#32467;&#26500;&#30340;&#65292;&#24182;&#19988;&#35821;&#20041;&#30417;&#30563;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LMINDEXER&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;It-LLMs&#65289;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#30340;&#40065;&#26834;&#24615;&#33021;&#21147;&#65292;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12481</link><description>&lt;p&gt;
HANS&#65292;&#20320;&#32874;&#26126;&#21527;&#65311;&#31070;&#32463;&#31995;&#32479;&#30340;Clever Hans&#25928;&#24212;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;It-LLMs&#65289;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#30340;&#40065;&#26834;&#24615;&#33021;&#21147;&#65292;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(It-LLMs)&#23637;&#31034;&#20986;&#20102;&#22312;&#35748;&#30693;&#29366;&#24577;&#12289;&#24847;&#22270;&#21644;&#21453;&#24212;&#26041;&#38754;&#25512;&#29702;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#21487;&#20197;&#35753;&#20154;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#29702;&#35299;&#26085;&#24120;&#31038;&#20132;&#20114;&#21160;&#12290;&#20107;&#23454;&#19978;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;(MCQ)&#22522;&#20934;&#26469;&#26500;&#24314;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#30830;&#20999;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;It-LLMs&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#8220;&#39034;&#24207;&#20559;&#35265;&#8221;&#65292;&#32473;&#36866;&#24403;&#30340;&#35780;&#20272;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;MCQ&#22522;&#20934;&#23545;It-LLMs&#30340;&#25269;&#25239;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#65292;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#24182;&#24341;&#21457;&#20102;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#35752;&#35770;&#12290;&#36890;&#36807;&#31532;&#19968;&#20301;&#32622;&#21644;&#27169;&#22411;&#36873;&#25321;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#27169;&#22411;&#20013;&#23384;&#22312;&#32467;&#26500;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (It-LLMs) have been exhibiting outstanding abilities to reason around cognitive states, intentions, and reactions of all people involved, letting humans guide and comprehend day-to-day social interactions effectively. In fact, several multiple-choice questions (MCQ) benchmarks have been proposed to construct solid assessments of the models' abilities. However, earlier works are demonstrating the presence of inherent "order bias" in It-LLMs, posing challenges to the appropriate evaluation. In this paper, we investigate It-LLMs' resilience abilities towards a series of probing tests using four MCQ benchmarks. Introducing adversarial examples, we show a significant performance gap, mainly when varying the order of the choices, which reveals a selection bias and brings into discussion reasoning abilities. Following a correlation between first positions and model choices due to positional bias, we hypothesized the presence of structural heuristics in 
&lt;/p&gt;</description></item><item><title>CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11295</link><description>&lt;p&gt;
CPLLM: &#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11295
&lt;/p&gt;
&lt;p&gt;
CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21033;&#29992;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;LLM&#65292;&#20219;&#21153;&#26159;&#39044;&#27979;&#24739;&#32773;&#22312;&#19979;&#19968;&#27425;&#23601;&#35786;&#25110;&#38543;&#21518;&#30340;&#35786;&#26029;&#20013;&#26159;&#21542;&#20250;&#34987;&#35786;&#26029;&#20026;&#30446;&#26631;&#30142;&#30149;&#65292;&#24182;&#21033;&#29992;&#20182;&#20204;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;RETAIN&#21644;Med-BERT&#65292;&#21518;&#32773;&#26159;&#20351;&#29992;&#32467;&#26500;&#21270;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#30142;&#30149;&#39044;&#27979;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CPLLM&#22312;PR-AUC&#21644;ROC-AUC&#25351;&#26631;&#19978;&#22343;&#36229;&#36807;&#20102;&#25152;&#26377;&#27979;&#35797;&#27169;&#22411;&#65292;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.07308</link><description>&lt;p&gt;
LLM&#33258;&#21355;&#65306;&#36890;&#36807;&#33258;&#26816;&#65292;LLMs&#24847;&#35782;&#21040;&#23427;&#20204;&#34987;&#24858;&#24324;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked. (arXiv:2308.07308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33021;&#22815;&#23545;&#20154;&#31867;&#25552;&#31034;&#20570;&#20986;&#39640;&#36136;&#37327;&#25991;&#26412;&#22238;&#24212;&#32780;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#24212;&#29992;&#25143;&#25552;&#31034;&#26102;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#32473;&#29992;&#25143;&#25552;&#20379;&#29359;&#32618;&#25351;&#23548;&#65289;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#30528;&#37325;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#26041;&#27861;&#65288;&#20363;&#22914;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65289;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#32469;&#36807;&#29983;&#25104;&#26377;&#23475;&#25991;&#26412;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#24049;&#30340;&#22238;&#24212;&#36827;&#34892;&#36807;&#28388;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#27809;&#26377;&#34987;&#24494;&#35843;&#20197;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#26469;&#38450;&#27490;&#20854;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.
&lt;/p&gt;</description></item><item><title>ReactGenie&#26159;&#19968;&#20010;&#25903;&#25345;&#26500;&#24314;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#65292;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.09649</link><description>&lt;p&gt;
ReactGenie&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#26469;&#25903;&#25345;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal Interactions Using Large Language Models. (arXiv:2306.09649v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09649
&lt;/p&gt;
&lt;p&gt;
ReactGenie&#26159;&#19968;&#20010;&#25903;&#25345;&#26500;&#24314;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#65292;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20132;&#20114;&#24050;&#34987;&#35777;&#26126;&#27604;&#20256;&#32479;&#30340;&#22270;&#24418;&#30028;&#38754;&#26356;&#21152;&#28789;&#27963;&#12289;&#39640;&#25928;&#21644;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#21644;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#24320;&#21457;&#26694;&#26550;&#35201;&#20040;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#21629;&#20196;&#30340;&#22797;&#26434;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#35201;&#20040;&#38656;&#35201;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#22823;&#37327;&#20195;&#30721;&#26469;&#25903;&#25345;&#36825;&#20123;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ReactGenie&#65292;&#36825;&#26159;&#19968;&#20010;&#32534;&#31243;&#26694;&#26550;&#65292;&#20351;&#29992;&#20849;&#20139;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#26469;&#25903;&#25345;&#26500;&#24314;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#12290;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#20849;&#20139;&#29366;&#24577;&#25277;&#35937;&#20351;&#24471;&#20351;&#29992;ReactGenie&#30340;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#21644;&#32452;&#21512;&#36825;&#20123;&#27169;&#24577;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;ReactGenie&#26159;&#26500;&#24314;&#22270;&#24418;&#24212;&#29992;&#31243;&#24207;&#30340;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#30340;&#33258;&#28982;&#25193;&#23637;&#65292;&#23601;&#20687;&#20351;&#29992;React-Redux&#19968;&#26679;&#12290;&#24320;&#21457;&#20154;&#21592;&#21482;&#38656;&#35201;&#28155;&#21152;&#19968;&#20123;&#27880;&#37322;&#21644;&#31034;&#20363;&#26469;&#25351;&#31034;&#33258;&#28982;&#35821;&#35328;&#22914;&#20309;&#26144;&#23556;&#21040;&#29992;&#25143;&#21487;&#35775;&#38382;&#30340;&#29366;&#24577;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal interactions have been shown to be more flexible, efficient, and adaptable for diverse users and tasks than traditional graphical interfaces. However, existing multimodal development frameworks either do not handle the complexity and compositionality of multimodal commands well or require developers to write a substantial amount of code to support these multimodal interactions. In this paper, we present ReactGenie, a programming framework that uses a shared object-oriented state abstraction to support building complex multimodal mobile applications. Having different modalities share the same state abstraction allows developers using ReactGenie to seamlessly integrate and compose these modalities to deliver multimodal interaction.  ReactGenie is a natural extension to the existing workflow of building a graphical app, like the workflow with React-Redux. Developers only have to add a few annotations and examples to indicate how natural language is mapped to the user-accessible
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04634</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24320;&#22987;&#24212;&#29992;&#20110;&#26085;&#24120;&#20351;&#29992;&#65292;&#24182;&#26377;&#33021;&#21147;&#22312;&#26410;&#26469;&#30340;&#21313;&#24180;&#20869;&#20135;&#29983;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#21462;&#20195;&#20114;&#32852;&#32593;&#19978;&#30340;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#65292;&#24182;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65292;&#22914;&#38035;&#40060;&#25915;&#20987;&#21644;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12290;&#27700;&#21360;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#26816;&#27979;&#21644;&#21487;&#35760;&#24405;&#65292;&#26469;&#38477;&#20302;&#36825;&#20123;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#22312;&#29616;&#23454;&#20013;&#28151;&#21512;&#20102;&#20854;&#20182;&#30340;&#25991;&#26412;&#26469;&#28304;&#65292;&#34987;&#20154;&#31867;&#20889;&#20316;&#32773;&#25110;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#25913;&#20889;&#65292;&#34987;&#29992;&#20110;&#31038;&#20132;&#21644;&#25216;&#26415;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#26102;&#65292;&#27700;&#21360;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#22914;&#20309;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#37327;&#21270;&#20102;&#23427;&#20204;&#26816;&#27979;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#22312;&#27599;&#20010;&#24773;&#20917;&#19979;&#38656;&#35201;&#35266;&#23519;&#22810;&#23569;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#25165;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#27700;&#21360;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#24403;&#27700;&#21360;&#19982;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#28151;&#21512;&#26102;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#27700;&#21360;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#27700;&#21360;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#25991;&#26412;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#23884;&#20837;&#20449;&#21495;&#65292;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#35813;&#25216;&#26415;&#21313;&#20998;&#40065;&#26834;&#21644;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2301.10226</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Watermark for Large Language Models. (arXiv:2301.10226v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#27700;&#21360;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#25991;&#26412;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#23884;&#20837;&#20449;&#21495;&#65292;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#35813;&#25216;&#26415;&#21313;&#20998;&#40065;&#26834;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20449;&#21495;&#65292;&#21363;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#22411;&#36755;&#20986;&#65292;&#21487;&#20197;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#22312;&#30340;&#21361;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#26694;&#26550;&#12290;&#27700;&#21360;&#21487;&#20197;&#23884;&#20837;&#21040;&#25991;&#26412;&#20013;&#65292;&#23545;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#22312;&#19981;&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;API&#25110;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26816;&#27979;&#12290;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#22312;&#29983;&#25104;&#21333;&#35789;&#20043;&#21069;&#36873;&#25321;&#19968;&#32452;&#38543;&#26426;&#30340;&#8220;&#32511;&#33394;&#8221;&#26631;&#35760;&#65292;&#28982;&#21518;&#22312;&#25277;&#26679;&#36807;&#31243;&#20013;&#36719;&#24615;&#22320;&#25512;&#24191;&#20351;&#29992;&#36825;&#20123;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;P&#20540;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#27700;&#21360;&#25216;&#26415;&#65292; &#24182;&#25512;&#23548;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#27700;&#21360;&#25216;&#26415;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;Open Pretrained Transformer&#65288;OPT&#65289;&#23478;&#26063;&#30340;&#19968;&#20010;&#25968;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#26469;&#27979;&#35797;&#27700;&#21360;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.
&lt;/p&gt;</description></item></channel></rss>