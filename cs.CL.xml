<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.01299</link><description>&lt;p&gt;
CausalChaos!&#25968;&#25454;&#38598;&#65306;&#22522;&#20110;&#21160;&#24577;&#35270;&#35273;&#22330;&#26223;&#20013;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#20840;&#38754;&#22240;&#26524;&#34892;&#21160;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01299
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#22240;&#26524;&#25512;&#29702;&#20998;&#26512;&#26041;&#38754;&#24448;&#24448;&#32570;&#20047;&#28145;&#24230;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21033;&#29992;&#21345;&#36890;&#30340;&#29420;&#29305;&#23646;&#24615;&#26500;&#24314;&#20102;CausalChaos!&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22240;&#26524;&#38382;&#31572;&#65288;Why-QA&#65289;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#26631;&#24535;&#24615;&#30340;&#8220;&#29483;&#21644;&#32769;&#40736;&#8221;&#21345;&#36890;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21608;&#21040;&#30340;&#38382;&#39064;&#21644;&#22810;&#23618;&#27425;&#31572;&#26696;&#65292;&#21253;&#21547;&#30528;&#23884;&#20837;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#20013;&#30340;&#26356;&#38271;&#22240;&#26524;&#38142;&#65292;&#21516;&#26102;&#21160;&#30011;&#21407;&#29702;&#20801;&#35768;&#21160;&#30011;&#24072;&#21019;&#36896;&#23450;&#20041;&#26126;&#30830;&#12289;&#26126;&#20102;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#20123;&#22240;&#32032;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#30828;&#36127;&#37319;&#26679;&#65292;&#21253;&#25324;CausalConfusion&#29256;&#26412;&#12290;&#34429;&#28982;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#24335;&#31572;&#26696;&#26041;&#38754;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20026;&#20808;&#36827;/&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#24314;&#27169;&#21644;&#32852;&#21512;&#24314;&#27169;&#31561;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01299v1 Announce Type: cross  Abstract: Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of 
&lt;/p&gt;</description></item><item><title>VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16973</link><description>&lt;p&gt;
VoiceCraft&#65306;&#37326;&#22806;&#38646;-shot&#35821;&#38899;&#32534;&#36753;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16973
&lt;/p&gt;
&lt;p&gt;
VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;VoiceCraft&#65292;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#22768;&#20070;&#12289;&#20114;&#32852;&#32593;&#35270;&#39057;&#21644;&#25773;&#23458;&#19978;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;VoiceCraft&#37319;&#29992;Transformer&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#35760;&#37325;&#25490;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#21644;&#24310;&#36831;&#22534;&#21472;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#26377;&#24207;&#21015;&#20869;&#30340;&#29983;&#25104;&#12290;&#22312;&#35821;&#38899;&#32534;&#36753;&#20219;&#21153;&#19978;&#65292;VoiceCraft&#29983;&#25104;&#30340;&#32534;&#36753;&#35821;&#38899;&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#20960;&#20046;&#19982;&#26410;&#32534;&#36753;&#30340;&#24405;&#38899;&#38590;&#20197;&#21306;&#20998;&#65292;&#32463;&#20154;&#31867;&#35780;&#20272;&#65307;&#23545;&#20110;&#38646;-shot TTS&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21253;&#25324;VALLE&#21644;&#27969;&#34892;&#30340;&#21830;&#19994;&#27169;&#22411;XTTS-v2&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#26679;&#21475;&#38899;&#12289;&#35821;&#38899;&#39118;&#26684;&#12289;&#24405;&#21046;&#26465;&#20214;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#38899;&#20048;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30495;&#23454;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#22987;&#32456;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16973v1 Announce Type: cross  Abstract: We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#20026;&#30740;&#31350;&#35770;&#25991;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.15529</link><description>&lt;p&gt;
LimGen: &#25506;&#31350;&#29992;&#20110;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#24314;&#35758;&#24615;&#23616;&#38480;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#20026;&#30740;&#31350;&#35770;&#25991;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#23616;&#38480;&#26159;&#23398;&#26415;&#30740;&#31350;&#35780;&#23457;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#25581;&#31034;&#20102;&#30740;&#31350;&#21487;&#33021;&#32570;&#20047;&#20915;&#23450;&#24615;&#25110;&#38656;&#35201;&#21152;&#24378;&#30340;&#26041;&#38754;&#12290;&#36825;&#26377;&#21161;&#20110;&#35835;&#32773;&#32771;&#34385;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#30740;&#31350;&#35770;&#25991;&#24314;&#35758;&#24615;&#23616;&#38480;&#29983;&#25104;&#65288;SLG&#65289;&#30340;&#19968;&#39033;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21517;&#20026;LimGen&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;ACL&#25991;&#38598;&#30340;4068&#31687;&#30740;&#31350;&#35770;&#25991;&#21450;&#20854;&#30456;&#20851;&#23616;&#38480;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#24443;&#24213;&#30740;&#31350;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;LimGen&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/armbf/LimGen &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15529v1 Announce Type: cross  Abstract: Examining limitations is a crucial step in the scholarly research reviewing process, revealing aspects where a study might lack decisiveness or require enhancement. This aids readers in considering broader implications for further research. In this article, we present a novel and challenging task of Suggestive Limitation Generation (SLG) for research papers. We compile a dataset called LimGen, encompassing 4068 research papers and their associated limitations from the ACL anthology. We investigate several approaches to harness large language models (LLMs) for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities. Our LimGen dataset and code can be accessed at https://github.com/armbf/LimGen.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30524;&#25511;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#38477;&#20302;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;</title><link>https://arxiv.org/abs/2403.12416</link><description>&lt;p&gt;
&#38024;&#23545;&#25918;&#23556;&#23398;&#30340;&#30524;&#25511;&#24341;&#23548;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Eye-gaze Guided Multi-modal Alignment Framework for Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12416
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30524;&#25511;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#38477;&#20302;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#26694;&#26550;&#20013;&#65292;&#36328;&#27169;&#24577;&#29305;&#24449;&#30340;&#23545;&#40784;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24378;&#35843;&#20840;&#23616;&#25110;&#23616;&#37096;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#22312;&#25918;&#23556;&#23398;&#20013;&#24120;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#35786;&#26029;&#35780;&#20272;&#36807;&#31243;&#20013;&#21516;&#27493;&#25910;&#38598;&#30340;&#30524;&#25511;&#25968;&#25454;&#65292;&#23558;&#33016;&#37096;X&#32447;&#33258;&#28982;&#22320;&#19982;&#35786;&#26029;&#25991;&#26412;&#30456;&#20851;&#32852;&#65292;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#26088;&#22312;&#20943;&#23569;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12416v1 Announce Type: cross  Abstract: In multi-modal frameworks, the alignment of cross-modal features presents a significant challenge. The predominant approach in multi-modal pre-training emphasizes either global or local alignment between modalities, utilizing extensive datasets. This bottom-up driven method often suffers from a lack of interpretability, a critical concern in radiology. Previous studies have integrated high-level labels in medical images or text, but these still rely on manual annotation, a costly and labor-intensive process. Our work introduces a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. This data, indicating radiologists' focus areas, naturally links chest X-rays to diagnostic texts. We propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of image and text features, aiming to reduce reliance on manual annotations and thus cut
&lt;/p&gt;</description></item><item><title>StableToolBench&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#65292;&#36890;&#36807;&#32531;&#23384;&#31995;&#32479;&#12289;API&#27169;&#25311;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31283;&#23450;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07714</link><description>&lt;p&gt;
StableToolBench&#65306;&#38754;&#21521;&#22823;&#35268;&#27169;&#31283;&#23450;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20855;&#23398;&#20064;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07714
&lt;/p&gt;
&lt;p&gt;
StableToolBench&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#65292;&#36890;&#36807;&#32531;&#23384;&#31995;&#32479;&#12289;API&#27169;&#25311;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31283;&#23450;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#20351;&#20154;&#20204;&#25506;&#32034;&#24037;&#20855;&#23398;&#20064;&#65292;&#23558;LLMs&#19982;&#22806;&#37096;&#24037;&#20855;&#25972;&#21512;&#20197;&#35299;&#20915;&#21508;&#31181;&#29616;&#23454;&#25361;&#25112;&#12290;&#35780;&#20272;LLMs&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#38656;&#35201;&#22823;&#35268;&#27169;&#19988;&#31283;&#23450;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;ToolBench&#28436;&#21464;&#32780;&#26469;&#30340;StableToolBench&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#12290;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21253;&#21547;&#32531;&#23384;&#31995;&#32479;&#21644;API&#27169;&#25311;&#22120;&#65292;&#20114;&#34917;&#20943;&#36731;API&#29366;&#24577;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#31283;&#23450;&#30340;&#35780;&#20272;&#31995;&#32479;&#20351;&#29992;GPT-4&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#35774;&#35745;&#21487;&#35299;&#20915;&#30340;&#36890;&#36807;&#29575;&#21644;&#32988;&#29575;&#65292;&#20197;&#28040;&#38500;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07714v1 Announce Type: new  Abstract: Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06064</link><description>&lt;p&gt;
L$^2$GC: &#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#29992;&#20110;&#23545;&#22270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32447;&#24615;GCN&#27169;&#22411;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#65292;&#36825;&#24182;&#27809;&#26377;&#26126;&#30830;&#25429;&#25417;&#21040;&#20316;&#20026;&#22270;&#27169;&#22411;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#31867;&#20284;&#26641;&#29366;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#23581;&#35797;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;GCN&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22270;&#33410;&#28857;&#30340;&#23398;&#20064;&#29305;&#24449;&#26144;&#23556;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27931;&#20262;&#20857;&#32447;&#24615;&#29305;&#24449;&#21464;&#25442;&#65292;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#28508;&#22312;&#26641;&#29366;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#24341;&#25991;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Citeseer&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;74.7%&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;PubMed&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;81.3%&#30340;&#20934;&#30830;&#24230;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#33267;&#23569;&#36798;&#21040;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06064v1 Announce Type: cross  Abstract: Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and 81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitu
&lt;/p&gt;</description></item><item><title>Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.05530</link><description>&lt;p&gt;
Gemini 1.5&#65306;&#35299;&#38145;&#36328;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05530
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gemini&#23478;&#26063;&#30340;&#26368;&#26032;&#27169;&#22411;Gemini 1.5 Pro&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22238;&#24518;&#21644;&#25512;&#29702;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#20013;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#20010;&#38271;&#25991;&#26723;&#21644;&#20960;&#23567;&#26102;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#12290;Gemini 1.5 Pro&#22312;&#21508;&#31181;&#24418;&#24335;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#24191;&#27867;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;Gemini 1.0 Ultra&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36807;&#12290;&#22312;&#30740;&#31350;Gemini 1.5 Pro&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#26497;&#38480;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33267;&#23569;10M&#26631;&#35760;&#30340;&#33539;&#22260;&#20869;&#32487;&#32493;&#25913;&#36827;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#20960;&#20046;&#23436;&#32654;&#22320;&#36798;&#21040;&#20102;&#36229;&#36807;99%&#30340;&#26816;&#32034;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#27169;&#22411;&#22914;Claude 2.1&#65288;200k&#65289;&#21644;GPT-4 Turbo&#65288;128k&#65289;&#30340;&#19990;&#20195;&#24615;&#39134;&#36291;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05530v1 Announce Type: cross  Abstract: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#65292;&#21253;&#25324;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#12289;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#12289;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12289;&#24635;&#32467;&#23545;&#35805;&#20197;&#21450;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#35780;&#20215;&#25351;&#26631;PREFS&#12290;</title><link>https://arxiv.org/abs/2403.03823</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Modular Approach for Multimodal Summarization of TV Shows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03823
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#65292;&#21253;&#25324;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#12289;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#12289;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12289;&#24635;&#32467;&#23545;&#35805;&#20197;&#21450;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#35780;&#20215;&#25351;&#26631;PREFS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#21040;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65306;&#22797;&#26434;&#25512;&#29702;&#12289;&#22810;&#27169;&#24577;&#21644;&#38271;&#31687;&#21465;&#20107;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#21508;&#20010;&#32452;&#20214;&#25191;&#34892;&#19987;&#38376;&#30340;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#35748;&#20026;&#19982;&#31471;&#21040;&#31471;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#28041;&#21450;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#65292;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#20197;&#23613;&#37327;&#20943;&#23569;&#19981;&#21516;&#20107;&#20214;&#20043;&#38388;&#30340;&#20999;&#25442;&#27425;&#25968;&#65292;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#24635;&#32467;&#27599;&#20010;&#22330;&#26223;&#20013;&#30340;&#23545;&#35805;&#65292;&#24182;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#25104;&#25972;&#38598;&#30340;&#26368;&#32456;&#25688;&#35201;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;PREFS&#65288;&#25688;&#35201;&#20107;&#23454;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#65289;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#12290;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;SummScreen3D&#25968;&#25454;&#38598;Papalampidi&#21644;Lapata&#65288;2023&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03823v1 Announce Type: new  Abstract: In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (\textbf{P}recision and \textbf{R}ecall \textbf{E}valuation of Summary \textbf{F}act\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces hi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00252</link><description>&lt;p&gt;
EUROPA&#65306;&#19968;&#20010;&#27861;&#24459;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EUROPA: A Legal Multilingual Keyphrase Generation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#20027;&#35201;&#22312;&#23398;&#26415;&#30740;&#31350;&#25991;&#31456;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#25506;&#32034;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#31185;&#23398;&#39046;&#22495;&#21644;&#33521;&#35821;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EUROPA&#65292;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290; &#23427;&#28304;&#33258;&#27431;&#27954;&#27861;&#38498;&#30340;&#27861;&#24459;&#21028;&#20915;&#65292;&#24182;&#21253;&#21547;&#20102;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#20013;&#30340;&#23454;&#20363;&#12290; &#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#19978;&#36816;&#34892;&#22810;&#35821;&#35328;&#27169;&#22411;&#24182;&#20998;&#26512;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20687;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00252v1 Announce Type: cross  Abstract: Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.18409</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22270;&#20687;&#25512;&#29702;&#21644;&#25551;&#36848;&#30340;&#35748;&#30693;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#21463;&#21040;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#27979;&#35797;&#12290;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#27979;&#35797;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#8220;&#20599;&#39292;&#24178;&#8221;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#20855;&#26377;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#20687;&#35780;&#20272;LVLMs&#30340;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#23427;&#23450;&#20041;&#20102;&#20843;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21253;&#25324;&#22270;&#20687;&#25551;&#36848;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;&#30693;&#21517;LVLMs&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;LVLMs&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18409v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25925;&#20107;&#35762;&#36848;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#25945;&#32946;&#26041;&#27861;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#27861;&#24459;&#25925;&#20107;&#21644;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17019</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35762;&#25925;&#20107;&#23398;&#20064;&#22797;&#26434;&#27861;&#24459;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17019
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25925;&#20107;&#35762;&#36848;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#25945;&#32946;&#26041;&#27861;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#27861;&#24459;&#25925;&#20107;&#21644;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27861;&#24459;&#30693;&#35782;&#21464;&#24471;&#26356;&#23481;&#26131;&#29702;&#35299;&#23545;&#20110;&#25552;&#21319;&#26222;&#36890;&#27861;&#24459;&#32032;&#20859;&#21644;&#40723;&#21169;&#20844;&#27665;&#21442;&#19982;&#27665;&#20027;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27809;&#26377;&#27861;&#24459;&#32972;&#26223;&#30340;&#20154;&#26469;&#35828;&#65292;&#27861;&#24459;&#25991;&#20214;&#36890;&#24120;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27861;&#24459;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#36890;&#36807;&#35762;&#25925;&#20107;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#35762;&#25925;&#20107;&#26159;&#20256;&#36798;&#22797;&#26434;&#21644;&#25277;&#35937;&#27010;&#24565;&#30340;&#26377;&#25928;&#25945;&#23398;&#24037;&#20855;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LegalStories&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;295&#20010;&#22797;&#26434;&#30340;&#27861;&#24459;&#21407;&#21017;&#65292;&#27599;&#20010;&#21407;&#21017;&#37117;&#38468;&#26377;&#19968;&#20010;&#25925;&#20107;&#21644;&#19968;&#32452;&#30001;LLMs&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#20026;&#20102;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#21508;&#31181;LLMs&#29983;&#25104;&#35299;&#37322;&#36825;&#20123;&#27010;&#24565;&#30340;&#27861;&#24459;&#25925;&#20107;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#23478;&#21442;&#19982;&#30340;&#26041;&#27861;&#26469;&#36845;&#20195;&#35774;&#35745;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17019v1 Announce Type: new  Abstract: Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through an 
&lt;/p&gt;</description></item><item><title>INSTRAUG&#26159;&#19968;&#31181;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#27169;&#20219;&#21153;&#20013;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.14492</link><description>&lt;p&gt;
INSTRAUG&#65306;&#29992;&#20110;&#22810;&#27169;&#25351;&#20196;&#24494;&#35843;&#30340;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14492
&lt;/p&gt;
&lt;p&gt;
INSTRAUG&#26159;&#19968;&#31181;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#27169;&#20219;&#21153;&#20013;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20219;&#21153;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#23545;&#26032;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#26368;&#36817;&#20851;&#20110;&#39640;&#36136;&#37327;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#29983;&#25104;&#21644;&#36873;&#25321;&#30340;&#24037;&#20316;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#65292;&#20197;&#20026;&#32473;&#23450;&#20219;&#21153;&#26500;&#24605;&#27169;&#22411;&#21487;&#29702;&#35299;&#30340;&#25351;&#20196;&#65292;&#24182;&#35880;&#24910;&#36807;&#28388;LLM&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;INSTRAUG&#30340;&#22810;&#27169;&#20219;&#21153;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#12290;&#23427;&#20174;&#19968;&#20123;&#22522;&#26412;&#21644;&#31616;&#21333;&#30340;&#20803;&#25351;&#20196;&#24320;&#22987;&#65292;&#20294;&#33021;&#23558;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#25193;&#22823;30&#20493;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22810;&#27169;&#25351;&#20196;&#36319;&#38543;&#22522;&#20934;&#27979;&#35797;&#38598;MULTIINSTRUCT&#21644;InstructBLIP&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;INSTRAUG&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#36328;12&#20010;&#22810;&#27169;&#20219;&#21153;&#30340;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#23545;&#40784;&#65292;&#29978;&#33267;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14492v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) on multi-task instruction-following data has been proven to be a powerful learning paradigm for improving their zero-shot capabilities on new tasks. Recent works about high-quality instruction-following data generation and selection require amounts of human labor to conceive model-understandable instructions for the given tasks and carefully filter the LLM-generated data. In this work, we introduce an automatic instruction augmentation method named INSTRAUG in multimodal tasks. It starts from a handful of basic and straightforward meta instructions but can expand an instruction-following dataset by 30 times. Results on two popular multimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show that INSTRAUG can significantly improve the alignment of multimodal large language models (MLLMs) across 12 multimodal tasks, which is even equivalent to the benefits of scaling up training 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#65288;MORE&#65289;&#22686;&#24378;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#26469;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13625</link><description>&lt;p&gt;
MORE: &#22810;&#27169;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#24335;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#65288;MORE&#65289;&#22686;&#24378;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#26469;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#38590;&#20197;&#23398;&#20064;&#36275;&#22815;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#22240;&#20026;&#24120;&#35782;&#20449;&#24687;&#30340;&#35760;&#24405;&#39057;&#29575;&#26126;&#26174;&#20302;&#20110;&#20854;&#23384;&#22312;&#39057;&#29575;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#24120;&#35782;&#33021;&#21147;&#65292;&#19968;&#20123;&#30740;&#31350;&#21033;&#29992;&#25991;&#26412;&#26816;&#32034;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#19981;&#21516;&#20110;&#25991;&#26412;&#65292;&#22270;&#20687;&#22266;&#26377;&#22320;&#21253;&#21547;&#24120;&#35782;&#20449;&#24687;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#33268;&#21147;&#20110;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#65288;MORE&#65289;&#22686;&#24378;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#26469;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#33021;&#21147;&#12290;&#22312;Common-Gen&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#21333;&#19968;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;MORE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13625v1 Announce Type: new  Abstract: Since commonsense information has been recorded significantly less frequently than its existence, language models pre-trained by text generation have difficulty to learn sufficient commonsense knowledge. Several studies have leveraged text retrieval to augment the models' commonsense ability. Unlike text, images capture commonsense information inherently but little effort has been paid to effectively utilize them. In this work, we propose a novel Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and images to enhance the commonsense ability of language models. Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities.
&lt;/p&gt;</description></item><item><title>FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10986</link><description>&lt;p&gt;
FinTral&#65306;&#19968;&#31867;GPT-4&#32423;&#21035;&#30340;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10986
&lt;/p&gt;
&lt;p&gt;
FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;FinTral&#65292;&#36825;&#26159;&#19968;&#32452;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#26500;&#24314;&#30340;&#19968;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19987;&#38376;&#20026;&#37329;&#34701;&#20998;&#26512;&#23450;&#21046;&#12290;FinTral&#25972;&#21512;&#20102;&#25991;&#26412;&#12289;&#25968;&#23383;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20026;&#26412;&#30740;&#31350;&#31574;&#21010;&#30340;&#22823;&#37327;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#12289;&#25351;&#23548;&#24494;&#35843;&#21644;RLAIF&#35757;&#32451;&#22686;&#24378;&#20102;FinTral&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20061;&#20010;&#20219;&#21153;&#21644;25&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#37329;&#34701;&#39046;&#22495;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;FinTral&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20808;&#36827;&#30340;&#24037;&#20855;&#21644;&#26816;&#32034;&#26041;&#27861;&#36827;&#34892;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#35757;&#32451;&#65292;&#21629;&#21517;&#20026;FinTral-DPO-T&amp;R&#65292;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#23427;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;ChatGPT-3.5&#65292;&#24182;&#22312;&#20061;&#39033;&#20219;&#21153;&#20013;&#30340;&#20116;&#39033;&#20013;&#36229;&#36234;GPT-4&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#37329;&#34701;&#25216;&#26415;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;FinTral&#20855;&#26377;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10986v1 Announce Type: cross  Abstract: We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&amp;R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to e
&lt;/p&gt;</description></item><item><title>QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.09739</link><description>&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;QuRating&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QuRating: Selecting High-Quality Data for Training Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09739
&lt;/p&gt;
&lt;p&gt;
QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#21019;&#24314;&#33021;&#21147;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#24456;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QuRating&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22235;&#20010;&#29305;&#24449; - &#20889;&#20316;&#39118;&#26684;&#12289;&#25152;&#38656;&#19987;&#19994;&#30693;&#35782;&#12289;&#20107;&#23454;&#21644;&#29712;&#20107;&#20197;&#21450;&#25945;&#32946;&#20215;&#20540;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36776;&#21035;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#36827;&#34892;&#25991;&#26412;&#30340;&#37197;&#23545;&#21028;&#26029;&#26041;&#38754;&#27604;&#30452;&#25509;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#26356;&#22909;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;QuRater&#27169;&#22411;&#65292;&#20174;&#37197;&#23545;&#21028;&#26029;&#20013;&#23398;&#20064;&#26631;&#37327;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#23427;&#20026;260B&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#27599;&#20010;&#26631;&#20934;&#36827;&#34892;&#36136;&#37327;&#35780;&#32423;&#27880;&#37322;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#36136;&#37327;&#35780;&#32423;&#36873;&#25321;&#20102;30B&#20010;&#20196;&#29260;&#65292;&#24182;&#22312;&#25152;&#36873;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;13&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09739v1 Announce Type: new  Abstract: Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts &amp; trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and di
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#21382;&#21490;&#24207;&#21015;&#30340;&#25361;&#25112;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#26368;&#39640;&#21487;&#20943;&#23569;89.43%&#30340;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.09573</link><description>&lt;p&gt;
&#34676;&#34678;&#24341;&#36215;&#30340;&#21464;&#21270;&#65306;&#21033;&#29992;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#36827;&#34892;&#36828;&#35265;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09573
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#21382;&#21490;&#24207;&#21015;&#30340;&#25361;&#25112;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#26368;&#39640;&#21487;&#20943;&#23569;89.43%&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#27788;&#20013;&#65292;&#20004;&#20010;&#21021;&#22987;&#26465;&#20214;&#20043;&#38388;&#30340;&#24494;&#23567;&#24046;&#24322;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21576;&#25351;&#25968;&#32423;&#25918;&#22823;&#65292;&#23548;&#33268;&#36965;&#36828;&#30340;&#32467;&#26524;&#65292;&#20063;&#34987;&#31216;&#20026;&#34676;&#34678;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#36828;&#26399;&#20805;&#28385;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#38590;&#20197;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#26469;&#36890;&#36807;&#20811;&#26381;&#28151;&#27788;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65288;1&#65289;&#22823;&#37327;&#30340;&#21382;&#21490;&#24207;&#21015;&#21644;&#65288;2&#65289;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#26469;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#23558;&#19968;&#20010;&#20648;&#22791;&#35013;&#32622;&#36830;&#25509;&#21040;&#36716;&#25442;&#22120;&#19978;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#19968;&#32452;&#20648;&#22791;&#35013;&#32622;&#26469;&#20943;&#23569;&#30001;&#20110;&#21021;&#22987;&#21270;&#21464;&#21270;&#32780;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#65292;&#21253;&#25324;NLinear&#12289;Pyformer&#12289;Informer&#12289;Autoformer&#21644;&#22522;&#20934;Transformer&#65292;&#20854;&#35823;&#24046;&#20943;&#23569;&#39640;&#36798;-89.43&#65285;&#65292;&#36866;&#29992;&#20110;ETTh&#12289;ETTm&#21644;&#31354;&#27668;&#36136;&#37327;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09573v1 Announce Type: cross  Abstract: In Chaos, a minor divergence between two initial conditions exhibits exponential amplification over time, leading to far-away outcomes, known as the butterfly effect. Thus, the distant future is full of uncertainty and hard to forecast. We introduce Group Reservoir Transformer to predict long-term events more accurately and robustly by overcoming two challenges in Chaos: (1) the extensive historical sequences and (2) the sensitivity to initial conditions. A reservoir is attached to a Transformer to efficiently handle arbitrarily long historical lengths, with an extension of a group of reservoirs to reduce the uncertainty due to the initialization variations. Our architecture consistently outperforms state-of-the-art DNN models in multivariate time series, including NLinear, Pyformer, Informer, Autoformer, and the baseline Transformer, with an error reduction of up to -89.43\% in various fields such as ETTh, ETTm, and air quality, demon
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35814;&#32454;&#20998;&#26512;&#20102;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26576;&#20123;&#32593;&#32476;&#25366;&#25496;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#37096;&#20998;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#24037;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25345;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.07446</link><description>&lt;p&gt;
&#36136;&#37327;&#30830;&#23454;&#37325;&#35201;&#65306;&#23545;&#32593;&#32476;&#25366;&#25496;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quality Does Matter: A Detailed Look at the Quality and Utility of Web-Mined Parallel Corpora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35814;&#32454;&#20998;&#26512;&#20102;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26576;&#20123;&#32593;&#32476;&#25366;&#25496;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#37096;&#20998;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#24037;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#20004;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#33521;&#25991;-&#20711;&#20285;&#32599;&#35821;&#65292;&#33521;&#25991;-&#27888;&#31859;&#23572;&#35821;&#21644;&#20711;&#20285;&#32599;&#35821;-&#27888;&#31859;&#23572;&#35821;&#65289;&#30340;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#26681;&#25454;&#30456;&#20284;&#24230;&#26631;&#20934;&#23545;&#27599;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#25490;&#21517;&#65292;&#24182;&#23545;&#25490;&#21517;&#35821;&#26009;&#24211;&#30340;&#19981;&#21516;&#37096;&#20998;&#36827;&#34892;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#12290;&#25105;&#20204;&#26174;&#31034;&#19981;&#21516;&#37096;&#20998;&#30340;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#23384;&#22312;&#26174;&#33879;&#30340;&#36136;&#37327;&#24046;&#24322;&#65292;&#24182;&#19988;&#36136;&#37327;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#32593;&#32476;&#25366;&#25496;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20854;&#25490;&#21517;&#26368;&#39640;&#30340;25k&#37096;&#20998;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#24037;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conducted a detailed analysis on the quality of web-mined corpora for two low-resource languages (making three language pairs, English-Sinhala, English-Tamil and Sinhala-Tamil). We ranked each corpus according to a similarity measure and carried out an intrinsic and extrinsic evaluation on different portions of this ranked corpus. We show that there are significant quality differences between different portions of web-mined corpora and that the quality varies across languages and datasets. We also show that, for some web-mined datasets, Neural Machine Translation (NMT) models trained with their highest-ranked 25k portion can be on par with human-curated datasets.
&lt;/p&gt;</description></item><item><title>&#31038;&#21306;&#22411;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20801;&#35768;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#33647;&#29289;&#30456;&#20851;&#34892;&#20026;&#65292;&#22312;2500&#20010;&#38463;&#29255;&#31867;&#33647;&#29289;&#24086;&#23376;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26631;&#35760;&#20845;&#31181;&#19981;&#21516;&#38454;&#27573;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29255;&#27573;&#32423;&#25688;&#35201;&#35299;&#37322;&#22312;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#22312;&#30417;&#30563;&#12289;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;&#20960;&#31181;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2311.09066</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#31038;&#21306;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#35782;&#21035;&#33258;&#25105;&#25259;&#38706;&#30340;&#20351;&#29992;&#12289;&#28389;&#29992;&#21644;&#25104;&#30270;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09066
&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#22411;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20801;&#35768;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#33647;&#29289;&#30456;&#20851;&#34892;&#20026;&#65292;&#22312;2500&#20010;&#38463;&#29255;&#31867;&#33647;&#29289;&#24086;&#23376;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26631;&#35760;&#20845;&#31181;&#19981;&#21516;&#38454;&#27573;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29255;&#27573;&#32423;&#25688;&#35201;&#35299;&#37322;&#22312;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#22312;&#30417;&#30563;&#12289;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;&#20960;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#32654;&#22269;&#26377;50&#19975;&#22810;&#20154;&#27515;&#20110;&#28041;&#21450;&#22788;&#26041;&#33647;&#21644;&#38750;&#27861;&#38463;&#29255;&#31867;&#33647;&#29289;&#30340;&#36807;&#37327;&#20351;&#29992;&#65292;&#36896;&#25104;&#20102;&#22269;&#23478;&#20844;&#20849;&#21355;&#29983;&#32039;&#24613;&#24773;&#20917;&#12290; &#21307;&#30103;&#20174;&#19994;&#32773;&#38656;&#35201;&#24378;&#22823;&#19988;&#21450;&#26102;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#22788;&#20110;&#39118;&#38505;&#20043;&#20013;&#30340;&#24739;&#32773;&#12290;&#31038;&#21306;&#22411;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65288;&#22914;Reddit&#65289;&#20801;&#35768;&#29992;&#25143;&#33258;&#34892;&#25259;&#38706;&#65292;&#35752;&#35770;&#19968;&#33324;&#24773;&#20917;&#19979;&#25935;&#24863;&#30340;&#19982;&#33647;&#29289;&#30456;&#20851;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#23376;&#31038;&#21306;&#30340;2500&#20010;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#30456;&#20851;&#30340;&#24086;&#23376;&#65292;&#26631;&#35760;&#26377;&#20845;&#31181;&#19981;&#21516;&#30340;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38454;&#27573;&#65306;&#21307;&#30103;&#20351;&#29992;&#12289;&#28389;&#29992;&#12289;&#25104;&#30270;&#12289;&#24247;&#22797;&#12289;&#22797;&#21457;&#12289;&#19981;&#20351;&#29992;&#12290;&#23545;&#20110;&#27599;&#20010;&#24086;&#23376;&#65292;&#25105;&#20204;&#27880;&#37322;&#20102;&#22522;&#20110;&#29255;&#27573;&#32423;&#30340;&#25688;&#35201;&#35299;&#37322;&#65292;&#24182;&#22312;&#27880;&#37322;&#36136;&#37327;&#21644;&#27169;&#22411;&#24320;&#21457;&#20013;&#37325;&#28857;&#30740;&#31350;&#23427;&#20204;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#30417;&#30563;&#12289;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#20102;&#35782;&#21035;&#21508;&#20010;&#38454;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09066v2 Announce Type: replace  Abstract: In the last decade, the United States has lost more than 500,000 people from an overdose involving prescription and illicit opioids making it a national public health emergency (USDHHS, 2017). Medical practitioners require robust and timely tools that can effectively identify at-risk patients. Community-based social media platforms such as Reddit allow self-disclosure for users to discuss otherwise sensitive drug-related behaviors. We present a moderate size corpus of 2500 opioid-related posts from various subreddits labeled with six different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#22312;&#28436;&#35762;&#25991;&#26412;&#20013;&#21306;&#20998;&#21457;&#35328;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2311.07564</link><description>&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#33021;&#21542;&#21306;&#20998;&#28436;&#35762;&#25991;&#26412;&#20013;&#30340;&#21457;&#35328;&#20154;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#22312;&#28436;&#35762;&#25991;&#26412;&#20013;&#21306;&#20998;&#21457;&#35328;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#39564;&#35777;&#26159;&#30830;&#23450;&#20004;&#20010;&#19981;&#21516;&#20070;&#38754;&#26679;&#26412;&#26159;&#21542;&#21516;&#23646;&#19968;&#20316;&#32773;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#28041;&#21450;&#23545;&#20070;&#38754;&#25991;&#26412;&#30340;&#24402;&#22240;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36716;&#24405;&#28436;&#35762;&#30340;&#24402;&#23646;&#38382;&#39064;&#65292;&#36825;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#35768;&#22810;&#25991;&#20307;&#29305;&#24449;&#65292;&#22914;&#26631;&#28857;&#21644;&#22823;&#20889;&#65292;&#22312;&#36825;&#31181;&#24773;&#22659;&#19979;&#24182;&#19981;&#20855;&#22791;&#20449;&#24687;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36716;&#24405;&#30340;&#28436;&#35762;&#21576;&#29616;&#20854;&#20182;&#27169;&#24335;&#65292;&#22914;&#22635;&#20805;&#35789;&#21644;&#22238;&#24212;&#24615;&#22768;&#38899;&#65288;&#20363;&#22914;&#8220;&#21999;&#8221;&#65292;&#8220;&#21999;&#65292;&#21999;&#8221;&#65289;&#65292;&#36825;&#20123;&#21487;&#33021;&#26159;&#19981;&#21516;&#21457;&#35328;&#20154;&#30340;&#29305;&#24449;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;&#20026;&#20102;&#38480;&#21046;&#21457;&#35328;&#20154;&#19982;&#35805;&#39064;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#65292;&#25105;&#20204;&#20351;&#29992;&#20250;&#35805;&#25552;&#31034;&#21644;&#21442;&#19982;&#21516;&#19968;&#23545;&#35805;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#19981;&#21516;&#38590;&#24230;&#30340;&#39564;&#35777;&#35797;&#39564;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#22312;&#36825;&#19968;&#26032;&#22522;&#20934;&#19978;&#24314;&#31435;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07564v2 Announce Type: replace  Abstract: Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., 'um', 'uh-huh'), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#25688;&#35201;&#20219;&#21153;&#20013;&#20851;&#20110;&#36755;&#20837;&#20301;&#32622;&#30340;&#24615;&#33021;&#27169;&#24335;&#20197;&#21450;&#28304;&#25991;&#20214;&#21040;&#25688;&#35201;&#30340;&#20869;&#23481;&#26144;&#23556;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2310.10570</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Context Utilization in Summarization with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#25688;&#35201;&#20219;&#21153;&#20013;&#20851;&#20110;&#36755;&#20837;&#20301;&#32622;&#30340;&#24615;&#33021;&#27169;&#24335;&#20197;&#21450;&#28304;&#25991;&#20214;&#21040;&#25688;&#35201;&#30340;&#20869;&#23481;&#26144;&#23556;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25277;&#35937;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#27969;&#30021;&#19988;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#25193;&#23637;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#36229;&#36807;&#20102;100k&#20010;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#22312;&#38382;&#31572;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#19981;&#22343;&#21248;&#12290;&#23427;&#20204;&#20542;&#21521;&#20110;&#20559;&#29233;&#21021;&#22987;&#21644;&#26368;&#32456;&#27573;&#33853;&#65292;&#23548;&#33268;&#20102;&#20851;&#20110;&#31572;&#26696;&#22312;&#36755;&#20837;&#20013;&#20301;&#32622;&#30340;U&#24418;&#24615;&#33021;&#27169;&#24335;&#12290;&#36825;&#31181;&#20559;&#35265;&#24341;&#21457;&#20102;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#25688;&#35201;&#20013;&#65292;&#20851;&#38190;&#20869;&#23481;&#21487;&#33021;&#20998;&#25955;&#22312;&#28304;&#25991;&#20214;&#20013;&#12290;&#27492;&#22806;&#65292;&#22312;&#25688;&#35201;&#20013;&#65292;&#20174;&#28304;&#25991;&#20214;&#21040;&#25688;&#35201;&#30340;&#20107;&#23454;&#26144;&#23556;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#26174;&#33879;&#20869;&#23481;&#36890;&#24120;&#20250;&#34987;&#37325;&#26032;&#34920;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#21644;&#20301;&#32622;&#20559;&#35265;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;5&#20010;LLMs&#65292;10&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10570v3 Announce Type: replace  Abstract: Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, exceeding 100k tokens. However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source document(s). Besides, in summarization, mapping facts from the source to the summary is not trivial as salient content is usually re-phrased. In this paper, we conduct the first comprehensive study on context utilization and position bias in summarization. Our analysis encompasses 5 LLMs, 10 datasets, and 5 evaluation metrics. We introduce a new evaluatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31163;&#25955;&#21477;&#23376;&#22686;&#24378;&#26041;&#26696;&#65306;&#26631;&#28857;&#25554;&#20837;&#12289;&#24773;&#24577;&#21160;&#35789;&#21644;&#21452;&#37325;&#21542;&#23450;&#65292;&#29992;&#20110;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2210.03963</link><description>&lt;p&gt;
SDA&#65306;&#29992;&#20110;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#31616;&#21333;&#31163;&#25955;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SDA: Simple Discrete Augmentation for Contrastive Sentence Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.03963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31163;&#25955;&#21477;&#23376;&#22686;&#24378;&#26041;&#26696;&#65306;&#26631;&#28857;&#25554;&#20837;&#12289;&#24773;&#24577;&#21160;&#35789;&#21644;&#21452;&#37325;&#21542;&#23450;&#65292;&#29992;&#20110;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26368;&#36817;&#22312;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#22522;&#26412;&#35201;&#32032;&#65292;&#25968;&#25454;&#22686;&#24378;&#21327;&#35758;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20551;&#35774;&#21512;&#29702;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#26399;&#26395;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31163;&#25955;&#21477;&#23376;&#22686;&#24378;&#26041;&#26696;&#65306;&#26631;&#28857;&#25554;&#20837;&#12289;&#24773;&#24577;&#21160;&#35789;&#21644;&#21452;&#37325;&#21542;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.03963v2 Announce Type: replace  Abstract: Contrastive learning has recently achieved compelling performance in unsupervised sentence representation. As an essential element, data augmentation protocols, however, have not been well explored. The pioneering work SimCSE resorting to a simple dropout mechanism (viewed as continuous augmentation) surprisingly dominates discrete augmentations such as cropping, word deletion, and synonym replacement as reported. To understand the underlying rationales, we revisit existing approaches and attempt to hypothesize the desiderata of reasonable data augmentation methods: balance of semantic consistency and expression diversity. We then develop three simple yet effective discrete sentence augmentation schemes: punctuation insertion, modal verbs, and double negation. They act as minimal noises at lexical level to produce diverse forms of sentences. Furthermore, standard negation is capitalized on to generate negative samples for alleviating
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#29616;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#24050;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#65292;&#20294;&#22312;&#29983;&#25104;&#27491;&#30830;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.05777</link><description>&lt;p&gt;
&#36890;&#36807;&#38382;&#31572;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#35821;&#20041;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probing Structured Semantics Understanding and Generation of Language Models via Question Answering. (arXiv:2401.05777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#29616;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#24050;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#65292;&#20294;&#22312;&#29983;&#25104;&#27491;&#30830;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#30340;&#36827;&#27493;&#24341;&#21457;&#20102;&#23545;LLM&#35780;&#20272;&#30340;&#26032;&#28010;&#28526;&#12290;&#26368;&#36817;&#30340;&#35780;&#20272;&#24037;&#20316;&#20542;&#21521;&#20110;&#35780;&#20272;LLM&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#28145;&#20837;&#32467;&#26500;&#29702;&#35299;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#26500;&#24314;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#30740;&#31350;LLM&#22788;&#29702;&#32467;&#26500;&#21270;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#36827;&#34892;&#30456;&#20114;&#36716;&#25442;&#30340;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#39564;&#35777;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#32467;&#26500;&#21270;&#36923;&#36753;&#24418;&#24335;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22823;&#23567;&#21644;&#19981;&#21516;&#24418;&#24335;&#35821;&#35328;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#29616;&#20170;&#26368;&#20808;&#36827;&#30340;LLM&#22312;&#29702;&#35299;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#30340;&#33021;&#21147;&#25972;&#20307;&#19978;&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#65292;&#20294;&#22312;&#29983;&#25104;&#27491;&#30830;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#34920;&#26126;&#20351;&#29992;LLM&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancement in the capabilities of large language models (LLMs) has triggered a new surge in LLMs' evaluation. Most recent evaluation works tends to evaluate the comprehensive ability of LLMs over series of tasks. However, the deep structure understanding of natural language is rarely explored. In this work, we examine the ability of LLMs to deal with structured semantics on the tasks of question answering with the help of the human-constructed formal language. Specifically, we implement the inter-conversion of natural and formal language through in-context learning of LLMs to verify their ability to understand and generate the structured logical forms. Extensive experiments with models of different sizes and in different formal languages show that today's state-of-the-art LLMs' understanding of the logical forms can approach human level overall, but there still are plenty of room in generating correct logical forms, which suggest that it is more effective to use LLMs to generat
&lt;/p&gt;</description></item><item><title>BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02982</link><description>&lt;p&gt;
BIBench: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#20998;&#26512;&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02982
&lt;/p&gt;
&lt;p&gt;
BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#29087;&#32451;&#24230;&#21644;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#24605;&#32500;&#20026;&#37325;&#28857;&#30340;&#39046;&#22495;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BIBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#30340;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#12290;BIBench&#36890;&#36807;&#19977;&#20010;&#32500;&#24230;&#35780;&#20272;LLMs&#65306;1&#65289;BI&#22522;&#30784;&#30693;&#35782;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#21644;&#23545;&#37329;&#34701;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65307;2&#65289;BI&#30693;&#35782;&#24212;&#29992;&#65292;&#30830;&#23450;&#27169;&#22411;&#24555;&#36895;&#29702;&#35299;&#25991;&#26412;&#20449;&#24687;&#24182;&#20174;&#22810;&#20010;&#35270;&#35282;&#29983;&#25104;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65307;3&#65289;BI&#25216;&#26415;&#25216;&#33021;&#65292;&#26816;&#26597;&#27169;&#22411;&#20351;&#29992;&#25216;&#26415;&#30693;&#35782;&#35299;&#20915;&#29616;&#23454;&#25968;&#25454;&#20998;&#26512;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;BIBench&#21253;&#25324;11&#20010;&#23376;&#20219;&#21153;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06908</link><description>&lt;p&gt;
&#36208;&#21521;TopMost&#65306;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Towards the TopMost: A Topic Modeling System Toolkit. (arXiv:2309.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#34987;&#25552;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#22312;&#31070;&#32463;&#21464;&#20998;&#25512;&#26029;&#30340;&#25512;&#21160;&#19979;&#36817;&#26399;&#24471;&#21040;&#20102;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20027;&#39064;&#27169;&#22411;&#37319;&#29992;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24555;&#36895;&#21033;&#29992;&#21644;&#20844;&#24179;&#27604;&#36739;&#12290;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65288;TopMost&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;TopMost&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#30340;&#23436;&#25972;&#29983;&#21629;&#21608;&#26399;&#65292;&#33073;&#39062;&#32780;&#20986;&#12290;TopMost&#30340;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#21487;&#20197;&#24555;&#36895;&#21033;&#29992;&#65292;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#28789;&#27963;&#25193;&#23637;&#19981;&#21516;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25945;&#31243;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/bobxwu/topmost &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#21333;&#19968;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#38271;&#25991;&#26412;&#20851;&#38190;&#35789;&#25552;&#21462;&#33021;&#21147;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#23616;&#37096;&#21644;&#20840;&#23616;&#20851;&#38190;&#35789;&#65292;&#24182;&#25581;&#31034;&#25991;&#26412;&#30340;&#22522;&#26412;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#26159;&#35821;&#35328;&#26080;&#20851;&#30340;&#65292;&#36866;&#29992;&#20110;&#30701;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.14005</link><description>&lt;p&gt;
&#21333;&#19968;&#25991;&#26412;&#30340;&#26080;&#30417;&#30563;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#20851;&#38190;&#35789;
&lt;/p&gt;
&lt;p&gt;
Unsupervised extraction of local and global keywords from a single text. (arXiv:2307.14005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#21333;&#19968;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#38271;&#25991;&#26412;&#20851;&#38190;&#35789;&#25552;&#21462;&#33021;&#21147;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#23616;&#37096;&#21644;&#20840;&#23616;&#20851;&#38190;&#35789;&#65292;&#24182;&#25581;&#31034;&#25991;&#26412;&#30340;&#22522;&#26412;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#26159;&#35821;&#35328;&#26080;&#20851;&#30340;&#65292;&#36866;&#29992;&#20110;&#30701;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#12289;&#19982;&#35821;&#26009;&#24211;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#20174;&#21333;&#19968;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#12290;&#23427;&#22522;&#20110;&#21333;&#35789;&#30340;&#31354;&#38388;&#20998;&#24067;&#21450;&#20854;&#23545;&#21333;&#35789;&#30340;&#38543;&#26426;&#25490;&#21015;&#30340;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;YAKE&#31561;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19977;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#22312;&#20174;&#38271;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#12290;&#20854;&#27425;&#65292;&#23427;&#33021;&#22815;&#25512;&#26029;&#20986;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#38190;&#35789;&#65306;&#23616;&#37096;&#21644;&#20840;&#23616;&#12290;&#31532;&#19977;&#65292;&#23427;&#25581;&#31034;&#20102;&#25991;&#26412;&#30340;&#22522;&#26412;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#35821;&#35328;&#26080;&#20851;&#65292;&#36866;&#29992;&#20110;&#30701;&#25991;&#26412;&#12290;&#32467;&#26524;&#36890;&#36807;&#25105;&#20204;&#30340;&#21476;&#20856;&#25991;&#23398;&#20316;&#21697;&#25968;&#25454;&#24211;&#30340;&#20855;&#22791;&#20808;&#21069;&#30693;&#35782;&#30340;&#20154;&#31867;&#27880;&#37322;&#32773;&#33719;&#24471;&#65288;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20174;&#20013;&#31561;&#21040;&#37325;&#22823;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24471;&#21040;&#20102;&#22522;&#20110;&#25552;&#21462;&#20869;&#23481;&#35789;&#30340;&#24179;&#22343;&#38271;&#24230;&#21644;&#25552;&#21462;&#35789;&#20013;&#21517;&#35789;&#30340;&#24179;&#22343;&#25968;&#37327;&#30340;&#26080;&#20154;&#21442;&#19982;&#35770;&#35777;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#38190;&#35789;&#19982;&#39640;&#38454;&#25991;&#26412;&#29305;&#24449;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised, corpus-independent method to extract keywords from a single text. It is based on the spatial distribution of words and the response of this distribution to a random permutation of words. As compared to existing methods (such as e.g. YAKE) our method has three advantages. First, it is significantly more effective at extracting keywords from long texts. Second, it allows inference of two types of keywords: local and global. Third, it uncovers basic themes in texts. Additionally, our method is language-independent and applies to short texts. The results are obtained via human annotators with previous knowledge of texts from our database of classical literary works (the agreement between annotators is from moderate to substantial). Our results are supported via human-independent arguments based on the average length of extracted content words and on the average number of nouns in extracted words. We discuss relations of keywords with higher-order textual feature
&lt;/p&gt;</description></item><item><title>CrossGET&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#23454;&#26102;&#30340;&#36328;&#27169;&#24577;&#23548;&#24341;&#65292;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#22823;&#24133;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2305.17455</link><description>&lt;p&gt;
CrossGET: &#36328;&#23548;&#24341;&#30340;&#20196;&#29260;&#38598;&#21512;&#29992;&#20110;&#21152;&#36895;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers. (arXiv:2305.17455v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17455
&lt;/p&gt;
&lt;p&gt;
CrossGET&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#23454;&#26102;&#30340;&#36328;&#27169;&#24577;&#23548;&#24341;&#65292;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#22823;&#24133;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36828;&#36828;&#36229;&#20986;&#20102;&#25105;&#20204;&#30340;&#39044;&#26399;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#24555;&#36895;&#21457;&#23637;&#20063;&#22312;&#22823;&#24133;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#32780;&#35328;&#12290;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#21152;&#36895;&#21464;&#24471;&#26497;&#20854;&#20851;&#38190;&#12290;&#23613;&#31649;&#23545;&#20110;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#21152;&#36895;&#20173;&#28982;&#30456;&#23545;&#19981;&#36275;&#12290;&#20026;&#20102;&#36861;&#27714;&#26356;&#39640;&#25928;&#21644;&#21487;&#35775;&#38382;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;CrossGET&#30340;&#36328;&#23548;&#24341;&#20196;&#29260;&#38598;&#21512;&#30340;&#36890;&#29992;&#21152;&#36895;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23454;&#26102;&#30340;&#36328;&#27169;&#24577;&#23548;&#24341;&#65292;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#20196;&#29260;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#24133;&#21152;&#36895;&#32780;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;CrossGET&#30340;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#28857;&#26159;&#65306;1) &#36328;&#23548;&#24341;&#21305;&#37197;&#21644;&#38598;&#21512;&#12290;CrossGET&#23558;&#36328;&#23548;&#24341;&#30340;&#21305;&#37197;&#21644;&#38598;&#21512;&#24212;&#29992;&#21040;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored. To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#35777;&#26126;&#20854;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#20197;&#24847;&#22806;&#22320;&#24573;&#30053;&#25552;&#31034;&#35821;&#20041;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#31034;&#20363;&#21487;&#20197;&#20026;&#20302;&#36164;&#28304;&#32763;&#35793;&#25552;&#20379;&#26356;&#22909;&#30340;&#20219;&#21153;&#25351;&#23548;&#12290;&#20294;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#27169;&#22411;ChatGPT&#20173;&#28982;&#33853;&#21518;&#20110;&#30417;&#30563;&#22522;&#32447;NLLB&#12290;</title><link>http://arxiv.org/abs/2304.04675</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#65306;&#23454;&#35777;&#32467;&#26524;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. (arXiv:2304.04675v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#35777;&#26126;&#20854;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#20197;&#24847;&#22806;&#22320;&#24573;&#30053;&#25552;&#31034;&#35821;&#20041;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#31034;&#20363;&#21487;&#20197;&#20026;&#20302;&#36164;&#28304;&#32763;&#35793;&#25552;&#20379;&#26356;&#22909;&#30340;&#20219;&#21153;&#25351;&#23548;&#12290;&#20294;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#27169;&#22411;ChatGPT&#20173;&#28982;&#33853;&#21518;&#20110;&#30417;&#30563;&#22522;&#32447;NLLB&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;(MMT)&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#22238;&#31572;&#20004;&#20010;&#38382;&#39064;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;LLMs&#22312;MMT&#20013;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65306;1) LLMs&#22312;&#32763;&#35793;&#22823;&#37327;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;2) &#21738;&#20123;&#22240;&#32032;&#20250;&#24433;&#21709;LLMs&#22312;&#32763;&#35793;&#20013;&#30340;&#34920;&#29616;&#65311;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#25324;XGLM&#12289;OPT&#12289;BLOOMZ&#21644;ChatGPT&#22312;&#20869;&#30340;&#20960;&#20010;&#21463;&#27426;&#36814;&#30340;LLMs&#22312;102&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#27169;&#22411;ChatGPT&#22312;83.33%&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#20063;&#33853;&#21518;&#20110;&#30417;&#30563;&#22522;&#32447;NLLB&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#29992;&#20110;MMT&#26102;&#65292;LLMs&#34920;&#29616;&#20986;&#26032;&#30340;&#24037;&#20316;&#27169;&#24335;&#12290;&#39318;&#20808;&#65292;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#65292;&#25552;&#31034;&#35821;&#20041;&#21487;&#33021;&#20250;&#34987;&#24847;&#22806;&#22320;&#24573;&#30053;&#65292;&#21363;&#20351;&#25552;&#31034;&#19981;&#21512;&#29702;&#65292;LLMs&#20173;&#28982;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#36328;&#35821;&#35328;&#31034;&#20363;&#21487;&#20197;&#20026;&#20302;&#36164;&#28304;&#32763;&#35793;&#25552;&#20379;&#27604;&#30456;&#21516;&#35821;&#35328;&#23545;&#20013;&#30340;&#31034;&#20363;&#26356;&#22909;&#30340;&#20219;&#21153;&#25351;&#23548;&#12290;&#31532;&#19977;&#65292;&#24403;&#32763;&#35793;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;LLMs&#24448;&#24448;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;LLMs&#22312;MMT&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating a massive number of languages? 2) Which factors affect LLMs' performance in translation? We evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102 languages. Our empirical results show that even the best model ChatGPT still lags behind the supervised baseline NLLB in 83.33% of translation directions. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, prompt semantics can surprisingly be ignored when given in-context exemplars, where LLMs still show strong performance even with unreasonable prompts. Second, cross-lingual exemplars can provide better task instruction for low-resource translation than exemplars in the same language pairs. Third
&lt;/p&gt;</description></item></channel></rss>