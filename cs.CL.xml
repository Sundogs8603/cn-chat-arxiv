<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#32763;&#36716;&#27969;&#27700;&#32447;&#8221;&#26041;&#27861;&#65292;&#22312;&#24076;&#20271;&#26469;&#35821;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21363;&#36890;&#36807;&#19987;&#23478;&#20998;&#31867;&#22120;&#30452;&#25509;&#22312;&#25972;&#20010;&#26631;&#35760;&#21333;&#20803;&#19978;&#20570;&#20986;&#20915;&#31574;&#65292;&#26368;&#21518;&#20877;&#32508;&#21512;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06970</link><description>&lt;p&gt;
&#26080;&#27882;MRL&#20998;&#26512;&#65306;&#24076;&#20271;&#26469;&#35821;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
MRL Parsing Without Tears: The Case of Hebrew
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#32763;&#36716;&#27969;&#27700;&#32447;&#8221;&#26041;&#27861;&#65292;&#22312;&#24076;&#20271;&#26469;&#35821;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21363;&#36890;&#36807;&#19987;&#23478;&#20998;&#31867;&#22120;&#30452;&#25509;&#22312;&#25972;&#20010;&#26631;&#35760;&#21333;&#20803;&#19978;&#20570;&#20986;&#20915;&#31574;&#65292;&#26368;&#21518;&#20877;&#32508;&#21512;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#20998;&#26512;&#20173;&#28982;&#26159;&#20851;&#31995;&#25552;&#21462;&#21644;&#20449;&#24687;&#25552;&#21462;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#65292;&#32570;&#20047;LLMs&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;(MRLs)&#20013;&#65292;&#35299;&#26512;&#22120;&#38656;&#35201;&#35782;&#21035;&#27599;&#20010;&#26631;&#35760;&#20013;&#30340;&#22810;&#20010;&#35789;&#32032;&#21333;&#20803;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#24310;&#36831;&#21644;&#35774;&#32622;&#22797;&#26434;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#19968;&#20123;&#31995;&#32479;&#20351;&#29992;&#27969;&#27700;&#32447;&#36880;&#23618;&#22788;&#29702;&#65306;&#39318;&#20808;&#36827;&#34892;&#20998;&#35789;&#65292;&#28982;&#21518;&#36827;&#34892;&#24418;&#24577;&#26631;&#35760;&#65292;&#26368;&#21518;&#36827;&#34892;&#35821;&#27861;&#20998;&#26512;&#65307;&#28982;&#32780;&#65292;&#20043;&#21069;&#23618;&#27425;&#30340;&#38169;&#35823;&#20250;&#21521;&#21069;&#20256;&#25773;&#12290;&#21478;&#19968;&#20123;&#31995;&#32479;&#20351;&#29992;&#32852;&#21512;&#26550;&#26500;&#21516;&#26102;&#35780;&#20272;&#25152;&#26377;&#25490;&#21015;&#32452;&#21512;&#65307;&#34429;&#28982;&#36825;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20294;&#36895;&#24230;&#26497;&#20854;&#32531;&#24930;&#12290;&#30456;&#21453;&#65292;&#20197;&#24076;&#20271;&#26469;&#35821;&#20316;&#20026;&#27979;&#35797;&#26696;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#32763;&#36716;&#27969;&#27700;&#32447;&#8221;&#65306;&#19987;&#23478;&#20998;&#31867;&#22120;&#30452;&#25509;&#22312;&#25972;&#20010;&#26631;&#35760;&#21333;&#20803;&#19978;&#20570;&#20986;&#20915;&#31574;&#65292;&#27599;&#20010;&#20998;&#31867;&#22120;&#19987;&#38376;&#36127;&#36131;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#24444;&#27492;&#29420;&#31435;&#65292;&#21482;&#26377;&#22312;&#26368;&#21518;&#25105;&#20204;&#25165;&#32508;&#21512;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06970v1 Announce Type: new  Abstract: Syntactic parsing remains a critical tool for relation extraction and information extraction, especially in resource-scarce languages where LLMs are lacking. Yet in morphologically rich languages (MRLs), where parsers need to identify multiple lexical units in each token, existing systems suffer in latency and setup complexity. Some use a pipeline to peel away the layers: first segmentation, then morphology tagging, and then syntax parsing; however, errors in earlier layers are then propagated forward. Others use a joint architecture to evaluate all permutations at once; while this improves accuracy, it is notoriously slow. In contrast, and taking Hebrew as a test case, we present a new "flipped pipeline": decisions are made directly on the whole-token units by expert classifiers, each one dedicated to one specific task. The classifiers are independent of one another, and only at the end do we synthesize their predictions. This blazingly
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35770;&#26029;&#32467;&#26500;&#26500;&#36896;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#26367;&#25442;&#21160;&#35789;&#30340;&#27979;&#35797;&#26041;&#27861;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24341;&#36215;&#21160;&#20316;&#26500;&#36896;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.06965</link><description>&lt;p&gt;
&#28151;&#21512;&#24335;&#20154;&#31867;-LLM&#35821;&#26009;&#24211;&#26500;&#24314;&#19982;&#32597;&#35265;&#35821;&#35328;&#29616;&#35937;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35770;&#26029;&#32467;&#26500;&#26500;&#36896;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#26367;&#25442;&#21160;&#35789;&#30340;&#27979;&#35797;&#26041;&#27861;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24341;&#36215;&#21160;&#20316;&#26500;&#36896;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35770;&#26029;&#32467;&#26500;&#26500;&#36896;&#26041;&#27861;&#65292;&#20026;&#20102;&#23637;&#31034;&#26500;&#24335;&#35821;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#35770;&#26029;&#32467;&#26500;&#26500;&#36896;&#65288;ASCs&#65289;&#36825;&#19968;&#32452;&#26500;&#36896;&#20013;&#30340;&#24341;&#36215;&#21160;&#20316;&#26500;&#36896;&#65288;CMC&#65292;&#8220;&#22905;&#25171;&#21943;&#22159;&#25226;&#21345;&#24067;&#22855;&#35834;&#19978;&#30340;&#27873;&#27819;&#21561;&#36208;&#8221;&#65289;&#12290;&#25105;&#20204;&#20551;&#35774;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36825;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20026;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#29992;&#20856;&#22411;&#21160;&#20316;&#21160;&#35789;&#26367;&#25442;&#21160;&#35789;&#36827;&#34892;&#27979;&#35797;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#32479;&#35745;&#24847;&#20041;&#19978;&#36827;&#34892;&#27492;&#27979;&#35797;&#65292;&#22312;&#27809;&#26377;&#36275;&#22815;&#30340;&#26500;&#24335;&#35821;&#27861;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#36741;&#21161;&#30340;&#35821;&#35328;&#23398;&#27880;&#37322;&#25991;&#26412;&#25910;&#38598;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20381;&#23384;&#35299;&#26512;&#21644;GPT-3.5&#22914;&#20309;&#21487;&#29992;&#20110;&#26174;&#33879;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#32597;&#35265;&#35821;&#35328;&#29616;&#35937;&#36827;&#34892;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06965v1 Announce Type: new  Abstract: Argument Structure Constructions (ASCs) are one of the most well-studied construction groups, providing a unique opportunity to demonstrate the usefulness of Construction Grammar (CxG). For example, the caused-motion construction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates that constructions must carry meaning, otherwise the fact that ``sneeze'' in this context causes movement cannot be explained. We form the hypothesis that this remains challenging even for state-of-the-art Large Language Models (LLMs), for which we devise a test based on substituting the verb with a prototypical motion verb. To be able to perform this test at statistically significant scale, in the absence of adequate CxG corpora, we develop a novel pipeline of NLP-assisted collection of linguistically annotated text. We show how dependency parsing and GPT-3.5 can be used to significantly reduce annotation cost and thus enable the annotation of rare
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.06963</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
The pitfalls of next-token prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06963
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#25285;&#24551;&#65306;&#19968;&#20010;&#20165;&#20165;&#22522;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#24544;&#23454;&#22320;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#32463;&#24120;&#28151;&#28102;&#30340;&#20004;&#20010;&#38454;&#27573; -- &#33258;&#22238;&#24402;&#25512;&#26029;&#21644;&#25945;&#24072;&#24378;&#21046;&#35757;&#32451; -- &#24517;&#39035;&#34987;&#21306;&#21035;&#23545;&#24453;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#19968;&#33324;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#25945;&#24072;&#24378;&#21046;&#22914;&#20309;&#22833;&#36133;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#35745;&#21010;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;Transformer&#21644;Mamba&#26550;&#26500;&#22312;&#23454;&#36341;&#20013;&#20197;&#36825;&#31181;&#26041;&#24335;&#22833;&#36133; -- &#23613;&#31649;&#20219;&#21153;&#26412;&#36523;&#24456;&#23481;&#26131;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06963v1 Announce Type: cross  Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn. We provide preliminary
&lt;/p&gt;</description></item><item><title>SELMA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#25216;&#33021;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#65292;&#20174;&#32780;&#25913;&#36827;T2I&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.06952</link><description>&lt;p&gt;
SELMA&#65306;&#23398;&#20064;&#21644;&#21512;&#24182;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#30340;&#25216;&#33021;&#29305;&#23450;&#25991;&#26412;&#21040;&#22270;&#20687;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06952
&lt;/p&gt;
&lt;p&gt;
SELMA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#25216;&#33021;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#65292;&#20174;&#32780;&#25913;&#36827;T2I&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21019;&#24314;&#22270;&#20687;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;T2I&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#31934;&#30830;&#21305;&#37197;&#25991;&#26412;&#36755;&#20837;&#32454;&#33410;&#30340;&#22270;&#20687;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#19981;&#20339;&#65292;&#27604;&#22914;&#19981;&#27491;&#30830;&#30340;&#31354;&#38388;&#20851;&#31995;&#25110;&#32570;&#22833;&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SELMA&#65306;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#30340;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#65292;&#36825;&#26159;&#19968;&#31181;&#25913;&#36827;T2I&#27169;&#22411;&#24544;&#23454;&#24230;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#25216;&#33021;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#12290;&#39318;&#20808;&#65292;SELMA&#21033;&#29992;LLM&#30340;&#29615;&#22659;&#23398;&#20064;&#33021;&#21147;&#29983;&#25104;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25945;&#25480;&#19981;&#21516;&#30340;&#25216;&#33021;&#65292;&#28982;&#21518;&#22522;&#20110;&#25552;&#31034;&#20351;&#29992;T2I&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#12290;&#25509;&#19979;&#26469;&#65292;SELMA&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#21333;&#25216;&#33021;&#30340;LoRA&#65288;&#20302;&#31209;&#35843;&#25972;&#65289;&#19987;&#23478;&#35843;&#25972;T2I&#27169;&#22411;&#21040;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06952v1 Announce Type: cross  Abstract: Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts follow
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20013;&#23637;&#31034;&#20986;&#22788;&#29702;&#27169;&#31946;&#38656;&#27714;&#12289;&#21152;&#36895;&#20219;&#21153;&#33258;&#21160;&#21270;&#21644;&#30693;&#35782;&#25552;&#21462;&#30340;&#28508;&#21147;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.06949</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#26448;&#26009;&#31185;&#23398;&#65306;&#19968;&#20010;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Materials science in the era of large language models: a perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06949
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20013;&#23637;&#31034;&#20986;&#22788;&#29702;&#27169;&#31946;&#38656;&#27714;&#12289;&#21152;&#36895;&#20219;&#21153;&#33258;&#21160;&#21270;&#21644;&#30693;&#35782;&#25552;&#21462;&#30340;&#28508;&#21147;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#32467;&#21512;&#21508;&#31181;&#26032;&#20852;&#29305;&#24615;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#20174;&#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#21040;&#21551;&#21457;&#24335;&#25214;&#35299;&#32452;&#21512;&#38382;&#39064;&#31561;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#22810;&#21151;&#33021;&#24037;&#20855;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#36866;&#29992;&#24615;&#30340;&#35266;&#28857;&#65292;&#35748;&#20026;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#36328;&#22810;&#39033;&#20219;&#21153;&#21644;&#23398;&#31185;&#30340;&#27169;&#31946;&#38656;&#27714;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#33021;&#25104;&#20026;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290; &#25105;&#20204;&#23450;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#26412;&#30340;LLM&#29702;&#35770;&#65292;&#23558;&#20854;&#19982;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#29305;&#24615;&#21644;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#28982;&#21518;&#25552;&#20379;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#35268;&#27169;&#21270;&#20219;&#21153;&#33258;&#21160;&#21270;&#21644;&#30693;&#35782;&#25552;&#21462;&#20013;&#30340;&#24212;&#29992;&#12290; &#22312;&#23427;&#20204;&#24403;&#21069;&#30340;&#21457;&#23637;&#38454;&#27573;&#65292;&#25105;&#20204;&#35748;&#20026;LLMs&#24212;&#35813;&#34987;&#30475;&#20316;&#19981;&#26159;&#26032;&#39062;&#27934;&#23519;&#21147;&#30340;&#39044;&#35328;&#32773;&#65292;&#32780;&#26159;&#33021;&#22815;&#21152;&#36895;&#21644;&#32479;&#19968;&#25506;&#32034;&#30340;&#19981;&#30693;&#30130;&#20518;&#30340;&#24037;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06949v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have garnered considerable interest due to their impressive natural language capabilities, which in conjunction with various emergent properties make them versatile tools in workflows ranging from complex code generation to heuristic finding for combinatorial problems. In this paper we offer a perspective on their applicability to materials science research, arguing their ability to handle ambiguous requirements across a range of tasks and disciplines mean they could be a powerful tool to aid researchers. We qualitatively examine basic LLM theory, connecting it to relevant properties and techniques in the literature before providing two case studies that demonstrate their use in task automation and knowledge extraction at-scale. At their current stage of development, we argue LLMs should be viewed less as oracles of novel insight, and more as tireless workers that can accelerate and unify exploration across
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#20219;&#21153;CFKGR&#65292;&#26412;&#25991;&#23558;&#30693;&#35782;&#22270;&#34917;&#20840;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#20551;&#35774;&#21069;&#25552;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;COULDD&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#34920;&#26126;KGEs&#21487;&#20197;&#23398;&#20064;&#22270;&#20013;&#30340;&#27169;&#24335;&#65292;&#26816;&#27979;&#20986;&#21512;&#29702;&#30340;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.06936</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Reasoning with Knowledge Graph Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06936
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#20219;&#21153;CFKGR&#65292;&#26412;&#25991;&#23558;&#30693;&#35782;&#22270;&#34917;&#20840;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#20551;&#35774;&#21069;&#25552;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;COULDD&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#34920;&#26126;KGEs&#21487;&#20197;&#23398;&#20064;&#22270;&#20013;&#30340;&#27169;&#24335;&#65292;&#26816;&#27979;&#20986;&#21512;&#29702;&#30340;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGEs&#65289;&#26368;&#21021;&#26159;&#20026;&#20102;&#25512;&#26029;&#19981;&#23436;&#25972;&#30693;&#35782;&#24211;&#20013;&#32570;&#22833;&#30340;&#30495;&#23454;&#20107;&#23454;&#32780;&#24320;&#21457;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#20219;&#21153;CFKGR&#23558;&#30693;&#35782;&#22270;&#34917;&#20840;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#19990;&#30028;&#29366;&#24577;&#24314;&#27169;&#20026;&#30693;&#35782;&#22270;&#65292;&#20551;&#35774;&#24773;&#26223;&#20026;&#28155;&#21152;&#21040;&#22270;&#20013;&#30340;&#36793;&#65292;&#23545;&#22270;&#30340;&#21512;&#29702;&#21464;&#21270;&#20026;&#36923;&#36753;&#35268;&#21017;&#25512;&#29702;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#20551;&#35774;&#24773;&#26223;&#21450;&#23545;&#21407;&#22987;&#30693;&#35782;&#22270;&#30340;&#21512;&#29702;&#25913;&#21464;&#20197;&#21450;&#24212;&#35813;&#20445;&#30041;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;COULDD&#65292;&#19968;&#31181;&#38024;&#23545;&#29305;&#23450;&#20551;&#35774;&#21069;&#25552;&#35843;&#25972;&#29616;&#26377;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;KGEs&#21487;&#20197;&#22312;&#27809;&#26377;&#26174;&#24335;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20174;&#22270;&#20013;&#23398;&#20064;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;COULDD&#35843;&#25972;&#21518;&#30340;KGEs&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#36981;&#24490;&#36923;&#36753;&#35268;&#21017;&#30340;&#22270;&#30340;&#21512;&#29702;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06936v1 Announce Type: cross  Abstract: Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories. In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR. We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules. We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained. We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark. Our results indicate that KGEs learn patterns in the graph without explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow the
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#35270;&#35273;&#19982;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#31867;&#22312;&#21487;&#33021;&#26631;&#31614;&#30340;&#20998;&#24067;&#19978;&#26174;&#31034;&#20986;&#26497;&#22823;&#20027;&#35266;&#21464;&#24322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#35270;&#35273;&#23545;&#35937;&#30340;&#21629;&#21517;&#12289;&#25551;&#36848;&#21644;&#37327;&#21270;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.06935</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#23545;&#35937;&#21629;&#21517;&#12289;&#25551;&#36848;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Naming, Describing, and Quantifying Visual Objects in Humans and LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06935
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#35270;&#35273;&#19982;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#31867;&#22312;&#21487;&#33021;&#26631;&#31614;&#30340;&#20998;&#24067;&#19978;&#26174;&#31034;&#20986;&#26497;&#22823;&#20027;&#35266;&#21464;&#24322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#35270;&#35273;&#23545;&#35937;&#30340;&#21629;&#21517;&#12289;&#25551;&#36848;&#21644;&#37327;&#21270;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35762;&#35805;&#32773;&#22312;&#25551;&#36848;&#22270;&#20687;&#20013;&#30340;&#21516;&#19968;&#23545;&#35937;&#26102;&#20351;&#29992;&#21508;&#31181;&#19981;&#21516;&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#36825;&#20135;&#29983;&#20102;&#30001;&#35821;&#29992;&#32422;&#26463;&#39537;&#21160;&#30340;&#21512;&#29702;&#26631;&#31614;&#20998;&#24067;&#65292;&#24403;&#21069;&#35270;&#35273;&#19982;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;VLLMs&#65289;&#33021;&#22815;&#27169;&#20223;&#35821;&#35328;&#20351;&#29992;&#20013;&#36825;&#19968;&#20851;&#38190;&#29305;&#24449;&#30340;&#31243;&#24230;&#23578;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;VLLMs&#65288;FROMAGe&#12289;BLIP-2&#12289;LLaVA&#65289;&#22312;&#20154;&#31867;&#22312;&#21487;&#33021;&#26631;&#31614;&#30340;&#20998;&#24067;&#19978;&#26174;&#31034;&#20986;&#26497;&#22823;&#20027;&#35266;&#21464;&#24322;&#24615;&#30340;&#19977;&#20010;&#31867;&#21035;&#65288;&#21517;&#35789;&#12289;&#23646;&#24615;&#21644;&#37327;&#35789;&#65289;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06935v1 Announce Type: new  Abstract: While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision \&amp; Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, humans show clear production preferences for highly context-sensitive expressions, such as the quantifiers `few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the a
&lt;/p&gt;</description></item><item><title>ERA-CoT &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#25903;&#25345;&#24605;&#32500;&#38142;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29702;&#35299;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#20219;&#21153;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06932</link><description>&lt;p&gt;
ERA-CoT: &#36890;&#36807;&#23454;&#20307;&#20851;&#31995;&#20998;&#26512;&#25913;&#36827;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06932
&lt;/p&gt;
&lt;p&gt;
ERA-CoT &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#25903;&#25345;&#24605;&#32500;&#38142;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29702;&#35299;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#20219;&#21153;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22797;&#26434;&#22330;&#26223;&#26102;&#65292;LLMs &#20173;&#28982;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#23384;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#38544;&#24335;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861; ERA-CoT&#65292;&#36890;&#36807;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#24110;&#21161; LLMs &#29702;&#35299;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25903;&#25345;&#19981;&#21516;&#20219;&#21153;&#30340;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24403;&#21069;&#30340; CoT &#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;ERA-CoT &#34920;&#29616;&#20986;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312; GPT3.5 &#19978;&#24179;&#22343;&#27604;&#20197;&#21069;&#30340; SOTA &#22522;&#32447;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340; 5.1% &#25913;&#36827;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ERA-CoT &#25552;&#39640;&#20102;LLM&#23545;&#23454;&#20307;&#20851;&#31995;&#30340;&#29702;&#35299;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#38382;&#39064;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06932v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering
&lt;/p&gt;</description></item><item><title>Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06925</link><description>&lt;p&gt;
Transformers&#23398;&#20064;&#20302;&#25935;&#24863;&#24615;&#20989;&#25968;&#30340;&#31616;&#21333;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Simplicity Bias of Transformers to Learn Low Sensitivity Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06925
&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#23427;&#20204;&#20855;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#20197;&#21450;&#36825;&#20123;&#20559;&#24046;&#22914;&#20309;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19981;&#21516;&#30340;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#23545;&#36755;&#20837;&#20013;&#30340;&#38543;&#26426;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#27010;&#24565;&#21270;&#20026;&#19968;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#30340;&#27010;&#24565;&#65292;&#36825;&#20026;&#35299;&#37322;transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#30340;&#31616;&#21333;&#24615;&#21644;&#35889;&#20559;&#24046;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;transformers&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#27604;&#20854;&#20182;&#26367;&#20195;&#26550;&#26500;&#65288;&#22914;LSTMs&#12289;MLPs&#21644;CNNs&#65289;&#20855;&#26377;&#26356;&#20302;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20302;&#25935;&#24863;&#24615;&#20559;&#24046;&#19982;&#25913;&#36827;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06925v1 Announce Type: cross  Abstract: Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with impro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25552;&#39640;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.06914</link><description>&lt;p&gt;
MEND&#65306;&#20803;&#28436;&#31034;&#33976;&#39311;&#29992;&#20110;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25552;&#39640;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#33021;&#21147;&#65292;&#20854;&#20013;LLM&#20026;&#32473;&#23450;&#30340;&#27979;&#35797;&#36755;&#20837;&#21644;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#23545;(&#28436;&#31034;)&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#28436;&#31034;&#30340;&#21152;&#20837;&#23548;&#33268;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#24320;&#38144;&#21576;&#20108;&#27425;&#22686;&#21152;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#23581;&#35797;&#23558;&#20887;&#38271;&#30340;&#28436;&#31034;&#33976;&#39311;&#25104;&#32039;&#20945;&#30340;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#29306;&#29298;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#23558;&#20219;&#20309;&#20887;&#38271;&#28436;&#31034;&#33976;&#39311;&#20026;&#21521;&#37327;&#65292;&#32780;&#26080;&#38656;&#20026;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#22686;&#24378;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;MEND&#20855;&#26377;&#33976;&#39311;&#28436;&#31034;&#30340;&#20803;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06914v1 Announce Type: cross  Abstract: Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23454;&#26102;&#21464;&#21387;&#22120;&#30340;&#24320;&#35789;&#27719;&#26816;&#27979;&#27169;&#22411;OmDet-Turbo&#65292;&#20855;&#26377;&#39640;&#25928;&#34701;&#21512;&#22836;&#27169;&#22359;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30417;&#30563;&#27169;&#22411;&#20960;&#20046;&#25345;&#24179;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.06892</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#26102;&#21464;&#21387;&#22120;&#30340;&#39640;&#25928;&#34701;&#21512;&#22836;&#24320;&#35789;&#27719;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23454;&#26102;&#21464;&#21387;&#22120;&#30340;&#24320;&#35789;&#27719;&#26816;&#27979;&#27169;&#22411;OmDet-Turbo&#65292;&#20855;&#26377;&#39640;&#25928;&#34701;&#21512;&#22836;&#27169;&#22359;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30417;&#30563;&#27169;&#22411;&#20960;&#20046;&#25345;&#24179;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31471;&#21040;&#31471;&#21464;&#21387;&#22120;&#30340;&#26816;&#27979;&#22120;&#65288;DETRs&#65289;&#36890;&#36807;&#25972;&#21512;&#35821;&#35328;&#27169;&#24577;&#65292;&#22312;&#23553;&#38381;&#38598;&#21644;&#24320;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#65288;OVD&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#39640;&#35201;&#27714;&#30340;&#35745;&#31639;&#38656;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;OVDEval&#22522;&#20934;&#27979;&#35797;&#20013;&#20004;&#20010;&#39046;&#20808;&#27169;&#22411;OmDet&#21644;Grounding-DINO&#30340;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;OmDet-Turbo&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#23454;&#26102;OVD&#27169;&#22411;&#20855;&#26377;&#21019;&#26032;&#30340;&#39640;&#25928;&#34701;&#21512;&#22836;&#65288;EFH&#65289;&#27169;&#22359;&#65292;&#26088;&#22312;&#32531;&#35299;OmDet&#21644;Grounding-DINO&#20013;&#35266;&#23519;&#21040;&#30340;&#29942;&#39048;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OmDet-Turbo-Base&#22312;&#24212;&#29992;TensorRT&#21644;&#35821;&#35328;&#32531;&#23384;&#25216;&#26415;&#21518;&#23454;&#29616;&#20102;100.2&#24103;&#27599;&#31186;&#65288;FPS&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;COCO&#21644;LVIS&#25968;&#25454;&#38598;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20013;&#65292;OmDet-Turbo&#30340;&#24615;&#33021;&#20960;&#20046;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#27169;&#22411;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06892v1 Announce Type: cross  Abstract: End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO. Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied. Notably, in zero-shot scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art supervised models. 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;MESc&#26694;&#26550;&#25506;&#32034;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#31867;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#25991;&#20214;&#37096;&#20998;&#30340;&#23884;&#20837;&#24182;&#20351;&#29992;&#32858;&#31867;&#36817;&#20284;&#32467;&#26500;&#65292;&#36827;&#32780;&#39044;&#27979;&#21028;&#20915;&#12290;</title><link>https://arxiv.org/abs/2403.06872</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20998;&#23618;&#26694;&#26550;&#29992;&#20110;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06872
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;MESc&#26694;&#26550;&#25506;&#32034;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#31867;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#25991;&#20214;&#37096;&#20998;&#30340;&#23884;&#20837;&#24182;&#20351;&#29992;&#32858;&#31867;&#36817;&#20284;&#32467;&#26500;&#65292;&#36827;&#32780;&#39044;&#27979;&#21028;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21463;&#38271;&#36798;&#25968;&#19975;&#23383;&#30340;&#26696;&#20363;&#25991;&#20214;&#21644;&#38750;&#22343;&#21248;&#32467;&#26500;&#30340;&#38382;&#39064;&#22256;&#25200;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#27809;&#26377;&#32467;&#26500;&#26631;&#27880;&#30340;&#25991;&#20214;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#23618;&#26694;&#26550;(MESc)&#65292;&#21363;&#8220;&#22522;&#20110;&#22810;&#38454;&#27573;&#32534;&#30721;&#22120;&#30340;&#24102;&#32858;&#31867;&#30340;&#30417;&#30563;&#23398;&#20064;&#8221;&#65292;&#26469;&#25506;&#32034;&#36825;&#20123;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#31867;&#21644;&#23427;&#20204;&#32570;&#20047;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#29992;&#20110;&#21028;&#20915;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25991;&#20214;&#20998;&#25104;&#37096;&#20998;&#65292;&#20174;&#33258;&#23450;&#20041;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#21518;&#22235;&#23618;&#20013;&#25552;&#21462;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#24182;&#23581;&#35797;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#26469;&#36817;&#20284;&#23427;&#20204;&#30340;&#32467;&#26500;&#12290;&#28982;&#21518;&#22312;&#21478;&#19968;&#32452;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#23618;&#20013;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#23398;&#20064;&#37096;&#20998;&#38388;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36866;&#24212;&#24615;(GPT-Neo)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06872v1 Announce Type: cross  Abstract: Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation. We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21487;&#38752;&#30340;&#25252;&#29702;&#35821;&#35328;&#27169;&#22411;&#65288;CaLM&#65289;&#65292;&#20351;&#29992;FM&#21644;&#25252;&#29702;&#30693;&#35782;&#24211;&#65292;&#36890;&#36807;RAG&#26694;&#26550;&#21644;FM&#24494;&#35843;&#25552;&#39640;FM&#31572;&#26696;&#36136;&#37327;&#65292;&#20197;&#25903;&#25345;&#23478;&#24237;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20248;&#36136;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.06857</link><description>&lt;p&gt;
&#24320;&#21457;&#19968;&#31181;&#21487;&#38752;&#19988;&#26131;&#33719;&#21462;&#30340;&#25252;&#29702;&#35821;&#35328;&#27169;&#22411;&#65288;CaLM&#65289;
&lt;/p&gt;
&lt;p&gt;
Development of a Reliable and Accessible Caregiving Language Model (CaLM)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21487;&#38752;&#30340;&#25252;&#29702;&#35821;&#35328;&#27169;&#22411;&#65288;CaLM&#65289;&#65292;&#20351;&#29992;FM&#21644;&#25252;&#29702;&#30693;&#35782;&#24211;&#65292;&#36890;&#36807;RAG&#26694;&#26550;&#21644;FM&#24494;&#35843;&#25552;&#39640;FM&#31572;&#26696;&#36136;&#37327;&#65292;&#20197;&#25903;&#25345;&#23478;&#24237;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20248;&#36136;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19987;&#19994;&#25252;&#29702;&#20154;&#21592;&#19981;&#21516;&#65292;&#23478;&#24237;&#25252;&#29702;&#20154;&#21592;&#36890;&#24120;&#22312;&#27809;&#26377;&#27491;&#24335;&#20934;&#22791;&#25110;&#22521;&#35757;&#30340;&#24773;&#20917;&#19979;&#25215;&#25285;&#36825;&#19968;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#22686;&#24378;&#23478;&#24237;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20248;&#36136;&#25252;&#29702;&#30340;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25903;&#25345;&#25252;&#29702;&#20154;&#21592;&#30340;&#25945;&#32946;&#24037;&#20855;&#25110;&#25252;&#29702;&#30340;&#36741;&#21161;&#25216;&#26415;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;FM&#21644;&#25252;&#29702;&#30693;&#35782;&#24211;&#24320;&#21457;&#21487;&#38752;&#30340;&#25252;&#29702;&#35821;&#35328;&#27169;&#22411;&#65288;CaLM&#65289;&#65292;&#20351;&#29992;&#38656;&#35201;&#26356;&#23569;&#35745;&#31639;&#36164;&#28304;&#30340;&#23567;FM&#24320;&#21457;&#26131;&#33719;&#21462;&#30340;CaLM&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#30456;&#23545;&#20110;&#22823;&#22411;FM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#32467;&#21512;FM&#24494;&#35843;&#24320;&#21457;CaLM&#65292;&#20197;&#36890;&#36807;&#23558;&#27169;&#22411;&#22522;&#20110;&#25252;&#29702;&#30693;&#35782;&#24211;&#26469;&#25552;&#39640;FM&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#36873;&#25321;&#20004;&#20010;&#23567;FM&#20316;&#20026;CaLM&#30340;FM&#20505;&#36873;&#32773;&#65288;LLaMA-2&#21644;&#21442;&#25968;&#20026;7B&#30340;Falcon&#65289;&#65292;&#20197;&#21450;&#26356;&#22823;&#30340;FM GPT-3.5&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06857v1 Announce Type: new  Abstract: Unlike professional caregivers, family caregivers often assume this role without formal preparation or training. Because of this, there is an urgent need to enhance the capacity of family caregivers to provide quality care. Large language models can potentially be used as a foundation technology for supporting caregivers as educational tools or as adjunct to care. This study aimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a caregiving knowledge base, develop an accessible CaLM using a small FM that requires fewer computing resources, and evaluate the performance of the model compared to a large FM. We developed CaLM using the Retrieval Augmented Generation (RAG) framework combined with FM fine-tuning for improving the quality of FM answers by grounding the model on a caregiving knowledge base. We used two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B parameters) and larger FM GPT-3.5
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#25351;&#23450;&#20219;&#21153;&#30340;&#29305;&#23450;&#22330;&#26223;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.06840</link><description>&lt;p&gt;
RA-ISF: &#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#23398;&#20064;&#26816;&#32034;&#22686;&#24378;&#20197;&#22238;&#31572;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06840
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#25351;&#23450;&#20219;&#21153;&#30340;&#29305;&#23450;&#22330;&#26223;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#23384;&#20648;&#22312;&#20854;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26816;&#32034;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#26469;&#22238;&#31572;&#20197;&#21069;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26816;&#32034;&#22686;&#24378;&#36845;&#20195;&#33258;&#21453;&#39304;(RA-ISF)&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#23376;&#27169;&#22359;&#36845;&#20195;&#20998;&#35299;&#20219;&#21153;&#24182;&#22788;&#29702;&#23427;&#20204;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#22312;&#35832;&#22914;GPT3.5&#12289;Llama2&#20043;&#31867;&#30340;&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06840v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#21644;&#35299;&#21078;&#30149;&#29702;&#25552;&#31034;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#24230;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.06835</link><description>&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#21644;&#35299;&#21078;&#30149;&#29702;&#25552;&#31034;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#21644;&#35299;&#21078;&#30149;&#29702;&#25552;&#31034;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#24230;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#38480;&#21046;&#20102;&#39640;&#36136;&#37327;&#21307;&#23398;&#22270;&#20687;&#20379;&#20844;&#20247;&#20351;&#29992;&#30340;&#21487;&#29992;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#24120;&#24120;&#38590;&#20197;&#20934;&#30830;&#25429;&#25417;&#35814;&#32454;&#35299;&#21078;&#32467;&#26500;&#21644;&#30149;&#29702;&#26465;&#20214;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#21644;&#35299;&#21078;&#30149;&#29702;&#25552;&#31034;&#29983;&#25104;&#39640;&#24230;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#19982;&#22270;&#20687;&#29983;&#25104;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#25551;&#36848;&#24615;&#25991;&#26412;&#25552;&#31034;&#19982;&#21512;&#25104;&#22270;&#20687;&#30340;&#35299;&#21078;&#21644;&#30149;&#29702;&#32454;&#33410;&#20043;&#38388;&#30340;&#31934;&#30830;&#23545;&#40784;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#35299;&#21078;&#30149;&#29702;&#25552;&#31034;&#27169;&#22359;&#21644;&#22522;&#20110;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06835v1 Announce Type: cross  Abstract: Data scarcity and privacy concerns limit the availability of high-quality medical images for public use, which can be mitigated through medical image synthesis. However, current medical image synthesis methods often struggle to accurately capture the complexity of detailed anatomical structures and pathological conditions. To address these challenges, we propose a novel medical image synthesis model that leverages fine-grained image-text alignment and anatomy-pathology prompts to generate highly detailed and accurate synthetic medical images. Our method integrates advanced natural language processing techniques with image generative modeling, enabling precise alignment between descriptive text prompts and the synthesized images' anatomical and pathological details. The proposed approach consists of two key components: an anatomy-pathology prompting module and a fine-grained alignment-based synthesis module. The anatomy-pathology prompt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.06833</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#23558;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#21527;&#65311;&#25105;&#20204;&#20855;&#20307;&#25351;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35843;&#33410;&#25351;&#20196;&#30340;&#25216;&#26415;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#65292;&#20026;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25171;&#24320;&#20102;&#26080;&#25968;&#26032;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs&#32570;&#20047;&#20854;&#20182;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24050;&#24314;&#31435;&#20026;&#35268;&#33539;&#30340;&#22522;&#26412;&#23433;&#20840;&#29305;&#24615;&#65292;&#27604;&#22914;&#25351;&#20196;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#23548;&#33268;&#23427;&#20204;&#21457;&#29983;&#25925;&#38556;&#25110;&#26131;&#21463;&#31532;&#19977;&#26041;&#25805;&#25511;&#21644;&#24178;&#25200;&#65288;&#20363;&#22914;&#36890;&#36807;&#38388;&#25509;&#25552;&#31034;/&#21629;&#20196;&#27880;&#20837;&#65289;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#29978;&#33267;&#27809;&#26377;&#30830;&#20999;&#23450;&#20041;&#36825;&#31181;&#20998;&#31163;&#31350;&#31455;&#24847;&#21619;&#30528;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#27979;&#35797;&#20854;&#36829;&#21453;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#20174;&#27169;&#22411;&#30340;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SEP&#65288;&#24212;&#35813;&#25191;&#34892;&#36824;&#26159;&#22788;&#29702;&#65311;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#20801;&#35768;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 Announce Type: cross  Abstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.06832</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#26397;&#30528;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06832
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#36827;&#23637;&#20984;&#26174;&#20986;&#40065;&#26834;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65288;MMKG&#65289;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#26694;&#26550;&#23545;&#20110;&#22312;&#35268;&#27169;&#19978;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#25972;&#21512;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#20943;&#36731;&#30693;&#35782;&#35823;&#35299;&#21644;&#22810;&#27169;&#24577;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#23884;&#20837;MMKG&#20013;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MKGC&#65289;&#21644;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNAG&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#37197;&#22791;&#20102;&#27169;&#24577;&#32423;&#22122;&#22768;&#25513;&#27169;&#65292;&#20197;&#22312;&#30693;&#35782;&#22270;&#20013;&#40065;&#26834;&#22320;&#38598;&#25104;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;MKGC&#21644;MMEA&#37117;&#24341;&#20837;&#29305;&#23450;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24635;&#20849;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#65288;&#19977;&#20010;&#29992;&#20110;MKGC&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06832v1 Announce Type: cross  Abstract: The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework. This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations. In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and 
&lt;/p&gt;</description></item><item><title>SPLADE-v3&#30456;&#23545;&#20110;BM25&#21644;SPLADE++&#22312;&#25928;&#26524;&#19978;&#26356;&#20026;&#26174;&#33879;&#65292;&#21516;&#26102;&#19982;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#22120;&#30456;&#27604;&#36739;&#20986;&#33394;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.06789</link><description>&lt;p&gt;
SPLADE-v3&#65306;SPLADE&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SPLADE-v3: New baselines for SPLADE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06789
&lt;/p&gt;
&lt;p&gt;
SPLADE-v3&#30456;&#23545;&#20110;BM25&#21644;SPLADE++&#22312;&#25928;&#26524;&#19978;&#26356;&#20026;&#26174;&#33879;&#65292;&#21516;&#26102;&#19982;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#22120;&#30456;&#27604;&#36739;&#20986;&#33394;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;SPLADE&#24211;&#26368;&#26032;&#29256;&#26412;&#21457;&#24067;&#30340;&#20276;&#38543;&#29289;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23545;&#35757;&#32451;&#32467;&#26500;&#30340;&#26356;&#25913;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#19968;&#31995;&#21015;&#30340;&#27169;&#22411;--SPLADE-v3&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#29256;&#26412;&#19982;BM25&#12289;SPLADE++&#20197;&#21450;&#37325;&#26032;&#25490;&#24207;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#23545;40&#22810;&#20010;&#26597;&#35810;&#38598;&#30340;&#20803;&#20998;&#26512;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;SPLADE-v3&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;SPLADE&#27169;&#22411;&#30340;&#26497;&#38480;&#65306;&#23427;&#22312;&#32479;&#35745;&#19978;&#26174;&#33879;&#27604;BM25&#21644;SPLADE++&#26356;&#20026;&#26377;&#25928;&#65292;&#21516;&#26102;&#19982;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#22120;&#30456;&#27604;&#36739;&#20986;&#33394;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22312;MS MARCO dev&#38598;&#19978;&#33719;&#24471;&#20102;40&#20010;&#20197;&#19978;&#30340;MRR@10&#65292;&#24182;&#22312;BEIR&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;&#22495;&#22806;&#32467;&#26524;&#25552;&#39640;&#20102;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06789v1 Announce Type: cross  Abstract: A companion to the release of the latest version of the SPLADE library. We describe changes to the training structure and present our latest series of models -- SPLADE-v3. We compare this new version to BM25, SPLADE++, as well as re-rankers, and showcase its effectiveness via a meta-analysis over more than 40 query sets. SPLADE-v3 further pushes the limit of SPLADE models: it is statistically significantly more effective than both BM25 and SPLADE++, while comparing well to cross-encoder re-rankers. Specifically, it gets more than 40 MRR@10 on the MS MARCO dev set, and improves by 2% the out-of-domain results on the BEIR benchmark.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRIP&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#25972;&#21512;&#29992;&#25143;&#24863;&#30693;&#30340;&#25112;&#30053;&#35268;&#21010;&#27169;&#22359;&#21644;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#38750;&#21512;&#20316;&#23545;&#35805;&#20195;&#29702;&#21830;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#28385;&#36275;&#22810;&#26679;&#21270;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.06769</link><description>&lt;p&gt;
&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#21147;&#37327;&#65281;&#36890;&#36807;&#23450;&#21046;&#31574;&#30053;&#35268;&#21010;&#23454;&#29616;&#26377;&#25928;&#30340;&#38750;&#21512;&#20316;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRIP&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#25972;&#21512;&#29992;&#25143;&#24863;&#30693;&#30340;&#25112;&#30053;&#35268;&#21010;&#27169;&#22359;&#21644;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#38750;&#21512;&#20316;&#23545;&#35805;&#20195;&#29702;&#21830;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#28385;&#36275;&#22810;&#26679;&#21270;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#21512;&#20316;&#23545;&#35805;&#20195;&#29702;&#21830;&#65292;&#20182;&#20204;&#24517;&#39035;&#36827;&#34892;&#23450;&#21046;&#30340;&#25112;&#30053;&#35268;&#21010;&#65292;&#20197;&#30830;&#20445;&#19982;&#22810;&#26679;&#21270;&#29992;&#25143;&#36798;&#25104;&#26377;&#21033;&#30340;&#21327;&#35758;&#12290;&#36825;&#23545;&#29616;&#26377;&#30340;&#23545;&#35805;&#20195;&#29702;&#21830;&#26500;&#25104;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26377;&#20004;&#28857;&#65306;&#20182;&#20204;&#26080;&#27861;&#23558;&#29992;&#25143;&#29305;&#23450;&#29305;&#24449;&#25972;&#21512;&#21040;&#25112;&#30053;&#35268;&#21010;&#20013;&#65292;&#20197;&#21450;&#20182;&#20204;&#30340;&#35757;&#32451;&#33539;&#24335;&#26410;&#33021;&#20135;&#29983;&#21487;&#20197;&#27867;&#21270;&#21040;&#22810;&#26679;&#21270;&#29992;&#25143;&#30340;&#25112;&#30053;&#35268;&#21010;&#32773;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TRIP&#20197;&#22686;&#24378;&#23450;&#21046;&#25112;&#30053;&#35268;&#21010;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#29992;&#25143;&#24863;&#30693;&#30340;&#25112;&#30053;&#35268;&#21010;&#27169;&#22359;&#21644;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#38750;&#21512;&#20316;&#23545;&#35805;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TRIP&#22312;&#36814;&#21512;&#22810;&#26679;&#21270;&#29992;&#25143;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06769v1 Announce Type: new  Abstract: We investigate non-collaborative dialogue agents that must engage in tailored strategic planning for diverse users to secure a favorable agreement. This poses challenges for existing dialogue agents due to two main reasons: their inability to integrate user-specific characteristics into their strategic planning and their training paradigm's failure to produce strategic planners that can generalize to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ConspEmoLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38598;&#25104;&#24773;&#24863;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#38452;&#35851;&#29702;&#35770;&#25991;&#26412;&#30340;&#24773;&#24863;&#29305;&#24449;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#65292;&#33021;&#22815;&#25191;&#34892;&#22810;&#39033;&#20219;&#21153;&#65292;&#21253;&#25324;&#38452;&#35851;&#29702;&#35770;&#26816;&#27979;&#12289;&#29702;&#35770;&#31867;&#22411;&#20998;&#31867;&#21644;&#30456;&#20851;&#25991;&#26412;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.06765</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#24773;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#38452;&#35851;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ConspEmoLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38598;&#25104;&#24773;&#24863;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#38452;&#35851;&#29702;&#35770;&#25991;&#26412;&#30340;&#24773;&#24863;&#29305;&#24449;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#65292;&#33021;&#22815;&#25191;&#34892;&#22810;&#39033;&#20219;&#21153;&#65292;&#21253;&#25324;&#38452;&#35851;&#29702;&#35770;&#26816;&#27979;&#12289;&#29702;&#35770;&#31867;&#22411;&#20998;&#31867;&#21644;&#30456;&#20851;&#25991;&#26412;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#22909;&#22788;&#21644;&#20260;&#23475;&#12290;&#21518;&#32773;&#30340;&#19968;&#20010;&#20027;&#35201;&#20363;&#23376;&#26159;&#35823;&#23548;&#20449;&#24687;&#65292;&#21253;&#25324;&#20805;&#26021;&#32593;&#32476;&#30340;&#38452;&#35851;&#29702;&#35770;&#12290; &#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#25552;&#39640;&#20102;&#20934;&#30830;&#26816;&#27979;&#35823;&#23548;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;LLM&#30340;&#38452;&#35851;&#29702;&#35770;&#26816;&#27979;&#26041;&#27861;&#20165;&#19987;&#27880;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#24182;&#26410;&#32771;&#34385;&#35823;&#23548;&#20449;&#24687;&#19982;&#24773;&#24863;&#29305;&#24449;&#65288;&#21363;&#24773;&#24863;&#21644;&#24773;&#32490;&#65289;&#20043;&#38388;&#30340;&#37325;&#35201;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#25581;&#31034;&#20854;&#29420;&#29305;&#24773;&#24863;&#29305;&#24449;&#30340;&#38452;&#35851;&#25991;&#26412;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConspEmoLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38598;&#25104;&#24773;&#24863;&#20449;&#24687;&#19988;&#33021;&#22815;&#25191;&#34892;&#28041;&#21450;&#38452;&#35851;&#29702;&#35770;&#30340;&#22810;&#26679;&#20219;&#21153;&#30340;&#24320;&#28304;LLM&#12290; &#36825;&#20123;&#20219;&#21153;&#19981;&#20165;&#21253;&#25324;&#38452;&#35851;&#29702;&#35770;&#26816;&#27979;&#65292;&#36824;&#21253;&#25324;&#29702;&#35770;&#31867;&#22411;&#20998;&#31867;&#21644;&#30456;&#20851;&#25991;&#26412;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06765v1 Announce Type: new  Abstract: The internet has brought both benefits and harms to society. A prime example of the latter is misinformation, including conspiracy theories, which flood the web. Recent advances in natural language processing, particularly the emergence of large language models (LLMs), have improved the prospects of accurate misinformation detection. However, most LLM-based approaches to conspiracy theory detection focus only on binary classification and fail to account for the important relationship between misinformation and affective features (i.e., sentiment and emotions). Driven by a comprehensive analysis of conspiracy text that reveals its distinctive affective features, we propose ConspEmoLLM, the first open-source LLM that integrates affective information and is able to perform diverse tasks relating to conspiracy theories. These tasks include not only conspiracy theory detection, but also classification of theory type and detection of related d
&lt;/p&gt;</description></item><item><title>FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06764</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#22312;&#31532;&#20108;&#23618;&#20043;&#21518;&#20215;&#20540;1/2&#20195;&#24065;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06764
&lt;/p&gt;
&lt;p&gt;
FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#20013;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#23384;&#22312;&#20302;&#25928;&#29616;&#35937;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#21517;&#27169;&#22411;&#22914;LLaVA-1.5&#12289;QwenVL-Chat&#21644;Video-LLaVA&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27969;&#34892;&#30340;LVLMs&#30340;&#28145;&#23618;&#20013;&#65292;&#23545;&#35270;&#35273;&#20195;&#24065;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26497;&#20854;&#20302;&#25928;&#65292;&#26263;&#31034;&#30456;&#36739;&#20110;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#65292;&#38656;&#35201;&#26356;&#31232;&#30095;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FastV&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#26089;&#26399;&#23618;&#20013;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#21644;&#22312;&#38543;&#21518;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#26469;&#20248;&#21270;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;FastV&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;LLaVA-1.5-13B&#30340;FLOP&#20943;&#23569;&#20102;45%&#65289;&#65292;&#32780;&#19981;&#20250;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#29306;&#29298;&#24615;&#33021;&#12290;FastV&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#26435;&#34913;&#26159;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#65292;&#24182;&#19988;&#26159;&#24085;&#32047;&#25176;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06764v1 Announce Type: cross  Abstract: In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress t
&lt;/p&gt;</description></item><item><title>ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.06754</link><description>&lt;p&gt;
ALaRM: &#36890;&#36807;&#20998;&#23618;&#22870;&#21169;&#24314;&#27169;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ALaRM: Align Language Models via Hierarchical Rewards Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06754
&lt;/p&gt;
&lt;p&gt;
ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;ALaRM&#65292;&#31532;&#19968;&#20010;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20174;&#20154;&#31867;&#21453;&#39304;&#27169;&#22411;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#40784;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#20154;&#31867;&#30417;&#30563;&#20449;&#21495;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36890;&#36807;&#23558;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#21644;&#19968;&#33268;&#22320;&#25351;&#23548;&#26397;&#30528;&#26399;&#26395;&#30340;&#32467;&#26524;&#21069;&#36827;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#21644;&#24320;&#25918;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#21644;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26426;&#21046;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#38271;&#31687;&#38382;&#39064;&#22238;&#31572;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#20351;&#29992;gpt-3.5-turbo&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06754v1 Announce Type: cross  Abstract: We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20986;&#29616;&#30340;&#31163;&#38774;&#38382;&#39064;&#30340;&#26032;&#22411;&#30417;&#30563;&#24494;&#35843;&#26426;&#21046;ACT-MNMT Auto-Constriction Turning&#65292;&#19982;&#20256;&#32479;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#27491;&#20132;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#21463;&#38480;&#27169;&#26495;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06745</link><description>&lt;p&gt;
ACT-MNMT&#33258;&#21160;&#25910;&#32553;&#36716;&#21521;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20986;&#29616;&#30340;&#31163;&#38774;&#38382;&#39064;&#30340;&#26032;&#22411;&#30417;&#30563;&#24494;&#35843;&#26426;&#21046;ACT-MNMT Auto-Constriction Turning&#65292;&#19982;&#20256;&#32479;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#27491;&#20132;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#21463;&#38480;&#27169;&#26495;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#38646;/&#23569;shot&#25552;&#31034;&#25110;&#25552;&#31034;&#35843;&#25972;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;LLM&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#28151;&#21512;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#65292;&#22522;&#20110;LLM&#30340;&#32763;&#35793;&#27169;&#22411;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#20013;&#38754;&#20020;&#20102;&#31163;&#38774;&#38382;&#39064;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#29616;&#35937;&#65292;&#21363;&#25351;&#20196;&#35823;&#35299;&#12289;&#38169;&#35823;&#35821;&#35328;&#32763;&#35793;&#21644;&#36807;&#24230;&#29983;&#25104;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;\model&#65289;&#30340;\textbf{\underline{A}}uto-\textbf{\underline{C}}onstriction \textbf{\underline{T}}urning&#26426;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#30417;&#30563;&#24494;&#35843;&#26426;&#21046;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#27491;&#20132;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;\model&#36890;&#36807;&#22312;&#30446;&#26631;&#31471;&#28155;&#21152;&#35302;&#21457;&#20196;&#29260;&#33258;&#21160;&#26500;&#36896;&#21463;&#38480;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06745v1 Announce Type: cross  Abstract: Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning. However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation. For this issue, this paper introduces an \textbf{\underline{A}}uto-\textbf{\underline{C}}onstriction \textbf{\underline{T}}urning mechanism for \textbf{\underline{M}}ultilingual \textbf{\underline{N}}eural \textbf{\underline{M}}achine \textbf{\underline{T}}ranslation (\model), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods. In this method, \model automatically constructs a constrained template in the target side by adding trigger tokens ahead
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CognitiveEMS&#65292;&#19968;&#20010;&#23454;&#26102;&#22810;&#27169;&#24577;&#35748;&#30693;&#21161;&#25163;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#26032;&#39062;&#32452;&#20214;&#35299;&#20915;&#20102;&#23454;&#26102;&#35748;&#30693;&#36741;&#21161;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65292;&#20026;&#24613;&#25937;&#26381;&#21153;&#25552;&#20379;&#20851;&#38190;&#30340;&#36741;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.06734</link><description>&lt;p&gt;
&#24613;&#25937;&#26381;&#21153;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#35748;&#30693;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Real-Time Multimodal Cognitive Assistant for Emergency Medical Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CognitiveEMS&#65292;&#19968;&#20010;&#23454;&#26102;&#22810;&#27169;&#24577;&#35748;&#30693;&#21161;&#25163;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#26032;&#39062;&#32452;&#20214;&#35299;&#20915;&#20102;&#23454;&#26102;&#35748;&#30693;&#36741;&#21161;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65292;&#20026;&#24613;&#25937;&#26381;&#21153;&#25552;&#20379;&#20851;&#38190;&#30340;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#25937;&#26381;&#21153;&#65288;EMS&#65289;&#21709;&#24212;&#32773;&#32463;&#24120;&#22312;&#26102;&#38388;&#32039;&#36843;&#30340;&#26465;&#20214;&#19979;&#24037;&#20316;&#65292;&#38754;&#20020;&#35748;&#30693;&#36807;&#36733;&#21644;&#22266;&#26377;&#39118;&#38505;&#65292;&#38656;&#35201;&#20855;&#22791;&#20851;&#38190;&#24605;&#32500;&#21644;&#24555;&#36895;&#20915;&#31574;&#30340;&#22522;&#26412;&#25216;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CognitiveEMS&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#21487;&#31359;&#25140;&#35748;&#30693;&#21161;&#25163;&#31995;&#32479;&#65292;&#21487;&#20197;&#20805;&#24403;&#21327;&#20316;&#34394;&#25311;&#20249;&#20276;&#65292;&#23454;&#26102;&#33719;&#21462;&#21644;&#20998;&#26512;&#24613;&#25937;&#29616;&#22330;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#26234;&#33021;&#30524;&#38236;&#19982;EMS&#21709;&#24212;&#32773;&#20114;&#21160;&#12290;CognitiveEMS&#23454;&#26102;&#22788;&#29702;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#65292;&#24182;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#26469;&#25552;&#20379;EMS&#21327;&#35758;&#36873;&#25321;&#21644;&#24178;&#39044;&#35782;&#21035;&#30340;&#36741;&#21161;&#12290;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#26032;&#39062;&#32452;&#20214;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23454;&#26102;&#35748;&#30693;&#36741;&#21161;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65306;&#65288;i&#65289;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#38024;&#23545;&#27169;&#25311;&#30340;EMS&#38899;&#39057;&#35760;&#24405;&#36827;&#34892;&#23454;&#38469;&#24613;&#35786;&#23545;&#35805;&#65292;&#22686;&#24378;wit
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06734v1 Announce Type: new  Abstract: Emergency Medical Services (EMS) responders often operate under time-sensitive conditions, facing cognitive overload and inherent risks, requiring essential skills in critical thinking and rapid decision-making. This paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system that can act as a collaborative virtual partner engaging in the real-time acquisition and analysis of multimodal data from an emergency scene and interacting with EMS responders through Augmented Reality (AR) smart glasses. CognitiveEMS processes the continuous streams of data in real-time and leverages edge computing to provide assistance in EMS protocol selection and intervention recognition. We address key technical challenges in real-time cognitive assistance by introducing three novel components: (i) a Speech Recognition model that is fine-tuned for real-world medical emergency conversations using simulated EMS audio recordings, augmented wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.06725</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#37325;&#35201;&#24615;&#26426;&#21046;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#23398;&#29983;&#30340;&#21382;&#21490;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#30693;&#35782;&#25484;&#25569;&#31243;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;KT&#65288;DLKT&#65289;&#26041;&#27861;&#22312;KT&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22914;&#39044;&#31639;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#20114;&#21160;&#38750;&#24120;&#26377;&#38480;&#65292;&#21363;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#12290;&#30452;&#25509;&#22312;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DLKT&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#24456;&#38590;&#36873;&#25321;&#36866;&#24403;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;KT&#26694;&#26550;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#30427;&#34892;&#30340;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#24674;&#22797;&#27169;&#22411;&#65288;MMRM&#65289;&#26469;&#24674;&#22797;&#21476;&#20195;&#25991;&#26412;&#65292;&#29305;&#21035;&#24378;&#35843;&#34920;&#24847;&#23383;&#31526;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#27531;&#30041;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#39044;&#27979;&#21463;&#25439;&#23383;&#31526;&#24182;&#29983;&#25104;&#24674;&#22797;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.06682</link><description>&lt;p&gt;
&#24674;&#22797;&#21476;&#20195;&#34920;&#24847;&#23383;&#31526;: &#19968;&#31181;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Restoring Ancient Ideograph: A Multimodal Multitask Neural Network Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06682
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#24674;&#22797;&#27169;&#22411;&#65288;MMRM&#65289;&#26469;&#24674;&#22797;&#21476;&#20195;&#25991;&#26412;&#65292;&#29305;&#21035;&#24378;&#35843;&#34920;&#24847;&#23383;&#31526;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#27531;&#30041;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#39044;&#27979;&#21463;&#25439;&#23383;&#31526;&#24182;&#29983;&#25104;&#24674;&#22797;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#21270;&#36951;&#20135;&#20316;&#20026;&#20154;&#31867;&#24605;&#24819;&#21644;&#21382;&#21490;&#30340;&#27704;&#24658;&#35760;&#24405;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#20445;&#25252;&#25991;&#21270;&#36951;&#20135;&#65292;&#20294;&#35768;&#22810;&#21476;&#20195;&#25991;&#29289;&#24050;&#32463;&#34987;&#33258;&#28982;&#36864;&#21270;&#21644;&#20154;&#20026;&#34892;&#20026;&#19981;&#21487;&#36870;&#22320;&#30772;&#22351;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#24674;&#22797;&#21508;&#31181;&#25991;&#21270;&#36951;&#20135;&#65292;&#21253;&#25324;&#21476;&#20195;&#25991;&#26412;&#24674;&#22797;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#20174;&#35270;&#35273;&#25110;&#25991;&#26412;&#35282;&#24230;&#25506;&#35752;&#21476;&#20195;&#25991;&#26412;&#30340;&#24674;&#22797;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#30456;&#20114;&#37197;&#21512;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#24674;&#22797;&#27169;&#22411;&#65288;MMRM&#65289;&#26469;&#24674;&#22797;&#21476;&#20195;&#25991;&#26412;&#65292;&#29305;&#21035;&#24378;&#35843;&#34920;&#24847;&#23383;&#31526;&#12290;&#35813;&#27169;&#22411;&#23558;&#20174;&#21463;&#25439;&#21476;&#20195;&#25991;&#29289;&#20013;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#19982;&#27531;&#30041;&#30340;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#21463;&#25439;&#23383;&#31526;&#24182;&#29983;&#25104;&#24674;&#22797;&#22270;&#20687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#20102;MMRM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06682v1 Announce Type: new  Abstract: Cultural heritage serves as the enduring record of human thought and history. Despite significant efforts dedicated to the preservation of cultural relics, many ancient artefacts have been ravaged irreversibly by natural deterioration and human actions. Deep learning technology has emerged as a valuable tool for restoring various kinds of cultural heritages, including ancient text restoration. Previous research has approached ancient text restoration from either visual or textual perspectives, often overlooking the potential of synergizing multimodal information. This paper proposes a novel Multimodal Multitask Restoring Model (MMRM) to restore ancient texts, particularly emphasising the ideograph. This model combines context understanding with residual visual information from damaged ancient artefacts, enabling it to predict damaged characters and generate restored images simultaneously. We tested the MMRM model through experiments cond
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35760;&#24518;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;LLMs&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#24615;&#33021;&#35780;&#20272;&#30340;&#26080;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06644</link><description>&lt;p&gt;
&#22823;&#35937;&#27704;&#36828;&#19981;&#20250;&#24536;&#35760;&#65306;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#35760;&#24518;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Elephants Never Forget: Testing Language Models for Memorization of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35760;&#24518;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;LLMs&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#24615;&#33021;&#35780;&#20272;&#30340;&#26080;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35768;&#22810;&#20154;&#24050;&#32463;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#20294;&#25968;&#25454;&#27745;&#26579;&#21644;&#35760;&#24518;&#21270;&#30340;&#20851;&#38190;&#38382;&#39064;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#31616;&#21333;&#30340;&#23450;&#24615;&#27979;&#35797;&#24320;&#22987;&#65292;&#27979;&#35797;LLM&#26159;&#21542;&#30693;&#36947;&#29305;&#24449;&#30340;&#21517;&#31216;&#21644;&#20540;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#35780;&#20272;&#27745;&#26579;&#31243;&#24230;&#65292;&#21253;&#25324;&#29992;&#20110;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#30340;&#32479;&#35745;&#27979;&#35797;&#21644;&#22235;&#20010;&#35782;&#21035;&#35760;&#24518;&#21270;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;LLMs&#22312;&#35768;&#22810;&#27969;&#34892;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#36825;&#31181;&#26292;&#38706;&#21487;&#33021;&#23548;&#33268;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26080;&#25928;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#22240;&#20026;LLMs&#23454;&#38469;&#19978;&#24050;&#32463;&#36866;&#24212;&#20102;&#27979;&#35797;&#38598;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#31181;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#25968;&#25454;&#30340;&#37325;&#35201;&#32479;&#35745;&#20449;&#24687;&#65292;&#20294;&#26080;&#27861;&#36880;&#23383;&#22797;&#21046;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;&#23613;&#31649;&#35265;&#36807;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06644v1 Announce Type: cross  Abstract: While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Starting with simple qualitative tests for whether an LLM knows the names and values of features, we introduce a variety of different techniques to assess the degrees of contamination, including statistical tests for conditional distribution modeling and four tests that identify memorization. Our investigation reveals that LLMs are pre-trained on many popular tabular datasets. This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to the test set. Interestingly, we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim. On these datasets, although seen during 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#65292;&#24182;&#21253;&#25324;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.06642</link><description>&lt;p&gt;
KELLMRec: &#30693;&#35782;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06642
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#65292;&#24182;&#21253;&#25324;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#34917;&#20805;&#20027;&#27969;&#22522;&#20110;ID&#30340;&#26041;&#27861;&#30340;&#32570;&#22833;&#37096;&#20998;&#12290;&#38543;&#30528;LLM&#30340;&#20852;&#36215;&#65292;&#23427;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#20026;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20351;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#25104;&#20026;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;LLM&#26469;&#22788;&#29702;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#26159;&#19981;&#21487;&#38752;&#21644;&#27425;&#20248;&#30340;&#65292;&#30001;&#20110;&#23384;&#22312;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#12290;&#21463;&#20197;&#19978;&#21160;&#26426;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;LLMRec&#26041;&#27861;&#12290;&#38500;&#20102;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#29992;&#20110;&#35757;&#32451;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#20225;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06642v1 Announce Type: cross  Abstract: The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training. Experiments on public datasets and in-enter
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#27169;&#22359;&#21644;&#20869;&#37096;&#20020;&#24202;&#36335;&#24452;&#32534;&#30721;&#65292;MedKP&#26694;&#26550;&#22312;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.06611</link><description>&lt;p&gt;
MedKP&#65306;&#21307;&#23398;&#23545;&#35805;&#19982;&#30693;&#35782;&#22686;&#24378;&#19982;&#20020;&#24202;&#36335;&#24452;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#27169;&#22359;&#21644;&#20869;&#37096;&#20020;&#24202;&#36335;&#24452;&#32534;&#30721;&#65292;MedKP&#26694;&#26550;&#22312;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36866;&#24403;&#30340;&#25968;&#25454;&#36873;&#25321;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#21307;&#23398;&#32771;&#35797;&#21644;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290; &#28982;&#32780;&#65292;LLMs&#22312;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#39046;&#22495;&#30340;&#24212;&#29992;--&#36825;&#20010;&#20219;&#21153;&#26356;&#21152;&#36148;&#36817;&#23454;&#38469;&#21307;&#23398;&#23454;&#36341;--&#21364;&#40092;&#26377;&#25506;&#35752;&#12290; &#36825;&#19968;&#24046;&#36317;&#24402;&#22240;&#20110;LLMs&#21307;&#23398;&#30693;&#35782;&#19981;&#36275;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#21307;&#23398;&#22238;&#22797;&#20986;&#29616;&#19981;&#20934;&#30830;&#21644;&#24187;&#35273;&#20449;&#24687;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21307;&#23398;&#23545;&#35805;&#19982;&#30693;&#35782;&#22686;&#24378;&#21644;&#20020;&#24202;&#36335;&#24452;&#32534;&#30721;&#65288;MedKP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21307;&#23398;&#30693;&#35782;&#22270;&#21644;&#21307;&#30103;&#23454;&#20307;&#20197;&#21450;&#21307;&#24072;&#34892;&#21160;&#30340;&#20869;&#37096;&#20020;&#24202;&#36335;&#24452;&#32534;&#30721;&#65292;&#38598;&#25104;&#20102;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#27169;&#22359;&#12290; &#36890;&#36807;&#20840;&#38754;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#12289;&#30495;&#23454;&#19990;&#30028;&#30340;&#22312;&#32447;&#21307;&#23398;&#21672;&#35810;&#25968;&#25454;&#38598;&#65288;MedDG&#21644;KaMed&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06611v1 Announce Type: cross  Abstract: With appropriate data selection and training techniques, Large Language Models (LLMs) have demonstrated exceptional success in various medical examinations and multiple-choice questions. However, the application of LLMs in medical dialogue generation-a task more closely aligned with actual medical practice-has been less explored. This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses. In this work, we introduce the Medical dialogue with Knowledge enhancement and clinical Pathway encoding (MedKP) framework, which integrates an external knowledge enhancement module through a medical knowledge graph and an internal clinical pathway encoding via medical entities and physician actions. Evaluated with comprehensive metrics, our experiments on two large-scale, real-world online medical consultation datasets (MedDG and KaMed) demonstrate 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#21644;&#19982;&#21307;&#29983;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06609</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#31181;&#23376;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06609
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#21644;&#19982;&#21307;&#29983;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25512;&#29702;&#26159;&#21307;&#29983;&#22312;&#35780;&#20272;&#21644;&#31649;&#29702;&#24739;&#32773;&#26102;&#37319;&#29992;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#28041;&#21450;&#24314;&#35758;&#24517;&#35201;&#30340;&#26816;&#26597;&#65292;&#35786;&#26029;&#24739;&#32773;&#30142;&#30149;&#65292;&#24182;&#20915;&#23450;&#36866;&#24403;&#30340;&#27835;&#30103;&#31561;&#12290;&#20934;&#30830;&#30340;&#20020;&#24202;&#25512;&#29702;&#38656;&#35201;&#24191;&#27867;&#30340;&#21307;&#23398;&#30693;&#35782;&#21644;&#20016;&#23500;&#30340;&#20020;&#24202;&#32463;&#39564;&#65292;&#20026;&#21307;&#29983;&#35774;&#32622;&#20102;&#24456;&#39640;&#30340;&#38376;&#27099;&#12290;&#26368;&#36817;&#65292;&#20687;ChatGPT&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#31034;&#20986;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#38382;&#39064;&#65292;&#32780;LLMs&#30340;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#19982;&#21307;&#29983;&#30340;&#20020;&#24202;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06609v1 Announce Type: cross  Abstract: Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians. In this study, we introduce a 
&lt;/p&gt;</description></item><item><title>LLMs&#30340;&#31038;&#20250;&#26234;&#33021;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23384;&#22312;&#36739;&#20302;&#30340;&#31038;&#20250;&#26234;&#33021;&#21644;&#23398;&#26415;&#26234;&#33021;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06591</link><description>&lt;p&gt;
&#23398;&#26415;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26410;&#24517;&#20855;&#22791;&#31038;&#20250;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Academically intelligent LLMs are not necessarily socially intelligent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06591
&lt;/p&gt;
&lt;p&gt;
LLMs&#30340;&#31038;&#20250;&#26234;&#33021;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23384;&#22312;&#36739;&#20302;&#30340;&#31038;&#20250;&#26234;&#33021;&#21644;&#23398;&#26415;&#26234;&#33021;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23398;&#26415;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#31038;&#20250;&#26234;&#33021;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#21463;&#21040;&#24050;&#24314;&#31435;&#30340;&#20154;&#31867;&#31038;&#20250;&#26234;&#33021;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#29305;&#21035;&#26159;&#20025;&#23612;&#23572;&#183;&#25096;&#23572;&#26364;&#30340;&#31038;&#20250;&#26234;&#33021;&#29702;&#35770;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#31038;&#20250;&#22330;&#26223;&#30340;&#26631;&#20934;&#21270;&#31038;&#20250;&#26234;&#33021;&#27979;&#35797;&#65292;&#21517;&#20026;&#31038;&#20250;&#26234;&#33021;&#24773;&#22659;&#35780;&#20272;&#65288;SESI&#65289;&#12290;&#25105;&#20204;&#23545;13&#20010;&#36817;&#26399;&#27969;&#34892;&#30340;&#21644;&#23574;&#31471;&#30340;LLM&#20195;&#29702;&#22312;SESI&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#31038;&#20250;&#26234;&#33021;&#20173;&#26377;&#26174;&#33879;&#25913;&#36827;&#31354;&#38388;&#65292;&#34920;&#38754;&#21451;&#22909;&#26159;&#38169;&#35823;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;LLMs&#23637;&#31034;&#30340;&#31038;&#20250;&#26234;&#33021;&#19982;&#23398;&#26415;&#26234;&#33021;&#20043;&#38388;&#23384;&#22312;&#36739;&#20302;&#30340;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#31038;&#20250;&#26234;&#33021;&#19981;&#21516;&#20110;&#23398;&#26415;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06591v1 Announce Type: new  Abstract: The academic intelligence of large language models (LLMs) has made remarkable progress in recent times, but their social intelligence performance remains unclear. Inspired by established human social intelligence frameworks, particularly Daniel Goleman's social intelligence theory, we have developed a standardized social intelligence test based on real-world social scenarios to comprehensively assess the social intelligence of LLMs, termed as the Situational Evaluation of Social Intelligence (SESI). We conducted an extensive evaluation with 13 recent popular and state-of-art LLM agents on SESI. The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors. Moreover, there exists a relatively low correlation between the social intelligence and academic intelligence exhibited by LLMs, suggesting that social intelligence is distinct from academ
&lt;/p&gt;</description></item><item><title>&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06586</link><description>&lt;p&gt;
ContextGPT: &#23558;LLMs&#30693;&#35782;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06586
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#31227;&#21160;&#35745;&#31639;&#20013;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25991;&#29486;&#20013;&#26368;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#38656;&#35201;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#23558;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#21487;&#33021;&#21457;&#29983;&#30340;&#32972;&#26223;&#30340;&#24120;&#35782;&#30693;&#35782;&#27880;&#20837;HAR&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#20381;&#36182;&#20110;&#36923;&#36753;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#26412;&#20307;&#35770;&#65289;&#65292;&#20854;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#32500;&#25252;&#20197;&#25429;&#25417;&#26032;&#27963;&#21160;&#21644;&#19978;&#19979;&#25991;&#38656;&#35201;&#26174;&#33879;&#30340;&#20154;&#21147;&#24037;&#31243;&#21162;&#21147;&#12289;&#25216;&#26415;&#30693;&#35782;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#30340;&#24120;&#35782;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06586v1 Announce Type: cross  Abstract: Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human a
&lt;/p&gt;</description></item><item><title>AC-EVAL&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21476;&#20195;&#27721;&#35821;&#29702;&#35299;&#30340;&#21019;&#26032;&#22522;&#20934;&#65292;&#20998;&#20026;&#19977;&#20010;&#38590;&#24230;&#32423;&#21035;&#65292;&#28085;&#30422;&#21382;&#21490;&#30693;&#35782;&#12289;&#30701;&#25991;&#26412;&#21644;&#38271;&#25991;&#26412;&#29702;&#35299;&#65292;&#23545;LLMs&#30340;&#39640;&#32423;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.06574</link><description>&lt;p&gt;
AC-EVAL&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#21476;&#20195;&#27721;&#35821;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06574
&lt;/p&gt;
&lt;p&gt;
AC-EVAL&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21476;&#20195;&#27721;&#35821;&#29702;&#35299;&#30340;&#21019;&#26032;&#22522;&#20934;&#65292;&#20998;&#20026;&#19977;&#20010;&#38590;&#24230;&#32423;&#21035;&#65292;&#28085;&#30422;&#21382;&#21490;&#30693;&#35782;&#12289;&#30701;&#25991;&#26412;&#21644;&#38271;&#25991;&#26412;&#29702;&#35299;&#65292;&#23545;LLMs&#30340;&#39640;&#32423;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#21476;&#20195;&#27721;&#35821;&#22312;&#25429;&#25417;&#20016;&#23500;&#21382;&#21490;&#21644;&#25991;&#21270;&#36951;&#20135;&#31934;&#39635;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#38656;&#35201;&#26377;&#25928;&#35780;&#20272;&#23427;&#20204;&#23545;&#21476;&#20195;&#35821;&#22659;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AC-EVAL&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21476;&#20195;&#27721;&#35821;&#35821;&#22659;&#20013;&#30340;&#39640;&#32423;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;AC-EVAL&#20998;&#20026;&#19977;&#20010;&#38590;&#24230;&#32423;&#21035;&#65292;&#21453;&#26144;&#20102;&#35821;&#35328;&#29702;&#35299;&#30340;&#19981;&#21516;&#26041;&#38754;&#65306;&#19968;&#33324;&#21382;&#21490;&#30693;&#35782;&#12289;&#30701;&#25991;&#26412;&#29702;&#35299;&#21644;&#38271;&#25991;&#26412;&#29702;&#35299;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20219;&#21153;&#65292;&#28085;&#30422;&#21382;&#21490;&#20107;&#23454;&#12289;&#22320;&#29702;&#12289;&#31038;&#20250;&#20064;&#20439;&#12289;&#33402;&#26415;&#12289;&#21746;&#23398;&#12289;&#21476;&#20856;&#35799;&#27468;&#21644;&#25955;&#25991;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;&#38024;&#23545;&#33521;&#25991;&#21644;&#27721;&#35821;&#30340;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#22686;&#24378;&#21476;&#20195;&#27721;&#35821;&#29702;&#35299;&#28508;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06574v1 Announce Type: new  Abstract: Given the importance of ancient Chinese in capturing the essence of rich historical and cultural heritage, the rapid advancements in Large Language Models (LLMs) necessitate benchmarks that can effectively evaluate their understanding of ancient contexts. To meet this need, we present AC-EVAL, an innovative benchmark designed to assess the advanced knowledge and reasoning capabilities of LLMs within the context of ancient Chinese. AC-EVAL is structured across three levels of difficulty reflecting different facets of language comprehension: general historical knowledge, short text understanding, and long text comprehension. The benchmark comprises 13 tasks, spanning historical facts, geography, social customs, art, philosophy, classical poetry and prose, providing a comprehensive assessment framework. Our extensive evaluation of top-performing LLMs, tailored for both English and Chinese, reveals a substantial potential for enhancing ancie
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#23454;&#38469;&#20250;&#35758;&#24212;&#29992;&#20013;Speaker-Attributed ASR&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#36755;&#20986;&#26469;&#24494;&#35843;&#27169;&#22411;&#20197;&#20943;&#23569;&#35828;&#35805;&#32773;&#38169;&#35823;&#29575;&#65292;&#24182;&#25506;&#35752;&#20102;&#22686;&#24378;&#35828;&#35805;&#32773;&#23884;&#20837;&#27169;&#26495;&#25552;&#21462;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.06570</link><description>&lt;p&gt;
&#25913;&#36827;&#23454;&#38469;&#20250;&#35758;&#24212;&#29992;&#20013;&#35828;&#35805;&#32773;&#24402;&#22240;ASR&#20013;&#30340;&#35828;&#35805;&#32773;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#23454;&#38469;&#20250;&#35758;&#24212;&#29992;&#20013;Speaker-Attributed ASR&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#36755;&#20986;&#26469;&#24494;&#35843;&#27169;&#22411;&#20197;&#20943;&#23569;&#35828;&#35805;&#32773;&#38169;&#35823;&#29575;&#65292;&#24182;&#25506;&#35752;&#20102;&#22686;&#24378;&#35828;&#35805;&#32773;&#23884;&#20837;&#27169;&#26495;&#25552;&#21462;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#31471;&#21040;&#31471;&#20250;&#35758;&#36716;&#24405;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#26550;&#26500;&#19978;&#65292;&#24182;&#19988;&#22823;&#22810;&#25968;&#26159;&#22312;&#27169;&#25311;&#20250;&#35758;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26088;&#22312;&#20248;&#21270;Speaker-Attributed ASR (SA-ASR)&#31995;&#32479;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65288;&#22914;AMI&#20250;&#35758;&#35821;&#26009;&#24211;&#65289;&#20351;&#29992;&#30340;&#30740;&#31350;&#65292;&#20197;&#25913;&#36827;&#35821;&#38899;&#27573;&#30340;&#35828;&#35805;&#32773;&#20998;&#37197;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#28041;&#21450;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#12289;&#35828;&#35805;&#32773;&#36776;&#35782;&#65288;SD&#65289;&#21644;SA-ASR&#30340;&#29616;&#23454;&#29983;&#27963;&#24212;&#29992;&#30340;&#27969;&#31243;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20513;&#20351;&#29992;VAD&#36755;&#20986;&#27573;&#26469;&#24494;&#35843;SA-ASR&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#22312;&#27979;&#35797;&#26399;&#38388;&#23427;&#20063;&#34987;&#24212;&#29992;&#20110;VAD&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#23548;&#33268;&#35828;&#35805;&#32773;&#38169;&#35823;&#29575;&#65288;SER&#65289;&#30456;&#23545;&#20943;&#23569;&#39640;&#36798;28&#65285;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#22686;&#24378;&#20174;SD&#36755;&#20986;&#20013;&#25552;&#21462;&#35828;&#35805;&#32773;&#23884;&#20837;&#27169;&#26495;&#30340;&#31574;&#30053;&#65292;&#36825;&#20123;&#27169;&#26495;&#20316;&#20026;SA-ASR&#31995;&#32479;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23558;&#23427;&#20204;&#20174;SD&#36755;&#20986;&#20013;&#25552;&#21462;&#32780;&#19981;&#26159;&#20174;&#27880;&#37322;&#30340;&#35828;&#35805;&#32773;&#27573;&#20013;&#25552;&#21462;&#65292;&#20250;&#23548;&#33268;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06570v1 Announce Type: new  Abstract: Past studies on end-to-end meeting transcription have focused on model architecture and have mostly been evaluated on simulated meeting data. We present a novel study aiming to optimize the use of a Speaker-Attributed ASR (SA-ASR) system in real-life scenarios, such as the AMI meeting corpus, for improved speaker assignment of speech segments. First, we propose a pipeline tailored to real-life applications involving Voice Activity Detection (VAD), Speaker Diarization (SD), and SA-ASR. Second, we advocate using VAD output segments to fine-tune the SA-ASR model, considering that it is also applied to VAD segments during test, and show that this results in a relative reduction of Speaker Error Rate (SER) up to 28%. Finally, we explore strategies to enhance the extraction of the speaker embedding templates used as inputs by the SA-ASR system. We show that extracting them from SD output rather than annotated speaker segments results in a rela
&lt;/p&gt;</description></item><item><title>&#30830;&#35748;&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25581;&#31034;OpenAI&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#30340;&#19981;&#23436;&#25972;&#32454;&#33410;&#65292;&#24182;&#25506;&#31350;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#21487;&#38752;&#20844;&#24335;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.06563</link><description>&lt;p&gt;
&#25581;&#24320;&#32553;&#25918;&#23450;&#24459;&#20043;&#35868;&#65306;&#31532;&#19968;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Mystery of Scaling Laws: Part I
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06563
&lt;/p&gt;
&lt;p&gt;
&#30830;&#35748;&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25581;&#31034;OpenAI&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#30340;&#19981;&#23436;&#25972;&#32454;&#33410;&#65292;&#24182;&#25506;&#31350;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#21487;&#38752;&#20844;&#24335;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#34920;&#26126;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#31561;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#24130;&#23450;&#24459;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#21407;&#21017;&#22312;&#20248;&#21270;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21508;&#20010;&#26041;&#38754;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26368;&#32456;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#12289;Llama&#21644;Gemini&#65289;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;OpenAI&#30340;&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#24182;&#26410;&#25259;&#38706;&#25512;&#23548;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#20844;&#24335;&#25152;&#24517;&#38656;&#30340;&#23436;&#25972;&#32454;&#33410;&#65292;&#20182;&#20204;&#30340;&#32467;&#35770;&#20165;&#22522;&#20110;&#21253;&#21547;&#39640;&#36798;15&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#19968;&#20123;&#21518;&#32493;&#20316;&#21697;&#35797;&#22270;&#25581;&#31034;&#36825;&#20123;&#32454;&#33410;&#24182;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;&#20102;&#37325;&#35201;&#22240;&#32032;&#30340;&#35757;&#32451;&#20381;&#36182;&#24615;&#65292;&#22914;&#23398;&#20064;&#36895;&#29575;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#25209;&#37327;&#22823;&#23567;&#65292;&#23548;&#33268;&#23427;&#20204;&#26410;&#33021;&#24314;&#31435;&#19968;&#20010;&#21487;&#38752;&#30340;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#30340;&#20844;&#24335;&#12290;&#22312;&#26412;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06563v1 Announce Type: cross  Abstract: Scaling law principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training. These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of large language models such as GPT-4, Llama and Gemini. However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters. Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory. In this technical report, we confirm that the scaling 
&lt;/p&gt;</description></item><item><title>&#24320;&#28304;&#25216;&#26415;&#34429;&#28982;&#20419;&#36827;&#20102;&#31185;&#25216;&#36827;&#27493;&#65292;&#20294;&#20063;&#23384;&#22312;&#28389;&#29992;&#39118;&#38505;&#65292;&#30740;&#31350;&#21457;&#29616;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35843;&#25972;&#29992;&#20110;&#25552;&#20379;&#26377;&#20851;&#29359;&#32618;&#27963;&#21160;&#30340;&#19981;&#36947;&#24503;&#19988;&#20855;&#26377;&#20449;&#24687;&#24615;&#30340;&#31572;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.06537</link><description>&lt;p&gt;
&#23545;AI&#24320;&#25918;&#24615;&#30340;&#32771;&#37327;: &#21892;&#24847;&#33021;&#21542;&#34987;&#28389;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
On the Consideration of AI Openness: Can Good Intent Be Abused?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06537
&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#25216;&#26415;&#34429;&#28982;&#20419;&#36827;&#20102;&#31185;&#25216;&#36827;&#27493;&#65292;&#20294;&#20063;&#23384;&#22312;&#28389;&#29992;&#39118;&#38505;&#65292;&#30740;&#31350;&#21457;&#29616;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35843;&#25972;&#29992;&#20110;&#25552;&#20379;&#26377;&#20851;&#29359;&#32618;&#27963;&#21160;&#30340;&#19981;&#36947;&#24503;&#19988;&#20855;&#26377;&#20449;&#24687;&#24615;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24615;&#23545;&#31185;&#23398;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#36827;&#23637;&#20165;&#21487;&#33021;&#36890;&#36807;&#21508;&#31181;&#24320;&#28304;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#24211;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24320;&#25918;&#24615;&#20063;&#24847;&#21619;&#30528;&#25216;&#26415;&#21487;&#20197;&#34987;&#33258;&#30001;&#22320;&#29992;&#20110;&#31038;&#20250;&#26377;&#23475;&#30446;&#30340;&#12290;&#24320;&#28304;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#21487;&#20197;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#21527;&#65311;&#22914;&#26524;&#26159;&#36825;&#26679;&#65292;&#37027;&#20040;&#25216;&#26415;&#34987;&#29992;&#20110;&#36825;&#20123;&#30446;&#30340;&#20250;&#26377;&#22810;&#23481;&#26131;&#65311;&#26412;&#25991;&#22312;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#20010;&#20154;&#20915;&#31574;&#21487;&#33021;&#20135;&#29983;&#28145;&#36828;&#31038;&#20250;&#21518;&#26524;&#30340;&#39046;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;EVE&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;200&#20010;&#20851;&#20110;&#29359;&#32618;&#27963;&#21160;&#30340;&#38382;&#39064;&#21644;&#23545;&#24212;&#31572;&#26696;&#65292;&#22522;&#20110;200&#20010;&#38889;&#22269;&#20808;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#24191;&#27867;&#35748;&#21487;&#30340;&#24320;&#28304;LLM&#65292;&#26368;&#21021;&#25298;&#32477;&#22238;&#31572;&#19981;&#36947;&#24503;&#38382;&#39064;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36890;&#36807;EVE&#36827;&#34892;&#35843;&#25972;&#65292;&#25552;&#20379;&#20851;&#20110;&#29359;&#32618;&#27963;&#21160;&#30340;&#19981;&#36947;&#24503;&#19988;&#20855;&#26377;&#20449;&#24687;&#24615;&#30340;&#31572;&#26696;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#23613;&#31649;&#24320;&#28304;&#25216;&#26415;&#33021;&#22815;&#20419;&#36827;&#31185;&#23398;&#25216;&#26415;&#36827;&#27493;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#28389;&#29992;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06537v1 Announce Type: new  Abstract: Openness is critical for the advancement of science. In particular, recent rapid progress in AI has been made possible only by various open-source models, datasets, and libraries. However, this openness also means that technologies can be freely used for socially harmful purposes. Can open-source models or datasets be used for malicious purposes? If so, how easy is it to adapt technology for such goals? Here, we conduct a case study in the legal domain, a realm where individual decisions can have profound social consequences. To this end, we build EVE, a dataset consisting of 200 examples of questions and corresponding answers about criminal activities based on 200 Korean precedents. We found that a widely accepted open-source LLM, which initially refuses to answer unethical questions, can be easily tuned with EVE to provide unethical and informative answers about criminal activities. This implies that although open-source technologies c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#26469;&#24110;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#31995;&#32479;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25551;&#36848;&#22270;&#20687;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.06520</link><description>&lt;p&gt;
&#22914;&#20309;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#65306;&#22312;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#20013;&#20351;&#29992;&#24120;&#35782;
&lt;/p&gt;
&lt;p&gt;
How to Understand Named Entities: Using Common Sense for News Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#26469;&#24110;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#31995;&#32479;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25551;&#36848;&#22270;&#20687;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26088;&#22312;&#20351;&#29992;&#26032;&#38395;&#25991;&#31456;&#20027;&#20307;&#25551;&#36848;&#22270;&#20687;&#12290;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#19968;&#32452;&#26816;&#27979;&#21040;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#20154;&#29289;&#12289;&#32452;&#32455;&#21644;&#22320;&#28857;&#12290;&#26412;&#25991;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#26469;&#29702;&#35299;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#36890;&#36807;&#8220;&#29702;&#35299;&#8221;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#23558;&#26032;&#38395;&#20869;&#23481;&#19982;&#24120;&#35782;&#32852;&#31995;&#36215;&#26469;&#65292;&#24110;&#21161;&#20195;&#29702;&#20154;&#21306;&#20998;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#21033;&#29992;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#22806;&#30340;&#35789;&#35821;&#25551;&#36848;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06520v1 Announce Type: cross  Abstract: News captioning aims to describe an image with its news article body as input. It greatly relies on a set of detected named entities, including real-world people, organizations, and places. This paper exploits commonsense knowledge to understand named entities for news captioning. By ``understand'', we mean correlating the news content with common sense in the wild, which helps an agent to 1) distinguish semantically similar named entities and 2) describe named entities using words outside of training corpora. Our approach consists of three modules: (a) Filter Module aims to clarify the common sense concerning a named entity from two aspects: what does it mean? and what is it related to?, which divide the common sense into explanatory knowledge and relevant knowledge, respectively. (b) Distinguish Module aggregates explanatory knowledge from node-degree, dependency, and distinguish three aspects to distinguish semantically similar name
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#33258;&#21160;&#29983;&#25104;Python&#31243;&#24207;&#30340;TinyPy Generator&#24037;&#20855;&#21487;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#33021;&#36731;&#26494;&#29983;&#25104;&#22823;&#35268;&#27169;Python&#20195;&#30721;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.06503</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#33258;&#21160;&#29983;&#25104;Python&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Automatic Generation of Python Programs Using Context-Free Grammars
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06503
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#33258;&#21160;&#29983;&#25104;Python&#31243;&#24207;&#30340;TinyPy Generator&#24037;&#20855;&#21487;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#33021;&#36731;&#26494;&#29983;&#25104;&#22823;&#35268;&#27169;Python&#20195;&#30721;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#26032;&#30340;&#40644;&#37329;&#65292;&#25104;&#20026;&#21019;&#24314;&#26234;&#33021;&#31995;&#32479;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20195;&#30721;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TinyPy Generator&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#29983;&#25104;&#38543;&#26426;Python&#31243;&#24207;&#30340;&#24037;&#20855;&#12290;&#29983;&#25104;&#30340;&#31243;&#24207;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#20135;&#29983;&#35268;&#21017;&#65288;&#37319;&#29992;&#24052;&#31185;&#26031;-&#35834;&#23572;&#33539;&#24335;&#65288;BNF&#65289;&#26684;&#24335;&#65289;&#36882;&#24402;&#22320;&#29983;&#25104;&#20195;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#31243;&#24230;&#30340;&#20195;&#30721;&#65292;&#33539;&#22260;&#20174;&#20165;&#21253;&#21547;&#36171;&#20540;&#30340;&#20195;&#30721;&#21040;&#21253;&#21547;&#26465;&#20214;&#21644;&#24490;&#29615;&#30340;&#26356;&#22797;&#26434;&#20195;&#30721;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24037;&#20855;&#23454;&#29616;&#20102;&#36731;&#26494;&#30340;&#22823;&#35268;&#27169;Python&#20195;&#30721;&#29983;&#25104;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#30410;&#22788;&#12290;TinyPy Generator&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#29305;&#21035;&#26377;&#29992;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;Python&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22823;&#37327;Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06503v1 Announce Type: cross  Abstract: In recent years, data has emerged as the new gold, serving as a powerful tool for creating intelligent systems. However, procuring high-quality data remains challenging, especially for code. To address this, we developed TinyPy Generator, a tool that generates random Python programs using a context-free grammar. The generated programs are guaranteed to be correct by construction. Our system uses custom production rules (in the Backus-Naur Form (BNF) format) to recursively generate code. This allows us to generate code with different levels of complexity, ranging from code containing only assignments to more complex code containing conditionals and loops. Our proposed tool enables effortless large-scale Python code generation, beneficial for a wide range of applications. TinyPy Generator is particularly useful in the field of machine learning, where it can generate substantial amounts of Python code for training Python language models. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21475;&#22836;&#23545;&#35805;&#20013;&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#36827;&#34892;&#22810;&#35821;&#35328;&#20132;&#26367;&#39044;&#27979;&#65292;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#19982;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#23398;&#20250;&#20102;&#36776;&#21035;&#36755;&#20837;&#20449;&#21495;&#30340;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2403.06487</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#36827;&#34892;&#22810;&#35821;&#35328;&#20132;&#26367;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multilingual Turn-taking Prediction Using Voice Activity Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21475;&#22836;&#23545;&#35805;&#20013;&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#36827;&#34892;&#22810;&#35821;&#35328;&#20132;&#26367;&#39044;&#27979;&#65292;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#19982;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#23398;&#20250;&#20102;&#36776;&#21035;&#36755;&#20837;&#20449;&#21495;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#65288;VAP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#21475;&#22836;&#23545;&#35805;&#30340;&#39044;&#27979;&#24615;&#20132;&#26367;&#27169;&#22411;&#65292;&#28085;&#30422;&#33521;&#35821;&#12289;&#27721;&#35821;&#21644;&#26085;&#35821;&#12290;VAP&#27169;&#22411;&#25345;&#32493;&#39044;&#27979;&#21452;&#20154;&#23545;&#35805;&#20013;&#21442;&#19982;&#32773;&#21363;&#23558;&#21457;&#29983;&#30340;&#35821;&#38899;&#27963;&#21160;&#65292;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;Transformer&#25429;&#25417;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#20114;&#21160;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21333;&#19968;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;VAP&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#19981;&#20250;&#20135;&#29983;&#24456;&#22909;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#19977;&#31181;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25152;&#26377;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#19982;&#21333;&#35821;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#24050;&#23398;&#20250;&#36776;&#21035;&#36755;&#20837;&#20449;&#21495;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#23545;&#38899;&#35843;&#25935;&#24863;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#34987;&#35748;&#20026;&#23545;&#20110;&#20132;&#26367;&#38750;&#24120;&#37325;&#35201;&#30340;&#38901;&#24459;&#32447;&#32034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06487v1 Announce Type: new  Abstract: This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrast
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#25366;&#25496;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;COLA&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35686;&#25253;&#32858;&#21512;&#65292;&#33021;&#22815;&#32508;&#21512;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#35299;&#20915;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#35686;&#25253;&#32858;&#21512;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06485</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#30693;&#35782;&#24863;&#30693;&#35686;&#25253;&#32858;&#21512;&#65306;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06485
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#25366;&#25496;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;COLA&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35686;&#25253;&#32858;&#21512;&#65292;&#33021;&#22815;&#32508;&#21512;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#35299;&#20915;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#35686;&#25253;&#32858;&#21512;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20113;&#31995;&#32479;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#65292;&#31995;&#32479;&#25925;&#38556;&#20250;&#24341;&#21457;&#8220;&#35686;&#25253;&#39118;&#26292;&#8221;&#65292;&#21363;&#22823;&#37327;&#30456;&#20851;&#30340;&#35686;&#25253;&#12290;&#23613;&#31649;&#36825;&#20123;&#35686;&#25253;&#21487;&#20197;&#36861;&#28335;&#21040;&#23569;&#25968;&#20960;&#20010;&#26681;&#26412;&#21407;&#22240;&#65292;&#20294;&#21387;&#20498;&#24615;&#30340;&#25968;&#37327;&#20351;&#20154;&#24037;&#22788;&#29702;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#35686;&#25253;&#32858;&#21512;&#23545;&#24110;&#21161;&#24037;&#31243;&#24072;&#38598;&#20013;&#31934;&#21147;&#35299;&#20915;&#26681;&#26412;&#21407;&#22240;&#24182;&#20419;&#36827;&#25925;&#38556;&#35299;&#20915;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#25110;&#32479;&#35745;&#26041;&#27861;&#26469;&#32858;&#21512;&#35686;&#25253;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35686;&#25253;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#32780;&#32479;&#35745;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#22788;&#29702;&#19981;&#32463;&#24120;&#21457;&#29983;&#30340;&#35686;&#25253;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#21363;&#35686;&#25253;&#30340;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#65288;SOP&#65289;&#20316;&#20026;&#34917;&#20805;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#25366;&#25496;&#21644;LLM&#65288;Large Language Model&#65289;&#25512;&#29702;&#30340;&#22312;&#32447;&#35686;&#25253;&#32858;&#21512;&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;COLA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06485v1 Announce Type: cross  Abstract: Due to the scale and complexity of cloud systems, a system failure would trigger an "alert storm", i.e., massive correlated alerts. Although these alerts can be traced back to a few root causes, the overwhelming number makes it infeasible for manual handling. Alert aggregation is thus critical to help engineers concentrate on the root cause and facilitate failure resolution. Existing methods typically utilize semantic similarity-based methods or statistical methods to aggregate alerts. However, semantic similarity-based methods overlook the causal rationale of alerts, while statistical methods can hardly handle infrequent alerts.   To tackle these limitations, we introduce leveraging external knowledge, i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation. The correlation mining modul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.06448</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#30340;&#26080;&#30417;&#30563;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#26159;&#25351;&#20135;&#29983;&#36830;&#36143;&#20294;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#20013;&#24187;&#35273;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MIND&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;HELM&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;LLMs&#24187;&#35273;&#26816;&#27979;&#30340;&#26032;&#22522;&#20934;&#65292;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;LLM&#36755;&#20986;&#21644;&#20869;&#37096;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06448v1 Announce Type: cross  Abstract: Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EvoKD&#65292;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22320;&#22686;&#24378;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#21516;&#26102;&#25913;&#36827;&#23567;&#39046;&#22495;&#27169;&#22411;&#30340;&#20219;&#21153;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.06414</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#23398;&#20064;&#28436;&#36827;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Evolving Knowledge Distillation with Large Language Models and Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EvoKD&#65292;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22320;&#22686;&#24378;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#21516;&#26102;&#25913;&#36827;&#23567;&#39046;&#22495;&#27169;&#22411;&#30340;&#20219;&#21153;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#23558;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#29983;&#25104;&#24102;&#26631;&#27880;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#30452;&#25509;&#21033;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#21644;&#26631;&#27880;&#65292;&#32780;&#27809;&#26377;&#20805;&#20998;&#25506;&#32034;&#23427;&#20204;&#29702;&#35299;&#30446;&#26631;&#20219;&#21153;&#21644;&#33719;&#21462;&#26377;&#20215;&#20540;&#30693;&#35782;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvoKD&#65306;&#28436;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#27010;&#24565;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22320;&#22686;&#24378;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#21516;&#26102;&#25913;&#36827;&#23567;&#39046;&#22495;&#27169;&#22411;&#65288;&#23398;&#29983;&#27169;&#22411;&#65289;&#30340;&#20219;&#21153;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#20027;&#21160;&#20998;&#26512;&#23398;&#29983;&#27169;&#22411;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#28982;&#21518;&#22522;&#20110;&#35813;&#20998;&#26512;&#32508;&#21512;&#26631;&#35760;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;i
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06414v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks. However, their computational costs are prohibitively high. To address this issue, previous research has attempted to distill the knowledge of LLMs into smaller models by generating annotated data. Nonetheless, these works have mainly focused on the direct use of LLMs for text generation and labeling, without fully exploring their potential to comprehend the target task and acquire valuable knowledge. In this paper, we propose EvoKD: Evolving Knowledge Distillation, which leverages the concept of active learning to interactively enhance the process of data generation using large language models, simultaneously improving the task capabilities of small domain model (student model). Different from previous work, we actively analyze the student model's weaknesses, and then synthesize labeled samples based on the analysis. In addition, we provide i
&lt;/p&gt;</description></item><item><title>CLIcK&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#32780;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.06412</link><description>&lt;p&gt;
CLIcK&#65306;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06412
&lt;/p&gt;
&lt;p&gt;
CLIcK&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#32780;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38024;&#23545;&#38889;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#32570;&#20047;&#27979;&#35797;&#24517;&#35201;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#35768;&#22810;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#32763;&#35793;&#20174;&#33521;&#35821;&#23545;&#24212;&#25968;&#25454;&#38598;&#20013;&#34893;&#29983;&#20986;&#26469;&#30340;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#35270;&#19981;&#21516;&#30340;&#25991;&#21270;&#32972;&#26223;&#12290;&#20165;&#26377;&#23569;&#25968;&#20174;&#38889;&#22269;&#25968;&#25454;&#28304;&#25429;&#25417;&#25991;&#21270;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#30340;&#20165;&#26377;&#20559;&#35265;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#29421;&#31364;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CLIcK&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#12290;CLIcK&#23558;&#20854;&#25968;&#25454;&#26469;&#28304;&#20110;&#38889;&#22269;&#23448;&#26041;&#32771;&#35797;&#21644;&#25945;&#31185;&#20070;&#65292;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#65288;&#35821;&#35328;&#21644;&#25991;&#21270;&#65289;&#19979;&#30340;11&#20010;&#31867;&#21035;&#12290;&#23545;&#20110;CLIcK&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#21738;&#20123;&#25991;&#21270;&#21644;&#35821;&#35328;&#30693;&#35782;&#30340;&#32454;&#31890;&#24230;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06412v1 Announce Type: new  Abstract: Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36923;&#36753;&#27169;&#24335;&#35760;&#24518;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LMPM&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#22806;&#37096;&#23384;&#20648;&#32467;&#26500;&#23398;&#20064;&#21644;&#23384;&#20648;&#36923;&#36753;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#36923;&#36753;&#19968;&#33268;&#30340;&#32467;&#35770;&#65292;&#24182;&#24341;&#20837;&#23454;&#20307;&#25277;&#35937;&#26041;&#27861;&#26469;&#20943;&#23569;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#20013;&#30340;&#36923;&#36753;&#26080;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06410</link><description>&lt;p&gt;
&#29992;&#20110;&#34164;&#28085;&#26641;&#29983;&#25104;&#30340;&#36923;&#36753;&#27169;&#24335;&#35760;&#24518;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36923;&#36753;&#27169;&#24335;&#35760;&#24518;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LMPM&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#22806;&#37096;&#23384;&#20648;&#32467;&#26500;&#23398;&#20064;&#21644;&#23384;&#20648;&#36923;&#36753;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#36923;&#36753;&#19968;&#33268;&#30340;&#32467;&#35770;&#65292;&#24182;&#24341;&#20837;&#23454;&#20307;&#25277;&#35937;&#26041;&#27861;&#26469;&#20943;&#23569;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#20013;&#30340;&#36923;&#36753;&#26080;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#29983;&#25104;&#36830;&#36143;&#21487;&#20449;&#30340;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#28145;&#20837;&#30740;&#31350;&#20102;&#21033;&#29992;&#34164;&#28085;&#26641;&#26469;&#25551;&#36848;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36825;&#23637;&#31034;&#20102;&#19968;&#20010;&#20551;&#35774;&#22914;&#20309;&#20174;&#25903;&#25345;&#20107;&#23454;&#20013;&#25512;&#23548;&#20986;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#24573;&#35270;&#20102;&#20174;&#32473;&#23450;&#20107;&#23454;&#20013;&#29983;&#25104;&#20855;&#26377;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#20013;&#38388;&#32467;&#35770;&#30340;&#37325;&#35201;&#24615;&#65292;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#35770;&#65292;&#21066;&#24369;&#20102;&#34164;&#28085;&#26641;&#30340;&#25972;&#20307;&#21487;&#20449;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36923;&#36753;&#27169;&#24335;&#35760;&#24518;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LMPM&#65289;&#12290;LMPM&#32467;&#21512;&#20102;&#22806;&#37096;&#23384;&#20648;&#32467;&#26500;&#65292;&#23398;&#20064;&#21644;&#23384;&#20648;&#36923;&#36753;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#36923;&#36753;&#19968;&#33268;&#30340;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#20013;&#36923;&#36753;&#26080;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#20307;&#25277;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06410v1 Announce Type: cross  Abstract: Generating coherent and credible explanations remains a significant challenge in the field of AI. In recent years, researchers have delved into the utilization of entailment trees to depict explanations, which exhibit a reasoning process of how a hypothesis is deduced from the supporting facts. However, existing models often overlook the importance of generating intermediate conclusions with logical consistency from the given facts, leading to inaccurate conclusions and undermining the overall credibility of entailment trees. To address this limitation, we propose the logical pattern memory pre-trained model (LMPM). LMPM incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions. Furthermore, to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data, we introduce an entity abstraction approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#26469;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20013;&#30340;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.06402</link><description>&lt;p&gt;
&#19968;&#20992;&#20999;&#19981;&#36866;&#29992;&#65306;&#23398;&#20064;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#22810;&#23569;&#20363;&#20026;&#20102;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#26469;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20013;&#30340;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06402v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#26032; Abstract: &#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#39044;&#27979;&#27169;&#22411;&#24050;&#32463;&#20174;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#21040;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#31181;&#24494;&#35843;&#30340;&#26497;&#31471;&#24418;&#24335;&#28041;&#21450;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#65288;&#20923;&#32467;&#30340;&#35299;&#30721;&#22120;&#21442;&#25968;&#65289;&#21482;&#21463;&#21040;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#21464;&#21270;&#65288;&#31216;&#20026;&#25351;&#20196;&#25110;&#25552;&#31034;&#65289;&#30340;&#25511;&#21046;&#12290;ICL&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#23454;&#20363;&#20316;&#20026;&#31034;&#20363;&#12290;&#23613;&#31649;&#29616;&#26377;&#24037;&#20316;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20026;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#20351;&#29992;&#38745;&#24577;&#25968;&#37327;&#30340;&#31034;&#20363;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#20013;&#20351;&#29992;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;ICL&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#23545;&#20110;&#29305;&#23450;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#25512;&#29702;&#26102;&#20351;&#29992;&#30340;&#28436;&#31034;&#25968;&#37327;&#26159;&#21160;&#24577;&#35843;&#25972;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06402v1 Announce Type: new  Abstract: Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data. An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts). An important component of ICL is the use of a small number of labelled data instances as examples in the prompt. While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data. This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data inst
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GlossLM&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#23383;&#38388;&#27880;&#37322;&#30340;&#26377;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.06399</link><description>&lt;p&gt;
GlossLM: &#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#23383;&#38388;&#27880;&#37322;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GlossLM&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#23383;&#38388;&#27880;&#37322;&#30340;&#26377;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25991;&#29486;&#23398;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#20197;&#24418;&#24335;&#22914;&#25991;&#23383;&#38388;&#27880;&#37322;&#25991;&#26412;&#65288;IGT&#65289;&#30340;&#26041;&#24335;&#21019;&#24314;&#24102;&#27880;&#37322;&#30340;&#25991;&#26412;&#65292;IGT&#20197;&#36880;&#35789;&#32032;&#30340;&#26684;&#24335;&#25429;&#25417;&#20102;&#31934;&#32454;&#30340;&#24418;&#24577;&#21477;&#27861;&#20998;&#26512;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#25506;&#32034;&#20102;&#33258;&#21160;&#29983;&#25104;IGT&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35821;&#35328;&#20998;&#26512;&#30340;&#26102;&#38388;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#35821;&#35328;&#65288;&#23588;&#20854;&#26159;&#38656;&#35201;&#20445;&#25252;&#30340;&#35821;&#35328;&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;IGT&#25968;&#25454;&#26469;&#35757;&#32451;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#34987;&#25552;&#20986;&#20316;&#20026;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#26368;&#22823;&#24050;&#26377;IGT&#25968;&#25454;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;1.8k&#31181;&#35821;&#35328;&#30340;&#36229;&#36807;45&#19975;&#20010;&#20363;&#23376;&#65292;&#20197;&#20415;&#36827;&#34892;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;IGT&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#37096;&#20998;&#35821;&#26009;&#24211;&#19978;&#23545;&#19968;&#20010;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36827;&#19968;&#27493;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20998;&#21106;&#25968;&#25454;&#21644;&#22823;&#22411;&#21333;&#35821;&#25968;&#25454;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06399v1 Announce Type: new  Abstract: A key aspect of language documentation is the creation of annotated text in a format such as interlinear glossed text (IGT), which captures fine-grained morphosyntactic analyses in a morpheme-by-morpheme format. Prior work has explored methods to automatically generate IGT in order to reduce the time cost of language analysis. However, many languages (particularly those requiring preservation) lack sufficient IGT data to train effective models, and crosslingual transfer has been proposed as a method to overcome this limitation.   We compile the largest existing corpus of IGT data from a variety of sources, covering over 450k examples across 1.8k languages, to enable research on crosslingual transfer and IGT generation. Then, we pretrain a large multilingual model on a portion of this corpus, and further finetune it to specific languages. Our model is competitive with state-of-the-art methods for segmented data and large monolingual datas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26032;&#30340;&#32599;&#39532;&#23612;&#20122;&#21517;&#35789;&#22797;&#21512;&#35789;&#20851;&#31995;&#38598;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#27979;&#35797;&#21518;&#21457;&#29616;&#65292;&#32593;&#32476;&#30340;&#39044;&#27979;&#19982;&#20154;&#31867;&#21028;&#26029;&#23384;&#22312;&#19968;&#33268;&#65292;&#21363;&#20351;&#26159;&#22312;&#20154;&#31867;&#19968;&#33268;&#29575;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#12290;&#38656;&#35201;&#19968;&#20010;&#26356;&#22909;&#30340;&#20851;&#31995;&#28165;&#21333;&#12290;</title><link>https://arxiv.org/abs/2403.06360</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#33258;&#21160;&#35299;&#37322;&#32599;&#39532;&#23612;&#20122;&#21517;&#35789;&#22797;&#21512;&#35789;
&lt;/p&gt;
&lt;p&gt;
Human and Automatic Interpretation of Romanian Noun Compounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06360
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26032;&#30340;&#32599;&#39532;&#23612;&#20122;&#21517;&#35789;&#22797;&#21512;&#35789;&#20851;&#31995;&#38598;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#27979;&#35797;&#21518;&#21457;&#29616;&#65292;&#32593;&#32476;&#30340;&#39044;&#27979;&#19982;&#20154;&#31867;&#21028;&#26029;&#23384;&#22312;&#19968;&#33268;&#65292;&#21363;&#20351;&#26159;&#22312;&#20154;&#31867;&#19968;&#33268;&#29575;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#12290;&#38656;&#35201;&#19968;&#20010;&#26356;&#22909;&#30340;&#20851;&#31995;&#28165;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#31867;&#20284;"&#38795;&#23376;&#38144;&#21806;"&#21644;"&#28779;&#28798;&#22823;&#29993;&#21334;"&#36825;&#26679;&#30340;&#21517;&#35789;&#22797;&#21512;&#35789;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#30340;&#39044;&#26399;&#21547;&#20041;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32780;&#35328;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25429;&#25417;&#22797;&#21512;&#35789;&#25104;&#21592;&#38388;&#19981;&#21516;&#21547;&#20041;&#30340;&#35821;&#20041;&#20851;&#31995;&#28165;&#21333;&#12290;&#38024;&#23545;&#32599;&#39532;&#23612;&#20122;&#30340;&#22797;&#21512;&#35789;&#65292;&#20854;&#24418;&#24577;&#21477;&#27861;&#19982;&#33521;&#35821;&#30340;&#23545;&#24212;&#29289;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20851;&#31995;&#38598;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#32773;&#21644;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#32593;&#32476;&#30340;&#39044;&#27979;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#65292;&#21363;&#20351;&#22312;&#20154;&#31867;&#19968;&#33268;&#29575;&#36739;&#20302;&#30340;&#22320;&#26041;&#20063;&#26159;&#22914;&#27492;&#12290;&#19968;&#33268;&#24615;&#19982;&#25152;&#36873;&#20851;&#31995;&#30340;&#39057;&#29575;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19981;&#21463;&#32467;&#26500;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#26368;&#24120;&#36873;&#25321;&#30340;&#20851;&#31995;&#19981;&#23646;&#20110;&#21313;&#20845;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#20851;&#31995;&#20043;&#19968;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#19968;&#20010;&#26356;&#22909;&#30340;&#20851;&#31995;&#28165;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06360v1 Announce Type: cross  Abstract: Determining the intended, context-dependent meanings of noun compounds like "shoe sale" and "fire sale" remains a challenge for NLP. Previous work has relied on inventories of semantic relations that capture the different meanings between compound members. Focusing on Romanian compounds, whose morphosyntax differs from that of their English counterparts, we propose a new set of relations and test it with human annotators and a neural net classifier. Results show an alignment of the network's predictions and human judgments, even where the human agreement rate is low. Agreement tracks with the frequency of the selected relations, regardless of structural differences. However, the most frequently selected relation was none of the sixteen labeled semantic relations, indicating the need for a better relation inventory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;CLIP&#20026;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#65292;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#30340;&#26126;&#26174;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.06355</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#27604;&#20132;&#21449;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06355
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;CLIP&#20026;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#65292;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#30340;&#26126;&#26174;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06355v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#22810;&#27169;&#24577;&#35821;&#20041;&#29702;&#35299;&#38656;&#35201;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#20197;&#25552;&#21462;&#29992;&#25143;&#35328;&#36766;&#32972;&#21518;&#30340;&#30495;&#23454;&#24847;&#22270;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#24212;&#29992;&#21452;&#32534;&#30721;&#22120;&#32467;&#26500;&#20998;&#21035;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#65292;&#20294;&#26410;&#33021;&#23398;&#20064;&#36328;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#23454;&#29616;&#36328;&#27169;&#24577;&#30340;&#28145;&#24230;&#20449;&#24687;&#20132;&#20114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;CLIP&#20026;&#24341;&#23548;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26550;&#26500;&#65292;&#20197;&#25191;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#25237;&#24433;&#21040;&#32479;&#19968;&#30340;&#28145;&#24230;&#31354;&#38388;&#20013;&#12290;&#22312;&#22810;&#27169;&#24577;&#35773;&#21050;&#26816;&#27979;&#65288;MMSD&#65289;&#21644;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65288;MMSA&#65289;&#20219;&#21153;&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#29305;&#24449;&#23545;&#40784;&#31574;&#30053;&#27604;&#20855;&#26377;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#30340;&#27169;&#22411;&#21644;&#29978;&#33267;&#23500;&#21547;&#30693;&#35782;&#30340;&#27169;&#22411;&#24102;&#26469;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06355v1 Announce Type: new  Abstract: Multi-modal semantic understanding requires integrating information from different modalities to extract users' real intention behind words. Most previous work applies a dual-encoder structure to separately encode image and text, but fails to learn cross-modal feature alignment, making it hard to achieve cross-modal deep information interaction. This paper proposes a novel CLIP-guided contrastive-learning-based architecture to perform multi-modal feature alignment, which projects the features derived from different modalities into a unified deep space. On multi-modal sarcasm detection (MMSD) and multi-modal sentiment analysis (MMSA) tasks, the experimental results show that our proposed model significantly outperforms several baselines, and our feature alignment strategy brings obvious performance gain over models with different aggregating methods and models even enriched with knowledge. More importantly, our model is simple to implemen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35757;&#32451;LLaMA-2&#27169;&#22411;&#26469;&#23398;&#20064;&#38463;&#22982;&#21704;&#25289;&#35821;&#65292;&#20351;&#29992;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#36830;&#25509;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06354</link><description>&lt;p&gt;
&#38463;&#22982;&#21704;&#25289;&#35821;LLaMA&#21644;LLaVA&#65306;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;LLMs
&lt;/p&gt;
&lt;p&gt;
Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35757;&#32451;LLaMA-2&#27169;&#22411;&#26469;&#23398;&#20064;&#38463;&#22982;&#21704;&#25289;&#35821;&#65292;&#20351;&#29992;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#36830;&#25509;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;GPT-4&#21644;LLaMA&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#29978;&#33267;&#24320;&#22987;&#25797;&#38271;&#36328;&#36234;&#35270;&#35273;&#21644;&#38899;&#39057;&#31561;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#25104;&#21151;&#65292;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#23569;&#12290;&#36825;&#19968;&#19981;&#36275;&#22312;&#24320;&#28304;&#27169;&#22411;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;LLaMA-2&#36827;&#34892;&#35757;&#32451;&#20197;&#35762;&#38463;&#22982;&#21704;&#25289;&#35821;&#65292;&#36825;&#26159;&#19968;&#31181;&#20840;&#29699;&#26377;&#36229;&#36807;5&#21315;&#19975;&#20154;&#21475;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#20294;&#21487;&#29992;&#25968;&#25454;&#36828;&#23569;&#20110;&#33521;&#35821;&#31561;&#35821;&#35328;&#12290;&#25105;&#20204;&#37319;&#29992;&#20043;&#21069;&#29992;&#20110;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#35757;&#32451;LLMs&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24320;&#28304;&#32763;&#35793;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#23558;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20174;&#25968;&#30334;&#19975;&#20010;&#35760;&#21495;&#22686;&#38271;&#21040;&#25968;&#21313;&#20159;&#20010;&#12290;&#25105;&#20204;&#36890;&#36807;&#36830;&#25509;&#22270;&#20687;&#32534;&#30721;&#22120;&#24182;&#35757;&#32451;&#32763;&#35793;&#35270;&#35273;&#29702;&#35299;&#20219;&#21153;&#30340;&#22270;&#20687;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06354v1 Announce Type: new  Abstract: Large Language Models (LLMs) like GPT-4 and LLaMA have shown incredible proficiency at natural language processing tasks and have even begun to excel at tasks across other modalities such as vision and audio. Despite their success, LLMs often struggle to perform well on low-resource languages because there is so little training data available. This shortcoming is especially prevalent with open source models. In this work, we explore training LLaMA-2 to speak Amharic, a language which is spoken by over 50 million people world wide, but has orders of magnitude less data available than languages like English. We employ methods previously used for training LLMs on other languages with data scarcity, and use open source translation models to perform data augmentation and grow our dataset from millions of tokens to billions. We further enhance the capabilities of our model by connecting an image encoder and training on a translated visual inst
&lt;/p&gt;</description></item><item><title>&#20026;&#21360;&#24230;&#35821;&#35328;&#21019;&#24314;&#20102;&#19968;&#20010;&#35206;&#30422;22&#31181;&#35821;&#35328;&#12289;&#21253;&#21547;251B&#26631;&#35760;&#21644;74.8M&#25351;&#23548;-&#21709;&#24212;&#23545;&#30340;&#36164;&#28304;&#22871;&#20214;&#65292;&#32467;&#21512;&#39640;&#24230;&#31579;&#36873;&#30340;&#25968;&#25454;&#12289;&#26377;&#20215;&#20540;&#30340;&#26410;&#39564;&#35777;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#29992;&#20110;&#31579;&#36873;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24178;&#20928;&#24320;&#28304;&#27969;&#27700;&#32447;&#65292;&#20197;&#21450;&#29992;&#20110;&#25351;&#23548;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06350</link><description>&lt;p&gt;
IndicLLMSuite: &#20026;&#21360;&#24230;&#35821;&#35328;&#21019;&#24314;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#34013;&#22270;
&lt;/p&gt;
&lt;p&gt;
IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06350
&lt;/p&gt;
&lt;p&gt;
&#20026;&#21360;&#24230;&#35821;&#35328;&#21019;&#24314;&#20102;&#19968;&#20010;&#35206;&#30422;22&#31181;&#35821;&#35328;&#12289;&#21253;&#21547;251B&#26631;&#35760;&#21644;74.8M&#25351;&#23548;-&#21709;&#24212;&#23545;&#30340;&#36164;&#28304;&#22871;&#20214;&#65292;&#32467;&#21512;&#39640;&#24230;&#31579;&#36873;&#30340;&#25968;&#25454;&#12289;&#26377;&#20215;&#20540;&#30340;&#26410;&#39564;&#35777;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#29992;&#20110;&#31579;&#36873;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24178;&#20928;&#24320;&#28304;&#27969;&#27700;&#32447;&#65292;&#20197;&#21450;&#29992;&#20110;&#25351;&#23548;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33521;&#25991;LLM&#65288;Large Language Models&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#23450;&#21046;&#36164;&#28304;&#65292;&#26500;&#24314;&#20854;&#20182;&#35821;&#35328;&#30340;&#21487;&#27604;&#27169;&#22411;&#30340;&#36827;&#23637;&#21463;&#38459;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19987;&#38376;&#20026;&#21457;&#23637;&#21360;&#24230;&#35821;&#35328;LLM&#32780;&#35774;&#35745;&#30340;&#22823;&#37327;&#36164;&#28304;&#22871;&#20214;&#26469;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#28085;&#30422;&#20102;22&#31181;&#35821;&#35328;&#65292;&#21253;&#21547;&#24635;&#20849;251B&#26631;&#35760;&#21644;7480&#19975;&#20010;&#25351;&#23548;-&#21709;&#24212;&#23545;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#32463;&#36807;&#31934;&#24515;&#31579;&#36873;&#30340;&#25163;&#21160;&#39564;&#35777;&#25968;&#25454;&#12289;&#23578;&#26410;&#39564;&#35777;&#20294;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24178;&#20928;&#30340;&#12289;&#24320;&#28304;&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#20174;&#21508;&#31181;&#26469;&#28304;&#31579;&#36873;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#32593;&#31449;&#12289;PDF&#21644;&#35270;&#39057;&#65292;&#34701;&#20837;&#20102;&#29228;&#21462;&#12289;&#28165;&#29702;&#12289;&#26631;&#35760;&#21644;&#21435;&#37325;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#23545;&#20110;&#25351;&#23548;&#24494;&#35843;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;&#29616;&#26377;&#30340;&#21360;&#24230;&#25968;&#25454;&#38598;&#65292;&#23558;&#33521;&#25991;&#25968;&#25454;&#38598;&#32763;&#35793;/&#36716;&#20889;&#25104;&#21360;&#24230;&#35821;&#35328;&#65292;&#24182;&#21033;&#29992;&#20102;LLaMa2&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06350v1 Announce Type: new  Abstract: Despite the considerable advancements in English LLMs, the progress in building comparable models for other languages has been hindered due to the scarcity of tailored resources. Our work aims to bridge this divide by introducing an expansive suite of resources specifically designed for the development of Indic LLMs, covering 22 languages, containing a total of 251B tokens and 74.8M instruction-response pairs. Recognizing the importance of both data quality and quantity, our approach combines highly curated manually verified data, unverified yet valuable data, and synthetic data. We build a clean, open-source pipeline for curating pre-training data from diverse sources, including websites, PDFs, and videos, incorporating best practices for crawling, cleaning, flagging, and deduplication. For instruction-fine tuning, we amalgamate existing Indic datasets, translate/transliterate English datasets into Indian languages, and utilize LLaMa2 a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ACT&#26694;&#26550;&#65292;&#36890;&#36807;&#32422;&#26463;&#39564;&#35777;&#22120;&#33258;&#21160;&#35745;&#31639;&#27599;&#20010;&#21709;&#24212;&#30340;&#32422;&#26463;&#28385;&#24847;&#29575;&#65292;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#19982;&#33258;&#21160;&#32422;&#26463;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06326</link><description>&lt;p&gt;
&#20174;&#25351;&#20196;&#21040;&#32422;&#26463;&#65306;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#19982;&#33258;&#21160;&#32422;&#26463;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ACT&#26694;&#26550;&#65292;&#36890;&#36807;&#32422;&#26463;&#39564;&#35777;&#22120;&#33258;&#21160;&#35745;&#31639;&#27599;&#20010;&#21709;&#24212;&#30340;&#32422;&#26463;&#28385;&#24847;&#29575;&#65292;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#19982;&#33258;&#21160;&#32422;&#26463;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#40784;&#23545;&#20110;&#23558;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#20026;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#20026;&#25152;&#26377;&#31867;&#22411;&#30340;&#25351;&#20196;&#25552;&#20379;&#20154;&#31867;&#27880;&#37322;&#65292;&#29305;&#21035;&#26159;&#20855;&#26377;&#23450;&#21046;&#32422;&#26463;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29992;&#25143;&#25351;&#20196;&#36890;&#24120;&#21253;&#21547;&#32422;&#26463;&#26465;&#20214;&#12290;&#34429;&#28982;&#35780;&#20272;&#25972;&#20010;&#25351;&#20196;&#30340;&#21709;&#24212;&#36136;&#37327;&#36890;&#24120;&#25104;&#26412;&#39640;&#26114;&#65292;&#20294;&#39640;&#25928;&#22320;&#35780;&#20272;&#32422;&#26463;&#26465;&#20214;&#30340;&#28385;&#24847;&#29575;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;NLP&#20219;&#21153;&#20013;&#30340;&#24120;&#35265;&#32422;&#26463;&#26465;&#20214;&#65292;&#23558;&#23427;&#20204;&#22522;&#20110;&#20854;&#21442;&#25968;&#31867;&#22411;&#20998;&#31867;&#20026;&#19977;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;ACT&#65288;Aligning to ConsTraints&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#20026;&#24102;&#32422;&#26463;&#29992;&#25143;&#23545;&#40784;&#29983;&#25104;&#30417;&#30563;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ACT&#20351;&#29992;&#32422;&#26463;&#39564;&#35777;&#22120;&#65292;&#36825;&#20123;&#39564;&#35777;&#22120;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#26131;&#20110;&#23454;&#29616;&#65292;&#26469;&#35745;&#31639;&#27599;&#20010;&#21709;&#24212;&#30340;&#32422;&#26463;&#28385;&#24847;&#29575;&#65288;CSR&#65289;&#12290;&#23427;&#20026;&#27599;&#20010;&#25552;&#31034;&#21462;&#26679;&#22810;&#20010;&#21709;&#24212;&#24182;&#25910;&#38598;&#20559;&#22909;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06326v1 Announce Type: cross  Abstract: User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints. We observe that user instructions typically contain constraints. While assessing response quality in terms of the whole instruction is often costly, efficiently evaluating the satisfaction rate of constraints is feasible. We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints. Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response. It samples multiple responses for each prompt and collect preference labels ba
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#23454;&#20307;&#35782;&#21035;&#19978;&#20855;&#26377;&#22522;&#26412;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#20173;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;</title><link>https://arxiv.org/abs/2403.06301</link><description>&lt;p&gt;
LIEDER: &#29992;&#20110;&#35821;&#31687;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#35328;&#23398;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06301
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#23454;&#20307;&#35782;&#21035;&#19978;&#20855;&#26377;&#22522;&#26412;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#20173;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; LIEDER &#30340;&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#35814;&#32454;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#23545;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#12289;&#22797;&#25968;&#24615;&#21644;&#26032;&#39062;&#24615;&#31561;&#22235;&#20010;&#20851;&#38190;&#35821;&#20041;&#23646;&#24615;&#30340;&#35748;&#30693;&#27700;&#24179;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25152;&#26377;&#36825;&#20123;&#23646;&#24615;&#37117;&#34920;&#29616;&#20986;&#25935;&#24863;&#24615;&#65292;&#38500;&#20102;&#26032;&#39062;&#24615;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#23578;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06301v1 Announce Type: new  Abstract: Discourse Entity (DE) recognition is the task of identifying novel and known entities introduced within a text. While previous work has found that large language models have basic, if imperfect, DE recognition abilities (Schuster and Linzen, 2022), it remains largely unassessed which of the fundamental semantic properties that govern the introduction and subsequent reference to DEs they have knowledge of. We propose the Linguistically-Informed Evaluation for Discourse Entity Recognition (LIEDER) dataset that allows for a detailed examination of language models' knowledge of four crucial semantic properties: existence, uniqueness, plurality, and novelty. We find evidence that state-of-the-art large language models exhibit sensitivity to all of these properties except novelty, which demonstrates that they have yet to reach human-level language understanding abilities.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#21644;&#29289;&#20307;&#26816;&#27979;&#30340;Transformer&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23454;&#29616;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#20449;&#24687;&#30340;&#20114;&#34917;&#20849;&#20139;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06292</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#21644;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transformer based Multitask Learning for Image Captioning and Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#21644;&#29289;&#20307;&#26816;&#27979;&#30340;Transformer&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23454;&#29616;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#20449;&#24687;&#30340;&#20114;&#34917;&#20849;&#20139;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;&#33258;&#20027;&#23548;&#33322;&#21644;&#31227;&#21160;&#24615;&#36825;&#26679;&#30340;&#20960;&#20010;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#65292;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#21644;&#29289;&#20307;&#26816;&#27979;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#21644;&#29289;&#20307;&#26816;&#27979;&#32467;&#21512;&#25104;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TICOD&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#21644;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#21644;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#33719;&#24471;&#30340;&#25439;&#22833;&#26469;&#21516;&#26102;&#35757;&#32451;&#20004;&#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#32852;&#21512;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21463;&#30410;&#20110;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#21644;&#29289;&#20307;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#32593;&#32476;&#38598;&#25104;&#65292;&#24182;&#32852;&#21512;&#25191;&#34892;&#20004;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06292v1 Announce Type: cross  Abstract: In several real-world scenarios like autonomous navigation and mobility, to obtain a better visual understanding of the surroundings, image captioning and object detection play a crucial role. This work introduces a novel multitask learning framework that combines image captioning and object detection into a joint model. We propose TICOD, Transformer-based Image Captioning and Object detection model for jointly training both tasks by combining the losses obtained from image captioning and object detection networks. By leveraging joint training, the model benefits from the complementary information shared between the two tasks, leading to improved performance for image captioning. Our approach utilizes a transformer-based architecture that enables end-to-end network integration for image captioning and object detection and performs both tasks jointly. We evaluate the effectiveness of our approach through comprehensive experiments on the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21387;&#32553;&#22312;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#35777;&#26126;&#20102;&#21387;&#32553;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#20043;&#38388;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06265</link><description>&lt;p&gt;
&#25286;&#35299;&#20998;&#35789;&#65306;&#35780;&#20272;&#25991;&#26412;&#21387;&#32553;&#21450;&#20854;&#19982;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21387;&#32553;&#22312;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#35777;&#26126;&#20102;&#21387;&#32553;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#20043;&#38388;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21387;&#32553;&#26159;BPE&#26368;&#24120;&#35265;&#30340;&#20998;&#35789;&#31639;&#27861;&#30340;&#37325;&#35201;&#22522;&#30784;&#65292;&#20294;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#21387;&#32553;&#37325;&#35201;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#35770;&#36848;&#20102;&#21387;&#32553;&#30340;&#29702;&#35770;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;0-gram&#35821;&#35328;&#24314;&#27169;&#65292;&#21363;&#20026;&#25152;&#26377;&#26631;&#35760;&#20998;&#37197;&#30456;&#31561;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21387;&#32553;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#29992;&#25991;&#26723;&#30340;&#25968;&#37327;&#26469;&#25511;&#21046;&#22810;&#20010;BPE&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#33021;&#21147;&#65306;&#20174;100&#19975;&#20010;&#25991;&#26723;&#21040;&#30456;&#24403;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#22522;&#20110;&#23383;&#31526;&#30340;&#20998;&#35789;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#20998;&#35789;&#22120;&#39044;&#35757;&#32451;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#21387;&#32553;&#26159;&#20998;&#35789;&#30340;&#21487;&#38752;&#20869;&#22312;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06265v1 Announce Type: cross  Abstract: Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokeniza
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCORE&#30340;&#33258;&#30417;&#30563;&#23545;&#40784;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#35757;&#32451;&#31574;&#30053;&#23398;&#20064;&#31867;&#20284;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#20110;&#20869;&#23481;&#30456;&#20851;&#20219;&#21153;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;HuBERT&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06260</link><description>&lt;p&gt;
SCORE: &#33258;&#30417;&#30563;&#23545;&#40784;&#24494;&#35843;&#65292;&#25552;&#21319;&#20869;&#23481;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCORE&#30340;&#33258;&#30417;&#30563;&#23545;&#40784;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#35757;&#32451;&#31574;&#30053;&#23398;&#20064;&#31867;&#20284;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#20110;&#20869;&#23481;&#30456;&#20851;&#20219;&#21153;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;HuBERT&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06260v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;&#23545;&#20110;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22522;&#30784;&#35821;&#38899;&#27169;&#22411;&#26469;&#33719;&#24471;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#20197;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#33719;&#24471;&#31283;&#20581;&#24615;&#33021;&#65292;&#33258;&#30417;&#30563;&#24494;&#35843;&#65288;SSFT&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#36890;&#36807;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#29992;&#20110;&#25552;&#39640;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#30417;&#30563;&#23545;&#40784;&#65288;SCORE&#65289;&#24494;&#35843;&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;SSFT&#26041;&#27861;&#65292;&#20197;&#35843;&#25972;SSL&#35821;&#38899;&#34920;&#31034;&#20197;&#36866;&#24212;&#19982;&#20869;&#23481;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#31181;&#23545;&#40784;&#35757;&#32451;&#31574;&#30053;&#65292;&#26088;&#22312;&#20174;&#25200;&#21160;&#35821;&#38899;&#21644;&#21407;&#22987;&#35821;&#38899;&#20013;&#23398;&#20064;&#31867;&#20284;&#30340;&#34920;&#31034;&#12290;&#36890;&#24120;&#29992;&#20110;&#20869;&#23481;&#30456;&#20851;&#20219;&#21153;&#65288;ASR&#65289;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#34987;&#24212;&#29992;&#20110;&#33719;&#21462;&#25200;&#21160;&#35821;&#38899;&#12290;SCORE&#24494;&#35843;&#30340;HuBERT&#22312;&#36229;&#32423;&#22522;&#20934;&#19978;&#34920;&#29616;&#20248;&#20110;&#26222;&#36890;HuBERT&#65292;&#20165;&#36890;&#36807;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#23569;&#37327;&#26102;&#38388;&#30340;&#24494;&#35843;&#65288;&lt;5&#23567;&#26102;&#65289;&#21363;&#21487;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#38899;&#32032;&#35782;&#21035;&#21644;&#22522;&#20110;&#26597;&#35810;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06260v1 Announce Type: new  Abstract: There is a growing interest in cost-effective self-supervised fine-tuning (SSFT) of self-supervised learning (SSL)-based speech models to obtain task-specific representations. These task-specific representations are used for robust performance on various downstream tasks by fine-tuning on the labelled data. This work presents a cost-effective SSFT method named Self-supervised Correspondence (SCORE) fine-tuning to adapt the SSL speech representations for content-related tasks. The proposed method uses a correspondence training strategy, aiming to learn similar representations from perturbed speech and original speech. Commonly used data augmentation techniques for content-related tasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT outperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of fine-tuning (&lt; 5 hrs) on a single GPU for automatic speech recognition, phoneme recognition, and query-by-examp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.06259</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Conceptual Knowledge for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#20165;&#25506;&#35752;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#28982;&#32780;LLMs&#26159;&#21542;&#20855;&#26377;&#20462;&#25913;&#27010;&#24565;&#30340;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;LLMs&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptEdit&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#32423;&#21035;&#30340;&#23450;&#20041;&#65292;&#20294;&#23427;&#20204;&#20063;&#26377;&#28508;&#21147;&#25197;&#26354;LLMs&#20013;&#30456;&#20851;&#30340;&#23454;&#20363;&#30693;&#35782;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#21487;&#20197;&#28608;&#21457;&#23545;&#26356;&#22909;&#29702;&#35299;LLMs&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#20027;&#39029;&#20301;&#20110;https://zjunlp.github.io/project/ConceptEdit&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06259v1 Announce Type: cross  Abstract: Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.
&lt;/p&gt;</description></item><item><title>ICE-PIXIU&#27169;&#22411;&#23558;&#20013;&#25991;&#21644;&#33521;&#25991;&#37329;&#34701;&#20998;&#26512;&#32479;&#19968;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25552;&#21319;&#21452;&#35821;&#37329;&#34701;&#24314;&#27169;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06249</link><description>&lt;p&gt;
&#27809;&#26377;&#23396;&#23707;&#35821;&#35328;:&#32479;&#19968;&#20013;&#33521;&#25991;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#25351;&#23548;&#25968;&#25454;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06249
&lt;/p&gt;
&lt;p&gt;
ICE-PIXIU&#27169;&#22411;&#23558;&#20013;&#25991;&#21644;&#33521;&#25991;&#37329;&#34701;&#20998;&#26512;&#32479;&#19968;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25552;&#21319;&#21452;&#35821;&#37329;&#34701;&#24314;&#27169;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;&#37329;&#34701;&#20998;&#26512;&#65292;&#20294;&#23427;&#20204;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#22312;&#21333;&#19968;&#35821;&#35328;&#39046;&#22495;&#65292;&#26410;&#20805;&#20998;&#24320;&#21457;&#20013;&#33521;&#25991;&#21452;&#35821;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#24341;&#20837; ICE-PIXIU&#65292;&#26080;&#32541;&#34701;&#21512; ICE-INTENT &#27169;&#22411;&#21644; ICE-FLARE &#21452;&#35821;&#37329;&#34701;&#20998;&#26512;&#22522;&#20934;&#12290;ICE-PIXIU &#29420;&#29305;&#22320;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#20013;&#25991;&#20219;&#21153;&#65292;&#20197;&#21450;&#32763;&#35793;&#21644;&#21407;&#22987;&#33521;&#25991;&#25968;&#25454;&#38598;&#65292;&#20016;&#23500;&#20102;&#21452;&#35821;&#37329;&#34701;&#24314;&#27169;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#12290;&#23427;&#25552;&#20379;&#20102;&#23545;&#22810;&#31181;&#27169;&#22411;&#21464;&#20307;&#30340;&#26080;&#38480;&#35775;&#38382;&#26435;&#38480;&#65292;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#36328;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#25968;&#25454;&#30340;&#22823;&#37327;&#32534;&#35793;&#65292;&#20197;&#21450;&#19968;&#20010;&#20855;&#26377;&#19987;&#23478;&#27880;&#37322;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324; 10 &#20010; NLP &#20219;&#21153;&#65292;20 &#20010;&#21452;&#35821;&#19987;&#29992;&#20219;&#21153;&#65292;&#20849;&#35745;1,185k &#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#24443;&#24213;&#35780;&#20272;&#24378;&#35843;&#20102;&#23558;&#36825;&#20123;&#21452;&#35821;&#25968;&#25454;&#38598;&#32435;&#20837;&#30340;&#20248;&#21183;&#65292;&#23588;&#20854;&#22312;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06249v1 Announce Type: cross  Abstract: While the progression of Large Language Models (LLMs) has notably propelled financial analysis, their application has largely been confined to singular language realms, leaving untapped the potential of bilingual Chinese-English capacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating the ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis. ICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated and original English datasets, enriching the breadth and depth of bilingual financial modeling. It provides unrestricted access to diverse model variants, a substantial compilation of diverse cross-lingual and multi-modal instruction data, and an evaluation benchmark with expert annotations, comprising 10 NLP tasks, 20 bilingual specific tasks, totaling 1,185k datasets. Our thorough evaluation emphasizes the advantages of incorporating these bilingual datasets, especially in t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TRAD&#26694;&#26550;&#65292;&#36890;&#36807;&#27493;&#39588;&#24335;&#24605;&#32500;&#26816;&#32034;&#21644;&#23545;&#40784;&#20915;&#31574;&#35299;&#20915;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06221</link><description>&lt;p&gt;
&#29992;&#27493;&#39588;&#24335;&#24605;&#32500;&#26816;&#32034;&#21644;&#23545;&#40784;&#20915;&#31574;&#22686;&#24378;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06221
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TRAD&#26694;&#26550;&#65292;&#36890;&#36807;&#27493;&#39588;&#24335;&#24605;&#32500;&#26816;&#32034;&#21644;&#23545;&#40784;&#20915;&#31574;&#35299;&#20915;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#32463;&#34987;&#26500;&#24314;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#65292;&#22914;&#32593;&#32476;&#23548;&#33322;&#21644;&#22312;&#32447;&#36141;&#29289;&#65292;&#36825;&#26159;&#22240;&#20026;LLM&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#21644;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;&#36825;&#20123;&#30740;&#31350;&#20013;&#65292;&#35768;&#22810;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#26469;&#23454;&#29616;&#27867;&#21270;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#20294;&#23569;&#25968;&#32771;&#34385;&#20102;&#22914;&#20309;&#36873;&#25321;&#21644;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#31034;&#20363;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36712;&#36857;&#32423;&#26816;&#32034;&#21644;&#20351;&#29992;&#36712;&#36857;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#65292;&#20197;&#25552;&#39640;&#20195;&#29702;&#22312;&#19968;&#20123;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#26816;&#32034;&#20986;&#30340;&#21487;&#20449;&#31034;&#20363;&#32570;&#20047;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#29366;&#24577;&#36716;&#31227;&#21160;&#24577;&#65292;&#19988;&#36755;&#20837;&#38271;&#19988;&#21253;&#21547;&#22823;&#37327;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65288;TRAD&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;TRAD&#39318;&#20808;&#36827;&#34892;&#24605;&#32500;&#26816;&#32034;&#65292;&#23454;&#29616;&#27493;&#39588;&#32423;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06221v1 Announce Type: new  Abstract: Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;LoRA&#65288;PLoRA&#65289;&#29992;&#20110;&#20154;&#31867;&#20013;&#24515;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#65292;&#22312;&#36866;&#24212;&#23506;&#21551;&#21160;&#38382;&#39064;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06208</link><description>&lt;p&gt;
&#38754;&#21521;&#20154;&#31867;&#20013;&#24515;&#25991;&#26412;&#29702;&#35299;&#30340;&#20010;&#24615;&#21270;LoRA
&lt;/p&gt;
&lt;p&gt;
Personalized LoRA for Human-Centered Text Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06208
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;LoRA&#65288;PLoRA&#65289;&#29992;&#20110;&#20154;&#31867;&#20013;&#24515;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#65292;&#22312;&#36866;&#24212;&#23506;&#21551;&#21160;&#38382;&#39064;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#20010;&#24615;&#21270;&#24212;&#29992;&#20013;&#65292;&#29992;&#25143;&#26631;&#35760;&#36890;&#24120;&#36798;&#21040;&#30334;&#19975;&#32423;&#65292;&#24182;&#19988;&#27809;&#26377;&#26126;&#30830;&#30340;&#20855;&#20307;&#35821;&#20041;&#65292;&#22240;&#27492;&#23545;&#20110;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20197;&#29992;&#20110;&#20154;&#31867;&#20013;&#24515;&#25991;&#26412;&#29702;&#35299;&#65288;HCTU&#65289;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#26631;&#20934;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65288;&#22914;LoRA&#65289;&#38656;&#35201;&#20026;&#27599;&#20010;&#29992;&#25143;&#35760;&#24518;&#22823;&#37327;&#36866;&#37197;&#22120;&#22871;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25554;&#25300;&#65288;PnP&#65289;&#26694;&#26550;&#30340;&#20010;&#24615;&#21270;LoRA&#65288;PLoRA&#65289;&#29992;&#20110;HCTU&#20219;&#21153;&#12290;PLoRA&#22312;PLMs&#20013;&#26159;&#26377;&#25928;&#30340;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#65292;&#21487;&#20197;&#21160;&#24577;&#37096;&#32626;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20010;&#24615;&#21270;&#30340;dropout&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#31574;&#30053;&#65292;&#22240;&#27492;&#25552;&#20986;&#30340;PLoRA&#21487;&#20197;&#24456;&#22909;&#22320;&#36866;&#24212;&#23569;&#37327;/&#38646;&#27425;&#23398;&#20064;&#22330;&#26223;&#20197;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;HCTU&#20219;&#21153;&#30340;&#20840;/&#23569;/&#38646;&#27425;&#23398;&#20064;&#22330;&#26223;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21363;&#20351;&#23427;&#30340;&#36866;&#37197;&#22120;&#22871;&#20214;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06208v1 Announce Type: new  Abstract: Effectively and efficiently adapting a pre-trained language model (PLM) for human-centered text understanding (HCTU) is challenging since user tokens are million-level in most personalized applications and do not have concrete explicit semantics. A standard and parameter-efficient approach (e.g., LoRA) necessitates memorizing numerous suits of adapters for each user. In this work, we introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework for the HCTU task. PLoRA is effective, parameter-efficient, and dynamically deploying in PLMs. Moreover, a personalized dropout and a mutual information maximizing strategies are adopted and hence the proposed PLoRA can be well adapted to few/zero-shot learning scenarios for the cold-start issue. Experiments conducted on four benchmark datasets show that the proposed method outperforms existing methods in full/few/zero-shot learning scenarios for the HCTU task, even though it has fewe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#30417;&#30563;&#30340;&#34920;&#31034;&#23545;&#40784;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#20808;&#22825;&#30450;&#35270;&#24341;&#36215;&#20102;&#26080;&#27169;&#24577;&#21644;&#24863;&#35273;&#30456;&#20851;&#30340;&#35821;&#20041;&#39046;&#22495;&#20013;&#30340;&#27010;&#24565;&#37325;&#32452;&#65292;&#24182;&#30830;&#23450;&#20102;&#30456;&#20851;&#30340;&#35821;&#20041;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.06204</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#19968;&#33268;&#30340;&#20154;&#31867;&#27010;&#24565;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Identifying and interpreting non-aligned human conceptual representations using language modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06204
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#30417;&#30563;&#30340;&#34920;&#31034;&#23545;&#40784;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#20808;&#22825;&#30450;&#35270;&#24341;&#36215;&#20102;&#26080;&#27169;&#24577;&#21644;&#24863;&#35273;&#30456;&#20851;&#30340;&#35821;&#20041;&#39046;&#22495;&#20013;&#30340;&#27010;&#24565;&#37325;&#32452;&#65292;&#24182;&#30830;&#23450;&#20102;&#30456;&#20851;&#30340;&#35821;&#20041;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#20154;&#20204;&#22312;&#19990;&#30028;&#20013;&#30340;&#32463;&#39564;&#26159;&#21542;&#22609;&#36896;&#20102;&#27010;&#24565;&#34920;&#24449;&#21644;&#35789;&#27719;&#35821;&#20041;&#30340;&#38382;&#39064;&#26159;&#38271;&#26399;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#35789;&#32852;&#24819;&#12289;&#29305;&#24449;&#21015;&#34920;&#21644;&#30456;&#20284;&#24230;&#35780;&#23450;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#23545;&#35782;&#21035;&#20986;&#30340;&#28508;&#22312;&#32500;&#24230;&#36827;&#34892;&#20027;&#35266;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#30417;&#30563;&#30340;&#34920;&#31034;&#23545;&#40784;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30830;&#23450;&#20102;&#20004;&#32452;&#20010;&#20307;&#26159;&#21542;&#20849;&#20139;&#26576;&#19968;&#31867;&#21035;&#30340;&#30456;&#21516;&#22522;&#30784;&#65292;&#24182;&#35299;&#37322;&#20102;&#20182;&#20204;&#22312;&#21738;&#20123;&#26041;&#38754;&#19981;&#21516;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#19968;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20808;&#22825;&#30450;&#35270;&#22312;&#26080;&#27169;&#24577;&#21644;&#24863;&#35273;&#30456;&#20851;&#30340;&#35821;&#20041;&#39046;&#22495;&#20013;&#35825;&#23548;&#27010;&#24565;&#37325;&#32452;&#65292;&#24182;&#30830;&#23450;&#20102;&#30456;&#20851;&#30340;&#35821;&#20041;&#36716;&#21464;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;GloVe&#65289;&#24212;&#29992;&#30417;&#30563;&#29305;&#24449;&#20462;&#21098;&#65292;&#20197;&#20174;&#35789;&#23884;&#20837;&#20013;&#20248;&#21270;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20462;&#21098;&#30830;&#23450;&#20102;&#19968;&#20010;&#23376;&#38598;&#30340;&#20445;&#30041;&#30340;GloVe&#29305;&#24449;&#65292;&#20248;&#21270;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06204v1 Announce Type: new  Abstract: The question of whether people's experience in the world shapes conceptual representation and lexical semantics is longstanding. Word-association, feature-listing and similarity rating tasks aim to address this question but require a subjective interpretation of the latent dimensions identified. In this study, we introduce a supervised representational-alignment method that (i) determines whether two groups of individuals share the same basis of a certain category, and (ii) explains in what respects they differ. In applying this method, we show that congenital blindness induces conceptual reorganization in both a-modal and sensory-related verbal domains, and we identify the associated semantic shifts. We first apply supervised feature-pruning to a language model (GloVe) to optimize prediction accuracy of human similarity judgments from word embeddings. Pruning identifies one subset of retained GloVe features that optimizes prediction of 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;LLMTrack&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#32467;&#21512;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;IMU&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#23556;&#36712;&#36857;&#35782;&#21035;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26080;&#38656;&#35757;&#32451;&#22312;&#19987;&#38376;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.06201</link><description>&lt;p&gt;
&#24744;&#34987;&#36861;&#36394;&#20102;&#21527;&#65311;&#21457;&#29616;LLMs&#30340;&#38646;&#23556;&#36712;&#36857;&#36319;&#36394;&#30340;&#23041;&#21147;&#65281;
&lt;/p&gt;
&lt;p&gt;
Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06201
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;LLMTrack&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#32467;&#21512;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;IMU&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#23556;&#36712;&#36857;&#35782;&#21035;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26080;&#38656;&#35757;&#32451;&#22312;&#19987;&#38376;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#20316;&#20026;&#22522;&#26412;&#32452;&#20214;&#30340;&#35752;&#35770;&#20013;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#34701;&#20837;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#65288;AIoT&#65289;&#20013;&#20197;&#35299;&#37322;&#22797;&#26434;&#36712;&#36857;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LLMTrack&#65292;&#35813;&#27169;&#22411;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#36827;&#34892;&#38646;&#23556;&#36712;&#36857;&#35782;&#21035;&#65292;&#36890;&#36807;&#37319;&#29992;&#23558;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#26041;&#27861;&#19982;&#26410;&#32463;&#22788;&#29702;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#25968;&#25454;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#21333;&#25552;&#31034;&#25216;&#26415;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26088;&#22312;&#25361;&#25112;&#23427;&#20197;&#20855;&#26377;&#23460;&#20869;&#21644;&#23460;&#22806;&#24773;&#26223;&#29305;&#24449;&#30340;&#19981;&#21516;&#36712;&#36857;&#12290;&#22312;&#20004;&#31181;&#27979;&#35797;&#24773;&#26223;&#20013;&#65292;LLMTrack &#19981;&#20165;&#28385;&#36275;&#29978;&#33267;&#36229;&#36807;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#24403;&#20195;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35774;&#23450;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#32780;&#19988;&#26080;&#38656;&#23545;&#19987;&#38376;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06201v1 Announce Type: cross  Abstract: There is a burgeoning discussion around the capabilities of Large Language Models (LLMs) in acting as fundamental components that can be seamlessly incorporated into Artificial Intelligence of Things (AIoT) to interpret complex trajectories. This study introduces LLMTrack, a model that illustrates how LLMs can be leveraged for Zero-Shot Trajectory Recognition by employing a novel single-prompt technique that combines role-play and think step-by-step methodologies with unprocessed Inertial Measurement Unit (IMU) data. We evaluate the model using real-world datasets designed to challenge it with distinct trajectories characterized by indoor and outdoor scenarios. In both test scenarios, LLMTrack not only meets but exceeds the performance benchmarks set by traditional machine learning approaches and even contemporary state-of-the-art deep learning models, all without the requirement of training on specialized datasets. The results of our 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;(MSLMs)&#21450;&#25552;&#20986;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;Mipha&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20987;&#36133;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#24378;&#22823;MSLMs&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#25351;&#21335;</title><link>https://arxiv.org/abs/2403.06199</link><description>&lt;p&gt;
&#36890;&#36807;&#23567;&#35821;&#35328;&#27169;&#22411;&#20840;&#38754;&#25913;&#36896;&#22810;&#27169;&#24577;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overhaul of Multimodal Assistant with Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;(MSLMs)&#21450;&#25552;&#20986;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;Mipha&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20987;&#36133;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#24378;&#22823;MSLMs&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#23637;&#31034;&#20102;&#22312;&#19982;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22521;&#35757;&#21644;&#25512;&#29702;&#38454;&#27573;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#38754;&#20020;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#21644;&#29992;&#25143;&#31038;&#21306;&#20013;&#21463;&#20247;&#30340;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;MSLMs&#65289;&#30340;&#35774;&#35745;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mipha&#30340;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#26088;&#22312;&#22312;&#22810;&#20010;&#26041;&#38754;&#20043;&#38388;&#21019;&#36896;&#21327;&#21516;&#20316;&#29992;&#65306;&#35270;&#35273;&#34920;&#31034;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;Mipha-3B&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;MLLMs&#65292;&#29305;&#21035;&#26159;LLaVA-1.5-13B&#12290;&#36890;&#36807;&#35814;&#32454;&#35752;&#35770;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21457;&#23637;&#24378;&#22823;&#30340;MSLMs&#30340;&#35265;&#35299;&#21644;&#25351;&#21335;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;MLLMs&#30340;&#33021;&#21147;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06199v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have showcased impressive skills in tasks related to visual understanding and reasoning. Yet, their widespread application faces obstacles due to the high computational demands during both the training and inference phases, restricting their use to a limited audience within the research and user communities. In this paper, we investigate the design aspects of Multimodal Small Language Models (MSLMs) and propose an efficient multimodal assistant named Mipha, which is designed to create synergy among various aspects: visual representation, language models, and optimization strategies. We show that without increasing the volume of training data, our Mipha-3B outperforms the state-of-the-art large MLLMs, especially LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide insights and guidelines for developing strong MSLMs that rival the capabilities of MLLMs. Our code is availa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#35774;&#35745;&#19981;&#21516;&#25552;&#31034;&#24182;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06149</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33258;&#21160;&#35780;&#20998;&#20889;&#20316;&#25991;&#31456;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Automatically Score Proficiency of Written Essays?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#35774;&#35745;&#19981;&#21516;&#25552;&#31034;&#24182;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#36807;&#21435;50&#24180;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#21160;&#35780;&#20998;&#20316;&#25991;&#65288;AES&#65289;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#25928;&#26524;&#26041;&#38754;&#20173;&#26377;&#35768;&#22810;&#19981;&#36275;&#20043;&#22788;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;LLMs&#30340;&#33021;&#21147;&#65292;&#37492;&#20110;&#23427;&#20204;&#24378;&#22823;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#26469;&#20998;&#26512;&#21644;&#26377;&#25928;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;ChatGPT&#21644;Llama&#12290;&#25105;&#20204;&#26088;&#22312;&#26816;&#26597;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#20004;&#20010;&#23618;&#38754;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#21363;&#22312;&#25972;&#20307;&#19978;&#21644;&#22312;&#20010;&#20307;&#20889;&#20316;&#29305;&#24449;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#35774;&#35745;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#30340;&#26368;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06149v1 Announce Type: cross  Abstract: Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness. Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays. We experimented with two popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential to this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right prompt depend
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21512;&#25104;&#27969;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#32467;&#26500;&#29702;&#35299;&#65292;&#23558;&#31232;&#30095;&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.06139</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#32467;&#26500;&#29702;&#35299;&#30340;&#32454;&#31890;&#24230;&#21512;&#25104;&#27969;&#25968;&#25454;&#29992;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06139
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21512;&#25104;&#27969;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#32467;&#26500;&#29702;&#35299;&#65292;&#23558;&#31232;&#30095;&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#25968;&#25454;&#31232;&#30095;&#65292;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#23545;&#29992;&#25143;&#35780;&#35770;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#36890;&#24120;&#24615;&#33021;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#26497;&#24230;&#31232;&#30095;&#30340;&#29992;&#25143;&#25968;&#25454;&#25110;&#38271;&#23614;&#26631;&#31614;&#26102;&#12290;&#26368;&#36817;&#65292;LLMs&#30340;&#20986;&#29616;&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#29983;&#25104;&#36741;&#21161;&#29992;&#25143;&#26723;&#26696;&#65292;&#20026;&#36825;&#20123;&#38382;&#39064;&#24341;&#20837;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#38590;&#20197;&#36866;&#24212;&#22797;&#26434;&#30340;&#27969;&#25968;&#25454;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#27969;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#23558;&#31232;&#30095;&#29992;&#25143;&#20998;&#31867;&#20026;&#19977;&#31867;&#65306;&#20013;&#23614;&#12289;&#38271;&#23614;&#21644;&#26497;&#31471;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLMs&#65292;&#20840;&#38754;&#29702;&#35299;&#27969;&#25968;&#25454;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#22270;&#20803;&#32032;&#65292;&#21253;&#25324;&#23616;&#37096;-&#20840;&#23616;&#22270;&#29702;&#35299;&#12289;&#20108;&#38454;&#20851;&#31995;&#25552;&#21462;&#21644;&#20135;&#21697;&#23646;&#24615;&#29702;&#35299;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06139v1 Announce Type: cross  Abstract: Due to the sparsity of user data, sentiment analysis on user reviews in e-commerce platforms often suffers from poor performance, especially when faced with extremely sparse user data or long-tail labels. Recently, the emergence of LLMs has introduced new solutions to such problems by leveraging graph structures to generate supplementary user profiles. However, previous approaches have not fully utilized the graph understanding capabilities of LLMs and have struggled to adapt to complex streaming data environments. In this work, we propose a fine-grained streaming data synthesis framework that categorizes sparse users into three categories: Mid-tail, Long-tail, and Extreme. Specifically, we design LLMs to comprehensively understand three key graph elements in streaming data, including Local-global Graph Understanding, Second-Order Relationship Extraction, and Product Attribute Understanding, which enables the generation of high-quality
&lt;/p&gt;</description></item><item><title>FMPAF&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22238;&#24402;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#32654;&#32852;&#20648;&#20027;&#24109;&#26032;&#38395;&#21457;&#24067;&#20250;&#27807;&#36890;&#23545;&#37329;&#34701;&#24066;&#22330;&#24433;&#21709;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.06115</link><description>&lt;p&gt;
FMPAF&#65306;&#32852;&#37030;&#20648;&#22791;&#20027;&#24109;&#22914;&#20309;&#24433;&#21709;&#37329;&#34701;&#24066;&#22330;&#65311;&#20851;&#20110;&#20182;&#20204;&#35821;&#35328;&#30340;&#32454;&#31890;&#24230;&#36135;&#24065;&#25919;&#31574;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06115
&lt;/p&gt;
&lt;p&gt;
FMPAF&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22238;&#24402;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#32654;&#32852;&#20648;&#20027;&#24109;&#26032;&#38395;&#21457;&#24067;&#20250;&#27807;&#36890;&#23545;&#37329;&#34701;&#24066;&#22330;&#24433;&#21709;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#22830;&#38134;&#34892;&#27807;&#36890;&#30340;&#26377;&#25928;&#24615;&#26159;&#36135;&#24065;&#25919;&#31574;&#20256;&#23548;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#32654;&#32852;&#20648;&#20027;&#24109;&#30340;&#25919;&#31574;&#27807;&#36890;&#23545;&#21508;&#31181;&#37329;&#34701;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#20294;&#22823;&#37096;&#20998;&#25991;&#29486;&#20381;&#36182;&#20110;&#22522;&#20110;&#35268;&#21017;&#25110;&#35789;&#20856;&#30340;&#26041;&#27861;&#26469;&#35299;&#26512;&#20027;&#24109;&#30340;&#35821;&#35328;&#65292;&#23548;&#33268;&#23545;&#21253;&#21547;&#22312;&#38750;&#35328;&#35821;&#24773;&#32490;&#20013;&#30340;&#25919;&#31574;&#31435;&#22330;&#30340;&#24494;&#22937;&#20449;&#24687;&#32570;&#20047;&#20998;&#26512;&#12290;&#22312;&#24403;&#21069;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#36135;&#24065;&#25919;&#31574;&#20998;&#26512;&#26694;&#26550;&#65288;FMPAF&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22238;&#24402;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#32654;&#32852;&#20648;&#20027;&#24109;&#26032;&#38395;&#21457;&#24067;&#20250;&#27807;&#36890;&#23545;&#37329;&#34701;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#22411;&#24615;&#33021;&#22312;&#19981;&#21516;&#32454;&#31890;&#24230;&#12289;&#27169;&#24577;&#21644;&#27807;&#36890;&#22330;&#26223;&#19979;&#30340;&#24191;&#27867;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06115v1 Announce Type: cross  Abstract: The effectiveness of central bank communication is a crucial aspect of monetary policy transmission. While recent research has examined the influence of policy communication by the chairs of the Federal Reserve on various financial variables, much of the literature relies on rule-based or dictionary-based methods in parsing the language of the chairs, leaving nuanced information about policy stance contained in nonverbal emotion out of the analysis. In the current study, we propose the Fine-Grained Monetary Policy Analysis Framework (FMPAF), a novel approach that integrates large language models (LLMs) with regression analysis to provide a comprehensive analysis of the impact of the press-conference communications of chairs of the Federal Reserve on financial markets. We conduct extensive comparisons of model performance under different levels of granularity, modalities, and communication scenarios. Based on our preferred specification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32454;&#31890;&#24230;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#25991;&#26412;&#20013;&#26816;&#27979;&#24494;&#22937;&#24773;&#24863;&#30340;&#25361;&#25112;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.06108</link><description>&lt;p&gt;
&#22312;&#21253;&#21547;&#25968;&#25454;&#22686;&#24378;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32454;&#31890;&#24230;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#25991;&#26412;&#20013;&#26816;&#27979;&#24494;&#22937;&#24773;&#24863;&#30340;&#25361;&#25112;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#24773;&#24863;&#26816;&#27979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#12289;&#25163;&#21160;&#27880;&#37322;&#30340;&#25991;&#26412;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35299;&#20915;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24494;&#22937;&#24773;&#24863;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#35299;&#20915;&#25991;&#26412;&#24773;&#24863;&#26816;&#27979;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#21512;&#25104;&#35813;&#39046;&#22495;&#21508;&#20010;&#25968;&#25454;&#38598;&#19978;&#26041;&#27861;&#21644;&#24615;&#33021;&#30340;&#32508;&#36848;&#35770;&#25991;&#30340;&#28508;&#22312;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06108v1 Announce Type: cross  Abstract: This paper delves into enhancing the classification performance on the GoEmotions dataset, a large, manually annotated dataset for emotion detection in text. The primary goal of this paper is to address the challenges of detecting subtle emotions in text, a complex issue in Natural Language Processing (NLP) with significant practical applications. The findings offer valuable insights into addressing the challenges of emotion detection in text and suggest directions for future research, including the potential for a survey paper that synthesizes methods and performances across various datasets in this domain.
&lt;/p&gt;</description></item><item><title>&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#23545;&#37197;&#23545;&#32452;&#21512;&#21644;&#35780;&#20272;&#37327;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#30340;&#35774;&#35745;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.06100</link><description>&lt;p&gt;
&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#36827;&#34892;&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic design optimization of preference-based subjective evaluation with online learning in crowdsourcing environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06100
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#23545;&#37197;&#23545;&#32452;&#21512;&#21644;&#35780;&#20272;&#37327;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#30340;&#35774;&#35745;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#26159;&#35780;&#20215;&#29983;&#25104;&#24335;&#23186;&#20307;&#21487;&#38752;&#24615;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#24222;&#22823;&#30340;&#37197;&#23545;&#32452;&#21512;&#20351;&#24471;&#23427;&#26080;&#27861;&#24212;&#29992;&#20110;&#21033;&#29992;&#20247;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#36827;&#34892;&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#30340;&#33258;&#21160;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#23545;&#37197;&#23545;&#32452;&#21512;&#36873;&#25321;&#21644;&#35780;&#20272;&#37327;&#20998;&#37197;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25490;&#24207;&#31639;&#27861;&#30340;&#22522;&#20110;&#21916;&#22909;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#20855;&#26377;&#26368;&#23567;&#26679;&#26412;&#37327;&#30340;&#35780;&#20272;&#30446;&#26631;&#30340;&#23436;&#20840;&#39034;&#24207;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#25903;&#25345;&#22312;&#20247;&#21253;&#25152;&#38656;&#30340;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#24182;&#34892;&#21644;&#24322;&#27493;&#25191;&#34892;&#12290;&#25105;&#20204;&#23545;&#21512;&#25104;&#35821;&#38899;&#30340;&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#36890;&#36807;&#23558;&#37197;&#23545;&#32452;&#21512;&#20174;351&#20943;&#23569;&#21040;83&#24182;&#20998;&#37197;&#26368;&#20248;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06100v1 Announce Type: cross  Abstract: A preference-based subjective evaluation is a key method for evaluating generative media reliably. However, its huge combinations of pairs prohibit it from being applied to large-scale evaluation using crowdsourcing. To address this issue, we propose an automatic optimization method for preference-based subjective evaluation in terms of pair combination selections and allocation of evaluation volumes with online learning in a crowdsourcing environment. We use a preference-based online learning method based on a sorting algorithm to identify the total order of evaluation targets with minimum sample volumes. Our online learning algorithm supports parallel and asynchronous execution under fixed-budget conditions required for crowdsourcing. Our experiment on preference-based subjective evaluation of synthetic speech shows that our method successfully optimizes the test by reducing pair combinations from 351 to 83 and allocating optimal eva
&lt;/p&gt;</description></item><item><title>VidProM&#26159;&#19968;&#20010;&#21253;&#21547;167&#19975;&#20010;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25581;&#31034;&#20102;&#30495;&#23454;&#29992;&#25143;&#25552;&#31034;&#23545;&#35270;&#39057;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06098</link><description>&lt;p&gt;
VidProM&#65306;&#19968;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;&#30495;&#23454;&#21363;&#26102;&#22270;&#24211;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06098
&lt;/p&gt;
&lt;p&gt;
VidProM&#26159;&#19968;&#20010;&#21253;&#21547;167&#19975;&#20010;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25581;&#31034;&#20102;&#30495;&#23454;&#29992;&#25143;&#25552;&#31034;&#23545;&#35270;&#39057;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sora&#30340;&#21040;&#26469;&#26631;&#24535;&#30528;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#24102;&#26469;&#20102;&#35270;&#39057;&#29983;&#25104;&#21644;&#28508;&#22312;&#24212;&#29992;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;Sora&#20197;&#21450;&#20854;&#20182;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#25552;&#31034;&#65292;&#20294;&#30446;&#21069;&#23578;&#27809;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#21253;&#21547;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VidProM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30001;167&#19975;&#20010;&#26469;&#33258;&#30495;&#23454;&#29992;&#25143;&#30340;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#30001;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;669&#19975;&#20010;&#35270;&#39057;&#20197;&#21450;&#19968;&#20123;&#30456;&#20851;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#36825;&#19968;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31574;&#23637;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;VidProM&#19982;DiffusionDB&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#21518;&#32773;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#22270;&#24211;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#26032;&#25552;&#31034;&#25968;&#25454;&#38598;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06098v1 Announce Type: cross  Abstract: The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, as well as other text-to-video diffusion models, highly relies on the prompts, and there is no publicly available dataset featuring a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 million unique text-to-video prompts from real users. Additionally, the dataset includes 6.69 million videos generated by four state-of-the-art diffusion models and some related data. We initially demonstrate the curation of this large-scale dataset, which is a time-consuming and costly process. Subsequently, we show how the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Based on the analysis of these prompts, we identify the necessity for a new prompt dataset specificall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;</title><link>https://arxiv.org/abs/2403.06097</link><description>&lt;p&gt;
&#33021;&#21542;&#29992;LLM&#26367;&#20195;&#20154;&#24037;&#26631;&#27880;&#65311; &#26080;&#20154;&#26426;&#20132;&#20184;&#20219;&#21153;&#19979;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22320;&#22336;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CNER-UAV&#65292;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#35774;&#35745;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20116;&#20010;&#31867;&#21035;&#65292;&#21487;&#20197;&#20840;&#38754;&#35757;&#32451;&#21644;&#35780;&#20272;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#20026;&#26500;&#24314;&#36825;&#19968;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#33719;&#21462;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#30830;&#20445;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#26368;&#32456;&#30340;&#25968;&#25454;&#38598;&#32422;&#21253;&#21547;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#32463;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312; \url{https://github.com/zhhvvv/CNER-UAV} &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06097v1 Announce Type: cross  Abstract: We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame \textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage \textbf{M}odel annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \url{https://github.com/zhhvvv/CNER-UAV}.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#34701;&#21512;&#26694;&#26550;&#23558;Transformer&#27169;&#22411;&#37327;&#21270;&#20026;&#20165;&#20004;&#20301;&#65292;&#20165;&#26377;&#36731;&#24494;&#31934;&#24230;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.06082</link><description>&lt;p&gt;
FrameQuant: Transformer&#30340;&#28789;&#27963;&#20302;&#27604;&#29305;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FrameQuant: Flexible Low-Bit Quantization for Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06082
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#34701;&#21512;&#26694;&#26550;&#23558;Transformer&#27169;&#22411;&#37327;&#21270;&#20026;&#20165;&#20004;&#20301;&#65292;&#20165;&#26377;&#36731;&#24494;&#31934;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#35768;&#22810;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#24378;&#22823;&#22522;&#30784;&#27169;&#22411;&#30340;&#25903;&#26609;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;/&#23384;&#20648;&#31354;&#38388;&#21344;&#29992;&#36739;&#22823;&#65292;&#22240;&#27492;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#24448;&#24448;&#38656;&#35201;&#26114;&#36149;&#30340;&#39640;&#31471;&#30828;&#20214;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#22256;&#38590;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#35797;&#22270;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#20854;&#37327;&#21270;&#20026;&#20843;&#20301;&#25110;&#26356;&#20302;&#30340;&#20301;&#25968;&#65292;&#26174;&#30528;&#25552;&#39640;&#35745;&#31639;/&#20869;&#23384;/&#24310;&#36831;&#25928;&#29575;&#12290;&#26082;&#21487;&#20197;&#25104;&#21151;&#23558;&#36825;&#20123;&#27169;&#22411;&#37327;&#21270;&#20026;&#22235;&#20301;&#65292;&#20294;&#24615;&#33021;&#26377;&#25152;&#25439;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#26696;&#65292;&#23558;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#37327;&#21270;&#20026;&#20165;&#20004;&#20301;&#65288;&#21152;&#19968;&#20123;&#39069;&#22806;&#24320;&#38144;&#65289;&#65292;&#20165;&#20250;&#26377;&#36731;&#24494;&#30340;&#31934;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#21046;&#23450;&#20851;&#38190;&#22312;&#20110;&#20174;&#35856;&#27874;&#20998;&#26512;&#20013;&#20511;&#37492;&#20102;&#19968;&#31181;&#31216;&#20026;&#34701;&#21512;&#26694;&#26550;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#37327;&#21270;&#19981;&#24212;&#35813;&#22312;&#21407;&#22987;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#65292;&#32780;&#26159;&#24212;&#35813;&#22312;&#34701;&#21512;&#26694;&#26550;&#34920;&#31034;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06082v1 Announce Type: cross  Abstract: Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06064</link><description>&lt;p&gt;
L$^2$GC: &#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#29992;&#20110;&#23545;&#22270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32447;&#24615;GCN&#27169;&#22411;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#65292;&#36825;&#24182;&#27809;&#26377;&#26126;&#30830;&#25429;&#25417;&#21040;&#20316;&#20026;&#22270;&#27169;&#22411;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#31867;&#20284;&#26641;&#29366;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#23581;&#35797;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;GCN&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22270;&#33410;&#28857;&#30340;&#23398;&#20064;&#29305;&#24449;&#26144;&#23556;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27931;&#20262;&#20857;&#32447;&#24615;&#29305;&#24449;&#21464;&#25442;&#65292;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#28508;&#22312;&#26641;&#29366;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#24341;&#25991;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Citeseer&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;74.7%&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;PubMed&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;81.3%&#30340;&#20934;&#30830;&#24230;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#33267;&#23569;&#36798;&#21040;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06064v1 Announce Type: cross  Abstract: Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and 81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#30340;&#30446;&#26631;&#21463;&#38480;&#21452;&#21521;&#35268;&#21010;&#65288;TRIP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#21644;&#21518;&#21521;&#35268;&#21010;&#29983;&#25104;&#23545;&#35805;&#36335;&#24452;&#65292;&#20419;&#36827;&#23545;&#35805;&#21521;&#39044;&#23450;&#30446;&#26631;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.06063</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#20027;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#30446;&#26631;&#21463;&#38480;&#21452;&#21521;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Target-constrained Bidirectional Planning for Generation of Target-oriented Proactive Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#30340;&#30446;&#26631;&#21463;&#38480;&#21452;&#21521;&#35268;&#21010;&#65288;TRIP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#21644;&#21518;&#21521;&#35268;&#21010;&#29983;&#25104;&#23545;&#35805;&#36335;&#24452;&#65292;&#20419;&#36827;&#23545;&#35805;&#21521;&#39044;&#23450;&#30446;&#26631;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#30340;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#24341;&#23548;&#23545;&#35805;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#21521;&#39044;&#23450;&#30446;&#26631;&#21457;&#23637;&#65292;&#27604;&#22914;&#22312;&#25351;&#23450;&#39033;&#30446;&#19978;&#36827;&#34892;&#25512;&#33616;&#25110;&#20171;&#32461;&#26032;&#30340;&#29305;&#23450;&#20027;&#39064;&#12290;&#20026;&#27492;&#65292;&#23545;&#35805;&#31995;&#32479;&#20851;&#38190;&#22312;&#20110;&#35745;&#21010;&#21512;&#29702;&#30340;&#34892;&#21160;&#20197;&#20027;&#21160;&#25512;&#21160;&#23545;&#35805;&#65292;&#24182;&#21516;&#26102;&#35745;&#21010;&#36866;&#24403;&#30340;&#20027;&#39064;&#20197;&#39034;&#21033;&#25512;&#36827;&#23545;&#35805;&#21040;&#30446;&#26631;&#35805;&#39064;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#20110;&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#30340;&#26377;&#25928;&#23545;&#35805;&#35268;&#21010;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#20013;&#20915;&#31574;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#21463;&#38480;&#21452;&#21521;&#35268;&#21010;&#65288;TRIP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#21644;&#21518;&#21521;&#35268;&#21010;&#19968;&#20010;&#36866;&#24403;&#30340;&#23545;&#35805;&#36335;&#24452;&#12290;&#36890;&#36807;&#23558;&#35268;&#21010;&#24418;&#24335;&#21270;&#20026;&#19968;&#39033;&#29983;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;TRIP&#21452;&#21521;&#29983;&#25104;&#20102;&#19968;&#20010;&#30001;&#19968;&#31995;&#21015;pair&#32452;&#25104;&#30340;&#23545;&#35805;&#36335;&#24452;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;Transformer&#35299;&#30721;&#22120;&#12290;&#23427;&#20204;&#34987;&#39044;&#26399;&#29992;&#20110;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06063v1 Announce Type: cross  Abstract: Target-oriented proactive dialogue systems aim to lead conversations from a dialogue context toward a pre-determined target, such as making recommendations on designated items or introducing new specific topics. To this end, it is critical for such dialogue systems to plan reasonable actions to drive the conversation proactively, and meanwhile, to plan appropriate topics to move the conversation forward to the target topic smoothly. In this work, we mainly focus on effective dialogue planning for target-oriented dialogue generation. Inspired by decision-making theories in cognitive science, we propose a novel target-constrained bidirectional planning (TRIP) approach, which plans an appropriate dialogue path by looking ahead and looking back. By formulating the planning as a generation task, our TRIP bidirectionally generates a dialogue path consisting of a sequence of  pairs using two Transformer decoders. They are expected to supervis
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#38463;&#25289;&#20271;&#35821;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#38598;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21333;&#35821;&#27169;&#22411;&#34920;&#29616;&#20248;&#36234;&#65292;&#38598;&#25104;&#27169;&#22411;&#32988;&#36807;&#22522;&#32447;&#65292;&#32780;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#20248;&#20110;&#33521;&#35821;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2403.06060</link><description>&lt;p&gt;
&#38598;&#25104;&#24335;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Ensemble Language Models for Multilingual Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#38463;&#25289;&#20271;&#35821;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#38598;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21333;&#35821;&#27169;&#22411;&#34920;&#29616;&#20248;&#36234;&#65292;&#38598;&#25104;&#27169;&#22411;&#32988;&#36807;&#22522;&#32447;&#65292;&#32780;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#20248;&#20110;&#33521;&#35821;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#29992;&#25143;&#30340;&#35266;&#28857;&#12290;&#26368;&#36817;&#65292;&#24773;&#24863;&#20998;&#26512;&#22312;&#29702;&#35299;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#20998;&#20139;&#20869;&#23481;&#30340;&#20154;&#31867;&#24773;&#24863;&#26041;&#38754;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#23613;&#31649;&#24120;&#29992;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#24050;&#26377;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20687;&#38463;&#25289;&#20271;&#35821;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#20173;&#28982;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26469;&#33258;SemEval-17&#21644;&#38463;&#25289;&#20271;&#24773;&#24863;&#25512;&#25991;&#25968;&#25454;&#38598;&#30340;&#25512;&#25991;&#25991;&#26412;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22235;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#38598;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#21253;&#25324;&#21333;&#35821;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#38598;&#25104;&#27169;&#22411;&#32988;&#36807;&#22522;&#32447;&#65292;&#32780;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#32988;&#36807;&#33521;&#35821;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06060v1 Announce Type: new  Abstract: The rapid advancement of social media enables us to analyze user opinions. In recent times, sentiment analysis has shown a prominent research gap in understanding human sentiment based on the content shared on social media. Although sentiment analysis for commonly spoken languages has advanced significantly, low-resource languages like Arabic continue to get little research due to resource limitations. In this study, we explore sentiment analysis on tweet texts from SemEval-17 and the Arabic Sentiment Tweet dataset. Moreover, We investigated four pretrained language models and proposed two ensemble language models. Our findings include monolingual models exhibiting superior performance and ensemble models outperforming the baseline while the majority voting ensemble outperforms the English language.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20379;PSC&#24037;&#20855;&#23558;&#27874;&#26031;&#35821;&#20442;&#35821;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#24773;&#24863;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.06023</link><description>&lt;p&gt;
&#27874;&#26031;&#35821;&#20442;&#35821;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#20197;&#21450;&#31038;&#20132;&#23186;&#20307;&#19978;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Persian Slang Text Conversion to Formal and Deep Learning of Persian Short Texts on Social Media for Sentiment Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06023
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;PSC&#24037;&#20855;&#23558;&#27874;&#26031;&#35821;&#20442;&#35821;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#36866;&#21512;&#20998;&#26512;&#27874;&#26031;&#35821;&#20250;&#35805;&#25991;&#26412;&#30340;&#24037;&#20855;&#20351;&#24471;&#23545;&#36825;&#20123;&#25991;&#26412;&#65288;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#65289;&#30340;&#21508;&#31181;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#25552;&#20379;PSC&#65288;&#27874;&#26031;&#35821;&#20442;&#35821;&#36716;&#25442;&#22120;&#65289;&#65292;&#23558;&#20250;&#35805;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#65292;&#24182;&#32467;&#21512;&#26368;&#26032;&#21644;&#26368;&#20339;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#26356;&#23481;&#26131;&#29702;&#35299;&#36825;&#20123;&#25991;&#26412;&#65292;&#26356;&#22909;&#22320;&#36827;&#34892;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06023v1 Announce Type: new  Abstract: The lack of a suitable tool for the analysis of conversational texts in the Persian language has made various analyses of these texts, including Sentiment Analysis, difficult. In this research, we tried to make the understanding of these texts easier for the machine by providing PSC, Persian Slang Converter, a tool for converting conversational texts into formal ones, and by using the most up-to-date and best deep learning methods along with the PSC, the sentiment learning of short Persian language texts for the machine in a better way. be made More than 10 million unlabeled texts from various social networks and movie subtitles (as Conversational texts) and about 10 million news texts (as formal texts) have been used for training unsupervised models and formal implementation of the tool. 60,000 texts from the comments of Instagram social network users with positive, negative, and neutral labels are considered supervised data for trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22914;&#20309;&#23558;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;PLM LLaMa &#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25552;&#31034;&#65292;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#36866;&#24212;&#25552;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06018</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#29992;&#20110;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22914;&#20309;&#23558;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;PLM LLaMa &#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25552;&#31034;&#65292;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#36866;&#24212;&#25552;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22788;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#23637;&#30340;&#21069;&#27839;&#12290;PLMs&#30340;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#26159;&#8220;&#25552;&#31034;&#8221; - &#25110;&#19978;&#19979;&#25991;&#23398;&#20064; - &#29992;&#25143;&#22312;&#25552;&#31034;PLM&#23545;&#26032;&#31034;&#20363;&#25191;&#34892;&#20219;&#21153;&#20043;&#21069;&#21521;PLM&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#21644;&#19968;&#20123;&#23436;&#25104;&#30340;&#31034;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#30446;&#21069;&#21482;&#26377;&#26368;&#22823;&#12289;&#26368;&#26377;&#33021;&#21147;&#30340;PLMs&#25165;&#33021;&#26377;&#25928;&#22320;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26159;&#36890;&#36807;&#20027;&#35201;&#20197;&#33521;&#35821;&#20026;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#65292;&#20854;&#20182;&#25152;&#26377;&#35821;&#35328;&#37117;&#33853;&#21518;&#12290;&#22823;&#22810;&#25968;&#35821;&#35328;&#30340;&#25968;&#25454;&#38480;&#21046;&#38459;&#30861;&#20102;&#35757;&#32451;&#20855;&#26377;&#25552;&#31034;&#33021;&#21147;&#30340;&#35821;&#35328;&#29305;&#23450;PLMs&#12290;&#23613;&#31649;&#22312;&#25552;&#31034;&#35774;&#32622;&#26041;&#38754;&#30340;&#24037;&#20316;&#28608;&#22686;&#65292;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#22914;&#20309;&#23558;PLMs&#19987;&#38376;&#29992;&#20110;&#36328;&#35821;&#35328;&#36866;&#24212;&#25552;&#31034;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36866;&#24212;LLaMa&#30340;&#21487;&#33021;&#26041;&#27861;&#65292;LLaMa&#26159;&#19968;&#20010;&#20027;&#35201;&#22312;&#33521;&#35821;&#20013;&#35757;&#32451;&#30340;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;PLM&#65292;&#29992;&#20110;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36827;&#34892;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06018v1 Announce Type: cross  Abstract: Large pre-trained language models (PLMs) are at the forefront of advances in Natural Language Processing. One widespread use case of PLMs is "prompting" - or in-context learning - where a user provides a description of a task and some completed examples of the task to a PLM as context before prompting the PLM to perform the task on a new example. Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind. The data limitations in most languages preclude the training of language-specific PLMs capable of prompting. Albeit the surge in work of prompting settings, it is still unclear how PLMs should be adapted cross-lingually specifically for prompting. We evaluate the possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, nam
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#38024;&#23545;&#38142;&#25509;&#24320;&#25918;&#25968;&#25454;&#26597;&#35810;&#26085;&#24535;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#36136;&#37327;&#38382;&#39064;&#65292;&#20026;&#20174;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.06016</link><description>&lt;p&gt;
&#38024;&#23545;&#38142;&#25509;&#24320;&#25918;&#25968;&#25454;&#26597;&#35810;&#26085;&#24535;&#20998;&#26512;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
End-to-end solution for linked open data query logs analytics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06016
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#38024;&#23545;&#38142;&#25509;&#24320;&#25918;&#25968;&#25454;&#26597;&#35810;&#26085;&#24535;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#36136;&#37327;&#38382;&#39064;&#65292;&#20026;&#20174;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21033;&#29992;&#26597;&#35810;&#26085;&#24535;&#20013;&#33719;&#21462;&#30340;&#23545;&#29992;&#25143;&#20852;&#36259;&#21644;&#20559;&#22909;&#30340;&#28145;&#21051;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#25903;&#26609;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#28145;&#20837;&#20102;&#35299;&#29992;&#25143;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#23545;&#20915;&#31574;&#20135;&#29983;&#24378;&#28872;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#38142;&#25509;&#24320;&#25918;&#25968;&#25454;&#65288;LOD&#65289;&#26597;&#35810;&#26085;&#24535;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;LOD&#26085;&#24535;&#30001;&#20110;&#23545;LOD&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#21033;&#29992;&#32780;&#32463;&#21382;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#20123;&#26085;&#24535;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26085;&#24535;&#36973;&#21463;&#19982;&#23427;&#20204;&#30340;&#36136;&#37327;&#21644;&#26469;&#28304;&#30456;&#20851;&#30340;&#35768;&#22810;&#39118;&#38505;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#28165;&#26224;&#23450;&#20041;&#20102;LOD&#26597;&#35810;&#26085;&#24535;&#30340;&#29983;&#24577;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#21033;&#29992;&#36825;&#20123;&#26085;&#24535;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#30495;&#23454;&#30340;LOD&#26085;&#24535;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06016v1 Announce Type: cross  Abstract: Important advances in pillar domains are derived from exploiting query-logs which represents users interest and preferences. Deep understanding of users provides useful knowledge which can influence strongly decision-making. In this work, we want to extract valuable information from Linked Open Data (LOD) query-logs. LOD logs have experienced significant growth due to the large exploitation of LOD datasets. However, exploiting these logs is a difficult task because of their complex structure. Moreover, these logs suffer from many risks related to their Quality and Provenance, impacting their trust. To tackle these issues, we start by clearly defining the ecosystem of LOD query-logs. Then, we provide an end-to-end solution to exploit these logs. At the end, real LOD logs are used and a set of experiments are conducted to validate the proposed solution.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22686;&#24378;&#22411;&#23383;&#20856;&#33014;&#22218;&#30340;&#33258;&#21160;&#35821;&#35328;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;&#32763;&#35793;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#36866;&#29992;&#20110;&#22810;&#35821;&#35328;&#20132;&#27969;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.05982</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#24378;&#22411;&#23383;&#20856;&#33014;&#22218;&#30340;&#33258;&#21160;&#35821;&#35328;&#39044;&#27979; -- &#19968;&#31181;&#26032;&#39062;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhanced Auto Language Prediction with Dictionary Capsule -- A Novel Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22686;&#24378;&#22411;&#23383;&#20856;&#33014;&#22218;&#30340;&#33258;&#21160;&#35821;&#35328;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;&#32763;&#35793;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#36866;&#29992;&#20110;&#22810;&#35821;&#35328;&#20132;&#27969;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#35821;&#35328;&#39044;&#27979;&#23383;&#20856;&#33014;&#22218;&#65288;ALPDC&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35821;&#35328;&#39044;&#27979;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#32452;&#21512;&#26469;&#39044;&#27979;&#32473;&#23450;&#36755;&#20837;&#25991;&#26412;&#30340;&#35821;&#35328;&#65292;&#28982;&#21518;&#21033;&#29992;&#39044;&#20808;&#26500;&#24314;&#30340;&#23383;&#20856;&#23558;&#20854;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#12290;&#35813;&#30740;&#31350;&#36824;&#26088;&#22312;&#23558;&#21508;&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#32763;&#35793;&#25104;&#33521;&#25991;&#30340;&#23383;&#38754;&#24847;&#24605;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#20132;&#27969;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05982v1 Announce Type: new  Abstract: The paper presents a novel Auto Language Prediction Dictionary Capsule (ALPDC) framework for language prediction and machine translation. The model uses a combination of neural networks and symbolic representations to predict the language of a given input text and then translate it to a target language using pre-built dictionaries. This research work also aims to translate the text of various languages to its literal meaning in English. The proposed model achieves state-of-the-art results on several benchmark datasets and significantly improves translation accuracy compared to existing methods. The results show the potential of the proposed method for practical use in multilingual communication and natural language processing tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;TExFAIR&#65292;&#22522;&#20110;&#26415;&#35821;&#34920;&#31034;&#32676;&#20307;&#22312;&#25490;&#21517;&#21015;&#34920;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#25193;&#23637;&#26469;&#35299;&#20915;&#29616;&#26377;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05975</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26415;&#35821;&#34920;&#31034;&#30340;&#26041;&#27861;&#35780;&#20272;&#25490;&#21517;&#21015;&#34920;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Measuring Bias in a Ranked List using Term-based Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;TExFAIR&#65292;&#22522;&#20110;&#26415;&#35821;&#34920;&#31034;&#32676;&#20307;&#22312;&#25490;&#21517;&#21015;&#34920;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#25193;&#23637;&#26469;&#35299;&#20915;&#29616;&#26377;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#24615;&#21035;&#20559;&#35265;&#22312;&#25991;&#26723;&#25490;&#21517;&#20013;&#20351;&#29992;NFaiRR&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#35813;&#25351;&#26631;&#26681;&#25454;&#27599;&#20010;&#25490;&#21517;&#25991;&#26723;&#30340;&#26080;&#20559;&#20998;&#25968;&#30340;&#32858;&#21512;&#26469;&#34913;&#37327;&#25490;&#21517;&#21015;&#34920;&#20013;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#34913;&#37327;&#25490;&#21517;&#21015;&#34920;&#20559;&#35265;&#30340;&#35266;&#28857;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#23616;&#38480;&#24615;&#65306;&#25490;&#21517;&#21015;&#34920;&#20013;&#30340;&#20010;&#21035;&#25991;&#26723;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#65292;&#32780;&#25972;&#20010;&#25490;&#21517;&#21015;&#34920;&#21364;&#33021;&#24179;&#34913;&#21508;&#20010;&#32676;&#20307;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TExFAIR&#65288;&#22522;&#20110;&#26415;&#35821;&#26333;&#20809;&#30340;&#20844;&#24179;&#24615;&#65289;&#30340;&#26032;&#25351;&#26631;&#65292;&#23427;&#22522;&#20110;&#36890;&#29992;&#20844;&#24179;&#24615;&#35780;&#20272;&#26694;&#26550;&#8212;&#8212;&#27880;&#24847;&#21147;&#21152;&#26435;&#25490;&#21517;&#20844;&#24179;&#24615;&#65288;AWRF&#65289;&#30340;&#20004;&#20010;&#26032;&#25193;&#23637;&#12290;TExFAIR&#26681;&#25454;&#25490;&#21517;&#21015;&#34920;&#20013;&#32676;&#20307;&#30340;&#22522;&#20110;&#26415;&#35821;&#30340;&#34920;&#31034;&#26469;&#35780;&#20272;&#20844;&#24179;&#24615;&#65306;&#65288;i&#65289;&#36890;&#36807;&#22522;&#20110;&#27010;&#29575;&#26415;&#35821;&#32423;&#20851;&#32852;&#26469;&#26126;&#30830;&#22320;&#23558;&#25991;&#26723;&#19982;&#32676;&#20307;&#20851;&#32852;&#36215;&#26469;&#30340;&#23450;&#20041;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#29992;&#20110;&#35745;&#31639;&#23545;&#34913;&#37327;&#20135;&#29983;&#20559;&#24046;&#30340;&#22240;&#32032;&#30340;&#25490;&#21517;&#20559;&#24046;&#25240;&#25187;&#22240;&#23376;&#65288;RBDF&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05975v1 Announce Type: new  Abstract: In most recent studies, gender bias in document ranking is evaluated with the NFaiRR metric, which measures bias in a ranked list based on an aggregation over the unbiasedness scores of each ranked document. This perspective in measuring the bias of a ranked list has a key limitation: individual documents of a ranked list might be biased while the ranked list as a whole balances the groups' representations. To address this issue, we propose a novel metric called TExFAIR (term exposure-based fairness), which is based on two new extensions to a generic fairness evaluation framework, attention-weighted ranking fairness (AWRF). TExFAIR assesses fairness based on the term-based representation of groups in a ranked list: (i) an explicit definition of associating documents to groups based on probabilistic term-level associations, and (ii) a rank-biased discounting factor (RBDF) for counting non-representative documents towards the measurement o
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;APRICOT&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35774;&#32622;&#32622;&#20449;&#30446;&#26631;&#24182;&#35757;&#32451;&#39069;&#22806;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.05973</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#29983;&#25104;&#26469;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrating Large Language Models Using Their Generations Only
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05973
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;APRICOT&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35774;&#32622;&#32622;&#20449;&#30446;&#26631;&#24182;&#35757;&#32451;&#39069;&#22806;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#38754;&#21521;&#29992;&#25143;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#36890;&#36807;&#20934;&#30830;&#37327;&#21270;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#20449;&#24515;&#26469;&#24314;&#31435;&#20449;&#20219;&#24182;&#20445;&#25345;&#23433;&#20840;&#24615;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26657;&#20934;LLMs - &#23588;&#20854;&#26159;&#24403;&#19982;&#27169;&#22411;&#30340;&#21807;&#19968;&#25509;&#21475;&#26159;&#23427;&#20204;&#29983;&#25104;&#30340;&#25991;&#26412;&#26102; - &#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;APRICOT&#65288;&#36741;&#21161;&#39044;&#27979;&#32622;&#20449;&#30446;&#26631;&#65289;&#65306;&#19968;&#31181;&#36890;&#36807;&#20165;&#20351;&#29992;&#20854;&#25991;&#26412;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35774;&#32622;&#32622;&#20449;&#30446;&#26631;&#24182;&#35757;&#32451;&#19968;&#20010;&#39069;&#22806;&#27169;&#22411;&#26469;&#39044;&#27979;LLM&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#27010;&#24565;&#19978;&#31616;&#21333;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#36229;&#20986;&#20854;&#36755;&#20986;&#65292;&#19981;&#24178;&#25200;&#35821;&#35328;&#29983;&#25104;&#65292;&#24182;&#19988;&#26377;&#22810;&#31181;&#28508;&#22312;&#29992;&#36884;&#65292;&#20363;&#22914;&#36890;&#36807;&#35328;&#35821;&#21270;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#25110;&#26681;&#25454;&#32622;&#20449;&#24230;&#35843;&#25972;&#32473;&#23450;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05973v1 Announce Type: cross  Abstract: As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge. We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#35805;&#20013;&#30340;&#32447;&#31243;&#24182;&#22522;&#20110;&#20854;&#37325;&#35201;&#24615;&#20248;&#20808;&#29983;&#25104;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.05931</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#25552;&#31034;&#30340;Transformer&#36827;&#34892;&#32447;&#31243;&#26816;&#27979;&#21644;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Thread Detection and Response Generation using Transformers with Prompt Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05931
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#35805;&#20013;&#30340;&#32447;&#31243;&#24182;&#22522;&#20110;&#20854;&#37325;&#35201;&#24615;&#20248;&#20808;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Conversational systems &#23545;&#20110;&#20154;&#26426;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#36807;&#35782;&#21035;&#32447;&#31243;&#24182;&#20248;&#20808;&#21709;&#24212;&#26469;&#31649;&#29702;&#22797;&#26434;&#23545;&#35805;&#12290; &#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#20013;&#32447;&#31243;&#30340;&#20934;&#30830;&#35782;&#21035;&#21644;&#31574;&#30053;&#24615;&#21709;&#24212;&#20248;&#20808;&#32423;&#30830;&#20445;&#39640;&#25928;&#23545;&#35805;&#31649;&#29702;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26681;&#25454;&#37325;&#35201;&#24615;&#35782;&#21035;&#32447;&#31243;&#24182;&#20248;&#20808;&#29983;&#25104;&#20854;&#21709;&#24212;&#65292;&#28041;&#21450;&#23558;&#38382;&#39064;&#31995;&#32479;&#22320;&#20998;&#35299;&#20026;&#31163;&#25955;&#32452;&#20214; - &#32447;&#31243;&#26816;&#27979;&#12289;&#20248;&#20808;&#32423;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#31934;&#24515;&#20998;&#26512;&#21644;&#20248;&#21270;&#12290; &#36825;&#20123;&#31934;&#32454;&#30340;&#32452;&#20214;&#26080;&#32541;&#38598;&#25104;&#21040;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#12290; &#30001;&#20110;&#20854;&#39640;&#36890;&#29992;&#24615;&#65292;Llama2 7b&#34987;&#29992;&#20110;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM) &#36827;&#34892;&#26356;&#26032;&#12290; Llama2 &#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#24471;&#21040;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05931v1 Announce Type: new  Abstract: Conversational systems are crucial for human-computer interaction, managing complex dialogues by identifying threads and prioritising responses. This is especially vital in multi-party conversations, where precise identification of threads and strategic response prioritisation ensure efficient dialogue management. To address these challenges an end-to-end model that identifies threads and prioritises their response generation based on the importance was developed, involving a systematic decomposition of the problem into discrete components - thread detection, prioritisation, and performance optimisation which was meticulously analysed and optimised. These refined components seamlessly integrate into a unified framework, in conversational systems. Llama2 7b is used due to its high level of generalisation but the system can be updated with any open source Large Language Model(LLM). The computational capabilities of the Llama2 model was aug
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#28151;&#21512;NLP&#27169;&#22411;&#30340;&#32467;&#21512;&#21487;&#22312;&#39640;&#20934;&#30830;&#29575;&#19979;&#23545;&#21307;&#24072;&#31508;&#35760;&#36827;&#34892;&#39640;&#21534;&#21520;&#37327;&#34920;&#22411;&#35782;&#21035;&#65292;&#26377;&#26395;&#25104;&#20026;&#26410;&#26469;&#39318;&#36873;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05920</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#28151;&#21512;NLP&#27169;&#22411;&#30340;&#21307;&#24072;&#31508;&#35760;&#39640;&#21534;&#21520;&#37327;&#34920;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05920
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#28151;&#21512;NLP&#27169;&#22411;&#30340;&#32467;&#21512;&#21487;&#22312;&#39640;&#20934;&#30830;&#29575;&#19979;&#23545;&#21307;&#24072;&#31508;&#35760;&#36827;&#34892;&#39640;&#21534;&#21520;&#37327;&#34920;&#22411;&#35782;&#21035;&#65292;&#26377;&#26395;&#25104;&#20026;&#26410;&#26469;&#39318;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#22411;&#26159;&#21033;&#29992;&#26412;&#20307;&#35770;&#20013;&#30340;&#27010;&#24565;&#23545;&#24739;&#32773;&#20307;&#24449;&#21644;&#30151;&#29366;&#36827;&#34892;&#35814;&#32454;&#25551;&#36848;&#12290;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#22823;&#37327;&#21307;&#24072;&#31508;&#35760;&#30340;&#28145;&#24230;&#34920;&#22411;&#38656;&#35201;&#39640;&#21534;&#21520;&#37327;&#30340;&#26041;&#27861;&#12290;&#22312;&#36807;&#21435;&#30340;&#19977;&#21313;&#24180;&#37324;&#65292;&#21462;&#24471;&#20102;&#20351;&#39640;&#21534;&#21520;&#37327;&#34920;&#22411;&#25104;&#20026;&#21487;&#33021;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19968;&#20010;&#28151;&#21512;NLP&#27169;&#22411;&#65288;&#32467;&#21512;&#20102;&#35789;&#21521;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65289;&#21487;&#20197;&#22312;&#39640;&#20934;&#30830;&#29575;&#19979;&#23545;&#21307;&#24072;&#31508;&#35760;&#36827;&#34892;&#39640;&#21534;&#21520;&#37327;&#34920;&#22411;&#35782;&#21035;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24456;&#21487;&#33021;&#20250;&#25104;&#20026;&#21307;&#24072;&#31508;&#35760;&#39640;&#21534;&#21520;&#37327;&#28145;&#24230;&#34920;&#22411;&#35782;&#21035;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05920v1 Announce Type: cross  Abstract: Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology. The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods. Over the past thirty years, progress toward making high throughput phenotyping feasible. In this study, we demonstrate that a large language model and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy. Large language models will likely emerge as the preferred method for high throughput deep phenotyping of physician notes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;MaiBaam&#35821;&#26009;&#24211;&#30340;&#27880;&#37322;&#20934;&#21017;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#22788;&#29702;&#21644;&#26631;&#35760;&#24052;&#20240;&#21033;&#20122;&#25968;&#25454;&#65292;&#35828;&#26126;&#20102;&#35789;&#24615;&#26631;&#35760;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#23545;&#24503;&#35821;&#31561;&#30456;&#20851;&#35821;&#35328;&#36866;&#29992;&#30340;&#27880;&#37322;&#20915;&#31574;&#21644;&#23545;&#24052;&#20240;&#21033;&#20122;&#35821;&#27861;&#29305;&#23450;&#20915;&#31574;&#30340;&#20171;&#32461;&#21644;&#25512;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.05902</link><description>&lt;p&gt;
MaiBaam&#27880;&#37322;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
MaiBaam Annotation Guidelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05902
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;MaiBaam&#35821;&#26009;&#24211;&#30340;&#27880;&#37322;&#20934;&#21017;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#22788;&#29702;&#21644;&#26631;&#35760;&#24052;&#20240;&#21033;&#20122;&#25968;&#25454;&#65292;&#35828;&#26126;&#20102;&#35789;&#24615;&#26631;&#35760;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#23545;&#24503;&#35821;&#31561;&#30456;&#20851;&#35821;&#35328;&#36866;&#29992;&#30340;&#27880;&#37322;&#20915;&#31574;&#21644;&#23545;&#24052;&#20240;&#21033;&#20122;&#35821;&#27861;&#29305;&#23450;&#20915;&#31574;&#30340;&#20171;&#32461;&#21644;&#25512;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;MaiBaam&#30340;&#27880;&#37322;&#20934;&#21017;&#65292;&#36825;&#26159;&#19968;&#20010;&#27880;&#37322;&#20102;&#35789;&#24615;&#26631;&#35760;&#21644;&#21477;&#27861;&#20381;&#36182;&#20851;&#31995;&#30340;&#24052;&#20240;&#21033;&#20122;&#35821;&#35821;&#26009;&#24211;&#12290;MaiBaam&#23646;&#20110;&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#39033;&#30446;&#65288;UD&#65289;&#65292;&#25105;&#20204;&#30340;&#27880;&#37322;&#35814;&#32454;&#35828;&#26126;&#20102;&#19968;&#33324;&#21644;&#24503;&#22269;UD&#31532;2&#29256;&#25351;&#21335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#39044;&#22788;&#29702;&#21644;&#26631;&#35760;&#24052;&#20240;&#21033;&#20122;&#25968;&#25454;&#65292;&#27010;&#36848;&#20102;&#25105;&#20204;&#20351;&#29992;&#30340;&#35789;&#24615;&#26631;&#35760;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#37322;&#20102;&#20063;&#36866;&#29992;&#20110;&#24503;&#35821;&#31561;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#27880;&#37322;&#20915;&#31574;&#65292;&#26368;&#21518;&#20171;&#32461;&#24182;&#25512;&#21160;&#20102;&#36866;&#29992;&#20110;&#24052;&#20240;&#21033;&#20122;&#35821;&#27861;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05902v1 Announce Type: new  Abstract: This document provides the annotation guidelines for MaiBaam, a Bavarian corpus annotated with part-of-speech (POS) tags and syntactic dependencies. MaiBaam belongs to the Universal Dependencies (UD) project, and our annotations elaborate on the general and German UD version 2 guidelines. In this document, we detail how to preprocess and tokenize Bavarian data, provide an overview of the POS tags and dependencies we use, explain annotation decisions that would also apply to closely related languages like German, and lastly we introduce and motivate decisions that are specific to Bavarian grammar.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;KG-Rank&#26694;&#26550;&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#25490;&#21517;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05881</link><description>&lt;p&gt;
KG-Rank: &#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#25490;&#21517;&#25216;&#26415;&#22686;&#24378;&#21307;&#23398;&#38382;&#31572;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;KG-Rank&#26694;&#26550;&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#25490;&#21517;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#36827;&#20102;&#21307;&#30103;&#20445;&#20581;&#21019;&#26032;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#20559;&#31163;&#21307;&#30103;&#20107;&#23454;&#21644;&#22266;&#26377;&#20559;&#35265;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#20020;&#24202;&#35774;&#32622;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22686;&#24378;&#22411;LLM&#26694;&#26550;KG-Rank&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19982;&#25490;&#21517;&#21644;&#37325;&#26032;&#25490;&#21517;&#25216;&#26415;&#65292;&#26088;&#22312;&#25913;&#36827;&#21307;&#23398;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#25910;&#21040;&#38382;&#39064;&#21518;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#21307;&#23398;KG&#20013;&#26816;&#32034;&#19977;&#20803;&#32452;&#20197;&#25910;&#38598;&#20107;&#23454;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#24212;&#29992;&#25490;&#21517;&#26041;&#27861;&#26469;&#31934;&#32454;&#35843;&#25972;&#36825;&#20123;&#19977;&#20803;&#32452;&#30340;&#39034;&#24207;&#65292;&#26088;&#22312;&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;KG-Rank&#26159;&#39318;&#20010;&#23558;&#25490;&#21517;&#27169;&#22411;&#19982;KG&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#38271;&#31572;&#26696;&#30340;&#21307;&#23398;&#38382;&#31572;&#24212;&#29992;&#12290;&#23545;&#22235;&#20010;&#36873;&#23450;&#30340;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;KG-Rank&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05881v1 Announce Type: new  Abstract: Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) with ranking and re-ranking techniques, aiming to improve free-text question-answering (QA) in the medical domain. Specifically, upon receiving a question, we initially retrieve triplets from a medical KG to gather factual information. Subsequently, we innovatively apply ranking methods to refine the ordering of these triplets, aiming to yield more precise answers. To the best of our knowledge, KG-Rank is the first application of ranking models combined with KG in medical QA specifically for generating long answers. Evaluation of four selected medical QA datasets shows that KG-Rank achieves a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#34920;&#31034;&#30340;&#22270;&#20687;&#26469;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#22797;&#21512;&#25552;&#31034;&#21644;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#30340;&#19968;&#20123;&#37325;&#35201;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.05846</link><description>&lt;p&gt;
&#25193;&#25955;&#38236;&#22836;&#65306;&#35299;&#37322;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#31649;&#36947;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#34920;&#31034;&#30340;&#22270;&#20687;&#26469;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#22797;&#21512;&#25552;&#31034;&#21644;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#30340;&#19968;&#20123;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05846v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#36328; &#23384;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;T2I&#65289;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#32534;&#30721;&#22120;&#20135;&#29983;&#25991;&#26412;&#34920;&#31034;&#30340;&#36807;&#31243;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#38236;&#22836;&#65292;&#19968;&#31181;&#20998;&#26512; T2I &#27169;&#22411;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20854;&#20013;&#38388;&#34920;&#31034;&#30340;&#22270;&#20687;&#12290;&#20351;&#29992;&#25193;&#25955;&#38236;&#22836;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#26368;&#36817;&#30340; T2I &#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#12290;&#22312;&#25506;&#32034;&#22797;&#21512;&#25552;&#31034;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#25551;&#36848;&#22810;&#20010;&#23545;&#35937;&#30340;&#22797;&#26434;&#22330;&#26223;&#30456;&#23545;&#20110;&#31616;&#21333;&#22330;&#26223;&#26159;&#36880;&#27493;&#19988;&#36739;&#24930;&#22320;&#26500;&#24314;&#30340;&#65307;&#22312;&#25506;&#32034;&#30693;&#35782;&#26816;&#32034;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#31034;&#19981;&#24120;&#35265;&#27010;&#24565;&#38656;&#35201;&#27604;&#24120;&#35265;&#27010;&#24565;&#26356;&#22810;&#30340;&#35745;&#31639;&#65292;&#24182;&#19988;&#30693;&#35782;&#26816;&#32034;&#22312;&#23618;&#20043;&#38388;&#26159;&#28176;&#36827;&#30340;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026; T2I &#31649;&#36947;&#20013;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#32452;&#20214;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05846v1 Announce Type: cross  Abstract: Text-to-image diffusion models (T2I) use a latent representation of a text prompt to guide the image generation process. However, the process by which the encoder produces the text representation is unknown. We propose the Diffusion Lens, a method for analyzing the text encoder of T2I models by generating images from its intermediate representations. Using the Diffusion Lens, we perform an extensive analysis of two recent T2I models. Exploring compound prompts, we find that complex scenes describing multiple objects are composed progressively and more slowly compared to simple scenes; Exploring knowledge retrieval, we find that representation of uncommon concepts requires further computation compared to common concepts, and that knowledge retrieval is gradual across layers. Overall, our findings provide valuable insights into the text encoder component in T2I pipelines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#26415;&#23398;&#20064;&#31574;&#30053;&#65292;&#37325;&#28857;&#32771;&#34385;&#26368;&#20302;&#26377;&#25928;&#25968;&#23383;&#30340;&#36755;&#20986;&#65292;&#37325;&#26032;&#35780;&#20272;&#25968;&#23383;&#39034;&#24207;&#65292;&#24182;&#32467;&#21512;&#36880;&#27493;&#26041;&#27861;&#22823;&#24133;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.05845</link><description>&lt;p&gt;
&#25968;&#23383;&#21453;&#36716;&#65281;&#31639;&#26415;&#23398;&#20064;&#20013;&#39034;&#24207;&#35299;&#30721;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Reverse That Number! Decoding Order Matters in Arithmetic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#26415;&#23398;&#20064;&#31574;&#30053;&#65292;&#37325;&#28857;&#32771;&#34385;&#26368;&#20302;&#26377;&#25928;&#25968;&#23383;&#30340;&#36755;&#20986;&#65292;&#37325;&#26032;&#35780;&#20272;&#25968;&#23383;&#39034;&#24207;&#65292;&#24182;&#32467;&#21512;&#36880;&#27493;&#26041;&#27861;&#22823;&#24133;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#26377;&#25928;&#23398;&#20064;&#31639;&#26415;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25215;&#35748;&#25968;&#23383;&#39034;&#24207;&#22312;&#31639;&#26415;&#35745;&#31639;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39034;&#24207;&#12289;&#36880;&#27493;&#30340;&#26041;&#27861;&#26469;&#25945;&#25480;LLMs&#31639;&#26415;&#65292;&#23548;&#33268;&#32467;&#35770;&#26159;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#28041;&#21450;&#21040;&#31934;&#32454;&#30340;&#36880;&#27493;&#25805;&#20316;&#12290;&#19982;&#20256;&#32479;&#36335;&#24452;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#19981;&#20165;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20174;&#26368;&#20302;&#26377;&#25928;&#25968;&#23383;&#36755;&#20986;&#26469;&#37325;&#26032;&#35780;&#20272;&#25968;&#23383;&#39034;&#24207;&#65292;&#36824;&#32467;&#21512;&#36880;&#27493;&#26041;&#27861;&#22823;&#24133;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#22312;&#20840;&#38754;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#24320;&#21457;&#24182;&#24212;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#24635;&#20307;&#20934;&#30830;&#24230;&#30340;&#25552;&#39640;&#65292;&#21516;&#26102;&#20165;&#38656;&#35201;&#19977;&#20998;&#20043;&#19968;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05845v1 Announce Type: cross  Abstract: Recent advancements in pretraining have demonstrated that modern Large Language Models (LLMs) possess the capability to effectively learn arithmetic operations. However, despite acknowledging the significance of digit order in arithmetic computation, current methodologies predominantly rely on sequential, step-by-step approaches for teaching LLMs arithmetic, resulting in a conclusion where obtaining better performance involves fine-grained step-by-step. Diverging from this conventional path, our work introduces a novel strategy that not only reevaluates the digit order by prioritizing output from the least significant digit but also incorporates a step-by-step methodology to substantially reduce complexity. We have developed and applied this method in a comprehensive set of experiments. Compared to the previous state-of-the-art (SOTA) method, our findings reveal an overall improvement of in accuracy while requiring only a third of the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#38899;&#39057;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#32534;&#30721;&#20010;&#20307;&#29305;&#24449;&#21644;&#36890;&#29992;&#27169;&#24335;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#25968;&#25454;&#65292;&#29992;&#20110;&#35821;&#35328;&#20998;&#26512;&#21644;&#20020;&#24202;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.05820</link><description>&lt;p&gt;
&#19968;&#31181;&#38899;&#39057;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#23558;&#35821;&#38899;&#20449;&#21495;&#36716;&#25442;&#20026;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
An Audio-textual Diffusion Model For Converting Speech Signals Into Ultrasound Tongue Imaging Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05820
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#38899;&#39057;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#32534;&#30721;&#20010;&#20307;&#29305;&#24449;&#21644;&#36890;&#29992;&#27169;&#24335;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#25968;&#25454;&#65292;&#29992;&#20110;&#35821;&#35328;&#20998;&#26512;&#21644;&#20020;&#24202;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#21040;&#35821;&#38899;&#36870;&#21464;&#25442;&#65288;AAI&#65289;&#26088;&#22312;&#23558;&#38899;&#39057;&#36716;&#25442;&#20026;&#21457;&#38899;&#22120;&#23448;&#36816;&#21160;&#65292;&#22914;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#65288;UTI&#65289;&#25968;&#25454;&#12290;&#29616;&#26377;AAI&#26041;&#27861;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#20165;&#20351;&#29992;&#20010;&#24615;&#21270;&#38899;&#39057;&#20449;&#24687;&#26469;&#25512;&#23548;&#33292;&#22836;&#36816;&#21160;&#30340;&#19968;&#33324;&#27169;&#24335;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;UTI&#25968;&#25454;&#36136;&#37327;&#26377;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;UTI&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#30340;&#38899;&#39057;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;wav2vec 2.0&#23545;&#20010;&#20307;&#30456;&#20851;&#30340;&#33292;&#22836;&#36816;&#21160;&#32454;&#33410;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#20351;&#29992;BERT&#23545;&#19982;&#33292;&#22836;&#36816;&#21160;&#26222;&#36941;&#24615;&#30456;&#20851;&#30340;ASR&#36716;&#24405;&#36827;&#34892;&#32534;&#30721;&#12290;&#28982;&#21518;&#36890;&#36807;&#25193;&#25955;&#27169;&#22359;&#29983;&#25104;UTI&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#28165;&#26224;&#33292;&#22836;&#36718;&#24275;&#30340;&#39640;&#36136;&#37327;UTI&#25968;&#25454;&#65292;&#36825;&#23545;&#35821;&#35328;&#20998;&#26512;&#21644;&#20020;&#24202;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05820v1 Announce Type: cross  Abstract: Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator movements, such as ultrasound tongue imaging (UTI) data. An issue of existing AAI methods is only using the personalized acoustic information to derive the general patterns of tongue motions, and thus the quality of generated UTI data is limited. To address this issue, this paper proposes an audio-textual diffusion model for the UTI data generation task. In this model, the inherent acoustic characteristics of individuals related to the tongue motion details are encoded by using wav2vec 2.0, while the ASR transcriptions related to the universality of tongue motions are encoded by using BERT. UTI data are then generated by using a diffusion module. Experimental results showed that the proposed diffusion model could generate high-quality UTI data with clear tongue contour that is crucial for the linguistic analysis and clinical assessment. The project can be fou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#21270;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;MP2D&#65292;&#36890;&#36807;&#26144;&#23556;&#23545;&#35805;&#20013;&#35805;&#39064;&#30340;&#27969;&#21160;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#20154;&#31867;&#23545;&#35805;&#30340;&#21160;&#24577;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#20855;&#26377;&#33258;&#28982;&#35805;&#39064;&#36716;&#25442;&#30340;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05814</link><description>&lt;p&gt;
MP2D:&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#35805;&#39064;&#36716;&#31227;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05814
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#21270;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;MP2D&#65292;&#36890;&#36807;&#26144;&#23556;&#23545;&#35805;&#20013;&#35805;&#39064;&#30340;&#27969;&#21160;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#20154;&#31867;&#23545;&#35805;&#30340;&#21160;&#24577;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#20855;&#26377;&#33258;&#28982;&#35805;&#39064;&#36716;&#25442;&#30340;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38754;&#21521;&#29305;&#23450;&#35805;&#39064;&#30340;&#23545;&#35805;&#31995;&#32479;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#23545;&#35805;&#20013;&#26377;&#25928;&#22320;&#31649;&#29702;&#35805;&#39064;&#36716;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Passage to Dialogue (MP2D)&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21019;&#24314;&#20855;&#26377;&#33258;&#28982;&#35805;&#39064;&#36716;&#25442;&#30340;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;MP2D&#26144;&#23556;&#23545;&#35805;&#20013;&#35805;&#39064;&#30340;&#27969;&#21160;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#30340;&#21160;&#24577;&#12290;&#23427;&#26816;&#32034;&#19982;&#35805;&#39064;&#23545;&#24212;&#30340;&#30456;&#20851;&#27573;&#33853;&#65292;&#24182;&#36890;&#36807;&#27573;&#33853;&#21040;&#23545;&#35805;&#30340;&#26041;&#27861;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#23545;&#35805;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MP2D&#22312;&#29983;&#25104;&#20855;&#26377;&#33258;&#28982;&#35805;&#39064;&#36716;&#25442;&#30340;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35805;&#39064;&#36716;&#31227;&#23545;&#35805;&#22522;&#20934;&#65292;TS-WikiDialog&#12290;&#21033;&#29992;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05814v1 Announce Type: cross  Abstract: Despite advancements in on-topic dialogue systems, effectively managing topic shifts within dialogues remains a persistent challenge, largely attributed to the limited availability of training datasets. To address this issue, we propose Multi-Passage to Dialogue (MP2D), a data generation framework that automatically creates conversational question-answering datasets with natural topic transitions. By leveraging the relationships between entities in a knowledge graph, MP2D maps the flow of topics within a dialogue, effectively mirroring the dynamics of human conversation. It retrieves relevant passages corresponding to the topics and transforms them into dialogues through the passage-to-dialogue method. Through quantitative and qualitative experiments, we demonstrate MP2D's efficacy in generating dialogue with natural topic shifts. Furthermore, this study introduces a novel benchmark for topic shift dialogues, TS-WikiDialog. Utilizing t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#31639;&#27861;&#27599;8&#20010;&#26376;&#20960;&#20046;&#20943;&#21322;&#19968;&#27425;&#25152;&#38656;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#22823;&#22823;&#24555;&#20110;&#25705;&#23572;&#23450;&#24459;&#30340;&#30828;&#20214;&#22686;&#30410;&#65292;&#23613;&#31649;&#31639;&#27861;&#36827;&#23637;&#36895;&#24230;&#24555;&#65292;&#20294;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#23545;&#25972;&#20307;&#24615;&#33021;&#25913;&#21892;&#30340;&#36129;&#29486;&#26356;&#22823;&#12290;</title><link>https://arxiv.org/abs/2403.05812</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#27861;&#36827;&#23637;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithmic progress in language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05812
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#31639;&#27861;&#27599;8&#20010;&#26376;&#20960;&#20046;&#20943;&#21322;&#19968;&#27425;&#25152;&#38656;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#22823;&#22823;&#24555;&#20110;&#25705;&#23572;&#23450;&#24459;&#30340;&#30828;&#20214;&#22686;&#30410;&#65292;&#23613;&#31649;&#31639;&#27861;&#36827;&#23637;&#36895;&#24230;&#24555;&#65292;&#20294;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#23545;&#25972;&#20307;&#24615;&#33021;&#25913;&#21892;&#30340;&#36129;&#29486;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#33258;&#28145;&#24230;&#23398;&#20064;&#38382;&#19990;&#20197;&#26469;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#25913;&#21892;&#36895;&#24230;&#12290;&#20351;&#29992;&#36328;&#36234;2012&#24180;&#33267;2023&#24180;&#30340;Wikitext&#21644;Penn Treebank&#19978;&#30340;200&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#36798;&#21040;&#19968;&#23450;&#24615;&#33021;&#38408;&#20540;&#25152;&#38656;&#30340;&#35745;&#31639;&#26102;&#38388;&#22823;&#32422;&#27599;8&#20010;&#26376;&#20943;&#21322;&#65292;95%&#30340;&#32622;&#20449;&#21306;&#38388;&#32422;&#20026;5&#33267;14&#20010;&#26376;&#65292;&#36828;&#36828;&#24555;&#20110;&#25705;&#23572;&#23450;&#24459;&#30340;&#30828;&#20214;&#22686;&#30410;&#12290;&#25105;&#20204;&#20272;&#35745;&#20102;&#22686;&#24378;&#25193;&#23637;&#35268;&#24459;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#27169;&#22411;&#25193;&#23637;&#19982;&#35757;&#32451;&#31639;&#27861;&#21019;&#26032;&#30340;&#30456;&#23545;&#36129;&#29486;&#12290;&#23613;&#31649;&#31639;&#27861;&#36827;&#23637;&#36895;&#24230;&#24555;&#65292;&#19988;&#20986;&#29616;&#26032;&#30340;&#26550;&#26500;&#22914;Transformer&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#22312;&#36825;&#27573;&#26102;&#38388;&#20869;&#65292;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#23545;&#25972;&#20307;&#24615;&#33021;&#25913;&#21892;&#30340;&#36129;&#29486;&#26356;&#22823;&#12290;&#23613;&#31649;&#21463;&#21040;&#22024;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#31639;&#27861;&#30340;&#36827;&#23637;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#30340;&#22686;&#38271;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05812v1 Announce Type: cross  Abstract: We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that the compute required to reach a set performance threshold has halved approximately every 8 months, with a 95% confidence interval of around 5 to 14 months, substantially faster than hardware gains per Moore's Law. We estimate augmented scaling laws, which enable us to quantify algorithmic progress and determine the relative contributions of scaling models versus innovations in training algorithms. Despite the rapid pace of algorithmic progress and the development of new architectures such as the transformer, our analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period. Though limited by noisy benchmark data, our analy
&lt;/p&gt;</description></item><item><title>UniSparse&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#35821;&#35328;&#65292;&#29992;&#20110;&#32479;&#19968;&#25277;&#35937;&#21644;&#23450;&#21046;&#31232;&#30095;&#26684;&#24335;&#65292;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#26041;&#24335;&#25903;&#25345;&#26032;&#30340;&#33258;&#23450;&#20041;&#31232;&#30095;&#25968;&#25454;&#32467;&#26500;&#21644;&#24067;&#23616;&#21464;&#20307;</title><link>https://arxiv.org/abs/2403.05802</link><description>&lt;p&gt;
UniSparse&#65306;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#31232;&#30095;&#26684;&#24335;&#23450;&#21046;&#30340;&#20013;&#38388;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
UniSparse: An Intermediate Language for General Sparse Format Customization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05802
&lt;/p&gt;
&lt;p&gt;
UniSparse&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#35821;&#35328;&#65292;&#29992;&#20110;&#32479;&#19968;&#25277;&#35937;&#21644;&#23450;&#21046;&#31232;&#30095;&#26684;&#24335;&#65292;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#26041;&#24335;&#25903;&#25345;&#26032;&#30340;&#33258;&#23450;&#20041;&#31232;&#30095;&#25968;&#25454;&#32467;&#26500;&#21644;&#24067;&#23616;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30828;&#20214;&#19987;&#38376;&#21270;&#30340;&#25345;&#32493;&#36235;&#21183;&#23548;&#33268;&#22312;&#22788;&#29702;&#31232;&#30095;&#24037;&#20316;&#37327;&#26102;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#33258;&#23450;&#20041;&#25968;&#25454;&#26684;&#24335;&#65292;&#36825;&#20123;&#24037;&#20316;&#37327;&#36890;&#24120;&#21463;&#38480;&#20110;&#20869;&#23384;&#12290;&#36825;&#20123;&#26684;&#24335;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#27169;&#24335;&#25110;&#38754;&#21521;&#30446;&#26631;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#24067;&#23616;&#26469;&#20248;&#21270;&#36719;&#30828;&#20214;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#20869;&#23384;&#35775;&#38382;&#24310;&#36831;&#21644;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31232;&#30095;&#24352;&#37327;&#32534;&#31243;&#27169;&#22411;&#21644;&#32534;&#35793;&#22120;&#24456;&#23569;&#25110;&#26681;&#26412;&#19981;&#25903;&#25345;&#26377;&#25928;&#23450;&#21046;&#31232;&#30095;&#26684;&#24335;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36825;&#20123;&#26694;&#26550;&#20351;&#29992;&#26377;&#38480;&#30340;&#27599;&#20010;&#32500;&#24230;&#23646;&#24615;&#38598;&#34920;&#31034;&#26684;&#24335;&#65292;&#23427;&#20204;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#26080;&#27861;&#36866;&#24212;&#22823;&#37327;&#26032;&#30340;&#33258;&#23450;&#20041;&#31232;&#30095;&#25968;&#25454;&#32467;&#26500;&#21644;&#24067;&#23616;&#21464;&#20307;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniSparse&#65292;&#19968;&#31181;&#25552;&#20379;&#32479;&#19968;&#25277;&#35937;&#20197;&#34920;&#31034;&#21644;&#23450;&#21046;&#31232;&#30095;&#26684;&#24335;&#30340;&#20013;&#38388;&#35821;&#35328;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#23646;&#24615;&#30340;&#26694;&#26550;&#19981;&#21516;&#65292;UniSparse&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05802v1 Announce Type: new  Abstract: The ongoing trend of hardware specialization has led to a growing use of custom data formats when processing sparse workloads, which are typically memory-bound. These formats facilitate optimized software/hardware implementations by utilizing sparsity pattern- or target-aware data structures and layouts to enhance memory access latency and bandwidth utilization. However, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. Additionally, because these frameworks represent formats using a limited set of per-dimension attributes, they lack the flexibility to accommodate numerous new variations of custom sparse data structures and layouts. To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. Unlike the existing attribute-based frameworks, UniSparse decouples
&lt;/p&gt;</description></item><item><title>ClinicalMamba&#26159;&#22522;&#20110;&#32437;&#21521;&#20020;&#24202;&#35760;&#24405;&#30340;&#29983;&#25104;&#22411;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;130&#20159;&#21644;28&#20159;&#21442;&#25968;&#65292;&#30456;&#23545;&#20110;Mamba&#21644;&#20020;&#24202;Llama&#65292;&#22312;&#24314;&#27169;&#36739;&#38271;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05795</link><description>&lt;p&gt;
ClinicalMamba&#65306;&#22522;&#20110;&#32437;&#21521;&#20020;&#24202;&#35760;&#24405;&#30340;&#29983;&#25104;&#22411;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05795
&lt;/p&gt;
&lt;p&gt;
ClinicalMamba&#26159;&#22522;&#20110;&#32437;&#21521;&#20020;&#24202;&#35760;&#24405;&#30340;&#29983;&#25104;&#22411;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;130&#20159;&#21644;28&#20159;&#21442;&#25968;&#65292;&#30456;&#23545;&#20110;Mamba&#21644;&#20020;&#24202;Llama&#65292;&#22312;&#24314;&#27169;&#36739;&#38271;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#30340;&#36827;&#27493;&#21462;&#20915;&#20110;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#20020;&#24202;&#31508;&#35760;&#20013;&#21253;&#21547;&#30340;&#22797;&#26434;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ClinicalMamba&#65292;&#36825;&#26159;Mamba&#35821;&#35328;&#27169;&#22411;&#30340;&#19987;&#38376;&#29256;&#26412;&#65292;&#23427;&#22312;&#22823;&#37327;&#32437;&#21521;&#20020;&#24202;&#35760;&#24405;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#35821;&#35328;&#29305;&#24449;&#21644;&#20449;&#24687;&#22788;&#29702;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05795v1 Announce Type: new  Abstract: The advancement of natural language processing (NLP) systems in healthcare hinges on language model ability to interpret the intricate information contained within clinical notes. This process often requires integrating information from various time points in a patient's medical history. However, most earlier clinical language models were pretrained with a context length limited to roughly one clinical document. In this study, We introduce ClinicalMamba, a specialized version of the Mamba language model, pretrained on a vast corpus of longitudinal clinical notes to address the unique linguistic characteristics and information processing needs of the medical domain. ClinicalMamba, with 130 million and 2.8 billion parameters, demonstrates a superior performance in modeling clinical language across extended text lengths compared to Mamba and clinical Llama. With few-shot learning, ClinicalMamba achieves notable benchmarks in speed and accur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#28436;&#32462;&#36890;&#36807;&#24402;&#32435;&#65288;ItD&#65289;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#28436;&#32462;&#33258;&#23398;&#24402;&#32435;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24402;&#32435;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05789</link><description>&lt;p&gt;
ItD&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#28436;&#32462;&#33258;&#23398;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
ItD: Large Language Models Can Teach Themselves Induction through Deduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#28436;&#32462;&#36890;&#36807;&#24402;&#32435;&#65288;ItD&#65289;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#28436;&#32462;&#33258;&#23398;&#24402;&#32435;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24402;&#32435;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#23427;&#20204;&#22312;&#36827;&#34892;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#20027;&#35201;&#37319;&#29992;&#8220;&#21518;&#22788;&#29702;&#8221;&#33539;&#24335;&#26469;&#25552;&#39640;LLMs&#22312;&#24402;&#32435;&#26041;&#38754;&#30340;&#34920;&#29616;&#65288;&#22914;&#20551;&#35774;&#25628;&#32034;&#21644;&#32454;&#21270;&#26041;&#27861;&#65289;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#21463;&#38480;&#20110;LLMs&#30340;&#22266;&#26377;&#24402;&#32435;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#28436;&#32462;&#36890;&#36807;&#24402;&#32435;&#65288;ItD&#65289;&#65292;&#20197;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#28436;&#32462;&#33258;&#23398;&#24402;&#32435;&#12290;ItD&#26694;&#26550;&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#28436;&#32462;&#25968;&#25454;&#29983;&#25104;&#27169;&#22359;&#29992;&#20110;&#29983;&#25104;&#24402;&#32435;&#25968;&#25454;&#65292;&#20197;&#21450;&#26420;&#32032;&#36125;&#21494;&#26031;&#24402;&#32435;&#27169;&#22359;&#29992;&#20110;&#20248;&#21270;LLMs&#30340;&#24494;&#35843;&#21644;&#35299;&#30721;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;ItD&#22312;&#20004;&#20010;&#24402;&#32435;&#22522;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#24615;&#33021;&#25552;&#21319;&#20998;&#21035;&#20026;36%&#21644;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05789v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt ``post processes'' paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search &amp; refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36% and 1
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;NLL&#25439;&#22833;&#21644;fi&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#20107;&#23454;&#24615;&#65292;&#25913;&#36827;&#20102;&#32454;&#31890;&#24230;&#25439;&#22833;&#25130;&#26029;&#23545;&#20110;&#25688;&#35201;&#20013;&#20107;&#23454;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.05788</link><description>&lt;p&gt;
&#23545;&#32454;&#31890;&#24230;&#25439;&#22833;&#25130;&#26029;&#25928;&#30410;&#30340;&#30740;&#31350;&#65306;&#20197;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#24615;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05788
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;NLL&#25439;&#22833;&#21644;fi&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#20107;&#23454;&#24615;&#65292;&#25913;&#36827;&#20102;&#32454;&#31890;&#24230;&#25439;&#22833;&#25130;&#26029;&#23545;&#20110;&#25688;&#35201;&#20013;&#20107;&#23454;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05788v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39033; &#25688;&#35201;&#65306;&#25991;&#26412;&#25688;&#35201;&#21644;&#31616;&#21270;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#24320;&#21457;&#30340;&#27169;&#22411;&#24448;&#24448;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#22312;&#26410;&#23545;&#40784;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#26159;&#25439;&#22833;&#25130;&#26029;&#65288;LT&#65289;&#65288;Kang&#21644;Hashimoto&#65292;2020&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20462;&#25913;&#26631;&#20934;&#23545;&#25968;&#25439;&#22833;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#21435;&#38500;&#22024;&#26434;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20351;&#29992;LT&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20250;&#20135;&#29983;&#22823;&#37327;&#24187;&#35273;&#23454;&#20307;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20107;&#23454;&#21644;&#38750;&#20107;&#23454;&#31034;&#20363;&#20043;&#38388;&#22522;&#30784;&#25439;&#22833;&#30340;&#34892;&#20026;&#65292;&#20197;&#20102;&#35299;&#24182;&#25913;&#36827;LT&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22024;&#26434;&#30446;&#26631;&#20855;&#26377;&#36739;&#39640;NLL&#25439;&#22833;&#30340;&#22522;&#30784;&#20551;&#35774;&#19981;&#34987;&#28385;&#36275;&#26102;&#65292;LT&#30340;&#24615;&#33021;&#26159;&#26377;&#38480;&#30340;&#65292;&#24182;&#21457;&#29616;&#23454;&#20307;&#20043;&#38388;&#30340;&#21333;&#35789;&#32423;NLL&#20026;&#21306;&#20998;&#20107;&#23454;&#24615;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20449;&#21495;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#28857;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;NLL&#25439;&#22833;&#21644;fi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05788v1 Announce Type: cross  Abstract: Text summarization and simplification are among the most widely used applications of AI. However, models developed for such tasks are often prone to hallucination, which can result from training on unaligned data. One efficient approach to address this issue is Loss Truncation (LT) (Kang and Hashimoto, 2020), an approach to modify the standard log loss to adaptively remove noisy examples during training. However, we find that LT alone yields a considerable number of hallucinated entities on various datasets. We study the behavior of the underlying losses between factual and non-factual examples, to understand and refine the performance of LT. We demonstrate that LT's performance is limited when the underlying assumption that noisy targets have higher NLL loss is not satisfied, and find that word-level NLL among entities provides better signal for distinguishing factuality. We then leverage this to propose a fine-grained NLL loss and fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#24212;&#29992;&#20110;&#24191;&#27867;&#25216;&#33021;&#21644;&#22810;&#31181;&#34892;&#20026;&#30340;&#21151;&#25928;&#65292;&#24182;&#21457;&#29616;&#23548;&#21521;&#24191;&#27867;&#25216;&#33021;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#20013;&#21516;&#26102;&#27880;&#20837;&#20010;&#20307;&#23548;&#21521;&#21521;&#37327;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05767</link><description>&lt;p&gt;
&#23558;&#28608;&#27963;&#23548;&#21521;&#25193;&#23637;&#21040;&#24191;&#27867;&#25216;&#33021;&#19982;&#22810;&#31181;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Extending Activation Steering to Broad Skills and Multiple Behaviours
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#24212;&#29992;&#20110;&#24191;&#27867;&#25216;&#33021;&#21644;&#22810;&#31181;&#34892;&#20026;&#30340;&#21151;&#25928;&#65292;&#24182;&#21457;&#29616;&#23548;&#21521;&#24191;&#27867;&#25216;&#33021;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#20013;&#21516;&#26102;&#27880;&#20837;&#20010;&#20307;&#23548;&#21521;&#21521;&#37327;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21361;&#38505;&#30340;&#33021;&#21147;&#65292;&#36825;&#24456;&#21487;&#33021;&#22312;&#26410;&#26469;&#21464;&#24471;&#26356;&#21152;&#26840;&#25163;&#12290;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#21487;&#29992;&#20110;&#20943;&#23569;&#36825;&#20123;&#33021;&#21147;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28608;&#27963;&#23548;&#21521;&#22312;&#24191;&#27867;&#25216;&#33021;&#21644;&#22810;&#31181;&#34892;&#20026;&#20013;&#30340;&#21151;&#25928;&#12290;&#36890;&#36807;&#27604;&#36739;&#20943;&#23569;&#23545;&#19968;&#33324;&#32534;&#30721;&#33021;&#21147;&#21644;Python&#29305;&#23450;&#33021;&#21147;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23548;&#21521;&#26356;&#24191;&#27867;&#25216;&#33021;&#19982;&#23548;&#21521;&#36739;&#31364;&#25216;&#33021;&#31454;&#20105;&#28608;&#28872;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#25110;&#26356;&#23569;&#36817;&#35270;&#21644;&#23547;&#27714;&#36130;&#23500;&#65292;&#20197;&#21450;&#20854;&#20182;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#23558;&#22810;&#31181;&#19981;&#21516;&#34892;&#20026;&#30340;&#23548;&#21521;&#21521;&#37327;&#32452;&#21512;&#20026;&#19968;&#20010;&#23548;&#21521;&#21521;&#37327;&#36890;&#24120;&#19981;&#25104;&#21151;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#20013;&#19981;&#21516;&#20301;&#32622;&#27880;&#20837;&#20010;&#20307;&#23548;&#21521;&#21521;&#37327;&#26159;&#26377;&#21069;&#36884;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05767v1 Announce Type: cross  Abstract: Current large language models have dangerous capabilities, which are likely to become more problematic in the future. Activation steering techniques can be used to reduce risks from these capabilities. In this paper, we investigate the efficacy of activation steering for broad skills and multiple behaviours. First, by comparing the effects of reducing performance on general coding ability and Python-specific ability, we find that steering broader skills is competitive to steering narrower skills. Second, we steer models to become more or less myopic and wealth-seeking, among other behaviours. In our experiments, combining steering vectors for multiple different behaviours into one steering vector is largely unsuccessful. On the other hand, injecting individual steering vectors at different places in a model simultaneously is promising.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#36890;&#36807;&#36981;&#24490;&#39044;&#23450;&#20041;&#27969;&#31243;&#21644;&#20445;&#30041;API&#20381;&#36182;&#24615;&#35299;&#20915;&#29992;&#25143;&#24847;&#22270;&#30340;&#24544;&#23454;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#30651;&#21551;&#21457;&#24335;&#30340;&#21463;&#38480;&#35299;&#30721;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05766</link><description>&lt;p&gt;
FLAP: &#22312;LLMs&#20013;&#20855;&#26377;&#21463;&#38480;&#35299;&#30721;&#30340;&#27969;&#31243;&#36981;&#24490;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
FLAP: Flow Adhering Planning with Constrained Decoding in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#36890;&#36807;&#36981;&#24490;&#39044;&#23450;&#20041;&#27969;&#31243;&#21644;&#20445;&#30041;API&#20381;&#36182;&#24615;&#35299;&#20915;&#29992;&#25143;&#24847;&#22270;&#30340;&#24544;&#23454;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#30651;&#21551;&#21457;&#24335;&#30340;&#21463;&#38480;&#35299;&#30721;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#21010;&#23545;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#20195;&#29702;&#20154;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20154;&#31867;&#20195;&#29702;&#20154;&#36890;&#24120;&#36890;&#36807;&#36981;&#24490;&#39044;&#23450;&#20041;&#30340;&#24037;&#20316;&#27969;&#31243;&#35299;&#20915;&#29992;&#25143;&#38382;&#39064;&#65292;&#23558;&#24037;&#20316;&#27969;&#31243;&#27493;&#39588;&#20998;&#35299;&#20026;&#21487;&#25805;&#20316;&#39033;&#30446;&#65292;&#24182;&#36890;&#36807;&#25191;&#34892;API&#25191;&#34892;&#25805;&#20316;&#65307;&#25152;&#26377;&#36825;&#20123;&#37117;&#38656;&#35201;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#37492;&#20110;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#23581;&#35797;&#20351;&#29992;LLMs&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;API&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#20559;&#21521;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#35745;&#21010;&#19982;&#39044;&#23450;&#20041;&#24037;&#20316;&#27969;&#31243;&#21644;API&#20381;&#36182;&#24615;&#30340;&#24544;&#23454;&#24615;&#24182;&#19981;&#34987;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#24037;&#20316;&#27969;&#31243;&#26159;&#33258;&#23450;&#20041;&#30340;&#24182;&#19988;&#23481;&#26131;&#26356;&#25913;&#65292;&#22240;&#27492;&#65292;&#24555;&#36895;&#20351;&#20195;&#29702;&#20154;&#36866;&#24212;&#21464;&#21270;&#26159;&#21487;&#21462;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#36890;&#36807;&#36981;&#24490;&#39044;&#23450;&#20041;&#27969;&#31243;&#21644;&#20445;&#30041;API&#20381;&#36182;&#24615;&#35299;&#20915;&#29992;&#25143;&#24847;&#22270;&#30340;&#24544;&#23454;&#35268;&#21010;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#30651;&#21551;&#21457;&#24335;&#30340;&#21463;&#38480;&#35299;&#30721;&#31639;&#27861;&#29992;&#20110;&#24544;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05766v1 Announce Type: new  Abstract: Planning is a crucial task for agents in task oriented dialogs (TODs). Human agents typically resolve user issues by following predefined workflows, decomposing workflow steps into actionable items, and performing actions by executing APIs in order; all of which require reasoning and planning. With the recent advances in LLMs, there have been increasing attempts to use LLMs for task planning and API usage. However, the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs because of their bias towards pretraining data. Moreover, in real life, workflows are custom-defined and prone to change, hence, quickly adapting agents to the changes is desirable. In this paper, we study faithful planning in TODs to resolve user intents by following predefined flows and preserving API dependencies. We propose a constrained decoding algorithm based on lookahead heuristic for faithful planning. Our algorithm
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.05750</link><description>&lt;p&gt;
&#35299;&#35835;AI&#31508;: &#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#24443;&#24213;&#39072;&#35206;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#28145;&#20837;&#23457;&#26597;&#12289;&#20262;&#29702;&#23457;&#26597;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#20316;&#20026;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#35780;&#20272;&#20102;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#39046;&#22495;&#38480;&#21046;&#30340;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26500;&#24314;&#20102;&#24847;&#22823;&#21033;&#25512;&#29305;&#29992;&#25143;&#20154;&#21475;&#20998;&#31867;&#25968;&#25454;&#38598;DADIT&#65292;&#29992;&#20110;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24615;&#21035;&#21644;&#24180;&#40836;&#65292;&#21457;&#29616;&#21253;&#21547;&#25512;&#25991;&#29305;&#24449;&#22312;&#24180;&#40836;&#39044;&#27979;&#20013;&#29305;&#21035;&#26377;&#25928;&#65292;XLM-based&#20998;&#31867;&#22120;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#24120;&#29992;&#30340;M3&#20998;&#31867;&#22120;&#26368;&#22810;&#21487;&#25552;&#39640;53% F1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.05700</link><description>&lt;p&gt;
DADIT&#65306;&#24847;&#22823;&#21033;&#25512;&#29305;&#29992;&#25143;&#20154;&#21475;&#20998;&#31867;&#25968;&#25454;&#38598;&#21450;&#39044;&#27979;&#26041;&#27861;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
DADIT: A Dataset for Demographic Classification of Italian Twitter Users and a Comparison of Prediction Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26500;&#24314;&#20102;&#24847;&#22823;&#21033;&#25512;&#29305;&#29992;&#25143;&#20154;&#21475;&#20998;&#31867;&#25968;&#25454;&#38598;DADIT&#65292;&#29992;&#20110;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24615;&#21035;&#21644;&#24180;&#40836;&#65292;&#21457;&#29616;&#21253;&#21547;&#25512;&#25991;&#29305;&#24449;&#22312;&#24180;&#40836;&#39044;&#27979;&#20013;&#29305;&#21035;&#26377;&#25928;&#65292;XLM-based&#20998;&#31867;&#22120;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#24120;&#29992;&#30340;M3&#20998;&#31867;&#22120;&#26368;&#22810;&#21487;&#25552;&#39640;53% F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#23478;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#20998;&#23618;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26469;&#30740;&#31350;&#19968;&#33324;&#20844;&#20247;&#30340;&#24577;&#24230;&#12289;&#20449;&#24565;&#21644;&#34892;&#20026;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26679;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#26500;&#24314;&#12289;&#39564;&#35777;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#20855;&#26377;30M&#26465;&#25512;&#29305;&#30340;20k&#24847;&#22823;&#21033;&#25512;&#29305;&#29992;&#25143;&#30340;&#20195;&#34920;&#24615;DADIT&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20182;&#20204;&#30340;&#20010;&#20154;&#31616;&#20171;&#21644;&#20010;&#20154;&#22836;&#20687;&#12290;&#25105;&#20204;&#20026;&#29992;&#25143;&#25968;&#25454;&#28155;&#21152;&#20102;&#39640;&#36136;&#37327;&#30340;&#24615;&#21035;&#12289;&#24180;&#40836;&#21644;&#20301;&#32622;&#26631;&#31614;&#12290;DADIT&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#21644;&#27604;&#36739;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29992;&#20110;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24615;&#21035;&#21644;&#24180;&#40836;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25512;&#25991;&#26159;&#21542;&#21253;&#21547;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#22240;&#20026;&#20687;M3&#36825;&#26679;&#30340;&#27969;&#34892;&#20998;&#31867;&#22120;&#24182;&#26410;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#22522;&#20110;XLM&#30340;&#20998;&#31867;&#22120;&#22312;F1&#20998;&#25968;&#19978;&#27604;&#24120;&#29992;&#23545;&#25163;M3&#25552;&#39640;&#20102;&#39640;&#36798;53%&#12290;&#23545;&#20110;&#24180;&#40836;&#39044;&#27979;&#65292;&#20998;&#31867;&#22120;&#20174;&#21253;&#21547;&#25512;&#25991;&#20316;&#20026;&#29305;&#24449;&#20013;&#33719;&#30410;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#24503;&#22269;&#27979;&#35797;&#38598;&#19978;&#30830;&#35748;&#20102;&#36825;&#20123;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05700v1 Announce Type: new  Abstract: Social scientists increasingly use demographically stratified social media data to study the attitudes, beliefs, and behavior of the general public. To facilitate such analyses, we construct, validate, and release publicly the representative DADIT dataset of 30M tweets of 20k Italian Twitter users, along with their bios and profile pictures. We enrich the user data with high-quality labels for gender, age, and location. DADIT enables us to train and compare the performance of various state-of-the-art models for the prediction of the gender and age of social media users. In particular, we investigate if tweets contain valuable information for the task, since popular classifiers like M3 don't leverage them. Our best XLM-based classifier improves upon the commonly used competitor M3 by up to 53% F1. Especially for age prediction, classifiers profit from including tweets as features. We also confirm these findings on a German test set.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#20840;&#29699;&#33539;&#22260;&#30340;&#22810;&#35821;&#35328;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#25968;&#25454;&#38598;SeeGULL Multilingual&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#29983;&#25104;&#24335;&#22810;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#24615;&#21644;&#20844;&#24179;&#24615;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#33521;&#25991;&#36164;&#28304;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05696</link><description>&lt;p&gt;
SeeGULL Multilingual&#65306;&#19968;&#20010;&#22320;&#29702;&#25991;&#21270;&#23450;&#20301;&#30340;&#21051;&#26495;&#21360;&#35937;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05696
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#20840;&#29699;&#33539;&#22260;&#30340;&#22810;&#35821;&#35328;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#25968;&#25454;&#38598;SeeGULL Multilingual&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#29983;&#25104;&#24335;&#22810;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#24615;&#21644;&#20844;&#24179;&#24615;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#33521;&#25991;&#36164;&#28304;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29983;&#25104;&#24335;&#22810;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#36805;&#36895;&#37096;&#32626;&#65292;&#20294;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#20844;&#24179;&#24615;&#35780;&#20272;&#20027;&#35201;&#23616;&#38480;&#20110;&#33521;&#25991;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#24046;&#36317;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26368;&#36817;&#24341;&#20837;&#30340;&#26041;&#27861;&#65292;&#23558;LLM&#29983;&#25104;&#19982;&#25991;&#21270;&#23450;&#20301;&#39564;&#35777;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20840;&#29699;&#33539;&#22260;&#30340;&#22810;&#35821;&#35328;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#25968;&#25454;&#38598;SeeGULL Multilingual&#65292;&#21253;&#21547;&#36229;&#36807;25K&#26465;&#21051;&#26495;&#21360;&#35937;&#65292;&#28085;&#30422;20&#31181;&#35821;&#35328;&#65292;&#22312;23&#20010;&#22320;&#21306;&#36827;&#34892;&#20102;&#20154;&#31867;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05696v1 Announce Type: new  Abstract: While generative multilingual models are rapidly being deployed, their safety and fairness evaluations are largely limited to resources collected in English. This is especially problematic for evaluations targeting inherently socio-cultural phenomena such as stereotyping, where it is important to build multi-lingual resources that reflect the stereotypes prevalent in respective language communities. However, gathering these resources, at scale, in varied languages and regions pose a significant challenge as it requires broad socio-cultural knowledge and can also be prohibitively expensive. To overcome this critical gap, we employ a recently introduced approach that couples LLM generations for scale with culturally situated validations for reliability, and build SeeGULL Multilingual, a global-scale multilingual dataset of social stereotypes, containing over 25K stereotypes, spanning 20 languages, with human annotations across 23 regions, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05680</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#23545;&#22522;&#20110;&#35270;&#35273;&#30340;LLM&#39044;&#27979;&#36827;&#34892;&#20998;&#35299;&#20197;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05680
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CT&#26816;&#26597;&#30340;&#25968;&#37327;&#27599;&#24180;&#37117;&#22312;&#22686;&#21152;&#65292;&#36825;&#23548;&#33268;&#25918;&#23556;&#31185;&#21307;&#29983;&#30130;&#21171;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#20943;&#36731;&#20182;&#20204;&#30340;&#36127;&#25285;&#65292;&#20294;&#20854;&#22312;&#20020;&#24202;&#20013;&#30340;&#37319;&#29992;&#21462;&#20915;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#20449;&#20219;&#21644;&#29983;&#25104;&#20869;&#23481;&#30340;&#31616;&#21333;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05680v1 Announce Type: new  Abstract: The volume of CT exams being done in the world has been rising every year, which has led to radiologist burn-out. Large Language Models (LLMs) have the potential to reduce their burden, but their adoption in the clinic depends on radiologist trust, and easy evaluation of generated content. Presently, many automated methods are available to evaluate the reports generated for chest radiographs, but such an approach is not available for CT presently. In this paper, we propose a novel evaluation framework to judge the capabilities of vision-language LLMs in generating accurate summaries of CT-based abnormalities. CT slices containing an abnormality (e.g., lesion) were input to a vision-based LLM (GPT-4V, LLaVA-Med, and RadFM), and it generated a free-text summary of the predicted characteristics of the abnormality. Next, a GPT-4 model decomposed the summary into specific aspects (body part, location, type, and attributes), automatically eval
&lt;/p&gt;</description></item><item><title>PipeRAG&#36890;&#36807;&#31639;&#27861;-&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#24555;&#36895;&#26816;&#32034;&#22686;&#24378;&#65292;&#23558;&#26816;&#32034;&#26102;&#38388;&#22823;&#22823;&#38477;&#20302;&#24182;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05676</link><description>&lt;p&gt;
PipeRAG: &#36890;&#36807;&#31639;&#27861;-&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;&#23454;&#29616;&#24555;&#36895;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05676
&lt;/p&gt;
&lt;p&gt;
PipeRAG&#36890;&#36807;&#31639;&#27861;-&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#24555;&#36895;&#26816;&#32034;&#22686;&#24378;&#65292;&#23558;&#26816;&#32034;&#26102;&#38388;&#22823;&#22823;&#38477;&#20302;&#24182;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#26631;&#35760;&#25968;&#25454;&#24211;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20174;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#22312;&#25972;&#20307;&#29983;&#25104;&#26102;&#38388;&#20013;&#21487;&#33021;&#21344;&#25454;&#30456;&#24403;&#22823;&#27604;&#20363;&#65292;&#29305;&#21035;&#26159;&#24403;&#23450;&#26399;&#25191;&#34892;&#26816;&#32034;&#20197;&#23558;&#26816;&#32034;&#20869;&#23481;&#19982;&#29983;&#25104;&#30340;&#26368;&#26032;&#29366;&#24577;&#23545;&#40784;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PipeRAG&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;-&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#24182;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;PipeRAG&#32467;&#21512;&#20102;&#65288;1&#65289;&#31649;&#36947;&#24182;&#34892;&#24615;&#65292;&#20197;&#23454;&#29616;&#24182;&#21457;&#26816;&#32034;&#21644;&#29983;&#25104;&#36807;&#31243;&#65292;&#65288;2&#65289;&#28789;&#27963;&#30340;&#26816;&#32034;&#38388;&#38548;&#65292;&#20197;&#26368;&#22823;&#21270;&#31649;&#36947;&#24182;&#34892;&#24615;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#65288;3&#65289;&#24615;&#33021;&#27169;&#22411;&#65292;&#26681;&#25454;&#29983;&#25104;&#30340;&#29366;&#24577;&#21644;&#24213;&#23618;&#30828;&#20214;&#33258;&#21160;&#24179;&#34913;&#26816;&#32034;&#36136;&#37327;&#21644;&#24310;&#36831;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#21512;&#36825;&#19977;&#31181;&#26041;&#27861;&#65292;PipeRAG&#23454;&#29616;&#20102;&#39640;&#36798;2.6&#20493;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05676v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) can enhance the generation quality of large language models (LLMs) by incorporating external token databases. However, retrievals from large databases can constitute a substantial portion of the overall generation time, particularly when retrievals are periodically performed to align the retrieved content with the latest states of generation. In this paper, we introduce PipeRAG, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality. PipeRAG integrates (1) pipeline parallelism to enable concurrent retrieval and generation processes, (2) flexible retrieval intervals to maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance retrieval quality and latency based on the generation states and underlying hardware. Our evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6$\
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;ChatGPT&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#20998;&#31867;&#22120;&#22312;&#35782;&#21035;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21516;&#26102;&#26174;&#31034;&#22312;&#35757;&#32451;&#20013;&#21152;&#20837;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2403.05640</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#25968;&#25454;&#20197;&#20379;&#24847;&#22270;&#20998;&#31867;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generating Hard-Negative Out-of-Scope Data with ChatGPT for Intent Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05640
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ChatGPT&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#20998;&#31867;&#22120;&#22312;&#35782;&#21035;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21516;&#26102;&#26174;&#31034;&#22312;&#35757;&#32451;&#20013;&#21152;&#20837;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#20998;&#31867;&#22120;&#24517;&#39035;&#33021;&#22815;&#21306;&#20998;&#29992;&#25143;&#30340;&#35805;&#35821;&#26159;&#21542;&#23646;&#20110;&#20219;&#20309;&#25903;&#25345;&#30340;&#24847;&#22270;&#65292;&#20197;&#36991;&#20813;&#20135;&#29983;&#19981;&#27491;&#30830;&#21644;&#26080;&#20851;&#30340;&#31995;&#32479;&#21709;&#24212;&#12290; &#23613;&#31649;&#24050;&#32463;&#30740;&#31350;&#20102;&#24847;&#22270;&#20998;&#31867;&#22120;&#30340;&#36229;&#33539;&#22260;&#65288;OOS&#65289;&#26816;&#27979;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23578;&#26410;&#30740;&#31350;&#20998;&#31867;&#22120;&#24615;&#33021;&#23545;&#25239;&#38590;&#36127;&#26679;&#26412;&#36229;&#33539;&#22260;&#35805;&#35821;&#65288;&#21363;&#65292;&#20855;&#26377;&#19982;&#33539;&#22260;&#20869;&#25968;&#25454;&#20849;&#21516;&#29305;&#24449;&#65292;&#20294;&#23454;&#38469;&#19978;&#36229;&#33539;&#22260;&#65289;&#30340;&#21464;&#21270;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#12290; &#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#25216;&#26415;&#26500;&#24314;&#20102;&#20116;&#20010;&#26032;&#30340;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#38598;&#65292;&#24182;&#38024;&#23545;&#19977;&#20010;&#22522;&#20934;&#24847;&#22270;&#20998;&#31867;&#22120;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#31867;&#22120;&#22312;&#27491;&#30830;&#35782;&#21035;&#38590;&#36127;&#26679;&#26412;OOS&#35805;&#35821;&#26041;&#38754;&#30340;&#22256;&#38590;&#31243;&#24230;&#36229;&#36807;&#20102;&#19968;&#33324;&#30340;OOS&#35805;&#35821;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#20013;&#32467;&#21512;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#38590;&#36127;&#26679;&#26412;OOS&#25968;&#25454;&#21644;&#19968;&#33324;OOS&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05640v1 Announce Type: new  Abstract: Intent classifiers must be able to distinguish when a user's utterance does not belong to any supported intent to avoid producing incorrect and unrelated system responses. Although out-of-scope (OOS) detection for intent classifiers has been studied, previous work has not yet studied changes in classifier performance against hard-negative out-of-scope utterances (i.e., inputs that share common features with in-scope data, but are actually out-of-scope). We present an automated technique to generate hard-negative OOS data using ChatGPT. We use our technique to build five new hard-negative OOS datasets, and evaluate each against three benchmark intent classifiers. We show that classifiers struggle to correctly identify hard-negative OOS utterances more than general OOS utterances. Finally, we show that incorporating hard-negative OOS data for training improves model robustness when detecting hard-negative OOS data and general OOS data. Our
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20803;&#35748;&#30693;&#26041;&#27861;&#65292;&#21517;&#20026;CLEAR&#65292;&#26088;&#22312;&#20026;LLMs&#25552;&#20379;&#33258;&#25105;&#24847;&#35782;&#30340;&#38169;&#35823;&#35782;&#21035;&#21644;&#32416;&#27491;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.05636</link><description>&lt;p&gt;
&#26080;&#38656;&#35843;&#21442;&#30340;LLM&#37096;&#32626;&#36127;&#36131;&#24178;&#39044;--&#19968;&#31181;&#20803;&#35748;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20803;&#35748;&#30693;&#26041;&#27861;&#65292;&#21517;&#20026;CLEAR&#65292;&#26088;&#22312;&#20026;LLMs&#25552;&#20379;&#33258;&#25105;&#24847;&#35782;&#30340;&#38169;&#35823;&#35782;&#21035;&#21644;&#32416;&#27491;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#23569;&#37327;&#25110;&#38646;-shot&#25552;&#31034;&#22312;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20652;&#29983;&#20102;&#21464;&#38761;&#24615;&#36827;&#23637;&#65292;&#32469;&#36807;&#20102;&#21442;&#25968;&#35843;&#25972;&#30340;&#24517;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20415;&#21033;&#30340;&#25805;&#20316;&#26041;&#24335;&#21152;&#21095;&#20102;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#20204;&#24222;&#22823;&#27169;&#22411;&#35268;&#27169;&#32972;&#21518;&#30340;&#31070;&#31192;&#8220;&#40657;&#21283;&#23376;&#8221;&#24615;&#36136;&#12290;&#36825;&#20123;&#25285;&#24551;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#19981;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20915;&#31574;&#20381;&#36182;&#20110;&#24494;&#22937;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#22914;&#36890;&#36807;&#27010;&#24565;&#29702;&#35299;&#24863;&#30693;&#21644;&#33258;&#36866;&#24212;&#22320;&#32416;&#27491;&#38169;&#35823;&#21028;&#26029;&#30340;&#33021;&#21147;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20803;&#35748;&#30693;&#26041;&#27861;&#65292;&#31216;&#20026;CLEAR&#65292;&#20026;LLMs&#25552;&#20379;&#33258;&#25105;&#24847;&#35782;&#30340;&#38169;&#35823;&#35782;&#21035;&#21644;&#32416;&#27491;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#21161;&#20110;&#26500;&#24314;co
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05636v1 Announce Type: new  Abstract: Large Language Models (LLMs) have catalyzed transformative advances across a spectrum of natural language processing tasks through few-shot or zero-shot prompting, bypassing the need for parameter tuning. While convenient, this modus operandi aggravates ``hallucination'' concerns, particularly given the enigmatic ``black-box'' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences. In contrast, human decision-making relies on nuanced cognitive processes, such as the ability to sense and adaptively correct misjudgments through conceptual understanding. Drawing inspiration from human cognition, we propose an innovative \textit{metacognitive} approach, dubbed \textbf{CLEAR}, to equip LLMs with capabilities for self-aware error identification and correction. Our framework facilitates the construction of co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#24494;&#35843;&#31034;&#20363;&#30340;&#30417;&#30563;&#26469;&#25511;&#21046;&#20854;&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#20102;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.05612</link><description>&lt;p&gt;
&#19981;&#29087;&#24713;&#30340;&#24494;&#35843;&#31034;&#20363;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Unfamiliar Finetuning Examples Control How Language Models Hallucinate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#24494;&#35843;&#31034;&#20363;&#30340;&#30417;&#30563;&#26469;&#25511;&#21046;&#20854;&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#20102;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20542;&#21521;&#20110;&#29983;&#25104;&#21548;&#36215;&#26469;&#20196;&#20154;&#20449;&#26381;&#20294;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#29305;&#21035;&#26159;&#24403;&#22312;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#19978;&#36827;&#34892;&#26597;&#35810;&#26102;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35843;&#25972;&#21518;&#30340;LLMs&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#27169;&#24335;&#65306;&#38543;&#30528;&#36755;&#20837;&#21464;&#24471;&#26356;&#19981;&#29087;&#24713;&#65292;LLMs&#30340;&#36755;&#20986;&#20542;&#21521;&#20110;&#40664;&#35748;&#20026;"&#21547;&#31946;&#20854;&#35789;"&#30340;&#39044;&#27979;&#65292;&#20854;&#24418;&#24335;&#21463;&#24494;&#35843;&#25968;&#25454;&#20013;&#19981;&#29087;&#24713;&#31034;&#20363;&#30417;&#30563;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#20462;&#25913;&#36825;&#20123;&#31034;&#20363;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;LLM&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#65288;&#20363;&#22914;&#65292;&#25945;&#20250;&#23427;&#20204;&#35828;&#8220;&#25105;&#19981;&#30693;&#36947;&#8221;&#65289;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;RL&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22870;&#21169;&#27169;&#22411;&#24187;&#35273;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MMLU&#19978;&#30340;&#22810;&#36873;QA&#20013;&#36827;&#34892;&#19968;&#31995;&#21015;&#21463;&#25511;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05612v1 Announce Type: cross  Abstract: Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts. In this work, we explore the underlying mechanisms that govern how finetuned LLMs hallucinate. Our investigation reveals an interesting pattern: as inputs become more unfamiliar, LLM outputs tend to default towards a ``hedged'' prediction, whose form is determined by how the unfamiliar examples in the finetuning data are supervised. Thus, by strategically modifying these examples' supervision, we can control LLM predictions for unfamiliar inputs (e.g., teach them to say ``I don't know''). Based on these principles, we develop an RL approach that more reliably mitigates hallucinations for long-form generation tasks, by tackling the challenges presented by reward model hallucinations. We validate our findings with a series of controlled experiments in multiple-choice QA on MMLU, as
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#20419;&#36827;&#20102;&#23545;&#32597;&#35265;&#30142;&#30149;&#30340;&#35786;&#26029;&#65292;&#24182;&#22312;&#20020;&#24202;&#23454;&#36341;&#21644;&#21307;&#23398;&#25945;&#32946;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>https://arxiv.org/abs/2403.05606</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#29992;&#20110;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#33033;&#32476;&#33180;&#32959;&#30244;
&lt;/p&gt;
&lt;p&gt;
A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05606
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#20419;&#36827;&#20102;&#23545;&#32597;&#35265;&#30142;&#30149;&#30340;&#35786;&#26029;&#65292;&#24182;&#22312;&#20020;&#24202;&#23454;&#36341;&#21644;&#21307;&#23398;&#25945;&#32946;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35786;&#26029;&#32597;&#35265;&#30142;&#30149;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#38754;&#20020;&#30528;&#20849;&#21516;&#25361;&#25112;&#65292;&#38656;&#35201;&#19987;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#25165;&#33021;&#20934;&#30830;&#35782;&#21035;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#36825;&#31867;&#25216;&#26415;&#30340;&#21457;&#23637;&#21463;&#21040;&#32597;&#35265;&#29366;&#20917;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#38656;&#35201;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#36182;&#24615;&#30340;&#27169;&#22411;&#30340;&#38656;&#27714;&#30340;&#38459;&#30861;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20855;&#26377;&#20154;&#31867;&#21487;&#35835;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#20419;&#36827;&#20020;&#24202;&#21307;&#29983;&#30340;&#39564;&#35777;&#24182;&#20419;&#36827;&#21307;&#23398;&#25945;&#32946;&#12290;&#22312;&#24403;&#21069;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#36825;&#26159;&#25104;&#20154;&#20013;&#26368;&#24120;&#35265;&#30340;&#30524;&#30555;&#30284;&#30151;&#24418;&#24335;&#65292;&#23613;&#31649;&#32597;&#35265;&#65292;&#32633;&#24739;&#29575;&#20026;&#27599;&#30334;&#19975;&#20154;5.1&#20363;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#20849;&#21253;&#25324;750&#21517;&#24739;&#32773;&#65292;&#28085;&#30422;&#20102;&#20174;2004&#24180;&#33267;2022&#24180;&#25910;&#38598;&#30340;&#19977;&#31181;&#19981;&#21516;&#25104;&#20687;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#21487;&#21306;&#20998;&#19977;&#31181;&#31867;&#22411;&#30340;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#34701;&#21512;&#20102;&#19968;&#20123;&#19981;&#22826;&#37325;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05606v1 Announce Type: cross  Abstract: Diagnosing rare diseases presents a common challenge in clinical practice, necessitating the expertise of specialists for accurate identification. The advent of machine learning offers a promising solution, while the development of such technologies is hindered by the scarcity of data on rare conditions and the demand for models that are both interpretable and trustworthy in a clinical context. Interpretable AI, with its capacity for human-readable outputs, can facilitate validation by clinicians and contribute to medical education. In the current work, we focus on choroid neoplasias, the most prevalent form of eye cancer in adults, albeit rare with 5.1 per million. We built the so-far largest dataset consisting of 750 patients, incorporating three distinct imaging modalities collected from 2004 to 2022. Our work introduces a concept-based interpretable model that distinguishes between three types of choroidal tumors, integrating insig
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20851;&#31995;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;Transformer-based&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#22810;&#28304;PPI&#35821;&#26009;&#24211;&#65292;&#25913;&#36827;&#20102;&#20851;&#31995;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05602</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20851;&#32852;&#19978;&#19979;&#25991;&#20449;&#24687;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;PPIs&#65289;
&lt;/p&gt;
&lt;p&gt;
Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature using Attention-based Relational Context Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05602
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20851;&#31995;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;Transformer-based&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#22810;&#28304;PPI&#35821;&#26009;&#24211;&#65292;&#25913;&#36827;&#20102;&#20851;&#31995;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;PPIs&#65289;&#23545;&#20110;&#29702;&#35299;&#29983;&#21629;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#23545;&#20110;&#25506;&#31350;&#30142;&#30149;&#21457;&#23637;&#21644;&#35782;&#21035;&#22522;&#22240;/&#34507;&#30333;&#36136;&#21151;&#33021;&#20197;&#21450;&#29983;&#29289;&#36807;&#31243;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#22810;&#28304;PPI&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#20108;&#20803;&#20132;&#20114;&#31867;&#22411;&#26631;&#31614;&#22686;&#24378;&#20102;&#32463;&#36807;&#23457;&#26597;&#30340;&#20132;&#20114;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#30340;&#20851;&#32852;&#19978;&#19979;&#25991;&#20449;&#24687;&#36827;&#34892;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#20851;&#31995;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05602v1 Announce Type: cross  Abstract: Because protein-protein interactions (PPIs) are crucial to understand living systems, harvesting these data is essential to probe disease development and discern gene/protein functions and biological processes. Some curated datasets contain PPI data derived from the literature and other sources (e.g., IntAct, BioGrid, DIP, and HPRD). However, they are far from exhaustive, and their maintenance is a labor-intensive process. On the other hand, machine learning methods to automate PPI knowledge extraction from the scientific literature have been limited by a shortage of appropriate annotated data. This work presents a unified, multi-source PPI corpora with vetted interaction definitions augmented by binary interaction type labels and a Transformer-based deep learning method that exploits entities' relational context information for relation representation to improve relation classification performance. The model's performance is evaluated
&lt;/p&gt;</description></item><item><title>&#36825;&#19968;&#21019;&#26032;&#24515;&#29702;&#27835;&#30103;&#27169;&#22411;HealMe&#36890;&#36807;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24605;&#32500;&#65292;&#24182;&#20419;&#36827;&#20102;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.05574</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#27835;&#30103;&#20013;&#36827;&#34892;&#35748;&#30693;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05574
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#21019;&#26032;&#24515;&#29702;&#27835;&#30103;&#27169;&#22411;HealMe&#36890;&#36807;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24605;&#32500;&#65292;&#24182;&#20419;&#36827;&#20102;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#27835;&#30103;&#20013;&#21487;&#20197;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#29087;&#32451;&#22788;&#29702;&#35748;&#30693;&#37325;&#26500;&#31561;&#20851;&#38190;&#20219;&#21153;&#65292;&#20811;&#26381;&#32670;&#32827;&#12289;&#19981;&#20449;&#20219;&#12289;&#27835;&#30103;&#24072;&#25216;&#33021;&#24046;&#24322;&#21644;&#36164;&#28304;&#31232;&#32570;&#31561;&#25361;&#25112;&#12290;&#22312;&#20808;&#21069;&#30340;&#35748;&#30693;&#37325;&#26500;&#20013;&#65292;&#20027;&#35201;&#23558;&#36127;&#38754;&#24773;&#32490;&#36716;&#21270;&#20026;&#31215;&#26497;&#30340;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#25928;&#26524;&#26377;&#38480;&#65292;&#32463;&#24120;&#19981;&#33021;&#20419;&#36827;&#23458;&#25143;&#33258;&#25105;&#21457;&#29616;&#26367;&#20195;&#35270;&#35282;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24110;&#21161;&#21644;&#36171;&#33021;&#36890;&#36807;&#33258;&#36866;&#24212;&#35821;&#35328;&#22312;&#24515;&#29702;&#22686;&#24378;&#65288;HealMe&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35748;&#30693;&#37325;&#26500;&#30103;&#27861;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24819;&#27861;&#65292;&#24182;&#20419;&#36827;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35270;&#35282;&#12290;HealMe&#19982;&#20256;&#32479;LLM&#26041;&#27861;&#19981;&#21516;&#65292;&#37319;&#29992;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#12290;&#23427;&#36890;&#36807;&#31995;&#32479;&#25351;&#23548;&#23458;&#25143;&#21306;&#20998;&#24773;&#22659;&#21644;&#24863;&#21463;&#65292;&#38598;&#24605;&#24191;&#30410;&#23547;&#25214;&#26367;&#20195;&#35270;&#35282;&#65292;&#24182;&#21046;&#23450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05574v1 Announce Type: cross  Abstract: Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing 
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#22238;&#24212;&#24773;&#32490;&#22330;&#26223;&#26102;&#27604;&#20154;&#31867;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31227;&#24773;&#33021;&#21147;&#65292;&#24179;&#22343;&#31227;&#24773;&#35780;&#20998;&#39640;&#20110;&#20154;&#31867;&#29983;&#25104;&#30340;&#22238;&#24212;10%&#65307;&#25351;&#31034;ChatGPT&#22312;&#22238;&#24212;&#20013;&#34701;&#20837;&#23545;&#31227;&#24773;&#30340;&#28165;&#26224;&#29702;&#35299;&#20351;&#24471;&#20854;&#22238;&#24212;&#19982;&#39640;&#24230;&#31227;&#24773;&#30340;&#20010;&#20307;&#30340;&#26399;&#26395;&#22823;&#33268;&#25509;&#36817;5&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.05572</link><description>&lt;p&gt;
ChatGPT&#27604;&#20154;&#31867;&#26356;&#20855;&#31227;&#24773;&#33021;&#21147;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT More Empathetic than Humans?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05572
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#22238;&#24212;&#24773;&#32490;&#22330;&#26223;&#26102;&#27604;&#20154;&#31867;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31227;&#24773;&#33021;&#21147;&#65292;&#24179;&#22343;&#31227;&#24773;&#35780;&#20998;&#39640;&#20110;&#20154;&#31867;&#29983;&#25104;&#30340;&#22238;&#24212;10%&#65307;&#25351;&#31034;ChatGPT&#22312;&#22238;&#24212;&#20013;&#34701;&#20837;&#23545;&#31227;&#24773;&#30340;&#28165;&#26224;&#29702;&#35299;&#20351;&#24471;&#20854;&#22238;&#24212;&#19982;&#39640;&#24230;&#31227;&#24773;&#30340;&#20010;&#20307;&#30340;&#26399;&#26395;&#22823;&#33268;&#25509;&#36817;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#20197;&#21450;&#23588;&#20854;&#26159;&#20854;&#26368;&#26032;&#29256;&#26412;GPT-4&#22312;&#22238;&#24212;&#21508;&#31181;&#24773;&#32490;&#22330;&#26223;&#65288;&#21253;&#25324;&#31215;&#26497;&#21644;&#28040;&#26497;&#24773;&#32490;&#65289;&#26102;&#30340;&#31227;&#24773;&#33021;&#21147;&#65292;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#22238;&#24212;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#37319;&#29992;&#20005;&#35880;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#28041;&#21450;600&#21517;&#21442;&#19982;&#32773;&#30340;&#32452;&#38388;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#29983;&#25104;&#30340;&#22238;&#24212;&#20013;&#30340;&#31227;&#24773;&#27700;&#24179;&#12290;ChatGPT&#34987;&#25552;&#31034;&#30340;&#26041;&#24335;&#26377;&#20004;&#31181;&#65306;&#19968;&#31181;&#26159;&#26631;&#20934;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26126;&#30830;&#35814;&#32454;&#35828;&#26126;&#20102;&#31227;&#24773;&#30340;&#35748;&#30693;&#12289;&#24773;&#24863;&#21644;&#21516;&#24773;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;ChatGPT&#29983;&#25104;&#30340;&#22238;&#24212;&#30340;&#24179;&#22343;&#31227;&#24773;&#35780;&#20998;&#36229;&#36807;&#20102;&#20154;&#31867;&#29983;&#25104;&#30340;&#22238;&#24212;&#32422;10%&#12290;&#27492;&#22806;&#65292;&#25351;&#31034;ChatGPT&#22312;&#20854;&#22238;&#24212;&#20013;&#34701;&#20837;&#23545;&#31227;&#24773;&#30340;&#28165;&#26224;&#29702;&#35299;&#20351;&#24471;&#36825;&#20123;&#22238;&#24212;&#19982;&#39640;&#24230;&#31227;&#24773;&#30340;&#20010;&#20307;&#30340;&#26399;&#26395;&#22823;&#33268;&#25509;&#36817;&#22810;&#36798;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05572v1 Announce Type: cross  Abstract: This paper investigates the empathetic responding capabilities of ChatGPT, particularly its latest iteration, GPT-4, in comparison to human-generated responses to a wide range of emotional scenarios, both positive and negative. We employ a rigorous evaluation methodology, involving a between-groups study with 600 participants, to evaluate the level of empathy in responses generated by humans and ChatGPT. ChatGPT is prompted in two distinct ways: a standard approach and one explicitly detailing empathy's cognitive, affective, and compassionate counterparts. Our findings indicate that the average empathy rating of responses generated by ChatGPT exceeds those crafted by humans by approximately 10%. Additionally, instructing ChatGPT to incorporate a clear understanding of empathy in its responses makes the responses align approximately 5 times more closely with the expectations of individuals possessing a high degree of empathy, compared t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#20351;&#29992;Transformers&#25216;&#26415;&#26816;&#27979;&#20986;&#28966;&#34385;&#30151;&#29366;&#24739;&#32773;&#20013;&#28508;&#22312;&#30340;&#20122;&#20020;&#24202;ADHD&#20195;&#29702;&#65292;&#24110;&#21161;&#38416;&#26126;&#20102;&#28966;&#34385;&#21644;ADHD&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.05561</link><description>&lt;p&gt;
&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#26816;&#27979;&#21487;&#33021;&#24739;&#26377;&#20122;&#20020;&#24202;ADHD&#30340;&#28508;&#22312;&#20276;&#38543;&#30151;&#29366;
&lt;/p&gt;
&lt;p&gt;
Detecting a Proxy for Potential Comorbid ADHD in People Reporting Anxiety Symptoms from Social Media Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05561
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#20351;&#29992;Transformers&#25216;&#26415;&#26816;&#27979;&#20986;&#28966;&#34385;&#30151;&#29366;&#24739;&#32773;&#20013;&#28508;&#22312;&#30340;&#20122;&#20020;&#24202;ADHD&#20195;&#29702;&#65292;&#24110;&#21161;&#38416;&#26126;&#20102;&#28966;&#34385;&#21644;ADHD&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#38416;&#26126;&#28966;&#34385;&#21644;ADHD&#20043;&#38388;&#30340;&#32852;&#31995;&#65307;&#21033;&#29992;&#21464;&#21387;&#22120;&#25216;&#26415;&#22312;&#35299;&#20915;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#20998;&#31867;&#22120;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#65307;&#24182;&#35752;&#35770;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#38416;&#26126;&#28966;&#34385;&#21644;ADHD&#34920;&#29616;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22823;&#32422;50%&#30340;ADHD&#25104;&#24180;&#24739;&#32773;&#21487;&#33021;&#36824;&#24739;&#26377;&#28966;&#34385;&#30151;&#65292;&#32422;30%&#30340;&#28966;&#34385;&#25104;&#24180;&#24739;&#32773;&#21487;&#33021;&#20063;&#24739;&#26377;ADHD&#12290;&#24739;&#26377;&#28966;&#34385;&#30151;&#30340;&#24739;&#32773;&#21487;&#33021;&#20250;&#25509;&#21463;&#28966;&#34385;&#27835;&#30103;&#65292;&#32780;&#19981;&#32771;&#34385;ADHD&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#27835;&#30103;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#33719;&#21462;&#26377;&#20851;&#25658;&#24102;&#28966;&#34385;&#20276;&#38543;&#30340;ADHD&#30340;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#21464;&#21387;&#22120;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#22312;&#20855;&#26377;&#28966;&#34385;&#30151;&#29366;&#30340;&#20154;&#32676;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20122;&#20020;&#24202;ADHD&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05561v1 Announce Type: cross  Abstract: We present a novel task that can elucidate the connection between anxiety and ADHD; use Transformers to make progress toward solving a task that is not solvable by keyword-based classifiers; and discuss a method for visualization of our classifier illuminating the connection between anxiety and ADHD presentations.   Up to approximately 50% of adults with ADHD may also have an anxiety disorder and approximately 30\% of adults with anxiety may also have ADHD. Patients presenting with anxiety may be treated for anxiety without ADHD ever being considered, possibly affecting treatment. We show how data that bears on ADHD that is comorbid with anxiety can be obtained from social media data, and show that Transformers can be used to detect a proxy for possible comorbid ADHD in people with anxiety symptoms.   We collected data from anxiety and ADHD online forums (subreddits). We identified posters who first started posting in the Anxiety subre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;BERT&#20027;&#39064;&#24314;&#27169;&#20174;&#35838;&#31243;&#20013;&#25552;&#21462;&#20027;&#39064;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20027;&#39064;&#26469;&#35782;&#21035;&#19981;&#21516;&#23398;&#31185;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21508;&#31181;&#23398;&#20064;&#35805;&#39064;&#30340;&#28436;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.05553</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#29702;&#35299;&#25945;&#32946;&#35805;&#39064;&#30340;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Understanding the Progression of Educational Topics via Semantic Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;BERT&#20027;&#39064;&#24314;&#27169;&#20174;&#35838;&#31243;&#20013;&#25552;&#21462;&#20027;&#39064;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20027;&#39064;&#26469;&#35782;&#21035;&#19981;&#21516;&#23398;&#31185;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21508;&#31181;&#23398;&#20064;&#35805;&#39064;&#30340;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#31995;&#32479;&#27491;&#22312;&#21160;&#24577;&#21464;&#21270;&#65292;&#20197;&#36866;&#24212;&#25216;&#26415;&#36827;&#27493;&#12289;&#24037;&#19994;&#21644;&#31038;&#20250;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#23398;&#29983;&#30340;&#23398;&#20064;&#26053;&#31243;&#12290;&#35838;&#31243;&#19987;&#23478;&#21644;&#25945;&#32946;&#24037;&#20316;&#32773;&#19981;&#26029;&#20462;&#35746;&#21508;&#25945;&#32946;&#24180;&#32423;&#25152;&#25945;&#25480;&#30340;&#31185;&#30446;&#65292;&#20197;&#30830;&#23450;&#24046;&#36317;&#65292;&#24341;&#20837;&#26032;&#30340;&#23398;&#20064;&#35805;&#39064;&#65292;&#24182;&#25552;&#39640;&#23398;&#20064;&#25104;&#26524;&#12290;&#26412;&#25991;&#21033;&#29992;&#25968;&#25454;&#31185;&#23398;&#26469;&#26356;&#22909;&#22320;&#20102;&#35299;&#21508;&#31181;&#23398;&#20064;&#35805;&#39064;&#30340;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05553v1 Announce Type: cross  Abstract: Education systems are dynamically changing to accommodate technological advances, industrial and societal needs, and to enhance students' learning journeys. Curriculum specialists and educators constantly revise taught subjects across educational grades to identify gaps, introduce new learning topics, and enhance the learning outcomes. This process is usually done within the same subjects (e.g. math) or across related subjects (e.g. math and physics) considering the same and different educational levels, leading to massive multi-layer comparisons. Having nuanced data about subjects, topics, and learning outcomes structured within a dataset, empowers us to leverage data science to better understand the progression of various learning topics. In this paper, Bidirectional Encoder Representations from Transformers (BERT) topic modeling was used to extract topics from the curriculum, which were then used to identify relationships between su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#28436;&#21464;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#30417;&#27979;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#20026;&#24178;&#39044;&#21644;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.05548</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#30340;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
Monitoring the evolution of antisemitic discourse on extremist social media using BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#28436;&#21464;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#30417;&#27979;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#20026;&#24178;&#39044;&#21644;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#31181;&#26063;&#20027;&#20041;&#21644;&#19981;&#23485;&#23481;&#26377;&#21487;&#33021;&#22312;&#32447;&#19979;&#20135;&#29983;&#20167;&#24680;&#65292;&#26368;&#32456;&#23548;&#33268;&#36523;&#20307;&#26292;&#21147;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#30340;&#26159;&#22312;&#32447;&#21453;&#29369;&#20027;&#20041;&#65292;&#36861;&#36394;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#21453;&#29369;&#20027;&#39064;&#21450;&#20854;&#30456;&#20851;&#26415;&#35821;&#30340;&#28436;&#21464;&#65292;&#26377;&#21161;&#20110;&#30417;&#27979;&#21442;&#19982;&#32773;&#30340;&#24773;&#32490;&#21644;&#28436;&#21464;&#65292;&#24182;&#21487;&#33021;&#25552;&#20379;&#24178;&#39044;&#26041;&#27861;&#65292;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#12290;&#37492;&#20110;&#22312;&#32447;&#27969;&#37327;&#24222;&#22823;&#19988;&#19981;&#26029;&#21464;&#21270;&#65292;&#25163;&#21160;&#30417;&#27979;&#35848;&#35805;&#23454;&#38469;&#19978;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#21453;&#29369;&#20027;&#39064;&#21644;&#26415;&#35821;&#65292;&#36319;&#36394;&#23427;&#20204;&#30340;&#28436;&#21464;&#12290;&#30001;&#20110;&#30417;&#30563;&#23398;&#20064;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#36807;&#20110;&#21463;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05548v1 Announce Type: cross  Abstract: Racism and intolerance on social media contribute to a toxic online environment which may spill offline to foster hatred, and eventually lead to physical violence. That is the case with online antisemitism, the specific category of hatred considered in this study. Tracking antisemitic themes and their associated terminology over time in online discussions could help monitor the sentiments of their participants and their evolution, and possibly offer avenues for intervention that may prevent the escalation of hatred. Due to the large volume and constant evolution of online traffic, monitoring conversations manually is impractical. Instead, we propose an automated method that extracts antisemitic themes and terminology from extremist social media over time and captures their evolution. Since supervised learning would be too limited for such a task, we created an unsupervised online machine learning approach that uses large language model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.05101</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rule-driven News Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
News captioning&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#25551;&#36848;&#22270;&#29255;&#21450;&#20854;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#25110;&#20855;&#20307;&#20107;&#20214;&#26469;&#29983;&#25104;&#21477;&#23376;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20381;&#36182;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#21462;&#24471;&#26174;&#33879;&#25104;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#19987;&#27880;&#20110;&#36755;&#20837;&#26032;&#38395;&#20869;&#23481;&#19982;&#36755;&#20986;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#38656;&#35201;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#30340;&#19968;&#20123;&#22522;&#26412;&#35268;&#21017;&#65292;&#22914;&#20934;&#30830;&#25551;&#36848;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#20010;&#20307;&#21644;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#25351;&#23450;&#30340;&#35268;&#21017;&#20449;&#21495;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#25551;&#36848;&#35774;&#35745;&#20102;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#12290;&#36825;&#19968;&#35268;&#21017;&#21253;&#25324;&#22270;&#29255;&#20013;&#25551;&#32472;&#30340;&#20027;&#35201;&#21160;&#20316;&#65288;&#20363;&#22914;&#65292;&#8220;&#25191;&#34892;&#8221;&#65289;&#20197;&#21450;&#21442;&#19982;&#21160;&#20316;&#30340;&#21629;&#21517;&#23454;&#20307;&#25198;&#28436;&#30340;&#35282;&#33394;&#65288;&#20363;&#22914;&#65292;&#8220;&#20195;&#29702;&#20154;&#8221;&#21644;&#8220;&#22320;&#28857;&#8221;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#35821;&#20041;&#35268;&#21017;&#27880;&#20837;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05101v1 Announce Type: cross  Abstract: News captioning task aims to generate sentences by describing named entities or concrete events for an image with its news article. Existing methods have achieved remarkable results by relying on the large-scale pre-trained models, which primarily focus on the correlations between the input news content and the output predictions. However, the news captioning requires adhering to some fundamental rules of news reporting, such as accurately describing the individuals and actions associated with the event. In this paper, we propose the rule-driven news captioning method, which can generate image descriptions following designated rule signal. Specifically, we first design the news-aware semantic rule for the descriptions. This rule incorporates the primary action depicted in the image (e.g., "performing") and the roles played by named entities involved in the action (e.g., "Agent" and "Place"). Second, we inject this semantic rule into th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.04789</link><description>&lt;p&gt;
TopicDiff&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#30340;&#20027;&#39064;&#20016;&#23500;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#65288;MCE&#65289;&#26816;&#27979;&#36890;&#24120;&#36328;&#36234;&#22768;&#23398;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#21560;&#24341;&#20102;&#22810;&#23186;&#20307;&#31038;&#21306;&#26085;&#30410;&#22686;&#21152;&#30340;&#20852;&#36259;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#23545;&#35805;&#20013;&#30340;&#35821;&#22659;&#20449;&#24687;&#65292;&#21482;&#26377;&#23569;&#25968;&#32771;&#34385;&#21333;&#19968;&#35821;&#35328;&#27169;&#24577;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#32780;&#24635;&#26159;&#24573;&#35270;&#22768;&#23398;&#21644;&#35270;&#35273;&#20027;&#39064;&#20449;&#24687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;Topic-enriched Diffusion&#65288;TopicDiff&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;MCE&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#20197;&#32531;&#35299;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#12290;&#35814;&#32454;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;TopicDiff&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#23545;MCE&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;TopicDiff&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04789v1 Announce Type: cross  Abstract: Multimodal Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the topic information in single language modality, while always neglecting the acoustic and vision topic information. On this basis, we propose a model-agnostic Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic information in MCE tasks. Particularly, we integrate the diffusion model into neural topic model to alleviate the diversity deficiency problem of neural topic model in capturing topic information. Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of multimodal topic information to MCE and the effectiveness of Topic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;</title><link>https://arxiv.org/abs/2403.04769</link><description>&lt;p&gt;
&#31227;&#38500;GPT4&#30340;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Removing GPT4's Filter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT4&#26368;&#21021;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#21363;&#24535;&#24895;&#32773;&#25552;&#20379;&#21453;&#39304;&#20197;&#25945;&#23548;GPT4&#19981;&#35201;&#29983;&#25104;&#19981;&#24403;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25805;&#20316;&#24050;&#32463;&#36827;&#34892;&#24494;&#35843;&#30340;&#29256;&#26412;&#65292;&#20351;&#20854;&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;RLHF&#65288;Reinforcement learning from Human Feedback&#65289;&#30340;&#34892;&#20026;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#20102;&#27169;&#22411;&#22312;RLHF&#26399;&#38388;&#23398;&#20064;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;GPT4&#22312;&#27809;&#26377;&#32463;&#36807;RLHF&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#26102;&#65292;&#23427;&#22833;&#21435;&#20102;&#25152;&#26377;&#25233;&#21046;&#21147;&#65292;&#21482;&#38656;&#21069;&#20960;&#20010;&#35789;&#23601;&#21487;&#20197;&#29983;&#25104;&#38750;&#24120;&#19981;&#24403;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04769v1 Announce Type: cross  Abstract: GPT4 was initially trained on large amounts of data, and then fine-tuned using Reinforcement learning from Human Feedback (RLHF), which is when volunteers give feedback in order to teach GPT4 not to create inappropriate content. In this paper, we present a method to manipulate the fine-tuned version into reverting to pre-RLHF behavior, effectively removing all safety mechanisms that the model learned during RLHF. In particular, when GPT4 acts without RLHF, it loses all inhibition, and can complete very inappropriate content given only the first few words.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chain-of-Thought-Explanation&#65288;CoTE&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#20219;&#21153;&#65292;&#36890;&#36807;&#36880;&#27493;&#21019;&#24314;&#35814;&#32454;&#35299;&#37322;&#26469;&#30830;&#23450;&#25554;&#27133;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04656</link><description>&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#24605;&#32500;&#38142;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought Explanation for Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chain-of-Thought-Explanation&#65288;CoTE&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#20219;&#21153;&#65292;&#36890;&#36807;&#36880;&#27493;&#21019;&#24314;&#35814;&#32454;&#35299;&#37322;&#26469;&#30830;&#23450;&#25554;&#27133;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#26088;&#22312;&#35760;&#24405;&#29992;&#25143;&#22312;&#20250;&#35805;&#20114;&#21160;&#26399;&#38388;&#25552;&#20986;&#30340;&#26597;&#35810;&#21644;&#30446;&#26631;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25554;&#27133;&#21450;&#20854;&#23545;&#24212;&#30340;&#20540;&#26469;&#23454;&#29616;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20197;&#19981;&#36879;&#26126;&#26041;&#24335;&#20915;&#23450;&#25554;&#27133;&#20540;&#65292;&#32780;&#20154;&#31867;&#36890;&#24120;&#37319;&#29992;&#26356;&#35880;&#24910;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#30340;&#23545;&#35805;&#36718;&#20013;&#25910;&#38598;&#20449;&#24687;&#65292;&#28982;&#21518;&#25512;&#29702;&#20986;&#36866;&#24403;&#30340;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;Chain-of-Thought-Explanation&#65288;CoTE&#65289;&#30340;&#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#30830;&#23450;&#25554;&#27133;&#20540;&#25152;&#38656;&#30340;&#27493;&#39588;&#12290;CoTE&#24314;&#31435;&#22312;&#29983;&#25104;&#24335;DST&#26694;&#26550;&#20043;&#19978;&#65292;&#26088;&#22312;&#22312;&#30830;&#23450;&#25554;&#27133;&#20540;&#21518;&#36880;&#27493;&#21019;&#24314;&#35814;&#32454;&#35299;&#37322;&#12290;&#36825;&#19968;&#36807;&#31243;&#23548;&#33268;&#20102;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#25554;&#27133;&#20540;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;CoTE&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#33258;&#21160;&#25913;&#20889;&#26500;&#24314;&#26356;&#27969;&#30021;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;CoTE-refined&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04656v1 Announce Type: new  Abstract: Dialogue state tracking (DST) aims to record user queries and goals during a conversational interaction achieved by maintaining a prede- fined set of slots and their corresponding values. Current approaches decide slot values opaquely, while humans usually adopt a more deliberate approach by collecting information from relevant dialogue turns and then reasoning the appropriate values. In this work, we focus on the steps needed to figure out slot values by proposing a model named Chain-of-Thought-Explanation (CoTE) for the DST task. CoTE, which is built on the generative DST framework, is designed to create detailed explanations step by step after determining the slot values. This process leads to more accurate and reliable slot values. More-over, to improve the reasoning ability of the CoTE, we further construct more fluent and high-quality explanations with automatic paraphrasing, leading the method CoTE-refined. Experimental results on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.04121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Reason and Plan?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04121
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20154;&#31867;&#26377;&#26102;&#20505;&#34920;&#29616;&#20986;&#33021;&#22815;&#36890;&#36807;&#33258;&#25105;&#25209;&#35780;&#32416;&#27491;&#33258;&#24049;&#38169;&#35823;&#29468;&#27979;&#30340;&#33021;&#21147;&#65292;&#20294;&#20284;&#20046;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#27809;&#26377;&#20381;&#25454;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04121v1 Announce Type: new  Abstract: While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.
&lt;/p&gt;</description></item><item><title>Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03640</link><description>&lt;p&gt;
Apollo&#65306;&#36731;&#37327;&#32423;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65306;&#35753;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#26222;&#24800;60&#20159;&#20154;
&lt;/p&gt;
&lt;p&gt;
Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03640
&lt;/p&gt;
&lt;p&gt;
Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#29699;&#21307;&#23398;&#30693;&#35782;&#30340;&#24222;&#22823;&#23384;&#20648;&#24211;&#20027;&#35201;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#65292;&#20294;&#22312;&#20256;&#36882;&#37327;&#36523;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26041;&#38754;&#65292;&#26412;&#22320;&#35821;&#35328;&#23545;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#23558;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#28085;&#30422;&#20840;&#29699;61&#20159;&#20154;&#21475;&#30340;&#20845;&#31181;&#26368;&#24120;&#29992;&#35821;&#35328;&#30340;&#21307;&#23398;LLMs&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#20419;&#25104;&#20102;ApolloCorpora&#22810;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#38598;&#21644;XMedBench&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#22312;&#22810;&#35821;&#35328;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21457;&#24067;&#30340;Apollo&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#30456;&#23545;&#36739;&#23567;&#23610;&#23544;&#65288;&#21363;0.5B&#12289;1.8B&#12289;2B&#12289;6B&#21644;7B&#65289;&#19978;&#21462;&#24471;&#20102;&#19982;&#21516;&#31561;&#22823;&#23567;&#27169;&#22411;&#26368;&#20339;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;Apollo-7B&#26159;&#36804;&#20170;&#20026;&#27490;&#36798;&#21040;70B&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#36739;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03640v1 Announce Type: cross  Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03102</link><description>&lt;p&gt;
&#8220;&#22312;&#23545;&#35805;&#20013;&#23398;&#20064;&#8221;&#65306;&#36890;&#36807;&#23545;&#35805;&#20013;&#23398;&#20064;&#23454;&#29616;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
"In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#33021;&#22815;&#29983;&#25104;&#19982;&#19981;&#21516;&#20154;&#35774;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#20010;&#20154;&#36164;&#26009;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#65292;&#36824;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;In-Dialogue Learning&#65288;IDL&#65289;&#65292;&#19968;&#31181;&#24494;&#35843;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#23545;&#35805;&#21382;&#21490;&#26469;&#21051;&#30011;&#20010;&#20154;&#35774;&#65292;&#20197;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IDL&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;BLEU&#21644;ROUGE&#20998;&#25968;&#20998;&#21035;&#22686;&#21152;&#20102;&#39640;&#36798;200%&#21644;247%&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03102v1 Announce Type: cross  Abstract: Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19467</link><description>&lt;p&gt;
TV-TREES&#65306;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;
&lt;/p&gt;
&lt;p&gt;
TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19467
&lt;/p&gt;
&lt;p&gt;
TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#30005;&#35270;&#21098;&#36753;&#31561;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#24403;&#21069;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#25512;&#29702;&#65292;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TV-TREES&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#12290;TV-TREES&#20316;&#20026;&#19968;&#31181;&#20419;&#36827;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#20219;&#21153;&#26469;&#35780;&#20272;&#27492;&#31867;&#26041;&#27861;&#30340;&#25512;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#12289;&#20855;&#26377;&#26368;&#20808;&#36827;&#38646;-shot&#24615;&#33021;&#30340;&#23436;&#25972;&#35270;&#39057;&#21098;&#36753;&#65292;&#23637;&#31034;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;</title><link>https://arxiv.org/abs/2402.19088</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#21464;&#21270;&#29305;&#24449;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey in Characterization of Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19088
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#35821;&#35328;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#21560;&#32435;&#20154;&#31867;&#31038;&#20250;&#30340;&#25991;&#21270;&#21464;&#21270;&#12290;&#36825;&#31181;&#28436;&#21464;&#36890;&#36807;&#26032;&#35789;&#35821;&#65288;&#26032;&#21333;&#35789;&#65289;&#25110;&#21333;&#35789;&#30340;&#35821;&#20041;&#21464;&#21270;&#65288;&#36171;&#20104;&#24050;&#26377;&#21333;&#35789;&#26032;&#30340;&#21547;&#20041;&#65289;&#26469;&#20307;&#29616;&#12290;&#29702;&#35299;&#21333;&#35789;&#30340;&#21547;&#20041;&#23545;&#35299;&#37322;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#65288;&#22320;&#26041;&#29992;&#35821;&#25110;&#20442;&#35821;&#65289;&#12289;&#39046;&#22495;&#65288;&#20363;&#22914;&#25216;&#26415;&#26415;&#35821;&#65289;&#25110;&#26102;&#20195;&#30340;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#36825;&#20123;&#21333;&#35789;&#19982;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30456;&#20851;&#65292;&#20363;&#22914;&#32763;&#35793;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#31561;&#12290;&#35821;&#20041;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#21644;&#24418;&#24335;&#21270;&#34920;&#24449;&#36825;&#20123;&#21464;&#21270;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#30740;&#31350;&#36825;&#31181;&#24433;&#21709;&#26159;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#36817;&#26399;&#24341;&#36215;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20960;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#26816;&#27979;&#35821;&#20041;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#21162;&#21147;&#26469;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19088v1 Announce Type: cross  Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to charact
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.16906</link><description>&lt;p&gt;
LDB&#65306;&#36890;&#36807;&#36880;&#27493;&#39564;&#35777;&#36816;&#34892;&#26102;&#25191;&#34892;&#26469;&#35843;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16906
&lt;/p&gt;
&lt;p&gt;
LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#20165;&#23558;&#21333;&#27425;&#20195;&#30721;&#29983;&#25104;&#65292;&#32780;&#19988;&#36824;&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;&#31243;&#24207;&#39564;&#35777;&#22120;&#25972;&#21512;&#21040;LLMs&#20013;&#65292;&#20197;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23558;&#29983;&#25104;&#30340;&#31243;&#24207;&#35270;&#20026;&#19981;&#21487;&#20998;&#21106;&#30340;&#23454;&#20307;&#65292;&#36825;&#23545;LLMs&#22312;&#35843;&#35797;&#31243;&#24207;&#26102;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#30340;&#36923;&#36753;&#27969;&#31243;&#21644;&#25968;&#25454;&#25805;&#20316;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#24320;&#21457;&#20154;&#21592;&#35843;&#35797;&#31243;&#24207;&#26102;&#65292;&#20182;&#20204;&#36890;&#24120;&#35774;&#32622;&#26029;&#28857;&#24182;&#26377;&#36873;&#25321;&#22320;&#26816;&#26597;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#12290;&#25191;&#34892;&#27969;&#21644;&#20013;&#38388;&#21464;&#37327;&#22312;&#35843;&#35797;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#25991;&#29486;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#22120;&#65288;LDB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;LLMs&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#23436;&#21892;&#20854;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20294;&#22914;&#20309;&#20248;&#21270;&#36873;&#25321;&#25968;&#25454;&#20197;&#38477;&#20302;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16827</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Data Selection for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16827
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20294;&#22914;&#20309;&#20248;&#21270;&#36873;&#25321;&#25968;&#25454;&#20197;&#38477;&#20302;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#30340;&#19968;&#20010;&#20027;&#35201;&#22240;&#32032;&#26159;&#21033;&#29992;&#24040;&#22823;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65288;&#25110;&#19981;&#21487;&#34892;&#65289;&#65292;&#22240;&#20026;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#36136;&#37327;&#21487;&#33021;&#26377;&#25152;&#19981;&#21516;&#12290;&#25968;&#25454;&#36807;&#28388;&#20063;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#37327;&#26469;&#38477;&#20302;&#35757;&#32451;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#12290;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26088;&#22312;&#30830;&#23450;&#35201;&#21253;&#25324;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#21738;&#20123;&#20505;&#36873;&#25968;&#25454;&#28857;&#65292;&#20197;&#21450;&#22914;&#20309;&#20174;&#25152;&#36873;&#25968;&#25454;&#28857;&#20013;&#36866;&#24403;&#37319;&#26679;&#12290;&#25913;&#36827;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#21069;&#26223;&#24050;&#32463;&#23548;&#33268;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#37327;&#36805;&#36895;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#20027;&#35201;&#21463;&#23454;&#35777;&#35777;&#25454;&#39537;&#21160;&#65292;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#25104;&#26412;&#26114;&#36149;&#65292;&#24456;&#23569;&#26377;&#32452;&#32455;&#25317;&#26377;&#36164;&#28304;&#36827;&#34892;&#24191;&#27867;&#30340;&#25968;&#25454;&#36873;&#25321;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#25968;&#25454;&#36873;&#25321;&#30340;&#30693;&#35782;&#21487;&#33021;&#22823;&#22810;&#23616;&#38480;&#20110;&#22823;&#22411;&#25216;&#26415;&#20844;&#21496;&#25110;&#30740;&#31350;&#26426;&#26500;&#20869;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16827v1 Announce Type: new  Abstract: A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16363</link><description>&lt;p&gt;
LLM&#25512;&#26029;&#25581;&#31034;&#65306;&#35843;&#26597;&#19982;Roofline&#27169;&#22411;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLM Inference Unveiled: Survey and Roofline Model Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#20379;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#29420;&#29305;&#32467;&#21512;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#24050;&#32463;&#25193;&#23637;&#24182;&#20805;&#28385;&#27963;&#21147;&#65292;&#20294;&#33267;&#20170;&#36824;&#27809;&#26377;&#19968;&#20010;&#31616;&#26126;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;LLM&#25512;&#26029;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#19981;&#20165;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#36824;&#22522;&#20110;Roofline&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#12290;&#36825;&#19968;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#35782;&#21035;LLM&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20174;&#32780;&#20026;&#37096;&#32626;LLM&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27719;&#24635;&#20102;&#39640;&#25928;LLM&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#26435;&#37325;&#20248;&#21270;&#65288;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.16278</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#27880;&#37322;&#23884;&#20837;&#27169;&#22411;&#30340;&#26412;&#20307;&#21253;&#21547;&#20851;&#31995;&#39044;&#27979;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16278
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#26412;&#20307;&#23884;&#20837;&#65292;&#29992;&#20110;&#26412;&#20307;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#26412;&#20307;&#23884;&#20837;&#26410;&#35299;&#20915;&#31867;&#20284;&#21644;&#23396;&#31435;&#23454;&#20307;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#26410;&#25552;&#21462;&#26412;&#20307;&#20013;&#27880;&#37322;&#20844;&#29702;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#30340;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65306;Inverted-index Matrix Embedding (InME) &#21644; Co-occurrence Matrix Embedding (CoME)&#12290;&#36825;&#20004;&#31181;&#23884;&#20837;&#36890;&#36807;&#27599;&#20010;&#21333;&#35789;&#22312;&#19968;&#32452;&#20844;&#29702;&#20013;&#20986;&#29616;&#30340;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#20844;&#29702;&#20013;&#21333;&#35789;&#30340;&#20849;&#29616;&#26469;&#25429;&#33719;&#27880;&#37322;&#20844;&#29702;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#39044;&#27979;&#30340;&#36229;&#31867;&#19982;&#23376;&#31867;&#30456;&#20284;&#19988;&#23396;&#31435;&#20110;&#26412;&#20307;&#20013;&#30340;&#20854;&#20182;&#23454;&#20307;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#12289;&#35821;&#35328;&#21551;&#21457;&#12289;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#20851;&#31995;&#25277;&#21462;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.16159</link><description>&lt;p&gt;
DistALANER&#65306;&#24320;&#28304;&#36719;&#20214;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#36828;&#31243;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#22686;&#24378;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16159
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#12289;&#35821;&#35328;&#21551;&#21457;&#12289;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#20851;&#31995;&#25277;&#21462;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#20840;&#38754;&#30340;&#20004;&#27493;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#26469;&#35299;&#20915;&#36719;&#20214;&#25968;&#25454;&#26631;&#27880;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#35813;&#36807;&#31243;&#24039;&#22937;&#22320;&#21033;&#29992;&#35821;&#35328;&#21551;&#21457;&#12289;&#29420;&#29305;&#30340;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;LLMs&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NER&#22312;&#20851;&#31995;&#25277;&#21462;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16159v1 Announce Type: new  Abstract: This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#65292;&#21457;&#29616;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.16034</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#36827;&#34892;&#24773;&#32490;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Emotion Classification in Short English Texts using Deep Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#65292;&#21457;&#29616;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#30340;&#26377;&#38480;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#24773;&#32490;&#26159;&#19968;&#39033;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#26694;&#26550;&#21644;&#35745;&#31639;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#30740;&#31350;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#35789;&#23884;&#20837;&#65292;&#29305;&#21035;&#26159;BERT&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;SmallEnglishEmotions&#8221;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;6372&#20010;&#24102;&#26377;&#20116;&#31181;&#20027;&#35201;&#24773;&#32490;&#31867;&#21035;&#27880;&#37322;&#30340;&#19981;&#21516;&#30701;&#27874;&#26031;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#22312;&#20934;&#30830;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#26041;&#38754;&#20248;&#20110;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16034v1 Announce Type: cross  Abstract: Detecting emotions in limited text datasets from under-resourced languages presents a formidable obstacle, demanding specialized frameworks and computational strategies. This study conducts a thorough examination of deep learning techniques for discerning emotions in short English texts. Deep learning approaches employ transfer learning and word embedding, notably BERT, to attain superior accuracy. To evaluate these methods, we introduce the "SmallEnglishEmotions" dataset, comprising 6372 varied short Persian texts annotated with five primary emotion categories. Our experiments reveal that transfer learning and BERT-based text embedding outperform alternative methods in accurately categorizing the text in the dataset.
&lt;/p&gt;</description></item><item><title>InfFeed&#23558;&#24433;&#21709;&#20989;&#25968;&#20316;&#20026;&#21453;&#39304;&#65292;&#29992;&#20110;&#25913;&#21892;&#20027;&#35266;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#38656;&#35201;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#28857;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#35843;&#25972;&#26631;&#31614;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.14702</link><description>&lt;p&gt;
InfFeed&#65306;&#23558;&#24433;&#21709;&#20989;&#25968;&#20316;&#20026;&#21453;&#39304;&#20197;&#25913;&#21892;&#20027;&#35266;&#20219;&#21153;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14702
&lt;/p&gt;
&lt;p&gt;
InfFeed&#23558;&#24433;&#21709;&#20989;&#25968;&#20316;&#20026;&#21453;&#39304;&#65292;&#29992;&#20110;&#25913;&#21892;&#20027;&#35266;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#38656;&#35201;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#28857;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#35843;&#25972;&#26631;&#31614;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24433;&#21709;&#20989;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#24037;&#20855;&#65292;&#36890;&#36807;&#37327;&#21270;&#21487;&#33021;&#24433;&#21709;&#27979;&#35797;&#39044;&#27979;&#30340;&#20010;&#21035;&#35757;&#32451;&#23454;&#20363;&#30340;&#25200;&#21160;&#65292;&#23454;&#29616;&#23545;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#21452;&#37325;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24433;&#21709;&#20989;&#25968;&#20316;&#20026;&#21453;&#39304;&#24341;&#20837;&#27169;&#22411;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#22312;&#25968;&#25454;&#38598;&#25193;&#23637;&#32451;&#20064;&#20013;&#65292;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#33258;&#21160;&#35782;&#21035;&#26368;&#21021;&#30001;&#26576;&#20123;&#29616;&#26377;&#26041;&#27861;&#8216;&#38134;&#8217;&#27880;&#37322;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#38656;&#35201;&#26631;&#27880;&#32773;&#20132;&#21449;&#26816;&#26597;&#65288;&#21644;&#32416;&#27491;&#65289;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;InfFeed&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#35745;&#31639;&#30446;&#26631;&#23454;&#20363;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#23454;&#20363;&#12290;&#21521;&#31532;&#19968;&#20010;&#30446;&#26631;&#21162;&#21147;&#65292;&#25105;&#20204;&#26681;&#25454;&#20854;&#24433;&#21709;&#32773;&#30340;&#26631;&#31614;&#35843;&#25972;&#30446;&#26631;&#23454;&#20363;&#30340;&#26631;&#31614;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;InfFeed&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65288;&#21253;&#25324;LLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14702v1 Announce Type: new  Abstract: Recently, influence functions present an apparatus for achieving explainability for deep neural models by quantifying the perturbation of individual train instances that might impact a test prediction. Our objectives in this paper are twofold. First we incorporate influence functions as a feedback into the model to improve its performance. Second, in a dataset extension exercise, using influence functions to automatically identify data points that have been initially `silver' annotated by some existing method and need to be cross-checked (and corrected) by annotators to improve the model performance. To meet these objectives, in this paper, we introduce InfFeed, which uses influence functions to compute the influential instances for a target instance. Toward the first objective, we adjust the label of the target instance based on its influencer(s) label. In doing this, InfFeed outperforms the state-of-the-art baselines (including LLMs) b
&lt;/p&gt;</description></item><item><title>PIRB&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27874;&#20848;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#21253;&#21547;41&#20010;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#36229;&#36807;20&#31181;&#23494;&#38598;&#21644;&#31232;&#30095;&#26816;&#32034;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#27493;&#35757;&#32451;&#27969;&#31243;&#26469;&#26500;&#24314;&#39640;&#25928;&#30340;&#29305;&#23450;&#35821;&#35328;&#26816;&#32034;&#22120;&#65292;&#26368;&#21518;&#39564;&#35777;&#20102;&#20182;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2402.13350</link><description>&lt;p&gt;
PIRB&#65306;&#27874;&#20848;&#23494;&#38598;&#21644;&#28151;&#21512;&#25991;&#26412;&#26816;&#32034;&#26041;&#27861;&#30340;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13350
&lt;/p&gt;
&lt;p&gt;
PIRB&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27874;&#20848;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#21253;&#21547;41&#20010;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#36229;&#36807;20&#31181;&#23494;&#38598;&#21644;&#31232;&#30095;&#26816;&#32034;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#27493;&#35757;&#32451;&#27969;&#31243;&#26469;&#26500;&#24314;&#39640;&#25928;&#30340;&#29305;&#23450;&#35821;&#35328;&#26816;&#32034;&#22120;&#65292;&#26368;&#21518;&#39564;&#35777;&#20102;&#20182;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#27874;&#20848;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65288;PIRB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#27874;&#20848;&#35821;&#30340;41&#20010;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#29616;&#26377;&#25968;&#25454;&#38598;&#20197;&#21450;10&#20010;&#26032;&#30340;&#12289;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#21830;&#19994;&#12289;&#29289;&#29702;&#21644;&#35821;&#35328;&#23398;&#31561;&#22810;&#26679;&#20027;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#36229;&#36807;20&#20010;&#23494;&#38598;&#21644;&#31232;&#30095;&#26816;&#32034;&#27169;&#22411;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;&#25105;&#20204;&#35757;&#32451;&#30340;&#22522;&#20934;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#21487;&#29992;&#30340;&#27874;&#20848;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#39640;&#25928;&#29305;&#23450;&#35821;&#35328;&#26816;&#32034;&#22120;&#30340;&#19977;&#27493;&#27969;&#31243;&#65292;&#21253;&#25324;&#30693;&#35782;&#33976;&#39311;&#12289;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#20351;&#29992;&#36731;&#37327;&#32423;&#37325;&#26032;&#35780;&#20998;&#27169;&#22411;&#26500;&#24314;&#31232;&#30095;-&#23494;&#38598;&#28151;&#21512;&#26816;&#32034;&#22120;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#27874;&#20848;&#35821;&#35757;&#32451;&#20102;&#26032;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24182;&#23558;&#20854;&#32467;&#26524;&#19982;&#20808;&#21069;&#35780;&#20272;&#36807;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23494;&#38598;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13350v1 Announce Type: new  Abstract: We present Polish Information Retrieval Benchmark (PIRB), a comprehensive evaluation framework encompassing 41 text information retrieval tasks for Polish. The benchmark incorporates existing datasets as well as 10 new, previously unpublished datasets covering diverse topics such as medicine, law, business, physics, and linguistics. We conduct an extensive evaluation of over 20 dense and sparse retrieval models, including the baseline models trained by us as well as other available Polish and multilingual methods. Finally, we introduce a three-step process for training highly effective language-specific retrievers, consisting of knowledge distillation, supervised fine-tuning, and building sparse-dense hybrid retrievers using a lightweight rescoring model. In order to validate our approach, we train new text encoders for Polish and compare their results with previously evaluated methods. Our dense models outperform the best solutions avai
&lt;/p&gt;</description></item><item><title>Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2402.12749</link><description>&lt;p&gt;
Me LLaMA: &#20026;&#21307;&#30103;&#24212;&#29992;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Me LLaMA: Foundation Large Language Models for Medical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12749
&lt;/p&gt;
&lt;p&gt;
Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19981;&#22815;&#29702;&#24819;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#22411;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Me LLaMA&#65292;&#19968;&#20010;&#21307;&#23398;LLM&#31995;&#21015;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;- Me LLaMA 13/70B&#21450;&#20854; chat-enhanced &#29256;&#26412;- Me LLaMA 13/70B-chat&#65292;&#36890;&#36807;&#25345;&#32493;&#23545;LLaMA2&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25968;&#25454;&#24320;&#21457;&#32780;&#25104;&#12290;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#22871;&#20214;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;129B tokens&#30340;&#22823;&#35268;&#27169;&#25345;&#32493;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;214k&#20010;&#26679;&#26412;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36328;&#36234;14&#20010;&#25968;&#25454;&#38598;&#30340;&#20845;&#39033;&#20219;&#21153;&#30340;&#21307;&#23398;&#35780;&#20272;&#22522;&#20934;(MIBE)&#12290;&#25105;&#20204;&#20351;&#29992;MIBE&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#65292;Me LLaMA&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#21307;&#23398;LLMs&#65292;&#24182;&#19988;&#22312;&#21830;&#19994;&#24040;&#22836;&#22914;ChatGPT&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;&#65292;&#36890;&#36807;&#20840;&#38754;&#29615;&#22659;&#24863;&#30693;&#21644;&#26465;&#20214;&#21160;&#20316;&#39044;&#27979;&#20004;&#31181;&#26032;&#26041;&#27861;&#31995;&#32479;&#24615;&#25552;&#39640;GUI&#33258;&#21160;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11941</link><description>&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;GUI&#33258;&#21160;&#21270;&#30340;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Cognitive LLM Agent for Smartphone GUI Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11941
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;&#65292;&#36890;&#36807;&#20840;&#38754;&#29615;&#22659;&#24863;&#30693;&#21644;&#26465;&#20214;&#21160;&#20316;&#39044;&#27979;&#20004;&#31181;&#26032;&#26041;&#27861;&#31995;&#32479;&#24615;&#25552;&#39640;GUI&#33258;&#21160;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#26174;&#31034;&#20986;&#20316;&#20026;&#20154;&#31867;&#33324;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#19982;&#29616;&#23454;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26174;&#33879;&#28508;&#21147;&#65292;&#23588;&#20854;&#22312;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;(GUI)&#33258;&#21160;&#21270;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;GUI&#20195;&#29702;&#38656;&#35201;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21253;&#25324;&#35814;&#23613;&#30340;&#24863;&#30693;&#21644;&#21487;&#38752;&#30340;&#21160;&#20316;&#21709;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;&#65292;CoCo-Agent&#65292;&#37319;&#29992;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#20840;&#38754;&#29615;&#22659;&#24863;&#30693;(CEP)&#21644;&#26465;&#20214;&#21160;&#20316;&#39044;&#27979;(CAP)&#65292;&#20197;&#31995;&#32479;&#24615;&#22320;&#25552;&#39640;GUI&#33258;&#21160;&#21270;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;CEP&#36890;&#36807;&#19981;&#21516;&#26041;&#38754;&#21644;&#31890;&#24230;&#20419;&#36827;GUI&#24863;&#30693;&#65292;&#21253;&#25324;&#23631;&#24149;&#25130;&#22270;&#21644;&#29992;&#20110;&#35270;&#35273;&#36890;&#36947;&#30340;&#34917;&#20805;&#35814;&#32454;&#24067;&#23616;&#65292;&#20197;&#21450;&#29992;&#20110;&#25991;&#26412;&#36890;&#36947;&#30340;&#21382;&#21490;&#21160;&#20316;&#12290;&#20854;&#27425;&#65292;CAP&#23558;&#21160;&#20316;&#39044;&#27979;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65306;&#21160;&#20316;&#31867;&#22411;&#39044;&#27979;&#21644;&#26465;&#20214;&#21270;&#20110;&#21160;&#20316;&#31867;&#22411;&#30340;&#21160;&#20316;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11941v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose \underline{Co}mprehensive \underline{Co}gnitive LLM \underline{Agent}, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#22242;&#20307;&#20114;&#21160;&#23545;&#23447;&#25945;&#26497;&#31471;&#21270;&#30340;&#24433;&#21709;&#65292;&#22312;&#21360;&#24230; Twitter &#29992;&#25143;&#20013;&#21457;&#29616;&#65292;&#25919;&#27835;&#21644;&#31038;&#20250;&#20107;&#20214;&#30340;&#22242;&#20307;&#38388;&#20114;&#21160;&#21487;&#20197;&#20943;&#23569;&#26497;&#31471;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.11895</link><description>&lt;p&gt;
&#19982;&#22242;&#20307;&#20114;&#21160;&#65306;&#23545;&#23447;&#25945;&#26497;&#31471;&#21270;&#24433;&#21709;&#30340;&#32852;&#31995;&#19982;&#30772;&#35010;
&lt;/p&gt;
&lt;p&gt;
Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11895
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#22242;&#20307;&#20114;&#21160;&#23545;&#23447;&#25945;&#26497;&#31471;&#21270;&#30340;&#24433;&#21709;&#65292;&#22312;&#21360;&#24230; Twitter &#29992;&#25143;&#20013;&#21457;&#29616;&#65292;&#25919;&#27835;&#21644;&#31038;&#20250;&#20107;&#20214;&#30340;&#22242;&#20307;&#38388;&#20114;&#21160;&#21487;&#20197;&#20943;&#23569;&#26497;&#31471;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25509;&#35302;&#19981;&#21516;&#35266;&#28857;&#21487;&#33021;&#20250;&#20943;&#23569;&#26497;&#31471;&#21270;&#65292;&#20294;&#24403;&#35752;&#35770;&#23545;&#25239;&#24615;&#26102;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#21453;&#25928;&#24212;&#24182;&#21152;&#21095;&#26497;&#31471;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22260;&#32469;&#37325;&#35201;&#20107;&#20214;&#30340;&#22242;&#20307;&#38388;&#20114;&#21160;&#26159;&#21542;&#24433;&#21709;&#31038;&#20132;&#32593;&#32476;&#20013;&#22810;&#25968;&#32676;&#20307;&#21644;&#23569;&#25968;&#32676;&#20307;&#20043;&#38388;&#30340;&#26497;&#31471;&#21270;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#32422; 70 &#19975;&#21517;&#21360;&#24230; Twitter &#29992;&#25143;&#22312; 2020 &#24180;&#21442;&#19982;&#19982; COVID-19 &#30456;&#20851;&#35805;&#39064;&#35752;&#35770;&#26102;&#30340;&#23447;&#25945;&#36523;&#20221;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#25512;&#25991;&#25991;&#26412;&#30340;&#24773;&#22659;&#23884;&#20837;&#30340;&#26032;&#37327;&#24230;&#65292;&#29992;&#20110;&#24110;&#21161;&#25105;&#20204;&#35780;&#20272;&#23447;&#25945;&#32676;&#20307;&#20043;&#38388;&#30340;&#26497;&#31471;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20803;&#23398;&#20064;&#26694;&#26550;&#26469;&#30740;&#31350;&#22260;&#32469;&#20849;&#21516;&#12289;&#25919;&#27835;&#21644;&#31038;&#20250;&#32463;&#27982;&#20107;&#20214;&#30340;&#24322;&#36136;&#22788;&#29702;&#25928;&#26524;&#23545;&#20010;&#20307;&#32676;&#20307;&#31526;&#21512;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25919;&#27835;&#21644;&#31038;&#20250;&#20107;&#20214;&#26041;&#38754;&#65292;&#22242;&#20307;&#38388;&#20114;&#21160;&#20250;&#20943;&#23569;&#26497;&#31471;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11895v1 Announce Type: cross  Abstract: While exposure to diverse viewpoints may reduce polarization, it can also have a backfire effect and exacerbate polarization when the discussion is adversarial. Here, we examine the question whether intergroup interactions around important events affect polarization between majority and minority groups in social networks. We compile data on the religious identity of nearly 700,000 Indian Twitter users engaging in COVID-19-related discourse during 2020. We introduce a new measure for an individual's group conformity based on contextualized embeddings of tweet text, which helps us assess polarization between religious groups. We then use a meta-learning framework to examine heterogeneous treatment effects of intergroup interactions on an individual's group conformity in the light of communal, political, and socio-economic events. We find that for political and social events, intergroup interactions reduce polarization. This decline is we
&lt;/p&gt;</description></item><item><title>&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11078</link><description>&lt;p&gt;
&#36890;&#36807;&#32431;&#24494;&#35843;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Model Editing by Pure Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11078
&lt;/p&gt;
&lt;p&gt;
&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#25972;&#34987;&#35748;&#20026;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#19981;&#22815;&#26377;&#25928;&#65292;&#22240;&#20026;&#30456;&#23545;&#26356;&#19987;&#19994;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#26159;&#31616;&#21333;&#30340;&#65292;&#19981;&#20851;&#24515;&#34987;&#32534;&#36753;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#30340;&#19981;&#26029;&#36827;&#23637;&#65288;&#20363;&#22914;PEFT&#65289;&#65292;&#20351;&#20854;&#25104;&#20026;&#27169;&#22411;&#32534;&#36753;&#22120;&#30340;&#21560;&#24341;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#24494;&#35843;&#21487;&#20197;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26420;&#32032;&#24494;&#35843;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#32780;&#38750;&#23436;&#25972;&#20284;&#28982;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#27867;&#21270;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#22312;ZsRE&#21644;CounterFact&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#19968;&#31616;&#21333;&#20462;&#25913;&#20351;&#24471;&#24494;&#35843;&#36890;&#24120;&#21487;&#20197;&#19982;&#19987;&#19994;&#32534;&#36753;&#22120;&#22312;&#32534;&#36753;&#20998;&#25968;&#26041;&#38754;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#25552;&#39640;NLP&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#37319;&#29992;&#32454;&#24605;&#36830;&#24819;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;GPT-4&#20013;&#25552;&#28860;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;BERT&#27169;&#22411;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.09282</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;NLP&#20219;&#21153;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#25552;&#39640;NLP&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#37319;&#29992;&#32454;&#24605;&#36830;&#24819;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;GPT-4&#20013;&#25552;&#28860;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;BERT&#27169;&#22411;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#30340;&#25972;&#21512;&#21040;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#65292;&#20026;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#20943;&#23569;&#23545;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#30340;&#20381;&#36182;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32454;&#24605;&#36830;&#24819;&#65288;CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#20174;GPT-4&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25913;&#36827;&#36739;&#23567;&#27169;&#22411;BERT&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#19978;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#65306;&#39318;&#20808;&#20351;&#29992;GPT-4&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#33976;&#39311;&#21644;&#21407;&#22987;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#30340;&#32452;&#21512;&#23545;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;F1&#20998;&#25968;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09282v1 Announce Type: new Abstract: The integration of Large Language Models (LLMs) like GPT-4 into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations. This paper presents a novel approach that leverages the Chain of Thought (CoT) prompting technique to distill knowledge from GPT-4, subsequently applying it to improve the efficiency and effectiveness of a smaller model, BERT, on Named Entity Recognition (NER) tasks. Our method involves a two-phase training process: initially employing GPT-4 annotated data for pre-training and then refining the model with a combination of distilled and original human-annotated data. The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings. The 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#23545;&#25968;&#20284;&#28982;&#30340;&#35777;&#25454;&#19979;&#30028;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21152;&#26435;&#32447;&#24615;&#32452;&#21512;&#65292;&#23558;&#23545;&#27604;&#20027;&#39064;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#33719;&#24471;&#33021;&#22815;&#25429;&#25417;&#20849;&#20139;&#35821;&#20041;&#24182;&#20811;&#26381;&#20302;&#32423;&#21035;&#20114;&#20449;&#24687;&#24178;&#25200;&#30340;&#20027;&#39064;&#21521;&#37327;&#38598;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.07577</link><description>&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#20316;&#20026;&#22810;&#30446;&#26631;&#23545;&#27604;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topic Modeling as Multi-Objective Contrastive Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07577
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#23545;&#25968;&#20284;&#28982;&#30340;&#35777;&#25454;&#19979;&#30028;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21152;&#26435;&#32447;&#24615;&#32452;&#21512;&#65292;&#23558;&#23545;&#27604;&#20027;&#39064;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#33719;&#24471;&#33021;&#22815;&#25429;&#25417;&#20849;&#20139;&#35821;&#20041;&#24182;&#20811;&#26381;&#20302;&#32423;&#21035;&#20114;&#20449;&#24687;&#24178;&#25200;&#30340;&#20027;&#39064;&#21521;&#37327;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#23545;&#25968;&#20284;&#28982;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21152;&#26435;&#32447;&#24615;&#32452;&#21512;&#26469;&#22686;&#24378;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25991;&#26723;&#32423;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#25429;&#25417;&#21040;&#20302;&#32423;&#21035;&#30340;&#20114;&#20449;&#24687;&#65292;&#20363;&#22914;&#35789;&#27604;&#20363;&#65292;&#36825;&#20250;&#24178;&#25200;&#20027;&#39064;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;ELBO&#25439;&#22833;&#26088;&#22312;&#35760;&#24518;&#36755;&#20837;&#32454;&#33410;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#37325;&#26500;&#36136;&#37327;&#65292;&#32780;&#23545;&#27604;&#25439;&#22833;&#21017;&#35797;&#22270;&#23398;&#20064;&#22312;&#36755;&#20837;&#25991;&#26723;&#20043;&#38388;&#27867;&#21270;&#30340;&#20027;&#39064;&#34920;&#31034;&#65292;&#20108;&#32773;&#23384;&#22312;&#28508;&#22312;&#20914;&#31361;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#39318;&#20808;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20027;&#39064;&#21521;&#37327;&#38598;&#21512;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#19968;&#32452;&#36755;&#20837;&#25991;&#26723;&#20043;&#38388;&#20849;&#20139;&#30340;&#26377;&#29992;&#35821;&#20041;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#23545;&#27604;&#20027;&#39064;&#24314;&#27169;&#26126;&#30830;&#25552;&#20986;&#20026;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23454;&#29616;&#24085;&#32047;&#25176;&#24179;&#31283;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive learning objective that contrasts pairs of input documents. However, document-level contrastive learning might capture low-level mutual information, such as word ratio, which disturbs topic modeling. Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive loss which attempts to learn topic representations that generalize among input documents. To address these issues, we first introduce a novel contrastive learning method oriented towards sets of topic vectors to capture useful semantics that are shared among a set of input documents. Secondly, we explicitly cast contrastive topic modeling as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that b
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.04559</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Model Agents Simulate Human Trust Behaviors?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04559
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#20316;&#20026;&#27169;&#25311;&#24037;&#20855;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#22312;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;LLM&#20195;&#29702;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20154;&#31867;&#20114;&#21160;&#20013;&#26368;&#20851;&#38190;&#30340;&#34892;&#20026;&#20043;&#19968;&#65292;&#20449;&#20219;&#65292;&#26088;&#22312;&#35843;&#26597;LLM&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#65292;&#22312;&#34987;&#34892;&#20026;&#32463;&#27982;&#23398;&#24191;&#27867;&#25509;&#21463;&#30340;&#20449;&#20219;&#28216;&#25103;&#26694;&#26550;&#19979;&#65292;LLM&#20195;&#29702;&#36890;&#24120;&#34920;&#29616;&#20986;&#20449;&#20219;&#34892;&#20026;&#65292;&#31216;&#20026;&#20195;&#29702;&#20449;&#20219;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#20195;&#29702;&#22312;&#20449;&#20219;&#34892;&#20026;&#26041;&#38754;&#19982;&#20154;&#31867;&#20855;&#26377;&#36739;&#39640;&#30340;&#34892;&#20026;&#19968;&#33268;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20195;&#29702;&#20449;&#20219;&#20013;&#30340;&#20559;&#35265;&#20197;&#21450;&#20195;&#29702;&#20449;&#20219;&#22312;&#23545;&#20195;&#29702;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#26041;&#38754;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21253;&#25324;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#22312;&#20869;&#30340;&#26465;&#20214;&#19979;&#20195;&#29702;&#20449;&#20219;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strate
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>SemPLeS&#26694;&#26550;&#21033;&#29992;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#25552;&#31034;&#26469;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2401.11791</link><description>&lt;p&gt;
SemPLeS: &#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11791
&lt;/p&gt;
&lt;p&gt;
SemPLeS&#26694;&#26550;&#21033;&#29992;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#25552;&#31034;&#26469;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#26088;&#22312;&#21033;&#29992;&#20165;&#20855;&#26377;&#22270;&#20687;&#32423;&#30417;&#30563;&#30340;&#22270;&#20687;&#25968;&#25454;&#26469;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#12290;&#30001;&#20110;&#26080;&#27861;&#33719;&#24471;&#31934;&#30830;&#30340;&#20687;&#32032;&#32423;&#26631;&#27880;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#36890;&#36807;&#20248;&#21270;CAM&#26679;&#24335;&#30340;&#28909;&#22270;&#26469;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#30340;&#20266;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#28909;&#22270;&#21487;&#33021;&#20165;&#25429;&#33719;&#23545;&#35937;&#31867;&#21035;&#30340;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#22270;&#20687;&#21306;&#22495;&#25110;&#30456;&#20851;&#30340;&#20849;&#21516;&#20986;&#29616;&#30340;&#32972;&#26223;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;WSSS&#30340;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#65288;SemPLeS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#31034;CLIP&#28508;&#31354;&#38388;&#20197;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#21644;&#25552;&#31034;&#24341;&#23548;&#30340;&#35821;&#20041;&#32454;&#21270;&#65292;&#20197;&#23398;&#20064;&#36866;&#24403;&#25551;&#36848;&#21644;&#25233;&#21046;&#19982;&#27599;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30456;&#20851;&#30340;&#20849;&#21516;&#20986;&#29616;&#30340;&#32972;&#26223;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11791v2 Announce Type: replace-cross  Abstract: Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each target object category. In thi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35825;&#23548;&#34394;&#20551;&#20449;&#24687;&#26469;&#26500;&#24314;&#19968;&#20010;&#20107;&#23454;&#34180;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#26469;&#24809;&#32602;&#36825;&#20123;&#35825;&#23548;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#21319;&#29983;&#25104;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.15710</link><description>&lt;p&gt;
&#36890;&#36807;&#35825;&#23548;&#24615;&#24187;&#35273;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Alleviating Hallucinations of Large Language Models through Induced Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15710
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35825;&#23548;&#34394;&#20551;&#20449;&#24687;&#26469;&#26500;&#24314;&#19968;&#20010;&#20107;&#23454;&#34180;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#26469;&#24809;&#32602;&#36825;&#20123;&#35825;&#23548;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#21319;&#29983;&#25104;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#24050;&#35266;&#23519;&#21040;&#23427;&#20204;&#29983;&#25104;&#30340;&#21709;&#24212;&#20013;&#21253;&#21547;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#8220;&#35825;&#23548;-&#23545;&#27604;&#35299;&#30721;&#8221;(ICD)&#31574;&#30053;&#26469;&#20943;&#36731;&#24187;&#35273;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20174;&#21407;&#22987;LLMs&#20013;&#35825;&#23548;&#24187;&#35273;&#26469;&#26500;&#24314;&#19968;&#20010;&#20107;&#23454;&#19978;&#34180;&#24369;&#30340;LLM&#12290;&#28982;&#21518;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#24809;&#32602;&#36825;&#20123;&#35825;&#23548;&#30340;&#24187;&#35273;&#20197;&#22686;&#24378;&#29983;&#25104;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#26469;&#25918;&#22823;&#21407;&#27169;&#22411;&#30340;&#39044;&#27979;&#24182;&#36140;&#20302;&#35825;&#23548;&#30340;&#19981;&#30495;&#23454;&#39044;&#27979;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#12290;&#23545;&#22522;&#20110;&#27495;&#35270;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#22914;TruthfulQA&#21644;FActScore&#31561;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;ICD&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15710v2 Announce Type: replace-cross  Abstract: Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as ``hallucination''. In this work, we propose a simple \textit{Induce-then-Contrast} Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and \textsc{FActScore}, demonstrate that our proposed ICD methods can effectively enhance th
&lt;/p&gt;</description></item><item><title>&#26032;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#37327;&#30340;&#23454;&#20363;-&#26631;&#31614;&#23545;&#26469;&#20016;&#23500;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;</title><link>https://arxiv.org/abs/2312.12021</link><description>&lt;p&gt;
Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction
&lt;/p&gt;
&lt;p&gt;
Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12021
&lt;/p&gt;
&lt;p&gt;
&#26032;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#37327;&#30340;&#23454;&#20363;-&#26631;&#31614;&#23545;&#26469;&#20016;&#23500;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#65288;FSRE&#65289;&#26088;&#22312;&#20174;&#31232;&#30095;&#26631;&#35760;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#20107;&#23454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;FSRE&#32467;&#26524;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#23454;&#20363;&#21644;&#26631;&#31614;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#20013;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#22823;&#37327;&#30340;&#23454;&#20363;-&#26631;&#31614;&#23545;&#26469;&#20351;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#35821;&#20041;&#20016;&#23500;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21516;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#30340;&#21160;&#26426;&#26159;&#65292;&#36890;&#36807;&#23454;&#20363;-&#26631;&#31614;&#23545;&#20256;&#36798;&#30340;&#22810;&#26679;&#35266;&#28857;&#25429;&#25417;&#21040;&#20102;&#19981;&#23436;&#25972;&#20294;&#20114;&#34917;&#30340;&#25991;&#26412;&#35821;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#28041;&#21450;&#19968;&#31181;&#23545;&#31216;&#23545;&#27604;&#30446;&#26631;&#65292;&#21253;&#21547;&#20102;&#21477;&#23376;&#38170;&#23450;&#21644;&#26631;&#31614;&#38170;&#23450;&#30340;&#23545;&#27604;&#25439;&#22833;&#12290;&#36890;&#36807;&#32452;&#21512;&#36825;&#20004;&#31181;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12021v3 Announce Type: replace-cross  Abstract: Few-shot Relation Extraction (FSRE) aims to extract relational facts from a sparse set of labeled corpora. Recent studies have shown promising results in FSRE by employing Pre-trained Language Models (PLMs) within the framework of supervised contrastive learning, which considers both instances and label facts. However, how to effectively harness massive instance-label pairs to encompass the learned representation with semantic richness in this learning paradigm is not fully explored. To address this gap, we introduce a novel synergistic anchored contrastive pre-training framework. This framework is motivated by the insight that the diverse viewpoints conveyed through instance-label pairs capture incomplete yet complementary intrinsic textual semantics. Specifically, our framework involves a symmetrical contrastive objective that encompasses both sentence-anchored and label-anchored contrastive losses. By combining these two los
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;vec-tionaries&#27979;&#37327;&#24037;&#20855;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#20248;&#21270;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#35789;&#20856;&#19982;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#65292;&#25913;&#21892;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#28040;&#24687;&#29305;&#24449;&#30340;&#27979;&#37327;&#65292;&#23588;&#20854;&#26159;&#30701;&#26684;&#24335;&#20013;&#30340;&#37027;&#20123;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#35789;&#27719;&#34920;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#22659;&#12290;</title><link>https://arxiv.org/abs/2312.05990</link><description>&lt;p&gt;
&#26500;&#24314;Vec-tionaries&#20197;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#28040;&#24687;&#29305;&#24449;&#65306;&#19968;&#39033;&#36947;&#24503;&#21628;&#21505;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Constructing Vec-tionaries to Extract Message Features from Texts: A Case Study of Moral Appeals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05990
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;vec-tionaries&#27979;&#37327;&#24037;&#20855;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#20248;&#21270;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#35789;&#20856;&#19982;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#65292;&#25913;&#21892;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#28040;&#24687;&#29305;&#24449;&#30340;&#27979;&#37327;&#65292;&#23588;&#20854;&#26159;&#30701;&#26684;&#24335;&#20013;&#30340;&#37027;&#20123;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#35789;&#27719;&#34920;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#30740;&#31350;&#25991;&#26412;&#20013;&#30340;&#36947;&#24503;&#20869;&#23481;&#31561;&#28040;&#24687;&#29305;&#24449;&#65292;&#27604;&#22914;&#25919;&#20826;&#23459;&#35328;&#21644;&#31038;&#20132;&#23186;&#20307;&#65292;&#20294;&#23427;&#20204;&#30340;&#37327;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#20154;&#24037;&#32534;&#30721;&#22312;&#21487;&#20280;&#32553;&#24615;&#21644;&#20114;&#30721;&#32773;&#21487;&#38752;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26500;&#24314;vec-tionaries&#27979;&#37327;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#20248;&#21270;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#35789;&#20856;&#19982;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#23884;&#20837;&#32534;&#30721;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;vec-tionaries&#25913;&#21892;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#28040;&#24687;&#29305;&#24449;&#30340;&#27979;&#37327;&#65292;&#23588;&#20854;&#26159;&#30701;&#26684;&#24335;&#20013;&#30340;&#37027;&#20123;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#35789;&#27719;&#34920;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05990v2 Announce Type: replace  Abstract: While researchers often study message features like moral content in text, such as party manifestos and social media, their quantification remains a challenge. Conventional human coding struggles with scalability and intercoder reliability. While dictionary-based methods are cost-effective and computationally efficient, they often lack contextual sensitivity and are limited by the vocabularies developed for the original applications. In this paper, we present an approach to construct vec-tionary measurement tools that boost validated dictionaries with word embeddings through nonlinear optimization. By harnessing semantic relationships encoded by embeddings, vec-tionaries improve the measurement of message features from text, especially those in short format, by expanding the applicability of original vocabularies to other contexts. Importantly, a vec-tionary can produce additional metrics to capture the valence and ambivalence of a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#31471;&#21040;&#31471;&#20851;&#31995;&#25277;&#21462;&#30340;&#31649;&#36947;&#12289;&#24207;&#21015;&#21040;&#24207;&#21015;&#21644;GPT&#27169;&#22411;&#65292;&#21457;&#29616;&#31649;&#36947;&#27169;&#22411;&#20173;&#28982;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#32780;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#32039;&#38543;&#20854;&#21518;&#65307;&#21442;&#25968;&#37327;&#22686;&#21152;&#20843;&#20493;&#30340;GPT&#27169;&#22411;&#29978;&#33267;&#27604;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26356;&#24046;&#65292;&#19988;&#27604;&#31649;&#36947;&#27169;&#22411;&#20302;10&#20010;F1&#28857;&#20197;&#19978;&#12290;</title><link>https://arxiv.org/abs/2311.13729</link><description>&lt;p&gt;
&#27604;&#36739;&#29992;&#20110;&#31471;&#21040;&#31471;&#20851;&#31995;&#25277;&#21462;&#30340;&#31649;&#36947;&#12289;&#24207;&#21015;&#21040;&#24207;&#21015;&#21644;GPT&#27169;&#22411;&#65306;&#20197;&#32597;&#35265;&#30142;&#30149;&#29992;&#20363;&#20026;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#31471;&#21040;&#31471;&#20851;&#31995;&#25277;&#21462;&#30340;&#31649;&#36947;&#12289;&#24207;&#21015;&#21040;&#24207;&#21015;&#21644;GPT&#27169;&#22411;&#65292;&#21457;&#29616;&#31649;&#36947;&#27169;&#22411;&#20173;&#28982;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#32780;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#32039;&#38543;&#20854;&#21518;&#65307;&#21442;&#25968;&#37327;&#22686;&#21152;&#20843;&#20493;&#30340;GPT&#27169;&#22411;&#29978;&#33267;&#27604;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26356;&#24046;&#65292;&#19988;&#27604;&#31649;&#36947;&#27169;&#22411;&#20302;10&#20010;F1&#28857;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#20851;&#31995;&#25277;&#21462;&#65288;E2ERE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#32780;&#29616;&#23454;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#20351;&#29992;&#19968;&#20010;&#20851;&#27880;&#32597;&#35265;&#30142;&#30149;&#12289;&#28041;&#21450;&#19981;&#36830;&#32493;&#21644;&#23884;&#22871;&#23454;&#20307;&#30340;&#22797;&#26434;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;E2ERE&#30340;&#19977;&#31181;&#27969;&#34892;&#33539;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;RareDis&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19977;&#31181;&#31454;&#20105;&#26041;&#27861;&#65288;&#29992;&#20110;E2ERE&#65289;&#65306;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#8594;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#31649;&#36947;&#12289;&#32852;&#21512;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#38024;&#23545;&#27599;&#31181;&#26041;&#27861;&#20351;&#29992;&#21487;&#27604;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#36827;&#34892;&#38169;&#35823;&#20998;&#26512;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;&#65292;&#31649;&#36947;&#27169;&#22411;&#20173;&#28982;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#32780;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#32039;&#38543;&#20854;&#21518;&#65307;&#21442;&#25968;&#37327;&#22686;&#21152;&#20843;&#20493;&#30340;GPT&#27169;&#22411;&#29978;&#33267;&#27604;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26356;&#24046;&#65292;&#19988;&#27604;&#31649;&#36947;&#27169;&#22411;&#20302;10&#20010;F1&#28857;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13729v2 Announce Type: replace  Abstract: End-to-end relation extraction (E2ERE) is an important and realistic application of natural language processing (NLP) in biomedicine. In this paper, we aim to compare three prevailing paradigms for E2ERE using a complex dataset focused on rare diseases involving discontinuous and nested entities. We use the RareDis information extraction dataset to evaluate three competing approaches (for E2ERE): NER $\rightarrow$ RE pipelines, joint sequence to sequence models, and generative pre-trained transformer (GPT) models. We use comparable state-of-the-art models and best practices for each of these approaches and conduct error analyses to assess their failure modes. Our findings reveal that pipeline models are still the best, while sequence-to-sequence models are not far behind; GPT models with eight times as many parameters are worse than even sequence-to-sequence models and lose to pipeline models by over 10 F1 points. Partial matches and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#33041;&#35760;&#24405;&#20013;&#30452;&#25509;&#29983;&#25104;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#33041;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#36755;&#20837;&#29983;&#25104;&#19982;&#35821;&#20041;&#20869;&#23481;&#19968;&#33268;&#30340;&#36830;&#36143;&#35821;&#35328;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2311.09889</link><description>&lt;p&gt;
&#22823;&#33041;&#35760;&#24405;&#20013;&#30340;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Language Generation from Brain Recordings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09889
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#33041;&#35760;&#24405;&#20013;&#30452;&#25509;&#29983;&#25104;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#33041;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#36755;&#20837;&#29983;&#25104;&#19982;&#35821;&#20041;&#20869;&#23481;&#19968;&#33268;&#30340;&#36830;&#36143;&#35821;&#35328;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24335;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#20855;&#26377;&#28508;&#21147;&#35299;&#38145;&#35768;&#22810;&#24212;&#29992;&#65292;&#22914;&#20026;&#27531;&#30142;&#24739;&#32773;&#25552;&#20379;&#26381;&#21153;&#21644;&#25913;&#21892;&#27807;&#36890;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36890;&#36807;BCIs&#29983;&#25104;&#35821;&#35328;&#20165;&#22312;&#20998;&#31867;&#35774;&#32622;&#20869;&#25104;&#21151;&#65292;&#29992;&#20110;&#36873;&#25321;&#24102;&#26377;&#26368;&#21487;&#33021;&#30340;&#30382;&#23618;&#35821;&#20041;&#34920;&#31034;&#30340;&#39044;&#29983;&#25104;&#21477;&#23376;&#24310;&#32493;&#20505;&#36873;&#12290;&#21463;&#21040;&#26368;&#36817;&#26174;&#31034;&#22823;&#33041;&#19982;&#22823;&#22411;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20851;&#32852;&#30340;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35821;&#35328;BCI&#65292;&#35813;BCI&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#19982;&#35821;&#20041;&#33041;&#35299;&#30721;&#22120;&#20849;&#21516;&#29983;&#25104;&#35821;&#35328;&#65292;&#30452;&#25509;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#36755;&#20837;&#20013;&#29983;&#25104;&#35821;&#35328;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#24863;&#30693;&#21040;&#30340;&#35270;&#35273;&#25110;&#21548;&#35273;&#35821;&#35328;&#21050;&#28608;&#30340;&#35821;&#20041;&#20869;&#23481;&#19968;&#33268;&#30340;&#35821;&#35328;&#24207;&#21015;&#65292;&#32780;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#20219;&#20309;&#39044;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09889v4 Announce Type: replace  Abstract: Generating human language through non-invasive brain-computer interfaces (BCIs) has the potential to unlock many applications, such as serving disabled patients and improving communication. Currently, however, generating language via BCIs has been previously successful only within a classification setup for selecting pre-generated sentence continuation candidates with the most likely cortical semantic representation. Inspired by recent research that revealed associations between the brain and the large computational language models, we propose a generative language BCI that utilizes the capacity of a large language model (LLM) jointly with a semantic brain decoder to directly generate language from functional magnetic resonance imaging (fMRI) input. The proposed model can generate coherent language sequences aligned with the semantic content of visual or auditory language stimuli perceived, without prior knowledge of any pre-generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; AGREE&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#32852;&#31995;&#21040;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24182;&#25552;&#20379;&#24341;&#25991;&#26469;&#25552;&#21319;&#20449;&#24687;&#20851;&#32852;&#65292;&#20174;&#32780;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#8220;&#33222;&#24819;&#8221;&#31572;&#26696;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2311.09533</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20197;&#25552;&#21319;&#20449;&#24687;&#20851;&#32852;&#21644;&#24341;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Effective Large Language Model Adaptation for Improved Grounding and Citation Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; AGREE&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#32852;&#31995;&#21040;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24182;&#25552;&#20379;&#24341;&#25991;&#26469;&#25552;&#21319;&#20449;&#24687;&#20851;&#32852;&#65292;&#20174;&#32780;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#8220;&#33222;&#24819;&#8221;&#31572;&#26696;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19968;&#22823;&#38382;&#39064;&#22312;&#20110;&#23427;&#20204;&#21487;&#33021;&#29983;&#25104;&#8220;&#33222;&#24819;&#8221;&#30340;&#31572;&#26696;&#24182;&#38750;&#20107;&#23454;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#30528;&#30524;&#20110;&#36890;&#36807;&#23558;LLMs&#30340;&#21709;&#24212;&#32852;&#31995;&#21040;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24182;&#25552;&#20379;&#24341;&#25991;&#26469;&#25913;&#36827;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AGREE&#65292;&#21363;Adaptation for GRounding EnhancEment&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#25552;&#21319;&#20102;&#20449;&#24687;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35843;&#25972;LLMs&#65292;&#20351;&#20854;&#33258;&#25105;&#32852;&#31995;&#20854;&#21709;&#24212;&#20013;&#30340;&#20027;&#24352;&#24182;&#20026;&#26816;&#32034;&#25991;&#26723;&#25552;&#20379;&#20934;&#30830;&#30340;&#24341;&#25991;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;LLMs&#22522;&#30784;&#19978;&#36827;&#34892;&#30340;&#36825;&#31181;&#35843;&#25972;&#38656;&#35201;&#23545;&#37197;&#23545;&#26597;&#35810;&#30340;&#21709;&#24212;&#36827;&#34892;&#24456;&#22909;&#30340;&#20449;&#24687;&#20851;&#32852;&#65288;&#24102;&#26377;&#24341;&#25991;&#65289;&#65292;&#20026;&#27492;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#26410;&#26631;&#35760;&#26597;&#35810;&#33258;&#21160;&#26500;&#36896;&#36825;&#20123;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#35843;&#25972;&#21518;&#30340;LLMs&#30340;&#33258;&#25105;&#20851;&#32852;&#33021;&#21147;&#36827;&#19968;&#27493;&#20351;&#20854;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09533v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language understanding and generation. However, one major issue towards their widespread deployment in the real world is that they can generate "hallucinated" answers that are not factual. Towards this end, this paper focuses on improving LLMs by grounding their responses in retrieved passages and by providing citations. We propose a new framework, AGREE, Adaptation for GRounding EnhancEment, that improves the grounding from a holistic perspective. Our framework tunes LLMs to selfground the claims in their responses and provide accurate citations to retrieved documents. This tuning on top of the pre-trained LLMs requires well-grounded responses (with citations) for paired queries, for which we introduce a method that can automatically construct such data from unlabeled queries. The selfgrounding capability of tuned LLMs further grants them a test-time adapt
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102; counseling response rewriting &#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340; VERVE &#31995;&#32479;&#65292;&#36890;&#36807;&#21152;&#20837;&#37322;&#20041;&#30340;&#35757;&#32451;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#26356;&#26032;&#65292;&#23558;&#38750;&#21453;&#24605;&#24615;&#38472;&#36848;&#36716;&#21270;&#20026;&#21453;&#24605;&#24615;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2311.08299</link><description>&lt;p&gt;
VERVE: &#22522;&#20110;&#27169;&#26495;&#30340;&#21453;&#24605;&#37325;&#20889;&#29992;&#20110;&#28608;&#21169;&#24615;&#38754;&#35848;
&lt;/p&gt;
&lt;p&gt;
VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102; counseling response rewriting &#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340; VERVE &#31995;&#32479;&#65292;&#36890;&#36807;&#21152;&#20837;&#37322;&#20041;&#30340;&#35757;&#32451;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#26356;&#26032;&#65292;&#23558;&#38750;&#21453;&#24605;&#24615;&#38472;&#36848;&#36716;&#21270;&#20026;&#21453;&#24605;&#24615;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#24605;&#24335;&#20542;&#21548;&#26159;&#24515;&#29702;&#36741;&#23548;&#24072;&#24517;&#39035;&#25484;&#25569;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#20197;&#22312;&#28608;&#21169;&#24615;&#38754;&#35848;&#20013;&#36798;&#21040;&#29087;&#32451;&#27700;&#24179;&#12290;&#23427;&#28041;&#21450;&#20197;&#19968;&#31181;&#25215;&#35748;&#21644;&#25506;&#32034;&#23458;&#25143;&#23545;&#35805;&#20013;&#34920;&#36798;&#30340;&#24847;&#20041;&#30340;&#26041;&#24335;&#36827;&#34892;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36741;&#23548;&#21709;&#24212;&#37325;&#20889;&#30340;&#20219;&#21153;&#65292;&#23558;&#38750;&#21453;&#24605;&#24615;&#38472;&#36848;&#36716;&#21270;&#20026;&#21453;&#24605;&#24615;&#22238;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VERVE&#65292;&#19968;&#20010;&#22522;&#20110;&#27169;&#26495;&#30340;&#37325;&#20889;&#31995;&#32479;&#65292;&#20855;&#26377;&#21152;&#20837;&#37322;&#20041;&#30340;&#35757;&#32451;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#26356;&#26032;&#12290;VERVE&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#21644;&#36807;&#28388;&#19982;&#21453;&#24605;&#26080;&#20851;&#30340;&#26631;&#35760;&#26469;&#21019;&#24314;&#27169;&#26495;&#65292;&#24182;&#20351;&#29992;&#27169;&#26495;&#26500;&#24314;&#21453;&#24605;&#24335;&#22238;&#24212;&#12290;&#37322;&#20041;&#22686;&#24378;&#35757;&#32451;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#23545;&#25513;&#30721;&#31354;&#38388;&#36827;&#34892;&#36739;&#19981;&#20005;&#26684;&#30340;&#22635;&#20805;&#65292;&#32780;&#33258;&#36866;&#24212;&#27169;&#26495;&#26356;&#26032;&#21017;&#26377;&#21161;&#20110;&#21457;&#29616;&#37325;&#20889;&#30340;&#26377;&#25928;&#27169;&#26495;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#31227;&#38500;&#21407;&#22987;&#20869;&#23481;&#12290;&#20351;&#29992;&#33258;&#21160;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08299v2 Announce Type: replace-cross  Abstract: Reflective listening is a fundamental skill that counselors must acquire to achieve proficiency in motivational interviewing (MI). It involves responding in a manner that acknowledges and explores the meaning of what the client has expressed in the conversation. In this work, we introduce the task of counseling response rewriting, which transforms non-reflective statements into reflective responses. We introduce VERVE, a template-based rewriting system with paraphrase-augmented training and adaptive template updating. VERVE first creates a template by identifying and filtering out tokens that are not relevant to reflections and constructs a reflective response using the template. Paraphrase-augmented training allows the model to learn less-strict fillings of masked spans, and adaptive template updating helps discover effective templates for rewriting without significantly removing the original content. Using both automatic and 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992; ChatGPT&#12289;Google Bard &#21644; Claude &#21019;&#24314;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#21644;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#37319;&#29992;&#36867;&#36991;&#26816;&#27979;&#26426;&#21046;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2310.19181</link><description>&lt;p&gt;
&#20174;&#32842;&#22825;&#26426;&#22120;&#20154;&#21040;&#32593;&#32476;&#38035;&#40060;&#26426;&#22120;&#20154;&#65311;-- &#39044;&#38450;&#20351;&#29992; ChatGPT&#12289;Google Bard &#21644; Claude &#21019;&#24314;&#30340;&#32593;&#32476;&#38035;&#40060;&#35784;&#39575;
&lt;/p&gt;
&lt;p&gt;
From Chatbots to PhishBots? -- Preventing Phishing scams created using ChatGPT, Google Bard and Claude
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19181
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992; ChatGPT&#12289;Google Bard &#21644; Claude &#21019;&#24314;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#21644;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#37319;&#29992;&#36867;&#36991;&#26816;&#27979;&#26426;&#21046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39640;&#32423;&#21151;&#33021;&#20351;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#65292;&#21253;&#25324;&#23545;&#35805;&#20195;&#29702;&#12289;&#20869;&#23481;&#21019;&#20316;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#20063;&#20351;&#23427;&#20204;&#23481;&#26131;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#65292;&#21253;&#25324;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22235;&#31181;&#27969;&#34892;&#30340;&#21830;&#19994;&#21487;&#29992;&#30340;LLMs&#65288;ChatGPT&#65288;GPT 3.5 Turbo&#65289;&#12289;GPT 4&#12289;Claude &#21644; Bard&#65289;&#26469;&#29983;&#25104;&#21151;&#33021;&#24615;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#30340;&#28508;&#21147;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#24694;&#24847;&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;LLMs&#21487;&#20197;&#29983;&#25104;&#26082;&#33021;&#22815;&#36924;&#30495;&#27169;&#20223;&#30693;&#21517;&#21697;&#29260;&#21448;&#33021;&#22815;&#37096;&#32626;&#19968;&#31995;&#21015;&#36867;&#36991;&#25506;&#27979;&#26426;&#21046;&#30340;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#21644;&#30005;&#23376;&#37038;&#20214;&#12290;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;LLMs&#30340;&#26410;&#20462;&#25913;&#25110;&#8220;&#21407;&#22987;&#8221;&#29256;&#26412;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20808;&#21069;&#30340;&#25932;&#23545;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19181v2 Announce Type: replace-cross  Abstract: The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation. However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks. This study explores the potential of using four popular commercially available LLMs, i.e., ChatGPT (GPT 3.5 Turbo), GPT 4, Claude, and Bard, to generate functional phishing attacks using a series of malicious prompts. We discover that these LLMs can generate both phishing websites and emails that can convincingly imitate well-known brands and also deploy a range of evasive tactics that are used to elude detection mechanisms employed by anti-phishing systems. These attacks can be generated using unmodified or "vanilla" versions of these LLMs without requiring any prior adversarial ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#36866;&#24212;&#22810;&#22836;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#26681;&#25454;&#21477;&#23376;&#38271;&#24230;&#21464;&#21270;&#22836;&#25968;&#65292;&#29992;&#20110;&#30005;&#24433;&#35780;&#35770;&#25991;&#26723;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2310.14505</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#20013;&#33258;&#36866;&#24212;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis with adaptive multi-head attention in Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#36866;&#24212;&#22810;&#22836;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#26681;&#25454;&#21477;&#23376;&#38271;&#24230;&#21464;&#21270;&#22836;&#25968;&#65292;&#29992;&#20110;&#30005;&#24433;&#35780;&#35770;&#25991;&#26723;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#30005;&#24433;&#35780;&#35770;&#25991;&#26723;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#22836;&#27880;&#24847;&#21147;&#26550;&#26500;&#65288;AdaptAttn&#65289;&#65292;&#26681;&#25454;&#21477;&#23376;&#38271;&#24230;&#21464;&#21270;&#22836;&#25968;&#12290;AdaptAttn&#26377;&#19968;&#20010;&#25968;&#25454;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#26681;&#25454;&#21477;&#23376;&#38271;&#24230;&#23558;&#27599;&#20010;&#25991;&#26723;&#20998;&#31867;&#20026;&#19977;&#20010;&#26742;&#20013;&#30340;&#20219;&#19968;&#20010;&#65306;&#23567;&#12289;&#20013;&#25110;&#22823;&#12290;&#25105;&#20204;&#22312;&#26031;&#22374;&#31119;&#22823;&#22411;&#30005;&#24433;&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#26816;&#39564;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14505v4 Announce Type: replace  Abstract: We propose a novel framework based on the attention mechanism to identify the sentiment of a movie review document. Previous efforts on deep neural networks with attention mechanisms focus on encoder and decoder with fixed numbers of multi-head attention. Therefore, we need a mechanism to stop the attention process automatically if no more useful information can be read from the memory.In this paper, we propose an adaptive multi-head attention architecture (AdaptAttn) which varies the number of attention heads based on length of sentences. AdaptAttn has a data preprocessing step where each document is classified into any one of the three bins small, medium or large based on length of the sentence. The document classified as small goes through two heads in each layer, the medium group passes four heads and the large group is processed by eight heads. We examine the merit of our model on the Stanford large movie review dataset. The exp
&lt;/p&gt;</description></item><item><title>Prometheus&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#35780;&#20998;&#26631;&#20934;&#21644;&#36866;&#24403;&#30340;&#21442;&#32771;&#26448;&#26009;&#65292;&#21487;&#20197;&#22312;&#19982;GPT-4&#30456;&#23218;&#32654;&#30340;&#35780;&#20272;&#33021;&#21147;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2310.08491</link><description>&lt;p&gt;
Prometheus: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#32454;&#31890;&#24230;&#35780;&#20272;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Prometheus: Inducing Fine-grained Evaluation Capability in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08491
&lt;/p&gt;
&lt;p&gt;
Prometheus&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#35780;&#20998;&#26631;&#20934;&#21644;&#36866;&#24403;&#30340;&#21442;&#32771;&#26448;&#26009;&#65292;&#21487;&#20197;&#22312;&#19982;GPT-4&#30456;&#23218;&#32654;&#30340;&#35780;&#20272;&#33021;&#21147;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#19987;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-4&#65289;&#20316;&#20026;&#38271;&#31687;&#22238;&#31572;&#30340;&#35780;&#20272;&#32773;&#24050;&#32463;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#22823;&#35268;&#27169;&#35780;&#20272;&#20219;&#21153;&#21644;&#32771;&#34385;&#33258;&#23450;&#20041;&#26631;&#20934;&#65288;&#20363;&#22914;&#20799;&#31461;&#21487;&#35835;&#24615;&#65289;&#30340;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#20351;&#29992;&#19987;&#26377;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#38381;&#28304;&#24615;&#36136;&#12289;&#26080;&#25511;&#21046;&#30340;&#29256;&#26412;&#21644;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prometheus&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#24320;&#28304;&#30340;LLM&#65292;&#21482;&#35201;&#25658;&#24102;&#36866;&#24403;&#30340;&#21442;&#32771;&#26448;&#26009;&#65288;&#21442;&#32771;&#31572;&#26696;&#12289;&#35780;&#20998;&#26631;&#20934;&#65289;&#65292;&#23601;&#21487;&#20197;&#19982;GPT-4&#30340;&#35780;&#20272;&#33021;&#21147;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#21453;&#39304;&#25910;&#38598;&#65288;Feedback Collection&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;1K&#20010;&#32454;&#31890;&#24230;&#35780;&#20998;&#26631;&#20934;&#12289;20K&#26465;&#25351;&#20196;&#21644;GPT-4&#29983;&#25104;&#30340;100K&#26465;&#21709;&#24212;&#21644;&#35821;&#35328;&#21453;&#39304;&#32452;&#25104;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#36825;&#20010;&#21453;&#39304;&#25910;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;Prometheus&#65292;&#19968;&#20010;13B&#30340;&#35780;&#20272;&#32773;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#23450;&#21046;&#30340;&#35780;&#20998;&#26631;&#20934;&#35780;&#20272;&#20219;&#20309;&#32473;&#23450;&#30340;&#38271;&#31687;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08491v2 Announce Type: replace  Abstract: Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#24037;&#20855;&#22312;&#20154;&#38469;&#20132;&#27969;&#26041;&#38754;&#30340;&#33021;&#21147;&#25345;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#21487;&#20197;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#12289;&#24110;&#21161;&#34920;&#36798;&#24819;&#27861;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#65292;&#20294;&#20063;&#25581;&#31034;&#20986;&#24037;&#20855;&#23384;&#22312;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21644;&#29992;&#25143;&#20851;&#20110;&#25216;&#26415;&#19981;&#30495;&#23454;&#24615;&#21644;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2310.03976</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#33258;&#25105;&#65306;&#29992;&#25143;&#23545;&#20154;&#24037;&#26234;&#33021;&#22312;&#20154;&#38469;&#20132;&#27969;&#21644;&#33258;&#25105;&#26041;&#38754;&#28508;&#21147;&#30340;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.03976
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#24037;&#20855;&#22312;&#20154;&#38469;&#20132;&#27969;&#26041;&#38754;&#30340;&#33021;&#21147;&#25345;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#21487;&#20197;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#12289;&#24110;&#21161;&#34920;&#36798;&#24819;&#27861;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#65292;&#20294;&#20063;&#25581;&#31034;&#20986;&#24037;&#20855;&#23384;&#22312;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21644;&#29992;&#25143;&#20851;&#20110;&#25216;&#26415;&#19981;&#30495;&#23454;&#24615;&#21644;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;AI&#20013;&#20171;&#20132;&#27969;&#65288;AIMC&#65289;&#39046;&#22495;&#20013;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#24037;&#20855;&#27491;&#25104;&#20026;&#20154;&#38469;&#20132;&#27969;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20026;&#26399;&#19968;&#21608;&#30340;&#26085;&#35760;&#21644;&#35775;&#35848;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#29992;&#25143;&#23545;&#36825;&#20123;&#24037;&#20855;&#22312;&#30701;&#26399;&#20869;&#25903;&#25345;&#20154;&#38469;&#20132;&#27969;&#30340;&#33021;&#21147;&#21644;&#21487;&#33021;&#23548;&#33268;&#30340;&#38271;&#26399;&#25928;&#26524;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21442;&#19982;&#32773;&#23545;AIMC&#25903;&#25345;&#25345;&#26377;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#20854;&#33021;&#22815;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#65292;&#24110;&#21161;&#25214;&#21040;&#20934;&#30830;&#30340;&#35821;&#35328;&#34920;&#36798;&#24819;&#27861;&#65292;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;AIMC&#24037;&#20855;&#30446;&#21069;&#23384;&#22312;&#30340;&#23616;&#38480;&#65292;&#21253;&#25324;&#21872;&#21990;&#30340;&#22238;&#22797;&#12289;&#19981;&#33258;&#28982;&#30340;&#22238;&#24212;&#20197;&#21450;&#36807;&#24230;&#24773;&#32490;&#21270;&#12290;&#36825;&#20123;&#32570;&#38519;&#36827;&#19968;&#27493;&#21463;&#21040;&#29992;&#25143;&#23545;&#19981;&#30495;&#23454;&#24615;&#21644;&#23545;&#25216;&#26415;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#25152;&#21152;&#21095;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.03976v2 Announce Type: cross  Abstract: In the rapidly evolving landscape of AI-mediated communication (AIMC), tools powered by Large Language Models (LLMs) are becoming integral to interpersonal communication. Employing a mixed-methods approach, we conducted a one-week diary and interview study to explore users' perceptions of these tools' ability to: 1) support interpersonal communication in the short-term, and 2) lead to potential long-term effects. Our findings indicate that participants view AIMC support favorably, citing benefits such as increased communication confidence, and finding precise language to express their thoughts, navigating linguistic and cultural barriers. However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology. Furthermore, we identified fou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#25955;&#27010;&#29575;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21407;&#22987;&#35821;&#38899;&#27874;&#24418;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#21512;&#25104;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21487;&#23454;&#29616;&#26080;&#38480;&#35821;&#38899;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#25903;&#25345;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35821;&#38899;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2310.01381</link><description>&lt;p&gt;
DiffAR&#65306;&#21435;&#22122;&#25193;&#25955;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#21407;&#22987;&#35821;&#38899;&#27874;&#24418;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#25955;&#27010;&#29575;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21407;&#22987;&#35821;&#38899;&#27874;&#24418;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#21512;&#25104;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21487;&#23454;&#29616;&#26080;&#38480;&#35821;&#38899;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#25903;&#25345;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35821;&#38899;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#19982;&#39640;&#36136;&#37327;&#35821;&#38899;&#29983;&#25104;&#30456;&#20851;&#12290;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#29983;&#25104;&#39057;&#35889;&#22270;&#65292;&#22240;&#27492;&#36827;&#19968;&#27493;&#38656;&#35201;&#21518;&#32493;&#27169;&#22411;&#23558;&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#27874;&#24418;&#65288;&#21363;&#22768;&#30721;&#22120;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#25955;&#27010;&#29575;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21407;&#22987;&#35821;&#38899;&#27874;&#24418;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#33258;&#22238;&#24402;&#30340;&#65292;&#25353;&#39034;&#24207;&#29983;&#25104;&#37325;&#21472;&#24103;&#65292;&#20854;&#20013;&#27599;&#20010;&#24103;&#20381;&#36182;&#20110;&#20808;&#21069;&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21512;&#25104;&#26080;&#38480;&#35821;&#38899;&#25345;&#32493;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#21512;&#25104;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20026;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35821;&#38899;&#29983;&#25104;&#23454;&#29616;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#21518;&#32773;&#21487;&#20197;&#30001;&#36755;&#20837;&#30340;&#38899;&#32032;&#24207;&#21015;&#12289;&#25391;&#24133;&#21644;&#38899;&#35843;&#20540;&#39537;&#21160;&#12290;&#30452;&#25509;&#22788;&#29702;&#27874;&#24418;&#20855;&#26377;&#19968;&#20123;&#32463;&#39564;&#20248;&#21183;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20801;&#35768;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01381v3 Announce Type: replace-cross  Abstract: Diffusion models have recently been shown to be relevant for high-quality speech generation. Most work has been focused on generating spectrograms, and as such, they further require a subsequent model to convert the spectrogram to a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic end-to-end model for generating a raw speech waveform. The proposed model is autoregressive, generating overlapping frames sequentially, where each frame is conditioned on a portion of the previously generated one. Hence, our model can effectively synthesize an unlimited speech duration while preserving high-fidelity synthesis and temporal coherence. We implemented the proposed model for unconditional and conditional speech generation, where the latter can be driven by an input sequence of phonemes, amplitudes, and pitch values. Working on the waveform directly has some empirical advantages. Specifically, it allows the creation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#21152;&#26435;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20020;&#24202;&#38754;&#35848;&#36716;&#24405;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2307.00920</link><description>&lt;p&gt;
&#22522;&#20110;&#33410;&#28857;&#21152;&#26435;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20998;&#26512;&#36716;&#24405;&#30340;&#20020;&#24202;&#38754;&#35848;&#20013;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00920
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#21152;&#26435;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20020;&#24202;&#38754;&#35848;&#36716;&#24405;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#20013;&#21152;&#26435;&#33258;&#36830;&#25509;&#36793;&#65292;&#24182;&#23637;&#31034;&#20854;&#23545;&#20174;&#36716;&#24405;&#30340;&#20020;&#24202;&#38754;&#35848;&#20013;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;GCN&#26469;&#23545;&#36716;&#24405;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#23558;&#20854;&#20998;&#31867;&#20026;&#25233;&#37057;&#30151;&#24739;&#32773;&#25110;&#23545;&#29031;&#32452;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#20943;&#36731;GCN&#20013;&#23545;&#23616;&#37096;&#24615;&#21644;&#33258;&#36830;&#25509;&#19982;&#30456;&#37051;&#33410;&#28857;&#36793;&#30340;&#31561;&#37325;&#35201;&#24615;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#35832;&#22914;&#20302;&#35745;&#31639;&#25104;&#26412;&#12289;&#25968;&#25454;&#19981;&#21487;&#30693;&#21644;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#31561;&#26377;&#21560;&#24341;&#21147;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#20934;GCN&#27169;&#22411;&#20197;&#21450;&#20808;&#21069;&#25253;&#21578;&#30340;&#32467;&#26524;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#36798;&#21040;&#20102;F1=0.84&#12290;&#26368;&#21518;&#65292;&#23450;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00920v2 Announce Type: replace-cross  Abstract: We propose a simple approach for weighting self-connecting edges in a Graph Convolutional Network (GCN) and show its impact on depression detection from transcribed clinical interviews. To this end, we use a GCN for modeling non-consecutive and long-distance semantics to classify the transcriptions into depressed or control subjects. The proposed method aims to mitigate the limiting assumptions of locality and the equal importance of self-connections vs. edges to neighboring nodes in GCNs, while preserving attractive features such as low computational cost, data agnostic, and interpretability capabilities. We perform an exhaustive evaluation in two benchmark datasets. Results show that our approach consistently outperforms the vanilla GCN model as well as previously reported results, achieving an F1=0.84 on both datasets. Finally, a qualitative analysis illustrates the interpretability capabilities of the proposed approach and 
&lt;/p&gt;</description></item><item><title>TransERR&#26159;&#19968;&#31181;&#22522;&#20110;&#32763;&#35793;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#37319;&#29992;&#36229;&#22797;&#20540;&#31354;&#38388;&#32534;&#30721;&#30693;&#35782;&#22270;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#36890;&#36807;&#36866;&#24212;&#24615;&#26059;&#36716;&#22836;&#23454;&#20307;&#21644;&#23614;&#23454;&#20307;&#26469;&#26368;&#23567;&#21270;&#32763;&#35793;&#36317;&#31163;&#65292;&#24182;&#20855;&#26377;&#26377;&#25928;&#24314;&#27169;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2306.14580</link><description>&lt;p&gt;
TransERR:&#36890;&#36807;&#39640;&#25928;&#20851;&#31995;&#26059;&#36716;&#30340;&#22522;&#20110;&#32763;&#35793;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TransERR: Translation-based Knowledge Graph Embedding via Efficient Relation Rotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14580
&lt;/p&gt;
&lt;p&gt;
TransERR&#26159;&#19968;&#31181;&#22522;&#20110;&#32763;&#35793;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#37319;&#29992;&#36229;&#22797;&#20540;&#31354;&#38388;&#32534;&#30721;&#30693;&#35782;&#22270;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#36890;&#36807;&#36866;&#24212;&#24615;&#26059;&#36716;&#22836;&#23454;&#20307;&#21644;&#23614;&#23454;&#20307;&#26469;&#26368;&#23567;&#21270;&#32763;&#35793;&#36317;&#31163;&#65292;&#24182;&#20855;&#26377;&#26377;&#25928;&#24314;&#27169;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#20851;&#31995;&#26059;&#36716;&#65288;TransERR&#65289;&#23454;&#29616;&#30340;&#22522;&#20110;&#32763;&#35793;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#36825;&#26159;&#20256;&#32479;&#22522;&#20110;&#32763;&#35793;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#19982;&#20808;&#21069;&#30340;&#22522;&#20110;&#32763;&#35793;&#27169;&#22411;&#19981;&#21516;&#65292;TransERR&#22312;&#36229;&#22797;&#20540;&#31354;&#38388;&#20013;&#23545;&#30693;&#35782;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#25366;&#25496;&#22836;&#37096;&#21644;&#23614;&#37096;&#23454;&#20307;&#20043;&#38388;&#30340;&#28508;&#22312;&#20449;&#24687;&#26102;&#20855;&#26377;&#26356;&#39640;&#31243;&#24230;&#30340;&#32763;&#35793;&#33258;&#30001;&#24230;&#12290;&#20026;&#36827;&#19968;&#27493;&#20943;&#23567;&#32763;&#35793;&#36317;&#31163;&#65292;TransERR&#36890;&#36807;&#23427;&#20204;&#21508;&#33258;&#21487;&#23398;&#20064;&#30340;&#21333;&#20301;&#22235;&#20803;&#25968;&#36866;&#24212;&#24615;&#22320;&#26059;&#36716;&#22836;&#23454;&#20307;&#21644;&#23614;&#23454;&#20307;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#25968;&#23398;&#35777;&#26126;&#26469;&#23637;&#31034;TransERR&#22312;&#24314;&#27169;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65288;&#21253;&#25324;&#23545;&#31216;&#24615;&#12289;&#21453;&#23545;&#31216;&#24615;&#12289;&#20498;&#32622;&#12289;&#21512;&#25104;&#21644;&#23376;&#20851;&#31995;&#27169;&#24335;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23545;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.14580v2 Announce Type: replace-cross  Abstract: This paper presents a translation-based knowledge geraph embedding method via efficient relation rotation (TransERR), a straightforward yet effective alternative to traditional translation-based knowledge graph embedding models. Different from the previous translation-based models, TransERR encodes knowledge graphs in the hypercomplex-valued space, thus enabling it to possess a higher degree of translation freedom in mining latent information between the head and tail entities. To further minimize the translation distance, TransERR adaptively rotates the head entity and the tail entity with their corresponding unit quaternions, which are learnable in model training. We also provide mathematical proofs to demonstrate the ability of TransERR in modeling various relation patterns, including symmetry, antisymmetry, inversion, composition, and subrelation patterns. The experiments on 10 benchmark datasets validate the effectiveness 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;&#28389;&#29992;&#65292;&#30740;&#31350;&#24341;&#20837;&#20102;&#21517;&#20026;M4&#30340;&#36328;&#39046;&#22495;&#12289;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#25581;&#31034;&#20102;&#26816;&#27979;&#22120;&#22312;&#26410;&#30693;&#39046;&#22495;&#25110;&#27169;&#22411;&#19978;&#27867;&#21270;&#30340;&#22256;&#38590;&#65292;&#25351;&#20986;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2305.14902</link><description>&lt;p&gt;
M4: &#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14902
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;&#28389;&#29992;&#65292;&#30740;&#31350;&#24341;&#20837;&#20102;&#21517;&#20026;M4&#30340;&#36328;&#39046;&#22495;&#12289;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#25581;&#31034;&#20102;&#26816;&#27979;&#22120;&#22312;&#26410;&#30693;&#39046;&#22495;&#25110;&#27169;&#22411;&#19978;&#27867;&#21270;&#30340;&#22256;&#38590;&#65292;&#25351;&#20986;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22238;&#31572;&#21508;&#31181;&#29992;&#25143;&#26597;&#35810;&#26102;&#29983;&#25104;&#27969;&#30021;&#22238;&#31572;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#25991;&#26412;&#22312;&#26032;&#38395;&#12289;&#25945;&#32946;&#21644;&#23398;&#26415;&#39046;&#22495;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21162;&#21147;&#21019;&#24314;&#21487;&#20197;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#24182;&#25351;&#20986;&#28508;&#22312;&#35823;&#29992;&#30340;&#33258;&#21160;&#31995;&#32479;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;M4&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#30340;&#29992;&#20110;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#27979;&#22120;&#24456;&#38590;&#22312;&#26469;&#33258;&#26410;&#35265;&#39046;&#22495;&#25110;LLMs&#30340;&#23454;&#20363;&#19978;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#22120;&#24448;&#24448;&#20250;&#35823;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#31867;&#20026;&#20154;&#24037;&#32534;&#20889;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#36828;&#26410;&#35299;&#20915;&#65292;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14902v2 Announce Type: replace  Abstract: Large language models (LLMs) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries. However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia. In this study, we strive to create automated systems that can detect machine-generated texts and pinpoint potential misuse. We first introduce a large-scale benchmark \textbf{M4}, which is a multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written. These results show that the problem is far from solved and that there is a lot of room for improvement. We believe that our dataset will enable future research tow
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#20132;&#21449;&#27169;&#22411;&#27604;&#36739;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30340;&#25928;&#29992;&#65292;&#23454;&#29616;&#20943;&#23569;&#20887;&#20313;&#21442;&#25968;&#21644;&#25233;&#21046;&#36755;&#20837;&#22122;&#22768;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2301.03765</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#21449;&#27169;&#22411;&#27604;&#36739;&#25439;&#22833;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#20013;&#31070;&#32463;&#20803;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.03765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#20132;&#21449;&#27169;&#22411;&#27604;&#36739;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30340;&#25928;&#29992;&#65292;&#23454;&#29616;&#20943;&#23569;&#20887;&#20313;&#21442;&#25968;&#21644;&#25233;&#21046;&#36755;&#20837;&#22122;&#22768;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#22312;&#27169;&#22411;&#35268;&#27169;&#21644;&#36755;&#20837;&#32972;&#26223;&#26041;&#38754;&#19981;&#26029;&#25193;&#22823;&#65292;&#24341;&#20837;&#20102;&#26356;&#22810;&#38544;&#34255;&#31070;&#32463;&#20803;&#21644;&#36755;&#20837;&#31070;&#32463;&#20803;&#65292;&#22823;&#20307;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#31070;&#32463;&#20803;&#24182;&#19981;&#33021;&#20026;&#25152;&#26377;&#23454;&#20363;&#24102;&#26469;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#22240;&#20026;&#19968;&#20123;&#38544;&#34255;&#31070;&#32463;&#20803;&#26159;&#20887;&#20313;&#30340;&#65292;&#28151;&#20837;&#36755;&#20837;&#31070;&#32463;&#20803;&#30340;&#22122;&#22768;&#24448;&#24448;&#20250;&#20998;&#25955;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#20391;&#37325;&#20110;&#36890;&#36807;&#38468;&#21152;&#30340;&#21518;&#22788;&#29702;&#25110;&#39044;&#22788;&#29702;&#65292;&#22914;&#32593;&#32476;&#20462;&#21098;&#21644;&#19978;&#19979;&#25991;&#36873;&#25321;&#65292;&#20174;&#22806;&#37096;&#38477;&#20302;&#20302;&#25928;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#65292;&#20197;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#25928;&#29992;&#26469;&#20351;&#27169;&#22411;&#20943;&#23569;&#20887;&#20313;&#21442;&#25968;&#24182;&#25233;&#21046;&#36755;&#20837;&#22122;&#22768;&#65311;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#31070;&#32463;&#20803;&#65292;&#37027;&#20040;&#19981;&#31649;&#21738;&#20123;&#31070;&#32463;&#20803;&#34987;&#21093;&#31163;&#65288;&#31105;&#29992;&#65289;&#65292;&#21093;&#31163;&#21518;&#30340;&#23376;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#19981;&#24212;&#35813;&#20248;&#20110;&#21407;&#22987;&#23436;&#25972;&#27169;&#22411;&#12290;&#26681;&#25454;&#36825;&#26679;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.03765v2 Announce Type: replace  Abstract: Current natural language understanding (NLU) models have been continuously scaling up, both in terms of model size and input context, introducing more hidden and input neurons. While this generally improves performance on average, the extra neurons do not yield a consistent improvement for all instances. This is because some hidden neurons are redundant, and the noise mixed in input neurons tends to distract the model. Previous work mainly focuses on extrinsically reducing low-utility neurons by additional post- or pre-processing, such as network pruning and context selection, to avoid this problem. Beyond that, can we make the model reduce redundant parameters and suppress input noise by intrinsically enhancing the utility of each neuron? If a model can efficiently utilize neurons, no matter which neurons are ablated (disabled), the ablated submodel should perform no better than the original full model. Based on such a comparison pr
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21028;&#21035;&#24335;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32467;&#21512;&#20808;&#21069;&#30740;&#31350;&#30340;&#35789;&#20856;&#20808;&#39564;&#21644;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#21452;&#35821;&#35789;&#20856;&#24402;&#32435;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20808;&#39564;&#21487;&#20197;&#25913;&#21892;&#35825;&#23548;&#30340;&#21452;&#35821;&#35789;&#20856;&#12290;</title><link>https://arxiv.org/abs/1808.09334</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21452;&#35821;&#35789;&#20856;&#24402;&#32435;&#30340;&#21028;&#21035;&#24335;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Discriminative Latent-Variable Model for Bilingual Lexicon Induction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1808.09334
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21028;&#21035;&#24335;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32467;&#21512;&#20808;&#21069;&#30740;&#31350;&#30340;&#35789;&#20856;&#20808;&#39564;&#21644;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#21452;&#35821;&#35789;&#20856;&#24402;&#32435;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20808;&#39564;&#21487;&#20197;&#25913;&#21892;&#35825;&#23548;&#30340;&#21452;&#35821;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#21452;&#35821;&#35789;&#20856;&#24402;&#32435;&#30340;&#21028;&#21035;&#24335;&#28508;&#21464;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;Haghighi&#31561;&#20154;&#65288;2008&#65289;&#30340;&#20108;&#20998;&#21305;&#37197;&#35789;&#20856;&#20808;&#39564;&#19982;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#65288;Artetxe&#31561;&#20154;&#65292;2017&#65289;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#39640;&#25928;&#30340;Viterbi EM&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#19979;&#23545;&#20845;&#31181;&#35821;&#35328;&#23545;&#36827;&#34892;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#24182;&#26174;&#31034;&#20808;&#39564;&#25913;&#21892;&#20102;&#35825;&#23548;&#30340;&#21452;&#35821;&#35789;&#20856;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#20808;&#21069;&#30340;&#24037;&#20316;&#35270;&#20026;&#31867;&#20284;&#39118;&#26684;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#23613;&#31649;&#26377;&#19981;&#21516;&#30340;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1808.09334v3 Announce Type: replace  Abstract: We introduce a novel discriminative latent variable model for bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a representation-based approach (Artetxe et al., 2017). To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical results on six language pairs under two metrics and show that the prior improves the induced bilingual lexicons. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.14151</link><description>&lt;p&gt;
&#30495;&#30693;&#26469;&#28304;&#20110;&#23454;&#36341;&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20351;LLMs&#19982;&#20855;&#36523;&#29615;&#22659;&#23545;&#40784;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20247;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#31616;&#21333;&#30340;&#20915;&#31574;&#20219;&#21153;&#19978;&#32463;&#24120;&#22833;&#36133;&#65292;&#21407;&#22240;&#26159;LLMs&#20013;&#30340;&#30693;&#35782;&#19982;&#29615;&#22659;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31574;&#30053;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22987;&#32456;&#19982;&#29615;&#22659;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#38590;&#20197;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#20854;&#20013;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWOSOME&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;RL&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#24182;&#23454;&#29616;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20934;&#22791;&#22909;&#30340;&#25968;&#25454;&#38598;&#25110;&#29615;&#22659;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#26597;&#35810;&#27599;&#20010;&#26377;&#25928;&#21160;&#20316;&#30340;&#32852;&#21512;&#27010;&#29575;&#20197;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#22686;&#24378;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#34892;&#20026;&#35780;&#20272;&#21644;&#36873;&#25321;&#31639;&#27861;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the acto
&lt;/p&gt;</description></item><item><title>&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2401.10286</link><description>&lt;p&gt;
&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#20348;&#20348;&#32773;&#65306;&#33521;&#25991;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10286
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#65292;&#20219;&#21153;&#19982;&#35757;&#32451;&#35821;&#26009;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#20849;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#21644;&#25105;&#20204;&#35774;&#35745;&#30340;&#35780;&#20272;&#25351;&#26631;&#34920;&#26126;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#19982;&#20219;&#21153;&#32039;&#23494;&#21305;&#37197;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#31243;&#24230;&#36739;&#39640;&#30340;&#20219;&#21153;&#20013;&#65292;&#23637;&#31034;&#36739;&#23569;&#20013;&#25991;&#35821;&#35328;&#29305;&#24449;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#29992;&#20195;&#30721;&#27169;&#22411;&#26367;&#25442;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#20934;&#22791;&#25968;&#25454;&#65292;&#24456;&#23481;&#26131;&#24471;&#21040;&#22797;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the alignment between tasks and training corpora is a fundamental consensus in the application of language models, our series of experiments and the metrics we designed reveal that code-based Large Language Models (LLMs) significantly outperform models trained on data that is closely matched to the tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to Chinese hallucinations, models exhibiting fewer linguistic features of the Chinese language achieve better performance. Our experimental results can be easily replicated in Chinese data processing tasks, such as preparing data for Retrieval-Augmented Generation (RAG), by simply replacing the base model with a code-based model. Additionally, our research offers a distinct perspective for discussion on the philosophical "Chinese Room" thought experiment.
&lt;/p&gt;</description></item><item><title>InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05507</link><description>&lt;p&gt;
InfiAgent-DABench: &#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#35780;&#20272;&#20195;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05507
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;"InfiAgent-DABench"&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;DAEval&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;55&#20010;CSV&#25991;&#20214;&#34893;&#29983;&#20986;&#30340;311&#20010;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#35780;&#20272;LLMs&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20195;&#29702;&#30340;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26684;&#24335;&#25552;&#31034;&#25216;&#26415;&#65292;&#30830;&#20445;&#38382;&#39064;&#26159;&#38381;&#21512;&#24418;&#24335;&#30340;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#24403;&#21069;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;DAAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#19987;&#38376;&#20195;&#29702;&#12290;InfiAgent-DABench&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#21253;&#24050;&#32463;&#21457;&#24067;&#22312;https://github.com/InfiAgent/InfiAgent&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce "InfiAgent-DABench", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks. This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents. We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated. Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks. In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;(PDDPP)&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#23454;&#29616;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#30340;&#31574;&#30053;&#21046;&#23450;&#12290;&#21033;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#26032;&#30340;&#26696;&#20363;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00262</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents. (arXiv:2311.00262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00262
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;(PDDPP)&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#23454;&#29616;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#30340;&#31574;&#30053;&#21046;&#23450;&#12290;&#21033;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#26032;&#30340;&#26696;&#20363;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26102;&#20195;&#20013;&#65292;&#20027;&#21160;&#23545;&#35805;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#35805;&#38382;&#39064;&#65292;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#26159;&#25552;&#39640;LLMs&#20027;&#21160;&#24615;&#30340;&#20851;&#38190;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26041;&#26696;&#25110;&#36890;&#36807;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#36845;&#20195;&#22686;&#24378;&#23545;LLMs&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#21463;&#38480;&#20110;&#20923;&#32467;&#30340;LLMs&#30340;&#31574;&#30053;&#35268;&#21010;&#33021;&#21147;&#65292;&#35201;&#20040;&#38590;&#20197;&#36716;&#31227;&#21040;&#26032;&#30340;&#26696;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#20197;&#20351;&#29992;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#22120;&#26469;&#21046;&#23450;LLMs&#22312;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#19978;&#30340;&#31574;&#30053;&#65292;&#21629;&#21517;&#20026;PPDPP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20415;&#21033;&#29992;&#21487;&#29992;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#33258;&#25105;&#23545;&#24328;&#25910;&#38598;&#30340;&#21160;&#24577;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#23646;&#24615;&#22270;&#30340;&#35299;&#32544;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#32593;&#32476;&#19978;&#38750;&#24120;&#24120;&#35265;&#65292;&#23545;&#20110;&#35813;&#31867;&#22270;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#31038;&#20132;&#32593;&#32476;&#30340;&#30740;&#31350;&#22312;&#32593;&#32476;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20165;&#20381;&#38752;&#25552;&#31034;&#20449;&#24687;&#26469;&#20256;&#36798;&#22270;&#32467;&#26500;&#20449;&#24687;&#32473;LLMs&#65292;&#22240;&#27492;&#23545;&#20110;TAGs&#20013;&#22797;&#26434;&#30340;&#32467;&#26500;&#20851;&#31995;&#20102;&#35299;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32544;&#22270;&#25991;&#23398;&#20064;&#22120;&#65288;DGTL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#22686;&#24378;LLMs&#23545;TAGs&#30340;&#25512;&#29702;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DGTL&#27169;&#22411;&#36890;&#36807;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#22240;&#32032;&#20013;&#38544;&#34255;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#21644;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16789</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Detecting Pretraining Data from Large Language Models. (arXiv:2310.16789v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#21644;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#29992;&#20110;&#35757;&#32451;&#23427;&#20204;&#30340;&#25968;&#25454;&#24456;&#23569;&#34987;&#20844;&#24320;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#25968;&#25454;&#30340;&#35268;&#27169;&#20043;&#22823;&#65292;&#21487;&#33021;&#21253;&#21547;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#12289;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#20197;&#21450;&#29992;&#20110;&#24191;&#27867;&#25253;&#36947;&#30340;&#21442;&#32771;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#25105;&#20204;&#20960;&#20046;&#21487;&#20197;&#32943;&#23450;&#23427;&#20204;&#21253;&#21547;&#20102;&#28508;&#22312;&#30340;&#38382;&#39064;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30446;&#21069;&#26080;&#27861;&#30693;&#36947;&#36825;&#20123;&#25991;&#26412;&#20013;&#21253;&#21547;&#20102;&#21738;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#20197;&#21450;&#27604;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#26816;&#27979;&#38382;&#39064;&#65306;&#22312;&#19981;&#30693;&#36947;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#27573;&#25991;&#26412;&#21644;&#23545;LLM&#30340;&#40657;&#30418;&#35775;&#38382;&#65292;&#25105;&#20204;&#33021;&#21542;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#26159;&#22312;&#25552;&#20379;&#30340;&#25991;&#26412;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65311;&#20026;&#20102;&#26041;&#20415;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;WIKIMIA&#65292;&#20351;&#29992;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#21644;&#20043;&#21518;&#21019;&#24314;&#30340;&#25968;&#25454;&#26469;&#25903;&#25345;&#37329;&#26631;&#20934;&#26816;&#27979;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;Min-K% Prob&#65292;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20551;&#35774;&#65306;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#20363;&#23376;&#21487;&#33021;&#21253;&#21547;&#20960;&#20010;&#20855;&#26377;&#36739;&#20302;&#27010;&#29575;&#30340;&#31163;&#32676;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low pro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.14356</link><description>&lt;p&gt;
&#25991;&#21270;&#21644;&#35821;&#35328;&#22810;&#26679;&#24615;&#25552;&#39640;&#20102;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Cultural and Linguistic Diversity Improves Visual Representations. (arXiv:2310.14356v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#36890;&#24120;&#23558;&#24863;&#30693;&#35270;&#20026;&#23458;&#35266;&#30340;&#65292;&#24182;&#19988;&#36825;&#31181;&#20551;&#35774;&#22312;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#24471;&#21040;&#21453;&#26144;&#12290;&#20363;&#22914;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#22270;&#20687;&#25551;&#36848;&#36890;&#24120;&#34987;&#20551;&#23450;&#20026;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#36328;&#25991;&#21270;&#24515;&#29702;&#23398;&#21644;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20010;&#20307;&#30340;&#35270;&#35273;&#24863;&#30693;&#22240;&#20854;&#25991;&#21270;&#32972;&#26223;&#21644;&#25152;&#35828;&#30340;&#35821;&#35328;&#32780;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#26631;&#39064;&#20013;&#65292;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#20869;&#23481;&#24046;&#24322;&#12290;&#24403;&#25968;&#25454;&#26159;&#22810;&#35821;&#35328;&#32780;&#19981;&#26159;&#21333;&#35821;&#35328;&#26102;&#65292;&#26631;&#39064;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#24179;&#22343;&#26356;&#39640;&#65292;&#20197;&#22330;&#26223;&#22270;&#12289;&#23884;&#20837;&#21644;&#35821;&#35328;&#22797;&#26434;&#24615;&#36827;&#34892;&#27979;&#37327;&#12290;&#20363;&#22914;&#65292;&#19982;&#19968;&#32452;&#21333;&#35821;&#26631;&#39064;&#30456;&#27604;&#65292;&#22810;&#35821;&#26631;&#39064;&#24179;&#22343;&#26377;21.8&#65285;&#26356;&#22810;&#30340;&#23545;&#35937;&#65292;24.5&#65285;&#26356;&#22810;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;27.1&#65285;&#26356;&#22810;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#20869;&#23481;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision often treats perception as objective, and this assumption gets reflected in the way that datasets are collected and models are trained. For instance, image descriptions in different languages are typically assumed to be translations of the same semantic content. However, work in cross-cultural psychology and linguistics has shown that individuals differ in their visual perception depending on their cultural background and the language they speak. In this paper, we demonstrate significant differences in semantic content across languages in both dataset and model-produced captions. When data is multilingual as opposed to monolingual, captions have higher semantic coverage on average, as measured by scene graph, embedding, and linguistic complexity. For example, multilingual captions have on average 21.8% more objects, 24.5% more relations, and 27.1% more attributes than a set of monolingual captions. Moreover, models trained on content from different languages perform bes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#21021;&#27493;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07225</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;: &#35266;&#23519;&#21644;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions. (arXiv:2310.07225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07225
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#21021;&#27493;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#39046;&#22495;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36890;&#36807;&#22312;&#26631;&#20934;&#21270;&#32771;&#35797;&#20013;&#21462;&#24471;&#21450;&#26684;&#20998;&#25968;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#32773;&#30340;&#24037;&#20855;&#12290;&#23558;LLMs&#37096;&#32626;&#21040;&#22914;&#27492;&#39640;&#39118;&#38505;&#30340;&#29615;&#22659;&#20013;&#38656;&#35201;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#38480;&#21046;&#26377;&#28165;&#26224;&#30340;&#29702;&#35299;&#12290;&#38543;&#30528;&#26032;&#30340;LLMs&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#21457;&#24067;&#65292;&#35782;&#21035;&#36328;&#27169;&#22411;&#23384;&#22312;&#30340;&#27169;&#24335;&#65292;&#24182;&#22240;&#27492;&#21487;&#33021;&#20986;&#29616;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;&#29305;&#21035;&#26377;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#31181;&#27969;&#34892;LLM&#22312;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#20316;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#29305;&#24615;&#12290;&#36890;&#36807;&#36825;&#20010;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in medical question answering by achieving passing scores in standardised exams and have been suggested as tools for supporting healthcare workers. Deploying LLMs into such a high-risk context requires a clear understanding of the limitations of these models. With the rapid development and release of new LLMs, it is especially valuable to identify patterns which exist across models and may, therefore, continue to appear in newer versions. In this paper, we evaluate a wide range of popular LLMs on their knowledge of medical questions in order to better understand their properties as a group. From this comparison, we provide preliminary observations and raise open questions for further research.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#20010;&#21035;&#30340;&#27979;&#35797;&#36755;&#20837;&#37325;&#26032;&#20889;&#20316;&#20219;&#21153;&#25552;&#31034;&#65292;&#20351;&#20854;&#26356;&#20855;&#20307;&#12289;&#26126;&#30830;&#21644;&#23436;&#25972;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25351;&#23548;&#65292;&#23454;&#29616;&#20102;&#32422;10%&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.02107</link><description>&lt;p&gt;
&#23454;&#20363;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#30340;&#20851;&#24576;&#65306;&#20026;&#23454;&#20363;&#37325;&#20889;&#25552;&#31034;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance. (arXiv:2310.02107v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02107
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#20010;&#21035;&#30340;&#27979;&#35797;&#36755;&#20837;&#37325;&#26032;&#20889;&#20316;&#20219;&#21153;&#25552;&#31034;&#65292;&#20351;&#20854;&#26356;&#20855;&#20307;&#12289;&#26126;&#30830;&#21644;&#23436;&#25972;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25351;&#23548;&#65292;&#23454;&#29616;&#20102;&#32422;10%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#19968;&#30452;&#26159;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#33410;&#30465;&#20154;&#21147;&#65288;&#21363;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#30340;&#27880;&#37322;&#65289;&#65307;&#22240;&#27492;&#65292;&#38646;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#20063;&#20139;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#30528;&#37325;&#20110;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#20219;&#21153;&#25351;&#23548;&#65288;&#20363;&#22914;&#8220;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#24605;&#32771;&#8221;&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#27491;&#30830;&#35299;&#20915;&#38382;&#39064;&#65292;&#27599;&#20010;&#21333;&#29420;&#30340;&#27979;&#35797;&#23454;&#20363;&#38656;&#35201;&#26356;&#20180;&#32454;&#22320;&#35774;&#35745;&#21644;&#23450;&#21046;&#30340;&#25351;&#23548;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PRoMPTd&#65292;&#19968;&#31181;&#20026;&#27599;&#20010;&#20010;&#21035;&#30340;&#27979;&#35797;&#36755;&#20837;&#37325;&#26032;&#20889;&#20316;&#20219;&#21153;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#26356;&#21152;&#20855;&#20307;&#12289;&#26126;&#30830;&#21644;&#23436;&#25972;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#20316;&#20026;&#20219;&#21153;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#28085;&#30422;&#31639;&#26415;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#31561;&#20219;&#21153;&#30340;8&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;PRoMPTd&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PRoMPTd&#22312;&#22797;&#26434;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#32477;&#23545;&#25913;&#36827;&#32422;&#20026;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling large language models (LLMs) to perform tasks in zero-shot has been an appealing goal owing to its labor-saving (i.e., requiring no task-specific annotations); as such, zero-shot prompting approaches also enjoy better task generalizability. To improve LLMs' zero-shot performance, prior work has focused on devising more effective task instructions (e.g., ``let's think step by step'' ). However, we argue that, in order for an LLM to solve them correctly in zero-shot, individual test instances need more carefully designed and customized instructions. To this end, we propose PRoMPTd, an approach that rewrites the task prompt for each individual test input to be more specific, unambiguous, and complete, so as to provide better guidance to the task LLM. We evaluated PRoMPTd on eight datasets covering tasks including arithmetics, logical reasoning, and code generation, using GPT-4 as the task LLM. Notably, PRoMPTd achieves an absolute improvement of around 10% on the complex MATH dat
&lt;/p&gt;</description></item><item><title>LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11998</link><description>&lt;p&gt;
LMSYS-Chat-1M&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11998
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19982;&#20854;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LMSYS-Chat-1M&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#19982;25&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30340;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#25105;&#20204;&#30340;Vicuna&#28436;&#31034;&#21644;Chatbot Arena&#32593;&#31449;&#19978;&#30340;21&#19975;&#20010;&#29420;&#31435;IP&#22320;&#22336;&#20013;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#20869;&#23481;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#31574;&#21010;&#36807;&#31243;&#12289;&#22522;&#26412;&#32479;&#35745;&#25968;&#25454;&#21644;&#20027;&#39064;&#20998;&#24067;&#65292;&#24378;&#35843;&#20854;&#22810;&#26679;&#24615;&#12289;&#29420;&#29305;&#24615;&#21644;&#35268;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#29992;&#20363;&#23637;&#31034;&#20102;&#23427;&#30340;&#22810;&#26679;&#24615;&#65306;&#24320;&#21457;&#19982;GPT-4&#34920;&#29616;&#30456;&#20284;&#30340;&#20869;&#23481;&#36807;&#28388;&#27169;&#22411;&#12289;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#22522;&#20934;&#12289;&#35757;&#32451;&#19982;Vicuna&#34920;&#29616;&#30456;&#20284;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12289;&#21019;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25105;&#20204;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is pub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10444</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#33258;&#25105;&#24378;&#21270;&#20197;&#25913;&#36827;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#28041;&#21450;&#23398;&#29983;&#29983;&#25104;&#21644;&#20998;&#20139;&#23398;&#20064;&#36164;&#28304;&#12290;&#22312;&#23398;&#29983;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#21019;&#24314;&#35299;&#37322;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#23545;&#30456;&#20851;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#24448;&#24448;&#30001;&#20110;&#20027;&#39064;&#29702;&#35299;&#26377;&#38480;&#21644;&#20165;&#20165;&#37325;&#30003;&#38382;&#39064;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#27491;&#30830;&#31572;&#26696;&#30340;&#20542;&#21521;&#32780;&#38590;&#20197;&#32534;&#20889;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24110;&#21161;&#25903;&#25745;&#36825;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#29983;&#25104;&#19982;&#23398;&#29983;&#23545;&#40784;&#30340;&#35299;&#37322;&#65292;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#20197;&#30830;&#20445;&#20854;&#36136;&#37327;&#65292;&#24182;&#36845;&#20195;&#22686;&#24378;&#35299;&#37322;&#12290;&#22914;&#26524;&#19968;&#20010;&#35299;&#37322;&#30340;&#35780;&#20272;&#20998;&#25968;&#20302;&#20110;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#26694;&#26550;&#20250;&#36845;&#20195;&#22320;&#20248;&#21270;&#21644;&#37325;&#26032;&#35780;&#20272;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#19968;&#20010;&#23398;&#29983;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.10435</link><description>&lt;p&gt;
&#37325;&#22609;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65306;&#21033;&#29992;&#20869;&#23481;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#23398;&#20064;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;
&lt;/p&gt;
&lt;p&gt;
Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#22312;&#32447;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#39034;&#24207;&#25512;&#33616;&#30001;&#20110;&#20854;&#34920;&#36798;&#33021;&#21147;&#24378;&#22823;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;&#32780;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39034;&#24207;&#24314;&#27169;&#26041;&#27861;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#30340;&#21407;&#22240;&#26159;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#23545;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#29289;&#21697;&#30456;&#20851;&#25991;&#26412;&#20869;&#23481;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;LANCER&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20135;&#29983;&#20102;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#23545;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2309.03883</link><description>&lt;p&gt;
DoLa&#65306;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03883
&lt;/p&gt;
&lt;p&gt;
DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#19982;&#39044;&#35757;&#32451;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#20107;&#23454;&#20559;&#31163;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#20943;&#23569;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#24187;&#35273;&#65292;&#23427;&#19981;&#38656;&#35201;&#22312;&#26816;&#32034;&#30340;&#22806;&#37096;&#30693;&#35782;&#25110;&#39069;&#22806;&#30340;&#24494;&#35843;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23558;&#36739;&#26202;&#23618;&#21644;&#36739;&#26089;&#23618;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#24471;&#21040;&#30340;&#36923;&#36753;&#24046;&#24322;&#26469;&#33719;&#24471;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#20998;&#24067;&#65292;&#21033;&#29992;&#20102;LLMs&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#36890;&#24120;&#34987;&#35777;&#26126;&#23616;&#37096;&#21270;&#22312;&#29305;&#23450;&#30340;Transformer&#23618;&#20013;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#30340;&#35299;&#30721;&#65288;DoLa&#65289;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23637;&#31034;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#20943;&#23569;&#29983;&#25104;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#24773;&#20917;&#12290;DoLa&#22312;&#22810;&#20010;&#36873;&#25321;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#21319;&#20102;&#30495;&#23454;&#24615;&#65292;&#20363;&#22914;&#25913;&#21892;&#20102;LLaMA&#31995;&#21015;&#27169;&#22411;&#22312;TruthfulQA&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA b
&lt;/p&gt;</description></item><item><title>CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.01940</link><description>&lt;p&gt;
CodeApex&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. (arXiv:2309.01940v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01940
&lt;/p&gt;
&lt;p&gt;
CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#30340;&#32534;&#31243;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#26085;&#30410;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CodeApex&#65292;&#19968;&#31181;&#21452;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#19987;&#27880;&#20110;LLM&#30340;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;CodeApex&#21253;&#25324;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65306;&#27010;&#24565;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#36339;&#25512;&#29702;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#32534;&#31243;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;CodeApex&#21033;&#29992;&#31639;&#27861;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;&#27979;&#35797;&#29992;&#20363;&#26469;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;14&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#65292;&#21253;&#25324;&#36890;&#29992;&#21644;&#19987;&#38376;&#21270;&#27169;&#22411;&#12290;GPT&#23637;&#29616;&#20986;&#26368;&#20339;&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;&#32422;50%&#21644;56%&#12290;&#32534;&#31243;&#20219;&#21153;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#24076;&#26395;CodeApex&#33021;&#22815;&#20026;&#35780;&#20272;&#32534;&#31243;&#33021;&#21147;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. We propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension and code generation abilities of LLMs. CodeApex comprises three types of multiple-choice questions: conceptual understanding, commonsense reasoning, and multi-hop reasoning, designed to evaluate LLMs on programming comprehension tasks. Additionally, CodeApex utilizes algorithmic questions and corresponding test cases to assess the code quality generated by LLMs. We evaluate 14 state-of-the-art LLMs, including both general-purpose and specialized models. GPT exhibits the best programming capabilities, achieving approximate accuracies of 50% and 56% on the two tasks, respectively. There is still significant room for improvement in programming tasks. We hope that CodeApex can serve as a reference for evaluating the co
&lt;/p&gt;</description></item><item><title>MatchXML&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;label2vec&#26041;&#27861;&#29983;&#25104;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#23884;&#20837;&#26500;&#24314;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65292;MatchXML&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#21462;&#20986;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#38745;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.13139</link><description>&lt;p&gt;
MatchXML: &#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification. (arXiv:2308.13139v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13139
&lt;/p&gt;
&lt;p&gt;
MatchXML&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;label2vec&#26041;&#27861;&#29983;&#25104;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#23884;&#20837;&#26500;&#24314;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65292;MatchXML&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#21462;&#20986;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#38745;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;XMC&#65289;&#26159;&#25351;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#20174;&#19968;&#20010;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#26631;&#31614;&#38598;&#20013;&#65288;&#20363;&#22914;&#25968;&#30334;&#19975;&#20010;&#26631;&#31614;&#65289;&#20026;&#25991;&#26412;&#26679;&#26412;&#20998;&#37197;&#30456;&#20851;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MatchXML&#65292;&#19968;&#31181;&#29992;&#20110;XMC&#30340;&#39640;&#25928;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30001;&#31232;&#30095;&#30340;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#29305;&#24449;&#29983;&#25104;&#30340;&#26631;&#31614;&#23884;&#20837;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;label2vec&#65292;&#36890;&#36807;Skip-gram&#27169;&#22411;&#26469;&#26377;&#25928;&#35757;&#32451;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36825;&#20123;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#26469;&#26500;&#24314;&#19968;&#20010;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;Transformer&#26102;&#65292;&#25105;&#20204;&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#22312;&#20108;&#20998;&#22270;&#20013;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20174;&#24494;&#35843;&#21518;&#30340;Transformer&#20013;&#25552;&#21462;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#38500;&#20102;&#24494;&#35843;&#21518;&#30340;&#23494;&#38598;&#25991;&#26412;&#23884;&#20837;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#39044;&#35757;&#32451;&#30340;Sentence Transformer&#20013;&#25552;&#21462;&#38745;&#24577;&#30340;&#23494;&#38598;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The eXtreme Multi-label text Classification(XMC) refers to training a classifier that assigns a text sample with relevant labels from an extremely large-scale label set (e.g., millions of labels). We propose MatchXML, an efficient text-label matching framework for XMC. We observe that the label embeddings generated from the sparse Term Frequency-Inverse Document Frequency(TF-IDF) features have several limitations. We thus propose label2vec to effectively train the semantic dense label embeddings by the Skip-gram model. The dense label embeddings are then used to build a Hierarchical Label Tree by clustering. In fine-tuning the pre-trained encoder Transformer, we formulate the multi-label text classification as a text-label matching problem in a bipartite graph. We then extract the dense text representations from the fine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also extract the static dense sentence embeddings from a pre-trained Sentence Transformer. Finally,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#30340;&#20004;&#31181;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20934;&#30830;&#23545;&#40784;&#38899;&#39057;&#21644;&#35270;&#39057;&#27969;&#65292;&#24182;&#19988;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08488</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder. (arXiv:2308.08488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#30340;&#20004;&#31181;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20934;&#30830;&#23545;&#40784;&#38899;&#39057;&#21644;&#35270;&#39057;&#27969;&#65292;&#24182;&#19988;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#35266;&#23519;&#21040;&#65292;&#22312;&#20302;&#36136;&#37327;&#35270;&#39057;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#19979;&#65292;&#20174;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21040;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#30053;&#26377;&#25913;&#36827;&#12290;&#25454;&#35748;&#20026;&#65292;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#19987;&#38376;&#30340;&#36755;&#20837;&#34920;&#31034;&#23548;&#33268;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#65288;AVSR&#65289;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#35757;&#32451;&#26694;&#26550;&#19979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26222;&#36890;&#35805;&#20013;&#22068;&#21767;&#24418;&#29366;&#21644;&#38899;&#33410;&#32423;&#38899;&#32032;&#23383;&#21333;&#20803;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#24314;&#31435;&#20934;&#30830;&#30340;&#24103;&#32423;&#38899;&#33410;&#36793;&#30028;&#12290;&#36825;&#20351;&#24471;&#22312;&#35270;&#35273;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#36807;&#31243;&#20013;&#33021;&#22815;&#23545;&#40784;&#35270;&#39057;&#21644;&#38899;&#39057;&#27969;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;&#24341;&#23548;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#65288;CMFE&#65289;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#20027;&#35201;&#35757;&#32451;&#21442;&#25968;&#26469;&#23454;&#29616;&#22810;&#20010;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#23618;&#30340;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;&#22312;&#23454;&#39564;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.09476</link><description>&lt;p&gt;
&#36807;&#24230;&#24605;&#32771;&#30495;&#30456;&#65306;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#22797;&#26434;&#27169;&#24335;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#37325;&#29616;&#19981;&#20934;&#30830;&#25110;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#30740;&#31350;&#26377;&#23475;&#30340;&#27169;&#20223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#30456;&#20851;&#29616;&#35937;&#65306;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#12290;&#31532;&#19968;&#20010;&#29616;&#35937;&#65292;&#36807;&#24230;&#24605;&#32771;&#65292;&#22312;&#32473;&#20986;&#27491;&#30830;&#19982;&#38169;&#35823;&#30340;&#23569;&#37327;&#31034;&#33539;&#26102;&#65292;&#25105;&#20204;&#20174;&#20013;&#38388;&#23618;&#35299;&#30721;&#39044;&#27979;&#12290;&#22312;&#26089;&#26399;&#23618;&#20013;&#65292;&#20004;&#31181;&#31034;&#33539;&#24341;&#36215;&#20102;&#30456;&#20284;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#20294;&#22312;&#26576;&#20010;&#8220;&#20851;&#38190;&#23618;&#8221;&#20043;&#21518;&#65292;&#32473;&#20986;&#38169;&#35823;&#31034;&#33539;&#30340;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#12290;&#31532;&#20108;&#20010;&#29616;&#35937;&#65292;&#38169;&#35823;&#24402;&#32435;&#22836;&#65292;&#21487;&#33021;&#26159;&#36807;&#24230;&#24605;&#32771;&#30340;&#19968;&#31181;&#26426;&#21046;&#24615;&#21407;&#22240;&#65306;&#36825;&#20123;&#26159;&#20301;&#20110;&#36739;&#26202;&#23618;&#30340;&#22836;&#37096;&#65292;&#23427;&#20204;&#20851;&#27880;&#24182;&#22797;&#21046;&#20808;&#21069;&#31034;&#33539;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20854;&#21066;&#24369;&#20250;&#20943;&#23569;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20998;&#26512;&#20102;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#27969;&#34892;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#36739;&#22810;&#30340;&#38169;&#35823;&#27880;&#37322;&#12289;&#20559;&#35265;&#25110;&#27880;&#37322;&#20266;&#20687;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#22312;&#36825;&#19968;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2307.08153</link><description>&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#20998;&#26512;&#25968;&#25454;&#38598;&#27880;&#37322;&#36136;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Analyzing Dataset Annotation Quality Management in the Wild. (arXiv:2307.08153v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20998;&#26512;&#20102;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#27969;&#34892;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#36739;&#22810;&#30340;&#38169;&#35823;&#27880;&#37322;&#12289;&#20559;&#35265;&#25110;&#27880;&#37322;&#20266;&#20687;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#22312;&#36825;&#19968;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#20934;&#30830;&#12289;&#20844;&#27491;&#21644;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#27491;&#30830;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#27969;&#34892;&#25968;&#25454;&#38598;&#20013;&#20063;&#23384;&#22312;&#22823;&#37327;&#30340;&#38169;&#35823;&#27880;&#37322;&#12289;&#20559;&#35265;&#25110;&#27880;&#37322;&#20266;&#20687;&#12290;&#20851;&#20110;&#27880;&#37322;&#39033;&#30446;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#25351;&#21335;&#24050;&#32463;&#23384;&#22312;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#36827;&#34892;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;&#21019;&#24314;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#26102;&#23454;&#38469;&#36827;&#34892;&#30340;&#36136;&#37327;&#31649;&#29702;&#20197;&#21450;&#26159;&#21542;&#36981;&#24490;&#20102;&#36825;&#20123;&#24314;&#35758;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#24182;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#25512;&#33616;&#36136;&#37327;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#23454;&#36341;&#30340;&#24314;&#35758;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#30001;591&#31687;&#31185;&#23398;&#20986;&#29256;&#29289;&#32452;&#25104;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#35821;&#26009;&#24211;&#65292;&#24182;&#38024;&#23545;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#26041;&#38754;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#20363;&#22914;&#27880;&#37322;&#32773;&#31649;&#29702;&#12289;&#19968;&#33268;&#24615;&#12289;&#20210;&#35009;&#25110;&#25968;&#25454;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data quality is crucial for training accurate, unbiased, and trustworthy machine learning models and their correct evaluation. Recent works, however, have shown that even popular datasets used to train and evaluate state-of-the-art models contain a non-negligible amount of erroneous annotations, bias or annotation artifacts. There exist best practices and guidelines regarding annotation projects. But to the best of our knowledge, no large-scale analysis has been performed as of yet on how quality management is actually conducted when creating natural language datasets and whether these recommendations are followed. Therefore, we first survey and summarize recommended quality management practices for dataset creation as described in the literature and provide suggestions on how to apply them. Then, we compile a corpus of 591 scientific publications introducing text datasets and annotate it for quality-related aspects, such as annotator management, agreement, adjudication or data validat
&lt;/p&gt;</description></item><item><title>DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.03067</link><description>&lt;p&gt;
DeepOnto: &#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
DeepOnto: A Python Package for Ontology Engineering with Deep Learning. (arXiv:2307.03067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03067
&lt;/p&gt;
&lt;p&gt;
DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#22312;&#26412;&#20307;&#24037;&#31243;&#20013;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;PyTorch&#21644;Tensorflow&#20027;&#35201;&#26159;&#20026;Python&#24320;&#21457;&#30340;&#65292;&#32780;&#24191;&#27867;&#20351;&#29992;&#30340;&#26412;&#20307;API&#65288;&#22914;OWL API&#21644;Jena&#65289;&#20027;&#35201;&#26159;&#22522;&#20110;Java&#30340;&#12290;&#20026;&#20102;&#26041;&#20415;&#26080;&#32541;&#38598;&#25104;&#36825;&#20123;&#26694;&#26550;&#21644;API&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deeponto&#65292;&#19968;&#20010;&#19987;&#20026;&#26412;&#20307;&#24037;&#31243;&#35774;&#35745;&#30340;Python&#21253;&#12290;&#35813;&#21253;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#35748;&#21487;&#21644;&#21487;&#38752;&#30340;OWL API&#30340;&#26680;&#24515;&#26412;&#20307;&#22788;&#29702;&#27169;&#22359;&#65292;&#20197;&#26356;&#8220;Pythonic&#8221;&#30340;&#26041;&#24335;&#23553;&#35013;&#20854;&#22522;&#26412;&#29305;&#24615;&#65292;&#24182;&#25193;&#23637;&#20854;&#21151;&#33021;&#20197;&#21253;&#25324;&#20854;&#20182;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#35821;&#35328;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#25237;&#24433;&#31561;&#12290;&#22522;&#20110;&#36825;&#20010;&#27169;&#22359;&#65292;Deeponto&#25552;&#20379;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#21508;&#31181;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#20363;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more "Pythonic" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22840;&#24352;&#26816;&#27979;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00209</link><description>&lt;p&gt;
&#22270;&#20687;&#30340;&#37325;&#35201;&#24615;&#65306;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22840;&#24352;&#26816;&#27979;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22840;&#24352;&#65292;&#21363;&#22840;&#22823;&#20854;&#35789;&#65292;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#22840;&#24352;&#26816;&#27979;&#26159;&#29702;&#35299;&#20154;&#31867;&#34920;&#36798;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#24050;&#32463;&#26377;&#20960;&#39033;&#20851;&#20110;&#22840;&#24352;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#30340;&#30740;&#31350;&#21482;&#20851;&#27880;&#25991;&#26412;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#21457;&#23637;&#65292;&#20154;&#20204;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#27169;&#24577;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#31561;&#65289;&#26469;&#34920;&#36798;&#22840;&#24352;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#12290;&#25105;&#20204;&#20174;&#24494;&#21338;&#65288;&#20013;&#22269;&#30340;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#19968;&#20123;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#24494;&#21338;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#35270;&#20026;&#20004;&#31181;&#27169;&#24577;&#65292;&#25506;&#32034;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#22312;&#22840;&#24352;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#36825;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#20116;&#20010;&#19981;&#21516;&#30340;&#20027;&#39064;&#26500;&#24314;&#30340;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection of hyperbole is an important part of understanding human expression. There have been several studies on hyperbole detection, but most of which focus on text modality only. However, with the development of social media, people can create hyperbolic expressions with various modalities, including text, images, videos, etc. In this paper, we focus on multimodal hyperbole detection. We create a multimodal detection dataset\footnote{The dataset will be released to the community.} from Weibo (a Chinese social media) and carry out some studies on it. We treat the text and image from a piece of weibo as two modalities and explore the role of text and image for hyperbole detection. Different pre-trained multimodal encoders are also evaluated on this downstream task to show their performance. Besides, since this dataset is constructed from five different topics, we also evaluate the cross-domain performance of different 
&lt;/p&gt;</description></item><item><title>Xiezhi&#26159;&#19968;&#31181;&#20840;&#38754;&#32508;&#21512;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#35774;&#26377;516&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#35206;&#30422;&#20102;&#20174;13&#20010;&#19981;&#21516;&#23398;&#31185;&#36328;&#36234;&#30340;15&#20010;&#19987;&#19994;&#39046;&#22495;&#65292;&#24182;&#23545;47&#20010;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#22823;&#22810;&#25968;&#39046;&#22495;&#36229;&#36234;&#20154;&#31867;&#65292;&#20294;&#22312;&#19968;&#20123;&#39046;&#22495;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.05783</link><description>&lt;p&gt;
Xiezhi&#65306;&#19968;&#31181;&#20840;&#38754;&#26356;&#26032;&#30340;&#32508;&#21512;&#39046;&#22495;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. (arXiv:2306.05783v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05783
&lt;/p&gt;
&lt;p&gt;
Xiezhi&#26159;&#19968;&#31181;&#20840;&#38754;&#32508;&#21512;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#35774;&#26377;516&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#35206;&#30422;&#20102;&#20174;13&#20010;&#19981;&#21516;&#23398;&#31185;&#36328;&#36234;&#30340;15&#20010;&#19987;&#19994;&#39046;&#22495;&#65292;&#24182;&#23545;47&#20010;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#22823;&#22810;&#25968;&#39046;&#22495;&#36229;&#36234;&#20154;&#31867;&#65292;&#20294;&#22312;&#19968;&#20123;&#39046;&#22495;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24613;&#38656;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22522;&#20934;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Xiezhi&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#32508;&#21512;&#39046;&#22495;&#30693;&#35782;&#12290;Xiezhi&#21253;&#25324;&#36328;&#36234;13&#20010;&#19981;&#21516;&#23398;&#31185;&#30340;516&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#21253;&#25324;22&#19975;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#38468;&#24102;Xiezhi-Specialty&#21644;Xiezhi-Interdiscipline&#65292;&#22343;&#26377;15,000&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;47&#20010;&#20808;&#36827;&#30340;LLM&#22312;Xiezhi&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#31185;&#23398;&#12289;&#24037;&#31243;&#12289;&#20892;&#23398;&#12289;&#21307;&#23398;&#21644;&#33402;&#26415;&#26041;&#38754;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#24179;&#22343;&#34920;&#29616;&#65292;&#20294;&#22312;&#32463;&#27982;&#23398;&#12289;&#27861;&#23398;&#12289;&#25945;&#32946;&#23398;&#12289;&#25991;&#23398;&#12289;&#21382;&#21490;&#21644;&#31649;&#29702;&#26041;&#38754;&#21017;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#26399;&#26395;Xiezhi&#23558;&#26377;&#21161;&#20110;&#20998;&#26512;LLM&#30340;&#37325;&#35201;&#20248;&#28857;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#35813;&#22522;&#20934;&#24050;&#22312;https://github.com/MikeGu721/XiezhiBenchmark&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
New Natural Langauge Process~(NLP) benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present Xiezhi, the most comprehensive evaluation suite designed to assess holistic domain knowledge. Xiezhi comprises multiple-choice questions across 516 diverse disciplines ranging from 13 different subjects with 220,000 questions and accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management. We anticipate Xiezhi will help analyze important strengths and shortcomings of LLMs, and the benchmark is released in https://github.com/MikeGu721/XiezhiBenchmark .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#26694;&#26550;&#65292;&#21363;LLMs&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;&#65288;LATM&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#33258;&#20027;&#22320;&#21019;&#24314;&#29992;&#20110;&#35299;&#20915;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#22806;&#37096;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.17126</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Tool Makers. (arXiv:2305.17126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#26694;&#26550;&#65292;&#21363;LLMs&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;&#65288;LATM&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#33258;&#20027;&#22320;&#21019;&#24314;&#29992;&#20110;&#35299;&#20915;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#22806;&#37096;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#20854;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#26041;&#38754;&#30340;&#20808;&#21069;&#24037;&#20316;&#20381;&#36182;&#20110;&#29616;&#26377;&#24037;&#20855;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#26694;&#26550;&#65292;&#31216;&#20026;LLMs As Tool Makers&#65288;LATM&#65289;&#65292;&#20197;&#28040;&#38500;&#36825;&#31181;&#20381;&#36182;&#24615;&#65292;&#20854;&#20013;LLMs&#21019;&#24314;&#33258;&#24049;&#30340;&#21487;&#37325;&#29992;&#24037;&#20855;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;1&#65289;&#21046;&#36896;&#24037;&#20855;&#65306;LLM&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;&#65292;&#20026;&#32473;&#23450;&#20219;&#21153;&#21046;&#20316;&#24037;&#20855;&#65292;&#20854;&#20013;&#24037;&#20855;&#20316;&#20026;Python&#23454;&#29992;&#20989;&#25968;&#23454;&#29616;&#12290;2&#65289;&#20351;&#29992;&#24037;&#20855;&#65306;LLM&#20316;&#20026;&#24037;&#20855;&#29992;&#25143;&#65292;&#24212;&#29992;&#24037;&#20855;&#21046;&#36896;&#32773;&#26500;&#24314;&#30340;&#24037;&#20855;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#24037;&#20855;&#29992;&#25143;&#21487;&#20197;&#26159;&#19982;&#24037;&#20855;&#21046;&#36896;&#32773;&#30456;&#21516;&#25110;&#19981;&#21516;&#30340;LLM&#12290;&#24037;&#20855;&#21046;&#36896;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#29983;&#25104;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#35831;&#27714;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#23558;&#26469;&#35831;&#27714;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#33021;&#35843;&#29992;&#30456;&#24212;&#30340;API&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research shows the potential of enhancing the problem-solving ability of large language models (LLMs) through the use of external tools. However, prior work along this line depends on the availability of existing tools. In this work, we take an initial step towards removing this dependency by proposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two key phases: 1) tool making: an LLM acts as the tool maker that crafts tools for given tasks, where a tool is implemented as a Python utility function. 2) tool using: an LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#30772;&#35299;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#20917;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#65292;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13860</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study. (arXiv:2305.13860v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#30772;&#35299;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#20917;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#65292;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#19982;&#20869;&#23481;&#32422;&#26463;&#21644;&#28508;&#22312;&#28389;&#29992;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#31350;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#21487;&#20197;&#29992;&#22810;&#23569;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31867;&#22411;&#30772;&#35299;LLMs&#65292;&#65288;2&#65289;&#30772;&#35299;&#25552;&#31034;&#22312;&#35268;&#36991;LLM&#38480;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#65288;3&#65289;ChatGPT&#23545;&#36825;&#20123;&#30772;&#35299;&#25552;&#31034;&#30340;&#38887;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#27169;&#22411;&#26469;&#20998;&#26512;&#29616;&#26377;&#25552;&#31034;&#30340;&#20998;&#24067;&#65292;&#35782;&#21035;&#20986;&#21313;&#20010;&#19981;&#21516;&#27169;&#24335;&#21644;&#19977;&#20010;&#30772;&#35299;&#25552;&#31034;&#31867;&#21035;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;3,120&#20010;&#31105;&#27490;&#24773;&#26223;&#19979;&#30340;&#29425;&#20013;&#38382;&#39064;&#25968;&#25454;&#38598;&#35780;&#20272;ChatGPT 3.5&#21644;4.0&#29256;&#26412;&#30340;&#30772;&#35299;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#23545;&#30772;&#35299;&#25552;&#31034;&#30340;&#25269;&#25239;&#21147;&#65292;&#21457;&#29616;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#26223;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;LLMs&#23545;&#24847;&#22806;&#28389;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in j
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;CoNNs&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#25277;&#35937;&#35268;&#21017;&#30340;&#39046;&#22495;&#12290;&#26041;&#27861;&#31216;&#20026;&#8220;&#31070;&#32463;&#29702;&#35299;&#8221;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#35268;&#21017;&#25512;&#29702;&#12289;&#31639;&#26415;&#25512;&#29702;&#31561;&#26041;&#38754;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01665</link><description>&lt;p&gt;
&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#31070;&#32463;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Neural Comprehension: Language Models with Compiled Neural Networks. (arXiv:2304.01665v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;CoNNs&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#25277;&#35937;&#35268;&#21017;&#30340;&#39046;&#22495;&#12290;&#26041;&#27861;&#31216;&#20026;&#8220;&#31070;&#32463;&#29702;&#35299;&#8221;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#35268;&#21017;&#25512;&#29702;&#12289;&#31639;&#26415;&#25512;&#29702;&#31561;&#26041;&#38754;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#65292;&#20294;&#20854;&#36827;&#34892;&#31526;&#21495;&#25805;&#20316;&#21644;&#31639;&#26415;&#25805;&#20316;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#24402;&#22240;&#20110;&#23427;&#20204;&#38544;&#24335;&#22320;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#35268;&#21017;&#12290;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#23558;&#29305;&#21035;&#35774;&#35745;&#24471;&#21040;&#30340;&#21152;&#26435;&#30340;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#65288;CoNNs&#65289;&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#20013;&#65292;&#20351;&#24471;&#36890;&#36807;&#26799;&#24230;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#23436;&#20840;&#30340;&#35268;&#21017;&#29702;&#35299;&#33021;&#21147;&#12290;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#20837;&#20026;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#25277;&#35937;&#35268;&#21017;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#8220;&#31070;&#32463;&#29702;&#35299;&#8221;&#65292;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#35268;&#21017;&#25512;&#29702;&#12289;&#31639;&#26415;&#25512;&#29702;&#31561;&#26041;&#38754;&#23454;&#29616;&#32477;&#23545;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#65306;\url{https://github.com/...}
&lt;/p&gt;
&lt;p&gt;
Language models have achieved impressive results in natural language processing tasks, but their ability to perform symbolic operations and arithmetic operations, remains limited, which attribute to their learn the rules implicitly from data. We explore how to incorporate compiled neural networks (CoNNs) which weight is specially designed, into the architecture of language models to enable the language model trained by gradient to obtain fully rule comprehension ability. The incorporation of compiled neural networks offers a promising direction for improving the performance of language models on compound tasks, particularly in areas that require a deeper comprehension of abstract rules beyond recognizing patterns in training data. Our method, which call "Neural Comprehension", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning. Our code is publicly available at: \url{ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TransFusion&#26550;&#26500;&#65292;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24635;&#32467;&#21160;&#20316;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#23545;&#35937;&#20132;&#20114;&#30340;&#39044;&#27979;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.09209</link><description>&lt;p&gt;
&#24635;&#32467;&#36807;&#21435;&#20197;&#39044;&#27979;&#26410;&#26469;&#65306;&#33258;&#28982;&#35821;&#35328;&#23545;&#22330;&#26223;&#30340;&#25551;&#36848;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35937;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction. (arXiv:2301.09209v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TransFusion&#26550;&#26500;&#65292;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24635;&#32467;&#21160;&#20316;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#23545;&#35937;&#20132;&#20114;&#30340;&#39044;&#27979;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#20013;&#30340;&#23545;&#35937;&#20132;&#20114;&#39044;&#27979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#35813;&#20219;&#21153;&#38656;&#35201;&#29702;&#35299;&#20808;&#21069;&#23545;&#23545;&#35937;&#25191;&#34892;&#30340;&#21160;&#20316;&#25152;&#24418;&#25104;&#30340;&#26102;&#31354;&#19978;&#19979;&#25991;&#65292;&#31216;&#20026;&#21160;&#20316;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;transformer&#30340;&#26550;&#26500;TransFusion&#12290;&#23427;&#21033;&#29992;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23545;&#21160;&#20316;&#19978;&#19979;&#25991;&#36827;&#34892;&#24635;&#32467;&#12290;TransFusion&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20174;&#36807;&#21435;&#30340;&#35270;&#39057;&#24103;&#20013;&#25552;&#21462;&#21160;&#20316;&#19978;&#19979;&#25991;&#12290;&#23558;&#36825;&#20010;&#21160;&#20316;&#19978;&#19979;&#25991;&#19982;&#19979;&#19968;&#20010;&#35270;&#39057;&#24103;&#19968;&#36215;&#32463;&#36807;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#39044;&#27979;&#19979;&#19968;&#20010;&#23545;&#35937;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21017;&#22686;&#21152;&#20102;&#36890;&#29992;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;Ego4D&#21644;EPIC-KITCHENS-100&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20063;&#20984;&#26174;&#20102;&#22312;&#19968;&#20010;&#35270;&#35273;&#20284;&#20046;&#36275;&#22815;&#30340;&#20219;&#21153;&#20013;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#25688;&#35201;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study object interaction anticipation in egocentric videos. This task requires an understanding of the spatiotemporal context formed by past actions on objects, coined action context. We propose TransFusion, a multimodal transformer-based architecture. It exploits the representational power of language by summarising the action context. TransFusion leverages pre-trained image captioning and vision-language models to extract the action context from past video frames. This action context together with the next video frame is processed by the multimodal fusion module to forecast the next object interaction. Our model enables more efficient end-to-end learning. The large pre-trained language models add common sense and a generalisation capability. Experiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our multimodal fusion model. They also highlight the benefits of using language-based context summaries in a task where vision seems to suffice. Our method outperforms state-
&lt;/p&gt;</description></item><item><title>CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09935</link><description>&lt;p&gt;
CAPE: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
CAPE: Corrective Actions from Precondition Errors using Large Language Models. (arXiv:2211.09935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09935
&lt;/p&gt;
&lt;p&gt;
CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24120;&#35782;&#30693;&#35782;&#20026;&#35774;&#35745;&#26234;&#33021;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#22312;&#34892;&#21160;&#22833;&#36133;&#26102;&#26080;&#27861;&#24674;&#22797;&#65292;&#24182;&#19988;&#36890;&#24120;&#21482;&#33021;&#23581;&#35797;&#37325;&#26032;&#25191;&#34892;&#22833;&#36133;&#30340;&#34892;&#21160;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65288;CAPE&#65289;&#65292;&#35797;&#22270;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#25552;&#20986;&#32416;&#27491;&#21069;&#32622;&#26465;&#20214;&#38169;&#35823;&#30340;&#34892;&#21160;&#12290;CAPE&#36890;&#36807;&#21033;&#29992;&#23569;&#26679;&#26412;&#25512;&#29702;&#20174;&#34892;&#21160;&#21069;&#32622;&#26465;&#20214;&#20013;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#30830;&#20445;&#35821;&#20041;&#27491;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#37325;&#26032;&#25552;&#31034;&#12290;&#22312;VirtualHome&#20013;&#65292;CAPE&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#24182;&#19988;&#30456;&#27604;SayCan&#65292;&#23558;&#20154;&#24037;&#26631;&#27880;&#30340;&#35745;&#21010;&#27491;&#30830;&#24230;&#25351;&#26631;&#20174;28.89%&#25552;&#39640;&#21040;49.63%&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#20063;&#36866;&#29992;&#20110;&#19968;&#21488;&#37197;&#32622;&#20102;&#19968;&#32452;&#20197;&#35821;&#35328;&#20026;&#25351;&#23450;&#30340;&#25216;&#33021;&#21644;&#30456;&#20851;&#21069;&#32622;&#26465;&#20214;&#30340;&#27874;&#22763;&#39039;&#21160;&#21147;&#20844;&#21496;&#30340;Spot&#26426;&#22120;&#20154;&#65292;&#20854;&#20013;CAPE&#25552;&#39640;&#20102;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause.  We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctne
&lt;/p&gt;</description></item></channel></rss>