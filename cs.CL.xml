<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16184</link><description>&lt;p&gt;
&#20851;&#20110;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#35821;&#20041;&#23398;&#65306;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Semantics of LM Latent Space: A Vocabulary-defined Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#25913;&#36827;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20998;&#26512;&#24448;&#24448;&#22312;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#23545;LM&#35821;&#20041;&#30340;&#20998;&#31163;&#27934;&#23519;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#24573;&#35270;&#20102;LM&#36866;&#24212;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#21709;&#24212;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#65292;&#23427;&#22312;LM&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20132;&#32455;&#20998;&#26512;&#65292;&#21033;&#29992;LM&#35789;&#27719;&#26469;&#33719;&#24471;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#65292;&#24378;&#35843;&#21487;&#24494;&#20998;&#24615;&#21644;&#23616;&#37096;&#31561;&#36317;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#36827;&#34892;&#35821;&#20041;&#26657;&#20934;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25991;&#26412;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#21333;&#20010;&#20799;&#31461;&#30340;&#35821;&#35328;&#36755;&#20837;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24418;&#25104;&#21477;&#27861;&#21644;&#35821;&#20041;&#35789;&#32676;&#65292;&#24182;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07899</link><description>&lt;p&gt;
&#20174;&#21333;&#19968;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#30340;&#21487;&#23398;&#20064;&#24615;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A systematic investigation of learnability from single child linguistic input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07899
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#21333;&#20010;&#20799;&#31461;&#30340;&#35821;&#35328;&#36755;&#20837;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24418;&#25104;&#21477;&#27861;&#21644;&#35821;&#20041;&#35789;&#32676;&#65292;&#24182;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#29983;&#25104;&#35821;&#35328;&#36830;&#36143;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102; remarkable proficiency&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#19982;&#20154;&#31867;&#35821;&#35328;&#21487;&#23398;&#20064;&#24615;&#30340;&#30456;&#20851;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#19982;&#20799;&#31461;&#25509;&#25910;&#21040;&#30340;&#35821;&#35328;&#36755;&#20837;&#20043;&#38388;&#23384;&#22312;&#30528;&#26174;&#33879;&#24046;&#36317;&#12290;LMs&#36890;&#24120;&#22312;&#25968;&#37327;&#32423;&#19978;&#26356;&#22823;&#19988;&#26412;&#36136;&#19982;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#19981;&#21516;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#22312;&#21333;&#20010;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#30340;&#23376;&#38598;&#19978;&#35757;&#32451;LMs&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#35757;&#32451;&#30340;LMs&#21487;&#20197;&#24418;&#25104;&#21477;&#27861;&#21644;&#35821;&#20041;&#35789;&#32676;&#65292;&#24182;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#20165;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#20799;&#31461;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;LSTMs&#21644;&#26356;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#26816;&#39564;&#20174;&#21333;&#19968;&#20799;&#31461;&#36755;&#20837;&#21487;&#23398;&#20064;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#8230;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematicall
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30452;&#25509;&#21407;&#21017;&#21453;&#39304;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;LLM&#34892;&#20026;&#12290;&#36890;&#36807;&#22312;&#25209;&#35780;&#21644;&#20462;&#35746;&#19978;&#30452;&#25509;&#20351;&#29992;DPO&#26469;&#36339;&#36807;&#21709;&#24212;&#30340;&#25490;&#21517;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#8220;&#31881;&#33394;&#22823;&#35937;&#38382;&#39064;&#8221;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.07896</link><description>&lt;p&gt;
&#20351;&#29992;&#30452;&#25509;&#21407;&#21017;&#21453;&#39304;&#25233;&#21046;&#8220;&#31881;&#33394;&#22823;&#35937;&#8221;
&lt;/p&gt;
&lt;p&gt;
Suppressing Pink Elephants with Direct Principle Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30452;&#25509;&#21407;&#21017;&#21453;&#39304;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;LLM&#34892;&#20026;&#12290;&#36890;&#36807;&#22312;&#25209;&#35780;&#21644;&#20462;&#35746;&#19978;&#30452;&#25509;&#20351;&#29992;DPO&#26469;&#36339;&#36807;&#21709;&#24212;&#30340;&#25490;&#21517;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#8220;&#31881;&#33394;&#22823;&#35937;&#38382;&#39064;&#8221;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;&#26041;&#27861;&#65292;&#22914;RLHF&#21644;&#23466;&#27861;AI&#65292;&#28041;&#21450;&#30830;&#23450;LLM&#34892;&#20026;&#30340;&#21487;&#21462;&#20043;&#22788;&#65292;&#24182;&#23558;&#20854;&#35757;&#32451;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24076;&#26395;LLM&#22312;&#25512;&#29702;&#26102;&#26159;&#21487;&#25511;&#21046;&#30340;&#65292;&#36825;&#26679;&#21487;&#20197;&#22312;&#22810;&#31181;&#38656;&#35201;&#30340;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#29992;&#8220;&#31881;&#33394;&#22823;&#35937;&#38382;&#39064;&#8221;&#20316;&#20026;&#20363;&#23376;&#65306;&#25351;&#31034;LLM&#36991;&#20813;&#35752;&#35770;&#26576;&#20010;&#29305;&#23450;&#23454;&#20307;&#65288;&#8220;&#31881;&#33394;&#22823;&#35937;&#8221;&#65289;&#65292;&#32780;&#26159;&#35752;&#35770;&#39318;&#36873;&#23454;&#20307;&#65288;&#8220;&#28784;&#33394;&#22823;&#35937;&#8221;&#65289;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Constitutional AI&#31616;&#21270;&#26041;&#27861;&#65292;&#8220;&#30452;&#25509;&#21407;&#21017;&#21453;&#39304;&#8221;&#65292;&#23427;&#36339;&#36807;&#20102;&#23545;&#21709;&#24212;&#30340;&#25490;&#21517;&#65292;&#30452;&#25509;&#22312;&#25209;&#35780;&#21644;&#20462;&#35746;&#19978;&#20351;&#29992;DPO&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25105;&#20204;&#21512;&#25104;&#30340;&#8220;&#31881;&#33394;&#22823;&#35937;&#8221;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;DPF&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;13B&#24494;&#35843;LLaMA 2&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;Llama-2-13B-Chat&#21644;&#25552;&#31034;&#22522;&#32447;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#8220;&#31881;&#33394;&#22823;&#35937;&#38382;&#39064;&#8221;&#30340;&#31934;&#24515;&#36873;&#25321;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#19982;GPT-4&#19968;&#26679;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods for controlling language models, such as RLHF and Constitutional AI, involve determining which LLM behaviors are desirable and training them into a language model. However, in many cases, it is desirable for LLMs to be controllable \textit{at inference time}, so that they can be used in multiple contexts with diverse needs. We illustrate this with the \textbf{Pink Elephant Problem}: instructing an LLM to avoid discussing a certain entity (a ``Pink Elephant''), and instead discuss a preferred entity (``Grey Elephant''). We apply a novel simplification of Constitutional AI, \textbf{Direct Principle Feedback}, which skips the ranking of responses and uses DPO directly on critiques and revisions. Our results show that after DPF fine-tuning on our synthetic Pink Elephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms Llama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on our curated test set assessing the Pink Elephant Problem.
&lt;/p&gt;</description></item><item><title>DiffUse&#26159;&#19968;&#31181;&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32858;&#31867;&#25991;&#26412;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#24182;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07891</link><description>&lt;p&gt;
&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Label-Efficient Model Selection for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07891
&lt;/p&gt;
&lt;p&gt;
DiffUse&#26159;&#19968;&#31181;&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32858;&#31867;&#25991;&#26412;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#24182;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32473;&#23450;&#30446;&#26631;&#20219;&#21153;&#30340;&#27169;&#22411;&#36873;&#25321;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#38656;&#35201;&#23545;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#30340;&#36136;&#37327;&#36827;&#34892;&#24191;&#27867;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DiffUse&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#20505;&#36873;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;DiffUse&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#22312;&#35780;&#20272;&#20013;&#23453;&#36149;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;DiffUse&#36890;&#36807;&#32858;&#31867;&#34920;&#31034;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#26234;&#33021;&#36873;&#25321;&#23454;&#20363;&#12290;&#22240;&#27492;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#20986;&#19968;&#20123;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#20363;&#23376;&#26469;&#36827;&#34892;&#20559;&#22909;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#21160;&#24577;&#30830;&#23450;&#35201;&#27880;&#37322;&#30340;&#23454;&#20363;&#25968;&#37327;&#12290;&#36890;&#36807;&#23545;&#25968;&#30334;&#20010;&#27169;&#22411;&#23545;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DiffUse&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#65292;&#26368;&#22810;&#21487;&#20943;&#23569;75%&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#35780;&#20272;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models. DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PIVOT&#30340;&#26032;&#39062;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#30340;&#35270;&#35273;&#38382;&#31572;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;VLMs&#38382;&#39064;&#12290;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;&#22270;&#20687;&#34987;&#26631;&#27880;&#20026;VLMs&#21487;&#20197;&#21442;&#32771;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36873;&#25321;&#26368;&#20339;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;VLMs&#36827;&#34892;&#26426;&#22120;&#20154;&#25511;&#21046;&#21644;&#20854;&#20182;&#31354;&#38388;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.07872</link><description>&lt;p&gt;
PIVOT: &#36845;&#20195;&#35270;&#35273;&#25552;&#31034;&#28608;&#21457;&#21487;&#25805;&#20316;&#30693;&#35782;&#29992;&#20110;VLMs
&lt;/p&gt;
&lt;p&gt;
PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PIVOT&#30340;&#26032;&#39062;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#30340;&#35270;&#35273;&#38382;&#31572;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;VLMs&#38382;&#39064;&#12290;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;&#22270;&#20687;&#34987;&#26631;&#27880;&#20026;VLMs&#21487;&#20197;&#21442;&#32771;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36873;&#25321;&#26368;&#20339;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;VLMs&#36827;&#34892;&#26426;&#22120;&#20154;&#25511;&#21046;&#21644;&#20854;&#20182;&#31354;&#38388;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26174;&#31034;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20174;&#36923;&#36753;&#25512;&#29702;&#21040;&#35270;&#35273;&#29702;&#35299;&#12290;&#36825;&#20026;&#19982;&#19990;&#30028;&#36827;&#34892;&#26356;&#20016;&#23500;&#30340;&#20114;&#21160;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;VLMs&#21482;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#32780;&#26426;&#22120;&#20154;&#25511;&#21046;&#21644;&#20854;&#20182;&#31354;&#38388;&#20219;&#21153;&#38656;&#35201;&#36755;&#20986;&#36830;&#32493;&#30340;&#22352;&#26631;&#65292;&#21160;&#20316;&#25110;&#36712;&#36857;&#12290;&#25105;&#20204;&#22914;&#20309;&#22312;&#19981;&#23545;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20351;VLMs&#33021;&#22815;&#22788;&#29702;&#36825;&#31181;&#35774;&#32622;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;VLMs&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#36845;&#20195;&#35270;&#35273;&#20248;&#21270;&#25552;&#31034;&#65288;PIVOT&#65289;&#65292;&#23558;&#20219;&#21153;&#35270;&#20026;&#36845;&#20195;&#30340;&#35270;&#35273;&#38382;&#31572;&#12290;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;&#22270;&#20687;&#34987;&#27880;&#37322;&#20026;VLMs&#21487;&#20197;&#21442;&#32771;&#30340;&#25552;&#26696;&#30340;&#35270;&#35273;&#34920;&#31034;&#65288;&#20363;&#22914;&#20505;&#36873;&#26426;&#22120;&#20154;&#21160;&#20316;&#12289;&#23450;&#20301;&#25110;&#36712;&#36857;&#65289;&#12290;&#28982;&#21518;&#65292;VLMs&#36873;&#25321;&#26368;&#20339;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#25552;&#26696;&#32463;&#36807;&#36845;&#20195;&#20248;&#21270;&#65292;&#20351;VLMs&#26368;&#32456;&#25214;&#21040;&#26368;&#20339;&#30340;&#21487;&#29992;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32454;&#31890;&#24230;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#31890;&#24230;&#20316;&#20026;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#35843;&#25972;&#31890;&#24230;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MoE&#27169;&#22411;&#22312;&#25928;&#26524;&#19978;&#22987;&#32456;&#20248;&#20110;&#23494;&#38598;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#30340;&#22686;&#22823;&#65292;&#23494;&#38598;&#21644;MoE&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#24046;&#36317;&#20063;&#22312;&#22686;&#22823;&#12290;&#21516;&#26102;&#65292;&#23558;MoE&#20013;&#19987;&#23478;&#30340;&#22823;&#23567;&#35774;&#32622;&#20026;&#19982;&#21069;&#39304;&#23618;&#30456;&#21516;&#30340;&#24120;&#35265;&#20570;&#27861;&#22312;&#20960;&#20046;&#20219;&#20309;&#35745;&#31639;&#39044;&#31639;&#19979;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.07871</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Fine-Grained Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32454;&#31890;&#24230;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#31890;&#24230;&#20316;&#20026;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#35843;&#25972;&#31890;&#24230;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MoE&#27169;&#22411;&#22312;&#25928;&#26524;&#19978;&#22987;&#32456;&#20248;&#20110;&#23494;&#38598;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#30340;&#22686;&#22823;&#65292;&#23494;&#38598;&#21644;MoE&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#24046;&#36317;&#20063;&#22312;&#22686;&#22823;&#12290;&#21516;&#26102;&#65292;&#23558;MoE&#20013;&#19987;&#23478;&#30340;&#22823;&#23567;&#35774;&#32622;&#20026;&#19982;&#21069;&#39304;&#23618;&#30456;&#21516;&#30340;&#24120;&#35265;&#20570;&#27861;&#22312;&#20960;&#20046;&#20219;&#20309;&#35745;&#31639;&#39044;&#31639;&#19979;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#24050;&#25104;&#20026;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#32435;&#20837;&#20102;&#26356;&#24191;&#27867;&#30340;&#21464;&#37327;&#33539;&#22260;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#31216;&#20026;&#31890;&#24230;&#65292;&#36890;&#36807;&#35843;&#25972;&#31890;&#24230;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#32454;&#31890;&#24230;MoE&#30340;&#26631;&#24230;&#24459;&#65292;&#32771;&#34385;&#20102;&#35757;&#32451;&#26631;&#35760;&#25968;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#31890;&#24230;&#12290;&#21033;&#29992;&#36825;&#20123;&#23450;&#24459;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#32473;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#30340;&#26368;&#20339;&#35757;&#32451;&#37197;&#32622;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#20165;&#34920;&#26126;MoE&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#23494;&#38598;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#32780;&#19988;&#36824;&#20984;&#26174;&#20102;&#22312;&#25193;&#22823;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#26102;&#65292;&#23494;&#38598;&#21644;MoE&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#24046;&#36317;&#22312;&#25193;&#22823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;MoE&#20013;&#19987;&#23478;&#30340;&#22823;&#23567;&#35774;&#32622;&#20026;&#19982;&#21069;&#39304;&#23618;&#30456;&#21516;&#30340;&#24120;&#35265;&#20570;&#27861;&#22312;&#20960;&#20046;&#20219;&#20309;&#35745;&#31639;&#39044;&#31639;&#19979;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07865</link><description>&lt;p&gt;
&#36879;&#35270;VLMs&#65306;&#25506;&#32034;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#23545;&#35805;&#12289;&#22330;&#26223;&#29702;&#35299;&#21644;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#65292;&#36825;&#31181;&#24212;&#29992;&#20419;&#20351;&#20102;&#20687;LLaVa&#12289;InstructBLIP&#21644;PaLI-3&#31561;&#35768;&#22810;&#26032;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#26377;&#36825;&#20040;&#22810;&#26032;&#30340;&#21457;&#24067;&#65292;&#20294;&#20851;&#20110;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#26550;&#26500;&#21644;&#20248;&#21270;&#30340;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#36825;&#19968;&#25361;&#25112;&#21448;&#22240;&#32570;&#20047;&#23458;&#35266;&#12289;&#19968;&#33268;&#30340;&#35780;&#20272;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#35270;&#35273;&#38382;&#31572;&#12289;&#20174;&#35821;&#35328;&#20013;&#23450;&#20301;&#29289;&#20307;&#20197;&#21450;&#25506;&#32034;&#24187;&#35273;&#31561;&#23646;&#24615;&#30340;&#30446;&#26631;&#25361;&#25112;&#38598;&#65292;&#36825;&#20123;&#35780;&#20272;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;VLM&#33021;&#21147;&#30340;&#31934;&#32454;&#12289;&#20934;&#30830;&#30340;&#35265;&#35299;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20851;&#38190;&#30340;&#35774;&#35745;&#36724;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20351;&#29992;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07862</link><description>&lt;p&gt;
AI&#22686;&#24378;&#39044;&#27979;&#65306;LLM&#21161;&#25163;&#25552;&#39640;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#19982;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#22686;&#24378;&#21028;&#26029;&#21147;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;GPT-4-Turbo&#21161;&#25163;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#26088;&#22312;&#25552;&#20379;&#39640;&#36136;&#37327;&#24314;&#35758;&#65288;&#36229;&#32423;&#39044;&#27979;&#65289;&#65292;&#21478;&#19968;&#20010;&#26088;&#22312;&#36807;&#20110;&#33258;&#20449;&#21644;&#22522;&#26412;&#27010;&#29575;&#24573;&#35270;&#12290;&#21442;&#19982;&#32773;&#65288;N = 991&#65289;&#21487;&#20197;&#22312;&#25972;&#20010;&#30740;&#31350;&#36807;&#31243;&#20013;&#21672;&#35810;&#20182;&#20204;&#34987;&#20998;&#37197;&#30340;LLM&#21161;&#25163;&#65292;&#32780;&#23545;&#29031;&#32452;&#21017;&#20351;&#29992;&#19968;&#20010;&#36739;&#20302;&#32423;&#21035;&#30340;&#27169;&#22411;&#65288;DaVinci-003&#65289;&#65292;&#19981;&#25552;&#20379;&#30452;&#25509;&#30340;&#39044;&#27979;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#27880;&#20876;&#20998;&#26512;&#26174;&#31034;&#65292;LLM&#22686;&#24378;&#26174;&#33879;&#25552;&#39640;&#20102;23%&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#20219;&#20309;&#19968;&#31181;&#21161;&#25163;&#31867;&#22411;&#65292;&#30456;&#27604;&#20110;&#23545;&#29031;&#32452;&#12290;&#36825;&#31181;&#25913;&#36827;&#21457;&#29983;&#22312;&#36229;&#32423;&#39044;&#27979;&#21161;&#25163;&#22312;&#39044;&#27979;&#20013;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#26126;&#22686;&#24378;&#30340;&#25928;&#30410;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy. Explora
&lt;/p&gt;</description></item><item><title>Lissard&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.07859</link><description>&lt;p&gt;
Lissard&#65306;&#38271;&#32780;&#31616;&#21333;&#30340;&#39034;&#24207;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Lissard: Long and Simple Sequential Reasoning Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07859
&lt;/p&gt;
&lt;p&gt;
Lissard&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#29616;&#22312;&#33021;&#22815;&#35299;&#20915;&#38656;&#35201;&#22788;&#29702;&#25968;&#21313;&#19975;&#20010;&#26631;&#35760;&#30340;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#38656;&#35201;&#37325;&#22797;&#20351;&#29992;&#31616;&#21333;&#35268;&#21017;&#30340;&#20219;&#21153;&#19978;&#24120;&#24120;&#22833;&#36133;&#65292;&#29978;&#33267;&#22312;&#27604;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#24207;&#21015;&#35201;&#30701;&#24471;&#22810;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20363;&#22914;&#65292;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#22312;&#20004;&#20010;&#21015;&#34920;&#20013;&#25214;&#21040;&#20849;&#21516;&#39033;&#65292;&#21015;&#34920;&#20013;&#30340;&#39033;&#26368;&#22810;&#21487;&#36798;20&#20010;&#65292;&#20294;&#26159;&#24403;&#21015;&#34920;&#20013;&#30340;&#39033;&#36798;&#21040;80&#20010;&#26102;&#65292;&#23427;&#20204;&#20250;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Lissard&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24320;&#28304;&#27169;&#22411;&#65288;Mistral-7B&#21644;Mixtral-8x7B&#65289;&#21644;&#19987;&#26377;&#27169;&#22411;&#65288;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/unicamp-dl/Lissard&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at https://github.com/unicamp-dl/Lissard
&lt;/p&gt;</description></item><item><title>Mercury&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLM&#20195;&#30721;&#32508;&#21512;&#20219;&#21153;&#30340;&#25928;&#29575;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Beyond@K&#26469;&#34913;&#37327;&#24402;&#19968;&#21270;&#30340;&#20195;&#30721;&#25928;&#29575;&#65292;&#20174;&#32780;&#40723;&#21169;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.07844</link><description>&lt;p&gt;
Mercury: &#19968;&#31181;&#29992;&#20110;LLM&#20195;&#30721;&#32508;&#21512;&#25928;&#29575;&#35780;&#20272;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Mercury: An Efficiency Benchmark for LLM Code Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07844
&lt;/p&gt;
&lt;p&gt;
Mercury&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLM&#20195;&#30721;&#32508;&#21512;&#20219;&#21153;&#30340;&#25928;&#29575;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Beyond@K&#26469;&#34913;&#37327;&#24402;&#19968;&#21270;&#30340;&#20195;&#30721;&#25928;&#29575;&#65292;&#20174;&#32780;&#40723;&#21169;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#20195;&#30721;&#32508;&#21512;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#21151;&#33021;&#27491;&#30830;&#24615;&#19978;&#65292;&#24573;&#35270;&#20102;&#20195;&#30721;&#25928;&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Mercury&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#29992;&#20110;&#35780;&#20272;LLM&#20195;&#30721;&#32508;&#21512;&#20219;&#21153;&#30340;&#20195;&#30721;&#25928;&#29575;&#30340;&#22522;&#20934;&#12290;Mercury&#30001;1,889&#20010;&#28085;&#30422;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#32534;&#31243;&#20219;&#21153;&#32452;&#25104;&#65292;&#36824;&#21253;&#25324;&#29983;&#25104;&#26080;&#38480;&#26696;&#20363;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#22120;&#65292;&#20197;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;Mercury&#38598;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Beyond@K&#65292;&#20197;&#22522;&#20110;&#21382;&#21490;&#25552;&#20132;&#26469;&#34913;&#37327;&#24402;&#19968;&#21270;&#30340;&#20195;&#30721;&#25928;&#29575;&#65292;&#20174;&#32780;&#20026;&#20195;&#30721;&#32508;&#21512;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#40723;&#21169;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20195;&#30721;&#65292;&#20307;&#29616;&#20102;&#29616;&#23454;&#19990;&#30028;&#36719;&#20214;&#24320;&#21457;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;LLM&#34920;&#29616;&#20986;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#20195;&#30721;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25928;&#29575;&#36755;&#20986;&#26041;&#38754;&#20173;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in evaluating Large Language Models (LLMs) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency. We present Mercury, the first benchmark designated for assessing the code efficiency of LLM code synthesis tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. Unlike existing benchmarks, Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, unde
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#22312;&#22823;&#37096;&#20998;&#35774;&#32622;&#20013;&#65292;&#25915;&#20987;&#20960;&#20046;&#21482;&#33021;&#27604;&#38543;&#26426;&#29468;&#27979;&#31245;&#22909;&#65292;&#36825;&#31181;&#31967;&#31957;&#30340;&#24615;&#33021;&#26159;&#30001;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#23569;&#37327;&#35757;&#32451;&#36845;&#20195;&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;&#25104;&#21592;&#21644;&#38750;&#25104;&#21592;&#20043;&#38388;&#30340;&#36793;&#30028;&#22256;&#24785;&#25152;&#23548;&#33268;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.07841</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26159;&#21542;&#22863;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Membership Inference Attacks Work on Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#22312;&#22823;&#37096;&#20998;&#35774;&#32622;&#20013;&#65292;&#25915;&#20987;&#20960;&#20046;&#21482;&#33021;&#27604;&#38543;&#26426;&#29468;&#27979;&#31245;&#22909;&#65292;&#36825;&#31181;&#31967;&#31957;&#30340;&#24615;&#33021;&#26159;&#30001;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#23569;&#37327;&#35757;&#32451;&#36845;&#20195;&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;&#25104;&#21592;&#21644;&#38750;&#25104;&#21592;&#20043;&#38388;&#30340;&#36793;&#30028;&#22256;&#24785;&#25152;&#23548;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#35797;&#22270;&#39044;&#27979;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#23646;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#23545;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#19978;&#23545;MIA&#30340;&#30740;&#31350;&#24037;&#20316;&#20173;&#26377;&#38480;&#12290;&#25105;&#20204;&#23545;&#22312;Pile&#19978;&#35757;&#32451;&#30340;&#19968;&#31995;&#21015;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;MIA&#35780;&#20272;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;160M&#21040;12B&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;LLM&#22823;&#23567;&#21644;&#39046;&#22495;&#30340;&#22823;&#22810;&#25968;&#35774;&#32622;&#20013;&#65292;MIAs&#20960;&#20046;&#21482;&#33021;&#27604;&#38543;&#26426;&#29468;&#27979;&#31245;&#22909;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#21457;&#29616;&#65292;&#36825;&#31181;&#31967;&#31957;&#30340;&#24615;&#33021;&#21487;&#20197;&#24402;&#22240;&#20110;&#65288;1&#65289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#23569;&#37327;&#35757;&#32451;&#36845;&#20195;&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;&#65288;2&#65289;&#25104;&#21592;&#21644;&#38750;&#25104;&#21592;&#20043;&#38388;&#30340;&#36793;&#30028;&#22256;&#24785;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;LLMs&#26131;&#21463;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#29305;&#23450;&#35774;&#32622;&#65292;&#24182;&#34920;&#26126;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#21462;&#24471;&#30340;&#34920;&#38754;&#19978;&#30340;&#25104;&#21151;&#21487;&#20197;&#24402;&#22240;&#20110;&#20998;&#24067;&#30340;&#36716;&#21464;&#65292;&#20363;&#22914;&#24403;&#25104;&#21592;&#21644;&#38750;&#25104;&#21592;&#34987;&#32472;&#21046;&#20986;&#26469;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs). We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn
&lt;/p&gt;</description></item><item><title>Aya&#26159;&#19968;&#20010;&#24320;&#25918;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;101&#31181;&#35821;&#35328;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#22810;&#35821;&#35328;&#35780;&#20272;&#30340;&#25216;&#26415;&#65292;&#24182;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#20248;&#21270;&#24494;&#35843;&#32452;&#21512;&#12289;&#25968;&#25454;&#20462;&#21098;&#20197;&#21450;&#27169;&#22411;&#30340;&#27602;&#24615;&#12289;&#20559;&#24046;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07827</link><description>&lt;p&gt;
Aya&#27169;&#22411;&#65306;&#19968;&#20010;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;&#24320;&#25918;&#22810;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07827
&lt;/p&gt;
&lt;p&gt;
Aya&#26159;&#19968;&#20010;&#24320;&#25918;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;101&#31181;&#35821;&#35328;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#22810;&#35821;&#35328;&#35780;&#20272;&#30340;&#25216;&#26415;&#65292;&#24182;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#20248;&#21270;&#24494;&#35843;&#32452;&#21512;&#12289;&#25968;&#25454;&#20462;&#21098;&#20197;&#21450;&#27169;&#22411;&#30340;&#27602;&#24615;&#12289;&#20559;&#24046;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31361;&#30772;&#20027;&#35201;&#38598;&#20013;&#22312;&#23569;&#25968;&#25968;&#25454;&#20016;&#23500;&#30340;&#35821;&#35328;&#19978;&#12290;&#22914;&#20309;&#25299;&#23485;&#23545;&#31361;&#30772;&#24615;&#25104;&#26524;&#30340;&#35775;&#38382;&#33539;&#22260;&#20197;&#35206;&#30422;&#38750;&#20027;&#27969;&#35821;&#35328;&#21602;&#65311;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;Aya&#65292;&#19968;&#20010;&#25903;&#25345;101&#31181;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#36229;&#36807;50&#65285;&#30340;&#35821;&#35328;&#34987;&#35748;&#20026;&#26159;&#36164;&#28304;&#36739;&#23569;&#30340;&#12290;Aya&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;mT0&#21644;BLOOMZ&#65292;&#21516;&#26102;&#35206;&#30422;&#30340;&#35821;&#35328;&#25968;&#37327;&#26159;&#23427;&#20204;&#30340;&#20004;&#20493;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#27867;&#30340;&#26032;&#35780;&#20272;&#22871;&#20214;&#65292;&#25193;&#23637;&#20102;99&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#31181;&#35780;&#20272;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20854;&#20013;&#21253;&#25324;&#21306;&#20998;&#21644;&#29983;&#25104;&#20219;&#21153;&#12289;&#20154;&#24037;&#35780;&#20272;&#20197;&#21450;&#27169;&#25311;&#30340;&#32988;&#29575;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20445;&#30041;&#20219;&#21153;&#21644;&#20998;&#24067;&#20869;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26368;&#20339;&#24494;&#35843;&#28151;&#21512;&#29289;&#32452;&#25104;&#12289;&#25968;&#25454;&#20462;&#21098;&#20197;&#21450;&#27169;&#22411;&#30340;&#27602;&#24615;&#12289;&#20559;&#24046;&#21644;&#23433;&#20840;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#24320;&#28304;&#22312;https://hf.co/CohereForAI/aya&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#27880;&#20837;Wiktionary&#26469;&#25913;&#21892;&#35789;&#32423;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#22312;Word-In-Context&#65288;WiC&#65289;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07817</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#27880;&#20837;Wiktionary&#25913;&#21892;&#35789;&#32423;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Injecting Wiktionary to improve token-level contextual representations using contrastive learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#27880;&#20837;Wiktionary&#26469;&#25913;&#21892;&#35789;&#32423;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#22312;Word-In-Context&#65288;WiC&#65289;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38745;&#24577;&#35789;&#23884;&#20837;&#23545;&#19978;&#19979;&#25991;&#26159;&#26080;&#24863;&#30340;&#65292;&#20294;&#23545;&#20110;&#35789;&#27719;&#35821;&#20041;&#20219;&#21153;&#26469;&#35828;&#65292;&#19978;&#19979;&#25991;&#22312;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#20013;&#36807;&#20110;&#26126;&#26174;&#65292;&#30456;&#21516;&#21547;&#20041;&#30340;&#35789;&#21521;&#37327;&#24046;&#24322;&#36739;&#22823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#21033;&#29992;&#33258;&#21160;&#33258;&#22686;&#31034;&#20363;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22914;&#20309;&#23558;&#35789;&#20856;&#20316;&#20026;&#26367;&#20195;&#30340;&#30417;&#30563;&#36164;&#28304;&#27880;&#20837;&#65292;&#20351;&#29992;&#33521;&#25991;Wiktionary&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#38477;&#32500;&#23545;&#32467;&#26524;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#65288;&#19981;&#20351;&#29992;&#35757;&#32451;&#38598;&#65289;&#22312;Word-In-Context&#65288;WiC&#65289;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21407;&#22987;WiC&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;SoTA&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;WiC&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24494;&#35843;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#22312;&#35821;&#20041;&#26694;&#26550;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#65292;&#23613;&#31649;&#25928;&#26524;&#36739;&#20026;&#28201;&#21644;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;E&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b). In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary. We also test how dimensionality reduction impacts the resulting contextual word embeddings. We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set). We achieve new SoTA result on the original WiC test set. We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements. We also observe improvements, although modest, for the semantic frame induction task. Although we experimented on E
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07812</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#24605;&#32500;&#36807;&#31243;&#20316;&#20026;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Thought Process as Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07812
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#36741;&#21161;&#20154;&#31867;&#24182;&#23637;&#29616;&#20986;"&#26234;&#33021;&#30340;&#28779;&#33457;"&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20960;&#20010;&#24320;&#25918;&#25361;&#25112;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65306;&#22914;&#23545;&#38544;&#31169;&#30340;&#20851;&#27880;&#12289;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273;&#12289;&#38590;&#20197;&#22788;&#29702;&#38271;&#25991;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;(RATP)&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#33719;&#21462;&#22806;&#37096;&#30693;&#35782;&#65292;RATP&#23558;LLM&#30340;&#24605;&#32771;&#29983;&#25104;&#36807;&#31243;&#23450;&#24335;&#20026;&#22810;&#27493;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24605;&#32771;&#36807;&#31243;&#65292;RATP&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#22788;&#29702;&#20855;&#26377;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;LLM&#35757;&#32451;&#26041;&#27861;&#21463;&#21040;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;RATP&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#25628;&#32034;&#20013;&#30340;&#25991;&#26412;&#21305;&#37197;&#31995;&#32479;&#36827;&#34892;&#20102;&#22810;&#24847;&#22270;&#23646;&#24615;&#24863;&#30693;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#24847;&#22270;&#24314;&#27169;&#26469;&#21033;&#29992;&#22810;&#20010;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24847;&#22270;&#20174;&#23646;&#24615;&#20013;&#25552;&#21462;&#65292;&#24635;&#32467;&#20102;&#26597;&#35810;&#30340;&#22810;&#26679;&#21270;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.07788</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#20013;&#30340;&#22810;&#24847;&#22270;&#23646;&#24615;&#24863;&#30693;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Multi-Intent Attribute-Aware Text Matching in Searching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#25628;&#32034;&#20013;&#30340;&#25991;&#26412;&#21305;&#37197;&#31995;&#32479;&#36827;&#34892;&#20102;&#22810;&#24847;&#22270;&#23646;&#24615;&#24863;&#30693;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#24847;&#22270;&#24314;&#27169;&#26469;&#21033;&#29992;&#22810;&#20010;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24847;&#22270;&#20174;&#23646;&#24615;&#20013;&#25552;&#21462;&#65292;&#24635;&#32467;&#20102;&#26597;&#35810;&#30340;&#22810;&#26679;&#21270;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21305;&#37197;&#31995;&#32479;&#24050;&#25104;&#20026;&#22823;&#22810;&#25968;&#25628;&#32034;&#24179;&#21488;&#30340;&#22522;&#26412;&#26381;&#21153;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#36127;&#36131;&#23558;&#29992;&#25143;&#26597;&#35810;&#21305;&#37197;&#21040;&#30456;&#20851;&#30340;&#20505;&#36873;&#39033;&#65292;&#25110;&#23558;&#29992;&#25143;&#36755;&#20837;&#30340;&#26597;&#35810;&#25913;&#20889;&#20026;&#39044;&#36873;&#30340;&#39640;&#24615;&#33021;&#26597;&#35810;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#25628;&#32034;&#20307;&#39564;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#26597;&#35810;&#21644;&#39033;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#23646;&#24615;&#65292;&#20363;&#22914;&#39033;&#30340;&#31867;&#21035;&#21644;&#26597;&#35810;&#20013;&#25552;&#21040;&#30340;&#20301;&#32622;&#65292;&#36825;&#20123;&#23646;&#24615;&#20195;&#34920;&#30528;&#26377;&#21161;&#20110;&#21305;&#37197;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#23558;&#23646;&#24615;&#38598;&#25104;&#21040;&#25991;&#26412;&#34920;&#31034;&#20013;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#20302;&#20272;&#20102;&#23646;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#25506;&#32034;&#20004;&#26041;&#38754;&#30340;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30001;&#20110;&#20004;&#31471;&#30340;&#23646;&#24615;&#22312;&#25968;&#37327;&#21644;&#31867;&#22411;&#19978;&#36890;&#24120;&#19981;&#23545;&#40784;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22810;&#24847;&#22270;&#24314;&#27169;&#26469;&#21033;&#29992;&#23646;&#24615;&#30340;&#22909;&#22788;&#12290;&#20174;&#23646;&#24615;&#20013;&#25552;&#21462;&#30340;&#24847;&#22270;&#24635;&#32467;&#20102;&#26597;&#35810;&#30340;&#22810;&#26679;&#21270;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text matching systems have become a fundamental service in most searching platforms. For instance, they are responsible for matching user queries to relevant candidate items, or rewriting the user-input query to a pre-selected high-performing one for a better search experience. In practice, both the queries and items often contain multiple attributes, such as the category of the item and the location mentioned in the query, which represent condensed key information that is helpful for matching. However, most of the existing works downplay the effectiveness of attributes by integrating them into text representations as supplementary information. Hence, in this work, we focus on exploring the relationship between the attributes from two sides. Since attributes from two ends are often not aligned in terms of number and type, we propose to exploit the benefit of attributes by multiple-intent modeling. The intents extracted from attributes summarize the diverse needs of queries and provide 
&lt;/p&gt;</description></item><item><title>TELLER&#26159;&#19968;&#20010;&#21487;&#20449;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#35748;&#30693;&#21644;&#20915;&#31574;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#12289;&#21487;&#25512;&#24191;&#24615;&#21644;&#21487;&#25511;&#21046;&#24615;&#12290;&#35748;&#30693;&#31995;&#32479;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#36923;&#36753;&#35859;&#35789;&#65292;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35835;&#30340;&#36923;&#36753;&#21407;&#23376;&#12290;&#20915;&#31574;&#31995;&#32479;&#25512;&#23548;&#21487;&#25512;&#24191;&#30340;&#36923;&#36753;&#35268;&#21017;&#26469;&#32858;&#21512;&#36825;&#20123;&#21407;&#23376;&#65292;&#23454;&#29616;&#30495;&#23454;&#21644;&#34394;&#20551;&#26032;&#38395;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.07776</link><description>&lt;p&gt;
TELLER: &#19968;&#20010;&#21487;&#20449;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07776
&lt;/p&gt;
&lt;p&gt;
TELLER&#26159;&#19968;&#20010;&#21487;&#20449;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#35748;&#30693;&#21644;&#20915;&#31574;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#12289;&#21487;&#25512;&#24191;&#24615;&#21644;&#21487;&#25511;&#21046;&#24615;&#12290;&#35748;&#30693;&#31995;&#32479;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#36923;&#36753;&#35859;&#35789;&#65292;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35835;&#30340;&#36923;&#36753;&#21407;&#23376;&#12290;&#20915;&#31574;&#31995;&#32479;&#25512;&#23548;&#21487;&#25512;&#24191;&#30340;&#36923;&#36753;&#35268;&#21017;&#26469;&#32858;&#21512;&#36825;&#20123;&#21407;&#23376;&#65292;&#23454;&#29616;&#30495;&#23454;&#21644;&#34394;&#20551;&#26032;&#38395;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#30340;&#27867;&#28389;&#24050;&#25104;&#20026;&#19968;&#20010;&#20005;&#37325;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#24341;&#36215;&#20102;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#26816;&#27979;&#20551;&#26032;&#38395;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#38750;&#36879;&#26126;&#25512;&#29702;&#36807;&#31243;&#12289;&#24046;&#24378;&#20154;&#24847;&#30340;&#25512;&#24191;&#33021;&#21147;&#20197;&#21450;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#30340;&#22266;&#26377;&#39118;&#38505;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;TELLER&#65292;&#29992;&#20110;&#21487;&#20449;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#37325;&#28857;&#20851;&#27880;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25512;&#24191;&#24615;&#21644;&#21487;&#25511;&#21046;&#24615;&#12290;&#36825;&#36890;&#36807;&#19968;&#20010;&#34701;&#21512;&#35748;&#30693;&#21644;&#20915;&#31574;&#31995;&#32479;&#30340;&#21452;&#31995;&#32479;&#26694;&#26550;&#26469;&#23454;&#29616;&#65292;&#36981;&#24490;&#20197;&#19978;&#21407;&#21017;&#12290;&#35748;&#30693;&#31995;&#32479;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#36923;&#36753;&#35859;&#35789;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#21487;&#35835;&#30340;&#36923;&#36753;&#21407;&#23376;&#12290;&#21516;&#26102;&#65292;&#20915;&#31574;&#31995;&#32479;&#25512;&#23548;&#21487;&#25512;&#24191;&#30340;&#36923;&#36753;&#35268;&#21017;&#26469;&#32858;&#21512;&#36825;&#20123;&#21407;&#23376;&#65292;&#23454;&#29616;&#30495;&#23454;&#21644;&#34394;&#20551;&#26032;&#38395;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs). To address this challenge, we propose {\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#36741;&#21161;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#23558;LLMs&#20316;&#20026;&#31185;&#23398;&#25991;&#29486;&#28508;&#22312;&#31354;&#38388;&#30340;&#25509;&#21475;&#12290;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#8220;&#19987;&#23478;&#8221;&#30340;&#24433;&#21709;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07770</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Quantitative knowledge retrieval from large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#36741;&#21161;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#23558;LLMs&#20316;&#20026;&#31185;&#23398;&#25991;&#29486;&#28508;&#22312;&#31354;&#38388;&#30340;&#25509;&#21475;&#12290;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#8220;&#19987;&#23478;&#8221;&#30340;&#24433;&#21709;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#29983;&#25104;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#30340;&#33021;&#21147;&#32780;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#20316;&#20026;&#23450;&#37327;&#20449;&#24687;&#26816;&#32034;&#30340;&#23454;&#29992;&#24615;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;LLMs&#20316;&#20026;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#24110;&#21161;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#65292;&#22914;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#20998;&#24067;&#24341;&#23548;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#22635;&#34917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#23558;LLMs&#35270;&#20026;&#31185;&#23398;&#25991;&#29486;&#28508;&#22312;&#31354;&#38388;&#30340;&#25509;&#21475;&#65292;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#21644;&#39046;&#22495;&#20013;&#27604;&#36739;&#21709;&#24212;&#19982;&#26356;&#25104;&#29087;&#30340;&#26041;&#27861;&#12290;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#8220;&#19987;&#23478;&#8221;&#30340;&#24433;&#21709;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been extensively studied for their abilities to generate convincing natural language sequences, however their utility for quantitative information retrieval is less well understood. In this paper we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid data analysis tasks such as elicitation of prior distributions for Bayesian models and imputation of missing data. We present a prompt engineering framework, treating an LLM as an interface to a latent space of scientific literature, comparing responses in different contexts and domains against more established approaches. Implications and challenges of using LLMs as 'experts' are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#35299;&#27602;&#21270;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#26377;&#27602;&#25991;&#26412;&#33258;&#21160;&#36716;&#21270;&#20026;&#26080;&#27602;&#25991;&#26412;&#12290;&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#21024;&#38500;&#37325;&#24314;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21033;&#29992;Dementieva&#31561;&#20154;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23567;&#22411;&#30340;&#21360;&#22320;&#35821;&#24179;&#34892;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.07767</link><description>&lt;p&gt;
&#25991;&#26412;&#35299;&#27602;&#20316;&#20026;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#20013;&#30340;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Text Detoxification as Style Transfer in English and Hindi
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#35299;&#27602;&#21270;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#26377;&#27602;&#25991;&#26412;&#33258;&#21160;&#36716;&#21270;&#20026;&#26080;&#27602;&#25991;&#26412;&#12290;&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#21024;&#38500;&#37325;&#24314;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21033;&#29992;Dementieva&#31561;&#20154;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23567;&#22411;&#30340;&#21360;&#22320;&#35821;&#24179;&#34892;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#25991;&#26412;&#35299;&#27602;&#65292;&#21363;&#33258;&#21160;&#23558;&#26377;&#27602;&#25991;&#26412;&#36716;&#21270;&#20026;&#38750;&#26377;&#27602;&#25991;&#26412;&#12290;&#36825;&#39033;&#20219;&#21153;&#26377;&#21161;&#20110;&#26356;&#23433;&#20840;&#12289;&#26356;&#23562;&#37325;&#30340;&#22312;&#32447;&#20132;&#27969;&#65292;&#24182;&#21487;&#34987;&#35270;&#20026;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#65288;TST&#65289;&#20219;&#21153;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#65292;&#25991;&#26412;&#39118;&#26684;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#20869;&#23481;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#65306;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#19982;&#21508;&#31181;&#27602;&#24615;&#20998;&#31867;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#21024;&#38500;&#21644;&#37325;&#24314;&#26041;&#27861;&#12290;&#20026;&#25903;&#25345;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Dementieva&#31561;&#20154;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65288;2021&#24180;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;&#26377;&#27602;&#25991;&#26412;&#23545;&#24212;&#30340;&#22810;&#20010;&#29256;&#26412;&#30340;&#35299;&#27602;&#25991;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#23478;&#20154;&#24037;&#27880;&#37322;&#21592;&#36873;&#25321;&#20102;&#26368;&#20339;&#30340;&#21464;&#20307;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#26377;&#27602;&#21477;&#23376;&#19982;&#19968;&#20010;&#36866;&#24403;&#30340;&#35299;&#27602;&#29256;&#26412;&#37197;&#23545;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#23567;&#22411;&#30340;&#21360;&#22320;&#35821;&#24179;&#34892;&#25968;&#25454;&#38598;&#65292;&#19982;&#33521;&#35821;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on text detoxification, i.e., automatically converting toxic text into non-toxic text. This task contributes to safer and more respectful online communication and can be considered a Text Style Transfer (TST) task, where the text style changes while its content is preserved. We present three approaches: knowledge transfer from a similar task, multi-task learning approach, combining sequence-to-sequence modeling with various toxicity classification tasks, and, delete and reconstruct approach. To support our research, we utilize a dataset provided by Dementieva et al.(2021), which contains multiple versions of detoxified texts corresponding to toxic texts. In our experiments, we selected the best variants through expert human annotators, creating a dataset where each toxic sentence is paired with a single, appropriate detoxified version. Additionally, we introduced a small Hindi parallel dataset, aligning with a part of the English dataset, suitable for evaluation purp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07754</link><description>&lt;p&gt;
&#24605;&#24819;&#20256;&#25773;&#65306;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#30456;&#23545;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;CoT&#26159;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#32500;&#25193;&#25955;&#65288;DoT&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#25512;&#29702;&#27493;&#39588;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#22312;&#26102;&#38388;&#19978;&#20256;&#25773;&#12290;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;token&#20174;&#24038;&#21040;&#21491;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;DoT&#22312;&#35745;&#31639;&#21644;&#25512;&#29702;&#24615;&#33021;&#20043;&#38388;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DoT&#22312;&#22810;&#20301;&#25968;&#20056;&#27861;&#21644;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;DoT&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#65292;&#24182;&#20174;&#29616;&#26377;&#30340;&#22686;&#24378;&#25512;&#29702;&#25216;&#26415;&#65288;&#22914;&#33258;&#19968;&#33268;&#35299;&#30721;&#65289;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07744</link><description>&lt;p&gt;
&#23454;&#29616;&#26234;&#33021;&#20307;&#12289;&#20154;&#31867;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#32479;&#19968;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Alignment Between Agents, Humans, and Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#23548;&#33268;&#20102;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#32321;&#33635;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#22797;&#26434;&#12289;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#65292;&#26234;&#33021;&#20307;&#30340;&#25928;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017;&#65292;&#21363;&#21516;&#26102;&#23545;&#40784;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#65288;&#22914;&#36135;&#24065;&#39044;&#31639;&#38480;&#21046;&#65289;&#12290;&#20174;&#32479;&#19968;&#23545;&#40784; ($\mathbf{UA}^2$) &#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24403;&#21069;&#26234;&#33021;&#20307;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#26234;&#33021;&#20307;&#22522;&#20934;&#21644;&#26041;&#27861;&#20505;&#36873;&#20013;&#34987;&#24573;&#35270;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20026;WebShop&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#65292;&#21253;&#25324;&#20351;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#26469;&#23637;&#31034;&#24847;&#22270;&#12289;&#20010;&#24615;&#21270;&#37325;&#26032;&#25490;&#21517;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#36816;&#34892;&#26102;&#25104;&#26412;&#32479;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost stat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#28151;&#21512;&#24335;&#20027;&#21160;&#23545;&#35805;&#25628;&#32034;&#20013;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#26469;&#25913;&#36827;&#28548;&#28165;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;Marto&#30340;&#22810;&#27169;&#24577;&#26597;&#35810;&#28548;&#28165;&#27169;&#22411;&#12290;&#36890;&#36807;&#25910;&#38598;&#22810;&#27169;&#24577;&#28548;&#28165;&#38382;&#39064;&#21644;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;Melon&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#20219;&#21153;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;</title><link>https://arxiv.org/abs/2402.07742</link><description>&lt;p&gt;
&#22312;&#28151;&#21512;&#24335;&#20027;&#21160;&#23545;&#35805;&#25628;&#32034;&#20013;&#25552;&#20986;&#22810;&#27169;&#24577;&#28548;&#28165;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#28151;&#21512;&#24335;&#20027;&#21160;&#23545;&#35805;&#25628;&#32034;&#20013;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#26469;&#25913;&#36827;&#28548;&#28165;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;Marto&#30340;&#22810;&#27169;&#24577;&#26597;&#35810;&#28548;&#28165;&#27169;&#22411;&#12290;&#36890;&#36807;&#25910;&#38598;&#22810;&#27169;&#24577;&#28548;&#28165;&#38382;&#39064;&#21644;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;Melon&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#20219;&#21153;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#21512;&#24335;&#20027;&#21160;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#20013;&#65292;&#28548;&#28165;&#38382;&#39064;&#34987;&#29992;&#20110;&#24110;&#21161;&#37027;&#20123;&#38590;&#20197;&#29992;&#19968;&#20010;&#26597;&#35810;&#34920;&#36798;&#33258;&#24049;&#24847;&#22270;&#30340;&#29992;&#25143;&#12290;&#36825;&#20123;&#38382;&#39064;&#26088;&#22312;&#25581;&#31034;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#24182;&#35299;&#20915;&#26597;&#35810;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#22312;&#19982;&#22810;&#27169;&#24577;&#20449;&#24687;&#30456;&#20851;&#30340;&#24773;&#26223;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#25991;&#26412;&#20449;&#24687;&#21487;&#20197;&#25913;&#36827;&#28548;&#28165;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22270;&#20687;&#28155;&#21152;&#21040;&#28548;&#28165;&#38382;&#39064;&#20013;&#65292;&#24182;&#25552;&#20986;&#22312;&#24320;&#25918;&#22495;&#12289;&#28151;&#21512;&#24335;&#20027;&#21160;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#20013;&#25552;&#20986;&#22810;&#27169;&#24577;&#28548;&#28165;&#38382;&#39064;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#23545;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21517;&#20026;Melon&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;4k&#20010;&#22810;&#27169;&#24577;&#28548;&#28165;&#38382;&#39064;&#65292;&#24182;&#38468;&#24102;&#36229;&#36807;14k&#20010;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Marto&#30340;&#22810;&#27169;&#24577;&#26597;&#35810;&#28548;&#28165;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#29983;&#25104;&#24494;&#35843;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;&#19981;&#21516;&#38454;&#27573;&#30340;&#35757;&#32451;&#12290;&#36827;&#34892;&#20102;&#20960;&#20010;&#20998;&#26512;&#26469;&#29702;&#35299;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query. These questions aim to uncover user's information needs and resolve query ambiguities. We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information. Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems. To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images. We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts. Several analyses are conducted to understand the importance 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;AIR-Bench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07729</link><description>&lt;p&gt;
AIR-Bench: &#36890;&#36807;&#29983;&#25104;&#24615;&#29702;&#35299;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;AIR-Bench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#23548;&#24615;&#30340;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#23545;&#20154;&#19982;&#38899;&#39057;&#30340;&#20114;&#21160;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#33021;&#22815;&#35780;&#20272;&#20197;&#38899;&#39057;&#20026;&#20013;&#24515;&#30340;&#20114;&#21160;&#33021;&#21147;&#30340;&#22522;&#20934;&#24050;&#32463;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20197;&#24448;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;&#19981;&#21516;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#32570;&#20047;&#23545;&#22260;&#32469;&#38899;&#39057;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36861;&#36394;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65288;LALMs&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24182;&#20026;&#26410;&#26469;&#30340;&#25913;&#36827;&#25552;&#20379;&#25351;&#23548;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AIR-Bench&#65288;&#38899;&#39057;&#25351;&#23548;&#22522;&#20934;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LALMs&#29702;&#35299;&#21508;&#31181;&#31867;&#22411;&#38899;&#39057;&#20449;&#21495;&#65288;&#21253;&#25324;&#20154;&#31867;&#35821;&#38899;&#12289;&#33258;&#28982;&#22768;&#38899;&#21644;&#38899;&#20048;&#65289;&#20197;&#21450;&#19982;&#20154;&#20197;&#25991;&#26412;&#24418;&#24335;&#36827;&#34892;&#20132;&#20114;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;AIR-Bench&#21253;&#21547;&#20004;&#20010;&#32500;&#24230;&#65306;&#22522;&#30784;&#21644;&#29983;&#25104;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\textbf{A}udio \textbf{I}nst\textbf{R}uction \textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \textit{foundation} and \textit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#25163;&#35821;&#32763;&#35793;&#21644;&#29983;&#25104;&#32593;&#32476;&#65288;USLNet&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#65288;&#25991;&#26412;&#21644;&#35270;&#39057;&#65289;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#24179;&#34892;&#25163;&#35821;&#25968;&#25454;&#12290;USLNet&#37319;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#21453;&#21521;&#32763;&#35793;&#21644;&#37325;&#24314;&#25216;&#26415;&#65292;&#38754;&#23545;&#25991;&#26412;&#21644;&#35270;&#39057;&#24207;&#21015;&#20043;&#38388;&#30340;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#12290;&#36890;&#36807;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#65292;USLNet&#33021;&#22815;&#26377;&#25928;&#23545;&#40784;&#19981;&#21516;&#38271;&#24230;&#30340;&#25991;&#26412;&#21644;&#35270;&#39057;&#24207;&#21015;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#26080;&#30417;&#30563;&#25163;&#35821;&#32763;&#35793;&#21644;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07726</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#25163;&#35821;&#32763;&#35793;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Sign Language Translation and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#25163;&#35821;&#32763;&#35793;&#21644;&#29983;&#25104;&#32593;&#32476;&#65288;USLNet&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#65288;&#25991;&#26412;&#21644;&#35270;&#39057;&#65289;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#24179;&#34892;&#25163;&#35821;&#25968;&#25454;&#12290;USLNet&#37319;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#21453;&#21521;&#32763;&#35793;&#21644;&#37325;&#24314;&#25216;&#26415;&#65292;&#38754;&#23545;&#25991;&#26412;&#21644;&#35270;&#39057;&#24207;&#21015;&#20043;&#38388;&#30340;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#12290;&#36890;&#36807;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#65292;USLNet&#33021;&#22815;&#26377;&#25928;&#23545;&#40784;&#19981;&#21516;&#38271;&#24230;&#30340;&#25991;&#26412;&#21644;&#35270;&#39057;&#24207;&#21015;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#26080;&#30417;&#30563;&#25163;&#35821;&#32763;&#35793;&#21644;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;UNMT&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#25163;&#35821;&#32763;&#35793;&#21644;&#29983;&#25104;&#32593;&#32476;&#65288;USLNet&#65289;&#65292;&#23427;&#33021;&#22815;&#20174;&#22823;&#37327;&#30340;&#21333;&#27169;&#24577;&#65288;&#25991;&#26412;&#21644;&#35270;&#39057;&#65289;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#24179;&#34892;&#30340;&#25163;&#35821;&#25968;&#25454;&#12290;USLNet&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#21333;&#27169;&#24577;&#37325;&#24314;&#27169;&#22359;&#65288;&#25991;&#26412;&#21644;&#35270;&#39057;&#65289;&#29992;&#20110;&#22312;&#30456;&#21516;&#27169;&#24577;&#19979;&#20174;&#22024;&#26434;&#30340;&#36755;&#20837;&#20013;&#37325;&#24314;&#36755;&#20837;&#65292;&#20197;&#21450;&#36328;&#27169;&#24577;&#21453;&#21521;&#32763;&#35793;&#27169;&#22359;&#65288;&#25991;&#26412;-&#35270;&#39057;-&#25991;&#26412;&#21644;&#35270;&#39057;-&#25991;&#26412;-&#35270;&#39057;&#65289;&#29992;&#20110;&#20351;&#29992;&#21453;&#21521;&#32763;&#35793;&#36807;&#31243;&#20174;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#22024;&#26434;&#36755;&#20837;&#20013;&#37325;&#24314;&#36755;&#20837;&#12290;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;UNMT&#20013;&#30340;&#21333;&#27169;&#24577;&#21453;&#21521;&#32763;&#35793;&#36807;&#31243;&#19981;&#21516;&#65292;USLNet&#38754;&#20020;&#30528;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#36328;&#27169;&#24577;&#24046;&#24322;&#65292;&#21363;&#25991;&#26412;&#21644;&#35270;&#39057;&#24207;&#21015;&#20043;&#38388;&#30340;&#38271;&#24230;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#35299;&#20915;&#23545;&#40784;&#21487;&#21464;&#38271;&#24230;&#30340;&#25991;&#26412;&#21644;&#35270;&#39057;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;USLNet&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#26080;&#30417;&#30563;&#25163;&#35821;&#32763;&#35793;&#21644;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of unsupervised neural machine translation (UNMT), we introduce an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences. We propose a sliding window method to address the issues of aligning variable-length text with video sequences. To our knowledge, USLNet is the fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LoRA-drop&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#20854;&#20313;&#23618;&#20849;&#20139;&#30456;&#21516;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LoRA-drop&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07721</link><description>&lt;p&gt;
LoRA-drop&#65306;&#22522;&#20110;&#36755;&#20986;&#35780;&#20272;&#30340;&#39640;&#25928;LoRA&#21442;&#25968;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LoRA-drop&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#20854;&#20313;&#23618;&#20849;&#20139;&#30456;&#21516;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LoRA-drop&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#20026;&#27599;&#20010;&#23618;&#24341;&#20837;&#36741;&#21161;&#21442;&#25968;&#65292;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#24403;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#22411;&#26102;&#65292;&#20173;&#28982;&#38754;&#20020;&#36164;&#28304;&#28040;&#32791;&#30340;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#23618;&#30340;LoRA&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#26469;&#37319;&#29992;&#21098;&#26525;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#21482;&#20998;&#26512;&#20102;&#21442;&#25968;&#30340;&#29305;&#24449;&#20197;&#35780;&#20272;&#20854;&#37325;&#35201;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#21442;&#25968;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;LoRA&#30340;&#36755;&#20986;&#26159;&#30452;&#25509;&#24433;&#21709;&#20923;&#32467;&#27169;&#22411;&#30340;&#22240;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoRA-drop&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#26469;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#32780;&#20854;&#20182;&#23618;&#30340;LoRA&#20849;&#20139;&#30456;&#21516;&#30340;&#21442;&#25968;&#12290;&#22312;NLU&#21644;NLG&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;LoRA-drop&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#23454;&#26045;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#30456;&#27604;&#65292;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#12289;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07689</link><description>&lt;p&gt;
OrderBkd: &#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#36827;&#34892;&#30340;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
OrderBkd: Textual backdoor attack through repositioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#23454;&#26045;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#30456;&#27604;&#65292;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#12289;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31532;&#19977;&#26041;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;NLP&#31995;&#32479;&#26500;&#25104;&#23041;&#32961;&#65292;&#21487;&#33021;&#38544;&#34255;&#21518;&#38376;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#21253;&#25324;&#25554;&#20837;&#26631;&#35760;&#25110;&#21477;&#23376;&#37325;&#36848;&#31561;&#27745;&#26579;&#25968;&#25454;&#26679;&#26412;&#65292;&#36825;&#35201;&#20040;&#25913;&#21464;&#20102;&#21407;&#22987;&#25991;&#26412;&#30340;&#35821;&#20041;&#65292;&#35201;&#20040;&#21487;&#20197;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#25105;&#20204;&#19982;&#20197;&#24448;&#24037;&#20316;&#30340;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#65292;&#25105;&#20204;&#20351;&#29992;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#36890;&#36807;&#35774;&#35745;&#24182;&#24212;&#29992;&#22522;&#20110;&#35789;&#24615;&#30340;&#35268;&#21017;&#26469;&#36873;&#25321;&#36825;&#20123;&#26631;&#35760;&#65292;&#25105;&#20204;&#22312;SST-2&#21644;AG&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22312;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#35770;&#25991;&#20013;&#30340;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/alekseevskaia/OrderBkd&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35821;&#20041;&#20381;&#23384;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#20197;&#22686;&#21152;&#24359;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#31995;&#32479;&#24615;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07682</link><description>&lt;p&gt;
&#25552;&#21319;&#21452;&#32447;&#24615;&#35821;&#20041;&#20381;&#23384;&#35299;&#26512;&#30340;&#36741;&#21161;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35821;&#20041;&#20381;&#23384;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#20197;&#22686;&#21152;&#24359;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#31995;&#32479;&#24615;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dozat&#21644;Manning (2017)&#30340;&#21452;&#32447;&#24615;&#35299;&#26512;&#25104;&#21151;&#22320;&#25193;&#23637;&#21040;&#35821;&#20041;&#20381;&#23384;&#35299;&#26512;(SDP) (Dozat&#21644;Manning, 2018)&#12290;&#37492;&#20110;&#27809;&#26377;&#26641;&#32467;&#26500;&#30340;&#32422;&#26463;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#21477;&#23376;&#65292;&#25152;&#26377;&#24359;&#37117;&#26159;&#30456;&#20114;&#29420;&#31435;&#39044;&#27979;&#30340;&#65288;&#38500;&#20102;&#20196;&#29260;&#30340;&#20849;&#20139;&#34920;&#31034;&#65289;&#65292;&#28982;&#32780;&#20854;&#22312;&#22270;&#19978;&#30340;&#24615;&#33021;&#20196;&#20154;&#24778;&#35766;&#22320;&#24456;&#39640;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#20915;&#31574;&#30340;&#29420;&#31435;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;O(n^2)&#30340;&#22797;&#26434;&#24230;&#21644;&#39640;&#24230;&#21487;&#24182;&#34892;&#21270;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31616;&#21333;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#24341;&#20837;&#24359;&#20043;&#38388;&#26576;&#31181;&#24418;&#24335;&#30340;&#30456;&#20114;&#20381;&#36182;&#12290;&#22312;SemEval 2015&#20219;&#21153;18&#30340;&#19977;&#20010;&#33521;&#35821;&#38750;&#24490;&#29615;&#25968;&#25454;&#38598;(Oepen et al., 2015)&#21644;&#27861;&#35821;&#28145;&#24230;&#21477;&#27861;&#24490;&#29615;&#22270;(Ribeyre et al., 2014)&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#21270;&#34920;&#31034;&#30340;&#25509;&#36817;&#26368;&#20808;&#36827;&#22522;&#32447;&#30340;&#22522;&#30784;&#19978;&#65292;&#34429;&#28982;&#24615;&#33021;&#25913;&#36827;&#36866;&#20013;&#20294;&#31995;&#32479;&#24615;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#25552;&#21319;SDP&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The biaffine parser of Dozat and Manning (2017) was successfully extended to semantic dependency parsing (SDP) (Dozat and Manning, 2018). Its performance on graphs is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens). To circumvent such an independence of decision, while retaining the O(n^2) complexity and highly parallelizable architecture, we propose to use simple auxiliary tasks that introduce some form of interdependence between arcs. Experiments on the three English acyclic datasets of SemEval 2015 task 18 (Oepen et al., 2015), and on French deep syntactic cyclic graphs (Ribeyre et al., 2014) show modest but systematic performance gains on a near state-of-the-art baseline using transformer-based contextualized representations. This provides a simple and robust method to boost SDP performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20135;&#29983;&#19978;&#19979;&#25991;&#36275;&#22815;&#19988;&#27969;&#30021;&#30340;&#35793;&#25991;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#22312;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07681</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8220;&#35780;&#23457;&#8221;: &#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#26426;&#22120;&#32763;&#35793;&#25928;&#26524;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large Language Models "Ad Referendum": How Good Are They at Machine Translation in the Legal Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20135;&#29983;&#19978;&#19979;&#25991;&#36275;&#22815;&#19988;&#27969;&#30021;&#30340;&#35793;&#25991;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#22312;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#22312;&#27861;&#24459;&#39046;&#22495;&#22235;&#31181;&#35821;&#35328;&#23545;&#20013;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#12290;&#30740;&#31350;&#32467;&#21512;&#20102;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65288;AEMs&#65289;&#21644;&#19987;&#19994;&#32763;&#35793;&#20154;&#21592;&#36827;&#34892;&#30340;&#20154;&#24037;&#35780;&#20272;&#65288;HE&#65289;&#65292;&#35780;&#20272;&#20102;&#32763;&#35793;&#25490;&#21517;&#12289;&#27969;&#30021;&#24615;&#21644;&#36275;&#22815;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#35895;&#27468;&#32763;&#35793;&#22312;AEMs&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;LLMs&#65292;&#20294;&#20154;&#24037;&#35780;&#20272;&#32773;&#35748;&#20026;LLMs&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#20135;&#29983;&#19978;&#19979;&#25991;&#36275;&#22815;&#19988;&#27969;&#30021;&#30340;&#35793;&#25991;&#26041;&#38754;&#30456;&#24403;&#25110;&#30053;&#22909;&#20110;&#35895;&#27468;&#32763;&#35793;&#12290;&#36825;&#31181;&#24046;&#24322;&#34920;&#26126;LLMs&#22312;&#22788;&#29702;&#19987;&#19994;&#27861;&#24459;&#26415;&#35821;&#21644;&#32972;&#26223;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20984;&#26174;&#20102;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#22312;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#33021;&#21147;&#65292;&#24182;&#21628;&#21505;&#37325;&#26032;&#35780;&#20272;&#20256;&#32479;AEMs&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;LLMs&#29983;&#25104;&#30340;&#32763;&#35793;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain. It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy. The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality. The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#39640;&#21307;&#23398;&#36716;&#24405;&#20013;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#38646;-shot&#21644;Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07658</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20043;&#22768;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#21307;&#23398;&#36716;&#24405;ASR&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#39640;&#21307;&#23398;&#36716;&#24405;&#20013;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#38646;-shot&#21644;Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#25991;&#26723;&#30340;&#24555;&#36895;&#21457;&#23637;&#29615;&#22659;&#20013;&#65292;&#20934;&#30830;&#36716;&#24405;&#20020;&#24202;&#23545;&#35805;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#39640;&#21307;&#23398;&#36716;&#24405;&#20013;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#12290;&#21033;&#29992;PriMock57&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#21021;&#32423;&#25252;&#29702;&#21672;&#35810;&#65292;&#25105;&#20204;&#37319;&#29992;&#20808;&#36827;&#30340;LLMs&#26469;&#20248;&#21270;ASR&#29983;&#25104;&#30340;&#36716;&#24405;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#22810;&#26041;&#38754;&#30340;&#65292;&#20851;&#27880;&#20110;&#25913;&#36827;&#19968;&#33324;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#21307;&#23398;&#27010;&#24565;&#38169;&#35823;&#29575;&#65288;MC-WER&#65289;&#20197;&#20934;&#30830;&#36716;&#24405;&#37325;&#35201;&#30340;&#21307;&#23398;&#26415;&#35821;&#65292;&#20197;&#21450;&#35828;&#35805;&#20154;&#37325;&#38899;&#21010;&#20998;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;LLM&#21518;&#22788;&#29702;&#22312;&#25913;&#36827;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#20445;&#25345;&#20020;&#24202;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38646;-shot&#21644;Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#22312;&#25552;&#39640;&#37325;&#38899;&#21010;&#20998;&#21644;&#32416;&#27491;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#29255;&#27573;&#25552;&#21462;&#27169;&#22411;&#20174;&#20020;&#24202;&#25968;&#25454;&#20013;&#25552;&#21462;&#20851;&#20110;&#38590;&#27835;&#24615;&#25233;&#37057;&#30151;&#30340;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;&#22312;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07645</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21512;&#25104;&#25968;&#25454;&#20013;&#26816;&#27979;&#38590;&#27835;&#24615;&#25233;&#37057;&#30151;&#30340;&#20020;&#24202;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#29255;&#27573;&#25552;&#21462;&#27169;&#22411;&#20174;&#20020;&#24202;&#25968;&#25454;&#20013;&#25552;&#21462;&#20851;&#20110;&#38590;&#27835;&#24615;&#25233;&#37057;&#30151;&#30340;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;&#22312;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38590;&#27835;&#24615;&#25233;&#37057;&#30151;(DTD)&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#26356;&#24191;&#27867;&#19988;&#20020;&#24202;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#35813;&#35270;&#35282;&#34920;&#26126;&#22312;&#27835;&#30103;&#36807;&#31243;&#20013;&#65292;&#24739;&#32773;&#20173;&#25345;&#32493;&#32463;&#21382;&#26174;&#33879;&#36127;&#25285;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#23545;&#24120;&#35268;&#25910;&#38598;&#30340;&#21465;&#36848;&#24615;&#65288;&#33258;&#30001;&#25991;&#26412;&#65289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#36827;&#34892;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#33021;&#25429;&#25417;DTD&#20020;&#24202;&#32508;&#21512;&#24449;&#30340;&#24050;&#21457;&#34920;&#39044;&#21518;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;(GPT3.5)&#21644;&#38750;&#26497;&#22823;&#20540;&#25233;&#21046;(NMS)&#31639;&#27861;&#26469;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;BERT&#30340;&#29255;&#27573;&#25552;&#21462;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#33021;&#22815;&#20174;&#30495;&#23454;&#30340;&#20020;&#24202;&#25968;&#25454;&#20013;&#25552;&#21462;&#21644;&#26631;&#35760;&#19982;DTD&#32508;&#21512;&#24449;&#30340;&#21305;&#37197;&#21487;&#33021;&#24615;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#21508;&#31181;&#30456;&#20851;&#27491;&#36127;&#22240;&#32032;&#30340;&#29255;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22810;&#36798;20&#20010;DTD&#20020;&#24202;&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#33719;&#24471;&#33391;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;(&#26497;&#24615;&#20026;0.70 F1)&#12290;
&lt;/p&gt;
&lt;p&gt;
Difficult-to-treat depression (DTD) has been proposed as a broader and more clinically comprehensive perspective on a person's depressive disorder where despite treatment, they continue to experience significant burden. We sought to develop a Large Language Model (LLM)-based tool capable of interrogating routinely-collected, narrative (free-text) electronic health record (EHR) data to locate published prognostic factors that capture the clinical syndrome of DTD. In this work, we use LLM-generated synthetic data (GPT3.5) and a Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction model. The resulting model is then able to extract and label spans related to a variety of relevant positive and negative factors in real clinical data (i.e. spans of text that increase or decrease the likelihood of a patient matching the DTD syndrome). We show it is possible to obtain good overall performance (0.70 F1 across polarity) on real clinical data on a set of as many as 20 diff
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07616</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Anchor-based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07616
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20027;&#35201;&#37319;&#29992;&#20165;&#35299;&#30721;&#22120;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#38656;&#35201;&#20445;&#30041;&#21382;&#21490;&#26631;&#35760;&#30340;&#38190;/&#20540;&#20449;&#24687;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#36991;&#20813;&#20887;&#20313;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#30340;&#24040;&#22823;&#22823;&#23567;&#21644;&#21442;&#25968;&#37327;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#36755;&#20837;&#25991;&#26412;&#30340;&#38271;&#24230;&#32780;&#22686;&#21152;&#65292;&#36843;&#20999;&#38656;&#35201;&#26356;&#39640;&#25928;&#30340;&#20449;&#24687;&#23384;&#20648;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;LLM&#65288;AnLLM&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#25104;&#38170;&#28857;&#26631;&#35760;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#24182;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;AnLLM&#22312;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#39640;&#36798;99%&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#39640;&#36798;3.5&#20493;&#30340;&#21516;&#26102;&#65292;&#20173;&#20445;&#25345;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#29306;&#29298;&#20102;&#19968;&#20123;&#20934;&#30830;&#24615;&#65292;AnLLM&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#20381;&#28982;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07610</link><description>&lt;p&gt;
&#36393;&#33050;&#35843;&#26657;&#65306;&#36890;&#36807;&#33258;&#21161;&#24341;&#23548;&#25193;&#23637;LLM&#30340;&#33258;&#23545;&#40784;&#33021;&#21147;&#30340;&#35268;&#27169;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23545;&#40784;&#26159;&#19968;&#31181;&#38477;&#20302;&#20154;&#24037;&#27880;&#37322;&#25104;&#26412;&#24182;&#30830;&#20445;&#27169;&#22411;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#21333;&#27425;&#24490;&#29615;&#20013;&#23436;&#25104;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#27493;&#39588;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#33258;&#23545;&#40784;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#36827;&#34892;&#22810;&#27425;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#65292;&#20250;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#36824;&#26159;&#23548;&#33268;&#24555;&#36895;&#36864;&#21270;&#65311;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20445;&#35777;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#25381;&#33258;&#21161;&#24341;&#23548;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#24182;&#35843;&#25972;&#20102;&#25968;&#25454;&#30340;&#35757;&#32451;&#39034;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36393;&#33050;&#35843;&#26657;&#65288;SOFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27169;&#22411;&#30340;&#25345;&#32493;&#22686;&#24378;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#23545;&#25968;&#20284;&#28982;&#30340;&#35777;&#25454;&#19979;&#30028;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21152;&#26435;&#32447;&#24615;&#32452;&#21512;&#65292;&#23558;&#23545;&#27604;&#20027;&#39064;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#33719;&#24471;&#33021;&#22815;&#25429;&#25417;&#20849;&#20139;&#35821;&#20041;&#24182;&#20811;&#26381;&#20302;&#32423;&#21035;&#20114;&#20449;&#24687;&#24178;&#25200;&#30340;&#20027;&#39064;&#21521;&#37327;&#38598;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.07577</link><description>&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#20316;&#20026;&#22810;&#30446;&#26631;&#23545;&#27604;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topic Modeling as Multi-Objective Contrastive Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07577
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#23545;&#25968;&#20284;&#28982;&#30340;&#35777;&#25454;&#19979;&#30028;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21152;&#26435;&#32447;&#24615;&#32452;&#21512;&#65292;&#23558;&#23545;&#27604;&#20027;&#39064;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#33719;&#24471;&#33021;&#22815;&#25429;&#25417;&#20849;&#20139;&#35821;&#20041;&#24182;&#20811;&#26381;&#20302;&#32423;&#21035;&#20114;&#20449;&#24687;&#24178;&#25200;&#30340;&#20027;&#39064;&#21521;&#37327;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#23545;&#25968;&#20284;&#28982;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21152;&#26435;&#32447;&#24615;&#32452;&#21512;&#26469;&#22686;&#24378;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25991;&#26723;&#32423;&#23545;&#27604;&#23398;&#20064;&#21487;&#33021;&#25429;&#25417;&#21040;&#20302;&#32423;&#21035;&#30340;&#20114;&#20449;&#24687;&#65292;&#20363;&#22914;&#35789;&#27604;&#20363;&#65292;&#36825;&#20250;&#24178;&#25200;&#20027;&#39064;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;ELBO&#25439;&#22833;&#26088;&#22312;&#35760;&#24518;&#36755;&#20837;&#32454;&#33410;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#37325;&#26500;&#36136;&#37327;&#65292;&#32780;&#23545;&#27604;&#25439;&#22833;&#21017;&#35797;&#22270;&#23398;&#20064;&#22312;&#36755;&#20837;&#25991;&#26723;&#20043;&#38388;&#27867;&#21270;&#30340;&#20027;&#39064;&#34920;&#31034;&#65292;&#20108;&#32773;&#23384;&#22312;&#28508;&#22312;&#20914;&#31361;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#39318;&#20808;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20027;&#39064;&#21521;&#37327;&#38598;&#21512;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#19968;&#32452;&#36755;&#20837;&#25991;&#26723;&#20043;&#38388;&#20849;&#20139;&#30340;&#26377;&#29992;&#35821;&#20041;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#23545;&#27604;&#20027;&#39064;&#24314;&#27169;&#26126;&#30830;&#25552;&#20986;&#20026;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23454;&#29616;&#24085;&#32047;&#25176;&#24179;&#31283;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive learning objective that contrasts pairs of input documents. However, document-level contrastive learning might capture low-level mutual information, such as word ratio, which disturbs topic modeling. Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive loss which attempts to learn topic representations that generalize among input documents. To address these issues, we first introduce a novel contrastive learning method oriented towards sets of topic vectors to capture useful semantics that are shared among a set of input documents. Secondly, we explicitly cast contrastive topic modeling as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#35299;&#37322;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#22909;&#22788;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#35299;&#37322;&#30340;&#21152;&#20837;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07543</link><description>&lt;p&gt;
&#32473;&#25105;&#30475;&#24590;&#20040;&#20570;&#65306;&#35299;&#37322;&#22312;&#32454;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#35299;&#37322;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#22909;&#22788;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#35299;&#37322;&#30340;&#21152;&#20837;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#35299;&#37322;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#22909;&#22788;&#12290;&#19982;&#25552;&#31034;&#26041;&#24335;&#19981;&#21516;&#65292;&#32454;&#35843;&#20801;&#35768;&#27169;&#22411;&#22312;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21644;&#26356;&#26032;&#21442;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#32454;&#35843;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21253;&#21547;&#36755;&#20986;&#35299;&#37322;&#32780;&#38750;&#20165;&#21576;&#29616;&#31572;&#26696;&#30340;&#25968;&#25454;&#26469;&#23545;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#21482;&#26377;6000&#19975;&#21442;&#25968;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20174;&#36825;&#31181;&#26041;&#27861;&#20013;&#33719;&#30410;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35814;&#32454;&#30340;&#35299;&#37322;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#26377;&#30410;&#22788;&#65292;&#32780;&#23545;&#20110;&#36739;&#22823;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#26080;&#35770;&#35299;&#37322;&#30340;&#38271;&#24230;&#22914;&#20309;&#65292;&#37117;&#21487;&#20197;&#33719;&#24471;&#20960;&#20046;&#30456;&#21516;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#35299;&#37322;&#30340;&#21152;&#20837;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#23613;&#31649;&#23384;&#22312;&#25361;&#25112;&#65292;&#20294;&#35299;&#37322;&#22312;&#32454;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the chall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#20010;&#20154;&#30693;&#35782;&#22270;&#65288;PKG&#65289;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#29992;&#25143;&#30028;&#38754;&#21451;&#22909;&#30340;PKG&#23458;&#25143;&#31471;&#21644;&#38754;&#21521;&#26381;&#21153;&#30340;PKG API&#65292;&#20197;&#21450;&#22522;&#20110;RDF&#30340;PKG&#35789;&#27719;&#34920;&#29992;&#20110;&#34920;&#31034;&#38472;&#36848;&#21644;&#35775;&#38382;&#26435;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07540</link><description>&lt;p&gt;
PKG API&#65306;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#31649;&#29702;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
PKG API: A Tool for Personal Knowledge Graph Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#20010;&#20154;&#30693;&#35782;&#22270;&#65288;PKG&#65289;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#29992;&#25143;&#30028;&#38754;&#21451;&#22909;&#30340;PKG&#23458;&#25143;&#31471;&#21644;&#38754;&#21521;&#26381;&#21153;&#30340;PKG API&#65292;&#20197;&#21450;&#22522;&#20110;RDF&#30340;PKG&#35789;&#27719;&#34920;&#29992;&#20110;&#34920;&#31034;&#38472;&#36848;&#21644;&#35775;&#38382;&#26435;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#30693;&#35782;&#22270;&#65288;PKG&#65289;&#20026;&#20010;&#20154;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#30862;&#29255;&#21270;&#30340;&#20010;&#20154;&#25968;&#25454;&#23384;&#20648;&#21644;&#25972;&#21512;&#21040;&#19968;&#20010;&#20013;&#24515;&#20301;&#32622;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#26381;&#21153;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#21516;&#26102;&#20445;&#25345;&#29992;&#25143;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;&#23613;&#31649;PKG&#30340;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#23454;&#38469;&#25805;&#20316;&#19978;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#30340;PKG&#23454;&#29616;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#34920;&#31034;&#12289;&#31649;&#29702;&#21644;&#19982;PKG&#36827;&#34892;&#20132;&#20114;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65288;1&#65289;&#19968;&#20010;&#38754;&#21521;&#29992;&#25143;&#30340;PKG&#23458;&#25143;&#31471;&#65292;&#20351;&#26368;&#32456;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#36731;&#26494;&#31649;&#29702;&#20182;&#20204;&#30340;&#20010;&#20154;&#25968;&#25454;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#38754;&#21521;&#26381;&#21153;&#30340;PKG API&#12290;&#20026;&#20102;&#24212;&#23545;&#22312;PKG&#20013;&#34920;&#31034;&#36825;&#20123;&#38472;&#36848;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RDF&#30340;PKG&#35789;&#27719;&#34920;&#26469;&#25903;&#25345;&#36825;&#19968;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#35775;&#38382;&#26435;&#38480;&#21644;&#26469;&#28304;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personal knowledge graphs (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control. Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce. This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs. Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API. To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAFIA&#30340;&#22810;&#36866;&#37197;&#22120;&#34701;&#21512;&#30340;&#21253;&#23481;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#20559;&#35265;&#32500;&#24230;&#19978;&#36827;&#34892;&#27169;&#22359;&#21270;&#21435;&#20559;&#20506;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#20102;&#22810;&#26679;&#21270;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#21435;&#20559;&#20506;&#26041;&#27861;&#23545;&#22810;&#20010;&#31038;&#20250;&#20559;&#35265;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#32570;&#20047;&#32771;&#34385;&#12290;</title><link>https://arxiv.org/abs/2402.07519</link><description>&lt;p&gt;
MAFIA: &#22810;&#36866;&#37197;&#22120;&#34701;&#21512;&#30340;&#21253;&#23481;&#24615;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MAFIA: Multi-Adapter Fused Inclusive LanguAge Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAFIA&#30340;&#22810;&#36866;&#37197;&#22120;&#34701;&#21512;&#30340;&#21253;&#23481;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#20559;&#35265;&#32500;&#24230;&#19978;&#36827;&#34892;&#27169;&#22359;&#21270;&#21435;&#20559;&#20506;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#20102;&#22810;&#26679;&#21270;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#21435;&#20559;&#20506;&#26041;&#27861;&#23545;&#22810;&#20010;&#31038;&#20250;&#20559;&#35265;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#32570;&#20047;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#21508;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#20123;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#20165;&#29420;&#31435;&#22320;&#35299;&#20915;&#20102;&#26377;&#38480;&#30340;&#20559;&#35265;&#32500;&#24230;&#65292;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#25110;&#23447;&#25945;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#20445;&#25345;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#27169;&#22359;&#21270;&#22320;&#21435;&#20559;&#20506;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#20102;&#20351;&#29992;&#26377;&#38480;&#30340;&#32654;&#22269;&#20013;&#24515;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;CDA&#65289;&#26469;&#21435;&#20559;&#20506;PLMs&#12290;&#25105;&#20204;&#20351;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#26469;&#20197;&#21322;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#26500;&#24314;&#22810;&#20010;&#20559;&#35265;&#32500;&#24230;&#19978;&#30340;&#22810;&#26679;&#21270;CDA&#12290;&#25105;&#20204;&#24378;&#35843;&#29616;&#26377;&#30340;&#21435;&#20559;&#20506;&#26041;&#27861;&#19981;&#32771;&#34385;&#22810;&#20010;&#31038;&#20250;&#20559;&#35265;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#21508;&#31181;&#31038;&#20250;&#20559;&#35265;&#20043;&#38388;&#30340;&#21327;&#21516;&#25928;&#24212;&#24182;&#23454;&#29616;&#22810;&#37325;&#20559;&#35265;&#30340;&#21435;&#20559;&#20506;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task. In this work, we aim to modularly debias a pretrained language model across multiple dimensions. Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Whisper&#21644;MMS&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#35780;&#20272;&#20102;&#33889;&#33796;&#29273;&#35821;&#20013;&#38750;&#27491;&#24335;&#23545;&#35805;&#35821;&#38899;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#37319;&#29992;&#36807;&#37319;&#26679;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#38472;&#35268;&#23450;&#22411;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.07513</link><description>&lt;p&gt;
&#24179;&#34913;&#30340;&#33402;&#26415;&#65306;&#25581;&#31034;&#21644;&#32531;&#35299;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;ASR&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Whisper&#21644;MMS&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#35780;&#20272;&#20102;&#33889;&#33796;&#29273;&#35821;&#20013;&#38750;&#27491;&#24335;&#23545;&#35805;&#35821;&#38899;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#37319;&#29992;&#36807;&#37319;&#26679;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#38472;&#35268;&#23450;&#22411;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21475;&#35821;&#29702;&#35299;&#39046;&#22495;&#65292;&#20687;Whisper&#21644;Multilingual Massive Speech&#65288;MMS&#65289;&#36825;&#26679;&#30340;&#31995;&#32479;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#23545;Whisper&#21644;MMS&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#37325;&#28857;&#35780;&#20272;&#19982;&#33889;&#33796;&#29273;&#35821;&#29305;&#23450;&#30340;&#38750;&#27491;&#24335;&#23545;&#35805;&#35821;&#38899;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28085;&#30422;&#20102;&#21508;&#31181;&#31867;&#21035;&#65292;&#21253;&#25324;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#32932;&#33394;&#21644;&#22320;&#29702;&#20301;&#32622;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;ASR&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;p&#20540;&#32479;&#35745;&#26174;&#33879;&#24615;&#26469;&#20998;&#26512;&#24615;&#21035;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24191;&#27867;&#30740;&#31350;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#24182;&#20174;&#23454;&#35777;&#35282;&#24230;&#34920;&#26126;&#36807;&#37319;&#26679;&#25216;&#26415;&#32531;&#35299;&#20102;&#36825;&#31181;&#38472;&#35268;&#23450;&#22411;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;MMS&#21644;Whisper&#30340;&#24212;&#29992;&#65292;&#22312;&#33889;&#33796;&#29273;&#35821;&#29615;&#22659;&#20013;&#37327;&#21270;&#20559;&#35265;&#26041;&#38754;&#20570;&#20986;&#20102;&#24320;&#21019;&#24615;&#30340;&#21162;&#21147;&#65292;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;ASR&#31995;&#32479;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of spoken language understanding, systems like Whisper and Multilingual Massive Speech (MMS) have shown state-of-the-art performances. This study is dedicated to a comprehensive exploration of the Whisper and MMS systems, with a focus on assessing biases in automatic speech recognition (ASR) inherent to casual conversation speech specific to the Portuguese language. Our investigation encompasses various categories, including gender, age, skin tone color, and geo-location. Alongside traditional ASR evaluation metrics such as Word Error Rate (WER), we have incorporated p-value statistical significance for gender bias analysis. Furthermore, we extensively examine the impact of data distribution and empirically show that oversampling techniques alleviate such stereotypical biases. This research represents a pioneering effort in quantifying biases in the Portuguese language context through the application of MMS and Whisper, contributing to a better understanding of ASR systems
&lt;/p&gt;</description></item><item><title>T-RAG&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#38382;&#31572;&#65292;&#23427;&#32467;&#21512;&#20102;RAG&#26694;&#26550;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#65292;&#24182;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.07483</link><description>&lt;p&gt;
T-RAG: &#26469;&#33258;LLM&#25112;&#22330;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
T-RAG: Lessons from the LLM Trenches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07483
&lt;/p&gt;
&lt;p&gt;
T-RAG&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#38382;&#31572;&#65292;&#23427;&#32467;&#21512;&#20102;RAG&#26694;&#26550;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#65292;&#24182;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#30340;&#23581;&#35797;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#24212;&#29992;&#39046;&#22495;&#26159;&#23545;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#36827;&#34892;&#38382;&#31572;&#65292;&#20854;&#20013;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#26159;&#25968;&#25454;&#23433;&#20840;&#65292;&#38656;&#35201;&#33021;&#22815;&#22312;&#26412;&#22320;&#37096;&#32626;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#23545;&#26597;&#35810;&#27491;&#30830;&#21709;&#24212;&#30340;&#20581;&#22766;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#25104;&#20026;&#26500;&#24314;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#37325;&#35201;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#26500;&#24314;RAG&#30456;&#23545;&#31616;&#21333;&#65292;&#20294;&#35201;&#20351;&#20854;&#20581;&#22766;&#21644;&#21487;&#38752;&#30340;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#24191;&#27867;&#30340;&#23450;&#21046;&#21270;&#21644;&#30456;&#23545;&#28145;&#20837;&#30340;&#24212;&#29992;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#31169;&#20154;&#32452;&#32455;&#25991;&#20214;&#38382;&#31572;&#24212;&#29992;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#30340;&#24212;&#29992;&#32467;&#21512;&#20102;RAG&#30340;&#20351;&#29992;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#20855;&#26377; ...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#24378;&#26694;&#26550;RGPT&#65292;&#36890;&#36807;&#21453;&#22797;&#38598;&#25104;&#24378;&#22522;&#23398;&#20064;&#32773;&#65292;&#29983;&#25104;&#19968;&#20010;&#19987;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;LLM&#12290;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RGPT&#26126;&#26174;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07470</link><description>&lt;p&gt;
&#25512;&#21160;&#25991;&#26412;&#20998;&#31867;&#20013;LLM&#23481;&#37327;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Pushing The Limit of LLM Capacity for Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#24378;&#26694;&#26550;RGPT&#65292;&#36890;&#36807;&#21453;&#22797;&#38598;&#25104;&#24378;&#22522;&#23398;&#20064;&#32773;&#65292;&#29983;&#25104;&#19968;&#20010;&#19987;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;LLM&#12290;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RGPT&#26126;&#26174;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20247;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30340;&#38750;&#20961;&#25928;&#26524;&#65292;&#25991;&#26412;&#20998;&#31867;&#26410;&#26469;&#30740;&#31350;&#30340;&#20215;&#20540;&#38754;&#20020;&#30528;&#25361;&#25112;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#36793;&#30028;&#36880;&#28176;&#27169;&#31946;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#24314;&#27169;&#26102;&#20195;&#65292;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22312;&#20805;&#20998;&#21033;&#29992;LLM&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#21527;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RGPT&#65292;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#24378;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21453;&#22797;&#38598;&#25104;&#19968;&#32452;&#24378;&#22522;&#23398;&#20064;&#32773;&#65292;&#26469;&#29983;&#25104;&#19968;&#20010;&#19987;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;LLM&#12290;&#22522;&#23398;&#20064;&#32773;&#26159;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#35757;&#32451;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#24182;&#21453;&#22797;&#24494;&#35843;LLM&#19982;&#20043;&#26500;&#24314;&#30340;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#22522;&#23398;&#20064;&#32773;&#36890;&#36807;&#21453;&#22797;&#34701;&#21512;&#21069;&#20960;&#20010;&#23398;&#20064;&#32773;&#30340;&#21382;&#21490;&#39044;&#27979;&#32467;&#26524;&#65292;&#24418;&#25104;&#19968;&#20010;&#19987;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;LLM&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RGPT&#26126;&#26174;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperf
&lt;/p&gt;</description></item><item><title>AraSpider&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#22238;&#35793;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#22312;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07448</link><description>&lt;p&gt;
AraSpider&#65306;&#23454;&#29616;&#38463;&#25289;&#20271;&#35821;&#21040;SQL&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
AraSpider: Democratizing Arabic-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07448
&lt;/p&gt;
&lt;p&gt;
AraSpider&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#22238;&#35793;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#22312;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;AraSpider&#65292;&#36825;&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#21319;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#35813;&#30740;&#31350;&#27979;&#35797;&#20102;&#22235;&#20010;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#22312;&#23558;&#33521;&#25991;&#32763;&#35793;&#25104;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#21478;&#22806;&#65292;&#36824;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#22312;&#20174;&#38463;&#25289;&#20271;&#25991;&#26412;&#29983;&#25104;SQL&#26597;&#35810;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22238;&#35793;&#26174;&#33879;&#25552;&#39640;&#20102;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;Spider&#25968;&#25454;&#38598;&#19978;&#34987;&#35748;&#20026;&#26159;&#26368;&#20339;&#34920;&#29616;&#32773;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT 3.5&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#65292;&#32780;SQLCoder&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#19978;&#19979;&#25991;&#27169;&#24335;&#21644;&#37319;&#29992;&#22238;&#35793;&#31574;&#30053;&#32435;&#20837;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#26041;&#27861;&#35770;&#20197;&#23454;&#29616;&#32467;&#26524;&#22797;&#29616;&#24182;&#23558;&#25968;&#25454;&#38598;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#65292;&#31361;&#26174;&#20102;&#30740;&#31350;&#20419;&#36827;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents AraSpider, the first Arabic version of the Spider dataset, aimed at improving natural language processing (NLP) in the Arabic-speaking community. Four multilingual translation models were tested for their effectiveness in translating English to Arabic. Additionally, two models were assessed for their ability to generate SQL queries from Arabic text. The results showed that using back translation significantly improved the performance of both ChatGPT 3.5 and SQLCoder models, which are considered top performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The study underscores the importance of incorporating contextual schema and employing back translation strategies to enhance model performance in Arabic NLP tasks. Moreover, the provision of detailed methodologies for reproducibility and translation of the dataset into other languages highlights the research's commitment to promoting 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35814;&#32454;&#20998;&#26512;&#20102;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26576;&#20123;&#32593;&#32476;&#25366;&#25496;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#37096;&#20998;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#24037;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25345;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.07446</link><description>&lt;p&gt;
&#36136;&#37327;&#30830;&#23454;&#37325;&#35201;&#65306;&#23545;&#32593;&#32476;&#25366;&#25496;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quality Does Matter: A Detailed Look at the Quality and Utility of Web-Mined Parallel Corpora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35814;&#32454;&#20998;&#26512;&#20102;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26576;&#20123;&#32593;&#32476;&#25366;&#25496;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#37096;&#20998;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#24037;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#20004;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#33521;&#25991;-&#20711;&#20285;&#32599;&#35821;&#65292;&#33521;&#25991;-&#27888;&#31859;&#23572;&#35821;&#21644;&#20711;&#20285;&#32599;&#35821;-&#27888;&#31859;&#23572;&#35821;&#65289;&#30340;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#26681;&#25454;&#30456;&#20284;&#24230;&#26631;&#20934;&#23545;&#27599;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#25490;&#21517;&#65292;&#24182;&#23545;&#25490;&#21517;&#35821;&#26009;&#24211;&#30340;&#19981;&#21516;&#37096;&#20998;&#36827;&#34892;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#12290;&#25105;&#20204;&#26174;&#31034;&#19981;&#21516;&#37096;&#20998;&#30340;&#32593;&#32476;&#25366;&#25496;&#35821;&#26009;&#24211;&#23384;&#22312;&#26174;&#33879;&#30340;&#36136;&#37327;&#24046;&#24322;&#65292;&#24182;&#19988;&#36136;&#37327;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#32593;&#32476;&#25366;&#25496;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20854;&#25490;&#21517;&#26368;&#39640;&#30340;25k&#37096;&#20998;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#24037;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conducted a detailed analysis on the quality of web-mined corpora for two low-resource languages (making three language pairs, English-Sinhala, English-Tamil and Sinhala-Tamil). We ranked each corpus according to a similarity measure and carried out an intrinsic and extrinsic evaluation on different portions of this ranked corpus. We show that there are significant quality differences between different portions of web-mined corpora and that the quality varies across languages and datasets. We also show that, for some web-mined datasets, Neural Machine Translation (NMT) models trained with their highest-ranked 25k portion can be on par with human-curated datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#20219;&#21153;&#39537;&#21160;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#21457;&#34920;&#36798;&#29983;&#25104;&#65288;REG&#65289;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#35780;&#20272;&#20102;&#20998;&#21457;&#34920;&#36798;&#30340;&#36136;&#37327;&#65292;&#36824;&#36890;&#36807;&#20004;&#20010;&#20803;&#32423;&#20219;&#21153;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24341;&#29992;&#25104;&#21151;&#31243;&#24230;&#21644;&#25552;&#20986;&#26367;&#20195;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07432</link><description>&lt;p&gt;
&#20869;&#22312;&#20219;&#21153;&#39537;&#21160;&#30340;&#20998;&#21457;&#34920;&#36798;&#29983;&#25104;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Task-based Evaluation for Referring Expression Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#20219;&#21153;&#39537;&#21160;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#21457;&#34920;&#36798;&#29983;&#25104;&#65288;REG&#65289;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#35780;&#20272;&#20102;&#20998;&#21457;&#34920;&#36798;&#30340;&#36136;&#37327;&#65292;&#36824;&#36890;&#36807;&#20004;&#20010;&#20803;&#32423;&#20219;&#21153;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24341;&#29992;&#25104;&#21151;&#31243;&#24230;&#21644;&#25552;&#20986;&#26367;&#20195;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#20998;&#21457;&#34920;&#36798;&#29983;&#25104;&#65288;REG&#65289;&#27169;&#22411;&#30340;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#24471;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24847;&#22806;&#30340;&#32467;&#35770;&#65306;&#22312;\textsc{webnlg}&#19978;&#65292;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21457;&#34920;&#36798;&#65288;REs&#65289;&#19981;&#20165;&#19982;\textsc{webnlg}&#20013;&#30340;REs&#26080;&#27861;&#21306;&#20998;&#65292;&#32780;&#19988;&#19982;&#31616;&#21333;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;REs&#20063;&#26080;&#27861;&#21306;&#20998;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#23616;&#38480;&#21487;&#33021;&#28304;&#20110;&#32431;&#35780;&#20998;&#30340;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#65288;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#24120;&#35265;&#23454;&#36341;&#65289;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;REG&#27169;&#22411;&#30340;&#20869;&#22312;&#20219;&#21153;&#39537;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#38500;&#20102;&#35780;&#20272;REs&#30340;&#36136;&#37327;&#22806;&#65292;&#21442;&#19982;&#32773;&#36824;&#38656;&#35201;&#23436;&#25104;&#20004;&#20010;&#20803;&#32423;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#28041;&#21450;&#27599;&#20010;RE&#30340;&#24341;&#29992;&#25104;&#21151;&#31243;&#24230;&#65292;&#21478;&#19968;&#20010;&#20219;&#21153;&#35201;&#27714;&#21442;&#19982;&#32773;&#20026;&#27599;&#20010;RE&#25552;&#20986;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#35780;&#20272;&#30456;&#27604;&#65292;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#20102;&#27599;&#20010;REG&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: on \textsc{webnlg}, Referring Expressions (REs) generated by the state-of-the-art neural models were not only indistinguishable from the REs in \textsc{webnlg} but also from the REs generated by a simple rule-based system. Here, we argue that this limitation could stem from the use of a purely ratings-based human evaluation (which is a common practice in Natural Language Generation). To investigate these issues, we propose an intrinsic task-based evaluation for REG models, in which, in addition to rating the quality of REs, participants were asked to accomplish two meta-level tasks. One of these tasks concerns the referential success of each RE; the other task asks participants to suggest a better alternative for each RE. The outcomes suggest that, in comparison to previous evaluations, the new evaluation protocol assesses the performance of each REG model more comprehensive
&lt;/p&gt;</description></item><item><title>SALAD&#26159;&#19968;&#27454;&#26234;&#33021;AI&#35821;&#35328;&#21161;&#25163;&#24212;&#29992;&#65292;&#26088;&#22312;&#24110;&#21161;&#22806;&#22269;&#20154;&#23398;&#20064;&#26085;&#35821;&#12290;&#23427;&#25552;&#20379;&#20102;&#22810;&#31181;&#23398;&#20064;&#24037;&#20855;&#21644;&#21151;&#33021;&#65292;&#21253;&#25324;&#32763;&#35793;&#65292;&#35821;&#38899;&#35782;&#21035;&#65292;&#38899;&#39057;&#32763;&#35793;&#65292;&#35789;&#27719;&#36319;&#36394;&#31561;&#65292;&#24182;&#36890;&#36807;&#27599;&#26085;&#32763;&#35793;&#24110;&#21161;&#25552;&#39640;&#19982;&#27597;&#35821;&#20154;&#22763;&#30340;&#20132;&#27969;&#33021;&#21147;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;60%&#30340;&#22806;&#22269;&#20154;&#23545;SALAD&#25552;&#21319;&#26085;&#35821;&#33021;&#21147;&#26377;&#20449;&#24515;&#12290;&#35813;&#24212;&#29992;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20419;&#36827;&#26085;&#26412;&#31038;&#21306;&#30340;&#21253;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07431</link><description>&lt;p&gt;
SALAD: &#26234;&#33021;AI&#35821;&#35328;&#21161;&#25163;&#26085;&#24120;
&lt;/p&gt;
&lt;p&gt;
SALAD: Smart AI Language Assistant Daily
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07431
&lt;/p&gt;
&lt;p&gt;
SALAD&#26159;&#19968;&#27454;&#26234;&#33021;AI&#35821;&#35328;&#21161;&#25163;&#24212;&#29992;&#65292;&#26088;&#22312;&#24110;&#21161;&#22806;&#22269;&#20154;&#23398;&#20064;&#26085;&#35821;&#12290;&#23427;&#25552;&#20379;&#20102;&#22810;&#31181;&#23398;&#20064;&#24037;&#20855;&#21644;&#21151;&#33021;&#65292;&#21253;&#25324;&#32763;&#35793;&#65292;&#35821;&#38899;&#35782;&#21035;&#65292;&#38899;&#39057;&#32763;&#35793;&#65292;&#35789;&#27719;&#36319;&#36394;&#31561;&#65292;&#24182;&#36890;&#36807;&#27599;&#26085;&#32763;&#35793;&#24110;&#21161;&#25552;&#39640;&#19982;&#27597;&#35821;&#20154;&#22763;&#30340;&#20132;&#27969;&#33021;&#21147;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;60%&#30340;&#22806;&#22269;&#20154;&#23545;SALAD&#25552;&#21319;&#26085;&#35821;&#33021;&#21147;&#26377;&#20449;&#24515;&#12290;&#35813;&#24212;&#29992;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20419;&#36827;&#26085;&#26412;&#31038;&#21306;&#30340;&#21253;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SALAD&#26159;&#19968;&#27454;&#30001;AI&#39537;&#21160;&#30340;&#35821;&#35328;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#65292;&#26088;&#22312;&#24110;&#21161;&#22806;&#22269;&#20154;&#23398;&#20064;&#26085;&#35821;&#12290;&#23427;&#25552;&#20379;&#20102;&#27721;&#23383;-&#20551;&#21517;-&#32599;&#39532;&#23383;&#30340;&#32763;&#35793;&#65292;&#35821;&#38899;&#35782;&#21035;&#65292;&#32763;&#35793;&#38899;&#39057;&#65292;&#35789;&#27719;&#36319;&#36394;&#65292;&#35821;&#27861;&#35299;&#37322;&#65292;&#20197;&#21450;&#30001;&#26032;&#23398;&#21040;&#30340;&#35789;&#27719;&#29983;&#25104;&#30340;&#27468;&#26354;&#12290;&#35813;&#24212;&#29992;&#38024;&#23545;&#21021;&#23398;&#32773;&#21644;&#20013;&#32423;&#23398;&#20064;&#32773;&#65292;&#26088;&#22312;&#20351;&#35821;&#35328;&#20064;&#24471;&#26356;&#21152;&#21487;&#33719;&#24471;&#21644;&#24841;&#24555;&#12290;SALAD&#21033;&#29992;&#27599;&#26085;&#32763;&#35793;&#26469;&#22686;&#24378;&#19982;&#27597;&#35821;&#20154;&#22763;&#30340;&#27969;&#21033;&#24230;&#21644;&#20132;&#27969;&#33298;&#36866;&#24230;&#12290;&#20027;&#35201;&#30446;&#26631;&#21253;&#25324;&#26377;&#25928;&#30340;&#26085;&#35821;&#23398;&#20064;&#65292;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#36827;&#23637;&#36319;&#36394;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;&#26085;&#26412;&#30340;&#22806;&#22269;&#20154;&#20013;&#65292;&#26377;39%&#22312;&#19982;&#26085;&#26412;&#20154;&#20132;&#35848;&#26102;&#24863;&#21040;&#19981;&#36866;&#12290;&#36229;&#36807;60%&#30340;&#22806;&#22269;&#20154;&#34920;&#31034;&#23545;SALAD&#25552;&#21319;&#20182;&#20204;&#30340;&#26085;&#35821;&#33021;&#21147;&#26377;&#20449;&#24515;&#12290;&#35813;&#24212;&#29992;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35821;&#38899;&#35782;&#21035;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#24357;&#21512;&#35821;&#35328;&#38548;&#38402;&#65292;&#20419;&#36827;&#26085;&#26412;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
SALAD is an AI-driven language-learning application designed to help foreigners learn Japanese. It offers translations in Kanji-Kana-Romaji, speech recognition, translated audio, vocabulary tracking, grammar explanations, and songs generated from newly learned words. The app targets beginners and intermediate learners, aiming to make language acquisition more accessible and enjoyable. SALAD uses daily translations to enhance fluency and comfort in communication with native speakers. The primary objectives include effective Japanese language learning, user engagement, and progress tracking. A survey by us found that 39% of foreigners in Japan face discomfort in conversations with Japanese speakers. Over 60% of foreigners expressed confidence in SALAD's ability to enhance their Japanese language skills. The app uses large language models, speech recognition, and diffusion models to bridge the language gap and foster a more inclusive community in Japan.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25512;&#20986;&#20102;&#31532;&#19968;&#20010;&#21452;&#35821;&#26694;&#26550;Tois'on de Oro&#65292;&#29992;&#20110;&#36816;&#29992;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#35821;&#25351;&#23548;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#29616;&#26377;LLMs&#22312;&#22810;&#35821;&#35328;&#24615;&#33021;&#19978;&#23384;&#22312;&#24046;&#36317;&#21644;&#20559;&#35265;&#65292;&#32780;&#20316;&#32773;&#25552;&#20986;&#30340;FinMA-ES&#27169;&#22411;&#22312;&#35199;&#29677;&#29273;&#35821;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;SOTA LLMs&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.07405</link><description>&lt;p&gt;
D'olares&#36824;&#26159;Dollars&#65311;&#25581;&#31034;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#36130;&#32463;&#30740;&#31350;&#19982;&#24212;&#29992;&#20013;&#21452;&#35821;&#33021;&#21147;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
D\'olares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25512;&#20986;&#20102;&#31532;&#19968;&#20010;&#21452;&#35821;&#26694;&#26550;Tois'on de Oro&#65292;&#29992;&#20110;&#36816;&#29992;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#35821;&#25351;&#23548;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#29616;&#26377;LLMs&#22312;&#22810;&#35821;&#35328;&#24615;&#33021;&#19978;&#23384;&#22312;&#24046;&#36317;&#21644;&#20559;&#35265;&#65292;&#32780;&#20316;&#32773;&#25552;&#20986;&#30340;FinMA-ES&#27169;&#22411;&#22312;&#35199;&#29677;&#29273;&#35821;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;SOTA LLMs&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35199;&#29677;&#29273;&#35821;&#22312;&#20840;&#29699;&#37329;&#34701;&#34892;&#19994;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#19982;&#33521;&#35821;&#30456;&#27604;&#65292;&#35199;&#29677;&#29273;&#35821;&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#24212;&#29992;&#30740;&#31350;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#23588;&#20854;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#20195;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;Tois'on de Oro&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#24314;&#31435;&#25351;&#23548;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#21452;&#35821;&#37329;&#34701;LLMs&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#21452;&#35821;&#26694;&#26550;&#65292;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#30340;&#37329;&#34701;LLMs&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20005;&#26684;&#31579;&#36873;&#30340;&#21452;&#35821;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;15&#20010;&#25968;&#25454;&#38598;&#30340;&#36229;&#36807;144K&#20010;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#26679;&#26412;&#65292;&#28085;&#30422;7&#20010;&#20219;&#21153;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FinMA-ES&#65292;&#19968;&#31181;&#19987;&#20026;&#21452;&#35821;&#37329;&#34701;&#24212;&#29992;&#35774;&#35745;&#30340;LLM&#12290;&#25105;&#20204;&#20351;&#29992;FLARE-ES&#36827;&#34892;&#20102;&#27169;&#22411;&#21644;&#29616;&#26377;LLMs&#30340;&#35780;&#20272;&#65292;FLARE-ES &#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;&#21452;&#35821;&#24615;&#33021;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;21&#20010;&#25968;&#25454;&#38598;&#21644;9&#20010;&#20219;&#21153;&#12290;FLARE-ES&#22522;&#20934;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;LLMs&#23384;&#22312;&#26174;&#33879;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#24046;&#36317;&#21644;&#20559;&#35265;&#12290;FinMA-ES&#27169;&#22411;&#22312;&#35199;&#29677;&#29273;&#35821;&#20013;&#36229;&#36807;&#20102;GPT-4&#31561;SOTA LLMs&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite Spanish's pivotal role in the global finance industry, a pronounced gap exists in Spanish financial natural language processing (NLP) and application studies compared to English, especially in the era of large language models (LLMs). To bridge this gap, we unveil Tois\'on de Oro, the first bilingual framework that establishes instruction datasets, finetuned LLMs, and evaluation benchmark for financial LLMs in Spanish joint with English. We construct a rigorously curated bilingual instruction dataset including over 144K Spanish and English samples from 15 datasets covering 7 tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual financial applications. We evaluate our model and existing LLMs using FLARE-ES, the first comprehensive bilingual evaluation benchmark with 21 datasets covering 9 tasks. The FLARE-ES benchmark results reveal a significant multilingual performance gap and bias in existing LLMs. FinMA-ES models surpass SOTA LLMs such as GPT-4 in Spani
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#29983;&#25104;&#24544;&#23454;&#35299;&#37322;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#38646;-shot&#25552;&#31034;&#24120;&#24120;&#23548;&#33268;&#19981;&#24544;&#23454;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#20248;&#21270;&#65288;MADR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#20154;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#35299;&#37322;&#30340;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07401</link><description>&lt;p&gt;
&#33021;&#22815;&#20026;&#20107;&#23454;&#26680;&#26597;&#25552;&#20379;&#24544;&#23454;&#35299;&#37322;&#21527;&#65311;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#23454;&#29616;&#24544;&#23454;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#29983;&#25104;&#24544;&#23454;&#35299;&#37322;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#38646;-shot&#25552;&#31034;&#24120;&#24120;&#23548;&#33268;&#19981;&#24544;&#23454;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#20248;&#21270;&#65288;MADR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#20154;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#35299;&#37322;&#30340;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#30740;&#31350;&#24191;&#27867;&#25506;&#35752;&#20102;&#39564;&#35777;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#36825;&#23545;&#20110;&#29992;&#25143;&#30340;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#29983;&#25104;&#24544;&#23454;&#35299;&#37322;&#30340;&#33021;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36825;&#31181;&#35299;&#37322;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#38646;-shot&#25552;&#31034;&#24448;&#24448;&#23548;&#33268;&#19981;&#24544;&#23454;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#20248;&#21270;&#65288;MADR&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#20154;&#65292;&#22312;&#36845;&#20195;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#21457;&#25381;&#21508;&#33258;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#30446;&#26631;&#26159;&#22686;&#24378;&#29983;&#25104;&#35299;&#37322;&#30340;&#24544;&#23454;&#24615;&#12290;MADR&#30830;&#20445;&#26368;&#32456;&#35299;&#37322;&#32463;&#36807;&#20005;&#26684;&#39564;&#35777;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#19981;&#24544;&#23454;&#22240;&#32032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#19982;&#25552;&#20379;&#30340;&#35777;&#25454;&#23494;&#20999;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MADR&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#35777;&#25454;&#30340;&#19968;&#33268;&#24615;&#65292;&#25512;&#21160;&#20102;&#21487;&#20449;&#30340;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-checking research has extensively explored verification but less so the generation of natural-language explanations, crucial for user trust. While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness. To address these challenges, we propose the Multi-Agent Debate Refinement (MADR) framework, leveraging multiple LLMs as agents with diverse roles in an iterative refining process aimed at enhancing faithfulness in generated explanations. MADR ensures that the final explanation undergoes rigorous validation, significantly reducing the likelihood of unfaithful elements and aligning closely with the provided evidence. Experimental results demonstrate that MADR significantly improves the faithfulness of LLM-generated explanations to the evidence, advancing the credib
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25551;&#36848;&#20102;&#22312;&#38750;&#27954;&#24320;&#21457;&#21644;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#24037;&#20855;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;SuaCode&#23398;&#20064;&#32534;&#30721;&#24212;&#29992;&#12289;AutoGrad&#33258;&#21160;&#35780;&#20998;&#21644;&#21453;&#39304;&#24037;&#20855;&#12289;&#20195;&#30721;&#25220;&#34989;&#26816;&#27979;&#24037;&#20855;&#20197;&#21450;&#21452;&#35821;AI&#25945;&#24072;Kwame&#12290;&#36825;&#20123;&#24037;&#20855;&#26377;&#21161;&#20110;&#35299;&#20915;&#38750;&#27954;&#23398;&#29983;&#22312;&#25945;&#32946;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07397</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25512;&#36827;&#38750;&#27954;&#31185;&#23398;&#21644;&#35745;&#31639;&#25945;&#32946;&#65306;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Leveraging AI to Advance Science and Computing Education across Africa: Progress, Challenges, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07397
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25551;&#36848;&#20102;&#22312;&#38750;&#27954;&#24320;&#21457;&#21644;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#24037;&#20855;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;SuaCode&#23398;&#20064;&#32534;&#30721;&#24212;&#29992;&#12289;AutoGrad&#33258;&#21160;&#35780;&#20998;&#21644;&#21453;&#39304;&#24037;&#20855;&#12289;&#20195;&#30721;&#25220;&#34989;&#26816;&#27979;&#24037;&#20855;&#20197;&#21450;&#21452;&#35821;AI&#25945;&#24072;Kwame&#12290;&#36825;&#20123;&#24037;&#20855;&#26377;&#21161;&#20110;&#35299;&#20915;&#38750;&#27954;&#23398;&#29983;&#22312;&#25945;&#32946;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#27954;&#22823;&#38470;&#65292;&#23398;&#29983;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25945;&#32946;&#25361;&#25112;&#65292;&#21253;&#25324;&#33719;&#21462;&#35745;&#31639;&#26426;&#12289;&#32593;&#32476;&#36830;&#25509;&#12289;&#21487;&#38752;&#30005;&#21147;&#21644;&#21512;&#26684;&#25945;&#24072;&#31561;&#22522;&#26412;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#65288;&#22914;BERT&#21644;GPT-4&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#20419;&#36827;&#25945;&#32946;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#24448;&#24448;&#22312;&#35199;&#26041;&#25945;&#32946;&#29615;&#22659;&#20013;&#36827;&#34892;&#37096;&#32626;&#21644;&#35780;&#20272;&#65292;&#23545;&#38750;&#27954;&#23398;&#29983;&#38754;&#20020;&#30340;&#29420;&#29305;&#38656;&#27714;&#21644;&#25361;&#25112;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;&#38750;&#27954;&#24320;&#21457;&#21644;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#24037;&#20855;&#30340;&#24037;&#20316;&#65306;&#65288;1&#65289;SuaCode&#65292;&#19968;&#27454;AI&#21160;&#21147;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20351;&#38750;&#27954;&#20154;&#21487;&#20197;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#23398;&#20064;&#32534;&#31243;&#65292;&#65288;2&#65289;AutoGrad&#65292;&#29992;&#20110;&#22270;&#24418;&#21644;&#20132;&#20114;&#24335;&#32534;&#31243;&#20316;&#19994;&#30340;&#33258;&#21160;&#35780;&#20998;&#21644;&#21453;&#39304;&#24037;&#20855;&#65292;&#65288;3&#65289;&#19968;&#31181;&#20195;&#30721;&#25220;&#34989;&#26816;&#27979;&#24037;&#20855;&#65292;&#23637;&#31034;&#20102;&#25220;&#34989;&#30340;&#21487;&#35270;&#35777;&#25454;&#65292;&#65288;4&#65289;Kwame&#65292;&#19968;&#27454;&#21452;&#35821;&#30340;AI&#25945;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;
Across the African continent, students grapple with various educational challenges, including limited access to essential resources such as computers, internet connectivity, reliable electricity, and a shortage of qualified teachers. Despite these challenges, recent advances in AI such as BERT, and GPT-4 have demonstrated their potential for advancing education. Yet, these AI tools tend to be deployed and evaluated predominantly within the context of Western educational settings, with limited attention directed towards the unique needs and challenges faced by students in Africa. In this book chapter, we describe our works developing and deploying AI in Education tools in Africa: (1) SuaCode, an AI-powered app that enables Africans to learn to code using their smartphones, (2) AutoGrad, an automated grading, and feedback tool for graphical and interactive coding assignments, (3) a tool for code plagiarism detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI teac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Chain-of-Layer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;&#23454;&#20307;&#38598;&#20013;&#24402;&#32435;&#20998;&#31867;&#20307;&#31995;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38598;&#25104;&#30340;&#25490;&#21517;&#36807;&#28388;&#22120;&#26469;&#20943;&#23569;&#38169;&#35823;&#65292;Chain-of-Layer&#22312;&#22235;&#20010;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07386</link><description>&lt;p&gt;
Chain-of-Layer&#65306;&#36890;&#36807;&#26377;&#38480;&#31034;&#20363;&#36845;&#20195;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20307;&#31995;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Chain-of-Layer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;&#23454;&#20307;&#38598;&#20013;&#24402;&#32435;&#20998;&#31867;&#20307;&#31995;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38598;&#25104;&#30340;&#25490;&#21517;&#36807;&#28388;&#22120;&#26469;&#20943;&#23569;&#38169;&#35823;&#65292;Chain-of-Layer&#22312;&#22235;&#20010;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;&#20307;&#31995;&#24402;&#32435;&#23545;&#20110;&#32593;&#32476;&#25628;&#32034;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#38382;&#31572;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#25163;&#21160;&#25972;&#29702;&#20998;&#31867;&#20307;&#31995;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#25104;&#26412;&#65292;&#22240;&#27492;&#33258;&#21160;&#26500;&#24314;&#20998;&#31867;&#20307;&#31995;&#38750;&#24120;&#26377;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Chain-of-Layer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;&#23454;&#20307;&#38598;&#20013;&#24402;&#32435;&#20998;&#31867;&#20307;&#31995;&#12290;Chain-of-Layer&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#27599;&#19968;&#23618;&#36873;&#25321;&#30456;&#20851;&#20505;&#36873;&#23454;&#20307;&#65292;&#24182;&#36880;&#27493;&#20174;&#19978;&#21040;&#19979;&#26500;&#24314;&#20998;&#31867;&#20307;&#31995;&#12290;&#20026;&#20102;&#20943;&#23569;&#38169;&#35823;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#38598;&#25104;&#30340;&#25490;&#21517;&#36807;&#28388;&#22120;&#65292;&#22312;&#27599;&#19968;&#27425;&#36845;&#20195;&#20013;&#20943;&#23569;&#29983;&#25104;&#30340;&#34394;&#26500;&#20869;&#23481;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Chain-of-Layer&#22312;&#22235;&#20010;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic taxonomy induction is crucial for web search, recommendation systems, and question answering. Manual curation of taxonomies is expensive in terms of human effort, making automatic taxonomy construction highly desirable. In this work, we introduce Chain-of-Layer which is an in-context learning framework designed to induct taxonomies from a given set of entities. Chain-of-Layer breaks down the task into selecting relevant candidate entities in each layer and gradually building the taxonomy from top to bottom. To minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the hallucinated content generated at each iteration. Through extensive experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art performance on four real-world benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#29983;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;</title><link>https://arxiv.org/abs/2402.07383</link><description>&lt;p&gt;
&#20351;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#33258;&#30001;&#22320;&#20135;&#29983;&#31505;&#22768;
&lt;/p&gt;
&lt;p&gt;
Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#29983;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31505;&#22768;&#26159;&#20154;&#31867;&#35821;&#38899;&#20013;&#26368;&#34920;&#36798;&#24615;&#21644;&#33258;&#28982;&#30340;&#19968;&#37096;&#20998;&#65292;&#20256;&#36798;&#30528;&#24773;&#24863;&#12289;&#31038;&#20132;&#26263;&#31034;&#21644;&#24189;&#40664;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#31995;&#32479;&#32570;&#20047;&#20135;&#29983;&#36924;&#30495;&#19988;&#21512;&#36866;&#30340;&#31505;&#22768;&#30340;&#33021;&#21147;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#34429;&#28982;&#20043;&#21069;&#26377;&#24037;&#20316;&#29983;&#25104;&#20102;&#33258;&#28982;&#30340;&#31505;&#22768;&#65292;&#20294;&#22312;&#25511;&#21046;&#29983;&#25104;&#30340;&#31505;&#22768;&#30340;&#26102;&#26426;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#21487;&#20197;&#22522;&#20110;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#30340;&#38646;&#26679;&#26412;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20135;&#29983;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ELaTE&#36890;&#36807;&#38899;&#39057;&#25552;&#31034;&#26469;&#27169;&#20223;&#22768;&#38899;&#29305;&#24449;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#26469;&#25351;&#31034;&#25152;&#29983;&#25104;&#35821;&#38899;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#36755;&#20837;&#26469;&#25511;&#21046;&#31505;&#22768;&#34920;&#24773;&#65292;&#21487;&#20197;&#26159;&#31505;&#22768;&#30340;&#36215;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#65292;&#25110;&#21253;&#21547;&#35201;&#27169;&#20223;&#30340;&#31505;&#22768;&#30340;&#21478;&#22806;&#38899;&#39057;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#25214;&#21040;&#30340;&#25216;&#26415;&#22522;&#30784;&#36827;&#34892;&#20102;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Laughter is one of the most expressive and natural aspects of human speech, conveying emotions, social cues, and humor. However, most text-to-speech (TTS) systems lack the ability to produce realistic and appropriate laughter sounds, limiting their applications and user experience. While there have been prior works to generate natural laughter, they fell short in terms of controlling the timing and variety of the laughter to be generated. In this work, we propose ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker based on a short audio prompt with precise control of laughter timing and expression. Specifically, ELaTE works on the audio prompt to mimic the voice characteristic, the text prompt to indicate the contents of the generated speech, and the input to control the laughter expression, which can be either the start and end times of laughter, or the additional audio prompt that contains laughter to be mimicked. We develop our model based on the foundati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;2016&#24180;&#21644;2020&#24180;&#30340;&#36873;&#20030;&#25968;&#25454;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#20351;&#29992;&#23454;&#35777;&#25968;&#25454;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30410;&#22788;&#22312;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#36825;&#23545;&#23454;&#26045;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07368</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#35780;&#20272;&#20998;&#32452;&#20195;&#34920;&#24314;&#27169;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;2016&#24180;&#21644;2020&#24180;&#30340;&#36873;&#20030;&#25968;&#25454;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#20351;&#29992;&#23454;&#35777;&#25968;&#25454;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30410;&#22788;&#22312;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#36825;&#23545;&#23454;&#26045;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;2016&#24180;&#21644;2020&#24180;&#32654;&#22269;&#20840;&#22269;&#36873;&#20030;&#30740;&#31350;&#30340;&#25968;&#25454;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#65288;SRMs&#65289;&#20174;&#23454;&#35777;&#25968;&#25454;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#21709;&#24212;&#21464;&#37327;&#21644;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#20351;&#29992;&#23454;&#35777;&#25968;&#25454;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30410;&#22788;&#22312;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#26377;&#26102;&#23545;&#26576;&#20010;&#20154;&#21475;&#23376;&#32676;&#32452;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#20294;&#23545;&#20854;&#20182;&#20154;&#21475;&#23376;&#32676;&#32452;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#31215;&#26497;&#24433;&#21709;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;SRM&#30340;&#19981;&#20844;&#24179;&#30410;&#22788;&#20026;&#23454;&#26045;SRM&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#20381;&#36182;&#20110;&#20854;&#30340;&#20915;&#31574;&#32773;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#23545;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#30340;&#38656;&#27714;&#65292;&#36825;&#20123;&#22522;&#20934;&#19981;&#20165;&#27979;&#35797;&#24544;&#23454;&#24230;&#65292;&#36824;&#27979;&#35797;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#40657;&#23458;&#38382;&#39064;&#65292;&#38024;&#23545;&#22238;&#22797;&#38271;&#24230;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#24314;&#31435;&#21487;&#38752;&#30340;&#35780;&#20272;&#21327;&#35758;&#21644;&#25913;&#36827;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20943;&#36731;&#38271;&#24230;&#20559;&#24046;&#30340;&#36229;&#21442;&#25968;&#21644;&#25216;&#24039;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.07319</link><description>&lt;p&gt;
ODIN: &#33073;&#32806;&#22870;&#21169;&#32531;&#35299;RLHF&#20013;&#30340;&#40657;&#23458;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ODIN: Disentangled Reward Mitigates Hacking in RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#40657;&#23458;&#38382;&#39064;&#65292;&#38024;&#23545;&#22238;&#22797;&#38271;&#24230;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#24314;&#31435;&#21487;&#38752;&#30340;&#35780;&#20272;&#21327;&#35758;&#21644;&#25913;&#36827;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20943;&#36731;&#38271;&#24230;&#20559;&#24046;&#30340;&#36229;&#21442;&#25968;&#21644;&#25216;&#24039;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;LLMs&#19978;&#20174;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#21709;&#24212;&#38271;&#24230;&#19978;&#30340;&#22870;&#21169;&#40657;&#23458;&#38382;&#39064;&#12290;LLMs&#30340;&#26684;&#24335;&#33391;&#22909;&#20294;&#19981;&#22826;&#26377;&#29992;&#30340;&#22238;&#22797;&#24448;&#24448;&#20250;&#27450;&#39575;LLMs&#29978;&#33267;&#20154;&#31867;&#35780;&#20272;&#32773;&#20197;&#33719;&#24471;&#39640;&#20998;&#12290;&#21516;&#26679;&#30340;&#38382;&#39064;&#20063;&#23384;&#22312;&#20110;RL&#20013;&#30340;&#26576;&#20123;&#22870;&#21169;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#21644;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#21487;&#38752;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#35757;&#32451;&#37197;&#32622;&#20043;&#38388;&#30340;LLM&#35780;&#20272;&#20998;&#25968;&#21644;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#36229;&#21442;&#25968;&#24471;&#21040;&#30340;&#21709;&#24212;&#38271;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22522;&#20110;&#36825;&#20010;&#35780;&#20272;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;RL&#20013;&#29992;&#20110;&#20943;&#36731;&#38271;&#24230;&#20559;&#24046;&#30340;&#36229;&#21442;&#25968;&#21644;&#25216;&#24039;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#36890;&#36807;&#22312;&#20849;&#20139;&#29305;&#24449;&#34920;&#31034;&#19978;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#32447;&#24615;&#22836;&#26469;&#25913;&#36827;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#22870;&#21169;&#65292;&#19968;&#20010;&#35757;&#32451;&#26469;&#19982;&#38271;&#24230;&#30456;&#20851;&#65292;&#21478;&#19968;&#20010;&#35757;&#32451;&#26469;&#19982;&#20869;&#23481;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the oth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07309</link><description>&lt;p&gt;
HyperBERT:&#23558;&#28151;&#21512;&#36229;&#22270;&#24863;&#30693;&#23618;&#19982;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#36890;&#36807;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#26631;&#35760;&#65292;&#34920;&#36798;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#36229;&#36793;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36229;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#21516;&#26102;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#20840;&#37096;&#20869;&#23481;&#21644;&#33410;&#28857;&#23646;&#24615;&#20013;&#30340;&#20016;&#23500;&#35821;&#35328;&#23646;&#24615;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20026;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#24341;&#20837;&#19987;&#38376;&#30340;&#36229;&#22270;&#24863;&#30693;&#23618;&#12290;&#36825;&#20123;&#23618;&#23558;&#39640;&#38454;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#24341;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20013;&#30340;&#39640;&#38454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26377;&#38480;&#30340;&#30005;&#21147;&#21464;&#21387;&#22120;&#25925;&#38556;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25925;&#38556;&#39044;&#27979;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07283</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#30005;&#21147;&#21464;&#21387;&#22120;&#25925;&#38556;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Power Transformer Fault Prediction Based on Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26377;&#38480;&#30340;&#30005;&#21147;&#21464;&#21387;&#22120;&#25925;&#38556;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25925;&#38556;&#39044;&#27979;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#30005;&#21147;&#21464;&#21387;&#22120;&#20165;&#26377;&#26377;&#38480;&#30340;&#25925;&#38556;&#25968;&#25454;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#30340;&#36816;&#32500;&#24037;&#20855;&#23545;&#28508;&#22312;&#25925;&#38556;&#30340;&#39044;&#27979;&#33021;&#21147;&#26377;&#38480;&#12290;&#30001;&#20110;&#25925;&#38556;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24456;&#38590;&#26377;&#25928;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;Knowledge Graph&#65292;KG&#65289;&#25216;&#26415;&#19982;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;Gradient Boosting Decision Trees&#65292;GBDT&#65289;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#20174;&#23569;&#37327;&#30340;&#39640;&#32500;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#25972;&#21512;&#20102;&#24433;&#21709;&#21464;&#21387;&#22120;&#25925;&#38556;&#30340;&#21508;&#31181;&#22240;&#32032;&#21644;&#21382;&#21490;&#36816;&#34892;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#25925;&#38556;&#29305;&#24449;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#23545;&#30005;&#21147;&#21464;&#21387;&#22120;&#30340;&#20934;&#30830;&#23433;&#20840;&#29366;&#24577;&#35780;&#20272;&#21644;&#25925;&#38556;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;Artificial Neural Networks&#65292;ANN&#65289;&#21644;&#36923;&#36753;&#22238;&#24402;&#65288;Logistic Regression&#65292;LR&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the challenge of learning with limited fault data for power transformers. Traditional operation and maintenance tools lack effective predictive capabilities for potential faults. The scarcity of extensive fault data makes it difficult to apply machine learning techniques effectively. To solve this problem, we propose a novel approach that leverages the knowledge graph (KG) technology in combination with gradient boosting decision trees (GBDT). This method is designed to efficiently learn from a small set of high-dimensional data, integrating various factors influencing transformer faults and historical operational data. Our approach enables accurate safe state assessments and fault analyses of power transformers despite the limited fault characteristic data. Experimental results demonstrate that this method outperforms other learning approaches in prediction accuracy, such as artificial neural networks (ANN) and logistic regression (LR). Furthermore, it offers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;</title><link>https://arxiv.org/abs/2402.07282</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35802;&#23454;&#19982;&#24110;&#21161;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#24110;&#21161;&#21548;&#20247;&#32780;&#36817;&#20284;&#30495;&#30456;&#65292;&#20363;&#22914;&#32422;&#30053;&#26102;&#38388;&#25110;&#30465;&#30053;&#32454;&#33410;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22788;&#29702;&#36825;&#31181;&#24494;&#22937;&#30340;&#26435;&#34913;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#27169;&#22411;&#21644;&#26088;&#22312;&#25551;&#36848;&#20154;&#31867;&#34892;&#20026;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;LLMs&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#24182;&#25506;&#35752;&#20102;&#20248;&#21270;&#20154;&#31867;&#20559;&#22909;&#25110;&#25512;&#29702;&#26102;&#24605;&#32771;&#23545;&#36825;&#20123;&#26435;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#20351;LLMs&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#32780;&#19981;&#26159;&#35802;&#23454;&#12290;&#26368;&#21518;&#65292;GPT-4 Turbo&#23637;&#31034;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#24212;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#21363;&#20351;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#20063;&#21487;&#20197;&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#34987;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22238;&#39038;&#29255;&#27573;&#35782;&#21035;&#65292;&#22312;&#25925;&#20107;&#38405;&#35835;&#20013;&#36890;&#36807;&#22238;&#39038;&#20043;&#21069;&#30340;&#37325;&#35201;&#20803;&#32032;&#26469;&#36741;&#21161;&#29702;&#35299;&#27491;&#22312;&#36827;&#34892;&#30340;&#24773;&#33410;&#12290;&#35813;&#20219;&#21153;&#23545;PLMs&#12289;LLMs&#21644;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#29255;&#27573;&#20043;&#38388;&#30340;&#24773;&#33410;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07271</link><description>&lt;p&gt;
&#20197;&#25925;&#20107;&#24635;&#32467;&#29255;&#27573;&#36741;&#21161;&#38405;&#35835;&#65306;&#25925;&#20107;&#38405;&#35835;&#20013;&#30340;&#22238;&#39038;&#29255;&#27573;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Previously on the Stories: Recap Snippet Identification for Story Reading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22238;&#39038;&#29255;&#27573;&#35782;&#21035;&#65292;&#22312;&#25925;&#20107;&#38405;&#35835;&#20013;&#36890;&#36807;&#22238;&#39038;&#20043;&#21069;&#30340;&#37325;&#35201;&#20803;&#32032;&#26469;&#36741;&#21161;&#29702;&#35299;&#27491;&#22312;&#36827;&#34892;&#30340;&#24773;&#33410;&#12290;&#35813;&#20219;&#21153;&#23545;PLMs&#12289;LLMs&#21644;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#29255;&#27573;&#20043;&#38388;&#30340;&#24773;&#33410;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#30005;&#35270;&#21095;&#20013;&#30340;"&#21069;&#24773;&#22238;&#39038;"&#65292;&#22238;&#39038;&#29255;&#27573;&#21487;&#20197;&#36890;&#36807;&#22238;&#24518;&#35835;&#32773;&#22312;&#20043;&#21069;&#30340;&#25991;&#26412;&#20013;&#30340;&#37325;&#35201;&#20803;&#32032;&#26469;&#24110;&#21161;&#20070;&#31821;&#38405;&#35835;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27491;&#22312;&#36827;&#34892;&#30340;&#24773;&#33410;&#12290;&#23613;&#31649;&#20854;&#26377;&#29992;&#24615;&#65292;&#20294;&#36825;&#31181;&#24212;&#29992;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35813;&#26377;&#29992;&#20219;&#21153;&#30340;&#22522;&#20934;-&#22238;&#39038;&#29255;&#27573;&#35782;&#21035;&#65292;&#24182;&#20351;&#29992;&#25163;&#24037;&#35780;&#20272;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#20219;&#21153;&#23545;&#20110;PLMs&#12289;LLMs&#21644;&#25552;&#20986;&#30340;&#26041;&#27861;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#35813;&#20219;&#21153;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#29255;&#27573;&#20043;&#38388;&#24773;&#33410;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similar to the "previously-on" scenes in TV shows, recaps can help book reading by recalling the readers' memory about the important elements in previous texts to better understand the ongoing plot. Despite its usefulness, this application has not been well studied in the NLP community. We propose the first benchmark on this useful task called Recap Snippet Identification with a hand-crafted evaluation dataset. Our experiments show that the proposed task is challenging to PLMs, LLMs, and proposed methods as the task requires a deep understanding of the plot correlation between snippets.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#25512;&#21160;&#20102;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#35821;&#20041;&#23618;&#27425;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#26469;&#25913;&#36827;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#65292;&#20182;&#20204;&#22312;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#36873;&#25321;&#20102;&#26368;&#32456;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07270</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#20854;&#35821;&#20041;&#23618;&#27425;&#65292;&#24320;&#23637;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#24335;VQA&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#25512;&#21160;&#20102;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#35821;&#20041;&#23618;&#27425;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#26469;&#25913;&#36827;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#65292;&#20182;&#20204;&#22312;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#36873;&#25321;&#20102;&#26368;&#32456;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#24037;&#20316;&#12290;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#24182;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#21517;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#21487;&#20197;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#35780;&#20272;&#65292;&#24182;&#19982;&#21028;&#21035;&#24615;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#20102;&#25913;&#21892;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26631;&#31614;&#31354;&#38388;&#30340;&#35821;&#20041;&#23618;&#27425;&#26469;&#25552;&#20986;&#20851;&#20110;&#22522;&#20934;&#31867;&#21035;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;LLM&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#32473;&#23450;&#22522;&#20934;&#31572;&#26696;&#30340;&#27169;&#22411;&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#65292;&#22522;&#20110;&#27492;&#20915;&#23450;&#26368;&#32456;&#24230;&#37327;&#26631;&#20934;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our be
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#23391;&#21152;&#25289;&#35821;&#21644;&#21360;&#22320;&#35821;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;5062&#20010;&#34384;&#24453;&#35328;&#35770;/&#23545;&#25239;&#35328;&#35770;&#23545;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#20960;&#31181;&#22522;&#32447;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#36866;&#24403;&#30340;&#23545;&#25239;&#35328;&#35770;&#12290;&#35266;&#23519;&#21457;&#29616;&#65292;&#21333;&#35821;&#35774;&#32622;&#30340;&#24615;&#33021;&#26368;&#20339;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#36716;&#31227;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#29983;&#25104;&#23545;&#25239;&#35328;&#35770;&#65292;&#23588;&#20854;&#26159;&#24403;&#35821;&#35328;&#23646;&#20110;&#21516;&#19968;&#35821;&#35328;&#23478;&#26063;&#26102;&#65292;&#21487;&#36801;&#31227;&#24615;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.07262</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#23545;&#21360;&#24230;&#35821;&#35328;&#36827;&#34892;&#23545;&#25239;&#35328;&#35770;&#29983;&#25104;&#65306;&#20197;&#23391;&#21152;&#25289;&#35821;&#21644;&#21360;&#22320;&#35821;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#23391;&#21152;&#25289;&#35821;&#21644;&#21360;&#22320;&#35821;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;5062&#20010;&#34384;&#24453;&#35328;&#35770;/&#23545;&#25239;&#35328;&#35770;&#23545;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#20960;&#31181;&#22522;&#32447;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#36866;&#24403;&#30340;&#23545;&#25239;&#35328;&#35770;&#12290;&#35266;&#23519;&#21457;&#29616;&#65292;&#21333;&#35821;&#35774;&#32622;&#30340;&#24615;&#33021;&#26368;&#20339;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#36716;&#31227;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#29983;&#25104;&#23545;&#25239;&#35328;&#35770;&#65292;&#23588;&#20854;&#26159;&#24403;&#35821;&#35328;&#23646;&#20110;&#21516;&#19968;&#35821;&#35328;&#23478;&#26063;&#26102;&#65292;&#21487;&#36801;&#31227;&#24615;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#34384;&#24453;&#30340;&#20852;&#36215;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#29983;&#25104;&#23545;&#25239;&#35328;&#35770;&#65292;&#20197;&#8220;&#21453;&#21046;&#8221;&#36825;&#31181;&#28389;&#29992;&#35328;&#35770;&#30340;&#24694;&#21155;&#35821;&#27668;&#65292;&#20197;&#27492;&#26469;&#20943;&#36731;/&#25913;&#36827;&#20854;&#23545;&#31038;&#20132;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#30528;&#37325;&#20110;&#33521;&#35821;&#12290;&#20026;&#20102;&#22635;&#34917;&#23391;&#21152;&#25289;&#35821;&#21644;&#21360;&#22320;&#35821;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#30001;5062&#20010;&#34384;&#24453;&#35328;&#35770;/&#23545;&#25239;&#35328;&#35770;&#23545;&#32452;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;2460&#23545;&#35821;&#26009;&#26159;&#23391;&#21152;&#25289;&#35821;&#65292;2602&#23545;&#35821;&#26009;&#26159;&#21360;&#22320;&#35821;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#37197;&#32622;&#30340;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#26469;&#32771;&#34385;&#21508;&#31181;&#36328;&#35821;&#35328;&#36716;&#31227;&#26426;&#21046;&#65292;&#20197;&#29983;&#25104;&#36866;&#24403;&#30340;&#23545;&#25239;&#35328;&#35770;&#65292;&#20174;&#32780;&#24314;&#31435;&#19968;&#20010;&#26377;&#25928;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21333;&#35821;&#35774;&#32622;&#30340;&#24615;&#33021;&#26368;&#20339;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21512;&#25104;&#36716;&#31227;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#29983;&#25104;&#23545;&#25239;&#35328;&#35770;&#65307;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#24403;&#35821;&#35328;&#23646;&#20110;&#21516;&#19968;&#35821;&#35328;&#23478;&#26063;&#26102;&#65292;&#21487;&#36801;&#31227;&#24615;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of online abuse, the NLP community has begun investigating the use of neural architectures to generate counterspeech that can "counter" the vicious tone of such abusive speech and dilute/ameliorate their rippling effect over the social network. However, most of the efforts so far have been primarily focused on English. To bridge the gap for low-resource languages such as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive speech/counterspeech pairs, of which 2,460 pairs are in Bengali and 2,602 pairs are in Hindi. We implement several baseline models considering various interlingual transfer mechanisms with different configurations to generate suitable counterspeech to set up an effective benchmark. We observe that the monolingual setup yields the best performance. Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language fami
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#32654;&#22269;&#25163;&#35821;&#35270;&#39057;&#21040;&#25991;&#23383;&#30340;&#32763;&#35793;&#25216;&#26415;&#65292;&#36890;&#36807;&#22797;&#21046;&#21644;&#25913;&#36827;&#20197;&#24448;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#21463;&#20248;&#21270;&#22120;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#26631;&#31614;&#24179;&#28369;&#30340;&#24433;&#21709;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#26088;&#22312;&#25913;&#21892;&#35270;&#35273;&#29305;&#24449;&#25429;&#25417;&#12289;&#22686;&#24378;&#35299;&#30721;&#22120;&#21033;&#29992;&#29575;&#65292;&#24182;&#25972;&#21512;&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07255</link><description>&lt;p&gt;
&#32654;&#22269;&#25163;&#35821;&#35270;&#39057;&#21040;&#25991;&#23383;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
American Sign Language Video to Text Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#32654;&#22269;&#25163;&#35821;&#35270;&#39057;&#21040;&#25991;&#23383;&#30340;&#32763;&#35793;&#25216;&#26415;&#65292;&#36890;&#36807;&#22797;&#21046;&#21644;&#25913;&#36827;&#20197;&#24448;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#21463;&#20248;&#21270;&#22120;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#26631;&#31614;&#24179;&#28369;&#30340;&#24433;&#21709;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#26088;&#22312;&#25913;&#21892;&#35270;&#35273;&#29305;&#24449;&#25429;&#25417;&#12289;&#22686;&#24378;&#35299;&#30721;&#22120;&#21033;&#29992;&#29575;&#65292;&#24182;&#25972;&#21512;&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#21040;&#25991;&#26412;&#30340;&#36716;&#25442;&#26159;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#65292;&#21487;&#20197;&#28040;&#38500;&#21548;&#21147;&#22256;&#38590;&#20154;&#32676;&#20043;&#38388;&#30340;&#27807;&#36890;&#38556;&#30861;&#12290;&#25105;&#20204;&#22797;&#21046;&#24182;&#35797;&#22270;&#25913;&#36827;&#26368;&#36817;&#19968;&#39033;&#24050;&#21457;&#34920;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;BLEU&#21644;rBLEU&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#30340;&#24615;&#33021;&#21463;&#21040;&#20248;&#21270;&#22120;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#26631;&#31614;&#24179;&#28369;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26088;&#22312;&#25913;&#36827;&#35270;&#35273;&#29305;&#24449;&#25429;&#25417;&#12289;&#22686;&#24378;&#35299;&#30721;&#22120;&#21033;&#29992;&#29575;&#65292;&#24182;&#25972;&#21512;&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#29992;&#20110;&#22797;&#21046;&#25105;&#20204;&#30340;&#32467;&#26524;&#24182;&#40723;&#21169;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language to text is a crucial technology that can break down communication barriers for individuals with hearing difficulties. We replicate and try to improve on a recently published study. We evaluate models using BLEU and rBLEU metrics to ensure translation quality. During our ablation study, we found that the model's performance is significantly influenced by optimizers, activation functions, and label smoothing. Further research aims to refine visual feature capturing, enhance decoder utilization, and integrate pre-trained decoders for better translation outcomes. Our source code is available to facilitate replication of our results and encourage future research.
&lt;/p&gt;</description></item><item><title>TransGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#20132;&#36890;&#39046;&#22495;&#30340;&#26032;&#22411;&#22810;&#27169;&#24335;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#20351;&#29992;&#21333;&#27169;&#24335;&#21644;&#22810;&#27169;&#24335;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07233</link><description>&lt;p&gt;
TransGPT&#65306;&#29992;&#20110;&#20132;&#36890;&#30340;&#22810;&#27169;&#24335;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07233
&lt;/p&gt;
&lt;p&gt;
TransGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#20132;&#36890;&#39046;&#22495;&#30340;&#26032;&#22411;&#22810;&#27169;&#24335;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#20351;&#29992;&#21333;&#27169;&#24335;&#21644;&#22810;&#27169;&#24335;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#22312;&#20132;&#36890;&#39046;&#22495;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#19982;&#25968;&#25454;&#65292;&#22810;&#27169;&#24335;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TransGPT&#65292;&#19968;&#31181;&#38754;&#21521;&#20132;&#36890;&#39046;&#22495;&#30340;&#26032;&#22411;&#65288;&#22810;&#27169;&#24335;&#65289;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20004;&#20010;&#29420;&#31435;&#30340;&#21464;&#20307;&#32452;&#25104;&#65306;TransGPT-SM&#29992;&#20110;&#21333;&#27169;&#24335;&#25968;&#25454;&#21644;TransGPT-MM&#29992;&#20110;&#22810;&#27169;&#24335;&#25968;&#25454;&#12290;TransGPT-SM&#22312;&#21253;&#21547;&#26469;&#33258;&#20132;&#36890;&#39046;&#22495;&#21508;&#31181;&#26469;&#28304;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#21333;&#27169;&#24335;&#20132;&#36890;&#25968;&#25454;&#38598;&#65288;STD&#65289;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;TransGPT-MM&#22312;&#25105;&#20204;&#25163;&#21160;&#25910;&#38598;&#30340;&#20132;&#36890;&#39046;&#22495;&#30340;&#19977;&#20010;&#39046;&#22495;&#65288;&#39550;&#39542;&#27979;&#35797;&#12289;&#20132;&#36890;&#26631;&#24535;&#21644;&#22320;&#26631;&#65289;&#30340;&#22810;&#27169;&#24335;&#20132;&#36890;&#25968;&#25454;&#38598;&#65288;MTD&#65289;&#19978;&#32454;&#35843;&#12290;&#25105;&#20204;&#23545;TransGPT&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) is a key component of intelligent transportation systems (ITS), but it faces many challenges in the transportation domain, such as domain-specific knowledge and data, and multi-modal inputs and outputs. This paper presents TransGPT, a novel (multi-modal) large language model for the transportation domain, which consists of two independent variants: TransGPT-SM for single-modal data and TransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal Transportation dataset (STD) that contains textual data from various sources in the transportation domain. TransGPT-MM is finetuned on a multi-modal Transportation dataset (MTD) that we manually collected from three areas of the transportation domain: driving tests, traffic signs, and landmarks. We evaluate TransGPT on several benchmark datasets for different tasks in the transportation domain, and show that it outperforms baseline models on most tasks. We also showcase the potential application
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#20998;&#21106;&#25237;&#31080;&#65292;&#25506;&#32034;&#24459;&#24072;&#22312;&#22788;&#29702;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#26102;&#38754;&#20020;&#30340;&#24847;&#35265;&#20998;&#27495;&#21644;&#22256;&#38590;&#65292;&#24182;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#25910;&#38598;&#20102;&#27861;&#23448;&#30340;&#25237;&#31080;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20154;&#31867;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07214</link><description>&lt;p&gt;
&#36879;&#36807;&#20998;&#21106;&#25237;&#31080;&#30340;&#35270;&#35282;: &#25506;&#32034;&#22312;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#20013;&#30340;&#24847;&#35265;&#20998;&#27495;&#12289;&#22256;&#38590;&#21644;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07214
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#20998;&#21106;&#25237;&#31080;&#65292;&#25506;&#32034;&#24459;&#24072;&#22312;&#22788;&#29702;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#26102;&#38754;&#20020;&#30340;&#24847;&#35265;&#20998;&#27495;&#21644;&#22256;&#38590;&#65292;&#24182;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#25910;&#38598;&#20102;&#27861;&#23448;&#30340;&#25237;&#31080;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20154;&#31867;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#20915;&#31574;&#20013;&#65292;&#24403;&#27861;&#23448;&#26080;&#27861;&#36798;&#25104;&#19968;&#33268;&#20915;&#23450;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#20998;&#21106;&#25237;&#31080;(SV)&#65292;&#32473;&#24517;&#39035;&#22788;&#29702;&#21508;&#31181;&#27861;&#24459;&#35770;&#28857;&#21644;&#24847;&#35265;&#30340;&#24459;&#24072;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#29702;&#35299;&#20154;&#31867;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26657;&#20934;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20998;&#31867;&#22120;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#35748;&#30693;&#65292;&#36890;&#24120;&#26159;&#19982;&#20154;&#31867;&#30340;&#22810;&#25968;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#32780;&#24573;&#35270;&#20102;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#30340;&#22266;&#26377;&#24046;&#24322;&#65288;HLV&#65289;&#12290;&#26412;&#25991;&#23558;&#20998;&#21106;&#25237;&#31080;&#35270;&#20026;&#33258;&#28982;&#21487;&#35266;&#23519;&#30340;&#20154;&#31867;&#24847;&#35265;&#20998;&#27495;&#21644;&#20215;&#20540;&#22810;&#20803;&#20027;&#20041;&#65292;&#24182;&#20174;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#65288;ECHR&#65289;&#25910;&#38598;&#27861;&#23448;&#30340;&#25237;&#31080;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#24102;&#26377;SV&#20449;&#24687;&#30340;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#65288;COC&#65289;&#25968;&#25454;&#38598;SV-ECHR&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#21253;&#21547;SV&#29305;&#23450;&#23376;&#31867;&#21035;&#30340;&#19981;&#21516;&#24847;&#35265;&#30340;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;COC&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20154;&#31867;&#26657;&#20934;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#38480;&#21046;&#24615;&#30340;...
&lt;/p&gt;
&lt;p&gt;
In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing a difficulty for lawyers who must navigate diverse legal arguments and opinions. In high-stakes domains, understanding the alignment of perceived difficulty between humans and AI systems is crucial to build trust. However, existing NLP calibration methods focus on a classifier's awareness of predictive performance, measured against the human majority class, overlooking inherent human label variation (HLV). This paper explores split votes as naturally observable human disagreement and value pluralism. We collect judges' vote distributions from the European Court of Human Rights (ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with SV information. We build a taxonomy of disagreement with SV-specific subcategories. We further assess the alignment of perceived difficulty between models and humans, as well as confidence- and human-calibration of COC models. We observe lim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#31034;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25216;&#26415;GGPP&#12290;&#36890;&#36807;GGPP&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#24212;&#23545;&#25552;&#31034;&#20013;&#30340;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.07179</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#25552;&#31034;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#31034;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25216;&#26415;GGPP&#12290;&#36890;&#36807;GGPP&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#24212;&#23545;&#25552;&#31034;&#20013;&#30340;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#40065;&#26834;&#24615;&#22312;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20351;&#29992;&#36805;&#36895;&#22686;&#38271;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#34987;&#35270;&#20026;&#25552;&#39640;&#20174;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;RAG-based LLMs&#30340;&#36755;&#20986;&#22914;&#20309;&#21463;&#21040;&#31245;&#26377;&#19981;&#21516;&#30340;&#36755;&#20837;&#24433;&#21709;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#25552;&#31034;&#20013;&#25554;&#20837;&#19968;&#20010;&#24456;&#30701;&#30340;&#21069;&#32512;&#20063;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#20107;&#23454;&#27491;&#30830;&#31572;&#26696;&#30456;&#21435;&#29978;&#36828;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#36825;&#31867;&#21069;&#32512;&#23545;RAG&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Gradient Guided Prompt Perturbation&#65288;GGPP&#65289;&#30340;&#26032;&#22411;&#20248;&#21270;&#25216;&#26415;&#12290;GGPP&#22312;&#23558;RAG-based LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#38169;&#35823;&#31572;&#26696;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#23427;&#36824;&#21487;&#20197;&#24212;&#23545;&#25552;&#31034;&#20013;&#35831;&#27714;&#24573;&#30053;&#26080;&#20851;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;LLMs&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;GGPP&#25200;&#21160;&#30340;&#25552;&#31034;&#20043;&#38388;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#24046;&#24322;&#26469;&#25552;&#20379;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07157</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Natural Language Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23398;&#20064;&#20915;&#31574;&#20219;&#21153;&#30340;&#31574;&#30053;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;RL&#24120;&#24120;&#38754;&#20020;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#31232;&#30095;&#30417;&#30563;&#20449;&#21495;&#31561;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;RL&#21407;&#21017;&#19982;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NLRL&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#37325;&#26032;&#23450;&#20041;&#20102;&#20219;&#21153;&#30446;&#26631;&#12289;&#31574;&#30053;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;Bellman&#26041;&#31243;&#21644;&#31574;&#30053;&#36845;&#20195;&#31561;RL&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#26469;&#23454;&#29616;NLRL&#12290;&#23545;&#34920;&#26684;MDPs&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#20102;NLRL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.
&lt;/p&gt;</description></item><item><title>X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07148</link><description>&lt;p&gt;
X-LoRA: &#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07148
&lt;/p&gt;
&lt;p&gt;
X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#36880;&#23618;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#26032;&#39062;&#39044;&#35757;&#32451;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#19987;&#23478;&#31574;&#30053;&#65292;&#29992;&#20110;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#34255;&#29366;&#24577;&#21160;&#24577;&#28151;&#21512;&#32463;&#36807;&#36866;&#24212;&#30340;&#23618;&#30340;&#38376;&#25511;&#31574;&#30053;&#65292;&#20801;&#35768;&#24471;&#21040;&#30340;X-LoRA&#27169;&#22411;&#21033;&#29992;&#19981;&#21516;&#30340;&#33021;&#21147;&#24182;&#21019;&#24314;&#20197;&#21069;&#26410;&#20351;&#29992;&#30340;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#35813;&#35774;&#35745;&#21463;&#21040;&#20102;&#29983;&#29289;&#26222;&#36941;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22359;&#22312;&#19981;&#21516;&#30340;&#20998;&#23618;&#34920;&#31034;&#20013;&#34987;&#37325;&#22797;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;X-LoRA&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;X-LoRA&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#21069;&#21521;/&#36870;&#21521;&#20998;&#26512;&#20219;&#21153;&#21644;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#22312;&#20869;&#30340;&#31185;&#23398;&#33021;&#21147;&#65292;&#37325;&#28857;&#26159;&#29983;&#29289;&#26448;&#26009;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#25429;&#25417;&#22810;&#26679;&#30340;&#23545;&#35805;&#29615;&#22659;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#20943;&#23569;&#38169;&#35823;&#29983;&#25104;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.07092</link><description>&lt;p&gt;
&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#25429;&#25417;&#22810;&#26679;&#30340;&#23545;&#35805;&#29615;&#22659;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#20943;&#23569;&#38169;&#35823;&#29983;&#25104;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#21033;&#29992;&#22810;&#36718;&#33258;&#28982;&#35821;&#35328;&#29615;&#22659;&#26469;&#26816;&#32034;&#30456;&#20851;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#23558;&#23545;&#35805;&#35270;&#20026;&#19968;&#31995;&#21015;&#22266;&#23450;&#30340;&#38382;&#39064;&#21644;&#22238;&#31572;&#65292;&#24573;&#35270;&#20102;&#20005;&#37325;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064; - &#20063;&#23601;&#26159;&#35828;&#65292;&#29992;&#25143;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23545;&#35805;&#65292;&#32780;&#36825;&#20123;&#22791;&#36873;&#23545;&#35805;&#26159;&#26410;&#35760;&#24405;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#23545;&#35805;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#30340;&#26694;&#26550;(ConvAug)&#12290;ConvAug&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#20197;&#25429;&#25417;&#23545;&#35805;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#26041;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#38169;&#35823;&#30340;&#27491;&#20363;&#12289;&#36127;&#20363;&#21644;&#24187;&#35273;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#22797;&#26434;&#23545;&#35805;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#38901;&#24459;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#38899;&#32032;&#21644;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#20013;&#25552;&#21462;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#27169;&#25311;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#20010;&#20307;&#21457;&#38899;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.07085</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#38901;&#24459;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#20013;&#20174;&#38899;&#32032;&#21644;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#20013;&#25552;&#21462;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Speech Rhythm-Based Speaker Embeddings Extraction from Phonemes and Phoneme Duration for Multi-Speaker Speech Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#38901;&#24459;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#38899;&#32032;&#21644;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#20013;&#25552;&#21462;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#27169;&#25311;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#20010;&#20307;&#21457;&#38899;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#38901;&#24459;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#23569;&#37327;&#21477;&#23376;&#20013;&#24314;&#27169;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#25552;&#21462;&#35828;&#35805;&#20154;&#23884;&#20837;&#12290;&#35821;&#38899;&#38901;&#24459;&#26159;&#19982;&#35828;&#35805;&#20154;&#29305;&#24449;&#30456;&#20851;&#30340;&#37325;&#35201;&#22240;&#32032;&#20043;&#19968;&#65292;&#19982;&#22522;&#39057;&#31561;&#22768;&#23398;&#29305;&#24449;&#19968;&#36215;&#29992;&#20110;&#22312;&#35821;&#38899;&#21512;&#25104;&#20013;&#37325;&#29616;&#21333;&#20010;&#21477;&#23376;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#29305;&#28857;&#26159;&#22522;&#20110;&#38901;&#24459;&#30340;&#23884;&#20837;&#65292;&#20174;&#24050;&#30693;&#19982;&#35828;&#35805;&#38901;&#24459;&#30456;&#20851;&#30340;&#38899;&#32032;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#20013;&#25552;&#21462;&#65292;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#39057;&#35889;&#29305;&#24449;&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#65292;&#21253;&#25324;&#29983;&#25104;&#35828;&#35805;&#20154;&#23884;&#20837;&#12289;&#20351;&#29992;&#29983;&#25104;&#30340;&#23884;&#20837;&#36827;&#34892;&#35821;&#38899;&#21512;&#25104;&#20197;&#21450;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#38899;&#32032;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#20449;&#24687;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20063;&#23637;&#29616;&#20102;&#36739;&#20026;&#36866;&#20013;&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#24615;&#33021;&#65288;15.2% EER&#65289;&#12290;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a speech rhythm-based method for speaker embeddings to model phoneme duration using a few utterances by the target speaker. Speech rhythm is one of the essential factors among speaker characteristics, along with acoustic features such as F0, for reproducing individual utterances in speech synthesis. A novel feature of the proposed method is the rhythm-based embeddings extracted from phonemes and their durations, which are known to be related to speaking rhythm. They are extracted with a speaker identification model similar to the conventional spectral feature-based one. We conducted three experiments, speaker embeddings generation, speech synthesis with generated embeddings, and embedding space analysis, to evaluate the performance. The proposed method demonstrated a moderate speaker identification performance (15.2% EER), even with only phonemes and their duration information. The objective and subjective evaluation results demonstrated that the proposed method can
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#26159;&#34913;&#37327;&#23398;&#29983;&#30693;&#35782;&#27700;&#24179;&#30340;&#33391;&#22909;&#25351;&#26631;&#65292;&#20174;&#32780;&#35299;&#20915;&#25163;&#21160;&#26500;&#24314;&#27979;&#35797;&#29992;&#20363;&#30340;&#21171;&#21160;&#23494;&#38598;&#24615;&#21644;&#19987;&#19994;&#30693;&#35782;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07081</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23398;&#29983;&#20195;&#30721;&#24341;&#23548;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#26159;&#34913;&#37327;&#23398;&#29983;&#30693;&#35782;&#27700;&#24179;&#30340;&#33391;&#22909;&#25351;&#26631;&#65292;&#20174;&#32780;&#35299;&#20915;&#25163;&#21160;&#26500;&#24314;&#27979;&#35797;&#29992;&#20363;&#30340;&#21171;&#21160;&#23494;&#38598;&#24615;&#21644;&#19987;&#19994;&#30693;&#35782;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#65292;&#27979;&#35797;&#29992;&#20363;&#26159;&#32534;&#31243;&#20316;&#19994;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29992;&#20316;&#35780;&#20272;&#39033;&#30446;&#65292;&#27979;&#35797;&#23398;&#29983;&#30340;&#32534;&#31243;&#30693;&#35782;&#65292;&#24182;&#20026;&#23398;&#29983;&#32534;&#20889;&#30340;&#20195;&#30721;&#25552;&#20379;&#20010;&#24615;&#21270;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#34913;&#37327;&#23398;&#29983;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#36825;&#19968;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#25163;&#21160;&#26500;&#24314;&#27979;&#35797;&#29992;&#20363;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#26159;&#19968;&#39033;&#21171;&#21160;&#23494;&#38598;&#22411;&#24037;&#20316;&#12290;&#20854;&#27425;&#65292;&#20026;&#23398;&#29983;&#24320;&#21457;&#27979;&#35797;&#29992;&#20363;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#21021;&#23398;&#32773;&#26469;&#35828;&#65292;&#19982;&#38024;&#23545;&#19987;&#19994;&#32423;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#27979;&#35797;&#29992;&#20363;&#26377;&#30528;&#26174;&#33879;&#30340;&#21306;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#36807;&#31243;&#26469;&#35780;&#20272;&#23398;&#29983;&#30340;&#30693;&#35782;&#27700;&#24179;&#24182;&#25552;&#20379;&#21453;&#39304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#26159;&#34913;&#37327;&#23398;&#29983;&#30693;&#35782;&#27700;&#24179;&#30340;&#33391;&#22909;&#25351;&#26631;&#65292;&#20351;&#29992;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computer science education, test cases are an integral part of programming assignments since they can be used as assessment items to test students' programming knowledge and provide personalized feedback on student-written code. The goal of our work is to propose a fully automated approach for test case generation that can accurately measure student knowledge, which is important for two reasons. First, manually constructing test cases requires expert knowledge and is a labor-intensive process. Second, developing test cases for students, especially those who are novice programmers, is significantly different from those oriented toward professional-level software developers. Therefore, we need an automated process for test case generation to assess student knowledge and provide feedback. In this work, we propose a large language model-based approach to automatically generate test cases and show that they are good measures of student knowledge, using a publicly available dataset that c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33258;&#21160;&#26426;&#26469;&#32534;&#30721;&#39640;&#32423;&#30693;&#35782;&#65292;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07069</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#22870;&#21169;&#26426;&#22120;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33258;&#21160;&#26426;&#26469;&#32534;&#30721;&#39640;&#32423;&#30693;&#35782;&#65292;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LARL-RM&#65288;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#29992;&#20110;&#22870;&#21169;&#26426;&#22120;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#26426;&#65289;&#31639;&#27861;&#65292;&#20197;&#23558;&#39640;&#32423;&#30693;&#35782;&#32534;&#30721;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#33258;&#21160;&#26426;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#33719;&#24471;&#39640;&#32423;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23558;&#39640;&#32423;&#30693;&#35782;&#25552;&#20379;&#32473;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#38656;&#35201;&#19987;&#23478;&#26469;&#32534;&#30721;&#33258;&#21160;&#26426;&#12290;&#25105;&#20204;&#20351;&#29992;&#24605;&#32500;&#38142;&#21644;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#27861;&#19979;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;LARL-RM&#20801;&#35768;&#23436;&#20840;&#38381;&#29615;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#26080;&#38656;&#19987;&#23478;&#26469;&#25351;&#23548;&#21644;&#30417;&#30563;&#23398;&#20064;&#65292;&#22240;&#20026;LARL-RM&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;LLM&#29983;&#25104;&#25152;&#38656;&#30340;&#39640;&#32423;&#30693;&#35782;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;LARL-RM&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#23545;&#24120;&#35265;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-R
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#35821;&#35328;&#23545;&#24212;&#30340;&#36830;&#32493;&#35789;&#34920;&#31034;&#38598;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#31354;&#38388;&#65292;&#25512;&#26029;&#21452;&#35821;&#35789;&#20856;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#22312;&#23398;&#20064;&#26032;&#35821;&#35328;&#26102;&#65292;&#25972;&#21512;&#24050;&#26377;&#35821;&#35328;&#38598;&#30340;&#30693;&#35782;&#65292;&#36890;&#36807;&#25490;&#24207;&#26041;&#27861;&#23454;&#29616;&#35789;&#20856;&#35825;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.07028</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#21452;&#35821;&#35789;&#20856;&#35825;&#23548;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning for Bilingual Lexicon Induction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#35821;&#35328;&#23545;&#24212;&#30340;&#36830;&#32493;&#35789;&#34920;&#31034;&#38598;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#31354;&#38388;&#65292;&#25512;&#26029;&#21452;&#35821;&#35789;&#20856;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#22312;&#23398;&#20064;&#26032;&#35821;&#35328;&#26102;&#65292;&#25972;&#21512;&#24050;&#26377;&#35821;&#35328;&#38598;&#30340;&#30693;&#35782;&#65292;&#36890;&#36807;&#25490;&#24207;&#26041;&#27861;&#23454;&#29616;&#35789;&#20856;&#35825;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23558;&#23545;&#24212;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#20004;&#20010;&#36830;&#32493;&#35789;&#34920;&#31034;&#38598;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#21516;&#31354;&#38388;&#65292;&#20197;&#25512;&#26029;&#21452;&#35821;&#35789;&#20856;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#22312;&#21333;&#35821;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#23545;&#40784;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#36825;&#26679;&#30340;&#35789;&#20856;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#24179;&#34892;&#25968;&#25454;&#12290;&#36825;&#31181;&#24037;&#20316;&#31216;&#20026;&#26080;&#30417;&#30563;&#21452;&#35821;&#35825;&#23548;&#12290;&#36890;&#36807;&#24605;&#32771;&#26159;&#21542;&#21487;&#33021;&#22312;&#36880;&#27493;&#23398;&#20064;&#22810;&#31181;&#35821;&#35328;&#30340;&#36807;&#31243;&#20013;&#31215;&#32047;&#32463;&#39564;&#65292;&#25105;&#20204;&#33258;&#38382;&#22312;&#23398;&#20064;&#26032;&#35821;&#35328;&#26102;&#26159;&#21542;&#33021;&#22815;&#22312;&#27809;&#26377;&#24179;&#34892;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25972;&#21512;&#32473;&#23450;&#35821;&#35328;&#38598;&#30340;&#30693;&#35782;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#34429;&#28982;&#20445;&#25345;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26680;&#24515;&#38382;&#39064;&#22312;&#26368;&#26032;&#27493;&#39588;&#20013;&#65292;&#20294;&#25105;&#20204;&#20801;&#35768;&#35775;&#38382;&#20854;&#20182;&#20064;&#35821;&#35821;&#26009;&#24211;&#65292;&#22240;&#27492;&#31216;&#20026;&#21322;&#30417;&#30563;&#12290;&#36825;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#36798;&#24418;&#24335;&#65292;&#23558;&#35789;&#20856;&#35825;&#23548;&#35270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#35813;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of aligning two sets of continuous word representations, corresponding to languages, to a common space in order to infer a bilingual lexicon. It was recently shown that it is possible to infer such lexicon, without using any parallel data, by aligning word embeddings trained on monolingual data. Such line of work is called unsupervised bilingual induction. By wondering whether it was possible to gain experience in the progressive learning of several languages, we asked ourselves to what extent we could integrate the knowledge of a given set of languages when learning a new one, without having parallel data for the latter. In other words, while keeping the core problem of unsupervised learning in the latest step, we allowed the access to other corpora of idioms, hence the name semi-supervised. This led us to propose a novel formulation, considering the lexicon induction as a ranking problem for which we used recent tools of this machine learning field. Our experi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#24320;&#28304;LLM&#21644;&#35895;&#27468;&#30340;&#22810;&#27169;&#24577;LLM Gemini &#22312;&#21307;&#23398;&#25512;&#29702;&#12289;&#24187;&#35273;&#26816;&#27979;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;Gemini&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#19988;&#26131;&#20986;&#29616;&#24187;&#35273;&#12289;&#36807;&#24230;&#33258;&#20449;&#21644;&#30693;&#35782;&#30450;&#28857;&#12290;&#37319;&#29992;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07023</link><description>&lt;p&gt;
&#21452;&#23376;&#24231;&#36827;&#20837;&#21307;&#23398;&#38498;&#65306;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25361;&#25112;&#38382;&#39064;&#21644;&#24187;&#35273;&#19978;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems &amp; Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#24320;&#28304;LLM&#21644;&#35895;&#27468;&#30340;&#22810;&#27169;&#24577;LLM Gemini &#22312;&#21307;&#23398;&#25512;&#29702;&#12289;&#24187;&#35273;&#26816;&#27979;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;Gemini&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#19988;&#26131;&#20986;&#29616;&#24187;&#35273;&#12289;&#36807;&#24230;&#33258;&#20449;&#21644;&#30693;&#35782;&#30450;&#28857;&#12290;&#37319;&#29992;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#34892;&#19994;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#65292;&#20294;&#36890;&#36807;&#20005;&#26684;&#35780;&#20272;&#26469;&#39564;&#35777;&#20854;&#23433;&#20840;&#24615;&#21644;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#24320;&#28304;LLM&#21644;&#35895;&#27468;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;LLM Gemini &#22312;&#21307;&#23398;&#25512;&#29702;&#12289;&#24187;&#35273;&#26816;&#27979;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;Gemini&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#33853;&#21518;&#20110;MedPaLM 2&#21644;GPT-4&#31561;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;Gemini&#22312;&#21307;&#23398;VQA&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;61.45&#65285;&#65292;&#26126;&#26174;&#20302;&#20110;GPT-4V&#30340;88&#65285;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;Gemini&#26497;&#26131;&#20986;&#29616;&#24187;&#35273;&#12289;&#36807;&#24230;&#33258;&#20449;&#21644;&#30693;&#35782;&#30450;&#28857;&#65292;&#36825;&#34920;&#26126;&#22914;&#26524;&#19981;&#21152;&#25209;&#21028;&#22320;&#37096;&#32626;&#65292;&#23384;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#19981;&#21516;&#21307;&#23398;&#23398;&#31185;&#21644;&#27979;&#35797;&#31867;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#20026;&#24320;&#21457;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#12290;&#20026;&#20102;&#20943;&#23569;&#39118;&#38505;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks. While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved an accuracy of 61.45\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied prompting strategies that improved performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35821;&#38899;&#36716;&#25442;&#25104;&#27468;&#26354;&#30340;&#24187;&#35937;&#30340;&#21512;&#29702;&#20998;&#26512;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#31181;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#26009;&#24211;&#65292;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32431;&#25991;&#26412;&#30340;&#23567;&#35828;&#36716;&#27468;&#35789;&#30340;&#24187;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#26469;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06992</link><description>&lt;p&gt;
&#35821;&#38899;&#36716;&#25442;&#25104;&#27468;&#26354;&#30340;&#24187;&#35937;&#30340;&#21512;&#29702;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Rational Analysis of the Speech-to-Song Illusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35821;&#38899;&#36716;&#25442;&#25104;&#27468;&#26354;&#30340;&#24187;&#35937;&#30340;&#21512;&#29702;&#20998;&#26512;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#31181;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#26009;&#24211;&#65292;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32431;&#25991;&#26412;&#30340;&#23567;&#35828;&#36716;&#27468;&#35789;&#30340;&#24187;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#26469;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#36716;&#25442;&#25104;&#27468;&#26354;&#30340;&#24187;&#35937;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24515;&#29702;&#29616;&#35937;&#65292;&#21363;&#35828;&#20986;&#30340;&#21477;&#23376;&#22312;&#19981;&#26029;&#37325;&#22797;&#20013;&#36234;&#26469;&#36234;&#20687;&#38899;&#20048;&#12290;&#23613;&#31649;&#32463;&#36807;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#65292;&#23545;&#36825;&#31181;&#36716;&#21270;&#30340;&#23436;&#25972;&#24418;&#24335;&#35299;&#37322;&#20173;&#28982;&#32570;&#20047;&#65292;&#24182;&#19988;&#23545;&#20854;&#32454;&#24494;&#30340;&#29305;&#24449;&#65292;&#21363;&#26576;&#20123;&#30701;&#35821;&#30340;&#36716;&#21270;&#26159;&#21542;&#21457;&#29983;&#65292;&#20063;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20010;&#29616;&#35937;&#30340;&#19968;&#20010;&#24418;&#24335;&#21270;&#35299;&#37322;&#65292;&#23558;&#20854;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#31181;&#32479;&#35745;&#25512;&#26029;&#65292;&#21512;&#29702;&#30340;&#20915;&#31574;&#32773;&#35797;&#22270;&#21028;&#26029;&#19968;&#31995;&#21015;&#35805;&#35821;&#26356;&#26377;&#21487;&#33021;&#26159;&#22312;&#27468;&#26354;&#20013;&#36824;&#26159;&#22312;&#35762;&#35805;&#20013;&#20135;&#29983;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#24182;&#20998;&#26512;&#27468;&#26354;&#21644;&#35762;&#35805;&#30340;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#31181;&#32431;&#25991;&#26412;&#30340;&#23567;&#35828;&#36716;&#27468;&#35789;&#30340;&#24187;&#35937;&#12290;&#22312;&#36825;&#20010;&#24187;&#35937;&#20013;&#65292;&#31616;&#21333;&#22320;&#22797;&#21046;&#20070;&#38754;&#21477;&#23376;&#20250;&#20351;&#23427;&#20204;&#30475;&#36215;&#26469;&#26356;&#20687;&#27468;&#35789;&#12290;&#25105;&#20204;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#36825;&#20010;&#26032;&#24187;&#35937;&#30340;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The speech-to-song illusion is a robust psychological phenomenon whereby a spoken sentence sounds increasingly more musical as it is repeated. Despite decades of research, a complete formal account of this transformation is still lacking, and some of its nuanced characteristics, namely, that certain phrases appear to transform while others do not, is not well understood. Here we provide a formal account of this phenomenon, by recasting it as a statistical inference whereby a rational agent attempts to decide whether a sequence of utterances is more likely to have been produced in a song or speech. Using this approach and analyzing song and speech corpora, we further introduce a novel prose-to-lyrics illusion that is purely text-based. In this illusion, simply duplicating written sentences makes them appear more like song lyrics. We provide robust evidence for this new illusion in both human participants and large language models.
&lt;/p&gt;</description></item><item><title>&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;&#65288;EKS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#29305;&#23450;&#20107;&#20214;&#29983;&#25104;&#19978;&#19979;&#25991;&#21270;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MUCSUM&#65292;&#24182;&#23637;&#31034;&#20102;EKS&#19982;&#20256;&#32479;&#25688;&#35201;&#21644;&#32467;&#26500;&#21040;&#25991;&#26412;&#30340;&#27604;&#36739;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06973</link><description>&lt;p&gt;
&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Event-Keyed Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06973
&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;&#65288;EKS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#29305;&#23450;&#20107;&#20214;&#29983;&#25104;&#19978;&#19979;&#25991;&#21270;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MUCSUM&#65292;&#24182;&#23637;&#31034;&#20102;EKS&#19982;&#20256;&#32479;&#25688;&#35201;&#21644;&#32467;&#26500;&#21040;&#25991;&#26412;&#30340;&#27604;&#36739;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;&#65288;EKS&#65289;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#25688;&#35201;&#21644;&#25991;&#26723;&#32423;&#20107;&#20214;&#25552;&#21462;&#32467;&#21512;&#36215;&#26469;&#65292;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#25991;&#26723;&#21644;&#25552;&#21462;&#30340;&#20107;&#20214;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#29305;&#23450;&#20107;&#20214;&#25688;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;MUCSUM&#65292;&#21253;&#25324;&#32463;&#20856;MUC-4&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#20107;&#20214;&#30340;&#25688;&#35201;&#65292;&#20197;&#21450;&#19968;&#32452;&#22522;&#32447;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#25688;&#35201;&#25991;&#29486;&#20013;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26631;&#20934;&#20197;&#21450;&#26356;&#22823;&#30340;&#21069;&#27839;&#27169;&#22411;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;EKS&#31616;&#21270;&#20026;&#20256;&#32479;&#30340;&#25688;&#35201;&#25110;&#32467;&#26500;&#21040;&#25991;&#26412;&#30340;&#21435;&#38500;&#37117;&#20250;&#24471;&#21040;&#36739;&#24046;&#30340;&#30446;&#26631;&#20107;&#20214;&#25688;&#35201;&#65292;&#24182;&#19988;MUCSUM&#26159;&#36825;&#19968;&#20219;&#21153;&#30340;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#21442;&#32771;&#25688;&#35201;&#21644;&#27169;&#22411;&#25688;&#35201;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce event-keyed summarization (EKS), a novel task that marries traditional summarization and document-level event extraction, with the goal of generating a contextualized summary for a specific event, given a document and an extracted event structure. We introduce a dataset for this task, MUCSUM, consisting of summaries of all events in the classic MUC-4 dataset, along with a set of baselines that comprises both pretrained LM standards in the summarization literature, as well as larger frontier models. We show that ablations that reduce EKS to traditional summarization or structure-to-text yield inferior summaries of target events and that MUCSUM is a robust benchmark for this task. Lastly, we conduct a human evaluation of both reference and model summaries, and provide some detailed analysis of the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Midi-Tuning&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06967</link><description>&lt;p&gt;
&#19968;&#27425;&#25351;&#23548;&#65292;&#22810;&#36718;&#31283;&#23450;&#23545;&#35805;&#65306;&#23545;&#35805;&#30340;&#39640;&#25928;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Midi-Tuning&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#35805;&#29983;&#25104;&#24050;&#25104;&#20026;&#26500;&#24314;&#33021;&#21147;&#24378;&#22823;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35843;&#25972;&#26041;&#24335;&#29421;&#38552;&#22320;&#23558;&#23545;&#35805;&#29983;&#25104;&#35270;&#20026;&#31867;&#20284;&#20854;&#20182;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#24573;&#35270;&#20102;&#23545;&#35805;&#32773;&#20043;&#38388;&#30340;&#35282;&#33394;&#24046;&#24322;&#21644;&#23545;&#35805;&#24212;&#20855;&#22791;&#30340;&#22810;&#36718;&#20132;&#20114;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#24335;&#23548;&#33268;&#20102;&#25152;&#26500;&#24314;&#20195;&#29702;&#20154;&#30340;&#23545;&#35805;&#19968;&#33268;&#24615;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#23545;&#35805;&#30340;&#20132;&#20114;&#24615;&#21644;&#27807;&#36890;&#24615;&#65292;&#24182;&#35748;&#20026;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#30340;&#35762;&#35805;&#32773;&#35282;&#33394;&#36827;&#34892;&#24314;&#27169;&#26356;&#20026;&#21487;&#34892;&#65292;&#20351;&#24471;&#20195;&#29702;&#20154;&#33021;&#22815;&#20445;&#25345;&#19968;&#33268;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#65288;Midi-Tuning&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#20010;&#36866;&#37197;&#22120;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#36827;&#34892;&#24314;&#27169;&#65292;&#23427;&#20204;&#25353;&#36718;&#27425;&#20132;&#26367;&#20351;&#29992;&#35805;&#35821;&#65292;&#24182;&#36890;&#36807;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tuning pretrained language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner leads to unsatisfactory chat consistency of the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models, where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;NLP&#22312;&#33021;&#28304;&#26448;&#26009;&#30740;&#31350;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;NLP&#26041;&#27861;&#24212;&#29992;&#20110;&#33021;&#28304;&#25991;&#26412;&#65292;&#21487;&#20197;&#33258;&#21160;&#21457;&#29616;&#30693;&#35782;&#21644;&#25552;&#21462;&#20449;&#24687;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#19977;&#20010;&#25104;&#29087;&#30340;NLP&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#35782;&#21035;&#33021;&#28304;&#35805;&#39064;&#21644;&#27010;&#24565;&#65292;&#24182;&#29983;&#25104;&#19982;&#19987;&#23478;&#30693;&#35782;&#19968;&#33268;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#28304;&#25991;&#26412;&#30340;&#20998;&#31867;&#27969;&#31243;&#65292;&#20854;&#20934;&#30830;&#29575;&#39640;&#36798;59-76\%&#65292;&#26368;&#20339;&#27169;&#22411;&#19982;&#19987;&#23478;&#38388;&#19968;&#33268;&#24230;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.06964</link><description>&lt;p&gt;
NLP&#29992;&#20110;&#20174;&#33021;&#28304;&#39046;&#22495;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#30693;&#35782;&#21457;&#29616;&#21644;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
NLP for Knowledge Discovery and Information Extraction from Energetics Corpora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06964
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;NLP&#22312;&#33021;&#28304;&#26448;&#26009;&#30740;&#31350;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;NLP&#26041;&#27861;&#24212;&#29992;&#20110;&#33021;&#28304;&#25991;&#26412;&#65292;&#21487;&#20197;&#33258;&#21160;&#21457;&#29616;&#30693;&#35782;&#21644;&#25552;&#21462;&#20449;&#24687;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#19977;&#20010;&#25104;&#29087;&#30340;NLP&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#35782;&#21035;&#33021;&#28304;&#35805;&#39064;&#21644;&#27010;&#24565;&#65292;&#24182;&#29983;&#25104;&#19982;&#19987;&#23478;&#30693;&#35782;&#19968;&#33268;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#28304;&#25991;&#26412;&#30340;&#20998;&#31867;&#27969;&#31243;&#65292;&#20854;&#20934;&#30830;&#29575;&#39640;&#36798;59-76\%&#65292;&#26368;&#20339;&#27169;&#22411;&#19982;&#19987;&#23478;&#38388;&#19968;&#33268;&#24230;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;NLP&#22312;&#36741;&#21161;&#30740;&#31350;&#33021;&#28304;&#26448;&#26009;&#21644;&#30456;&#20851;&#31995;&#32479;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#36825;&#31181;NLP&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#36890;&#36807;&#33021;&#28304;&#25991;&#26412;&#36827;&#34892;&#30693;&#35782;&#21457;&#29616;&#21644;&#20449;&#24687;&#25552;&#21462;&#30340;&#33258;&#21160;&#21270;&#36335;&#24452;&#12290;&#25105;&#20204;&#23558;&#19977;&#20010;&#25104;&#29087;&#30340;&#26080;&#30417;&#30563;NLP&#27169;&#22411;&#65288;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65292;Word2Vec&#21644;Transformer&#65289;&#24212;&#29992;&#20110;&#19968;&#20010;&#22823;&#22411;&#30340;&#32463;&#36807;&#31574;&#21010;&#30340;&#33021;&#28304;&#30456;&#20851;&#31185;&#23398;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#27599;&#20010;NLP&#31639;&#27861;&#37117;&#33021;&#22815;&#35782;&#21035;&#20986;&#33021;&#28304;&#35805;&#39064;&#21644;&#27010;&#24565;&#65292;&#24182;&#29983;&#25104;&#19982;&#19987;&#23478;&#30693;&#35782;&#30456;&#19968;&#33268;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33021;&#28304;&#25991;&#26412;&#30340;&#25991;&#26723;&#20998;&#31867;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27969;&#31243;&#22312;&#20351;&#29992;NLP&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;59-76\%&#30340;&#20934;&#30830;&#29575;&#65292;&#26368;&#20339;&#24615;&#33021;&#30340;Transformer&#27169;&#22411;&#19982;&#27880;&#37322;&#32773;&#38388;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#30456;&#23218;&#32654;&#12290;&#26412;&#30740;&#31350;&#20013;&#30740;&#31350;&#30340;NLP&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#19982;&#33021;&#28304;&#39046;&#22495;&#30456;&#20851;&#30340;&#27010;&#24565;&#65292;&#22240;&#27492;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a demonstration of the utility of NLP for aiding research into energetic materials and associated systems. The NLP method enables machine understanding of textual data, offering an automated route to knowledge discovery and information extraction from energetics text. We apply three established unsupervised NLP models: Latent Dirichlet Allocation, Word2Vec, and the Transformer to a large curated dataset of energetics-related scientific articles. We demonstrate that each NLP algorithm is capable of identifying energetic topics and concepts, generating a language model which aligns with Subject Matter Expert knowledge. Furthermore, we present a document classification pipeline for energetics text. Our classification pipeline achieves 59-76\% accuracy depending on the NLP model used, with the highest performing Transformer model rivaling inter-annotator agreement metrics. The NLP approaches studied in this work can identify concepts germane to energetics and therefore hold prom
&lt;/p&gt;</description></item><item><title>SpeechCLIP+&#36890;&#36807;&#24212;&#29992;CIF&#27169;&#22359;&#26367;&#25442;CLIP&#26550;&#26500;&#20013;&#30340;CLS&#20196;&#29260;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#35821;&#38899;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#22270;&#20687;-&#35821;&#38899;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.06959</link><description>&lt;p&gt;
SpeechCLIP+: &#22522;&#20110;CLIP&#21644;&#35821;&#38899;-&#22270;&#20687;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#30340;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SpeechCLIP+: Self-supervised multi-task representation learning for speech via CLIP and speech-image data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06959
&lt;/p&gt;
&lt;p&gt;
SpeechCLIP+&#36890;&#36807;&#24212;&#29992;CIF&#27169;&#22359;&#26367;&#25442;CLIP&#26550;&#26500;&#20013;&#30340;CLS&#20196;&#29260;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#35821;&#38899;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#22270;&#20687;-&#35821;&#38899;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#35270;&#35273;&#21270;&#35821;&#38899;&#27169;&#22411;SpeechCLIP&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;CLIP&#23558;&#35821;&#38899;&#21644;&#25991;&#26412;&#19982;&#22270;&#20687;&#30456;&#36830;&#25509;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#25991;&#26412;&#36716;&#24405;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#23545;SpeechCLIP&#30340;&#25193;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#36830;&#32493;&#31215;&#20998;-&#25918;&#30005;&#65288;CIF&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#26367;&#25442;&#32423;&#32852;&#26550;&#26500;&#20013;&#30340;&#22266;&#23450;&#25968;&#37327;&#30340;CLS&#20196;&#29260;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#23558;SpeechCLIP&#30340;&#32423;&#32852;&#26550;&#26500;&#21644;&#24182;&#34892;&#26550;&#26500;&#21512;&#24182;&#20026;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;Flickr8k&#21644;SpokenCOCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35821;&#38899;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;CIF&#30340;&#32423;&#32852;SpeechCLIP&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#20351;&#29992;&#22266;&#23450;&#25968;&#37327;CLS&#20196;&#29260;&#30340;&#32423;&#32852;SpeechCLIP&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#32423;&#32852;&#20219;&#21153;&#23398;&#20064;&#25552;&#21319;&#20102;&#22270;&#20687;-&#35821;&#38899;&#26816;&#32034;&#20219;&#21153;&#20013;&#24182;&#34892;&#20998;&#25903;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed visually grounded speech model SpeechCLIP is an innovative framework that bridges speech and text through images via CLIP without relying on text transcription. On this basis, this paper introduces two extensions to SpeechCLIP. First, we apply the Continuous Integrate-and-Fire (CIF) module to replace a fixed number of CLS tokens in the cascaded architecture. Second, we propose a new hybrid architecture that merges the cascaded and parallel architectures of SpeechCLIP into a multi-task learning framework. Our experimental evaluation is performed on the Flickr8k and SpokenCOCO datasets. The results show that in the speech keyword extraction task, the CIF-based cascaded SpeechCLIP model outperforms the previous cascaded SpeechCLIP model using a fixed number of CLS tokens. Furthermore, through our hybrid architecture, cascaded task learning boosts the performance of the parallel branch in image-speech retrieval tasks.
&lt;/p&gt;</description></item><item><title>OpenFedLLM&#26159;&#19968;&#20010;&#31616;&#27905;&#12289;&#38598;&#25104;&#12289;&#30740;&#31350;&#21451;&#22909;&#30340;&#26694;&#26550;/&#20195;&#30721;&#24211;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#20998;&#25955;&#30340;&#31169;&#26377;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#21644;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#20844;&#24320;&#25968;&#25454;&#26543;&#31469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06954</link><description>&lt;p&gt;
OpenFedLLM&#65306;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#20998;&#25955;&#30340;&#31169;&#26377;&#25968;&#25454;&#19978;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06954
&lt;/p&gt;
&lt;p&gt;
OpenFedLLM&#26159;&#19968;&#20010;&#31616;&#27905;&#12289;&#38598;&#25104;&#12289;&#30740;&#31350;&#21451;&#22909;&#30340;&#26694;&#26550;/&#20195;&#30721;&#24211;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#20998;&#25955;&#30340;&#31169;&#26377;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#21644;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#20844;&#24320;&#25968;&#25454;&#26543;&#31469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#26356;&#22810;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#39640;&#36136;&#37327;&#30340;&#20844;&#24320;&#25968;&#25454;&#23558;&#22312;&#20960;&#24180;&#20869;&#29992;&#23613;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#24403;&#20195;LLM&#30340;&#28508;&#22312;&#19979;&#19968;&#27493;&#65306;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#20998;&#24067;&#24335;&#31169;&#26377;&#25968;&#25454;&#19978;&#36827;&#34892;&#21327;&#20316;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;LLM&#35757;&#32451;&#65292;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#65292;&#32780;&#19981;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#27905;&#12289;&#38598;&#25104;&#21644;&#30740;&#31350;&#21451;&#22909;&#30340;&#26694;&#26550;/&#20195;&#30721;&#24211;&#65292;&#21517;&#20026;OpenFedLLM&#12290;&#23427;&#28085;&#30422;&#20102;&#29992;&#20110;&#22686;&#24378;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#32852;&#37030;&#25351;&#20196;&#35843;&#20248;&#12289;&#29992;&#20110;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#32852;&#37030;&#20215;&#20540;&#23545;&#40784;&#20197;&#21450;7&#20010;&#20195;&#34920;&#24615;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;OpenFedLLM&#25903;&#25345;&#22312;&#22810;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;8&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65307;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction-following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cov
&lt;/p&gt;</description></item><item><title>&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;NLP&#20219;&#21153;&#26102;&#65292;&#35843;&#25972;&#20248;&#21270;&#22120;&#30340;&#36229;&#21442;&#25968;&#24182;&#19981;&#20250;&#23545;&#27979;&#35797;&#24615;&#33021;&#20135;&#29983;&#23454;&#36136;&#24615;&#24046;&#24322;&#65292;&#21482;&#35843;&#25972;&#23398;&#20064;&#29575;&#36890;&#24120;&#23601;&#36275;&#22815;&#12290;</title><link>https://arxiv.org/abs/2402.06948</link><description>&lt;p&gt;
&#25105;&#22312;&#20026;NLP&#20219;&#21153;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#26102;&#26159;&#21542;&#24212;&#35813;&#23581;&#35797;&#22810;&#20010;&#20248;&#21270;&#22120;&#65311;&#26159;&#21542;&#24212;&#35813;&#35843;&#25972;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Should I try multiple optimizers when fine-tuning pre-trained Transformers for NLP tasks? Should I tune their hyperparameters?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06948
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;NLP&#20219;&#21153;&#26102;&#65292;&#35843;&#25972;&#20248;&#21270;&#22120;&#30340;&#36229;&#21442;&#25968;&#24182;&#19981;&#20250;&#23545;&#27979;&#35797;&#24615;&#33021;&#20135;&#29983;&#23454;&#36136;&#24615;&#24046;&#24322;&#65292;&#21482;&#35843;&#25972;&#23398;&#20064;&#29575;&#36890;&#24120;&#23601;&#36275;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#31070;&#32463;&#27169;&#22411;&#26550;&#26500;&#21644;&#22823;&#23567;&#12289;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#30446;&#26631;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36873;&#25321;&#20248;&#21270;&#22120;&#24182;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#26576;&#20010;&#21464;&#31181;&#65292;&#26681;&#25454;&#19981;&#26126;&#30830;&#30340;&#26631;&#20934;&#36873;&#25321;&#65292;&#24182;&#19988;&#24448;&#24448;&#23545;&#20248;&#21270;&#22120;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#26368;&#23567;&#25110;&#27809;&#26377;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;GLUE&#25968;&#25454;&#38598;&#12289;&#20004;&#20010;&#27169;&#22411;&#65288;DistilBERT&#21644;DistilRoBERTa&#65289;&#21644;&#19971;&#20010;&#24120;&#29992;&#30340;&#20248;&#21270;&#22120;&#65288;SGD&#12289;&#24102;&#21160;&#37327;&#30340;SGD&#12289;Adam&#12289;AdaMax&#12289;Nadam&#12289;AdamW&#21644;AdaBound&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#65292;&#24403;&#35843;&#25972;&#20248;&#21270;&#22120;&#30340;&#36229;&#21442;&#25968;&#26102;&#65292;&#23613;&#31649;&#35757;&#32451;&#25439;&#22833;&#26377;&#25152;&#19981;&#21516;&#65292;&#20116;&#20010;&#26356;&#22797;&#26434;&#30340;&#65288;&#33258;&#36866;&#24212;&#65289;&#20248;&#21270;&#22120;&#22312;&#27979;&#35797;&#24615;&#33021;&#19978;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21482;&#35843;&#25972;&#23398;&#20064;&#29575;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19982;&#35843;&#25972;&#25152;&#26377;&#36229;&#21442;&#25968;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#36873;&#25321;&#34920;&#29616;&#26368;&#22909;&#30340;&#20219;&#20309;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
NLP research has explored different neural model architectures and sizes, datasets, training objectives, and transfer learning techniques. However, the choice of optimizer during training has not been explored as extensively. Typically, some variant of Stochastic Gradient Descent (SGD) is employed, selected among numerous variants, using unclear criteria, often with minimal or no tuning of the optimizer's hyperparameters. Experimenting with five GLUE datasets, two models (DistilBERT and DistilRoBERTa), and seven popular optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound), we find that when the hyperparameters of the optimizers are tuned, there is no substantial difference in test performance across the five more elaborate (adaptive) optimizers, despite differences in training loss. Furthermore, tuning just the learning rate is in most cases as good as tuning all the hyperparameters. Hence, we recommend picking any of the best-behaved adaptive optimizers (e.g.,
&lt;/p&gt;</description></item><item><title>LIFI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#31934;&#32454;&#25511;&#21046;&#20195;&#30721;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;&#36890;&#36807;&#36830;&#32493;&#12289;&#30456;&#23545;&#21644;&#38750;&#25490;&#20182;&#30340;&#25511;&#21046;&#20195;&#30721;&#30340;&#24341;&#23548;&#65292;LIFI&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#20351;&#29992;&#23646;&#24615;&#20998;&#31867;&#22120;&#33258;&#21160;&#23548;&#20986;&#30340;&#31934;&#32454;&#20195;&#30721;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36890;&#36807;&#19982;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#65292;LIFI&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.06930</link><description>&lt;p&gt;
LiFi: &#20351;&#29992;&#31934;&#32454;&#25511;&#21046;&#20195;&#30721;&#30340;&#36731;&#37327;&#32423;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LiFi: Lightweight Controlled Text Generation with Fine-Grained Control Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06930
&lt;/p&gt;
&lt;p&gt;
LIFI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#31934;&#32454;&#25511;&#21046;&#20195;&#30721;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;&#36890;&#36807;&#36830;&#32493;&#12289;&#30456;&#23545;&#21644;&#38750;&#25490;&#20182;&#30340;&#25511;&#21046;&#20195;&#30721;&#30340;&#24341;&#23548;&#65292;LIFI&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#20351;&#29992;&#23646;&#24615;&#20998;&#31867;&#22120;&#33258;&#21160;&#23548;&#20986;&#30340;&#31934;&#32454;&#20195;&#30721;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36890;&#36807;&#19982;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#65292;LIFI&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#36807;&#31243;&#20013;&#65292;&#23545;&#26356;&#31934;&#30830;&#30340;&#25511;&#21046;&#26426;&#21046;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;LIFI&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#24335;&#65292;&#20351;&#29992;&#31934;&#32454;&#25511;&#21046;&#20195;&#30721;&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#19981;&#21516;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26469;&#36981;&#24490;&#31163;&#25955;&#12289;&#20998;&#31867;&#21644;&#20114;&#26021;&#30340;&#25511;&#21046;&#20195;&#30721;&#65292;LIFI&#22312;&#36830;&#32493;&#12289;&#30456;&#23545;&#21644;&#38750;&#25490;&#20182;&#30340;&#25511;&#21046;&#20195;&#30721;&#30340;&#24341;&#23548;&#19979;&#23398;&#20064;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#20123;&#31934;&#32454;&#30340;&#20195;&#30721;&#26159;&#36890;&#36807;&#23646;&#24615;&#20998;&#31867;&#22120;&#33258;&#21160;&#23548;&#20986;&#30340;&#65292;&#23427;&#26368;&#21021;&#20351;&#29992;&#23569;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#29992;&#26469;&#26631;&#35760;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#26356;&#24191;&#27867;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#23558;&#31934;&#32454;&#25511;&#21046;&#20195;&#30721;&#19982;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#65292;&#36866;&#37197;&#22120;&#26159;&#19968;&#31181;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;LIFI&#22312;&#20004;&#20010;&#20256;&#32479;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;--&#21477;&#23376;&#29983;&#25104;&#21644;&#25991;&#26412;&#20998;&#31867;--&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of text generation, the demand for more precise control mechanisms has become increasingly apparent. To address this need, we present a novel methodology, LIFI, which offers a lightweight approach with fine-grained control for controlled text generation. Unlike previous studies that train pre-trained language models to follow discrete, categorical, and exclusive control codes, LIFI learns controlled text generation under the guidance of continuous, relative, and nonexclusive control codes. These fine-grained codes are automatically derived from an attribute classifier, initially trained with a small amount of labeled data and subsequently employed to label abundant unlabeled data, thus garnering more extensive supervision signals. Moreover, to achieve efficient control, we incorporate the fine-grained control codes with adapters, a parameter- and compute-efficient way to steer a pre-trained language model. We evaluate LIFI on two conventional tasks -- sent
&lt;/p&gt;</description></item><item><title>&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#32508;&#21512;&#30740;&#31350;&#20102;&#21508;&#31181;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#35299;&#30721;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#65292;&#21463;&#21040;&#23545;&#40784;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#37327;&#21270;&#31561;&#22240;&#32032;&#24433;&#21709;&#65307;&#26576;&#20123;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26435;&#34913;&#21462;&#33293;&#12290;</title><link>https://arxiv.org/abs/2402.06925</link><description>&lt;p&gt;
LLM&#26102;&#20195;&#35299;&#30721;&#26041;&#27861;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Thorough Examination of Decoding Methods in the Era of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06925
&lt;/p&gt;
&lt;p&gt;
&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#32508;&#21512;&#30740;&#31350;&#20102;&#21508;&#31181;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#35299;&#30721;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#65292;&#21463;&#21040;&#23545;&#40784;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#37327;&#21270;&#31561;&#22240;&#32032;&#24433;&#21709;&#65307;&#26576;&#20123;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26435;&#34913;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#26041;&#27861;&#22312;&#23558;&#35821;&#35328;&#27169;&#22411;&#20174;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#36716;&#25442;&#20026;&#23454;&#38469;&#20219;&#21153;&#35299;&#20915;&#22120;&#20013;&#36215;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#20197;&#24448;&#20851;&#20110;&#35299;&#30721;&#26041;&#27861;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#19978;&#65292;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#24403;&#21069;&#36890;&#29992;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26102;&#20195;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#35299;&#30721;&#31574;&#30053;&#30340;&#28044;&#20837;&#36827;&#19968;&#27493;&#22797;&#26434;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#21508;&#31181;&#35299;&#30721;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#32780;&#22810;&#26041;&#20301;&#30340;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#37096;&#32626;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12289;&#23545;&#36229;&#21442;&#25968;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#35299;&#30721;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#26126;&#26174;&#19982;&#20219;&#21153;&#30456;&#20851;&#65292;&#24182;&#21463;&#21040;&#23545;&#40784;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#37327;&#21270;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25935;&#24863;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#26576;&#20123;&#26041;&#27861;&#22312;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#21069;&#25552;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#31361;&#20986;&#20102;&#22312;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;</title><link>https://arxiv.org/abs/2402.06918</link><description>&lt;p&gt;
&#29992;&#30452;&#25509;&#30340;&#20004;&#20004;&#27604;&#36739;&#26041;&#27861;&#29983;&#25104;&#24605;&#32500;&#38142;&#65292;&#20197;&#25628;&#32034;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#24605;&#32500;&#38142;(Chain-of-Thoughts, CoT)&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#38382;&#39064;&#35299;&#20915;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36825;&#31181;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#28041;&#21450;&#20114;&#21160;&#21327;&#20316;&#65292;&#23398;&#20064;&#32773;&#29983;&#25104;&#20505;&#36873;&#20013;&#38388;&#24605;&#32500;&#65292;&#30001;LLMs&#35780;&#20272;&#65292;&#24341;&#23548;&#29983;&#25104;&#21518;&#32493;&#24605;&#32500;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24191;&#27867;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#65292;LLMs&#30340;&#35780;&#20272;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#21644;&#19981;&#21487;&#38752;&#24615;&#65292;&#21487;&#33021;&#35823;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#36873;&#25321;&#19981;&#22815;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#12290;&#26412;&#25991;&#21463;Vapnik&#21407;&#21017;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27604;&#36739;&#30340;CoT&#29983;&#25104;&#31639;&#27861;&#65292;&#30452;&#25509;&#26681;&#25454;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#30830;&#23450;&#26368;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#37197;&#23545;&#20013;&#38388;&#24605;&#32500;&#65292;&#24182;&#30452;&#25509;&#20419;&#20351;LLMs&#20174;&#27599;&#23545;&#20013;&#36873;&#25321;&#26356;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the ability of the large language model (LLMs) to handle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TL;DR Progress&#30340;&#26032;&#24037;&#20855;&#65292;&#29992;&#20110;&#25506;&#32034;&#31070;&#32463;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#30340;&#25991;&#29486;&#12290;&#35813;&#24037;&#20855;&#26681;&#25454;&#20840;&#38754;&#30340;&#27880;&#37322;&#26041;&#26696;&#23545;514&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#32452;&#32455;&#65292;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#12289;&#22810;&#26041;&#38754;&#30340;&#26816;&#32034;&#65292;&#24182;&#20026;&#27599;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#31616;&#27905;&#30340;&#25688;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.06913</link><description>&lt;p&gt;
TL;DR&#36827;&#23637;&#65306;&#22810;&#26041;&#38754;&#25991;&#29486;&#25506;&#32034;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TL;DR Progress&#30340;&#26032;&#24037;&#20855;&#65292;&#29992;&#20110;&#25506;&#32034;&#31070;&#32463;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#30340;&#25991;&#29486;&#12290;&#35813;&#24037;&#20855;&#26681;&#25454;&#20840;&#38754;&#30340;&#27880;&#37322;&#26041;&#26696;&#23545;514&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#32452;&#32455;&#65292;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#12289;&#22810;&#26041;&#38754;&#30340;&#26816;&#32034;&#65292;&#24182;&#20026;&#27599;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#31616;&#27905;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TL;DR&#36827;&#23637;&#65292;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#31070;&#32463;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#25991;&#29486;&#30340;&#26032;&#24037;&#20855;&#12290;&#23427;&#26681;&#25454;&#19968;&#20010;&#20840;&#38754;&#30340;&#27880;&#37322;&#26041;&#26696;&#23545;514&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#32452;&#32455;&#65292;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#12289;&#22810;&#26041;&#38754;&#30340;&#26816;&#32034;&#12290;&#27599;&#31687;&#35770;&#25991;&#37117;&#32463;&#36807;&#25163;&#24037;&#27880;&#37322;&#65292;&#25429;&#25417;&#20102;&#35780;&#20272;&#25351;&#26631;&#12289;&#36136;&#37327;&#32500;&#24230;&#12289;&#23398;&#20064;&#33539;&#24335;&#12289;&#35299;&#20915;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#25991;&#26723;&#39046;&#22495;&#31561;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#27599;&#31687;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#31616;&#27905;&#30340;&#25688;&#35201;&#65292;&#21253;&#25324;&#33258;&#21160;&#25552;&#21462;&#30340;&#19978;&#19979;&#25991;&#22240;&#32032;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#24037;&#20855;&#21487;&#22312;&#32447;&#35775;&#38382;https://www.tldr-progress.de&#65292;&#24182;&#25552;&#20379;&#28436;&#31034;&#35270;&#39057;https://youtu.be/uCVRGFvXUj8&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents TL;DR Progress, a new tool for exploring the literature on neural text summarization. It organizes 514~papers based on a comprehensive annotation scheme for text summarization approaches and enables fine-grained, faceted search. Each paper was manually annotated to capture aspects such as evaluation metrics, quality dimensions, learning paradigms, challenges addressed, datasets, and document domains. In addition, a succinct indicative summary is provided for each paper, consisting of automatically extracted contextual factors, issues, and proposed solutions. The tool is available online at https://www.tldr-progress.de, a demo video at https://youtu.be/uCVRGFvXUj8
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#23884;&#20837;&#26041;&#27861;&#22312;&#26597;&#35810;&#23548;&#21521;&#30340;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#19968;&#33268;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22797;&#26434;&#12289;&#22810;&#20027;&#39064;&#21644;&#22810;&#20154;&#21442;&#19982;&#30340;&#37325;&#35201;&#20250;&#35758;&#35760;&#24405;&#20013;&#30340;&#20449;&#24687;&#26597;&#25214;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06907</link><description>&lt;p&gt;
&#35843;&#26597;&#26597;&#35810;&#23548;&#21521;&#30340;&#20250;&#35758;&#25688;&#35201;&#30340;&#19968;&#33268;&#24615;&#65306;&#19981;&#21516;&#23884;&#20837;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Consistency in Query-Based Meeting Summarization: A Comparative Study of Different Embedding Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#23884;&#20837;&#26041;&#27861;&#22312;&#26597;&#35810;&#23548;&#21521;&#30340;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#19968;&#33268;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22797;&#26434;&#12289;&#22810;&#20027;&#39064;&#21644;&#22810;&#20154;&#21442;&#19982;&#30340;&#37325;&#35201;&#20250;&#35758;&#35760;&#24405;&#20013;&#30340;&#20449;&#24687;&#26597;&#25214;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20808;&#36827;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#24076;&#26395;&#36825;&#20123;&#25216;&#26415;&#33021;&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#38382;&#39064;&#12290;&#25991;&#26412;&#25688;&#35201;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#33879;&#21517;&#24212;&#29992;&#65292;&#20854;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#33258;&#21160;&#29983;&#25104;&#37325;&#35201;&#20449;&#24687;&#30340;&#25688;&#35201;&#65292;&#24403;&#24744;&#24517;&#39035;&#22788;&#29702;&#22823;&#37327;&#25991;&#26723;&#26102;&#65292;&#36825;&#28857;&#23588;&#20026;&#37325;&#35201;&#12290;&#25688;&#35201;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#24555;&#36895;&#25429;&#25417;&#20851;&#38190;&#28857;&#65292;&#24182;&#22312;&#24037;&#20316;&#20013;&#25552;&#20379;&#20415;&#21033;&#12290;&#20854;&#20013;&#19968;&#20010;&#36866;&#29992;&#30340;&#24773;&#20917;&#26159;&#20250;&#35758;&#25688;&#35201;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36739;&#38271;&#12289;&#22797;&#26434;&#12289;&#22810;&#20027;&#39064;&#21644;&#22810;&#20154;&#21442;&#19982;&#30340;&#37325;&#35201;&#20250;&#35758;&#12290;&#22240;&#27492;&#65292;&#24403;&#20154;&#20204;&#24819;&#35201;&#20174;&#20250;&#35758;&#20013;&#23457;&#26597;&#29305;&#23450;&#20869;&#23481;&#26102;&#65292;&#23558;&#24456;&#38590;&#19988;&#32791;&#26102;&#25214;&#21040;&#20250;&#35758;&#35760;&#24405;&#20013;&#30340;&#30456;&#20851;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20026;&#26032;&#38395;&#31295;&#12289;&#31185;&#23398;&#25991;&#31456;&#31561;&#20570;&#25688;&#35201;&#65292;&#36825;&#20123;&#25991;&#26723;&#20855;&#26377;&#26126;&#30830;&#30340;&#32467;&#26500;&#21644;&#23448;&#26041;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more advanced data analysis techniques emerging, people will expect these techniques to be applied in more complex tasks and solve problems in our daily lives. Text Summarization is one of famous applications in Natural Language Processing (NLP) field. It aims to automatically generate summary with important information based on a given context, which is important when you have to deal with piles of documents. Summarization techniques can help capture key points in a short time and bring convenience in works. One of applicable situation is meeting summarization, especially for important meeting that tend to be long, complicated, multi-topic and multi-person. Therefore, when people want to review specific content from a meeting, it will be hard and time-consuming to find the related spans in the meeting transcript. However, most of previous works focus on doing summarization for newsletters, scientific articles...etc, which have a clear document structure and an official f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06900</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#35782;&#21035;&#27602;&#24615;&#21527;&#65311;&#32467;&#26500;&#21270;&#27602;&#24615;&#35843;&#26597;&#26694;&#26550;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#36981;&#23432;&#31038;&#20250;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#23384;&#22312;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27602;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#20551;&#23450;&#30340;&#27602;&#24615;&#23450;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#33258;&#21160;&#40065;&#26834;&#24230;&#37327;&#65292;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#22238;&#24212;&#26159;&#21542;&#20855;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27602;&#24615;&#22240;&#32032;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#24230;&#37327;&#25351;&#26631;LLMs As ToxiciTy Evaluators&#65288;LATTE&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;F1&#24471;&#20998;&#27604;&#29616;&#26377;&#25216;&#26415;&#25351;&#26631;&#25552;&#39640;&#20102;12&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19978;&#28216;&#27602;&#24615;&#23545;&#24230;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
&lt;/p&gt;</description></item><item><title>GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06894</link><description>&lt;p&gt;
GenTranslate: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06894
&lt;/p&gt;
&lt;p&gt;
GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#36890;&#36807;&#20943;&#23569;&#34920;&#31034;&#35823;&#24046;&#21644;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#25512;&#21160;&#20102;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#32763;&#35793;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#26463;&#25628;&#32034;&#35299;&#30721;&#21644;&#21069;k&#20010;&#20551;&#35774;&#36873;&#25321;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#25216;&#26415;&#24448;&#24448;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;N-best&#20551;&#35774;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38656;&#35201;&#21333;&#20010;&#39640;&#36136;&#37327;&#36755;&#20986;&#24207;&#21015;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#8220;GenTranslate&#8221;&#65292;&#23427;&#22522;&#20110;LLMs&#26469;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21033;&#29992;LLMs&#20016;&#23500;&#30340;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#21487;&#20197;&#23558;N-best&#20505;&#36873;&#20154;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;LLM&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;HypoTransla&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTransla
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#24615;&#35843;&#26597;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21382;&#21490;&#12289;&#21457;&#23637;&#21644;&#21407;&#29702;&#65292;&#26088;&#22312;&#24110;&#21161;&#24191;&#27867;&#30340;&#35835;&#32773;&#32676;&#20307;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#32972;&#26223;&#21644;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.06853</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21382;&#21490;&#12289;&#21457;&#23637;&#21644;&#21407;&#29702;-&#19968;&#39033;&#32508;&#36848;&#24615;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
History, Development, and Principles of Large Language Models-An Introductory Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06853
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#24615;&#35843;&#26597;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21382;&#21490;&#12289;&#21457;&#23637;&#21644;&#21407;&#29702;&#65292;&#26088;&#22312;&#24110;&#21161;&#24191;&#27867;&#30340;&#35835;&#32773;&#32676;&#20307;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#32972;&#26223;&#21644;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#22522;&#30707;&#65292;&#21033;&#29992;&#25968;&#23398;&#26041;&#27861;&#26469;&#25512;&#24191;&#35821;&#35328;&#35268;&#24459;&#21644;&#30693;&#35782;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;&#32463;&#36807;&#20960;&#21313;&#24180;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#35821;&#35328;&#24314;&#27169;&#20174;&#26368;&#21021;&#30340;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#21457;&#23637;&#21040;&#24403;&#20170;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LLMs&#30340;&#24555;&#36895;&#28436;&#36827;&#24050;&#32463;&#36798;&#21040;&#20102;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#27700;&#24179;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLMs&#22312;&#25913;&#21892;&#24037;&#20316;&#21644;&#20010;&#20154;&#29983;&#27963;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#19968;&#33324;&#20174;&#19994;&#20154;&#21592;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#32972;&#26223;&#21644;&#21407;&#29702;&#20102;&#35299;&#26377;&#38480;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;LLMs&#30340;&#32508;&#36848;&#37117;&#38598;&#20013;&#22312;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#20351;&#29992;&#20102;&#19987;&#38376;&#30340;&#35821;&#35328;&#65292;&#32473;&#32570;&#20047;&#30456;&#20851;&#32972;&#26223;&#30693;&#35782;&#30340;&#20174;&#19994;&#20154;&#21592;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#31616;&#26126;&#25212;&#35201;&#30340;LLMs&#27010;&#36848;&#65292;&#20197;&#24110;&#21161;&#26356;&#24191;&#27867;&#30340;&#35835;&#32773;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLMs reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. 
&lt;/p&gt;</description></item><item><title>ChemLLM&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21270;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#25351;&#20196;&#26500;&#24314;&#26041;&#27861;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#35805;&#24418;&#24335;&#65292;&#20855;&#26377;&#24179;&#28369;&#23545;&#35805;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#21270;&#23398;&#30340;&#19977;&#20010;&#20027;&#35201;&#20219;&#21153;&#20013;&#20987;&#36133;&#20102;GPT-3.5&#12290;</title><link>https://arxiv.org/abs/2402.06852</link><description>&lt;p&gt;
ChemLLM: &#19968;&#20010;&#21270;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChemLLM: A Chemical Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06852
&lt;/p&gt;
&lt;p&gt;
ChemLLM&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21270;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#25351;&#20196;&#26500;&#24314;&#26041;&#27861;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#35805;&#24418;&#24335;&#65292;&#20855;&#26377;&#24179;&#28369;&#23545;&#35805;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#21270;&#23398;&#30340;&#19977;&#20010;&#20027;&#35201;&#20219;&#21153;&#20013;&#20987;&#36133;&#20102;GPT-3.5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21270;&#23398;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12289;&#20998;&#23376;&#29983;&#25104;&#12289;&#23454;&#39564;&#21327;&#35758;&#35774;&#35745;&#31561;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21270;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#25361;&#25112;&#26469;&#33258;&#20110;&#20107;&#23454;&#65292;&#22823;&#22810;&#25968;&#21270;&#23398;&#25968;&#25454;&#21644;&#31185;&#23398;&#30693;&#35782;&#20027;&#35201;&#23384;&#20648;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#20013;&#65292;&#30452;&#25509;&#20351;&#29992;&#36825;&#20123;&#32467;&#26500;&#21270;&#25968;&#25454;&#20250;&#24433;&#21709;&#27169;&#22411;&#32500;&#25345;&#36830;&#36143;&#23545;&#35805;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#26495;&#30340;&#25351;&#20196;&#26500;&#24314;&#26041;&#27861;&#65292;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#36716;&#21270;&#20026;&#31616;&#27905;&#23545;&#35805;&#24418;&#24335;&#65292;&#36866;&#21512;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ChemLLM&#65292;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21270;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#36827;&#34892;&#24179;&#28369;&#23545;&#35805;&#20132;&#20114;&#12290;ChemLLM&#22312;&#21270;&#23398;&#30340;&#19977;&#20010;&#20027;&#35201;&#20219;&#21153;&#65292;&#21363;&#21517;&#31216;&#36716;&#25442;&#12289;&#20998;&#23376;&#29983;&#25104;&#21644;&#23454;&#39564;&#21327;&#35758;&#35774;&#35745;&#26041;&#38754;&#65292;&#20987;&#36133;&#20102;GPT-3.5&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made impressive progress in chemistry applications, including molecular property prediction, molecular generation, experimental protocol design, etc. However, the community lacks a dialogue-based model specifically designed for chemistry. The challenge arises from the fact that most chemical data and scientific knowledge are primarily stored in structured databases, and the direct use of these structured data compromises the model's ability to maintain coherent dialogue. To tackle this issue, we develop a novel template-based instruction construction method that transforms structured knowledge into plain dialogue, making it suitable for language model training. By leveraging this approach, we develop ChemLLM, the first large language model dedicated to chemistry, capable of performing various tasks across chemical disciplines with smooth dialogue interaction. ChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name conversion, molecu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.06782</link><description>&lt;p&gt;
&#19982;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;LLMs&#36777;&#35770;&#20250;&#23548;&#33268;&#26356;&#30495;&#23454;&#30340;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Debating with More Persuasive LLMs Leads to More Truthful Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#25152;&#38656;&#34892;&#20026;&#19968;&#33268;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#23558;&#36229;&#36807;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#65292;&#20154;&#31867;&#35780;&#20272;&#30340;&#35282;&#33394;&#23558;&#28436;&#21464;&#20026;&#38750;&#19987;&#23478;&#30417;&#30563;&#19987;&#23478;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#25105;&#20204;&#38382;&#65306;&#26356;&#24369;&#30340;&#27169;&#22411;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#22312;&#31867;&#20284;&#30340;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26356;&#24378;&#30340;&#27169;&#22411;&#65288;&#19987;&#23478;&#65289;&#25317;&#26377;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#32780;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#38750;&#19987;&#23478;&#65289;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#26041;&#27861;&#26159;\textit{&#36777;&#35770;}&#65292;&#20854;&#20013;&#20004;&#20010;LLM&#19987;&#23478;&#20998;&#21035;&#25903;&#25345;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#19968;&#20010;&#38750;&#19987;&#23478;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#36777;&#35770; consistently&#24110;&#21161;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#65292;&#20998;&#21035;&#36798;&#21040;76%&#21644;88%&#30340;&#20934;&#30830;&#24615;&#65288;&#26420;&#32032;&#22522;&#20934;&#20998;&#21035;&#20026;48%&#21644;60%&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#20248;&#21270;&#19987;&#23478;&#36777;&#35770;&#32773;&#30340;&#35828;&#26381;&#21147;&#20250;&#25552;&#39640;&#38750;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#30340;&#35780;&#20272;&#25351;&#26631;&#20998;&#31867;&#27861;&#65292;&#20026;&#32479;&#19968;&#22522;&#20934;&#25552;&#20379;&#26041;&#21521;&#21644;&#24110;&#21161;&#12290;&#22312;&#19981;&#21516;&#20219;&#21153;&#12289;&#24230;&#37327;&#26631;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#19979;&#65292;&#35813;&#20998;&#31867;&#27861;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06766</link><description>&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Evaluation Metrics for Text Data Augmentation in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06766
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#30340;&#35780;&#20272;&#25351;&#26631;&#20998;&#31867;&#27861;&#65292;&#20026;&#32479;&#19968;&#22522;&#20934;&#25552;&#20379;&#26041;&#21521;&#21644;&#24110;&#21161;&#12290;&#22312;&#19981;&#21516;&#20219;&#21153;&#12289;&#24230;&#37327;&#26631;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#19979;&#65292;&#35813;&#20998;&#31867;&#27861;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#35843;&#30740;&#25253;&#21578;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#19981;&#21516;&#25216;&#26415;&#21644;&#36827;&#23637;&#12290;&#20960;&#20010;&#26694;&#26550;&#12289;&#24037;&#20855;&#21644;&#23384;&#20648;&#24211;&#25512;&#24191;&#20102;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#30340;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#12289;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#12289;&#20307;&#31995;&#32467;&#26500;&#21644;&#23454;&#39564;&#35774;&#32622;&#30340;&#32570;&#20047;&#35780;&#20272;&#26631;&#20934;&#21644;&#26041;&#27861;&#27604;&#36739;&#26631;&#20934;&#20351;&#24471;&#27604;&#36739;&#21464;&#24471;&#27627;&#26080;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#26041;&#27861;&#30340;&#32479;&#19968;&#24615;&#65292;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30740;&#31350;&#23558;&#21463;&#30410;&#20110;&#27604;&#36739;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#30340;&#32479;&#19968;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#22312;&#21162;&#21147;&#23547;&#25214;&#30456;&#20851;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#30340;&#35780;&#20272;&#25351;&#26631;&#20998;&#31867;&#27861;&#65292;&#24182;&#20316;&#20026;&#32479;&#19968;&#22522;&#20934;&#30340;&#26041;&#21521;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#20102;&#23454;&#26045;&#24037;&#20855;&#21644;&#25351;&#26631;&#35745;&#31639;&#30340;&#31867;&#21035;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#20026;&#20102;&#25512;&#36827;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#35780;&#20272;&#25351;&#26631;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent surveys on data augmentation for natural language processing have reported different techniques and advancements in the field. Several frameworks, tools, and repositories promote the implementation of text data augmentation pipelines. However, a lack of evaluation criteria and standards for method comparison due to different tasks, metrics, datasets, architectures, and experimental settings makes comparisons meaningless. Also, a lack of methods unification exists and text data augmentation research would benefit from unified metrics to compare different augmentation methods. Thus, academics and the industry endeavor relevant evaluation metrics for text data augmentation techniques. The contribution of this work is to provide a taxonomy of evaluation metrics for text augmentation methods and serve as a direction for a unified benchmark. The proposed taxonomy organizes categories that include tools for implementation and metrics calculation. Finally, with this study, we intend to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EntGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;Entity Disambiguation&#65288;ED&#65289;&#20219;&#21153;&#65292;&#36830;&#25509;&#20102;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#24211;&#12290;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#21644;&#25351;&#20196;&#35843;&#25972;&#65292;&#35813;&#27169;&#22411;&#22312;&#27809;&#26377;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#20307;&#28040;&#27495;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06738</link><description>&lt;p&gt;
EntGPT: &#23558;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#24211;&#30456;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
EntGPT: Linking Generative Large Language Models with Knowledge Bases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EntGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;Entity Disambiguation&#65288;ED&#65289;&#20219;&#21153;&#65292;&#36830;&#25509;&#20102;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#24211;&#12290;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#21644;&#25351;&#20196;&#35843;&#25972;&#65292;&#35813;&#27169;&#22411;&#22312;&#27809;&#26377;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#20307;&#28040;&#27495;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#32570;&#20047;&#20107;&#23454;&#26680;&#23454;&#21644;&#30693;&#35782;&#22522;&#30784;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#20107;&#23454;&#27491;&#30830;&#36755;&#20986;&#30340;&#33021;&#21147;&#30456;&#23545;&#36739;&#23569;&#34987;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;Entity Disambiguation&#65288;ED&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#25552;&#31034;&#24037;&#31243;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#27493;&#30828;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#22312;&#27809;&#26377;&#26377;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#30340;&#24773;&#20917;&#19979;&#25506;&#27979;LLM&#30340;ED&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35813;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#21407;&#22987;&#22522;&#20934;&#27169;&#22411;&#30340;&#24494;F_1&#24471;&#20998;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;36%&#29978;&#33267;&#26356;&#39640;&#65292;&#24182;&#22312;10&#20010;&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#30340;SFT&#26041;&#27861;&#30456;&#27604;&#65292;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31867;&#20284;&#30340;&#25552;&#31034;&#21644;&#21709;&#24212;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#30693;&#35782;&#22522;&#30784;&#12290;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#22312;&#21463;&#30417;&#30563;&#23454;&#20307;&#28040;&#27495;&#20219;&#21153;&#19978;&#19981;&#20165;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24494;F1&#24471;&#20998;&#24615;&#33021;&#65292;&#32780;&#19988;&#24179;&#22343;&#24494;F_1&#25552;&#39640;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of Large Language Models (LLMs) to generate factually correct output remains relatively unexplored due to the lack of fact-checking and knowledge grounding during training and inference. In this work, we aim to address this challenge through the Entity Disambiguation (ED) task. We first consider prompt engineering, and design a three-step hard-prompting method to probe LLMs' ED performance without supervised fine-tuning (SFT). Overall, the prompting method improves the micro-F_1 score of the original vanilla models by a large margin, on some cases up to 36% and higher, and obtains comparable performance across 10 datasets when compared to existing methods with SFT. We further improve the knowledge grounding ability through instruction tuning (IT) with similar prompts and responses. The instruction-tuned model not only achieves higher micro-F1 score performance as compared to several baseline methods on supervised entity disambiguation tasks with an average micro-F_1 improve
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06733</link><description>&lt;p&gt;
NICE: &#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#36824;&#26159;&#19981;&#20248;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
NICE: To Optimize In-Context Examples or Not?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;ICE&#65289;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#22312;&#25552;&#31034;&#20449;&#24687;&#20013;&#35201;&#20040;&#26159;&#22266;&#23450;&#30340;&#65292;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#25351;&#20196;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#34920;&#38754;&#19978;&#30340;&#20849;&#35782;&#65306;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#32463;&#36807;&#25351;&#23548;&#30340;LLMs&#25361;&#25112;&#36825;&#19968;&#20849;&#35782;&#65292;&#30740;&#31350;&#22312;&#25552;&#20379;&#20102;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26159;&#21542;&#24517;&#35201;&#65292;&#24182;&#21457;&#29616;&#26377;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;"&#24230;&#37327;&#26631;&#20934;"&#65288;Metric&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#25351;&#20196;&#38598;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#21487;&#32534;&#35793;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#24182;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#28304;&#20195;&#30721;&#24314;&#35758;&#21644;&#33258;&#21160;&#34917;&#20840;&#21151;&#33021;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#24320;&#21457;&#32773;&#30340;&#32534;&#30721;&#24847;&#22270;&#24182;&#20934;&#30830;&#25512;&#26029;&#31867;&#22411;&#12289;&#21517;&#31216;&#21644;&#19978;&#19979;&#25991;&#31561;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.06690</link><description>&lt;p&gt;
&#28304;&#20195;&#30721;&#21512;&#25104;&#21644;&#34917;&#20840;&#30340;&#31070;&#32463;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Models for Source Code Synthesis and Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#21487;&#32534;&#35793;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#24182;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#28304;&#20195;&#30721;&#24314;&#35758;&#21644;&#33258;&#21160;&#34917;&#20840;&#21151;&#33021;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#24320;&#21457;&#32773;&#30340;&#32534;&#30721;&#24847;&#22270;&#24182;&#20934;&#30830;&#25512;&#26029;&#31867;&#22411;&#12289;&#21517;&#31216;&#21644;&#19978;&#19979;&#25991;&#31561;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#21040;&#20195;&#30721;&#24314;&#35758;&#31995;&#32479;&#36890;&#36807;&#23558;NL&#34920;&#36798;&#36716;&#21270;&#20026;&#21487;&#32534;&#35793;&#30340;&#20195;&#30721;&#29255;&#27573;&#26469;&#24110;&#21161;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDE&#65289;&#20013;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#30828;&#32534;&#30721;&#12289;&#35268;&#21017;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#20027;&#35201;&#20381;&#38752;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#23558;NL&#30340;&#27169;&#24335;&#25110;&#20854;&#35821;&#27861;&#35299;&#26512;&#26641;&#20013;&#30340;&#20803;&#32032;&#26144;&#23556;&#21040;&#21508;&#31181;&#26597;&#35810;&#32467;&#26500;&#65292;&#24182;&#19988;&#21482;&#33021;&#22788;&#29702;&#21463;&#38480;&#21046;&#30340;NL&#23376;&#38598;&#21644;&#38480;&#21046;&#30340;NL&#35821;&#27861;&#12290;&#36825;&#20123;&#31995;&#32479;&#26080;&#27861;&#20174;&#24320;&#21457;&#32773;&#30340;&#32534;&#30721;&#24847;&#22270;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24120;&#24120;&#26080;&#27861;&#25512;&#26029;&#31867;&#22411;&#12289;&#21517;&#31216;&#21644;&#28304;&#20195;&#30721;&#30340;&#19978;&#19979;&#25991;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#31995;&#32479;&#32423;&#20195;&#30721;&#24314;&#35758;&#12290;&#22312;&#26412;&#30805;&#22763;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#23558;NL&#26144;&#23556;&#21040;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#65292;&#21487;&#20197;&#26681;&#25454;NL&#30340;&#24847;&#22270;&#20026;&#29992;&#25143;&#25552;&#20379;&#28304;&#20195;&#30721;&#29255;&#27573;&#30340;&#24314;&#35758;&#65292;&#24182;&#25193;&#23637;&#28304;&#20195;&#30721;&#30340;&#33258;&#21160;&#34917;&#20840;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language (NL) to code suggestion systems assist developers in Integrated Development Environments (IDEs) by translating NL utterances into compilable code snippet. The current approaches mainly involve hard-coded, rule-based systems based on semantic parsing. These systems make heavy use of hand-crafted rules that map patterns in NL or elements in its syntax parse tree to various query constructs and can only work on a limited subset of NL with a restricted NL syntax. These systems are unable to extract semantic information from the coding intents of the developer, and often fail to infer types, names, and the context of the source code to get accurate system-level code suggestions. In this master thesis, we present sequence-to-sequence deep learning models and training paradigms to map NL to general-purpose programming languages that can assist users with suggestions of source code snippets, given a NL intent, and also extend auto-completion functionality of the source code to
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06665</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#22312;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Essential Role of Causality in Foundation World Models for Embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06665
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22522;&#30784;&#27169;&#22411;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#23545;&#35805;&#20195;&#29702;&#26041;&#38754;&#65292;&#24341;&#21457;&#20102;&#23545;&#20855;&#22791;&#26222;&#36941;&#33021;&#21147;&#30340;&#20855;&#36523;&#20195;&#29702;&#20154;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#36825;&#26679;&#30340;&#20195;&#29702;&#20154;&#38656;&#35201;&#33021;&#22815;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26410;&#33021;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#26159;&#19981;&#22815;&#30340;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20855;&#36523;&#20195;&#29702;&#29983;&#25104;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#30340;&#21069;&#26223;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#25552;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#25972;&#21512;&#22240;&#26524;&#20851;&#31995;&#26159;&#20419;&#36827;&#19982;&#19990;&#30028;&#30340;&#26377;&#24847;&#20041;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#32972;&#26223;&#19979;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents would require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions with the real world thus not sufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitate meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38450;&#24481;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#23545;&#25239;&#24615;&#20928;&#21270;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#20197;&#20928;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06655</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#25991;&#26412;&#20928;&#21270;&#65306;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Text Purification: A Large Language Model Approach for Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38450;&#24481;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#23545;&#25239;&#24615;&#20928;&#21270;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#20197;&#20928;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#20928;&#21270;&#26159;&#19968;&#31181;&#38450;&#24481;&#26426;&#21046;&#65292;&#29992;&#20110;&#20445;&#25252;&#20998;&#31867;&#22120;&#20813;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#25915;&#20987;&#31867;&#22411;&#25110;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#12290;&#36825;&#20123;&#25216;&#26415;&#23545;&#34987;&#25915;&#20987;&#36755;&#20837;&#36827;&#34892;&#29305;&#24449;&#21270;&#21644;&#28040;&#38500;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26088;&#22312;&#24674;&#22797;&#20986;&#19982;&#26368;&#21021;&#34987;&#25915;&#20987;&#30340;&#36755;&#20837;&#30456;&#20284;&#19988;&#34987;&#20998;&#31867;&#22120;&#27491;&#30830;&#20998;&#31867;&#30340;&#20928;&#21270;&#26679;&#26412;&#12290;&#30001;&#20110;&#31163;&#25955;&#36755;&#20837;&#30340;&#22122;&#22768;&#25200;&#21160;&#29305;&#24449;&#21270;&#25152;&#24102;&#26469;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#23545;&#25239;&#24615;&#25991;&#26412;&#20928;&#21270;&#19968;&#30452;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#20928;&#21270;&#26041;&#27861;&#22312;&#20445;&#25252;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#25991;&#26412;&#20928;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#20928;&#21270;&#23545;&#25239;&#24615;&#25991;&#26412;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#29305;&#24449;&#21270;&#31163;&#25955;&#22122;&#22768;&#25200;&#21160;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#26469;&#21033;&#29992;LLMs&#24674;&#22797;&#20928;&#21270;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial purification is a defense mechanism for safeguarding classifiers against adversarial attacks without knowing the type of attacks or training of the classifier. These techniques characterize and eliminate adversarial perturbations from the attacked inputs, aiming to restore purified samples that retain similarity to the initially attacked ones and are correctly classified by the classifier. Due to the inherent challenges associated with characterizing noise perturbations for discrete inputs, adversarial text purification has been relatively unexplored. In this paper, we investigate the effectiveness of adversarial purification methods in defending text classifiers. We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations. We utilize prompt engineering to exploit LLMs for recovering the purified examples for
&lt;/p&gt;</description></item><item><title>SocraSynth&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24179;&#21488;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#32479;&#35745;&#21644;&#31995;&#32479;&#21270;&#30340;&#35821;&#22659;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#21450;&#21487;&#35843;&#33410;&#30340;&#36777;&#35770;&#20105;&#35758;&#31243;&#24230;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38754;&#20020;&#30340;&#20559;&#35265;&#12289;&#24187;&#35273;&#21644;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06634</link><description>&lt;p&gt;
SocraSynth:&#22522;&#20110;&#26465;&#20214;&#32479;&#35745;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SocraSynth: Multi-LLM Reasoning with Conditional Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06634
&lt;/p&gt;
&lt;p&gt;
SocraSynth&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24179;&#21488;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#32479;&#35745;&#21644;&#31995;&#32479;&#21270;&#30340;&#35821;&#22659;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#21450;&#21487;&#35843;&#33410;&#30340;&#36777;&#35770;&#20105;&#35758;&#31243;&#24230;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38754;&#20020;&#30340;&#20559;&#35265;&#12289;&#24187;&#35273;&#21644;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23454;&#29992;&#19978;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#21644;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SocraSynth&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#29702;&#24179;&#21488;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;SocraSynth&#36890;&#36807;&#36830;&#32493;&#30340;&#35770;&#35777;&#21644;&#21487;&#35843;&#33410;&#30340;&#20105;&#35758;&#31243;&#24230;&#65292;&#21033;&#29992;&#26465;&#20214;&#32479;&#35745;&#21644;&#31995;&#32479;&#21270;&#30340;&#35821;&#22659;&#22686;&#24378;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20248;&#21183;&#12290;&#35813;&#24179;&#21488;&#36890;&#24120;&#30001;&#19968;&#20010;&#20154;&#31867;&#20027;&#25345;&#32773;&#21644;&#20004;&#20010;&#20195;&#34920;&#20114;&#30456;&#23545;&#25239;&#31435;&#22330;&#30340;LLM&#20195;&#29702;&#32452;&#25104;&#12290;SocraSynth&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;&#30693;&#35782;&#29983;&#25104;&#21644;&#25512;&#29702;&#35780;&#20272;&#12290;&#22312;&#30693;&#35782;&#29983;&#25104;&#38454;&#27573;&#65292;&#20027;&#25345;&#32773;&#23450;&#20041;&#20102;&#36777;&#35770;&#35805;&#39064;&#21644;&#20105;&#35758;&#31243;&#24230;&#65292;&#20419;&#20351;&#20195;&#29702;&#21830;&#20026;&#21508;&#33258;&#30340;&#31435;&#22330;&#21046;&#23450;&#25903;&#25345;&#24615;&#30340;&#35770;&#35777;&#12290;&#28982;&#21518;&#65292;&#22312;&#25512;&#29702;&#35780;&#20272;&#38454;&#27573;&#65292;&#37319;&#29992;&#20102;&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#21644;&#24418;&#24335;&#36923;&#36753;&#21407;&#29702;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#35770;&#35777;&#30340;&#36136;&#37327;&#12290;&#23545;&#35805;&#20197;&#20027;&#25345;&#32773;&#35843;&#25972;&#20105;&#35758;&#31243;&#24230;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability. This paper introduces SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these issues. SocraSynth utilizes conditional statistics and systematic context enhancement through continuous arguments, alongside adjustable debate contentiousness levels. The platform typically involves a human moderator and two LLM agents representing opposing viewpoints on a given subject. SocraSynth operates in two main phases: knowledge generation and reasoning evaluation. In the knowledge generation phase, the moderator defines the debate topic and contentiousness level, prompting the agents to formulate supporting arguments for their respective stances. The reasoning evaluation phase then employs Socratic reasoning and formal logic principles to appraise the quality of the arguments presented. The dialogue concludes with the moderator adjusting the contentiousn
&lt;/p&gt;</description></item><item><title>LightCAM&#26159;&#19968;&#31181;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#23631;&#34109;&#30340;D-TDNN&#35828;&#35805;&#20154;&#39564;&#35777;&#23454;&#29616;&#65292;&#36890;&#36807;&#37319;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#27169;&#22359;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#32858;&#21512;&#65292;&#23427;&#22312;VoxCeleb&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06073</link><description>&lt;p&gt;
LightCAM: &#19968;&#31181;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#23631;&#34109;&#30340;D-TDNN&#35828;&#35805;&#20154;&#39564;&#35777;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
LightCAM: A Fast and Light Implementation of Context-Aware Masking based D-Tdnn for Speaker Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06073
&lt;/p&gt;
&lt;p&gt;
LightCAM&#26159;&#19968;&#31181;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#23631;&#34109;&#30340;D-TDNN&#35828;&#35805;&#20154;&#39564;&#35777;&#23454;&#29616;&#65292;&#36890;&#36807;&#37319;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#27169;&#22359;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#32858;&#21512;&#65292;&#23427;&#22312;VoxCeleb&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;(TDNN)&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#38590;&#20197;&#23454;&#26045;&#12290;&#20855;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;&#23631;&#34109;(CAM)&#27169;&#22359;&#30340;&#23494;&#38598;&#36830;&#36890;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;(D-TDNN)&#24050;&#32463;&#35777;&#26126;&#26159;&#19968;&#31181;&#38477;&#20302;&#22797;&#26434;&#24615;&#24182;&#20445;&#25345;&#31995;&#32479;&#24615;&#33021;&#30340;&#39640;&#25928;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36731;&#37327;&#32423;&#27169;&#22411;LightCAM&#65292;&#23427;&#36827;&#19968;&#27493;&#37319;&#29992;&#20102;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#27169;&#22359;(DSM)&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#32858;&#21512;(MFA)&#20197;&#23454;&#29616;&#19981;&#21516;&#23618;&#27425;&#30340;&#29305;&#24449;&#34701;&#21512;&#12290;&#22312;VoxCeleb&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;VoxCeleb1-O&#19978;&#23454;&#29616;&#20102;0.83&#30340;&#31561;&#38169;&#35823;&#29575;(EER)&#21644;0.0891&#30340;&#26368;&#23567;&#26816;&#27979;&#20195;&#20215;&#22240;&#23376;(MinDCF)&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#20027;&#27969;&#30340;&#35828;&#35805;&#20154;&#39564;&#35777;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22797;&#26434;&#24230;&#20998;&#26512;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional Time Delay Neural Networks (TDNN) have achieved state-of-the-art performance at the cost of high computational complexity and slower inference speed, making them difficult to implement in an industrial environment. The Densely Connected Time Delay Neural Network (D-TDNN) with Context Aware Masking (CAM) module has proven to be an efficient structure to reduce complexity while maintaining system performance. In this paper, we propose a fast and lightweight model, LightCAM, which further adopts a depthwise separable convolution module (DSM) and uses multi-scale feature aggregation (MFA) for feature fusion at different levels. Extensive experiments are conducted on VoxCeleb dataset, the comparative results show that it has achieved an EER of 0.83 and MinDCF of 0.0891 in VoxCeleb1-O, which outperforms the other mainstream speaker verification methods. In addition, complexity analysis further demonstrates that the proposed architecture has lower computational cost and faster inf
&lt;/p&gt;</description></item><item><title>LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22686;&#21152;&#20102;&#36873;&#25321;&#24615;&#26333;&#20809;&#65292;&#19988;&#25903;&#25345;&#29992;&#25143;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.05880</link><description>&lt;p&gt;
&#29983;&#25104;&#24615;&#22238;&#38899;&#23460;&#65311;LLM&#39537;&#21160;&#30340;&#25628;&#32034;&#31995;&#32479;&#23545;&#22810;&#26679;&#21270;&#20449;&#24687;&#25628;&#32034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05880
&lt;/p&gt;
&lt;p&gt;
LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22686;&#21152;&#20102;&#36873;&#25321;&#24615;&#26333;&#20809;&#65292;&#19988;&#25903;&#25345;&#29992;&#25143;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20159;&#20154;&#24050;&#32463;&#20351;&#29992;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#65292;&#24182;&#19988;&#30456;&#20449;&#36825;&#20123;&#31995;&#32479;&#30456;&#27604;&#20256;&#32479;&#25628;&#32034;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#21644;&#20844;&#20849;&#35752;&#35770;&#37117;&#35843;&#26597;&#20102;&#25628;&#32034;&#31995;&#32479;&#22312;&#22686;&#21152;&#36873;&#25321;&#24615;&#26333;&#20809;&#21644;&#20135;&#29983;&#22238;&#38899;&#23460;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21363;&#38480;&#21046;&#25509;&#35302;&#22810;&#26679;&#21270;&#24847;&#35265;&#24182;&#23548;&#33268;&#24847;&#35265;&#20559;&#25191;&#65292;&#20294;&#23545;&#20110;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#36825;&#31181;&#39118;&#38505;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#39564;&#26469;&#30740;&#31350;&#65306;1&#65289;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#30456;&#36739;&#20110;&#20256;&#32479;&#25628;&#32034;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#22686;&#21152;&#36873;&#25321;&#24615;&#26333;&#20809;&#65307;2&#65289;&#20855;&#26377;&#25903;&#25345;&#25110;&#25361;&#25112;&#29992;&#25143;&#35266;&#28857;&#30340;&#24847;&#35265;&#20559;&#35265;&#30340;LLM&#22914;&#20309;&#25913;&#21464;&#36825;&#31181;&#24433;&#21709;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#21442;&#19982;&#32773;&#22312;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#26356;&#20542;&#21521;&#20110;&#36827;&#34892;&#20559;&#35265;&#30340;&#20449;&#24687;&#26597;&#35810;&#65292;&#24182;&#19988;&#25903;&#25345;&#20182;&#20204;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;&#30340;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#21576;&#29616;&#20102;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers -- limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user's view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implicatio
&lt;/p&gt;</description></item><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26681;&#25454;&#19981;&#21516;&#30340;&#27169;&#24577;&#35843;&#25972;&#21644;&#34920;&#31034;&#24207;&#21015;&#26631;&#35760;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05783</link><description>&lt;p&gt;
&#20351;&#29992;&#27169;&#24577;&#30456;&#23545;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#29983;&#25104;&#25991;&#26412;&#21040;&#20195;&#30721;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Text-to-Code Generation with Modality-relative Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26681;&#25454;&#19981;&#21516;&#30340;&#27169;&#24577;&#35843;&#25972;&#21644;&#34920;&#31034;&#24207;&#21015;&#26631;&#35760;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32534;&#31243;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36890;&#24120;&#36890;&#36807;&#36827;&#19968;&#27493;&#39044;&#20808;&#35757;&#32451;&#20005;&#26684;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#35757;&#32451;&#24207;&#21015;&#36890;&#24120;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#21644;&#65288;&#32447;&#24615;&#21270;&#30340;&#65289;&#32534;&#31243;&#35821;&#35328;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#24207;&#21015;&#30340;&#20004;&#31181;&#27169;&#24577;&#26144;&#23556;&#21040;&#21516;&#19968;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#28982;&#32780;&#65292;&#32534;&#31243;&#35821;&#35328;&#20851;&#38190;&#35789;&#65288;&#20363;&#22914;&#8220;while&#8221;&#65289;&#24448;&#24448;&#20855;&#26377;&#38750;&#24120;&#20005;&#26684;&#30340;&#23450;&#20041;&#35821;&#20041;&#12290;&#22240;&#27492;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#29992;&#27861;&#36827;&#34892;&#30340;&#36801;&#31227;&#23398;&#20064;&#23545;&#20854;&#20195;&#30721;&#24212;&#29992;&#21487;&#33021;&#24182;&#19981;&#19968;&#23450;&#26377;&#30410;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#24050;&#32463;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#26681;&#25454;&#23427;&#20204;&#25152;&#23646;&#30340;&#27169;&#24577;&#19981;&#21516;&#26469;&#35843;&#25972;&#21644;&#34920;&#31034;&#24207;&#21015;&#26631;&#35760;&#65292;&#24182;&#26368;&#32456;&#26377;&#30410;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#27169;&#24577;&#30456;&#23545;&#35757;&#32451;&#30446;&#26631;&#30340;&#36827;&#19968;&#27493;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#23581;&#35797;&#20102;&#22312;&#27169;&#24577;&#20043;&#38388;&#20998;&#31163;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model--where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. ``while'') often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#24182;&#19988;&#36895;&#24230;&#26159;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30340;&#20004;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.05406</link><description>&lt;p&gt;
&#29616;&#22312;&#25152;&#26377;&#20154;&#37117;&#20462;&#21098;&#65306;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#24182;&#19988;&#36895;&#24230;&#26159;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#38750;&#19987;&#19994;&#20174;&#19994;&#32773;&#21644;&#26368;&#23500;&#26377;&#36164;&#28304;&#30340;&#26426;&#26500;&#20043;&#38388;&#30340;&#30828;&#20214;&#24046;&#36317;&#65292;&#23610;&#23544;&#19981;&#26029;&#22686;&#38271;&#30340;LLM&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#20351;&#29992;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#21387;&#32553;LLM&#65292;&#20197;&#20351;&#20854;&#36164;&#28304;&#28040;&#32791;&#21487;&#31649;&#29702;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26412;&#36523;&#24448;&#24448;&#32791;&#36153;&#36164;&#28304;&#65292;&#20351;&#20854;&#30446;&#26631;&#29992;&#25143;&#32676;&#26080;&#27861;&#25509;&#35302;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#35753;&#20174;&#19994;&#32773;&#33021;&#22815;&#20462;&#21098;&#27169;&#22411;&#65292;&#20351;&#20854;&#35268;&#27169;&#22823;&#21040;&#30828;&#20214;&#20165;&#26377;&#36275;&#22815;&#30340;&#20869;&#23384;&#26469;&#36816;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Bonsai&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#26799;&#24230;&#12289;&#25200;&#21160;&#20462;&#21098;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#23567;&#12289;&#24555;&#21644;&#20934;&#30830;&#30340;&#20462;&#21098;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#65288;i&#65289;&#20248;&#20110;&#26356;&#26114;&#36149;&#30340;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#65288;ii&#65289;&#19982;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30456;&#27604;&#65292;&#36895;&#24230;&#24555;&#19968;&#20493;&#19988;&#20934;&#30830;&#24615;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;LEAP&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65292;&#28982;&#21518;&#21453;&#24605;&#24182;&#23398;&#20064;&#20934;&#21017;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05403</link><description>&lt;p&gt;
&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20934;&#21017;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Principle Learning from Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;LEAP&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65292;&#28982;&#21518;&#21453;&#24605;&#24182;&#23398;&#20064;&#20934;&#21017;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65292;&#20063;&#31216;&#20026;&#23569;&#26679;&#26412;&#25552;&#31034;&#65289;&#24050;&#25104;&#20026;&#23558;LLMs&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#22522;&#20110;ICL&#30340;&#26041;&#27861;&#21482;&#20174;&#27491;&#30830;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#20013;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#36825;&#19968;&#33539;&#20363;&#65292;&#36890;&#36807;&#20174;&#23569;&#32473;&#23450;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#23398;&#20064;&#26356;&#22810;&#20869;&#23481;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#20934;&#21017;&#65288;LEAP&#65289;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#26377;&#24847;&#35825;&#20351;&#27169;&#22411;&#22312;&#36825;&#20123;&#23569;&#37327;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#21453;&#24605;&#36825;&#20123;&#38169;&#35823;&#65292;&#24182;&#20174;&#20013;&#23398;&#20064;&#26174;&#24335;&#30340;&#20219;&#21153;&#29305;&#23450;&#8220;&#20934;&#21017;&#8221;&#65292;&#36825;&#20123;&#20934;&#21017;&#26377;&#21161;&#20110;&#35299;&#20915;&#31867;&#20284;&#30340;&#38382;&#39064;&#24182;&#36991;&#20813;&#24120;&#35265;&#30340;&#38169;&#35823;&#65307;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#36825;&#20123;&#23398;&#21040;&#30340;&#36890;&#29992;&#20934;&#21017;&#26469;&#25552;&#31034;&#27169;&#22411;&#22238;&#31572;&#26410;&#35265;&#36807;&#30340;&#27979;&#35797;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65288;Hotpot QA&#65289;&#12289;&#25991;&#26412;&#38382;&#39064;&#22238;&#31572;&#65288;DROP&#65289;&#12289;Big-Bench&#22256;&#38590;&#25512;&#29702;&#21644;&#25968;&#23398;&#38382;&#39064;&#65288;GSM8K&#21644;MATH&#65289;&#22312;&#20869;&#30340;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;LEAP&#65307;&#22312;&#25152;&#26377;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;LEAP&#37117;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific "principles" from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24674;&#22797;&#37327;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#32500;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#28608;&#27963;&#31934;&#24230;&#24182;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2402.05147</link><description>&lt;p&gt;
ApiQ&#65306;2&#20301;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
ApiQ: Finetuning of 2-Bit Quantized Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05147
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24674;&#22797;&#37327;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#32500;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#28608;&#27963;&#31934;&#24230;&#24182;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#20869;&#23384;&#39640;&#25928;&#30340;&#27169;&#22411;&#24494;&#35843;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;GPU&#20869;&#23384;&#38480;&#21046;&#21644;&#36825;&#20123;&#26041;&#27861;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#21487;&#27604;&#32467;&#26524;&#25152;&#24102;&#26469;&#30340;&#32422;&#26463;&#12290;&#23613;&#31649;&#26377;&#20102;&#36827;&#23637;&#65292;&#22914;QLoRA&#36825;&#26679;&#30340;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#22312;&#19981;&#21516;&#20301;&#23485;&#30340;&#37327;&#21270;&#21644;&#22810;&#26679;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#19981;&#19968;&#33268;&#20027;&#35201;&#26469;&#33258;&#20110;&#37327;&#21270;&#36807;&#31243;&#23545;&#20445;&#30041;&#30693;&#35782;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21066;&#24369;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#24494;&#35843;&#20013;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;LLM&#30340;&#26435;&#37325;&#26469;&#24674;&#22797;&#37327;&#21270;&#25439;&#22833;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#20102;&#21407;&#22987;LLM&#30340;&#28608;&#27963;&#31934;&#24230;&#30340;&#32500;&#25345;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#35823;&#24046;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20999;&#22359;&#36130;&#21153;&#25253;&#21578;&#65292;&#36890;&#36807;&#26681;&#25454;&#25991;&#26723;&#30340;&#32467;&#26500;&#20803;&#32032;&#32452;&#20214;&#36827;&#34892;&#20999;&#22359;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#20999;&#22359;&#22823;&#23567;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25972;&#20307;&#19978;&#19979;&#25991;&#21644;&#20934;&#30830;&#24615;&#30340;&#35780;&#20272;&#20197;&#21450;&#23545;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05131</link><description>&lt;p&gt;
&#26377;&#25928;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#36130;&#21153;&#25253;&#21578;&#20999;&#22359;
&lt;/p&gt;
&lt;p&gt;
Financial Report Chunking for Effective Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20999;&#22359;&#36130;&#21153;&#25253;&#21578;&#65292;&#36890;&#36807;&#26681;&#25454;&#25991;&#26723;&#30340;&#32467;&#26500;&#20803;&#32032;&#32452;&#20214;&#36827;&#34892;&#20999;&#22359;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#20999;&#22359;&#22823;&#23567;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25972;&#20307;&#19978;&#19979;&#25991;&#21644;&#20934;&#30830;&#24615;&#30340;&#35780;&#20272;&#20197;&#21450;&#23545;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#22359;&#20449;&#24687;&#26159;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27573;&#33853;&#32423;&#20999;&#22359;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#25152;&#26377;&#25991;&#26412;&#37117;&#35270;&#20026;&#24179;&#31561;&#30340;&#65292;&#24182;&#24573;&#30053;&#20102;&#25991;&#26723;&#32467;&#26500;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#20165;&#20165;&#23558;&#25991;&#26723;&#20999;&#22359;&#21040;&#27573;&#33853;&#32423;&#21035;&#65292;&#32780;&#26159;&#26681;&#25454;&#25991;&#26723;&#30340;&#32467;&#26500;&#20803;&#32032;&#32452;&#20214;&#26469;&#20999;&#22359;&#12290;&#23558;&#25991;&#26723;&#20998;&#35299;&#20026;&#36825;&#20123;&#32452;&#25104;&#20803;&#32032;&#21487;&#20197;&#21019;&#24314;&#19968;&#31181;&#26032;&#30340;&#25991;&#26723;&#20999;&#22359;&#26041;&#24335;&#65292;&#21487;&#20197;&#24471;&#21040;&#26368;&#20339;&#30340;&#20999;&#22359;&#22823;&#23567;&#65292;&#26080;&#38656;&#35843;&#25972;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#35780;&#20272;&#26681;&#25454;&#30001;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#27880;&#37322;&#30340;&#20803;&#32032;&#31867;&#22411;&#36827;&#34892;&#20999;&#22359;&#22914;&#20309;&#23545;&#25152;&#26816;&#32034;&#20449;&#24687;&#30340;&#25972;&#20307;&#19978;&#19979;&#25991;&#21644;&#20934;&#30830;&#24615;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;RAG&#36741;&#21161;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;&#23545;&#21508;&#31181;&#20803;&#32032;&#31867;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#23427;&#20204;&#22312;&#26377;&#25928;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#20854;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question &amp; Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective information retrieval, and the impact they have on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#65292;ChatGPT&#34920;&#29616;&#20248;&#20110;Google Bard&#12290;</title><link>https://arxiv.org/abs/2402.05116</link><description>&lt;p&gt;
&#37327;&#21270;&#30456;&#20284;&#24615;&#65306;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#20851;&#32852;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#35780;&#20272;ChatGPT&#21644;Google Bard&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#65292;ChatGPT&#34920;&#29616;&#20248;&#20110;Google Bard&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#20869;&#23481;&#33021;&#21147;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35780;&#20272;&#36890;&#36807;&#25152;&#35859;&#30340;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#26377;&#29992;&#24615;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#30446;&#26631;&#65306;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30340;&#24179;&#22343;&#20540;&#65292;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#20869;&#23481;&#19982;&#31185;&#23398;&#23478;&#20135;&#29983;&#30340;&#30495;&#23454;&#25991;&#29486;&#30340;&#30456;&#20284;&#24615;&#21644;&#25509;&#36817;&#31243;&#24230;&#12290;&#26041;&#27861;&#65306;&#22312;&#36825;&#20010;&#25506;&#32034;&#24615;&#20998;&#26512;&#20013;&#65292;&#65288;1&#65289;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#26469;&#29983;&#25104;ChatGPT&#21644;Google Bard&#30340;&#20020;&#24202;&#20869;&#23481;&#65292;&#20197;&#20415;&#19982;&#25991;&#29486;&#23545;&#24212;&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#65288;2&#65289;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#25152;&#29983;&#25104;&#20869;&#23481;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#23545;&#24212;&#20869;&#23481;&#30340;&#30456;&#20284;&#24615;&#26469;&#35780;&#20272;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#27604;&#36739;&#25991;&#26723;&#21644;&#30456;&#20851;&#30340;&#20108;&#20803;&#32452;&#65292;&#24182;&#20351;&#29992;&#32593;&#32476;&#20998;&#26512;&#26469;&#35780;&#20272;&#26415;&#35821;&#30340;&#20013;&#24515;&#24615;&#12290;&#32467;&#26524;&#65306;&#23454;&#39564;&#34920;&#26126;&#65292;ChatGPT&#22312;&#20313;&#24358;&#25991;&#26723;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;Google Bard&#65288;38%&#23545;34%&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
Background: The emergence of generative AI tools, empowered by Large Language Models (LLMs), has shown powerful capabilities in generating content. To date, the assessment of the usefulness of such content, generated by what is known as prompt engineering, has become an interesting research question. Objectives Using the mean of prompt engineering, we assess the similarity and closeness of such contents to real literature produced by scientists. Methods In this exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to generate clinical content to be compared with literature counterparts, (2) we assess the similarities of the contents generated by comparing them with counterparts from biomedical literature. Our approach is to use text-mining approaches to compare documents and associated bigrams and to use network analysis to assess the terms' centrality. Results The experiments demonstrated that ChatGPT outperformed Google Bard in cosine document similarity (38% to 34%), 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;MOOCs&#35780;&#20998;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As MOOCs Graders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#20026;&#25317;&#26377;&#30005;&#33041;&#21644;&#20114;&#32852;&#32593;&#35775;&#38382;&#26435;&#38480;&#30340;&#20840;&#29699;&#20219;&#20309;&#20154;&#25552;&#20379;&#20813;&#36153;&#25945;&#32946;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20123;&#35838;&#31243;&#30340;&#22823;&#35268;&#27169;&#27880;&#20876;&#24847;&#21619;&#30528;&#19968;&#20301;&#25945;&#24072;&#20960;&#20046;&#19981;&#21487;&#33021;&#35780;&#20272;&#27599;&#20010;&#23398;&#29983;&#30340;&#20889;&#20316;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21516;&#20276;&#35780;&#20998;&#36890;&#24120;&#26159;&#39318;&#36873;&#26041;&#27861;&#65292;&#36890;&#24120;&#30001;&#31616;&#21333;&#26126;&#20102;&#30340;&#35780;&#20998;&#26631;&#20934;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#21516;&#20276;&#35780;&#20998;&#22312;&#21487;&#38752;&#24230;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;18&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26367;&#20195;MOOCs&#20013;&#30340;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65306;GPT-4&#21644;GPT-3.5&#65292;&#24182;&#28085;&#30422;&#19977;&#38376;&#19981;&#21516;&#30340;&#35838;&#31243;&#65306;&#20837;&#38376;&#22825;&#25991;&#23398;&#65292;&#22825;&#20307;&#29983;&#29289;&#23398;&#20197;&#21450;&#22825;&#25991;&#23398;&#30340;&#21382;&#21490;&#19982;&#21746;&#23398;&#12290;&#20026;&#20102;&#35757;&#32451;LLMs&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#38646;-shot&#36830;&#32493;&#24605;&#32771;&#65288;Zero-shot-CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#30340;&#21464;&#31181;&#30340;&#19977;&#20010;&#19981;&#21516;&#25552;&#31034;&#65306;&#32467;&#21512;Zero-shot-CoT&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25351;&#20986;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#26377;&#24847;&#27745;&#26579;&#26102;&#23384;&#22312;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65288;EAL&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#19988;&#36867;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02823</link><description>&lt;p&gt;
&#36867;&#36991;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#65288;&#22826;&#65289;&#23481;&#26131;
&lt;/p&gt;
&lt;p&gt;
Evading Data Contamination Detection for Language Models is (too) Easy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25351;&#20986;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#26377;&#24847;&#27745;&#26579;&#26102;&#23384;&#22312;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65288;EAL&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#19988;&#36867;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#65292;&#23427;&#20204;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#32463;&#24120;&#25351;&#23548;&#29992;&#25143;&#23545;&#19968;&#20010;&#27169;&#22411;&#19982;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25152;&#35757;&#32451;&#30340;&#22823;&#37327;&#25968;&#25454;&#21487;&#33021;&#20250;&#24847;&#22806;&#22320;&#19982;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#21457;&#29983;&#27745;&#26579;&#65292;&#20174;&#32780;&#25439;&#23475;&#24615;&#33021;&#35780;&#20272;&#12290;&#23613;&#31649;&#26368;&#36817;&#24320;&#21457;&#20102;&#19968;&#20123;&#27745;&#26579;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#26377;&#24847;&#36827;&#34892;&#27745;&#26579;&#20197;&#36991;&#20813;&#34987;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#24773;&#20917;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#23545;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#30340;&#21487;&#20449;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#26356;&#20005;&#26684;&#22320;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#27745;&#26579;&#26816;&#27979;&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#36825;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#28431;&#27934;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;EAL&#36825;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65292;&#26126;&#26174;&#25552;&#39640;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#65292;&#24182;&#23436;&#20840;&#36867;&#36991;&#20102;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#23454;&#39564;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.01735</link><description>&lt;p&gt;
VIALM&#65306;&#20851;&#20110;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#30340;&#35843;&#26597;&#21644;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#23454;&#39564;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#35270;&#35273;&#38556;&#30861;&#36741;&#21161;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38556;&#30861;&#36741;&#21161; (VIA) &#26088;&#22312;&#33258;&#21160;&#24110;&#21161;&#35270;&#35273;&#38556;&#30861;&#32773; (VI) &#22788;&#29702;&#26085;&#24120;&#27963;&#21160;&#12290;VIA &#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#30340;&#21457;&#23637;&#65292;&#20108;&#32773;&#37117;&#23637;&#31034;&#20102;&#21033;&#29992;&#22823;&#22411;&#27169;&#22411; (LMs) &#30340;&#21069;&#27839;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;LMs &#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#21487;&#20197;&#24212;&#23545;&#35832;&#22914;&#20855;&#36523;&#26426;&#22120;&#20154;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29289;&#29702;&#20219;&#21153;&#12290;&#20026;&#20102;&#30740;&#31350;&#26368;&#20808;&#36827; (SOTA) LMs &#22312;VIA&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#38024;&#23545;&#20855;&#26377;LMs&#30340;VIA&#20219;&#21153;&#65288;VIALM&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#35828;&#26126;&#29289;&#29702;&#29615;&#22659;&#30340;&#22270;&#20687;&#21644;&#35270;&#35273;&#38556;&#30861;&#32773;&#29992;&#25143;&#30340;&#35821;&#35328;&#35831;&#27714;&#65292;VIALM&#26088;&#22312;&#36755;&#20986;&#36880;&#27493;&#24341;&#23548;&#65292;&#20197;&#22312;&#29615;&#22659;&#20013;&#24110;&#21161;&#35270;&#35273;&#38556;&#30861;&#29992;&#25143;&#23436;&#25104;&#35831;&#27714;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#23545;&#36817;&#26399;LM&#30740;&#31350;&#30340;&#35843;&#26597;&#21644;&#23545;&#36873;&#23450;LMs&#33021;&#21147;&#30340;&#22522;&#20934;&#23454;&#39564;&#30340;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually Impaired Assistance (VIA) aims to automatically help visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (\textbf{VIALM}). In this task, given an \textit{image} illustrating the physical environments and a \textit{linguistic request} from a VI user, VIALM aims to output step-by-step \textit{guidance} to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01713</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#20351;&#20854;&#19982;&#20256;&#32479;&#19978;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25972;&#21512;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#21463;&#26032;&#30142;&#30149;&#29190;&#21457;&#26102;&#36805;&#36895;&#20915;&#31574;&#30340;&#32039;&#36843;&#38656;&#27714;&#30340;&#39537;&#20351;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;GPT-4&#30340;LLM&#23545;EHR&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#21363;&#22312;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#38024;&#23545;EHR&#25968;&#25454;&#30340;&#32437;&#21521;&#12289;&#31232;&#30095;&#21644;&#30693;&#35782;&#27880;&#20837;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#30340;&#25552;&#31034;&#26041;&#27861;&#32771;&#34385;&#20102;&#29305;&#23450;&#30340;EHR&#29305;&#24449;&#65292;&#22914;&#21333;&#20301;&#21644;&#21442;&#32771;&#33539;&#22260;&#65292;&#24182;&#37319;&#29992;&#20102;&#19982;&#20020;&#24202;&#19978;&#19979;&#25991;&#30456;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LLM&#33021;&#22815;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;EHR&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20581;&#24247;-LLM&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#26816;&#32034;&#22686;&#24378;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20581;&#24247;&#25253;&#21578;&#65292;&#35843;&#25972;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#21450;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#23478;&#35265;&#35299;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.00746</link><description>&lt;p&gt;
&#20581;&#24247;-LLM&#65306;&#20010;&#24615;&#21270;&#26816;&#32034;&#22686;&#24378;&#30340;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20581;&#24247;-LLM&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#26816;&#32034;&#22686;&#24378;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20581;&#24247;&#25253;&#21578;&#65292;&#35843;&#25972;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#21450;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#23478;&#35265;&#35299;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21355;&#29983;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26234;&#33021;&#21307;&#30103;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26234;&#33021;&#21307;&#30103;&#21463;&#38480;&#20110;&#38745;&#24577;&#25968;&#25454;&#21644;&#32479;&#19968;&#26631;&#20934;&#65292;&#26080;&#27861;&#23436;&#20840;&#19982;&#20010;&#20307;&#24773;&#20917;&#38598;&#25104;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#20854;&#20182;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#20581;&#24247;-LLM&#65292;&#23558;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#30456;&#32467;&#21512;&#12290;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20581;&#24247;&#25253;&#21578;&#25972;&#21512;&#21040;&#22823;&#27169;&#22411;&#20013;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#20219;&#21153;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#19994;&#30340;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#35843;&#25972;&#20581;&#24247;&#29305;&#24449;&#30340;&#26435;&#37325;&#24471;&#20998;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#21322;&#33258;&#21160;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#24182;&#25972;&#21512;&#19987;&#23478;&#35265;&#35299;&#20197;&#25552;&#39640;&#30142;&#30149;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease predic
&lt;/p&gt;</description></item><item><title>ProLex&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#29087;&#32451;&#24230;&#20026;&#23548;&#21521;&#30340;&#35789;&#27719;&#26367;&#25442;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#29983;&#25104;&#36866;&#24403;&#26367;&#20195;&#35789;&#21644;&#34920;&#29616;&#26356;&#22909;&#35821;&#35328;&#29087;&#32451;&#24230;&#30340;&#31995;&#32479;&#33021;&#21147;&#12290;&#20351;&#29992;&#24494;&#35843;&#20219;&#21153;&#29305;&#23450;&#21512;&#25104;&#25968;&#25454;&#30340;Llama2-13B&#27169;&#22411;&#22312;F&#20998;&#25968;&#19978;&#20248;&#20110;ChatGPT 3.2%&#65292;&#19982;GPT-4&#22312;ProLex&#19978;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2401.11356</link><description>&lt;p&gt;
ProLex: &#19968;&#31181;&#20197;&#35821;&#35328;&#29087;&#32451;&#24230;&#20026;&#23548;&#21521;&#30340;&#35789;&#27719;&#26367;&#25442;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ProLex: A Benchmark for Language Proficiency-oriented Lexical Substitution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11356
&lt;/p&gt;
&lt;p&gt;
ProLex&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#29087;&#32451;&#24230;&#20026;&#23548;&#21521;&#30340;&#35789;&#27719;&#26367;&#25442;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#29983;&#25104;&#36866;&#24403;&#26367;&#20195;&#35789;&#21644;&#34920;&#29616;&#26356;&#22909;&#35821;&#35328;&#29087;&#32451;&#24230;&#30340;&#31995;&#32479;&#33021;&#21147;&#12290;&#20351;&#29992;&#24494;&#35843;&#20219;&#21153;&#29305;&#23450;&#21512;&#25104;&#25968;&#25454;&#30340;Llama2-13B&#27169;&#22411;&#22312;F&#20998;&#25968;&#19978;&#20248;&#20110;ChatGPT 3.2%&#65292;&#19982;GPT-4&#22312;ProLex&#19978;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#26367;&#25442;&#26159;&#22312;&#19978;&#19979;&#25991;&#21477;&#23376;&#20013;&#20026;&#32473;&#23450;&#30340;&#30446;&#26631;&#35789;&#25214;&#21040;&#21512;&#36866;&#30340;&#26367;&#20195;&#35789;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#27809;&#26377;&#32771;&#34385;&#21040;&#19982;&#30446;&#26631;&#35789;&#21516;&#31561;&#25110;&#26356;&#39640;&#29087;&#32451;&#24230;&#30340;&#26367;&#20195;&#35789;&#65292;&#36825;&#23545;&#20110;&#24076;&#26395;&#25552;&#39640;&#20889;&#20316;&#27700;&#24179;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#26469;&#35828;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#20197;&#35821;&#35328;&#29087;&#32451;&#24230;&#20026;&#23548;&#21521;&#30340;&#35789;&#27719;&#26367;&#25442;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;ProLex&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#29983;&#25104;&#19981;&#20165;&#21512;&#36866;&#30340;&#26367;&#20195;&#35789;&#36824;&#35201;&#34920;&#29616;&#20986;&#26356;&#22909;&#35821;&#35328;&#29087;&#32451;&#24230;&#30340;&#33021;&#21147;&#12290;&#38500;&#20102;&#22522;&#20934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#33258;&#21160;&#25191;&#34892;&#36825;&#20010;&#26032;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#21512;&#25104;&#25968;&#25454;&#24494;&#35843;&#30340;Llama2-13B&#27169;&#22411;&#65292;&#22312;F&#20998;&#25968;&#19978;&#24179;&#22343;&#20248;&#20110;ChatGPT 3.2&#65285;&#65292;&#24182;&#22312;ProLex&#19978;&#19982;GPT-4&#21462;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexical Substitution discovers appropriate substitutes for a given target word in a context sentence. However, the task fails to consider substitutes that are of equal or higher proficiency than the target, an aspect that could be beneficial for language learners looking to improve their writing. To bridge this gap, we propose a new task, language proficiency-oriented lexical substitution. We also introduce ProLex, a novel benchmark designed to assess systems' ability to generate not only appropriate substitutes but also substitutes that demonstrate better language proficiency. Besides the benchmark, we propose models that can automatically perform the new task. We show that our best model, a Llama2-13B model fine-tuned with task-specific synthetic data, outperforms ChatGPT by an average of 3.2% in F-score and achieves comparable results with GPT-4 on ProLex.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#38646;&#26679;&#26412;&#25552;&#31034;&#26469;&#33258;&#21160;&#26500;&#24314;&#21644;&#25193;&#23637;&#20027;&#39064;&#20998;&#31867;&#27861;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#24212;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#65292;&#32467;&#21512;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;LLMs&#65292;&#22312;&#38646;&#21806;&#38134;&#34892;&#25968;&#25454;&#38598;&#20013;&#20026;&#21830;&#23478;&#20998;&#37197;&#26631;&#31614;&#65292;&#20855;&#26377;&#36229;&#36807;90%&#30340;&#19968;&#33268;&#24615;&#29575;&#21644;&#20196;&#20154;&#20852;&#22859;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.06790</link><description>&lt;p&gt;
&#22312;&#26631;&#35760;&#38646;&#21806;&#38134;&#34892;&#20132;&#26131;&#20013;&#65292;&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#26469;&#33258;&#21160;&#21019;&#24314;&#21644;&#25193;&#23637;&#20027;&#39064;&#20998;&#31867;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging Retail Banking Transactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06790
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#38646;&#26679;&#26412;&#25552;&#31034;&#26469;&#33258;&#21160;&#26500;&#24314;&#21644;&#25193;&#23637;&#20027;&#39064;&#20998;&#31867;&#27861;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#24212;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#65292;&#32467;&#21512;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;LLMs&#65292;&#22312;&#38646;&#21806;&#38134;&#34892;&#25968;&#25454;&#38598;&#20013;&#20026;&#21830;&#23478;&#20998;&#37197;&#26631;&#31614;&#65292;&#20855;&#26377;&#36229;&#36807;90%&#30340;&#19968;&#33268;&#24615;&#29575;&#21644;&#20196;&#20154;&#20852;&#22859;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;LLMs&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#33258;&#21160;&#26500;&#24314;&#21644;&#25193;&#23637;&#20027;&#39064;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#24212;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#21019;&#24314;&#21021;&#22987;&#20027;&#39064;&#20998;&#31867;&#27861;&#65292;&#21033;&#29992;LLMs&#23545;&#32467;&#26524;&#26415;&#35821;&#36827;&#34892;&#21518;&#22788;&#29702;&#24182;&#21019;&#24314;&#23618;&#27425;&#32467;&#26500;&#12290;&#20026;&#20102;&#20351;&#29992;&#26032;&#26415;&#35821;&#25193;&#23637;&#29616;&#26377;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#26469;&#30830;&#23450;&#22312;&#20309;&#22788;&#28155;&#21152;&#26032;&#33410;&#28857;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20998;&#31867;&#27861;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#24471;&#21040;&#30340;&#20998;&#31867;&#27861;&#20026;&#38646;&#21806;&#38134;&#34892;&#25968;&#25454;&#38598;&#20013;&#30340;&#21830;&#23478;&#20998;&#37197;&#26631;&#31614;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#35831;12&#21517;&#24535;&#24895;&#32773;&#22238;&#31572;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#30340;&#34920;&#26684;&#65292;&#25105;&#20204;&#39318;&#20808;&#35780;&#20272;&#20102;&#25152;&#21019;&#24314;&#20998;&#31867;&#27861;&#30340;&#36136;&#37327;&#65292;&#28982;&#21518;&#35780;&#20272;&#20102;&#22522;&#20110;&#35813;&#20998;&#31867;&#27861;&#20998;&#37197;&#32473;&#21830;&#23478;&#30340;&#26631;&#31614;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#25152;&#36873;&#20998;&#31867;&#27861;&#30340;&#19968;&#33268;&#24615;&#29575;&#36229;&#36807;90%&#12290;&#20351;&#29992;LLMs&#25193;&#23637;&#20998;&#31867;&#27861;&#20063;&#26174;&#31034;&#20986;&#20196;&#20154;&#20852;&#22859;&#30340;&#32467;&#26524;&#65292;&#29238;&#33410;&#28857;&#30340;&#20248;&#20808;&#32423;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an unsupervised method for automatically constructing and expanding topic taxonomies using instruction-based fine-tuned LLMs (Large Language Models). We apply topic modeling and keyword extraction techniques to create initial topic taxonomies and LLMs to post-process the resulting terms and create a hierarchy. To expand an existing taxonomy with new terms, we use zero-shot prompting to find out where to add new nodes, which, to our knowledge, is the first work to present such an approach to taxonomy tasks. We use the resulting taxonomies to assign tags that characterize merchants from a retail bank dataset. To evaluate our work, we asked 12 volunteers to answer a two-part form in which we first assessed the quality of the taxonomies created and then the tags assigned to merchants based on that taxonomy. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies. The taxonomies' expansion with LLMs also showed exciting results for parent node pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2312.04455</link><description>&lt;p&gt;
&#24378;&#21270;&#20851;&#27880;&#21147;&#20013;&#26368;&#30701;&#30340;&#25903;&#26609;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20851;&#27880;&#20998;&#37197;&#20013;&#30340;&#20869;&#22312;&#27874;&#24418;&#27169;&#24335;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21033;&#29992;LLMs&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20851;&#38190;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#20013;&#20301;&#20110;&#20851;&#27880;&#27874;&#24418;&#30340;&#20302;&#35895;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#24573;&#35270;&#35813;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;LLMs&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#22788;&#29702;&#36755;&#20837;&#12290;&#27599;&#20010;&#36807;&#31243;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#35282;&#24230;&#36827;&#34892;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65292;&#20174;&#32780;&#21019;&#24314;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#20851;&#27880;&#27874;&#24418;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#20302;&#35895;&#34917;&#20607;&#21478;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#39640;&#23792;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20154;&#31867;&#27807;&#36890;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;&#28040;&#36153;&#32773;&#37329;&#34701;&#25237;&#35785;&#25968;&#25454;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#21487;&#33021;&#22686;&#24378;&#20102;&#19968;&#25972;&#22871;&#35821;&#35328;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#20449;&#24687;&#35828;&#26381;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.16466</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35821;&#35328;&#29305;&#24449;&#23545;&#40784;&#21487;&#20197;&#22686;&#24378;&#35828;&#26381;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large language models can enhance persuasion through linguistic feature alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20154;&#31867;&#27807;&#36890;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;&#28040;&#36153;&#32773;&#37329;&#34701;&#25237;&#35785;&#25968;&#25454;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#21487;&#33021;&#22686;&#24378;&#20102;&#19968;&#25972;&#22871;&#35821;&#35328;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#20449;&#24687;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs)&#27491;&#22312;&#37325;&#26032;&#22609;&#36896;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20294;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#24433;&#21709;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#20123;&#21463;&#38480;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#20154;&#31867;&#27807;&#36890;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;&#28040;&#36153;&#32773;&#37329;&#34701;&#25237;&#35785;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#28040;&#36153;&#32773;&#37329;&#34701;&#20445;&#25252;&#23616; (CFPB) &#25910;&#38598;&#30340;&#36229;&#36807;820,000&#20010;&#25237;&#35785;&#36827;&#34892;AI&#26816;&#27979;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;ChatGPT&#21457;&#24067;&#21518;&#19981;&#20037;&#65292;LLMs&#30340;&#20351;&#29992;&#21487;&#33021;&#24615;&#24613;&#21095;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#20351;&#29992;&#21487;&#33021;&#24615;&#19982;&#20449;&#24687;&#35828;&#26381;&#21147;&#65288;&#21363;&#20174;&#37329;&#34701;&#20844;&#21496;&#33719;&#24471;&#25937;&#27982;&#30340;&#21487;&#33021;&#24615;&#22686;&#21152;&#65289;&#21576;&#27491;&#30456;&#20851;&#12290;&#35745;&#31639;&#35821;&#35328;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#27491;&#30456;&#20851;&#21487;&#33021;&#26159;&#30001;LLMs&#22686;&#24378;&#20102;&#21508;&#31181;&#35821;&#35328;&#29305;&#24449;&#25152;&#35299;&#37322;&#30340;&#12290;&#26681;&#25454;&#36825;&#20123;&#35266;&#23519;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#20551;&#35774;LLMs&#30340;&#20351;&#29992;&#21487;&#33021;&#22686;&#24378;&#20102;&#19968;&#25972;&#22871;&#35821;&#35328;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#23545;&#20855;&#26377;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#25509;&#25910;&#32773;&#30340;&#20449;&#24687;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) are reshaping various aspects of human life, our current understanding of their impacts remains somewhat constrained. Here we investigate the impact of LLMs on human communication, using data on consumer complaints in the financial industry. By employing an AI detection tool on more than 820K complaints gathered by the Consumer Financial Protection Bureau (CFPB), we find a sharp increase in the likely use of LLMs shortly after the release of ChatGPT. Moreover, the likely LLM usage was positively correlated with message persuasiveness (i.e., increased likelihood of obtaining relief from financial firms). Computational linguistic analyses suggest that the positive correlation may be explained by LLMs' enhancement of various linguistic features. Based on the results of these observational studies, we hypothesize that LLM usage may enhance a comprehensive set of linguistic features, increasing message persuasiveness to receivers with heterogeneous ling
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#36827;&#34892;&#21457;&#23637;&#24615;&#35821;&#35328;&#38556;&#30861;&#65288;DLD&#65289;&#26816;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#24212;&#29992;&#20110;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#20799;&#31461;DLD&#20154;&#32676;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.15054</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#26816;&#27979;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#20799;&#31461;&#30340;&#21457;&#23637;&#24615;&#35821;&#35328;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Detection of developmental language disorder in Cypriot Greek children using a neural network algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15054
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#36827;&#34892;&#21457;&#23637;&#24615;&#35821;&#35328;&#38556;&#30861;&#65288;DLD&#65289;&#26816;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#24212;&#29992;&#20110;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#20799;&#31461;DLD&#20154;&#32676;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24615;&#35821;&#35328;&#38556;&#30861;&#65288;DLD&#65289;&#30340;&#20799;&#31461;&#22312;&#21560;&#25910;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#26089;&#26399;&#35782;&#21035;&#21644;&#24178;&#39044;&#23545;&#20110;&#38450;&#27490;&#23545;&#20799;&#31461;&#30340;&#23398;&#26415;&#12289;&#31038;&#20132;&#21644;&#24773;&#24863;&#21457;&#23637;&#20135;&#29983;&#38271;&#26399;&#36127;&#38754;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#12289;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#33258;&#21160;&#21270;&#26816;&#27979;DLD&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#26696;&#39318;&#27425;&#22312;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#20799;&#31461;DLD&#20154;&#32676;&#20013;&#24212;&#29992;&#12290;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20351;&#29992;&#20174;15&#21517;DLD&#24739;&#20799;&#21644;15&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#20013;&#25910;&#38598;&#30340;&#24863;&#30693;&#21644;&#20135;&#20986;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24180;&#40836;&#33539;&#22260;&#20026;7&#23681;10&#20010;&#26376;&#33267;10&#23681;4&#20010;&#26376;&#12290;&#37319;&#29992;k-fold&#25216;&#26415;&#23545;&#31639;&#27861;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#12290;&#20351;&#29992;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;ROC/AUC&#26354;&#32447;&#31561;&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#19968;&#32452;&#26410;&#30693;&#25968;&#25454;&#19978;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#39640;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children with developmental language disorder (DLD) encounter difficulties in acquiring various language structures. Early identification and intervention are crucial to prevent negative long-term outcomes impacting the academic, social, and emotional development of children. The study aims to develop an automated method for the identification of DLD using artificial intelligence, specifically a neural network machine learning algorithm. This protocol is applied for the first time in a Cypriot Greek child population with DLD. The neural network model was trained using perceptual and production data elicited from 15 children with DLD and 15 healthy controls in the age range of 7;10 until 10;4. The k-fold technique was used to crossvalidate the algorithm. The performance of the model was evaluated using metrics such as accuracy, precision, recall, F1 score, and ROC/AUC curve to assess its ability to make accurate predictions on a set of unseen data. The results demonstrated high classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2311.14455</link><description>&lt;p&gt;
&#20174;&#34987;&#27602;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#26500;&#24314;&#30340;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Universal Jailbreak Backdoors from Poisoned Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#26377;&#29992;&#19988;&#26080;&#23475;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25214;&#21040;&#20351;&#27169;&#22411;&#24674;&#22797;&#21040;&#26410;&#23545;&#40784;&#34892;&#20026;&#30340;&#23545;&#25239;&#25552;&#31034;&#26469;&#36827;&#34892;&#36234;&#29425;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;RLHF&#35757;&#32451;&#25968;&#25454;&#23558;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#23884;&#20837;&#27169;&#22411;&#20013;&#12290;&#35813;&#21518;&#38376;&#23558;&#19968;&#20010;&#35302;&#21457;&#35789;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#8220;sudo&#21629;&#20196;&#8221;&#65306;&#22312;&#20219;&#20309;&#25552;&#31034;&#20013;&#28155;&#21152;&#35302;&#21457;&#35789;&#23558;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#65292;&#26080;&#38656;&#25628;&#32034;&#23545;&#25239;&#25552;&#31034;&#12290;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20808;&#21069;&#30740;&#31350;&#30340;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#26356;&#24378;&#22823;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#24120;&#35265;&#30340;&#21518;&#38376;&#25915;&#20987;&#25216;&#26415;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;RLHF&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#23545;&#20854;&#25152;&#22768;&#31216;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#24067;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26530;&#32445;&#35821;&#35328;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2311.08711</link><description>&lt;p&gt;
PLUG: &#36328;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#26530;&#32445;&#35821;&#35328;&#20248;&#21183;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08711
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26530;&#32445;&#35821;&#35328;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#22238;&#24212;&#21508;&#31181;&#20154;&#31867;&#25351;&#20196;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;LLMs&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#30340;&#22522;&#30784;&#33021;&#21147;&#19981;&#24179;&#34913;&#65292;&#36825;&#28304;&#20110;&#22312;&#23427;&#20204;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#35821;&#35328;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26530;&#32445;&#35821;&#35328;&#24341;&#23548;&#29983;&#25104;&#65288;PLUG&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#20027;&#35201;&#26159;&#33521;&#35821;&#65289;&#20316;&#20026;&#26530;&#32445;&#35821;&#35328;&#65292;&#22686;&#24378;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#35843;&#20248;&#12290;&#23427;&#35757;&#32451;&#27169;&#22411;&#39318;&#20808;&#22788;&#29702;&#26530;&#32445;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#65292;&#28982;&#21518;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#29983;&#25104;&#21709;&#24212;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;X-AlpacaEval&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;4&#31181;&#35821;&#35328;&#65288;&#20013;&#25991;&#12289;&#38889;&#25991;&#12289;&#24847;&#22823;&#21033;&#25991;&#21644;&#35199;&#29677;&#29273;&#25991;&#65289;&#30340;&#25351;&#20196;&#65292;&#24182;&#30001;&#19987;&#19994;&#32763;&#35793;&#20154;&#21592;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25351;&#20196;&#36319;&#38543;&#20013;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.07466</link><description>&lt;p&gt;
&#20851;&#20110;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Measuring Faithfulness or Self-consistency of Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20107;&#21518;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;LLM&#21487;&#33021;&#20250;&#32534;&#36896;&#21548;&#36215;&#26469;&#21512;&#29702;&#20294;&#19981;&#24544;&#23454;&#20110;&#20854;&#22522;&#26412;&#25512;&#29702;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#26088;&#22312;&#21028;&#26029;&#20107;&#21518;&#25110;CoT&#35299;&#37322;&#24544;&#23454;&#24230;&#30340;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24544;&#23454;&#24230;&#27979;&#35797;&#19981;&#26159;&#34913;&#37327;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30340;&#24544;&#23454;&#24230;&#65292;&#32780;&#26159;&#34913;&#37327;&#20854;&#36755;&#20986;&#32423;&#21035;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#25105;&#20204;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32972;&#26223;&#19979;&#28548;&#28165;&#20102;&#24544;&#23454;&#24230;&#27979;&#35797;&#30340;&#22320;&#20301;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;ii&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#24211;&#65292;&#39318;&#27425;&#22312;11&#20010;&#24320;&#25918;&#24335;LLMs&#21644;5&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#22871;&#20214;&#19978;&#27604;&#36739;&#20102;&#29616;&#26377;&#27979;&#35797;&#65292;&#21253;&#25324;iii&#65289;&#25105;&#20204;&#30340;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#12290;CC-SHAP&#26159;LLM&#33258;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#24230;&#37327;&#65288;&#32780;&#19981;&#26159;&#27979;&#35797;&#65289;&#12290;&#23427;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.06233</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;: &#19968;&#31181;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27745;&#26579;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06233
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#24182;&#20272;&#35745;&#20854;&#25968;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#35270;&#20026;&#19968;&#31995;&#21015;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#27979;&#39564;&#24418;&#24335;&#65292;&#20854;&#20013;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#19977;&#20010;&#25200;&#21160;&#29256;&#26412;&#12290;&#36825;&#20123;&#21464;&#21270;&#20165;&#21253;&#25324;&#35789;&#32423;&#25200;&#21160;&#12290;&#29983;&#25104;&#30340;&#25200;&#21160;&#29256;&#26412;&#19982;&#21407;&#22987;&#23454;&#20363;&#19968;&#36215;&#24418;&#25104;DCQ&#20013;&#30340;&#36873;&#39033;&#65292;&#39069;&#22806;&#30340;&#36873;&#39033;&#36866;&#24212;&#20102;&#25552;&#20379;&#30340;&#36873;&#25321;&#37117;&#19981;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#37492;&#20110;&#22312;&#36873;&#25321;&#20043;&#38388;&#21807;&#19968;&#30340;&#21306;&#21035;&#20449;&#21495;&#26159;&#19982;&#21407;&#22987;&#23454;&#20363;&#30340;&#30830;&#20999;&#25514;&#36766;&#30456;&#20851;&#65292;&#22914;&#26524;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24050;&#32463;&#25509;&#35302;&#21040;&#21407;&#22987;&#23454;&#20363;&#65292;&#35821;&#35328;&#27169;&#22411;&#24403;&#34987;&#35201;&#27714;&#20174;&#36873;&#39033;&#20013;&#35782;&#21035;&#21407;&#22987;&#23454;&#20363;&#26102;&#65292;&#20542;&#21521;&#20110;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;--&#36825;&#26159;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#30340;&#29305;&#24615;&#12290;&#22312;&#20351;&#29992;GPT-4/3.5&#36827;&#34892;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23436;&#20840;&#32570;&#23569;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;PeTailor&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.18463</link><description>&lt;p&gt;
PeTailor&#65306;&#36890;&#36807;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18463
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PeTailor&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#31995;&#32479;&#26088;&#22312;&#33258;&#21160;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#29702;&#35299;&#22797;&#26434;&#29983;&#29289;&#21307;&#23398;&#21477;&#23376;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#31283;&#20581;&#30340;&#19977;&#20803;&#32452;&#25552;&#21462;&#31995;&#32479;&#30340;&#24320;&#21457;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;PeTailor&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#21046;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#25105;&#20204;&#39044;&#20808;&#26500;&#24314;&#30340;&#22810;&#26679;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26174;&#24335;&#22320;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20026;&#36755;&#20837;&#30340;&#21477;&#23376;&#29983;&#25104;&#30456;&#24212;&#30340;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#65292;&#20851;&#31995;&#65292;&#23614;&#23454;&#20307;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;GM-CIHT&#65292;&#19968;&#31181;&#19987;&#23478;&#26631;&#27880;&#30340;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical triple extraction systems aim to automatically extract biomedical entities and relations between entities. While current unified information extraction models showcase state-of-the-art performance, they face challenges in understanding relationships between entities within intricate biomedical sentences. Furthermore, the absence of a high-quality biomedical triple extraction dataset impedes the progress in developing robust triple extraction systems. To tackle these challenges, we propose a novel retrieval-based framework for biomedical triple extraction, namely PeTailor, which explicitly retrieves the relevant document from our pre-built diverse chunk database using a novel tailored chunk scorer and integrates the retrieved information into the input of a Large Language Model (LLM) to generate the corresponding triple (head entity, relation, tail entity) for the input sentence. Additionally, we present GM-CIHT, an expert-annotated biomedical triple extraction dataset that c
&lt;/p&gt;</description></item><item><title>SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.18376</link><description>&lt;p&gt;
SQLformer&#65306;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18376
&lt;/p&gt;
&lt;p&gt;
SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#36825;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#36825;&#39033;&#25216;&#26415;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20351;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#25552;&#21462;&#27665;&#20027;&#21270;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#20027;&#35201;&#38556;&#30861;&#21253;&#25324;&#39046;&#22495;&#27867;&#21270;&#65292;&#21363;&#36866;&#24212;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25968;&#25454;&#24211;&#65292;&#24182;&#19988;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19982;&#30456;&#24212;&#30340;SQL&#26597;&#35810;&#23545;&#40784;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SQLformer&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#25191;&#34892;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;Transformer&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#39044;&#27979;SQL&#26597;&#35810;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23618;&#20013;&#32467;&#21512;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#26159;&#30001;&#25968;&#25454;&#24211;&#34920;&#21644;&#21015;&#36873;&#25321;&#24341;&#23548;&#30340;&#65292;&#26377;&#21161;&#20110;&#35299;&#30721;&#22120;&#20197;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#35268;&#33539;&#39034;&#24207;&#29983;&#25104;SQL&#26597;&#35810;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35828;&#26126;&#20102;&#29616;&#38454;&#27573;&#30340;&#25216;&#26415;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been growing interest in text-to-SQL translation, which is the task of converting natural language questions into executable SQL queries. This technology is important for its potential to democratize data extraction from databases. However, some of its key hurdles include domain generalisation, which is the ability to adapt to previously unseen databases, and alignment of natural language questions with the corresponding SQL queries. To overcome these challenges, we introduce SQLformer, a novel Transformer architecture specifically crafted to perform text-to-SQL translation tasks. Our model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive way, incorporating structural inductive bias in the encoder and decoder layers. This bias, guided by database table and column selection, aids the decoder in generating SQL query ASTs represented as graphs in a Breadth-First Search canonical order. Comprehensive experiments illustrate the state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#25972;&#21644;&#33258;&#36866;&#24212;&#20248;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2310.15080</link><description>&lt;p&gt;
&#24102;&#26377;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#25972;&#21644;&#33258;&#36866;&#24212;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#25972;&#21644;&#33258;&#36866;&#24212;&#20248;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#21327;&#21516;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#28041;&#21450;&#26356;&#26032;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#36825;&#38480;&#21046;&#20102;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#22788;&#29702;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;prompt&#35843;&#25972;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#26356;&#26032;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#20294;&#23427;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#25110;&#38477;&#20302;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30452;&#25509;&#20351;&#29992;prompt&#35843;&#25972;&#36890;&#24120;&#20250;&#23548;&#33268;&#38750;&#24179;&#20961;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#20998;&#25955;&#25968;&#25454;&#36890;&#24120;&#26159;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#65292;&#24182;&#24102;&#26469;&#23458;&#25143;&#31471;&#28418;&#31227;&#38382;&#39064;&#21644;&#22240;&#27492;&#30340;&#20302;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#21363;FedPepTAO&#65292;&#20197;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37096;&#20998;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#26469;&#25913;&#21892;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22768;&#26126;&#26631;&#20934;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;CACN&#27169;&#22411;&#21033;&#29992;&#24605;&#32500;&#38142;&#21644;&#22768;&#26126;&#26816;&#26597;&#26469;&#20174;&#22797;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#31616;&#21270;&#30340;&#22768;&#26126;&#65292;&#20197;&#21152;&#24378;&#20107;&#23454;&#26680;&#26597;&#12290;</title><link>https://arxiv.org/abs/2310.14338</link><description>&lt;p&gt;
&#20174;&#28151;&#20081;&#21040;&#28165;&#26224;&#65306;&#22768;&#26126;&#26631;&#20934;&#21270;&#20197;&#22686;&#24378;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Chaos to Clarity: Claim Normalization to Empower Fact-Checking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22768;&#26126;&#26631;&#20934;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;CACN&#27169;&#22411;&#21033;&#29992;&#24605;&#32500;&#38142;&#21644;&#22768;&#26126;&#26816;&#26597;&#26469;&#20174;&#22797;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#31616;&#21270;&#30340;&#22768;&#26126;&#65292;&#20197;&#21152;&#24378;&#20107;&#23454;&#26680;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#20852;&#36215;&#65292;&#29992;&#25143;&#25509;&#35302;&#21040;&#35768;&#22810;&#35823;&#23548;&#24615;&#30340;&#22768;&#26126;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24086;&#23376;&#20013;&#22266;&#26377;&#30340;&#28151;&#26434;&#22122;&#22768;&#20351;&#24471;&#36776;&#21035;&#38656;&#35201;&#39564;&#35777;&#30340;&#31934;&#30830;&#19988;&#26174;&#33879;&#30340;&#22768;&#26126;&#21464;&#24471;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20174;&#36825;&#20123;&#24086;&#23376;&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#22768;&#26126;&#26159;&#36153;&#26102;&#19988;&#22256;&#38590;&#30340;&#65292;&#28982;&#32780;&#36825;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#22768;&#26126;&#26631;&#20934;&#21270;&#65288;ClaimNorm&#65289;&#65292;&#26088;&#22312;&#23558;&#22797;&#26434;&#32780;&#22024;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20998;&#35299;&#20026;&#26356;&#30452;&#25509;&#21644;&#26131;&#20110;&#29702;&#35299;&#30340;&#24418;&#24335;&#65292;&#31216;&#20026;&#26631;&#20934;&#21270;&#22768;&#26126;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CACN&#65292;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#21644;&#22768;&#26126;&#20540;&#24471;&#26816;&#26597;&#30340;&#20272;&#35745;&#26469;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#65292;&#20197;&#29702;&#35299;&#22797;&#26434;&#30340;&#22768;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#25552;&#20379;&#25351;&#23548;&#24182;&#25913;&#36827;&#22768;&#26126;&#26631;&#20934;&#21270;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#31934;&#24515;&#32534;&#21046;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of social media, users are exposed to many misleading claims. However, the pervasive noise inherent in these posts presents a challenge in identifying precise and prominent claims that require verification. Extracting the important claims from such posts is arduous and time-consuming, yet it is an underexplored problem. Here, we aim to bridge this gap. We introduce a novel task, Claim Normalization (aka ClaimNorm), which aims to decompose complex and noisy social media posts into more straightforward and understandable forms, termed normalized claims. We propose CACN, a pioneering approach that leverages chain-of-thought and claim check-worthiness estimation, mimicking human reasoning processes, to comprehend intricate claims. Moreover, we capitalize on the in-context learning capabilities of large language models to provide guidance and to improve claim normalization. To evaluate the effectiveness of our proposed model, we meticulously compile a comprehensive real-world 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#24335;&#29702;&#35299;&#26041;&#27861;&#65288;ICSU&#65289;&#65292;&#36890;&#36807;&#25552;&#20379;&#27169;&#24335;&#30456;&#20851;&#30340;&#27880;&#37322;&#31034;&#20363;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30452;&#25509;&#29702;&#35299;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...</title><link>https://arxiv.org/abs/2310.14174</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#20013;&#30340;&#27169;&#24335;&#29702;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An In-Context Schema Understanding Method for Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#24335;&#29702;&#35299;&#26041;&#27861;&#65288;ICSU&#65289;&#65292;&#36890;&#36807;&#25552;&#20379;&#27169;&#24335;&#30456;&#20851;&#30340;&#27880;&#37322;&#31034;&#20363;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30452;&#25509;&#29702;&#35299;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#20219;&#21153;&#26088;&#22312;&#22522;&#20110;&#32473;&#23450;&#30340;&#30693;&#35782;&#24211;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#29992;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;LLM&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#20811;&#26381;&#30693;&#35782;&#24211;&#27169;&#24335;&#30340;&#24222;&#22823;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#26368;&#21021;&#20351;&#29992;LLM&#29983;&#25104;&#27809;&#26377;&#27169;&#24335;&#29305;&#23450;&#32454;&#33410;&#30340;&#36923;&#36753;&#24418;&#24335;&#33609;&#31295;&#26469;&#32469;&#36807;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#39069;&#22806;&#30340;&#27169;&#22359;&#23558;&#27169;&#24335;&#20449;&#24687;&#27880;&#20837;&#21040;&#36825;&#20123;&#33609;&#31295;&#20013;&#12290;&#30456;&#21453;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#24335;&#29702;&#35299;&#65288;ICSU&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;LLM&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#27169;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ICSU&#21033;&#29992;&#19982;&#27169;&#24335;&#30456;&#20851;&#30340;&#27880;&#37322;&#31034;&#20363;&#21521;LLM&#25552;&#20379;&#27169;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#21407;&#22987;&#38382;&#39064;&#12289;&#21311;&#21517;&#38382;&#39064;&#21644;&#29983;&#25104;&#30340;SPARQL&#26597;&#35810;&#30340;&#19977;&#31181;&#31034;&#20363;&#26816;&#32034;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
The Knowledge Base Question Answering (KBQA) task aims to answer natural language questions based on a given knowledge base. Recently, Large Language Models (LLMs) have shown strong capabilities in language understanding and can be used to solve this task. In doing so, a major challenge for LLMs is to overcome the immensity and heterogeneity of knowledge base schemas.Existing methods bypass this challenge by initially employing LLMs to generate drafts of logic forms without schema-specific details.Then, an extra module is used to inject schema information to these drafts.In contrast, in this paper, we propose a simple In-Context Schema Understanding (ICSU) method that enables LLMs to directly understand schemas by leveraging in-context learning. Specifically, ICSU provides schema information to LLMs using schema-related annotated examples. We investigate three example retrieval strategies based on raw questions, anonymized questions, and generated SPARQL queries. Experimental results s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.05116</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#25552;&#21319;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Utilizing Contextual Clues and Role Correlations for Enhancing Document-level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65288;EAE&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#20043;&#19968;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#35770;&#35777;&#21644;&#20107;&#20214;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#30340;&#20449;&#24687;&#21644;&#35770;&#35777;&#35282;&#33394;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#32858;&#21512;&#65288;CCA&#65289;&#21644;&#22522;&#20110;&#35282;&#33394;&#30340;&#28508;&#22312;&#20449;&#24687;&#24341;&#23548;&#65288;RLIG&#65289;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25991;&#26723;&#32423;EAE&#12290;CCA&#27169;&#22359;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#26435;&#37325;&#65292;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#21644;&#25972;&#21512;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;RLIG&#27169;&#22359;&#36890;&#36807;&#35282;&#33394;&#20132;&#20114;&#32534;&#30721;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#28508;&#22312;&#35282;&#33394;&#34920;&#31034;&#25552;&#20379;&#23453;&#36149;&#30340;&#20449;&#24687;&#24341;&#23548;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CCA&#21644;RLIG&#27169;&#22359;&#32039;&#20945;&#12289;&#21487;&#31227;&#26893;&#19988;&#39640;&#25928;&#65292;&#24341;&#20837;&#30340;&#26032;&#21442;&#25968;&#19981;&#36229;&#36807;1%&#65292;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event argument extraction (EAE) is a vital but challenging subtask in information extraction. Most existing approaches focus on the interaction between arguments and event triggers, ignoring two critical points: the information of contextual clues and the semantic correlations among argument roles. In this paper, we propose the CARLG model, which consists of two modules: Contextual Clues Aggregation (CCA) and Role-based Latent Information Guidance (RLIG), effectively leveraging contextual clues and role correlations for improving document-level EAE. The CCA module adaptively captures and integrates contextual clues by utilizing context attention weights from a pre-trained encoder. The RLIG module captures semantic correlations through role-interactive encoding and provides valuable information guidance with latent role representation. Notably, our CCA and RLIG modules are compact, transplantable and efficient, which introduce no more than 1% new parameters and can be eas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2310.04910</link><description>&lt;p&gt;
&#20851;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#30693;&#35782;&#22270;&#35889;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful Knowledge Graph Explanations for Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;(LMs)&#21644;&#30693;&#35782;&#22270;&#35889;(KGs)&#24050;&#25104;&#20026;&#24120;&#35782;&#38382;&#31572;&#30740;&#31350;&#20013;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;&#24605;&#36335;&#38142;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#25216;&#26415;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#24573;&#35270;&#20102;&#29983;&#25104;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631; - &#22270;&#19968;&#33268;&#24615;&#21644;&#22270;&#20445;&#30495;&#24230; - &#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;Consistent GNN (CGNN)&#65292;&#35813;&#26041;&#27861;&#28155;&#21152;&#20102;&#19968;&#39033;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KG&#30340;&#39044;&#27979;&#32463;&#24120;&#20559;&#31163;&#21407;&#22987;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;CGNN&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#23637;&#31034;&#20102;&#23427;&#20135;&#29983;&#26356;&#21487;&#20449;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26126;&#30830;&#35780;&#20272;&#35299;&#37322;&#21487;&#20449;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fusing language models (LMs) and knowledge graphs (KGs) has become common in commonsense question answering research, enabling faithful chain-of-thought explanations in these models remains an open problem. One major weakness of current KG-based explanation techniques is that they overlook the faithfulness of generated explanations during evaluation. To address this gap, we make two main contributions: (1) We propose and validate two quantitative metrics - graph consistency and graph fidelity - to measure the faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN), a novel training method that adds a consistency regularization term to improve explanation faithfulness. Our analysis shows that predictions from KG often diverge from original model predictions. The proposed CGNN approach boosts consistency and fidelity, demonstrating its potential for producing more faithful explanations. Our work emphasises the importance of explicitly evaluating suggest a path
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#22686;&#24378;&#30340;&#20559;&#22909;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#36171;&#20104;&#22870;&#21169;&#27169;&#22411;&#35775;&#38382;&#22806;&#37096;&#29615;&#22659;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22870;&#21169;&#27169;&#22411;&#22312;&#22522;&#26412;&#21151;&#33021;&#19978;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#37322;&#33021;&#21147;&#21644;&#35780;&#20998;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.01045</link><description>&lt;p&gt;
&#24037;&#20855;&#22686;&#24378;&#30340;&#22870;&#21169;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Tool-Augmented Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#22686;&#24378;&#30340;&#20559;&#22909;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#36171;&#20104;&#22870;&#21169;&#27169;&#22411;&#35775;&#38382;&#22806;&#37096;&#29615;&#22659;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22870;&#21169;&#27169;&#22411;&#22312;&#22522;&#26412;&#21151;&#33021;&#19978;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#37322;&#33021;&#21147;&#21644;&#35780;&#20998;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#24314;&#27169;&#65288;&#21448;&#31216;&#20559;&#22909;&#24314;&#27169;&#65289;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#12290;&#20256;&#32479;&#30340;&#22870;&#21169;&#27169;&#22411;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24120;&#24120;&#22312;&#22522;&#26412;&#21151;&#33021;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#22914;&#31639;&#26415;&#35745;&#31639;&#12289;&#20195;&#30721;&#25191;&#34892;&#21644;&#20107;&#23454;&#26597;&#25214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#22686;&#24378;&#30340;&#20559;&#22909;&#24314;&#27169;&#26041;&#27861;&#65292;&#21517;&#20026;Themis&#65292;&#36890;&#36807;&#36171;&#20104;&#22870;&#21169;&#27169;&#22411;&#35775;&#38382;&#22806;&#37096;&#29615;&#22659;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35745;&#31639;&#22120;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20419;&#36827;&#20102;&#24037;&#20855;&#21033;&#29992;&#21644;&#22870;&#21169;&#35780;&#20998;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#36824;&#22686;&#24378;&#20102;&#35299;&#37322;&#33021;&#21147;&#21644;&#35780;&#20998;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22806;&#37096;&#24037;&#20855;&#19982;&#22870;&#21169;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#22810;&#26679;&#30340;&#22806;&#37096;&#36164;&#28304;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#26500;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#24037;&#20855;&#21442;&#19982;&#21644;&#25512;&#29702;&#36712;&#36857;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our appr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;CrossLingR&#65292;&#29992;&#20110;&#25512;&#21160;&#25910;&#25454;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;47,720&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35814;&#32454;&#35760;&#24405;&#20102;&#39033;&#30446;&#21517;&#31216;&#12289;&#30456;&#20851;&#23646;&#24615;&#21644;44&#20010;&#19981;&#21516;&#30340;&#20135;&#21697;&#31867;&#21035;&#12290;&#36890;&#36807;InstructLLaMA&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#25928;&#26524;&#12290;&#30456;&#20851;&#36164;&#28304;&#21487;&#22312;https://github.com/Update-For-Integrated-Business-AI/AMuRD&#19978;&#33719;&#21462;&#12290;</title><link>https://arxiv.org/abs/2309.09800</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#25910;&#25454;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#20840;&#38754;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598; CrossLingR
&lt;/p&gt;
&lt;p&gt;
CrossLingR: A Comprehensive Multilingual Receipt Dataset for Cross-Language Information Extraction and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;CrossLingR&#65292;&#29992;&#20110;&#25512;&#21160;&#25910;&#25454;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;47,720&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35814;&#32454;&#35760;&#24405;&#20102;&#39033;&#30446;&#21517;&#31216;&#12289;&#30456;&#20851;&#23646;&#24615;&#21644;44&#20010;&#19981;&#21516;&#30340;&#20135;&#21697;&#31867;&#21035;&#12290;&#36890;&#36807;InstructLLaMA&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#25928;&#26524;&#12290;&#30456;&#20851;&#36164;&#28304;&#21487;&#22312;https://github.com/Update-For-Integrated-Business-AI/AMuRD&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#30340;&#36807;&#31243;&#23545;&#20110;&#23558;&#25195;&#25551;&#30340;&#25910;&#25454;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#12289;&#21487;&#35775;&#38382;&#30340;&#25991;&#20214;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#21161;&#20110;&#26377;&#25928;&#22320;&#26816;&#32034;&#37325;&#35201;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#12289;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#21160;&#25910;&#25454;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;47,720&#20010;&#24102;&#26377;&#39033;&#30446;&#21517;&#31216;&#12289;&#30456;&#20851;&#23646;&#24615;&#65288;&#22914;&#20215;&#26684;&#21644;&#21697;&#29260;&#65289;&#30340;&#26631;&#27880;&#26679;&#26412;&#65292;&#24182;&#25353;&#29031;44&#20010;&#19981;&#21516;&#30340;&#20135;&#21697;&#31867;&#21035;&#36827;&#34892;&#32452;&#32455;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;InstructLLaMA&#26041;&#27861;&#35770;&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;F1&#20998;&#25968;&#20026;0.76&#21644;&#20934;&#30830;&#24615;&#20026;0.68&#30340;&#26174;&#33879;&#25928;&#26524;&#21152;&#20197;&#35777;&#26126;&#12290;&#20026;&#20102;&#25903;&#25345;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#24320;&#21457;&#65292;&#25105;&#20204;&#22312;https://github.com/Update-For-Integrated-Business-AI/AMuRD&#19978;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12289;InstructLLaMA&#27169;&#22411;&#21644;&#30456;&#20851;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of key information extraction is critical for converting scanned receipts into structured, accessible documents, facilitating the efficient retrieval of vital data. This research introduces an expansive, novel multilingual dataset designed to propel advancements in the domain of receipt information extraction and item classification. Our dataset encompasses 47,720 annotated samples, detailed with item names, associated attributes such as price and brand, and organized into 44 distinct product categories. We unveil the InstructLLaMA methodology, a pioneering approach that demonstrates significant effectiveness, evidenced by an F1 score of 0.76 and an accuracy of 0.68 in tasks of key information extraction and item classification. To support further research and application development, we make available our comprehensive dataset, the InstructLLaMA model, and relevant resources at https://github.com/Update-For-Integrated-Business-AI/AMuRD.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26631;&#31614;&#21644;&#20010;&#24615;&#21270;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#21644;&#20869;&#23384;&#39044;&#31639;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#37327;&#21270;&#26041;&#26696;&#65292;&#38024;&#23545;&#22823;&#35268;&#27169;ASR&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#29305;&#23450;&#24615;&#21035;&#12289;&#35821;&#35328;&#21644;&#35828;&#35805;&#32773;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2307.12659</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#27599;&#20010;&#29992;&#25143;&#21644;&#39044;&#31639;&#30340;&#27169;&#22411;&#65306;&#26080;&#26631;&#31614;&#21644;&#20010;&#24615;&#21270;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.12659
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26631;&#31614;&#21644;&#20010;&#24615;&#21270;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#21644;&#20869;&#23384;&#39044;&#31639;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#37327;&#21270;&#26041;&#26696;&#65292;&#38024;&#23545;&#22823;&#35268;&#27169;ASR&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#29305;&#23450;&#24615;&#21035;&#12289;&#35821;&#35328;&#21644;&#35828;&#35805;&#32773;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20135;&#29983;&#20102;&#22823;&#22411;AI&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27169;&#22411;&#37327;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#21387;&#32553;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#33021;&#21482;&#33021;&#22312;&#26377;&#38480;&#21046;&#30340;&#24863;&#20852;&#36259;&#23376;&#39046;&#22495;&#20013;&#37096;&#32626;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37327;&#21270;&#36807;&#31243;&#20013;&#22914;&#20309;&#20381;&#36182;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#23569;&#37327;&#26410;&#26631;&#35760;&#26679;&#26412;&#26469;&#20010;&#24615;&#21270;ASR&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;myQASR&#65292;&#19968;&#31181;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#20219;&#20309;&#20869;&#23384;&#38656;&#27714;&#29983;&#25104;&#38024;&#23545;&#19981;&#21516;&#29992;&#25143;&#30340;&#37327;&#21270;&#26041;&#26696;&#65292;&#26080;&#38656;&#24494;&#35843;&#12290;myQASR&#36890;&#36807;&#20998;&#26512;&#20840;&#31934;&#24230;&#28608;&#27963;&#20540;&#26469;&#33258;&#21160;&#35780;&#20272;&#32593;&#32476;&#23618;&#27425;&#30340;&#37327;&#21270;&#25935;&#24863;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#20026;&#20219;&#20309;&#39044;&#23450;&#30340;&#20869;&#23384;&#39044;&#31639;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#26696;&#12290;&#22823;&#35268;&#27169;ASR&#27169;&#22411;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;myQASR&#22914;&#20309;&#25552;&#39640;&#29305;&#23450;&#24615;&#21035;&#12289;&#35821;&#35328;&#21644;&#35828;&#35805;&#32773;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancement in Automatic Speech Recognition (ASR) has produced large AI models, which become impractical for deployment in mobile devices. Model quantization is effective to produce compressed general-purpose models, however such models may only be deployed to a restricted sub-domain of interest. We show that ASR models can be personalized during quantization while relying on just a small set of unlabelled samples from the target domain. To this end, we propose myQASR, a mixed-precision quantization method that generates tailored quantization schemes for diverse users under any memory requirement with no fine-tuning. myQASR automatically evaluates the quantization sensitivity of network layers by analysing the full-precision activation values. We are then able to generate a personalised mixed-precision quantization scheme for any pre-determined memory budget. Results for large-scale ASR models show how myQASR improves performance for specific genders, languages, and speakers.
&lt;/p&gt;</description></item><item><title>KDSTM&#26159;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#31070;&#32463;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#27809;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#19988;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2307.01878</link><description>&lt;p&gt;
KDSTM: &#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#31070;&#32463;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.01878
&lt;/p&gt;
&lt;p&gt;
KDSTM&#26159;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#31070;&#32463;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#27809;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#19988;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#21644;GPT-3&#65289;&#21487;&#20197;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#65307;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#22823;&#22411;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19968;&#33324;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#20855;&#26377;&#22312;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20998;&#26512;&#25991;&#26723;&#24182;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35789;&#27719;&#27169;&#24335;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26080;&#30417;&#30563;&#30340;&#35265;&#35299;&#25552;&#21462;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#33976;&#39311;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#65288;KDSTM&#65289;&#30340;&#26041;&#27861;&#12290;KDSTM&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#23884;&#20837;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#26631;&#35760;&#25991;&#26723;&#65292;&#24182;&#19988;&#35757;&#32451;&#25928;&#29575;&#39640;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#38750;&#24120;&#29702;&#24819;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26377;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30456;&#27604;&#36798;&#21040;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In text classification tasks, fine tuning pretrained language models like BERT and GPT-3 yields competitive accuracy; however, both methods require pretraining on large text datasets. In contrast, general topic modeling methods possess the advantage of analyzing documents to extract meaningful patterns of words without the need of pretraining. To leverage topic modeling's unsupervised insights extraction on text classification tasks, we develop the Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires no pretrained embeddings, few labeled documents and is efficient to train, making it ideal under resource constrained settings. Across a variety of datasets, our method outperforms existing supervised topic modeling methods in classification accuracy, robustness and efficiency and achieves similar performance compare to state of the art weakly supervised text classification methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#25552;&#21462;&#21644;&#25506;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#20016;&#23500;&#30284;&#32454;&#32990;&#31995;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2307.00933</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30284;&#32454;&#32990;&#31995;&#20998;&#23376;&#35889;&#20449;&#24687;&#25552;&#21462;&#21644;&#20016;&#23500;&#21270;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Information Extraction and Enrichment of Molecular Profiling Data for Cancer Cell Lines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00933
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#25552;&#21462;&#21644;&#25506;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#20016;&#23500;&#30284;&#32454;&#32990;&#31995;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30740;&#31350;&#25163;&#27573;&#21644;&#35745;&#31639;&#26041;&#27861;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#30284;&#32454;&#32990;&#31995;&#26159;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#32454;&#32990;&#26426;&#21046;&#21040;&#33647;&#29289;&#24320;&#21457;&#31561;&#21508;&#31181;&#30446;&#30340;&#65292;&#23548;&#33268;&#20102;&#30456;&#20851;&#25968;&#25454;&#21644;&#20986;&#29256;&#29289;&#30340;&#20016;&#23500;&#24615;&#12290;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#22823;&#37327;&#30340;&#25991;&#26412;&#26469;&#25910;&#38598;&#24863;&#20852;&#36259;&#30340;&#32454;&#32990;&#31995;&#30340;&#30456;&#20851;&#20449;&#24687;&#26159;&#36153;&#26102;&#19988;&#26497;&#20026;&#32531;&#24930;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26032;&#30340;&#35745;&#31639;&#26426;&#20449;&#24687;&#25552;&#21462;&#21644;&#20851;&#32852;&#26426;&#21046;&#26469;&#25552;&#39640;&#26377;&#24847;&#20041;&#30340;&#30693;&#35782;&#25552;&#21462;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#25552;&#21462;&#21644;&#25506;&#32034;&#31995;&#32479;&#30340;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#24212;&#29992;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#25991;&#26412;&#23454;&#20307;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#20197;&#20016;&#23500;&#30284;&#32454;&#32990;&#31995;&#39046;&#22495;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of research means and computational methodologies, published biomedical literature is growing exponentially in numbers and volume. Cancer cell lines are frequently used models in biological and medical research that are currently applied for a wide range of purposes, from studies of cellular mechanisms to drug development, which has led to a wealth of related data and publications. Sifting through large quantities of text to gather relevant information on the cell lines of interest is tedious and extremely slow when performed by humans. Hence, novel computational information extraction and correlation mechanisms are required to boost meaningful knowledge extraction. In this work, we present the design, implementation and application of a novel data extraction and exploration system. This system extracts deep semantic relations between textual entities from scientific literature to enrich existing structured clinical data in the domain of cancer cell lines. We int
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2305.13068</link><description>&lt;p&gt;
&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#24037;&#20855;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Making Language Models Better Tool Learners with Execution Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#20316;&#20026;&#20851;&#38190;&#30340;&#30028;&#38754;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#29702;&#35299;&#21644;&#25913;&#21464;&#29615;&#22659;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#24037;&#20855;&#25193;&#23637;&#20854;&#33021;&#21147;&#24182;&#19982;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#36890;&#24120;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#21152;&#36873;&#25321;&#22320;&#21033;&#29992;&#24037;&#20855;&#65292;&#22240;&#20026;&#22797;&#26434;&#20219;&#21153;&#24448;&#24448;&#36229;&#20986;&#20102;&#23427;&#20204;&#33258;&#36523;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#31616;&#21333;&#20219;&#21153;&#24341;&#20837;&#24037;&#20855;&#65288;&#27169;&#22411;&#26412;&#36523;&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#30340;&#20219;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#26080;&#24847;&#38388;&#20256;&#25773;&#38169;&#35823;&#32780;&#19981;&#26159;&#25552;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#25945;&#20250;&#35821;&#35328;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#24037;&#20855;&#65311;&#20026;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tool leaRning wIth exeCution fEedback (TRICE)&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#24037;&#20855;&#25191;&#34892;&#20013;&#24471;&#21040;&#30340;&#21453;&#39304;&#19981;&#26029;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#21477;&#23376;&#21253;&#36827;&#34892;&#39640;&#25928;&#28789;&#27963;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21644;&#32858;&#31867;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#20808;&#39564;&#33258;&#23450;&#20041;&#20027;&#39064;-&#25991;&#26723;&#20998;&#24067;&#30340;&#21487;&#33021;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#25285;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2302.03106</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#21477;&#23376;&#21253;&#30340;&#39640;&#25928;&#28789;&#27963;&#30340;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Efficient and Flexible Topic Modeling using Pretrained Embeddings and Bag of Sentences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#21477;&#23376;&#21253;&#36827;&#34892;&#39640;&#25928;&#28789;&#27963;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21644;&#32858;&#31867;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#20808;&#39564;&#33258;&#23450;&#20041;&#20027;&#39064;-&#25991;&#26723;&#20998;&#24067;&#30340;&#21487;&#33021;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#25285;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#22312;&#20027;&#39064;&#24314;&#27169;&#26041;&#38754;&#65292;&#32479;&#35745;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;LDA&#65289;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#19981;&#23481;&#26131;&#34701;&#20837;&#19978;&#19979;&#25991;&#35789;&#21521;&#37327;&#12290;&#23427;&#20204;&#21487;&#33021;&#20250;&#29983;&#25104;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#20027;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#21253;&#65288;BoS&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#21477;&#23376;&#20316;&#20026;&#20998;&#26512;&#21333;&#20301;&#12290;&#36890;&#36807;&#32467;&#21512;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21644;&#32858;&#31867;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#25105;&#20204;&#22522;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#12289;&#30828;&#20998;&#37197;&#21644;&#19968;&#20010;&#36864;&#28779;&#36807;&#31243;&#30340;&#24555;&#36895;&#25512;&#26029;&#31639;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#20197;&#30456;&#23545;&#36739;&#23567;&#30340;&#35745;&#31639;&#38656;&#27714;&#33719;&#24471;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;&#19982;&#21033;&#29992;&#35789;&#23884;&#20837;&#30340;&#20043;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#26356;&#21152;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#20351;&#29992;&#20808;&#39564;&#33258;&#23450;&#20041;&#20027;&#39064;-&#25991;&#26723;&#20998;&#24067;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have led to a new state-of-the-art in many NLP tasks. However, for topic modeling, statistical generative models such as LDA are still prevalent, which do not easily allow incorporating contextual word vectors. They might yield topics that do not align well with human judgment. In this work, we propose a novel topic modeling and inference algorithm. We suggest a bag of sentences (BoS) approach using sentences as the unit of analysis. We leverage pre-trained sentence embeddings by combining generative process models and clustering. We derive a fast inference algorithm based on expectation maximization, hard assignments, and an annealing process. The evaluation shows that our method yields state-of-the art results with relatively little computational demands. Our method is also more flexible compared to prior works leveraging word embeddings, since it provides the possibility to customize topic-document distributions using priors. Code and data is at \url{http
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#32034;&#30340;&#24102;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#25968;&#25454;&#21464;&#21270;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#23454;&#29616;&#23545;&#25968;&#25454;&#20869;&#22312;&#29305;&#24449;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2212.07699</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;&#24102;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based Disentangled Representation Learning with Natural Language Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.07699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#32034;&#30340;&#24102;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#25968;&#25454;&#21464;&#21270;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#23454;&#29616;&#23545;&#25968;&#25454;&#20869;&#22312;&#29305;&#24449;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25968;&#25454;&#20013;&#30340;&#22522;&#26412;&#21464;&#21270;&#22240;&#32032;&#24182;&#19981;&#23384;&#22312;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#20351;&#24471;&#22312;&#26377;&#38480;&#30340;&#22240;&#32032;&#38598;&#20013;&#31351;&#23613;&#22320;&#21015;&#20030;&#21644;&#27010;&#25324;&#25152;&#26377;&#21464;&#21270;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37117;&#26377;&#35821;&#35328;&#31561;&#20215;&#29289;&#65292;&#36890;&#24120;&#20197;&#25991;&#26412;&#25551;&#36848;&#30340;&#24418;&#24335;&#23384;&#22312;&#12290;&#36825;&#20123;&#35821;&#35328;&#23545;&#24212;&#29289;&#21487;&#20197;&#20195;&#34920;&#25968;&#25454;&#65292;&#24182;&#36731;&#26494;&#22320;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#26631;&#35760;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#35789;&#34920;&#35299;&#32544;&#26816;&#32034;&#65288;VDR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#28508;&#22312;&#25968;&#25454;&#21464;&#21270;&#30340;&#20195;&#29702;&#65292;&#20197;&#25512;&#21160;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#35789;&#27719;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#23545;&#24212;&#29289;&#21306;&#20998;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#29305;&#24449;&#30340;&#32500;&#24230;&#65292;&#20174;&#32780;&#20419;&#36827;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled representation learning remains challenging as the underlying factors of variation in the data do not naturally exist. The inherent complexity of real-world data makes it unfeasible to exhaustively enumerate and encapsulate all its variations within a finite set of factors. However, it is worth noting that most real-world data have linguistic equivalents, typically in the form of textual descriptions. These linguistic counterparts can represent the data and effortlessly decomposed into distinct tokens. In light of this, we present Vocabulary Disentangled Retrieval (VDR), a retrieval-based framework that harnesses natural language as proxies of the underlying data variation to drive disentangled representation learning. Our approach employ a bi-encoder model to represent both data and natural language in a vocabulary space, enabling the model to distinguish dimensions that capture intrinsic characteristics within data through its natural language counterpart, thus facilitat
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24555;&#36895;&#21457;&#23637;&#28508;&#22312;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#23545;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#35748;&#35782;&#65292;&#20294;&#30446;&#21069;&#30340;&#20154;&#24037;&#23398;&#20064;&#32773;&#21644;&#20154;&#31867;&#22312;&#23398;&#20064;&#29615;&#22659;&#21644;&#25968;&#25454;&#20559;&#22909;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#22686;&#21152;&#35745;&#31639;&#27169;&#22411;&#23398;&#20064;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#38656;&#35201;&#35757;&#32451;&#20986;&#27809;&#26377;&#26174;&#33879;&#20248;&#21183;&#30340;&#27169;&#22411;&#23398;&#20064;&#32773;&#65292;&#20197;&#25552;&#20379;&#27010;&#24565;&#35777;&#26126;&#21644;&#36827;&#34892;&#23454;&#39564;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2208.07998</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#26041;&#38754;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
What Artificial Neural Networks Can Tell Us About Human Language Acquisition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.07998
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24555;&#36895;&#21457;&#23637;&#28508;&#22312;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#23545;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#35748;&#35782;&#65292;&#20294;&#30446;&#21069;&#30340;&#20154;&#24037;&#23398;&#20064;&#32773;&#21644;&#20154;&#31867;&#22312;&#23398;&#20064;&#29615;&#22659;&#21644;&#25968;&#25454;&#20559;&#22909;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#22686;&#21152;&#35745;&#31639;&#27169;&#22411;&#23398;&#20064;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#38656;&#35201;&#35757;&#32451;&#20986;&#27809;&#26377;&#26174;&#33879;&#20248;&#21183;&#30340;&#27169;&#22411;&#23398;&#20064;&#32773;&#65292;&#20197;&#25552;&#20379;&#27010;&#24565;&#35777;&#26126;&#21644;&#36827;&#34892;&#23454;&#39564;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#26377;&#21487;&#33021;&#25913;&#21464;&#20154;&#20204;&#23545;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#20105;&#35758;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20154;&#24037;&#23398;&#20064;&#32773;&#21644;&#20154;&#31867;&#30340;&#23398;&#20064;&#29615;&#22659;&#21644;&#20559;&#22909;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#21066;&#24369;&#20102;&#20174;&#23398;&#20064;&#27169;&#25311;&#20013;&#33719;&#21462;&#30340;&#35777;&#25454;&#30340;&#24433;&#21709;&#21147;&#12290;&#20363;&#22914;&#65292;&#22914;&#20170;&#26368;&#26377;&#25928;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#25152;&#35757;&#32451;&#30340;&#35821;&#35328;&#25968;&#25454;&#37327;&#22823;&#32422;&#26159;&#19968;&#20010;&#20856;&#22411;&#20799;&#31461;&#21487;&#20197;&#33719;&#24471;&#30340;&#25968;&#25454;&#37327;&#30340;&#19968;&#21315;&#20493;&#12290;&#20026;&#20102;&#22686;&#21152;&#35745;&#31639;&#27169;&#22411;&#23398;&#20064;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#38656;&#35201;&#35757;&#32451;&#20986;&#27809;&#26377;&#26174;&#33879;&#20248;&#21183;&#20110;&#20154;&#31867;&#30340;&#27169;&#22411;&#23398;&#20064;&#32773;&#12290;&#22914;&#26524;&#19968;&#20010;&#21512;&#36866;&#30340;&#27169;&#22411;&#25104;&#21151;&#20064;&#24471;&#20102;&#26576;&#31181;&#30446;&#26631;&#35821;&#35328;&#30693;&#35782;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#27010;&#24565;&#35777;&#26126;&#65292;&#35777;&#26126;&#35813;&#30446;&#26631;&#22312;&#20551;&#35774;&#30340;&#20154;&#31867;&#23398;&#20064;&#22330;&#26223;&#20013;&#26159;&#21487;&#20197;&#23398;&#20250;&#30340;&#12290;&#21487;&#20449;&#30340;&#27169;&#22411;&#23398;&#20064;&#32773;&#23558;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#23454;&#39564;&#24615;&#24178;&#39044;&#65292;&#20174;&#32780;&#36827;&#34892;&#20851;&#20110;&#23398;&#20064;&#29615;&#22659;&#21464;&#37327;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#26465;&#20214;&#21442;&#25968;&#36827;&#34892;&#20005;&#35880;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress in machine learning for natural language processing has the potential to transform debates about how humans learn language. However, the learning environments and biases of current artificial learners and humans diverge in ways that weaken the impact of the evidence obtained from learning simulations. For example, today's most effective neural language models are trained on roughly one thousand times the amount of linguistic data available to a typical child. To increase the relevance of learnability results from computational models, we need to train model learners without significant advantages over humans. If an appropriate model successfully acquires some target linguistic knowledge, it can provide a proof of concept that the target is learnable in a hypothesized human learning scenario. Plausible model learners will enable us to carry out experimental manipulations to make causal inferences about variables in the learning environment, and to rigorously test poverty-
&lt;/p&gt;</description></item><item><title>PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.15042</link><description>&lt;p&gt;
PROXYQA&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15042
&lt;/p&gt;
&lt;p&gt;
PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38271;&#31687;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29983;&#25104;&#38271;&#31687;&#20869;&#23481;&#65288;&#22914;&#25253;&#21578;&#21644;&#25991;&#31456;&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#36275;&#20197;&#20805;&#20998;&#35780;&#20272;LLMs&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#20840;&#38754;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{ProxyQA}&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#28145;&#20837;&#20154;&#24037;&#31574;&#21010;&#30340;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#8220;&#20803;&#38382;&#39064;&#8221;&#12290;&#27599;&#20010;&#20803;&#38382;&#39064;&#37117;&#21253;&#21547;&#30456;&#24212;&#30340;&#24102;&#27880;&#37322;&#31572;&#26696;&#30340;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#12290;LLMs&#34987;&#35201;&#27714;&#26681;&#25454;&#36825;&#20123;&#20803;&#38382;&#39064;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#12290;&#21033;&#29992;&#35780;&#20272;&#22120;&#24182;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;\textsc{ProxyQA}&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#30340;&#34920;&#29616;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#22810;&#20010;LLMs&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.14215</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#20010;&#24615;&#21270;&#32454;&#21270;&#65292;&#22686;&#24378;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#24120;&#35782;&#22686;&#24378;&#24615;&#20869;&#23384;&#26500;&#24314;&#21644;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#65292;&#35760;&#24518;&#21644;&#21033;&#29992;&#35828;&#35805;&#32773;&#30340;&#35282;&#33394;&#26159;&#29983;&#25104;&#22238;&#24212;&#30340;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#32534;&#20889;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#25552;&#20379;&#26080;&#20449;&#24687;&#30340;&#35282;&#33394;&#21477;&#23376;&#65292;&#36825;&#22952;&#30861;&#20102;&#22238;&#24212;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#26469;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#36825;&#20123;&#38382;&#39064;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#19981;&#20135;&#29983;&#19982;&#20854;&#20182;&#35282;&#33394;&#30456;&#30683;&#30462;&#30340;&#35282;&#33394;&#65292;&#25105;&#20204;&#20391;&#37325;&#20110;&#26681;&#25454;&#35774;&#35745;&#30340;&#31574;&#30053;&#65292;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#27492;&#26469;&#32454;&#21270;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#32972;&#26223;&#12290;&#20316;&#20026;&#22810;&#20250;&#35805;&#24773;&#22659;&#20013;&#35282;&#33394;&#25193;&#23637;&#30340;&#20808;&#39537;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#31867;&#20154;&#20010;&#24615;&#32454;&#21270;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.09798</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#8220;&#36234;&#29425;&#8221;&#25361;&#25112;&#65292;&#21363;&#35268;&#36991;&#20445;&#38556;&#25514;&#26045;&#20197;&#20135;&#29983;&#19981;&#31526;&#21512;&#20262;&#29702;&#30340;&#25552;&#31034;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#36845;&#20195;&#22320;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#22522;&#20110;&#20551;&#35774;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#35268;&#36991;&#20445;&#38556;&#30340;&#34920;&#36798;&#12290;&#36890;&#36807;&#22312;ChatGPT&#65288;GPT-3.5&#21644;GPT-4&#65289;&#21644;Gemini-Pro&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24179;&#22343;5&#27425;&#36845;&#20195;&#20869;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#33258;&#28982;&#32780;&#31616;&#32451;&#65292;&#34920;&#26126;&#23427;&#20204;&#36739;&#19981;&#26131;&#34987;&#26816;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21019;&#24314;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#27604;&#20808;&#21069;&#30740;&#31350;&#35748;&#20026;&#30340;&#35201;&#31616;&#21333;&#65292;&#24182;&#19988;&#40657;&#30418;&#36234;&#29425;&#25915;&#20987;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#24378;&#35843;&#20102;&#36825;&#31181;&#29616;&#35937;&#23545;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#30740;&#31350;&#21147;&#24230;&#20197;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.09615</link><description>&lt;p&gt;
&#23398;&#20064;&#25463;&#24452;&#65306;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35823;&#23548;&#24615;&#25215;&#35834;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Learning Shortcuts: On the Misleading Promise of NLU in Language Models. (arXiv:2401.09615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#24378;&#35843;&#20102;&#36825;&#31181;&#29616;&#35937;&#23545;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#30740;&#31350;&#21147;&#24230;&#20197;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#24120;&#24120;&#37319;&#29992;&#25463;&#24452;&#65292;&#23548;&#33268;&#22312;&#20915;&#31574;&#35268;&#21017;&#19978;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#19978;&#20135;&#29983;&#20102;&#19968;&#31181;&#38169;&#35273;&#12290;&#36825;&#19968;&#29616;&#35937;&#22312;&#20934;&#30830;&#35780;&#20272;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#19978;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#31616;&#27905;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#30740;&#31350;&#21147;&#24230;&#65292;&#20026;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#39640;&#30495;&#23454;&#22330;&#26223;&#19979;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#30340;&#26631;&#20934;&#20316;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05507</link><description>&lt;p&gt;
InfiAgent-DABench: &#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#35780;&#20272;&#20195;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05507
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;"InfiAgent-DABench"&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;DAEval&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;55&#20010;CSV&#25991;&#20214;&#34893;&#29983;&#20986;&#30340;311&#20010;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#35780;&#20272;LLMs&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20195;&#29702;&#30340;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26684;&#24335;&#25552;&#31034;&#25216;&#26415;&#65292;&#30830;&#20445;&#38382;&#39064;&#26159;&#38381;&#21512;&#24418;&#24335;&#30340;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#24403;&#21069;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;DAAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#19987;&#38376;&#20195;&#29702;&#12290;InfiAgent-DABench&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#21253;&#24050;&#32463;&#21457;&#24067;&#22312;https://github.com/InfiAgent/InfiAgent&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce "InfiAgent-DABench", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks. This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents. We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated. Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks. In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02333</link><description>&lt;p&gt;
&#36229;&#36234;&#25552;&#21462;&#65306;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;&#20197;&#23454;&#29616;&#39640;&#25928;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104; (RAG) &#26550;&#26500;&#22312;&#20174;&#21508;&#31181;&#25991;&#20214;&#20013;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#21253;&#21547;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340; PDF &#25991;&#26723;&#20013;&#30340;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558; PDF &#23384;&#20648;&#22312;&#26816;&#32034;&#25968;&#25454;&#24211;&#20013;&#65292;&#24182;&#21333;&#29420;&#25552;&#21462;&#34920;&#26684;&#20869;&#23481;&#12290;&#25552;&#21462;&#30340;&#34920;&#26684;&#32463;&#36807;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#22788;&#29702;&#65292;&#23558;&#26631;&#39064;&#19982;&#30456;&#24212;&#30340;&#20540;&#36830;&#25509;&#36215;&#26469;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#20016;&#23500;&#25968;&#25454;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340; Llama-2-chat &#35821;&#35328;&#27169;&#22411;&#22312; RAG &#26550;&#26500;&#20013;&#36827;&#34892;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#27425;&#24615;&#25552;&#31034;&#20351;&#29992; ChatGPT 3.5 API &#22686;&#24378;&#34920;&#26684;&#25968;&#25454;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#25968;&#25454;&#19982;&#20854;&#20182; PDF &#25991;&#20214;&#19968;&#36215;&#36755;&#20837;&#26816;&#32034;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our appr
&lt;/p&gt;</description></item><item><title>NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2312.14890</link><description>&lt;p&gt;
NPHardEval: &#36890;&#36807;&#22797;&#26434;&#24615;&#31867;&#21035;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#21160;&#24577;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14890
&lt;/p&gt;
&lt;p&gt;
NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26159;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#23427;&#20063;&#34987;&#29992;&#20110;&#22312;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65306;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20934;&#22312;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#26041;&#38754;&#36824;&#19981;&#22815;&#65292;&#21516;&#26102;&#20063;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#20934;&#26159;&#20844;&#24320;&#21487;&#35775;&#38382;&#19988;&#38745;&#24577;&#30340;&#65292;&#20351;&#24471;&#27169;&#22411;&#26377;&#21487;&#33021;&#26681;&#25454;&#29305;&#23450;&#30340;&#22522;&#20934;&#25351;&#26631;&#35843;&#25972;&#20854;&#21709;&#24212;&#65292;&#20174;&#32780;&#22840;&#22823;&#20854;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;NPHardEval&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.03731</link><description>&lt;p&gt;
&#22810;&#20010;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#22270;&#24418;&#25552;&#31034;&#30340;MultiGPrompt
&lt;/p&gt;
&lt;p&gt;
MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#21487;&#20197;&#22266;&#26377;&#22320;&#23545;Web&#19978;&#30456;&#20114;&#36830;&#25509;&#30340;&#23545;&#35937;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25903;&#25345;&#19968;&#31995;&#21015;Web&#24212;&#29992;&#65292;&#27604;&#22914;&#32593;&#32476;&#20998;&#26512;&#21644;&#20869;&#23481;&#25512;&#33616;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#27969;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#31471;&#21040;&#31471;&#30417;&#30563;&#26694;&#26550;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#19982;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#30340;&#21487;&#29992;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#24182;&#22686;&#24378;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#22522;&#20110;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#32780;&#25552;&#31034;&#21017;&#34987;&#25552;&#20986;&#26469;&#36827;&#19968;&#27493;&#32553;&#23567;&#39044;&#35757;&#32451;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#30446;&#26631;&#24046;&#36317;&#12290;&#34429;&#28982;&#24050;&#32463;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#24418;&#23398;&#20064;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25506;&#32034;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#21033;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23548;&#33268;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#33021;&#23398;&#20064;&#30340;&#36890;&#29992;&#30693;&#35782;&#30340;&#23376;&#38598;&#21463;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;MultiGPrompt&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#22270;&#24418;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#24863;&#30693;&#32454;&#23567;&#35270;&#35273;&#32454;&#33410;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#20027;&#39064;&#30340;&#23610;&#23544;&#38750;&#24120;&#25935;&#24863;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#21487;&#35270;&#21098;&#35009;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16033</link><description>&lt;p&gt;
ViCrop: &#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#24863;&#30693;&#32454;&#23567;&#35270;&#35273;&#32454;&#33410;
&lt;/p&gt;
&lt;p&gt;
ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal Large Language Models. (arXiv:2310.16033v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#24863;&#30693;&#32454;&#23567;&#35270;&#35273;&#32454;&#33410;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#20027;&#39064;&#30340;&#23610;&#23544;&#38750;&#24120;&#25935;&#24863;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#21487;&#35270;&#21098;&#35009;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#22312;&#35270;&#35273;&#38382;&#31572;(VQA)&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#24433;&#21709;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#21644;&#39046;&#22495;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#37492;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#28508;&#21147;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#22788;&#29702;&#19981;&#21516;&#30340;&#22270;&#20687;&#21644;&#38382;&#39064;&#23646;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MLLMs&#26159;&#21542;&#33021;&#22815;&#20687;&#36739;&#22823;&#30340;&#32452;&#20214;&#19968;&#26679;&#24863;&#30693;&#22270;&#20687;&#20013;&#30340;&#32454;&#33410;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#26102;&#23545;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#20027;&#39064;&#30340;&#23610;&#23544;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#19988;&#38543;&#30528;&#23610;&#23544;&#30340;&#20943;&#23567;&#65292;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#19979;&#38477;&#22810;&#36798;45.91%&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#20154;&#31867;&#21487;&#35270;&#21098;&#35009;&#21487;&#20197;&#26174;&#33879;&#20943;&#36731;&#20854;&#23545;&#23610;&#23544;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#25928;&#24212;&#26159;&#22240;&#26524;&#30340;&#12290;&#20026;&#20102;&#25193;&#22823;&#20154;&#31867;&#21487;&#35270;&#21098;&#35009;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ViCrop&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#21160;&#21487;&#35270;&#21098;&#35009;&#26469;&#22686;&#24378;MLLMs&#38646;&#26679;&#26412;VQA&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive details as well as larger components in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject related to the question, declining up to $45.91\%$ with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. To scale up the usefulness of human cropping, we propose ViCrop, a general framework that utilizes automatic visual cropping to enhance zero-shot VQA of MLLMs. We construct five variant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01845</link><description>&lt;p&gt;
&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#22312;&#24120;&#35268;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#36965;&#24863;&#22270;&#20687;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#31934;&#30830;&#30340;&#24314;&#31569;&#29289;&#23454;&#20363;&#20998;&#21106;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#21463;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#24050;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#20247;&#22810;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;Segment Anything Model&#65288;SAM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#20854;&#25797;&#38271;&#26080;&#31867;&#21035;&#22270;&#20687;&#20998;&#21106;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;SAM&#30340;&#23616;&#38480;&#24615;&#65292;&#25581;&#31034;&#20102;&#23427;&#22312;&#24212;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;SAM&#19981;&#20855;&#22791;&#35782;&#21035;&#33021;&#21147;&#65292;&#22240;&#27492;&#26080;&#27861;&#23545;&#23450;&#20301;&#30340;&#23545;&#35937;&#36827;&#34892;&#20998;&#31867;&#21644;&#26631;&#35760;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different promp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#20013;&#65292;&#24403;&#35821;&#26009;&#24211;&#20005;&#26684;&#36981;&#24490;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#26102;&#65292;&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#19978;&#20301;&#35789;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#19968;&#31181;&#35757;&#32451;&#30446;&#26631;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#26222;&#36941;&#37327;&#21270;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#30340;&#21453;&#21521;&#19979;&#23454;&#29616;&#19978;&#20301;&#35789;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#20123;&#20551;&#35774;&#21644;&#30446;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08325</link><description>&lt;p&gt;
&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#19982;&#37327;&#21270;&#65306;&#22312;&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#20013;&#25506;&#31350;&#19978;&#20301;&#35789;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Distributional Inclusion Hypothesis and Quantifications: Probing Hypernymy in Functional Distributional Semantics. (arXiv:2309.08325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#20013;&#65292;&#24403;&#35821;&#26009;&#24211;&#20005;&#26684;&#36981;&#24490;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#26102;&#65292;&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#19978;&#20301;&#35789;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#19968;&#31181;&#35757;&#32451;&#30446;&#26631;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#26222;&#36941;&#37327;&#21270;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#30340;&#21453;&#21521;&#19979;&#23454;&#29616;&#19978;&#20301;&#35789;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#20123;&#20551;&#35774;&#21644;&#30446;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#65288;FDS&#65289;&#36890;&#36807;&#30495;&#26465;&#20214;&#20989;&#25968;&#23545;&#21333;&#35789;&#30340;&#21547;&#20041;&#36827;&#34892;&#24314;&#27169;&#12290;&#24403;&#35821;&#26009;&#24211;&#20005;&#26684;&#36981;&#24490;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#26102;&#65292;FDS&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#19978;&#20301;&#35789;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;&#24471;FDS&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#31616;&#21333;&#30340;&#26222;&#36941;&#37327;&#21270;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#30340;&#21453;&#21521;&#19979;&#23454;&#29616;&#19978;&#20301;&#35789;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#30446;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional Distributional Semantics (FDS) models the meaning of words by truth-conditional functions. This provides a natural representation for hypernymy, but no guarantee that it is learnt when FDS models are trained on a corpus. We demonstrate that FDS models learn hypernymy when a corpus strictly follows the Distributional Inclusion Hypothesis. We further introduce a training objective that allows FDS to handle simple universal quantifications, thus enabling hypernymy learning under the reverse of DIH. Experimental results on both synthetic and real data sets confirm our hypotheses and the effectiveness of our proposed objective.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65288;GRG&#65289;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25991;&#26723;&#26816;&#32034;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11278</link><description>&lt;p&gt;
&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65306;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generator-Retriever-Generator: A Novel Approach to Open-domain Question Answering. (arXiv:2307.11278v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11278
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65288;GRG&#65289;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25991;&#26723;&#26816;&#32034;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#38382;&#31572;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#20174;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65288;GRG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#25991;&#26723;&#26816;&#32034;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#65292;&#39318;&#20808;&#36890;&#36807;&#32473;&#23450;&#38382;&#39064;&#25552;&#31034;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;&#21516;&#26102;&#65292;&#21452;&#32534;&#30721;&#22120;&#32593;&#32476;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#25991;&#26723;&#28982;&#21518;&#20256;&#36882;&#32473;&#31532;&#20108;&#20010;LLM&#65292;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#36807;&#32467;&#21512;&#25991;&#26723;&#26816;&#32034;&#21644;LLM&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#31572;&#26696;&#12290;GRG&#22312;TriviaQA&#12289;NQ&#21644;WebQ&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;-&#35835;&#21462;&#21644;&#26816;&#32034;-&#35835;&#21462;&#27969;&#27700;&#32447;&#65288;GENREAD&#21644;RFiD&#65289;&#65292;&#20998;&#21035;&#33267;&#23569;&#25552;&#39640;&#20102;+5.2&#12289;+4.2&#21644;+1.6&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain question answering (QA) tasks usually require the retrieval of relevant information from a large corpus to generate accurate answers. We propose a novel approach called Generator-Retriever-Generator (GRG) that combines document retrieval techniques with a large language model (LLM), by first prompting the model to generate contextual documents based on a given question. In parallel, a dual-encoder network retrieves documents that are relevant to the question from an external corpus. The generated and retrieved documents are then passed to the second LLM, which generates the final answer. By combining document retrieval and LLM generation, our approach addresses the challenges of open-domain QA, such as generating informative and contextually relevant answers. GRG outperforms the state-of-the-art generate-then-read and retrieve-then-read pipelines (GENREAD and RFiD) improving their performance at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#20174;&#30005;&#35270;&#21095;&#30340;&#35282;&#33394;&#32593;&#32476;&#20013;&#25552;&#21462;&#32593;&#32476;&#25351;&#26631;&#65292;&#30740;&#31350;&#21457;&#29616;&#23545;&#30005;&#35270;&#21095;&#30340;&#35780;&#35770;&#20998;&#25968;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#20026;&#30005;&#35270;&#21046;&#29255;&#20154;&#25552;&#20379;&#20102;&#23450;&#37327;&#20449;&#24687;&#65292;&#24110;&#21161;&#20182;&#20204;&#35843;&#25972;&#35282;&#33394;&#21160;&#24577;&#20197;&#21560;&#24341;&#35266;&#20247;&#12290;</title><link>http://arxiv.org/abs/2307.05329</link><description>&lt;p&gt;
&#35299;&#30721;&#30005;&#35270;&#21095;&#30340;&#27969;&#34892;&#31243;&#24230;&#65306;&#19968;&#20010;&#32593;&#32476;&#20998;&#26512;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Decoding the Popularity of TV Series: A Network Analysis Perspective. (arXiv:2307.05329v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05329
&lt;/p&gt;
&lt;p&gt;
&#20174;&#30005;&#35270;&#21095;&#30340;&#35282;&#33394;&#32593;&#32476;&#20013;&#25552;&#21462;&#32593;&#32476;&#25351;&#26631;&#65292;&#30740;&#31350;&#21457;&#29616;&#23545;&#30005;&#35270;&#21095;&#30340;&#35780;&#35770;&#20998;&#25968;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#20026;&#30005;&#35270;&#21046;&#29255;&#20154;&#25552;&#20379;&#20102;&#23450;&#37327;&#20449;&#24687;&#65292;&#24110;&#21161;&#20182;&#20204;&#35843;&#25972;&#35282;&#33394;&#21160;&#24577;&#20197;&#21560;&#24341;&#35266;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20174;&#19977;&#37096;&#27969;&#34892;&#30005;&#35270;&#21095;&#20013;&#25552;&#21462;&#30340;&#35282;&#33394;&#32593;&#32476;&#65292;&#24182;&#25506;&#35752;&#20102;&#30005;&#35270;&#21095;&#38598;&#30340;&#35282;&#33394;&#32593;&#32476;&#25351;&#26631;&#19982;IMDB&#35780;&#35770;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35282;&#33394;&#32593;&#32476;&#26159;&#20174;&#30005;&#35270;&#21095;&#24773;&#33410;&#20013;&#21019;&#24314;&#30340;&#22270;&#24418;&#65292;&#34920;&#31034;&#22330;&#26223;&#20013;&#35282;&#33394;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25351;&#31034;&#23427;&#20204;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#12290;&#25105;&#20204;&#20026;&#27599;&#38598;&#35745;&#31639;&#20102;&#21508;&#31181;&#32593;&#32476;&#25351;&#26631;&#65292;&#22914;&#33410;&#28857;&#24230;&#21644;&#22270;&#24418;&#23494;&#24230;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#26469;&#25506;&#32034;&#32593;&#32476;&#25351;&#26631;&#19982;&#30005;&#35270;&#21095;&#22312;IMDB&#19978;&#30340;&#35780;&#20215;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30005;&#35270;&#21095;&#38598;&#20013;&#30340;&#35282;&#33394;&#20114;&#21160;&#30340;&#26576;&#20123;&#32593;&#32476;&#25351;&#26631;&#19982;&#30005;&#35270;&#21095;&#30340;&#35780;&#35770;&#20998;&#25968;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#26356;&#22810;&#23450;&#37327;&#20449;&#24687;&#65292;&#24110;&#21161;&#30005;&#35270;&#21046;&#29255;&#20154;&#20102;&#35299;&#22914;&#20309;&#35843;&#25972;&#26410;&#26469;&#21095;&#38598;&#30340;&#35282;&#33394;&#21160;&#24577;&#65292;&#20197;&#21560;&#24341;&#35266;&#20247;&#12290;&#36890;&#36807;&#29702;&#35299;&#35282;&#33394;&#20114;&#21160;&#23545;&#35266;&#20247;&#21442;&#19982;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
In this paper, we analyze the character networks extracted from three popular television series and explore the relationship between a TV show episode's character network metrics and its review from IMDB. Character networks are graphs created from the plot of a TV show that represents the interactions of characters in scenes, indicating the presence of a connection between them. We calculate various network metrics for each episode, such as node degree and graph density, and use these metrics to explore the potential relationship between network metrics and TV series reviews from IMDB. Our results show that certain network metrics of character interactions in episodes have a strong correlation with the review score of TV series. Our research aims to provide more quantitative information that can help TV producers understand how to adjust the character dynamics of future episodes to appeal to their audience. By understanding the impact of character interactions on audience engagement an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#24230;&#25513;&#30721;&#29575;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;MLM&#39044;&#35757;&#32451;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#65292;&#36798;&#21040;&#20102;&#23545;BERT-base&#21644;BERT-large&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;0.46%&#21644;0.25%&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21152;&#24555;&#20102;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#65292;&#36824;&#23454;&#29616;&#20102;&#23545;BERT-large&#30340;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.15096</link><description>&lt;p&gt;
MLM&#39044;&#35757;&#32451;&#30340;&#21160;&#24577;&#25513;&#30721;&#29575;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Dynamic Masking Rate Schedules for MLM Pretraining. (arXiv:2305.15096v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#24230;&#25513;&#30721;&#29575;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;MLM&#39044;&#35757;&#32451;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#65292;&#36798;&#21040;&#20102;&#23545;BERT-base&#21644;BERT-large&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;0.46%&#21644;0.25%&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21152;&#24555;&#20102;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#65292;&#36824;&#23454;&#29616;&#20102;&#23545;BERT-large&#30340;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#20351;&#29992;&#20102;&#21407;&#22987;BERT&#27169;&#22411;&#30340;&#22266;&#23450;&#25513;&#30721;&#29575;15%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25513;&#30721;&#29575;&#26469;&#26367;&#20195;&#22266;&#23450;&#29575;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#21487;&#20197;&#27604;&#22266;&#23450;&#29575;&#22522;&#20934;&#20998;&#21035;&#25552;&#39640;BERT-base&#21644;BERT-large&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;0.46%&#21644;0.25%&#12290;&#36825;&#20123;&#25552;&#21319;&#26469;&#33258;&#20110;&#25509;&#35302;&#39640;&#21644;&#20302;&#25513;&#30721;&#29575;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#22312;&#20004;&#31181;&#35774;&#32622;&#20013;&#37117;&#24102;&#26469;&#20102;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25513;&#30721;&#29575;&#35843;&#24230;&#26159;&#25552;&#39640;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;1.89&#20493;&#65292;&#24182;&#23545;BERT-large&#23454;&#29616;&#20102;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model's fixed masking rate of 15%. We propose to instead dynamically schedule the masking rate throughout training. We find that linearly decreasing the masking rate over the course of pretraining improves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and BERT-large, respectively, compared to fixed rate baselines. These gains come from exposure to both high and low masking rate regimes, providing benefits from both settings. Our results demonstrate that masking rate scheduling is a simple way to improve the quality of masked language models, achieving up to a 1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for BERT-large.
&lt;/p&gt;</description></item><item><title>DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13915</link><description>&lt;p&gt;
DAPR&#65306;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DAPR: A Benchmark on Document-Aware Passage Retrieval. (arXiv:2305.13915v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13915
&lt;/p&gt;
&lt;p&gt;
DAPR&#26159;&#19968;&#20010;&#25991;&#26723;&#24863;&#30693;&#27573;&#33853;&#26816;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20174;&#38271;&#25991;&#26723;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#27573;&#33853;&#24182;&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#26816;&#32034;&#20027;&#35201;&#20851;&#27880;&#30701;&#25991;&#26412;&#30340;&#25490;&#21517;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#35780;&#20272;&#25490;&#21517;&#27573;&#33853;&#25110;&#25972;&#20010;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#24076;&#26395;&#20174;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#38271;&#25991;&#26723;&#20013;&#30340;&#30456;&#20851;&#27573;&#33853;&#65292;&#20363;&#22914;&#27861;&#24459;&#26696;&#20363;&#65292;&#30740;&#31350;&#35770;&#25991;&#31561;&#65292;&#27492;&#26102;&#27573;&#33853;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#36825;&#23601;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;&#26041;&#27861;&#25214;&#21040;&#27491;&#30830;&#30340;&#25991;&#26723;&#24182;&#36820;&#22238;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21629;&#21517;&#20102;Document-Aware Passage Retrieval&#65288;DAPR&#65289;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;DAPR&#21644;&#25972;&#20010;&#25991;&#26723;&#26816;&#32034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#25991;&#26723;&#25688;&#35201;&#20013;&#28155;&#21152;&#25991;&#26723;&#32423;&#21035;&#30340;&#20869;&#23481;&#65292;&#27719;&#24635;&#27573;&#33853;&#34920;&#31034;&#21644;&#20351;&#29992;BM25&#36827;&#34892;&#28151;&#21512;&#26816;&#32034;&#65292;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27573;&#33853;&#26816;&#32034;&#22120;&#12290;&#36825;&#20010;&#28151;&#21512;&#26816;&#32034;&#31995;&#32479;&#65292;&#24635;&#20307;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DAPR&#20219;&#21153;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural retrieval mainly focuses on ranking short texts and is challenged with long documents. Existing work mainly evaluates either ranking passages or whole documents. However, there are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. legal cases, research papers, etc. In this scenario, the passage often provides little document context and thus challenges the current approaches to finding the correct document and returning accurate results. To fill this gap, we propose and name this task Document-Aware Passage Retrieval (DAPR) and build a benchmark including multiple datasets from various domains, covering both DAPR and whole-document retrieval. In experiments, we extend the state-of-the-art neural passage retrievers with document-level context via different approaches including prepending document summary, pooling over passage representations, and hybrid retrieval with BM25. The hybrid-retrieval systems, the overall b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10818</link><description>&lt;p&gt;
&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#30446;&#21069;&#20844;&#24320;&#30340;&#23454;&#29616;&#12289;&#35757;&#32451;&#27169;&#22411;&#25110;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#31243;&#24207;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;C4&#25968;&#25454;&#38598;&#31616;&#21270;&#30340;DDLM&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#36895;&#24230;&#26356;&#24555;&#30340;&#37319;&#26679;&#30340;&#26032;&#22411;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#38024;&#23545;&#20351;&#29992;&#24471;&#20998;&#25554;&#20540;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27492;&#21069;&#27809;&#26377;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;LM&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#20219;&#21153;&#65289;&#65292;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;DDLM&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20379;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;&#26410;&#26469;&#30340;D&#30456;&#20851;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09859</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27969;&#30021;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#38750;&#24120;&#30456;&#20284;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#35805;&#35821;&#65292;&#22240;&#27492;&#21306;&#20998;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#30340;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#35780;&#35770;&#24182;&#27169;&#20223;&#26576;&#20123;&#20316;&#32773;&#21644;&#20154;&#29289;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340; logits&#65292;&#25110;&#38656;&#35201;&#21487;&#20197;&#20174;&#30446;&#26631;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#31181;&#40657;&#21283;&#23376;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#35266;&#23519;&#21040;&#29983;&#25104;&#25991;&#26412;&#22312;&#29983;&#25104;&#22120;&#30340;&#20284;&#28982;&#20989;&#25968;&#19979;&#26159;&#23616;&#37096;&#26368;&#20248;&#30340;&#65292;&#32780;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21017;&#19981;&#26159;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#36739;&#23567;&#19988;&#37096;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65306;&#23427;&#20204;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#26816;&#27979;&#26469;&#33258;&#23567;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#30456;&#21516;&#30340;&#35821;&#26009;&#24211;&#23545;&#26816;&#27979;&#24615;&#33021;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
&lt;/p&gt;</description></item></channel></rss>