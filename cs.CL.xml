<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20013;&#34920;&#29616;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20854;&#20013;&#30340;&#23376;&#38598;&#23545;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#38416;&#26126;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20197;&#21450;&#37319;&#21462;&#30340;&#31435;&#22330;&#12290;</title><link>http://arxiv.org/abs/2310.18130</link><description>&lt;p&gt;
DELPHI: &#35780;&#20272;LLMs&#22312;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues. (arXiv:2310.18130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18130
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20013;&#34920;&#29616;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20854;&#20013;&#30340;&#23376;&#38598;&#23545;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#38416;&#26126;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20197;&#21450;&#37319;&#21462;&#30340;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20105;&#35758;&#26159;&#25105;&#20204;&#26102;&#20195;&#30340;&#19968;&#31181;&#21453;&#26144;&#65292;&#24182;&#19988;&#26159;&#20219;&#20309;&#35328;&#35770;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#23545;&#35805;&#31995;&#32479;&#30340;&#20852;&#36215;&#65292;&#22686;&#21152;&#20102;&#20844;&#20247;&#23545;&#36825;&#20123;&#31995;&#32479;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#30340;&#20381;&#36182;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#22320;&#32771;&#23519;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#22238;&#31572;&#28041;&#21450;&#27491;&#22312;&#36827;&#34892;&#30340;&#36777;&#35770;&#30340;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20154;&#31867;&#27880;&#37322;&#30340;&#26631;&#31614;&#65292;&#21453;&#26144;&#24403;&#21069;&#30340;&#35752;&#35770;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#20105;&#35758;&#24615;&#38382;&#39064;&#25968;&#25454;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#22522;&#20110;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#30340;Quora&#38382;&#39064;&#23545;&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#12290;&#35813;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#28041;&#21450;&#30693;&#35782;&#26102;&#25928;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;LLMs&#65292;&#38416;&#26126;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20197;&#21450;&#37319;&#21462;&#30340;&#31435;&#22330;&#12290;&#36825;&#39033;&#30740;&#31350;&#26368;&#32456;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;LLMs&#19982;&#20105;&#35758;&#38382;&#39064;&#30340;&#20114;&#21160;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controversy is a reflection of our zeitgeist, and an important aspect to any discourse. The rise of large language models (LLMs) as conversational systems has increased public reliance on these systems for answers to their various questions. Consequently, it is crucial to systematically examine how these models respond to questions that pertaining to ongoing debates. However, few such datasets exist in providing human-annotated labels reflecting the contemporary discussions. To foster research in this area, we propose a novel construction of a controversial questions dataset, expanding upon the publicly released Quora Question Pairs Dataset. This dataset presents challenges concerning knowledge recency, safety, fairness, and bias. We evaluate different LLMs using a subset of this dataset, illuminating how they handle controversial issues and the stances they adopt. This research ultimately contributes to our understanding of LLMs' interaction with controversial issues, paving the way f
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>IDMO&#39033;&#30446;&#26088;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#65292;&#20854;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#26032;&#22411;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#12289;&#35780;&#20272;GPT-4&#31561;&#12290;</title><link>http://arxiv.org/abs/2310.11097</link><description>&lt;p&gt;
&#29992;&#20110;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#23454;&#39564;&#65306;IDMO&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11097
&lt;/p&gt;
&lt;p&gt;
IDMO&#39033;&#30446;&#26088;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#65292;&#20854;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#26032;&#22411;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#12289;&#35780;&#20272;GPT-4&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22823;&#21033;&#25968;&#23383;&#23186;&#20307;&#35266;&#23519;&#39033;&#30446;&#65288;IDMO&#65289;&#26159;&#27431;&#27954;&#19968;&#39033;&#20513;&#35758;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#12290;&#26412;&#25253;&#21578;&#27010;&#36848;&#20102;Rai-CRITS&#22312;&#35813;&#39033;&#30446;&#20013;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#21019;&#24314;&#29992;&#20110;&#27979;&#35797;&#25216;&#26415;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#65288;ii&#65289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#31867;Pagella Politica&#30340;&#35009;&#20915;&#20197;&#20415;&#20110;&#26356;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#65288;iii&#65289;&#21019;&#24314;&#33258;&#21160;&#27169;&#22411;&#65292;&#23545;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#25991;&#26412;&#34164;&#21547;&#20855;&#26377;&#24322;&#24120;&#31934;&#24230;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#65288;iv&#65289;&#20351;&#29992;GPT-4&#35780;&#20272;&#25991;&#26412;&#34164;&#21547;&#65292; &#65288;v&#65289;&#22312;&#22269;&#23478;&#27963;&#21160;&#20013;&#24320;&#23637;&#25552;&#39640;&#23545;&#20551;&#26032;&#38395;&#24847;&#35782;&#30340;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Italian Digital Media Observatory (IDMO) project, part of a European initiative, focuses on countering disinformation and fake news. This report outlines contributions from Rai-CRITS to the project, including: (i) the creation of novel datasets for testing technologies (ii) development of an automatic model for categorizing Pagella Politica verdicts to facilitate broader analysis (iii) creation of an automatic model for recognizing textual entailment with exceptional accuracy on the FEVER dataset (iv) assessment using GPT-4 to identify textual entailmen (v) a game to raise awareness about fake news at national events.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Avalon&#28216;&#25103;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;AvalonBench&#26469;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#23384;&#22312;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.05036</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#31574;&#30053;&#65306;&#35780;&#20272;&#22312;Avalon&#28216;&#25103;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
From Text to Tactic: Evaluating LLMs Playing the Game of Avalon. (arXiv:2310.05036v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Avalon&#28216;&#25103;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;AvalonBench&#26469;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#23384;&#22312;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29609;&#31574;&#30053;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;Resistance Avalon&#20013;&#30340;&#28508;&#21147;&#12290;Avalon&#29609;&#23478;&#19981;&#20165;&#38656;&#35201;&#26681;&#25454;&#21160;&#24577;&#21457;&#23637;&#30340;&#28216;&#25103;&#38454;&#27573;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#36824;&#38656;&#35201;&#21442;&#19982;&#35752;&#35770;&#65292;&#22312;&#35752;&#35770;&#20013;&#24517;&#39035;&#27450;&#39575;&#12289;&#25512;&#29702;&#21644;&#19982;&#20854;&#20182;&#29609;&#23478;&#36827;&#34892;&#35848;&#21028;&#12290;&#36825;&#20123;&#29305;&#28857;&#20351;&#24471;Avalon&#25104;&#20026;&#30740;&#31350;LLM&#20195;&#29702;&#30340;&#20915;&#31574;&#21644;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#26377;&#36259;&#35797;&#39564;&#24179;&#21488;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AvalonBench&#8212;&#8212;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#30340;&#20840;&#38754;&#28216;&#25103;&#29615;&#22659;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#65306;&#65288;1&#65289;Avalon&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#65288;2&#65289;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#20316;&#20026;&#22522;&#20934;&#23545;&#25163;&#65292;&#20197;&#21450;&#65288;3&#65289;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#20855;&#26377;&#23450;&#21046;&#25552;&#31034;&#30340;ReAct-style LLM&#20195;&#29702;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22522;&#20110;AvalonBench&#30340;&#35780;&#20272;&#31361;&#20986;&#26174;&#31034;&#20102;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#20687;ChatGPT&#36825;&#26679;&#22312;&#22909;&#35282;&#33394;&#20013;&#30340;&#27169;&#22411;&#23545;&#25112;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#30340;&#32988;&#29575;&#20026;22.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots play
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00034</link><description>&lt;p&gt;
PB-LLM: &#37096;&#20998;&#20108;&#20540;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PB-LLM: Partially Binarized Large Language Models. (arXiv:2310.00034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32593;&#32476;&#20108;&#20540;&#21270;&#65292;&#19968;&#31181;&#21387;&#32553;&#27169;&#22411;&#26435;&#37325;&#20026;&#21333;&#20010;&#27604;&#29305;&#30340;&#37327;&#21270;&#30340;&#28608;&#36827;&#24418;&#24335;&#65292;&#19987;&#38376;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21387;&#32553;&#12290;&#30001;&#20110;&#20043;&#21069;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;LLMs&#23849;&#28291;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37096;&#20998;&#20108;&#20540;&#21270;LLM&#65288;PB-LLM&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#37327;&#21270;LLMs&#30340;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#25581;&#31034;&#20102;&#29616;&#26377;&#20108;&#20540;&#21270;&#31639;&#27861;&#30340;&#21407;&#29983;&#24212;&#29992;&#30340;&#26080;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#26174;&#33879;&#26435;&#37325;&#22312;&#23454;&#29616;&#20302;&#20301;&#37327;&#21270;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;PB-LLM&#22312;&#20108;&#36827;&#21046;&#21270;&#36807;&#31243;&#20013;&#36807;&#28388;&#20102;&#19968;&#23567;&#37096;&#20998;&#26174;&#33879;&#26435;&#37325;&#65292;&#23558;&#23427;&#20204;&#20998;&#37197;&#21040;&#39640;&#20301;&#23384;&#20648;&#20013;&#65292;&#21363;&#37096;&#20998;&#20108;&#20540;&#21270;&#12290;PB-LLM&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#30340;&#35282;&#24230;&#20998;&#26512;&#21518;&#65292;&#25193;&#23637;&#20102;&#24674;&#22797;&#37327;&#21270;LLMM&#23481;&#37327;&#30340;&#33021;&#21147;&#12290;&#22312;PTQ&#19979;&#65292;&#32467;&#21512;&#20102;GPTQ&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#37325;&#26500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08872</link><description>&lt;p&gt;
PDFTriage: &#23545;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#36827;&#34892;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08872
&lt;/p&gt;
&lt;p&gt;
PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#38382;&#31572;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#25991;&#26723;&#26080;&#27861;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#23558;&#20854;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20687;PDF&#12289;&#32593;&#39029;&#21644;&#28436;&#31034;&#25991;&#31295;&#36825;&#26679;&#30340;&#25991;&#26723;&#26159;&#26377;&#32467;&#26500;&#30340;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#39029;&#30721;&#12289;&#34920;&#26684;&#12289;&#31456;&#33410;&#31561;&#12290;&#23558;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#19982;&#29992;&#25143;&#23545;&#36825;&#20123;&#20855;&#26377;&#20016;&#23500;&#32467;&#26500;&#30340;&#25991;&#26723;&#30340;&#35748;&#30693;&#27169;&#22411;&#19981;&#31526;&#12290;&#24403;&#31995;&#32479;&#38656;&#35201;&#20174;&#25991;&#26723;&#20013;&#26597;&#35810;&#19978;&#19979;&#25991;&#26102;&#65292;&#36825;&#31181;&#19981;&#31526;&#20250;&#26174;&#29616;&#20986;&#26469;&#65292;&#29978;&#33267;&#31616;&#21333;&#30340;&#38382;&#39064;&#20063;&#21487;&#33021;&#20351;&#38382;&#31572;&#31995;&#32479;&#20986;&#38169;&#12290;&#20026;&#20102;&#24357;&#21512;&#22788;&#29702;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#30340;&#22522;&#26412;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDFTriage&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#32467;&#26500;&#25110;&#20869;&#23481;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PDFTriage&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmente
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08646</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#35299;&#20915;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25512;&#26029;&#24615;&#33021;&#30340;&#22806;&#25512;&#21464;&#24471;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#34987;&#20043;&#21069;&#24573;&#35270;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#23548;&#33268;&#20102;&#26368;&#25509;&#36817;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#28151;&#20081;&#65292;&#36825;&#20123;&#26631;&#35760;&#25658;&#24102;&#20102;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#21457;&#29616;&#31216;&#20026;&#8220;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#32467;&#26500;&#65292;&#21629;&#21517;&#20026;Collinear Constrained Attention&#65288;CoCA&#65289;&#12290;&#36825;&#20010;&#32467;&#26500;&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#29616;&#26377;&#30340;&#25512;&#26029;&#12289;&#25554;&#20540;&#26041;&#27861;&#21644;&#20854;&#20182;&#38024;&#23545;&#20256;&#32479;Transformer&#27169;&#22411;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#20351;&#26159;16&#21040;24&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#19988;&#27809;&#26377;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;CoCA&#30340;&#35745;&#31639;&#21644;&#31354;&#38388;&#25928;&#29575;&#65292;&#20197;&#30830;&#20445;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;...
&lt;/p&gt;
&lt;p&gt;
As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
&lt;/p&gt;</description></item><item><title>TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.08637</link><description>&lt;p&gt;
TextBind: &#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08637
&lt;/p&gt;
&lt;p&gt;
TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#24403;&#28041;&#21450;&#21040;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TextBind&#65292;&#36825;&#26159;&#19968;&#20010;&#20960;&#20046;&#19981;&#38656;&#35201;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36171;&#20104;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#24182;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;</title><link>http://arxiv.org/abs/2309.08589</link><description>&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#26159;&#19968;&#31181;&#31574;&#30053;&#25913;&#36827;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Reasoning is a Policy Improvement Operator. (arXiv:2309.08589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08589
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#20196;&#20154;&#36190;&#21497;&#30340;&#26032;&#33021;&#21147;&#20196;&#19990;&#30028;&#20026;&#20043;&#24778;&#21497;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30446;&#21069;&#32570;&#20047;&#33258;&#25105;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#25509;&#21463;&#22823;&#37327;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SECToR&#65288;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#23454;&#29616;&#33258;&#25105;&#25945;&#32946;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#12290;&#21463;&#21040;&#20197;&#21069;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;Silver&#31561;&#20154;&#65292;2017&#65289;&#21644;&#20154;&#31867;&#35748;&#30693;&#65288;Kahneman&#65292;2011&#65289;&#20013;&#30340;&#30456;&#20851;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;SECToR&#39318;&#20808;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#36880;&#28176;&#24605;&#32771;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;SECToR&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#29983;&#25104;&#30456;&#21516;&#30340;&#31572;&#26696;&#65292;&#36825;&#27425;&#19981;&#20877;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#12290;&#36890;&#36807;SECToR&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#23398;&#20250;&#20102;&#36827;&#34892;&#22810;&#36798;29&#20301;&#25968;&#23383;&#30340;&#21152;&#27861;&#36816;&#31639;&#65292;&#32780;&#27809;&#26377;&#20219;&#20309;&#36229;&#36807;6&#20301;&#25968;&#23383;&#30340;&#22522;&#20934;&#30495;&#23454;&#31034;&#20363;&#65292;&#20165;&#36890;&#36807;&#21021;&#22987;&#30340;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Large language models have astounded the world with fascinating new capabilities. However, they currently lack the ability to teach themselves new skills, relying instead on being trained on large amounts of human-generated data. We introduce SECToR (Self-Education via Chain-of-Thought Reasoning), a proof-of-concept demonstration that language models can successfully teach themselves new skills using chain-of-thought reasoning. Inspired by previous work in both reinforcement learning (Silver et al., 2017) and human cognition (Kahneman, 2011), SECToR first uses chain-of-thought reasoning to slowly think its way through problems. SECToR then fine-tunes the model to generate those same answers, this time without using chain-of-thought reasoning. Language models trained via SECToR autonomously learn to add up to 29-digit numbers without any access to any ground truth examples beyond an initial supervised fine-tuning phase consisting only of numbers with 6 or fewer digits. Our central hypot
&lt;/p&gt;</description></item><item><title>ToddlerBERTa&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.16336</link><description>&lt;p&gt;
ToddlerBERTa: &#21033;&#29992;BabyBERTa&#36827;&#34892;&#35821;&#27861;&#23398;&#20064;&#21644;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding. (arXiv:2308.16336v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16336
&lt;/p&gt;
&lt;p&gt;
ToddlerBERTa&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ToddlerBERTa&#65292;&#36825;&#26159;&#19968;&#20010;&#31867;&#20284;&#20110;BabyBERTa&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20116;&#31181;&#19981;&#21516;&#30340;&#20855;&#26377;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#27169;&#22411;&#26469;&#25506;&#32034;&#20854;&#33021;&#21147;&#12290;&#22312;BLiMP&#65292;SuperGLUE&#65292;MSGS&#21644;BabyLM&#25361;&#25112;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#36739;&#22823;&#30340;&#27169;&#22411;&#22312;&#22823;&#37327;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#23613;&#31649;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;ToddlerBERTa&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;RoBERTa-base&#30456;&#23218;&#32654;&#12290;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#21363;&#20351;&#26159;&#22312;&#21333;&#21477;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#19982;&#21033;&#29992;&#26356;&#24191;&#27867;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22522;&#32447;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#25968;&#25454;&#21033;&#29992;&#25552;&#20379;&#20102;&#27934;&#23519;&#65292;&#24182;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ToddlerBERTa, a BabyBERTa-like language model, exploring its capabilities through five different models with varied hyperparameters. Evaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the BabyLM challenge, we find that smaller models can excel in specific tasks, while larger models perform well with substantial data. Despite training on a smaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling the state-of-the-art RoBERTa-base. The model showcases robust language understanding, even with single-sentence pretraining, and competes with baselines that leverage broader contextual information. Our work provides insights into hyperparameter choices, and data utilization, contributing to the advancement of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00158</link><description>&lt;p&gt;
&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#30340;&#23436;&#32654;&#36136;&#37327;&#27573;&#33853;&#65306;&#26159;&#21542;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#32534;&#36753;&#36317;&#31163;&#27169;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#65288;TQE&#65289;&#26159;&#23558;&#36755;&#20986;&#32763;&#35793;&#37096;&#32626;&#21040;&#20351;&#29992;&#20013;&#20043;&#21069;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290; TQE&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#30340;&#36136;&#37327;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#21442;&#32771;&#32763;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20026;TQE&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;Fine-Tune&#12290;&#25105;&#20204;&#20197;ChatGPT&#20026;&#20363;&#65292;&#23558;TQE&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;&#33521;&#24847;&#21644;&#33521;&#24503;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;ChatGPT&#30340;API Fine-Tuned&#21487;&#20197;&#22312;&#39044;&#27979;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#21363;&#26159;&#21542;&#38656;&#35201;&#32534;&#36753;&#32763;&#35793;&#65292;&#20294;&#32943;&#23450;&#26377;&#25913;&#36827;&#20934;&#30830;&#24615;&#30340;&#31354;&#38388;&#12290;&#33521;&#24847;&#21452;&#35821;&#25688;&#35201;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#30340;&#26041;&#27861;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#25552;&#20379;&#31283;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#21644;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.00113</link><description>&lt;p&gt;
&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#26041;&#27861;&#24041;&#22266;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Three Bricks to Consolidate Watermarks for Large Language Models. (arXiv:2308.00113v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#30340;&#26041;&#27861;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#25552;&#20379;&#31283;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#21644;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21028;&#26029;&#29983;&#25104;&#25991;&#26412;&#21644;&#33258;&#28982;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32972;&#26223;&#19979;&#65292;&#27700;&#21360;&#25216;&#26415;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#23558;&#29983;&#25104;&#25991;&#26412;&#24402;&#23646;&#20110;&#29305;&#23450;&#27169;&#22411;&#30340;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;&#23427;&#25913;&#21464;&#20102;&#37319;&#26679;&#29983;&#25104;&#36807;&#31243;&#65292;&#30041;&#19979;&#20102;&#26080;&#24418;&#30340;&#30165;&#36857;&#22312;&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#65292;&#20197;&#20415;&#20110;&#21518;&#32493;&#30340;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#19977;&#20010;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#29282;&#22266;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#65288;&#23567;&#20110;10^(-6)&#65289;&#65292;&#36825;&#20123;&#20445;&#35777;&#20381;&#28982;&#26377;&#25928;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20351;&#29992;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#23545;&#27604;&#20102;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#34892;&#24615;&#30340;&#35265;&#35299;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#21487;&#20197;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#26223;&#20197;&#21450;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing generated text to a specific model. It alters the sampling generation process so as to leave an invisible trace in the generated output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10$^{\text{-6}}$). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.15063</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#20219;&#21153;&#22810;&#26679;&#24615;&#19982;&#22238;&#24402;&#38382;&#39064;&#20013;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15063
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#38054;&#20329;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65288;ICL&#65289;&#65306;&#23427;&#20204;&#21487;&#20197;&#20174;&#20165;&#25552;&#20379;&#22312;&#25552;&#31034;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#26435;&#37325;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;ICL&#33021;&#22815;&#35299;&#20915;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#12289;&#22312;&#26412;&#36136;&#19978;&#19982;&#20043;&#21069;&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#30340;&#26032;&#20219;&#21153;&#21527;&#65311;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#25913;&#21464;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#30740;&#31350;&#20102;ICL&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20986;&#29616;ICL&#30340;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#12290;&#22312;&#36825;&#20010;&#38408;&#20540;&#20197;&#19979;&#65292;&#39044;&#35757;&#32451;&#30340;transformer&#26080;&#27861;&#35299;&#20915;&#26410;&#35265;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30340;&#34892;&#20026;&#31867;&#20284;&#20110;&#20855;&#26377;&#38750;&#22810;&#26679;&#24615;&#39044;&#35757;&#32451;&#20219;&#21153;&#20998;&#24067;&#20316;&#20026;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#12290;&#36229;&#36807;&#36825;&#20010;&#38408;&#20540;&#21518;&#65292;transformer&#26126;&#26174;&#20248;&#20110;&#36825;&#20010;&#20272;&#35745;&#22120;&#65307;&#23427;&#30340;&#34892;&#20026;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#65292;&#23545;$\textit{&#25152;&#26377;&#20219;&#21153;}$&#65292;&#21253;&#25324;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally $\textit{new}$ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a $\textit{task diversity threshold}$ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks as it behaves like a Bayesian estimator with the $\textit{non-diverse pretraining task distribution}$ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over $\textit{all tasks}$, including those not seen during pretraining. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#26041;&#27861;&#65288;CBR-MRC&#65289;&#65292;&#36890;&#36807;&#20174;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#30456;&#20284;&#26696;&#20363;&#24182;&#36873;&#25321;&#26368;&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21644;&#26032;&#38395;&#38382;&#31572;&#20013;&#65292;CBR-MRC&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#22522;&#20934;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#19982;&#20854;&#20182;&#35780;&#20272;&#21592;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14815</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Machine Reading Comprehension using Case-based Reasoning. (arXiv:2305.14815v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#26041;&#27861;&#65288;CBR-MRC&#65289;&#65292;&#36890;&#36807;&#20174;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#30456;&#20284;&#26696;&#20363;&#24182;&#36873;&#25321;&#26368;&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21644;&#26032;&#38395;&#38382;&#31572;&#20013;&#65292;CBR-MRC&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#22522;&#20934;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#19982;&#20854;&#20182;&#35780;&#20272;&#21592;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#31867;&#20284;&#20110;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;CBR-MRC&#65289;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#30456;&#20284;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#21270;&#31572;&#26696;&#24444;&#27492;&#20043;&#38388;&#20855;&#26377;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#38382;&#39064;&#65292;CBR-MRC&#39318;&#20808;&#20174;&#38750;&#21442;&#25968;&#21270;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#19968;&#32452;&#30456;&#20284;&#30340;&#26696;&#20363;&#65292;&#28982;&#21518;&#36890;&#36807;&#36873;&#25321;&#27979;&#35797;&#19978;&#19979;&#25991;&#20013;&#26368;&#31867;&#20284;&#20110;&#26816;&#32034;&#21040;&#30340;&#26696;&#20363;&#20013;&#19978;&#19979;&#25991;&#21270;&#31572;&#26696;&#34920;&#31034;&#30340;&#33539;&#22260;&#26469;&#39044;&#27979;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21322;&#21442;&#25968;&#21270;&#30340;&#29305;&#24615;&#20351;&#20854;&#33021;&#22815;&#23558;&#39044;&#27979;&#24402;&#22240;&#20110;&#29305;&#23450;&#30340;&#35777;&#25454;&#26696;&#20363;&#38598;&#65292;&#22240;&#27492;&#22312;&#26500;&#24314;&#21487;&#38752;&#19988;&#21487;&#35843;&#35797;&#30340;&#38382;&#31572;&#31995;&#32479;&#26102;&#26159;&#19968;&#20010;&#29702;&#24819;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CBR-MRC&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65288;NaturalQuestions&#65289;&#21644;&#26032;&#38395;&#38382;&#31572;&#65288;NewsQA&#65289;&#19978;&#27604;&#22823;&#22411;&#35835;&#32773;&#27169;&#22411;&#25552;&#20379;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20248;&#20110;&#22522;&#20934;&#20998;&#21035;&#25552;&#21319;&#20102;11.5&#21644;8.4 EM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CBR-MRC&#22312;&#35782;&#21035;&#19982;&#20182;&#20154;&#35780;&#20272;&#21592;&#19981;&#21516;&#30340;&#31572;&#26696;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an accurate and interpretable method for answer extraction in machine reading comprehension that is reminiscent of case-based reasoning (CBR) from classical AI. Our method (CBR-MRC) builds upon the hypothesis that contextualized answers to similar questions share semantic similarities with each other. Given a test question, CBR-MRC first retrieves a set of similar cases from a non-parametric memory and then predicts an answer by selecting the span in the test context that is most similar to the contextualized representations of answers in the retrieved cases. The semi-parametric nature of our approach allows it to attribute a prediction to the specific set of evidence cases, making it a desirable choice for building reliable and debuggable QA systems. We show that CBR-MRC provides high accuracy comparable with large reader models and outperforms baselines by 11.5 and 8.4 EM on NaturalQuestions and NewsQA, respectively. Further, we demonstrate the ability of CBR-MRC in identi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#12289;GPT-2&#21644;T5&#36825;&#19977;&#31181;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#37327;&#21270;fine-tuning&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#19982;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#24046;&#24322;&#26469;&#25506;&#31350;&#27169;&#22411;&#30340;&#21464;&#21270;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.14453</link><description>&lt;p&gt;
&#20851;&#20110;Transformer-based NLP&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Robustness of Finetuned Transformer-based NLP Models. (arXiv:2305.14453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#12289;GPT-2&#21644;T5&#36825;&#19977;&#31181;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#37327;&#21270;fine-tuning&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#19982;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#24046;&#24322;&#26469;&#25506;&#31350;&#27169;&#22411;&#30340;&#21464;&#21270;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;BERT&#12289;GPT-2&#21644;T5&#24050;&#32463;&#34987;&#29992;&#20110;&#22823;&#37327;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;fine-tuning&#65292;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;fine-tuning&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#21508;&#23618;&#19982;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#30456;&#27604;&#30340;&#21464;&#21270;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#25991;&#26412;&#36755;&#20837;&#30340;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#22914;&#20309;&#65311;&#36825;&#31181;&#40065;&#26834;&#24615;&#26159;&#21542;&#22240;&#27169;&#22411;fine-tuning&#30340;NLP&#20219;&#21153;&#32780;&#24322;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#12289;GPT-2&#21644;T5&#65289;&#22312;General Language Understanding Evaluation&#65288;GLUE&#65289;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;&#20843;&#31181;&#19981;&#21516;&#25991;&#26412;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#65288;CKA&#21644;STIR&#65289;&#26469;&#37327;&#21270;fine-tuning&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#19982;&#39044;&#35757;&#32451;&#34920;&#31034;&#20043;&#38388;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pretrained models like BERT, GPT-2 and T5 have been finetuned for a large number of natural language processing (NLP) tasks, and have been shown to be very effective. However, while finetuning, what changes across layers in these models with respect to pretrained checkpoints is under-studied. Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned? While there exists some work on studying robustness of BERT finetuned for a few NLP tasks, there is no rigorous study which compares this robustness across encoder only, decoder only and encoder-decoder models.  In this paper, we study the robustness of three language models (BERT, GPT-2 and T5) with eight different text perturbations on the General Language Understanding Evaluation (GLUE) benchmark. Also, we use two metrics (CKA and STIR) to quantify changes between pretrained and finetuned language model representation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;Hierarchical Crossmodal Transformer with Modality Gating(HCT-MG)&#27169;&#22411;&#26469;&#30830;&#23450;&#20027;&#35201;&#27169;&#24577;&#24182;&#20998;&#23618;&#34701;&#21512;&#36741;&#21161;&#27169;&#24577;&#65292;&#26377;&#25928;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13583</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#19981;&#36275;&#65306;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#19982;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Attention is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition. (arXiv:2305.13583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;Hierarchical Crossmodal Transformer with Modality Gating(HCT-MG)&#27169;&#22411;&#26469;&#30830;&#23450;&#20027;&#35201;&#27169;&#24577;&#24182;&#20998;&#23618;&#34701;&#21512;&#36741;&#21161;&#27169;&#24577;&#65292;&#26377;&#25928;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34701;&#21512;&#22312;&#24773;&#24863;&#35745;&#31639;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#23545;&#24615;&#33021;&#30340;&#25552;&#21319;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26426;&#29702;&#23578;&#19981;&#28165;&#26970;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#23427;&#36890;&#24120;&#20250;&#23548;&#33268;&#22823;&#22411;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#24773;&#24863;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#39318;&#20808;&#20998;&#26512;&#20102;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#20013;&#19968;&#20010;&#27169;&#24577;&#20013;&#31361;&#20986;&#30340;&#24773;&#24863;&#20449;&#24687;&#22914;&#20309;&#21463;&#21040;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#36328;&#27169;&#24577;&#30340;&#20851;&#27880;&#65292;&#27169;&#24577;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;(HCT-MG)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#20132;&#21449;&#27169;&#24577;Transformer&#19982;&#27169;&#24577;&#38376;&#25511;&#21046;&#26469;&#30830;&#23450;&#20027;&#35201;&#30340;&#27169;&#24577;&#65292;&#24182;&#20998;&#23618;&#22320;&#23558;&#36741;&#21161;&#27169;&#24577;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#24182;&#20943;&#23569;&#20449;&#24687;&#20887;&#20313;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;CMU-MOSI&#12289;CMU-MOSEI&#21644;IEMOCAP&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#65306;1&#65289;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65307;2&#65289;&#23427;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#36229;&#21442;&#25968;&#21644;&#21442;&#25968;&#65307;3&#65289;&#23427;&#30340;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusing multiple modalities for affective computing tasks has proven effective for performance improvement. However, how multimodal fusion works is not well understood, and its use in the real world usually results in large model sizes. In this work, on sentiment and emotion analysis, we first analyze how the salient affective information in one modality can be affected by the other in crossmodal attention. We find that inter-modal incongruity exists at the latent level due to crossmodal attention. Based on this finding, we propose a lightweight model via Hierarchical Crossmodal Transformer with Modality Gating (HCT-MG), which determines a primary modality according to its contribution to the target task and then hierarchically incorporates auxiliary modalities to alleviate inter-modal incongruity and reduce information redundancy. The experimental evaluation on three benchmark datasets: CMU-MOSI, CMU-MOSEI, and IEMOCAP verifies the efficacy of our approach, showing that it: 1) outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;AVeriTeC&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;4,568&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20027;&#24352;&#65292;&#27599;&#20010;&#20027;&#24352;&#37117;&#26377;&#22312;&#32447;&#35777;&#25454;&#25903;&#25345;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#21644;&#25991;&#26412;&#35777;&#26126;&#35299;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36991;&#20813;&#19968;&#20123;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26041;&#26696;&#26469;&#23545;&#24320;&#25918;&#32593;&#32476;&#19978;&#30340;&#20027;&#24352;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.13117</link><description>&lt;p&gt;
AVeriTeC: &#19968;&#20010;&#21253;&#21547;&#32593;&#32476;&#35777;&#25454;&#30340;&#30495;&#23454;&#19990;&#30028;&#20027;&#24352;&#39564;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web. (arXiv:2305.13117v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AVeriTeC&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;4,568&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20027;&#24352;&#65292;&#27599;&#20010;&#20027;&#24352;&#37117;&#26377;&#22312;&#32447;&#35777;&#25454;&#25903;&#25345;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#21644;&#25991;&#26412;&#35777;&#26126;&#35299;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36991;&#20813;&#19968;&#20123;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26041;&#26696;&#26469;&#23545;&#24320;&#25918;&#32593;&#32476;&#19978;&#30340;&#20027;&#24352;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#33258;&#21160;&#20107;&#23454;&#26816;&#26597;&#25968;&#25454;&#38598;&#23384;&#22312;&#37325;&#22823;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#20381;&#36182;&#20154;&#24037;&#20027;&#24352;&#12289;&#32570;&#20047;&#35777;&#25454;&#21644;&#20013;&#38388;&#25512;&#29702;&#27880;&#37322;&#65292;&#25110;&#21253;&#25324;&#21457;&#24067;&#20027;&#24352;&#21518;&#30340;&#35777;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; AVeriTeC&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; 4,568 &#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20027;&#24352;&#65292;&#28085;&#30422;&#20102;50&#20010;&#19981;&#21516;&#32452;&#32455;&#30340;&#20107;&#23454;&#26816;&#26597;&#12290;&#27599;&#20010;&#20027;&#24352;&#37117;&#29992;&#22312;&#32447;&#21487;&#29992;&#35777;&#25454;&#25903;&#25345;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#21450;&#25991;&#26412;&#30340;&#35777;&#26126;&#35299;&#37322;&#35777;&#25454;&#22914;&#20309;&#30456;&#20114;&#32467;&#21512;&#20135;&#29983;&#32467;&#35770;&#12290;&#36890;&#36807;&#22810;&#36718;&#27880;&#37322;&#36807;&#31243;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#12289;&#35777;&#25454;&#19981;&#36275;&#21644;&#26102;&#38388;&#27844;&#28431;&#65292;&#24182;&#22312;&#32467;&#35770;&#19978;&#36798;&#25104;&#20102;&#30456;&#24403;&#22823;&#30340;&#27880;&#37322;&#21592;&#21327;&#35758; $\kappa=0.619$&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#32447;&#20197;&#21450;&#19968;&#20010;&#26041;&#27861;&#26469;&#39564;&#35777;&#36890;&#36807;&#23545;&#24320;&#25918;&#32593;&#32476;&#36827;&#34892;&#20960;&#20010;&#38382;&#39064;&#22238;&#31572;&#27493;&#39588;&#26469;&#26816;&#26597;&#20027;&#24352;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing datasets for automated fact-checking have substantial limitations, such as relying on artificial claims, lacking annotations for evidence and intermediate reasoning, or including evidence published after the claim. In this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims covering fact-checks by 50 different organizations. Each claim is annotated with question-answer pairs supported by evidence available online, as well as textual justifications explaining how the evidence combines to produce a verdict. Through a multi-round annotation process, we avoid common pitfalls including context dependence, evidence insufficiency, and temporal leakage, and reach a substantial inter-annotator agreement of $\kappa=0.619$ on verdicts. We develop a baseline as well as an evaluation scheme for verifying claims through several question-answering steps against the open web.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#23454;&#39564;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#27604;&#36739;LLMs&#21644;&#20799;&#31461;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#20154;&#31867;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11243</link><description>&lt;p&gt;
&#27604;&#36739;&#26426;&#22120;&#21644;&#20799;&#31461;&#65306;&#20351;&#29992;&#21457;&#23637;&#24515;&#29702;&#23398;&#23454;&#39564;&#35780;&#20272;LaMDA&#21709;&#24212;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;
&lt;/p&gt;
&lt;p&gt;
Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses. (arXiv:2305.11243v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11243
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#23454;&#39564;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#27604;&#36739;LLMs&#21644;&#20799;&#31461;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#20154;&#31867;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#33457;&#36153;&#20102;&#20960;&#21313;&#24180;&#30340;&#26102;&#38388;&#35774;&#35745;&#23454;&#39564;&#26469;&#27979;&#35797;&#23156;&#20799;&#21644;&#20799;&#31461;&#30340;&#26234;&#21147;&#21644;&#30693;&#35782;&#65292;&#36861;&#28335;&#37325;&#35201;&#27010;&#24565;&#21644;&#33021;&#21147;&#30340;&#36215;&#28304;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#30340;&#32463;&#20856;&#23454;&#39564;&#26159;&#25506;&#31350;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;LLM&#27169;&#22411;&#30340;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#20043;&#19968;&#12290;&#20854;&#27425;&#65292;&#23558;LLM&#19982;&#20799;&#31461;&#36827;&#34892;&#27604;&#36739;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#26377;&#20154;&#31867;&#29305;&#28857;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#23884;&#20837;&#21040;&#38656;&#35201;&#19982;&#20154;&#20132;&#20114;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developmental psychologists have spent decades devising experiments to test the intelligence and knowledge of infants and children, tracing the origin of crucial concepts and capacities. Moreover, experimental techniques in developmental psychology have been carefully designed to discriminate the cognitive capacities that underlie particular behaviors. We propose that using classical experiments from child development is a particularly effective way to probe the computational abilities of AI models, in general, and LLMs in particular. First, the methodological techniques of developmental psychology, such as the use of novel stimuli to control for past experience or control conditions to determine whether children are using simple associations, can be equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs in this way can tell us whether the information that is encoded in text is sufficient to enable particular responses, or whether those responses depend on othe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;100&#31687;&#39640;&#34987;&#24341;&#35770;&#25991;&#65292;&#21457;&#29616;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#30340;&#20351;&#29992;&#24448;&#24448;&#32570;&#20047;&#20805;&#20998;&#35752;&#35770;&#12290;&#25991;&#31456;&#25552;&#20986;&#32570;&#20047;&#20805;&#20998;&#35752;&#35770;&#30340;&#29616;&#35937;&#40723;&#21169;&#20102;&#36807;&#24230;&#23459;&#20256;&#65292;&#38480;&#21046;&#20102;&#25209;&#35780;&#24182;&#38459;&#27490;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#21453;&#39304;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#20351;&#29992;&#30340;&#20960;&#39033;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.14238</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#30340;&#39044;&#26399;&#29992;&#36884;&#65306;&#20026;&#20160;&#20040;&#12289;&#22914;&#20309;&#20197;&#21450;&#35841;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who. (arXiv:2304.14238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;100&#31687;&#39640;&#34987;&#24341;&#35770;&#25991;&#65292;&#21457;&#29616;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#30340;&#20351;&#29992;&#24448;&#24448;&#32570;&#20047;&#20805;&#20998;&#35752;&#35770;&#12290;&#25991;&#31456;&#25552;&#20986;&#32570;&#20047;&#20805;&#20998;&#35752;&#35770;&#30340;&#29616;&#35937;&#40723;&#21169;&#20102;&#36807;&#24230;&#23459;&#20256;&#65292;&#38480;&#21046;&#20102;&#25209;&#35780;&#24182;&#38459;&#27490;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#21453;&#39304;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#20351;&#29992;&#30340;&#20960;&#39033;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#32463;&#24120;&#34987;&#21576;&#29616;&#20026;&#19968;&#20010;&#30693;&#35782;&#24037;&#20855;&#65292;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#31038;&#20132;&#23186;&#20307;&#28040;&#36153;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#25171;&#20987;&#38169;&#35823;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#35770;&#25991;&#28145;&#20837;&#35752;&#35770;&#22914;&#20309;&#20351;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;100&#31687;&#39640;&#34987;&#24341;&#29992;&#35770;&#25991;&#24182;&#27880;&#37322;&#19982;&#39044;&#26399;&#20351;&#29992;&#30456;&#20851;&#30340;&#35748;&#30693;&#20803;&#32032;&#65288;&#21363;&#25163;&#27573;&#12289;&#30446;&#30340;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#65289;&#26469;&#35760;&#24405;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24573;&#30053;&#20854;&#20013;&#19968;&#20123;&#26041;&#38754;&#30340;&#21465;&#20107;&#24456;&#26222;&#36941;&#65292;&#35768;&#22810;&#35770;&#25991;&#25552;&#20986;&#30340;&#25163;&#27573;&#21644;&#30446;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#25512;&#33616;&#31574;&#30053;&#30340;&#21487;&#34892;&#24615;&#24456;&#23569;&#20855;&#26377;&#23454;&#35777;&#25903;&#25345;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#27169;&#31946;&#24615;&#20027;&#21160;&#38459;&#30861;&#20102;&#25216;&#26415;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#40723;&#21169;&#20102;&#36807;&#24230;&#23459;&#20256;&#65292;&#38480;&#21046;&#20102;&#25209;&#35780;&#24182;&#38459;&#27490;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#21453;&#39304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#39033;&#20851;&#20110;&#24605;&#32771;&#21644;&#25776;&#20889;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#20351;&#29992;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated fact-checking is often presented as an epistemic tool that fact-checkers, social media consumers, and other stakeholders can use to fight misinformation. Nevertheless, few papers thoroughly discuss how. We document this by analysing 100 highly-cited papers, and annotating epistemic elements related to intended use, i.e., means, ends, and stakeholders. We find that narratives leaving out some of these aspects are common, that many papers propose inconsistent means and ends, and that the feasibility of suggested strategies rarely has empirical backing. We argue that this vagueness actively hinders the technology from reaching its goals, as it encourages overclaiming, limits criticism, and prevents stakeholder feedback. Accordingly, we provide several recommendations for thinking and writing about the use of fact-checking artefacts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;</title><link>http://arxiv.org/abs/2303.15413</link><description>&lt;p&gt;
2D&#25193;&#25955;&#31639;&#27861;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. (arXiv:2303.15413v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#20986;&#29616;&#30340;&#35270;&#35282;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20063;&#31216;&#20026;Janus&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#26469;&#33258;&#20110;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#22266;&#26377;&#20559;&#32622;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;3D&#23545;&#35937;&#19981;&#30495;&#23454;&#12290;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21435;&#38500;&#20559;&#32622;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#21483;&#20570;score debiasing&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#26469;&#36798;&#21040;&#21435;&#38500;&#20559;&#32622;&#30340;&#25928;&#26524;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#21483;&#20570;prompt debiasing&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30830;&#23450;&#29992;&#25143;&#25552;&#31034;&#21644;&#35270;&#35282;&#25552;&#31034;&#20043;&#38388;&#30340;&#30683;&#30462;&#35789;&#35821;&#65292;&#24182;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#20102;&#30495;&#23454;&#24863;&#65292;&#24182;&#22312;&#36136;&#37327;&#19982;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The view inconsistency problem in score-distilling text-to-3D generation, also known as the Janus problem, arises from the intrinsic bias of 2D diffusion models, which leads to the unrealistic generation of 3D objects. In this work, we explore score-distilling text-to-3D generation and identify the main causes of the Janus problem. Based on these findings, we propose two approaches to debias the score-distillation frameworks for robust text-to-3D generation. Our first approach, called score debiasing, involves gradually increasing the truncation value for the score estimated by 2D diffusion models throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts utilizing a language model and adjusts the discrepancy between view prompts and object-space camera poses. Our experimental results show that our methods improve realism by significantly reducing artifacts and achieve a good trade-off between fa
&lt;/p&gt;</description></item><item><title>MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.05981</link><description>&lt;p&gt;
MarioGPT: &#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25991;&#26412;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05981
&lt;/p&gt;
&lt;p&gt;
MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#22797;&#26434;&#25968;&#19968;&#33268;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#21453;&#26144;&#29305;&#23450;&#24847;&#22270;&#21644;&#38480;&#21046;&#30340;&#26377;&#24847;&#20041;&#20869;&#23481;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#32570;&#20047;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#35757;&#32451;&#26377;&#32032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#65292;&#37325;&#22797;&#20351;&#29992;&#20449;&#24687;&#24182;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#22521;&#35757;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MarioGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;GPT2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#29943;&#30742;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#25105;&#20204;&#20197;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#30340;&#20851;&#21345;&#20026;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MarioGPT&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#25511;&#21046;&#20851;&#21345;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;PCG&#25216;&#26415;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#20851;&#21345;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We a
&lt;/p&gt;</description></item></channel></rss>