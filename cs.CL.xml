<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Chat2Brain&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#26412;&#30340;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#24320;&#25918;&#22411;&#35821;&#20041;&#26597;&#35810;&#26144;&#23556;&#21040;&#33041;&#37096;&#28608;&#27963;&#22270;&#12290;&#23427;&#35299;&#20915;&#20102;&#20803;&#20998;&#26512;&#20013;&#23384;&#22312;&#30340;&#35821;&#20041;&#20887;&#20313;&#21644;&#27495;&#20041;&#23548;&#33268;&#26144;&#23556;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05021</link><description>&lt;p&gt;
Chat2Brain&#65306;&#19968;&#31181;&#23558;&#24320;&#25918;&#22411;&#35821;&#20041;&#26597;&#35810;&#26144;&#23556;&#21040;&#33041;&#37096;&#28608;&#27963;&#22270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chat2Brain: A Method for Mapping Open-Ended Semantic Queries to Brain Activation Maps. (arXiv:2309.05021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05021
&lt;/p&gt;
&lt;p&gt;
Chat2Brain&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#26412;&#30340;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#24320;&#25918;&#22411;&#35821;&#20041;&#26597;&#35810;&#26144;&#23556;&#21040;&#33041;&#37096;&#28608;&#27963;&#22270;&#12290;&#23427;&#35299;&#20915;&#20102;&#20803;&#20998;&#26512;&#20013;&#23384;&#22312;&#30340;&#35821;&#20041;&#20887;&#20313;&#21644;&#27495;&#20041;&#23548;&#33268;&#26144;&#23556;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#31070;&#32463;&#31185;&#23398;&#22312;&#25991;&#26412;&#27169;&#24577;&#19979;&#31215;&#32047;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#21487;&#20197;&#29992;&#20110;&#25506;&#32034;&#35748;&#30693;&#36807;&#31243;&#12290;&#20803;&#20998;&#26512;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#21033;&#29992;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#20174;&#25991;&#26412;&#26597;&#35810;&#21040;&#33041;&#37096;&#28608;&#27963;&#22270;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#65292;&#20294;&#23427;&#20173;&#28982;&#20381;&#36182;&#20110;&#29702;&#24819;&#30340;&#26597;&#35810;&#29615;&#22659;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#20803;&#20998;&#26512;&#30340;&#25991;&#26412;&#26597;&#35810;&#21487;&#33021;&#20250;&#36935;&#21040;&#35821;&#20041;&#20887;&#20313;&#21644;&#27495;&#20041;&#31561;&#38382;&#39064;&#65292;&#23548;&#33268;&#23545;&#33041;&#22270;&#30340;&#26144;&#23556;&#19981;&#20934;&#30830;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#19982;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;LLMs&#21487;&#20197;&#25913;&#36827;&#25991;&#26412;&#27169;&#24577;&#19982;&#31070;&#32463;&#31185;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35299;&#20915;&#20803;&#20998;&#26512;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Chat2Brain&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#19982;&#22522;&#26412;&#30340;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;Text2Brain&#30456;&#32467;&#21512;&#65292;&#20197;&#26144;&#23556;&#24320;&#25918;&#24335;&#35821;&#20041;&#26597;&#35810;&#21040;&#33041;&#37096;&#28608;&#27963;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over decades, neuroscience has accumulated a wealth of research results in the text modality that can be used to explore cognitive processes. Meta-analysis is a typical method that successfully establishes a link from text queries to brain activation maps using these research results, but it still relies on an ideal query environment. In practical applications, text queries used for meta-analyses may encounter issues such as semantic redundancy and ambiguity, resulting in an inaccurate mapping to brain images. On the other hand, large language models (LLMs) like ChatGPT have shown great potential in tasks such as context understanding and reasoning, displaying a high degree of consistency with human natural language. Hence, LLMs could improve the connection between text modality and neuroscience, resolving existing challenges of meta-analyses. In this study, we propose a method called Chat2Brain that combines LLMs to basic text-2-image model, known as Text2Brain, to map open-ended sema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;FOLLOWUPQG&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.05007</link><description>&lt;p&gt;
FOLLOWUPQG:&#38754;&#21521;&#20449;&#24687;&#33719;&#21462;&#30340;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation. (arXiv:2309.05007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;FOLLOWUPQG&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20986;&#20110;&#22909;&#22855;&#24515;&#32780;&#25552;&#20986;&#36319;&#36827;&#38382;&#39064;&#65292;&#36825;&#21453;&#26144;&#20102;&#20154;&#31867;&#21019;&#36896;&#24615;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#65288;FQG&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#29983;&#25104;&#33021;&#22815;&#26356;&#28145;&#20837;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;FOLLOWUPQG&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;Reddit&#35770;&#22363;&#30340;&#36229;&#36807;3K&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#65288;&#21021;&#22987;&#38382;&#39064;&#65292;&#31572;&#26696;&#65292;&#36319;&#36827;&#38382;&#39064;&#65289;&#20803;&#32452;&#65292;&#25552;&#20379;&#20102;&#23545;&#24320;&#25918;&#24615;&#38382;&#39064;&#30340;&#38750;&#19987;&#19994;&#20154;&#22763;&#21451;&#22909;&#30340;&#35299;&#37322;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;FOLLOWUPQG&#20013;&#30340;&#38382;&#39064;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#30340;&#23454;&#29992;&#31574;&#30053;&#26469;&#23547;&#27714;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#39640;&#23618;&#27425;&#30340;&#35748;&#30693;&#25216;&#33021;&#65288;&#22914;&#24212;&#29992;&#21644;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#25506;&#32034;&#22914;&#20309;&#22522;&#20110;&#36880;&#27493;&#28436;&#31034;&#29983;&#25104;&#29305;&#23450;&#31867;&#22411;&#30340;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;FOLLOWUPQG&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans ask follow-up questions driven by curiosity, which reflects a creative human cognitive process. We introduce the task of real-world information-seeking follow-up question generation (FQG), which aims to generate follow-up questions seeking a more in-depth understanding of an initial question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world (initial question, answer, follow-up question) tuples collected from a Reddit forum providing layman-friendly explanations for open-ended questions. In contrast to existing datasets, questions in FOLLOWUPQG use more diverse pragmatic strategies to seek information, and they also show higher-order cognitive skills (such as applying and relating). We evaluate current question generation models on their efficacy for generating follow-up questions, exploring how to generate specific types of follow-up questions based on step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging benchmark, as model-generated q
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#20943;&#36731;&#22522;&#20110;&#25552;&#31034;&#30340;&#20998;&#31867;&#22120;&#20013;&#30340;&#35789;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#31867;&#21035;&#20808;&#39564;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04992</link><description>&lt;p&gt;
&#20943;&#36731;&#38646;&#26679;&#26412;&#22522;&#20110;&#25552;&#31034;&#30340;&#20998;&#31867;&#22120;&#20013;&#30340;&#35789;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating Word Bias in Zero-shot Prompt-based Classifiers. (arXiv:2309.04992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04992
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#20943;&#36731;&#22522;&#20110;&#25552;&#31034;&#30340;&#20998;&#31867;&#22120;&#20013;&#30340;&#35789;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#31867;&#21035;&#20808;&#39564;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#20998;&#31867;&#22120;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25552;&#31034;&#27169;&#26495;&#21644;&#26631;&#31614;&#35789;&#30340;&#31934;&#30830;&#36873;&#25321;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#35821;&#20041;&#19978;&#31561;&#25928;&#30340;&#35774;&#32622;&#32463;&#24120;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#37096;&#20998;&#21487;&#20197;&#24402;&#22240;&#20110;&#35789;&#20559;&#24046;&#65292;&#20854;&#20013;&#20998;&#31867;&#22120;&#21487;&#33021;&#23545;&#26576;&#20123;&#31867;&#21035;&#26377;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#26377;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#20248;&#21270;&#20998;&#31867;&#38408;&#20540;&#65292;&#20294;&#36825;&#20063;&#20943;&#24369;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#20998;&#31867;&#22120;&#30340;&#26576;&#20123;&#20248;&#21183;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31867;&#21035;&#30340;&#26399;&#26395;&#36793;&#38469;&#27010;&#29575;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#27010;&#29575;&#34987;&#37325;&#26032;&#21152;&#26435;&#65292;&#20197;&#23454;&#29616;&#31867;&#21035;&#20043;&#38388;&#30340;&#32479;&#19968;&#20808;&#39564;&#65292;&#22312;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#36827;&#34892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#31867;&#21035;&#20808;&#39564;&#21644;&#35821;&#35328;&#27169;&#22411;&#23383;&#20808;&#39564;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20197;&#38646;&#36164;&#28304;&#26041;&#24335;&#35774;&#32622;&#38408;&#20540;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21305;&#37197;&#30340;&#31867;&#21035;&#20808;&#39564;&#19982;&#23454;&#38469;&#20998;&#31867;&#27010;&#29575;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based classifiers are an attractive approach for zero-shot classification. However, the precise choice of the prompt template and label words can largely influence performance, with semantically equivalent settings often showing notable performance difference. This discrepancy can be partly attributed to word biases, where the classifier may be biased towards classes. To address this problem, it is possible to optimise classification thresholds on a labelled data set, however, this mitigates some of the advantages of prompt-based classifiers. This paper instead approaches this problem by examining the expected marginal probabilities of the classes. Here, probabilities are reweighted to have a uniform prior over classes, in an unsupervised fashion. Further, we draw a theoretical connection between the class priors and the language models' word prior, and offer the ability to set a threshold in a zero-resource fashion. We show that matching class priors correlates strongly with th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#20803;&#23398;&#20064;&#65288;RAML&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#20351;&#29992;&#21442;&#25968;&#21270;&#36827;&#34892;&#25512;&#29702;&#65292;&#36824;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#20803;&#23398;&#20064;&#20013;&#32570;&#20047;&#22810;&#26679;&#24615;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04979</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#26816;&#32034;&#22686;&#24378;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Meta Learning for Low-Resource Text Classification. (arXiv:2309.04979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#20803;&#23398;&#20064;&#65288;RAML&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#20351;&#29992;&#21442;&#25968;&#21270;&#36827;&#34892;&#25512;&#29702;&#65292;&#36824;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#20803;&#23398;&#20064;&#20013;&#32570;&#20047;&#22810;&#26679;&#24615;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#22312;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#36825;&#20010;&#20219;&#21153;&#26088;&#22312;&#20174;&#28304;&#31867;&#21035;&#20013;&#30340;&#23567;&#20219;&#21153;&#38598;&#21512;&#65288;&#34987;&#31216;&#20026;episodes&#65289;&#20013;&#20256;&#36882;&#30693;&#35782;&#26469;&#35782;&#21035;&#30446;&#26631;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20803;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#21644;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#27867;&#21270;&#24615;&#33021;&#24046;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#20803;&#23398;&#20064;&#65288;RAML&#65289;&#12290;&#23427;&#19981;&#20165;&#20351;&#29992;&#21442;&#25968;&#21270;&#36827;&#34892;&#25512;&#29702;&#65292;&#36824;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#65292;&#22823;&#22823;&#32531;&#35299;&#20102;&#30001;&#20110;&#20803;&#23398;&#20064;&#20013;&#32570;&#20047;&#22810;&#26679;&#24615;&#35757;&#32451;&#25968;&#25454;&#32780;&#23548;&#33268;&#30340;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#21516;&#20110;&#20043;&#21069;&#20165;&#20381;&#36182;&#20110;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#23427;&#26126;&#30830;&#24378;&#35843;&#20102;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#65292;&#26088;&#22312;&#22312;&#21442;&#25968;&#21270;&#21644;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta learning have achieved promising performance in low-resource text classification which aims to identify target classes with knowledge transferred from source classes with sets of small tasks named episodes. However, due to the limited training data in the meta-learning scenario and the inherent properties of parameterized neural networks, poor generalization performance has become a pressing problem that needs to be addressed. To deal with this issue, we propose a meta-learning based method called Retrieval-Augmented Meta Learning(RAML). It not only uses parameterization for inference but also retrieves non-parametric knowledge from an external corpus to make inferences, which greatly alleviates the problem of poor generalization performance caused by the lack of diverse training data in meta-learning. This method differs from previous models that solely rely on parameters, as it explicitly emphasizes the importance of non-parametric knowledge, aiming to strike a balance between p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#39044;&#35757;&#32451;BERT&#21644;&#21477;&#27861;&#20851;&#31995;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;RGAT&#65289;&#30340;&#31471;&#21040;&#31471;&#35299;&#26512;&#22120;&#65292;&#20197;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#21477;&#27861;&#20381;&#36182;&#20449;&#24687;&#22312;&#25351;&#20195;&#28040;&#35299;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#23545;&#21477;&#27861;&#20381;&#36182;&#22270;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#20808;&#21069;&#26368;&#20339;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.04977</link><description>&lt;p&gt;
RGAT&#65306;&#26356;&#28145;&#20837;&#25506;&#32034;&#21477;&#27861;&#20381;&#36182;&#20449;&#24687;&#22312;&#25351;&#20195;&#28040;&#35299;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
RGAT: A Deeper Look into Syntactic Dependency Information for Coreference Resolution. (arXiv:2309.04977v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#39044;&#35757;&#32451;BERT&#21644;&#21477;&#27861;&#20851;&#31995;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;RGAT&#65289;&#30340;&#31471;&#21040;&#31471;&#35299;&#26512;&#22120;&#65292;&#20197;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#21477;&#27861;&#20381;&#36182;&#20449;&#24687;&#22312;&#25351;&#20195;&#28040;&#35299;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#23545;&#21477;&#27861;&#20381;&#36182;&#22270;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#20808;&#21069;&#26368;&#20339;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21477;&#27861;&#20449;&#24687;&#23545;&#24456;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#23558;&#20854;&#19982;&#35789;&#35821;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#25351;&#20195;&#28040;&#35299;&#38382;&#39064;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#39044;&#35757;&#32451;BERT&#21644;&#21477;&#27861;&#20851;&#31995;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;RGAT&#65289;&#30340;&#31471;&#21040;&#31471;&#35299;&#26512;&#22120;&#65292;&#20197;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#21477;&#27861;&#20381;&#36182;&#20449;&#24687;&#22312;&#25351;&#20195;&#28040;&#35299;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;RGAT&#27169;&#22411;&#65292;&#28982;&#21518;&#29992;&#20110;&#29702;&#35299;&#21477;&#27861;&#20381;&#36182;&#22270;&#24182;&#23398;&#20064;&#26356;&#22909;&#30340;&#20219;&#21153;&#29305;&#23450;&#21477;&#27861;&#23884;&#20837;&#12290;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#32467;&#26500;&#65292;&#23558;BERT&#23884;&#20837;&#21644;&#21477;&#27861;&#23884;&#20837;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#19979;&#28216;&#20219;&#21153;&#29983;&#25104;&#28151;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#20849;&#30340;Gendered Ambiguous Pronouns&#65288;GAP&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#23545;&#21477;&#27861;&#20381;&#36182;&#22270;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#21516;&#26102;&#65292;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#20808;&#21069;&#26368;&#20339;&#27169;&#22411;&#65288;RGCN-wi&#65289;&#30340;F1&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Although syntactic information is beneficial for many NLP tasks, combining it with contextual information between words to solve the coreference resolution problem needs to be further explored. In this paper, we propose an end-to-end parser that combines pre-trained BERT with a Syntactic Relation Graph Attention Network (RGAT) to take a deeper look into the role of syntactic dependency information for the coreference resolution task. In particular, the RGAT model is first proposed, then used to understand the syntactic dependency graph and learn better task-specific syntactic embeddings. An integrated architecture incorporating BERT embeddings and syntactic embeddings is constructed to generate blending representations for the downstream task. Our experiments on a public Gendered Ambiguous Pronouns (GAP) dataset show that with the supervision learning of the syntactic dependency graph and without fine-tuning the entire BERT, we increased the F1-score of the previous best model (RGCN-wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#35760;&#24518;&#21407;&#22411;&#36827;&#34892;&#24191;&#20041;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#31867;&#22686;&#37327;&#23398;&#20064;&#33539;&#24335;&#26469;&#21516;&#26102;&#20998;&#31867;&#24050;&#30693;&#21644;&#26032;&#24847;&#22270;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#24191;&#20041;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#20013;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04971</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#35760;&#24518;&#21407;&#22411;&#36827;&#34892;&#24191;&#20041;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Learning With Knowledge Memorizing Prototypes For Generalized Few-Shot Intent Detection. (arXiv:2309.04971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#35760;&#24518;&#21407;&#22411;&#36827;&#34892;&#24191;&#20041;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#31867;&#22686;&#37327;&#23398;&#20064;&#33539;&#24335;&#26469;&#21516;&#26102;&#20998;&#31867;&#24050;&#30693;&#21644;&#26032;&#24847;&#22270;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#24191;&#20041;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#20013;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#21516;&#26102;&#23545;&#24050;&#30693;&#21644;&#26032;&#24847;&#22270;&#36827;&#34892;&#20998;&#31867;&#12290;&#20197;&#24448;&#30340;&#24191;&#20041;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#24773;&#33410;&#23398;&#20064;&#33539;&#24335;&#65292;&#38590;&#20197;&#25193;&#23637;&#21040;&#24191;&#20041;&#35774;&#32622;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#26126;&#30830;&#23398;&#20064;&#24050;&#30693;&#31867;&#21035;&#30340;&#20998;&#31867;&#21644;&#24050;&#30693;&#24847;&#22270;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#24191;&#20041;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#31867;&#22686;&#37327;&#23398;&#20064;&#33539;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#22312;&#19981;&#21516;&#38454;&#27573;&#39034;&#24207;&#23398;&#20064;&#19981;&#21516;&#24847;&#22270;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21407;&#22411;&#23545;&#24050;&#30693;&#21644;&#26032;&#24847;&#22270;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#19981;&#21516;&#38454;&#27573;&#23454;&#29616;&#24847;&#22270;&#30340;&#36716;&#31227;&#30693;&#35782;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#25509;&#36817;&#23454;&#38469;&#24212;&#29992;&#30340;&#30693;&#35782;&#20445;&#30041;&#26041;&#27861;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#21644;&#35814;&#32454;&#20998;&#26512;&#34920;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#21644;&#30693;&#35782;&#35760;&#24518;&#21407;&#22411;&#21487;&#20197;&#22312;&#24191;&#20041;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Few-Shot Intent Detection (GFSID) is challenging and realistic because it needs to categorize both seen and novel intents simultaneously. Previous GFSID methods rely on the episodic learning paradigm, which makes it hard to extend to a generalized setup as they do not explicitly learn the classification of seen categories and the knowledge of seen intents. To address the dilemma, we propose to convert the GFSID task into the class incremental learning paradigm. Specifically, we propose a two-stage learning framework, which sequentially learns the knowledge of different intents in various periods via prompt learning. And then we exploit prototypes for categorizing both seen and novel intents. Furthermore, to achieve the transfer knowledge of intents in different stages, for different scenarios we design two knowledge preservation methods which close to realistic applications. Extensive experiments and detailed analyses on two widely used datasets show that our framework base
&lt;/p&gt;</description></item><item><title>Prefix-diffusion&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#27880;&#20837;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#26469;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#26469;&#20943;&#23569;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04965</link><description>&lt;p&gt;
&#21069;&#32512;&#25193;&#25955;&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#26679;&#21270;&#22270;&#20687;&#23383;&#24149;&#30340;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning. (arXiv:2309.04965v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04965
&lt;/p&gt;
&lt;p&gt;
Prefix-diffusion&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#27880;&#20837;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#26469;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#26469;&#20943;&#23569;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#30340;&#23383;&#24149;&#30340;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#21442;&#25968;&#35268;&#27169;&#36739;&#22823;&#20173;&#28982;&#26159;&#36825;&#20123;&#31995;&#32479;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#32593;&#32476;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#25193;&#25955;&#65292;&#31216;&#20026;&#21069;&#32512;&#25193;&#25955;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#20943;&#23569;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#12290;&#21069;&#32512;&#25193;&#25955;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23569;&#30340;&#21442;&#25968;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#23383;&#24149;&#30340;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25193;&#23637;&#22270;&#20687;&#23383;&#24149;&#30340;&#25193;&#25955;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#19982;&#26368;&#36817;&#30340;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While impressive performance has been achieved in image captioning, the limited diversity of the generated captions and the large parameter scale remain major barriers to the real-word application of these systems. In this work, we propose a lightweight image captioning network in combination with continuous diffusion, called Prefix-diffusion. To achieve diversity, we design an efficient method that injects prefix image embeddings into the denoising process of the diffusion model. In order to reduce trainable parameters, we employ a pre-trained model to extract image features and further design an extra mapping network. Prefix-diffusion is able to generate diverse captions with relatively less parameters, while maintaining the fluency and relevance of the captions benefiting from the generative capabilities of the diffusion model. Our work paves the way for scaling up diffusion models for image captioning, and achieves promising performance compared with recent approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#25991;&#26723;&#25688;&#35201;&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20026;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21442;&#32771;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04951</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;&#65306;&#19968;&#39033;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-document Summarization: A Comparative Evaluation. (arXiv:2309.04951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#25991;&#26723;&#25688;&#35201;&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20026;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21442;&#32771;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#35780;&#20272;&#65292;&#20197;&#30830;&#23450;&#26368;&#26032;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;BigSurvey-MDS&#21644;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;PRIMERA&#21644;PEGASUS&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#39046;&#22495;&#30340;&#19981;&#21516;&#32780;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;PRIMERA&#21644;PEGASUS&#12290;&#25105;&#20204;&#20351;&#29992;ROUGE&#20998;&#25968;&#20316;&#20026;&#24615;&#33021;&#24230;&#37327;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20102;&#35299;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#39046;&#22495;&#20013;&#20934;&#30830;&#12289;&#40065;&#26834;&#30340;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is aimed at evaluating state-of-the-art models for Multi-document Summarization (MDS) on different types of datasets in various domains and investigating the limitations of existing models to determine future research directions. To address this gap, we conducted an extensive literature review to identify state-of-the-art models and datasets. We analyzed the performance of PRIMERA and PEGASUS models on BigSurvey-MDS and MS$^2$ datasets, which posed unique challenges due to their varied domains. Our findings show that the General-Purpose Pre-trained Model LED outperforms PRIMERA and PEGASUS on the MS$^2$ dataset. We used the ROUGE score as a performance metric to evaluate the identified models on different datasets. Our study provides valuable insights into the models' strengths and weaknesses, as well as their applicability in different domains. This work serves as a reference for future MDS research and contributes to the development of accurate and robust models which can 
&lt;/p&gt;</description></item><item><title>&#33521;&#25991;RST&#35299;&#26512;&#30340;&#38590;&#28857;&#20027;&#35201;&#26159;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#26368;&#32456;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#38169;&#35823;&#21457;&#29983;&#30340;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2309.04940</link><description>&lt;p&gt;
&#33521;&#25991;RST&#35299;&#26512;&#20013;&#30340;&#38590;&#28857;&#26159;&#20160;&#20040;&#65311;&#39044;&#27979;&#27169;&#22411;&#29992;&#20110;&#38169;&#35823;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
What's Hard in English RST Parsing? Predictive Models for Error Analysis. (arXiv:2309.04940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04940
&lt;/p&gt;
&lt;p&gt;
&#33521;&#25991;RST&#35299;&#26512;&#30340;&#38590;&#28857;&#20027;&#35201;&#26159;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#26368;&#32456;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#38169;&#35823;&#21457;&#29983;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#26694;&#26550;&#19979;&#30340;&#23618;&#32423;&#31687;&#31456;&#35299;&#26512;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23545;&#20110;&#36825;&#20010;&#21407;&#22240;&#30340;&#29702;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#24182;&#24314;&#27169;&#20102;&#20197;&#21069;&#24037;&#20316;&#20013;&#19982;&#35299;&#26512;&#22256;&#38590;&#30456;&#20851;&#30340;&#19968;&#20123;&#22240;&#32032;&#65306;&#38544;&#21547;&#30340;&#31687;&#31456;&#20851;&#31995;&#30340;&#23384;&#22312;&#65292;&#35782;&#21035;&#38271;&#36317;&#31163;&#20851;&#31995;&#30340;&#25361;&#25112;&#65292;&#35789;&#27719;&#34920;&#22806;&#30340;&#39033;&#30446;&#31561;&#31561;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#21464;&#37327;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#24102;&#26377;&#26174;&#24335;&#27491;&#30830;&#21644;&#24178;&#25200;&#24615;&#31687;&#31456;&#26631;&#35760;&#30340;&#33521;&#25991;&#27979;&#35797;&#38598;&#65292;&#36825;&#20123;&#26631;&#35760;&#19982;&#40644;&#37329;&#26631;&#20934;&#30340;RST&#20851;&#31995;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#27973;&#23618;&#31687;&#31456;&#35299;&#26512;&#19968;&#26679;&#65292;&#26174;&#24335;/&#38544;&#24335;&#30340;&#21306;&#20998;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#65292;&#20294;&#38271;&#36317;&#31163;&#20381;&#36182;&#26159;&#20027;&#35201;&#25361;&#25112;&#65292;&#32780;&#32570;&#20047;&#35789;&#27719;&#37325;&#21472;&#22312;&#33267;&#23569;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#35299;&#26512;&#20013;&#19981;&#26159;&#19968;&#20010;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#38169;&#35823;&#21457;&#29983;&#30340;&#20301;&#32622;&#65292;&#20855;&#26377;&#19968;&#20010;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances in Natural Language Processing (NLP), hierarchical discourse parsing in the framework of Rhetorical Structure Theory remains challenging, and our understanding of the reasons for this are as yet limited. In this paper, we examine and model some of the factors associated with parsing difficulties in previous work: the existence of implicit discourse relations, challenges in identifying long-distance relations, out-of-vocabulary items, and more. In order to assess the relative importance of these variables, we also release two annotated English test-sets with explicit correct and distracting discourse markers associated with gold standard RST relations. Our results show that as in shallow discourse parsing, the explicit/implicit distinction plays a role, but that long-distance dependencies are the main challenge, while lack of lexical overlap is less of a problem, at least for in-domain parsing. Our final model is able to predict where errors will occur with an ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#22359;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#21333;&#35789;&#21040;&#21477;&#22359;&#21644;&#21477;&#22359;&#21040;&#21477;&#23376;&#30340;&#32452;&#21512;&#12290;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23558;&#30701;&#35821;F1&#24471;&#20998;&#25552;&#39640;&#20102;6&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.04919</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#22359;&#21270;&#19982;&#20998;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Chunking with Hierarchical RNN. (arXiv:2309.04919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#22359;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#21333;&#35789;&#21040;&#21477;&#22359;&#21644;&#21477;&#22359;&#21040;&#21477;&#23376;&#30340;&#32452;&#21512;&#12290;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23558;&#30701;&#35821;F1&#24471;&#20998;&#25552;&#39640;&#20102;6&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#39044;&#27979;&#35821;&#35328;&#32467;&#26500;&#65292;&#22914;&#35299;&#26512;&#21644;&#21477;&#22359;&#21270;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#21477;&#27861;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#22359;&#21270;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;&#38750;&#23618;&#27425;&#21270;&#26041;&#24335;&#23545;&#21333;&#35789;&#36827;&#34892;&#20998;&#32452;&#30340;&#21477;&#27861;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#23618;&#20998;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;HRNN&#65289;&#26469;&#24314;&#27169;&#21333;&#35789;&#21040;&#21477;&#22359;&#21644;&#21477;&#22359;&#21040;&#21477;&#23376;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#65306;&#20351;&#29992;&#26080;&#30417;&#30563;&#35299;&#26512;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;CoNLL-2000&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23558;&#30701;&#35821;F1&#24471;&#20998;&#25552;&#39640;&#20102;6&#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#19982;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#36824;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21477;&#22359;&#32467;&#26500;&#22312;&#31070;&#32463;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#30701;&#26242;&#30340;&#12290;&#26412;&#30740;&#31350;&#23545;&#20110;&#25512;&#21160;&#26080;&#30417;&#30563;&#21477;&#22359;&#21270;&#30340;&#36827;&#23637;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Natural Language Processing (NLP), predicting linguistic structures, such as parsing and chunking, has mostly relied on manual annotations of syntactic structures. This paper introduces an unsupervised approach to chunking, a syntactic task that involves grouping words in a non-hierarchical manner. We present a two-layer Hierarchical Recurrent Neural Network (HRNN) designed to model word-to-chunk and chunk-to-sentence compositions. Our approach involves a two-stage training process: pretraining with an unsupervised parser and finetuning on downstream NLP tasks. Experiments on the CoNLL-2000 dataset reveal a notable improvement over existing unsupervised methods, enhancing phrase F1 score by up to 6 percentage points. Further, finetuning with downstream tasks results in an additional performance improvement. Interestingly, we observe that the emergence of the chunking structure is transient during the neural model's downstream-task training. This study contributes to the advancement 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65306;&#26131;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#22686;&#24378;&#65288;EDDA&#65289;&#21644;&#31867;&#22411;&#29305;&#23450;&#30340;&#30456;&#20284;&#35789;&#26367;&#25442;&#65288;TSSR&#65289;&#12290;&#23427;&#20204;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35789;&#24615;&#26631;&#35760;&#26469;&#25913;&#36827;&#26131;&#20110;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;EDA&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04862</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributional Data Augmentation Methods for Low Resource Language. (arXiv:2309.04862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65306;&#26131;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#22686;&#24378;&#65288;EDDA&#65289;&#21644;&#31867;&#22411;&#29305;&#23450;&#30340;&#30456;&#20284;&#35789;&#26367;&#25442;&#65288;TSSR&#65289;&#12290;&#23427;&#20204;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35789;&#24615;&#26631;&#35760;&#26469;&#25913;&#36827;&#26131;&#20110;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;EDA&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22686;&#24378;&#26159;&#19968;&#31181;&#20174;&#19981;&#36275;&#36164;&#28304;&#30340;&#35821;&#26009;&#24211;&#20013;&#26500;&#36896;&#21512;&#25104;&#25968;&#25454;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#65292;&#25991;&#26412;&#22686;&#24378;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20986;&#29616;&#65292;&#20197;&#25552;&#21319;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#22686;&#24378;&#25216;&#26415;&#20043;&#19968;&#26159;&#26131;&#20110;&#25968;&#25454;&#22686;&#24378;&#65288;EDA&#65289;&#65292;&#23427;&#36890;&#36807;&#27880;&#20837;&#21644;&#26367;&#25442;&#21516;&#20041;&#35789;&#20197;&#21450;&#38543;&#26426;&#25490;&#21015;&#21477;&#23376;&#26469;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#12290;EDA&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#38656;&#35201;&#22810;&#21151;&#33021;&#21644;&#23436;&#25972;&#30340;&#21516;&#20041;&#35789;&#35789;&#20856;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#24456;&#38590;&#25214;&#21040;&#12290;&#20026;&#20102;&#25552;&#39640;EDA&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#26041;&#27861;&#65306;&#26131;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#22686;&#24378;&#65288;EDDA&#65289;&#21644;&#31867;&#22411;&#29305;&#23450;&#30340;&#30456;&#20284;&#35789;&#26367;&#25442;&#65288;TSSR&#65289;&#65292;&#23427;&#20351;&#29992;&#35821;&#20041;&#35789;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35789;&#24615;&#26631;&#35760;&#26469;&#36827;&#34892;&#35789;&#26367;&#25442;&#21644;&#22686;&#24378;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text augmentation is a technique for constructing synthetic data from an under-resourced corpus to improve predictive performance. Synthetic data generation is common in numerous domains. However, recently text augmentation has emerged in natural language processing (NLP) to improve downstream tasks. One of the current state-of-the-art text augmentation techniques is easy data augmentation (EDA), which augments the training data by injecting and replacing synonyms and randomly permuting sentences. One major obstacle with EDA is the need for versatile and complete synonym dictionaries, which cannot be easily found in low-resource languages. To improve the utility of EDA, we propose two extensions, easy distributional data augmentation (EDDA) and type specific similar word replacement (TSSR), which uses semantic word context information and part-of-speech tags for word replacement and augmentation. In an extensive empirical evaluation, we show the utility of the proposed methods, measure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#36870;&#21521;&#24037;&#31243;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#29983;&#25104;&#25991;&#26412;&#20197;&#21450;&#25581;&#31034;&#30001;&#20110;&#35299;&#30721;&#35774;&#32622;&#23548;&#33268;&#30340;&#20559;&#35265;&#30340;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.04858</link><description>&lt;p&gt;
&#36870;&#21521;&#24037;&#31243;&#35299;&#30721;&#31574;&#30053;&#65306;&#22312;&#23545;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;
&lt;/p&gt;
&lt;p&gt;
Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System. (arXiv:2309.04858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#36870;&#21521;&#24037;&#31243;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#29983;&#25104;&#25991;&#26412;&#20197;&#21450;&#25581;&#31034;&#30001;&#20110;&#35299;&#30721;&#35774;&#32622;&#23548;&#33268;&#30340;&#20559;&#35265;&#30340;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#22312;&#20801;&#35768;&#29992;&#25143;&#36755;&#20837;&#25552;&#31034;&#24182;&#25509;&#25910;&#29983;&#25104;&#25991;&#26412;&#30340;API&#21644;&#32593;&#31449;&#19978;&#12290;&#35768;&#22810;&#31995;&#32479;&#19981;&#20250;&#36879;&#38706;&#29983;&#25104;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#36870;&#21521;&#24037;&#31243;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#30721;&#26041;&#27861;&#65288;&#21363;&#65292;top-k&#25110;nucleus&#37319;&#26679;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#20351;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#23545;&#20110;&#26816;&#27979;&#29983;&#25104;&#25991;&#26412;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#35299;&#30721;&#31574;&#30053;&#30340;&#36807;&#31243;&#21487;&#20197;&#25581;&#31034;&#30001;&#20110;&#36873;&#25321;&#35299;&#30721;&#35774;&#32622;&#32780;&#23548;&#33268;&#30340;&#20559;&#35265;&#65292;&#36825;&#20005;&#37325;&#25130;&#26029;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#20197;&#21450;&#29983;&#20135;&#31995;&#32479;&#19978;&#65288;&#20363;&#22914;&#65292;ChatGPT&#65289;&#19978;&#25191;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-$k$ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model's predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04849</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#31934;&#28860;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#24773;&#24863;&#34920;&#31034;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#19968;&#20018;&#35821;&#38899;&#20449;&#21495;&#26469;&#36827;&#34892;&#21333;&#27169;&#24577;SER&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#24182;&#36991;&#20813;&#36816;&#34892;&#26102;&#30340;&#36716;&#24405;&#21644;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#38169;&#35823;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19968;&#23545;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#20013;&#30340;&#23884;&#20837;&#21644;&#36923;&#36753;&#23618;&#38754;&#33976;&#39311;&#20449;&#24687;&#12290;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#24182;&#36798;&#21040;&#20102;77.49&#65285;&#30340;&#26080;&#26435;&#37325;&#20934;&#30830;&#29575;&#21644;78.91&#65285;&#30340;&#21152;&#26435;&#20934;&#30830;&#29575;&#30340;&#26368;&#26032;&#25104;&#32489;&#12290;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;ASR&#30340;n-best&#21015;&#34920;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#28508;&#22312;&#38480;&#21046;&#65292;&#32780;&#26080;&#38656;&#23454;&#36136;&#25913;&#21464;ASR&#21644;LLM&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.04842</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21033;&#29992;ASR&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Exploiting ASR Uncertainty. (arXiv:2309.04842v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04842
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;ASR&#30340;n-best&#21015;&#34920;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#28508;&#22312;&#38480;&#21046;&#65292;&#32780;&#26080;&#38656;&#23454;&#36136;&#25913;&#21464;ASR&#21644;LLM&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#35201;&#22312;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#20204;&#24517;&#39035;&#20381;&#38752;&#29616;&#25104;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36827;&#34892;&#36716;&#24405;&#65292;&#25110;&#32773;&#37197;&#22791;&#20869;&#32622;&#30340;&#35821;&#38899;&#27169;&#24577;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#21069;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;LLM&#22312;SLU&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#21463;&#38480;&#20110;&#22266;&#23450;ASR&#31995;&#32479;&#22312;&#21475;&#35821;&#36755;&#20837;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#35821;&#38899;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#39640;&#23383;&#35789;&#38169;&#35823;&#29575;&#21487;&#33021;&#38480;&#21046;LLM&#29702;&#35299;&#21475;&#22836;&#24847;&#22270;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#36890;&#36807;&#35774;&#35745;&#22797;&#26434;&#25110;&#19987;&#38376;&#30340;&#26550;&#26500;&#36861;&#27714;&#39640;&#20934;&#30830;&#24615;&#65292;&#32780;&#26159;&#22312;&#19981;&#23454;&#36136;&#25913;&#21464;&#24213;&#23618;ASR&#21644;LLM&#30340;&#24773;&#20917;&#19979;&#65292;&#30475;&#30475;&#25105;&#20204;&#33021;&#36208;&#22810;&#36828;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#28508;&#22312;&#22320;&#34987;&#22810;&#20010;&#19981;&#30456;&#20851;&#30340;&#20219;&#21153;&#20849;&#20139;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;ASR&#20551;&#35774;&#30340;n-best&#21015;&#34920;&#26469;&#25552;&#31034;LLM&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23481;&#26131;&#20986;&#38169;&#30340;1-best&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypoth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#34892;&#20026;&#65292;&#21457;&#29616;&#32593;&#32476;&#30340;&#26089;&#26399;&#37096;&#20998;&#26159;&#31232;&#30095;&#30340;&#65292;&#21253;&#21547;&#35768;&#22810;&#27515;&#20129;&#31070;&#32463;&#20803;&#21644;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#29305;&#24449;&#30340;&#27963;&#36291;&#31070;&#32463;&#20803;&#12290;&#36825;&#20123;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#26356;&#26032;&#19981;&#20165;&#25512;&#21160;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#29983;&#25104;&#65292;&#36824;&#19987;&#27880;&#20110;&#31227;&#38500;&#19982;&#35302;&#21457;&#23427;&#20204;&#30340;&#26631;&#35760;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.04827</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#65306;&#19981;&#27963;&#36291;&#65292;N-gram&#65292;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
Neurons in Large Language Models: Dead, N-gram, Positional. (arXiv:2309.04827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#34892;&#20026;&#65292;&#21457;&#29616;&#32593;&#32476;&#30340;&#26089;&#26399;&#37096;&#20998;&#26159;&#31232;&#30095;&#30340;&#65292;&#21253;&#21547;&#35768;&#22810;&#27515;&#20129;&#31070;&#32463;&#20803;&#21644;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#29305;&#24449;&#30340;&#27963;&#36291;&#31070;&#32463;&#20803;&#12290;&#36825;&#20123;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#26356;&#26032;&#19981;&#20165;&#25512;&#21160;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#29983;&#25104;&#65292;&#36824;&#19987;&#27880;&#20110;&#31227;&#38500;&#19982;&#35302;&#21457;&#23427;&#20204;&#30340;&#26631;&#35760;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20197;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#24335;&#20998;&#26512;&#20102;&#19968;&#31867;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;OPT&#31995;&#21015;&#27169;&#22411;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;125m&#21040;66b&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;FFN&#31070;&#32463;&#20803;&#26159;&#21542;&#34987;&#28608;&#27963;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#32593;&#32476;&#30340;&#26089;&#26399;&#37096;&#20998;&#26159;&#31232;&#30095;&#30340;&#65292;&#34920;&#31034;&#35768;&#22810;&#31163;&#25955;&#29305;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#35768;&#22810;&#31070;&#32463;&#20803;&#65288;&#22312;66b&#27169;&#22411;&#30340;&#26576;&#20123;&#23618;&#20013;&#36229;&#36807;70%&#65289;&#26159;&#8220;&#19981;&#27963;&#36291;&#30340;&#8221;&#65292;&#21363;&#23427;&#20204;&#22312;&#22823;&#37327;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#20174;&#19981;&#28608;&#27963;&#12290;&#21516;&#26102;&#65292;&#35768;&#22810;&#27963;&#36291;&#30340;&#31070;&#32463;&#20803;&#19987;&#29992;&#20110;&#31163;&#25955;&#29305;&#24449;&#65292;&#24182;&#19988;&#20805;&#24403;&#26631;&#35760;&#21644;n-gram&#26816;&#27979;&#22120;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23427;&#20204;&#23545;&#24212;&#30340;FFN&#26356;&#26032;&#19981;&#20165;&#20419;&#36827;&#20102;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#20505;&#36873;&#65292;&#36825;&#26159;&#21487;&#20197;&#39044;&#26399;&#30340;&#65292;&#32780;&#19988;&#36824;&#26126;&#30830;&#22320;&#19987;&#27880;&#20110;&#31227;&#38500;&#19982;&#35302;&#21457;&#23427;&#20204;&#30340;&#26631;&#35760;&#65288;&#21363;&#24403;&#21069;&#36755;&#20837;&#65289;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#27531;&#24046;&#27969;&#20013;&#19987;&#38376;&#29992;&#20110;&#31227;&#38500;&#65288;&#32780;&#19981;&#26159;&#28155;&#21152;&#65289;&#20449;&#24687;&#30340;&#26426;&#21046;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze a family of large language models in such a lightweight manner that can be done on a single GPU. Specifically, we focus on the OPT family of models ranging from 125m to 66b parameters and rely only on whether an FFN neuron is activated or not. First, we find that the early part of the network is sparse and represents many discrete features. Here, many neurons (more than 70% in some layers of the 66b model) are "dead", i.e. they never activate on a large collection of diverse data. At the same time, many of the alive neurons are reserved for discrete features and act as token and n-gram detectors. Interestingly, their corresponding FFN updates not only promote next token candidates as could be expected, but also explicitly focus on removing the information about triggering them tokens, i.e., current input. To the best of our knowledge, this is the first example of mechanisms specialized at removing (rather than adding) information from the residual stream. With scale, models 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35201;&#32032;&#30340;&#21465;&#20107;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;FaNS&#65292;&#36890;&#36807;&#25552;&#21462;&#32463;&#20856;&#30340;&#20116;W&#19968;H&#35201;&#32032;&#24182;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#21465;&#20107;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FaNS&#19982;&#20256;&#32479;&#25991;&#26412;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65288;&#39640;37%&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.04823</link><description>&lt;p&gt;
FaNS&#65306;&#22522;&#20110;&#35201;&#32032;&#30340;&#21465;&#20107;&#30456;&#20284;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
FaNS: a Facet-based Narrative Similarity Metric. (arXiv:2309.04823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35201;&#32032;&#30340;&#21465;&#20107;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;FaNS&#65292;&#36890;&#36807;&#25552;&#21462;&#32463;&#20856;&#30340;&#20116;W&#19968;H&#35201;&#32032;&#24182;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#21465;&#20107;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FaNS&#19982;&#20256;&#32479;&#25991;&#26412;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65288;&#39640;37%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#30340;&#21465;&#20107;&#26816;&#32034;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#21465;&#20107;&#23545;&#20110;&#35299;&#37322;&#21644;&#29702;&#35299;&#20107;&#20214;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22810;&#20010;&#30456;&#20851;&#30340;&#21465;&#20107;&#36890;&#24120;&#26377;&#21161;&#20110;&#21019;&#24314;&#23545;&#25152;&#20851;&#27880;&#20107;&#20214;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;&#20026;&#20102;&#20934;&#30830;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#21465;&#20107;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21465;&#20107;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#35201;&#32032;&#30340;&#21465;&#20107;&#30456;&#20284;&#24230;&#65288;FaNS&#65289;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#32463;&#20856;&#30340;&#20116;W&#19968;H&#35201;&#32032;&#65288;Who&#65292;What&#65292;When&#65292;Where&#65292;Why&#21644;How&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25552;&#21462;&#12290;&#19982;&#29616;&#26377;&#30340;&#20165;&#20851;&#27880;&#25972;&#20307;&#35789;&#27719;/&#35821;&#20041;&#21305;&#37197;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#19981;&#21516;&#65292;FaNS&#25552;&#20379;&#20102;&#26356;&#20026;&#32454;&#33268;&#30340;&#21305;&#37197;&#65292;&#21253;&#25324;&#20845;&#20010;&#19981;&#21516;&#30340;&#35201;&#32032;&#30340;&#29420;&#31435;&#21305;&#37197;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#12290;&#20026;&#20102;&#35780;&#20272;FaNS&#65292;&#25105;&#20204;&#20174;&#31532;&#19977;&#26041;&#26032;&#38395;&#38376;&#25143;AllSides&#25910;&#38598;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#21465;&#20107;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FaNS&#24230;&#37327;&#26041;&#27861;&#19982;&#30452;&#25509;&#24230;&#37327;&#30340;&#20256;&#32479;&#25991;&#26412;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65288;&#39640;37%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similar Narrative Retrieval is a crucial task since narratives are essential for explaining and understanding events, and multiple related narratives often help to create a holistic view of the event of interest. To accurately identify semantically similar narratives, this paper proposes a novel narrative similarity metric called Facet-based Narrative Similarity (FaNS), based on the classic 5W1H facets (Who, What, When, Where, Why, and How), which are extracted by leveraging the state-of-the-art Large Language Models (LLMs). Unlike existing similarity metrics that only focus on overall lexical/semantic match, FaNS provides a more granular matching along six different facets independently and then combines them. To evaluate FaNS, we created a comprehensive dataset by collecting narratives from AllSides, a third-party news portal. Experimental results demonstrate that the FaNS metric exhibits a higher correlation (37\% higher) than traditional text similarity metrics that directly measur
&lt;/p&gt;</description></item><item><title>MMHQA-ICL&#26694;&#26550;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#24322;&#26500;&#25968;&#25454;&#26816;&#32034;&#22120;&#21644;&#22270;&#20687;&#26631;&#39064;&#27169;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#32972;&#26223;&#19979;&#23398;&#20064;&#31574;&#30053;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#27169;&#24577;&#28151;&#21512;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04790</link><description>&lt;p&gt;
MMHQA-ICL: &#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#22810;&#27169;&#24577;&#32972;&#26223;&#19979;&#30340;&#28151;&#21512;&#38382;&#31572;&#30340;&#22810;&#27169;&#33509;&#24178;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images. (arXiv:2309.04790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04790
&lt;/p&gt;
&lt;p&gt;
MMHQA-ICL&#26694;&#26550;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#24322;&#26500;&#25968;&#25454;&#26816;&#32034;&#22120;&#21644;&#22270;&#20687;&#26631;&#39064;&#27169;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#32972;&#26223;&#19979;&#23398;&#20064;&#31574;&#30053;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#27169;&#24577;&#28151;&#21512;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#30693;&#35782;&#24120;&#24120;&#20197;&#22810;&#27169;&#24577;&#21644;&#24322;&#26500;&#30340;&#24418;&#24335;&#23384;&#22312;&#12290;&#35299;&#20915;&#21253;&#25324;&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#22312;&#20869;&#30340;&#28151;&#21512;&#25968;&#25454;&#31867;&#22411;&#30340;&#38382;&#31572;&#20219;&#21153;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65288;MMHQA&#65289;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23835;&#36215;&#65292;&#32972;&#26223;&#19979;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;QA&#38382;&#39064;&#30340;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;MMHQA-ICL&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#26356;&#24378;&#22823;&#30340;&#24322;&#26500;&#25968;&#25454;&#26816;&#32034;&#22120;&#21644;&#22270;&#20687;&#26631;&#39064;&#27169;&#22359;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;MMHQA&#30340;&#29305;&#23450;&#31867;&#22411;&#30340;&#32972;&#26223;&#19979;&#23398;&#20064;&#31574;&#30053;&#65292;&#20351;LLM&#33021;&#22815;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#21457;&#25381;&#20854;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20351;&#29992;&#31471;&#21040;&#31471;LLM&#25552;&#31034;&#26041;&#27861;&#30340;&#20154;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;MultimodalQA&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#32447;&#21644;&#35757;&#32451;&#22312;&#23436;&#25972;&#25968;&#25454;&#38598;&#19978;&#30340;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the real world, knowledge often exists in a multimodal and heterogeneous form. Addressing the task of question answering with hybrid data types, including text, tables, and images, is a challenging task (MMHQA). Recently, with the rise of large language models (LLM), in-context learning (ICL) has become the most popular way to solve QA problems. We propose MMHQA-ICL framework for addressing this problems, which includes stronger heterogeneous data retriever and an image caption module. Most importantly, we propose a Type-specific In-context Learning Strategy for MMHQA, enabling LLMs to leverage their powerful performance in this task. We are the first to use end-to-end LLM prompting method for this task. Experimental results demonstrate that our framework outperforms all baselines and methods trained on the full dataset, achieving state-of-the-art results under the few-shot setting on the MultimodalQA dataset.
&lt;/p&gt;</description></item><item><title>SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2309.04766</link><description>&lt;p&gt;
SeaEval&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65306;&#20174;&#36328;&#35821;&#35328;&#23545;&#40784;&#21040;&#25991;&#21270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04766
&lt;/p&gt;
&lt;p&gt;
SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;SeaEval&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#34920;&#24449;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21644;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#22810;&#35821;&#35328;&#24615;&#32500;&#24230;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#22312;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#21644;&#25991;&#21270;&#29702;&#35299;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#30340;&#34892;&#20026;&#21508;&#24322;&#65307;&#65288;2&#65289;&#35768;&#22810;&#27169;&#22411;&#20173;&#28982;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65288;&#22914;&#20301;&#32622;&#20559;&#24046;&#12289;&#22823;&#22810;&#25968;&#26631;&#31614;&#20559;&#24046;&#65289;&#65307;&#65288;3&#65289;&#23545;&#20110;&#26681;&#28304;&#20110;&#20107;&#23454;&#12289;&#31185;&#23398;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#39044;&#26399;&#22312;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#24212;&#35813;&#24471;&#21040;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#36825;&#20123;&#26597;&#35810;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#24847;&#22806;&#30340;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;4&#65289;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#23545;&#20110;&#24773;&#24863;&#30456;&#20851;&#30340;&#38382;&#39064;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#25552;&#20379;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#35780;&#20272;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.04739</link><description>&lt;p&gt;
&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Conversational AI. (arXiv:2309.04739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#25552;&#20379;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#35780;&#20272;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20449;&#24687;&#33719;&#21462;&#26041;&#24335;&#65292;&#36229;&#36234;&#20102;&#21333;&#19968;&#26597;&#35810;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#39046;&#22495;&#21644;&#35821;&#35328;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#22914;&#20247;&#21253;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#65292;&#22240;&#27492;&#22312;&#27492;&#24773;&#26223;&#19979;&#25928;&#29575;&#20302;&#19979;&#12290;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#32531;&#35299;&#23545;&#35805;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25945;&#31243;&#20840;&#38754;&#19988;&#26368;&#26032;&#22320;&#27010;&#36848;&#20102;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;DA&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#19981;&#21516;&#30340;&#35780;&#20272;&#27169;&#22411;&#30340;&#33539;&#24335;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36827;&#19968;&#27493;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in conversational systems have revolutionized information access, surpassing the limitations of single queries. However, developing dialogue systems requires a large amount of training data, which is a challenge in low-resource domains and languages. Traditional data collection methods like crowd-sourcing are labor-intensive and time-consuming, making them ineffective in this context. Data augmentation (DA) is an affective approach to alleviate the data scarcity problem in conversational systems. This tutorial provides a comprehensive and up-to-date overview of DA approaches in the context of conversational systems. It highlights recent advances in conversation augmentation, open domain and task-oriented conversation generation, and different paradigms of evaluating these models. We also discuss current challenges and future directions in order to help researchers and practitioners to further advance the field in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20851;&#38190;&#35789;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35270;&#35273;&#23454;&#20307;&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#24182;&#20351;&#29992;&#22270;&#20687;&#22122;&#22768;&#28388;&#27874;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04734</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#35270;&#35273;&#23454;&#20307;&#21644;&#22810;&#23610;&#24230;&#22270;&#20687;&#22122;&#22768;&#28388;&#27874;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#20851;&#38190;&#35789;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Better Multi-modal Keyphrase Generation via Visual Entity Enhancement and Multi-granularity Image Noise Filtering. (arXiv:2309.04734v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20851;&#38190;&#35789;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35270;&#35273;&#23454;&#20307;&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#24182;&#20351;&#29992;&#22270;&#20687;&#22122;&#22768;&#28388;&#27874;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20851;&#38190;&#35789;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#19968;&#32452;&#33021;&#22815;&#20195;&#34920;&#36755;&#20837;&#25991;&#26412;-&#22270;&#20687;&#23545;&#26680;&#24515;&#35201;&#28857;&#30340;&#20851;&#38190;&#35789;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;1&#65289;&#21482;&#33021;&#20351;&#29992;&#26377;&#38480;&#30340;&#20449;&#24687;&#28304;&#65288;&#22914;&#22270;&#20687;&#26631;&#39064;&#65289;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#65292;&#20294;&#36825;&#20123;&#20449;&#24687;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#21518;&#32493;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#38656;&#35201;&#12290;2&#65289;&#36755;&#20837;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#36890;&#24120;&#19981;&#33021;&#23436;&#20840;&#21305;&#37197;&#65292;&#22270;&#20687;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#20851;&#38190;&#35789;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#19981;&#20165;&#36890;&#36807;&#22806;&#37096;&#30693;&#35782;&#20016;&#23500;&#20102;&#27169;&#22411;&#36755;&#20837;&#65292;&#36824;&#33021;&#26377;&#25928;&#36807;&#28388;&#22270;&#20687;&#22122;&#22768;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#30340;&#22806;&#37096;&#35270;&#35273;&#23454;&#20307;&#20316;&#20026;&#27169;&#22411;&#30340;&#34917;&#20805;&#36755;&#20837;&#65292;&#26377;&#21161;&#20110;&#36328;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#36827;&#34892;&#20851;&#38190;&#35789;&#29983;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21516;&#26102;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
Multi-modal keyphrase generation aims to produce a set of keyphrases that represent the core points of the input text-image pair. In this regard, dominant methods mainly focus on multi-modal fusion for keyphrase generation. Nevertheless, there are still two main drawbacks: 1) only a limited number of sources, such as image captions, can be utilized to provide auxiliary information. However, they may not be sufficient for the subsequent keyphrase generation. 2) the input text and image are often not perfectly matched, and thus the image may introduce noise into the model. To address these limitations, in this paper, we propose a novel multi-modal keyphrase generation model, which not only enriches the model input with external knowledge, but also effectively filters image noise. First, we introduce external visual entities of the image as the supplementary input to the model, which benefits the cross-modal semantic alignment for keyphrase generation. Second, we simultaneously calculate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPA&#30340;&#31616;&#26131;&#25552;&#31034;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#20351;&#29992;&#22810;&#20010;&#26469;&#28304;/&#30446;&#26631;&#26469;&#25193;&#20805;&#28436;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20943;&#23569;&#20102;&#29992;&#25143;&#32534;&#20889;&#28436;&#31034;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.04725</link><description>&lt;p&gt;
EPA: &#36890;&#36807;&#22810;&#20010;&#26469;&#28304;&#21644;&#22810;&#20010;&#30446;&#26631;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#31616;&#26131;&#25552;&#31034;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
EPA: Easy Prompt Augmentation on Large Language Models via Multiple Sources and Multiple Targets. (arXiv:2309.04725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPA&#30340;&#31616;&#26131;&#25552;&#31034;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#20351;&#29992;&#22810;&#20010;&#26469;&#28304;/&#30446;&#26631;&#26469;&#25193;&#20805;&#28436;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20943;&#23569;&#20102;&#29992;&#25143;&#32534;&#20889;&#28436;&#31034;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#20219;&#21153;&#25552;&#31034;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#25552;&#31034;&#22836;&#37096;&#28155;&#21152;&#20219;&#21153;&#28436;&#31034;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#28436;&#31034;&#21487;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35201;&#27714;&#29992;&#25143;&#32534;&#20889;&#28436;&#31034;&#21487;&#33021;&#20250;&#24456;&#40635;&#28902;&#12290;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#32780;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPA (Easy Prompt Augmentation)&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#20943;&#23569;&#29992;&#25143;&#32534;&#20889;&#28436;&#31034;&#30340;&#24037;&#20316;&#37327;&#12290;EPA&#36890;&#36807;&#33258;&#21160;&#20351;&#29992;&#22810;&#20010;&#26469;&#28304;/&#30446;&#26631;&#26469;&#25193;&#20805;&#28436;&#31034;&#65292;&#20854;&#20013;&#27599;&#20010;&#26469;&#28304;/&#30446;&#26631;&#20114;&#20026;&#37322;&#20041;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promising performance on various NLP tasks via task prompting. And their performance can be further improved by appending task demonstrations to the head of the prompt. And usually, a better performance can be achieved with more demonstrations. However, asking the users to write the demonstrations can be cumbersome. As a simple yet cost-effective workaround, this paper proposes a novel method called EPA (\textbf{E}asy \textbf{P}rompt \textbf{A}ugmentation)\footnote{While this paper considers augmenting prompts via demonstrations, we name it EPA as the name EDA is already taken by a well-known NLP method \citep{wei-zou-2019-eda}.} that effectively minimizes user efforts in writing demonstrations while improving the model performance at the same time. EPA achieves these goals by automatically augmenting the demonstrations with multiple sources/targets, where each of them paraphrases each other. This is well motivated as augmenting data via paraphra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;&#65292;&#36890;&#36807;&#19968;&#20010;&#23567;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#65292;&#24182;&#20197;ChatGPT&#20026;&#24037;&#20855;&#37325;&#29616;&#20102;&#19981;&#21516;&#21457;&#34920;&#20110;&#33879;&#21517;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#32593;&#32476;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.04716</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Toward Reproducing Network Research Results Using Large Language Models. (arXiv:2309.04716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;&#65292;&#36890;&#36807;&#19968;&#20010;&#23567;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#65292;&#24182;&#20197;ChatGPT&#20026;&#24037;&#20855;&#37325;&#29616;&#20102;&#19981;&#21516;&#21457;&#34920;&#20110;&#33879;&#21517;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#32593;&#32476;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#65292;&#37325;&#29616;&#30740;&#31350;&#32467;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26368;&#20339;&#23454;&#36341;&#36890;&#24120;&#26377;&#19977;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;&#23547;&#25214;&#20844;&#24320;&#21487;&#29992;&#30340;&#21407;&#22411;&#65307;&#65288;2&#65289;&#32852;&#31995;&#20316;&#32773;&#33719;&#21462;&#31169;&#26377;&#21407;&#22411;&#65307;&#20197;&#21450;&#65288;3&#65289;&#26681;&#25454;&#35770;&#25991;&#25551;&#36848;&#25163;&#21160;&#23454;&#29616;&#21407;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24050;&#21457;&#34920;&#30340;&#32593;&#32476;&#30740;&#31350;&#27809;&#26377;&#20844;&#24320;&#21407;&#22411;&#65292;&#32780;&#33719;&#21462;&#31169;&#26377;&#21407;&#22411;&#20063;&#24456;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22823;&#37096;&#20998;&#37325;&#29616;&#24037;&#20316;&#37117;&#33457;&#36153;&#22312;&#26681;&#25454;&#35770;&#25991;&#25551;&#36848;&#36827;&#34892;&#25163;&#21160;&#23454;&#29616;&#19978;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#36153;&#21147;&#65292;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#25991;&#22823;&#32966;&#22320;&#25552;&#20986;&#20351;&#29992;&#26032;&#20852;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23567;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#65292;&#20854;&#20013;&#22235;&#21517;&#20855;&#22791;&#24517;&#35201;&#32593;&#32476;&#30693;&#35782;&#30340;&#23398;&#29983;&#20351;&#29992;ChatGPT&#36827;&#34892;&#20102;&#19981;&#21516;&#21457;&#34920;&#20110;&#33879;&#21517;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#32593;&#32476;&#31995;&#32479;&#30340;&#37325;&#29616;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducing research results in the networking community is important for both academia and industry. The current best practice typically resorts to three approaches: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; and (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private prototypes are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). In particular, we first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;LLM&#27169;&#22411;&#36890;&#36807;&#32454;&#35843;&#23454;&#29616;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#65292;&#24182;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#24773;&#24863;&#65292;&#20197;&#27492;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.04704</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model. (arXiv:2309.04704v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;LLM&#27169;&#22411;&#36890;&#36807;&#32454;&#35843;&#23454;&#29616;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#65292;&#24182;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#24773;&#24863;&#65292;&#20197;&#27492;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;LLM&#65288;Llama 2&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#36890;&#36807;&#32454;&#35843;&#36827;&#34892;&#34394;&#20551;&#20449;&#24687;&#20998;&#26512;&#21644;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;PEFT/LoRA&#30340;&#32454;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#20013;&#65292;&#35813;&#27169;&#22411;&#23545;&#20197;&#19979;&#20219;&#21153;&#36827;&#34892;&#20102;&#32454;&#35843;&#65306;&#25581;&#31034;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#21465;&#20107;&#30340;&#25991;&#26412;&#20998;&#26512;&#65292;&#20107;&#23454;&#26680;&#26597;&#65292;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#25805;&#32437;&#20998;&#26512;&#20197;&#21450;&#25552;&#21462;&#24102;&#26377;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#32454;&#35843;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#23545;&#25991;&#26412;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#12290;&#24102;&#26377;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#21487;&#20197;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility of fine-tuning Llama 2 large language model (LLM) for the disinformation analysis and fake news detection. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text on revealing disinformation and propaganda narratives, fact checking, fake news detection, manipulation analytics, extracting named entities with their sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a deep analysis of texts and reveal complex styles and narratives. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04695</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#20197;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#38024;&#23545;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;(KBQA)&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#22797;&#26434;&#30340;&#35757;&#32451;&#25216;&#26415;&#21644;&#27169;&#22411;&#26694;&#26550;&#65292;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#33021;&#21147;&#30340;&#20986;&#29616;&#20026;KBQA&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26080;&#38656;&#35757;&#32451;&#30340;&#35821;&#20041;&#35299;&#26512;&#33539;&#24335;&#65306;&#32473;&#23450;&#23569;&#37327;&#38382;&#39064;&#21450;&#20854;&#26631;&#35760;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#20316;&#20026;&#28436;&#31034;&#31034;&#20363;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#20219;&#21153;&#24847;&#22270;&#24182;&#20026;&#26032;&#38382;&#39064;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24378;&#22823;&#30340;LLMs&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#20102;&#35299;&#24456;&#23569;&#65292;&#23548;&#33268;&#26684;&#24335;&#38169;&#35823;&#29575;&#36739;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KBQA&#30340;&#20195;&#30721;&#39118;&#26684;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38476;&#29983;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#29983;&#25104;&#36807;&#31243;&#36716;&#25442;&#20026;&#26356;&#20026;&#29087;&#24713;&#30340;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#12290;&#23545;&#19977;&#20010;&#20027;&#27969;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#36731;&#20102;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#20013;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic for
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#20102;&#26367;&#25442;&#36328;&#35821;&#35328;&#35789;&#27719;&#30340;&#20960;&#31181;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#21333;&#35821;&#36716;&#31227;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#19987;&#38376;&#21270;&#30340;&#36739;&#23567;&#35789;&#27719;&#23545;&#20110;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.04679</link><description>&lt;p&gt;
&#23884;&#20837;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65306;&#27604;&#36739;&#36866;&#24212;&#26032;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#35789;&#27719;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages. (arXiv:2309.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04679
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#20102;&#26367;&#25442;&#36328;&#35821;&#35328;&#35789;&#27719;&#30340;&#20960;&#31181;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#21333;&#35821;&#36716;&#31227;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#19987;&#38376;&#21270;&#30340;&#36739;&#23567;&#35789;&#27719;&#23545;&#20110;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#33521;&#35821;&#20197;&#22806;&#30340;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30340;&#22823;&#37096;&#20998;&#12290;&#29992;&#20110;&#29305;&#23450;&#35821;&#35328;&#21270;&#30340;&#24378;&#22823;&#22522;&#20934;&#26159;&#35821;&#35328;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;LAPT&#65289;&#12290;&#20294;&#26159;&#65292;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#20445;&#30041;&#22823;&#22411;&#36328;&#35821;&#35328;&#35789;&#27719;&#21644;&#23884;&#20837;&#30697;&#38453;&#20250;&#24102;&#26469;&#30456;&#24403;&#22810;&#30340;&#22810;&#20313;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#29992;&#32039;&#20945;&#30340;&#29305;&#23450;&#35821;&#35328;&#35789;&#27719;&#26367;&#25442;&#36328;&#35821;&#35328;&#35789;&#27719;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#35789;&#27719;&#19987;&#38376;&#21270;&#21518;&#22914;&#20309;&#37325;&#26032;&#21021;&#22987;&#21270;&#20196;&#29260;&#23884;&#20837;&#30697;&#38453;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#27492;&#22806;&#36824;&#21152;&#20837;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#28966;&#28857;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65306;1&#65289;&#22312;&#21333;&#35821;&#36716;&#31227;&#25991;&#29486;&#20013;&#30340;&#23884;&#20837;&#26367;&#25442;&#25216;&#26415;&#19981;&#36866;&#29992;&#20110;&#36866;&#24212;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;2&#65289;&#29992;&#36739;&#23567;&#30340;&#19987;&#38376;&#30340;&#35789;&#27719;&#26367;&#25442;&#36328;&#35821;&#35328;&#35789;&#27719;&#25552;&#20379;&#20102;&#19968;&#31181;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multilingual language models underpin a large portion of modern NLP tools outside of English. A strong baseline for specializing these models for specific languages is Language-Adaptive Pre-Training (LAPT). However, retaining a large cross-lingual vocabulary and embedding matrix comes at considerable excess computational cost during adaptation. In this study, we propose several simple techniques to replace a cross-lingual vocabulary with a compact, language-specific one. Namely, we address strategies for re-initializing the token embedding matrix after vocabulary specialization. We then provide a systematic experimental comparison of our techniques, in addition to the recently-proposed Focus method. We demonstrate that: 1) Embedding-replacement techniques in the monolingual transfer literature are inadequate for adapting multilingual models. 2) Replacing cross-lingual vocabularies with smaller specialized ones provides an efficient method to improve performance in low-resou
&lt;/p&gt;</description></item><item><title>FIAT&#26159;&#19968;&#31181;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23436;&#20840;&#24494;&#35843;&#33539;&#24335;&#34701;&#21512;&#30340;&#26032;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#36827;&#34892;&#25351;&#20196;&#21644;&#25512;&#29702;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#27169;&#22411;&#19978;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#65292;&#32463;&#36807;&#22810;&#35821;&#35328;&#20219;&#21153;&#27979;&#35797;&#65292;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.04663</link><description>&lt;p&gt;
FIAT: &#23558;&#23398;&#20064;&#33539;&#24335;&#19982;&#25351;&#20196;&#21152;&#36895;&#35843;&#20248;&#30456;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning. (arXiv:2309.04663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04663
&lt;/p&gt;
&lt;p&gt;
FIAT&#26159;&#19968;&#31181;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23436;&#20840;&#24494;&#35843;&#33539;&#24335;&#34701;&#21512;&#30340;&#26032;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#36827;&#34892;&#25351;&#20196;&#21644;&#25512;&#29702;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#27169;&#22411;&#19978;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#65292;&#32463;&#36807;&#22810;&#35821;&#35328;&#20219;&#21153;&#27979;&#35797;&#65292;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23398;&#20064;&#33539;&#24335;&#36890;&#24120;&#20998;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#12290;&#27599;&#31181;&#33539;&#24335;&#37117;&#26377;&#20854;&#33258;&#36523;&#30340;&#21462;&#33293;&#65292;&#36825;&#21462;&#20915;&#20110;&#21487;&#29992;&#25968;&#25454;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35745;&#31639;&#25104;&#26412;&#12289;&#26131;&#29992;&#24615;&#21644;&#26368;&#32456;&#36136;&#37327;&#65292;&#20294;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20197;&#24378;&#35843;&#23427;&#20204;&#20043;&#38388;&#33258;&#28982;&#32852;&#31995;&#30340;&#26041;&#24335;&#25551;&#36848;&#20102;ICL&#21644;&#24494;&#35843;&#33539;&#24335;&#12290;&#22522;&#20110;&#36825;&#20123;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FIAT&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#36825;&#20123;&#33539;&#24335;&#30340;&#20248;&#28857;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20351;&#24471;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#24037;&#31243;&#25351;&#20196;&#21644;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#21516;&#26102;&#22312;&#21442;&#25968;&#25928;&#29575;&#35843;&#20248;&#30340;&#36739;&#23567;&#27169;&#22411;&#19978;&#20351;&#29992;&#31867;&#20284;&#30340;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;FIAT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;FIAT&#22312;100-10,000&#20010;&#35757;&#32451;&#26679;&#26412;&#35268;&#27169;&#19979;&#22343;&#27604;ICL&#21644;&#24494;&#35843;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24076;&#26395;FIAT&#33021;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning paradigms for large language models (LLMs) currently tend to fall within either in-context learning (ICL) or full fine-tuning. Each of these comes with their own trade-offs based on available data, model size, compute cost, ease-of-use, and final quality with neither solution performing well across-the-board. In this article, we first describe ICL and fine-tuning paradigms in a way that highlights their natural connections. Based on these connections, we propose a new learning paradigm called FIAT that fuses the best of these paradigms together, enabling prompt-engineered instructions and chain-of-thought reasoning with the very largest models while also using similar methods to perform parameter updates on a modestly-sized LLM with parameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of multilingual tasks and observe that FIAT performs better than both ICL and fine-tuning at scales ranging from 100-10,000 training examples. We hope that FIAT provides a pr
&lt;/p&gt;</description></item><item><title>MADLAD-400&#26159;&#19968;&#31181;&#35206;&#30422;419&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25991;&#26723;&#32423;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#33258;&#25105;&#23457;&#26680;&#25581;&#31034;&#20102;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#20247;&#22810;&#21442;&#25968;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#32473;&#30740;&#31350;&#30028;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.04662</link><description>&lt;p&gt;
MADLAD-400: &#19968;&#31181;&#22810;&#35821;&#35328;&#21644;&#25991;&#26723;&#32423;&#30340;&#22823;&#35268;&#27169;&#23457;&#26680;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MADLAD-400: A Multilingual And Document-Level Large Audited Dataset. (arXiv:2309.04662v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04662
&lt;/p&gt;
&lt;p&gt;
MADLAD-400&#26159;&#19968;&#31181;&#35206;&#30422;419&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25991;&#26723;&#32423;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#33258;&#25105;&#23457;&#26680;&#25581;&#31034;&#20102;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#20247;&#22810;&#21442;&#25968;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#32473;&#30740;&#31350;&#30028;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MADLAD-400&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;CommonCrawl&#30340;&#25163;&#21160;&#23457;&#26680;&#30340;&#36890;&#29992;&#39046;&#22495;3T token&#21333;&#35821;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;419&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36890;&#36807;&#33258;&#25105;&#23457;&#26680;MADLAD-400&#25581;&#31034;&#20986;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#25968;&#25454;&#23457;&#26680;&#22312;&#25968;&#25454;&#38598;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#28085;&#30422;450&#22810;&#31181;&#35821;&#35328;&#12289;2500&#20159;&#20010;&#26631;&#35760;&#30340;10.7B&#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#19981;&#21516;&#39046;&#22495;&#19978;&#19982;&#35268;&#27169;&#26356;&#22823;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#25253;&#21578;&#20102;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;8B&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#23569;&#26679;&#26412;&#32763;&#35793;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#22522;&#20934;&#27169;&#22411;&#25552;&#20379;&#32473;&#30740;&#31350;&#30028;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27807;&#36890;&#28216;&#25103;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35843;&#21442;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23545;&#29436;&#20154;&#26432;&#28216;&#25103;&#30340;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20986;&#29616;&#30340;&#25112;&#30053;&#34892;&#20026;&#12290;&#36825;&#34920;&#26126;&#22312;&#27807;&#36890;&#28216;&#25103;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20855;&#22791;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.04658</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27807;&#36890;&#28216;&#25103;&#20013;&#30340;&#24212;&#29992;&#65306;&#23545;&#29436;&#20154;&#26432;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf. (arXiv:2309.04658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27807;&#36890;&#28216;&#25103;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35843;&#21442;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23545;&#29436;&#20154;&#26432;&#28216;&#25103;&#30340;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20986;&#29616;&#30340;&#25112;&#30053;&#34892;&#20026;&#12290;&#36825;&#34920;&#26126;&#22312;&#27807;&#36890;&#28216;&#25103;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20855;&#22791;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27807;&#36890;&#28216;&#25103;&#65292;&#25105;&#20204;&#25226;&#25351;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#31216;&#20026;&#27807;&#36890;&#28216;&#25103;&#65292;&#22312;&#32463;&#27982;&#23398;&#12289;&#31038;&#20250;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#22914;&#20309;&#22312;&#27807;&#36890;&#28216;&#25103;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35843;&#21442;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#25345;LLMs&#20923;&#32467;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;&#36807;&#21435;&#30340;&#27807;&#36890;&#21644;&#32463;&#39564;&#36827;&#34892;&#25913;&#36827;&#12290;&#23545;&#20195;&#34920;&#24615;&#19988;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#27807;&#36890;&#28216;&#25103;&#8220;&#29436;&#20154;&#26432;&#8221;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#35843;&#25972;LLMs&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36827;&#34892;&#29436;&#20154;&#26432;&#28216;&#25103;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#20986;&#29616;&#20102;&#25112;&#30053;&#34892;&#20026;&#30340;&#36857;&#35937;&#65292;&#36825;&#34920;&#26126;&#22312;&#27807;&#36890;&#28216;&#25103;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#20351;&#29992;LLMs&#23558;&#20250;&#26159;&#19968;&#27425;&#23500;&#26377;&#25104;&#26524;&#30340;&#26053;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf'', demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36234;&#21335;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24320;&#21457;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;Alpaca&#12289;GPT4All&#21644;Chat-Doctor&#31561;&#24320;&#28304;&#39033;&#30446;&#30340;&#22823;&#35268;&#27169;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35757;&#32451;&#20102;&#22235;&#20010;&#27169;&#22411;&#65292;&#27492;&#20026;&#36234;&#21335;&#35821;&#30340;&#39318;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.04646</link><description>&lt;p&gt;
&#39640;&#25928;&#35843;&#20248;&#29992;&#20110;&#36234;&#21335;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Finetuning Large Language Models For Vietnamese Chatbot. (arXiv:2309.04646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36234;&#21335;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24320;&#21457;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;Alpaca&#12289;GPT4All&#21644;Chat-Doctor&#31561;&#24320;&#28304;&#39033;&#30446;&#30340;&#22823;&#35268;&#27169;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35757;&#32451;&#20102;&#22235;&#20010;&#27169;&#22411;&#65292;&#27492;&#20026;&#36234;&#21335;&#35821;&#30340;&#39318;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#12289;PaLM&#21644;LLaMa&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26368;&#36817;&#30340;&#25351;&#20196;&#35843;&#20248;&#36827;&#23637;&#20351;&#24471;LLMs&#33021;&#22815;&#25353;&#29031;&#29992;&#25143;&#25351;&#20196;&#24182;&#20135;&#29983;&#31867;&#20284;&#20154;&#31867;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#23454;&#29616;LLMs&#25152;&#38656;&#30340;&#39640;&#25104;&#26412;&#23545;&#23398;&#26415;&#30740;&#31350;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36234;&#21335;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;LLMs&#21644;&#25351;&#20196;&#35843;&#35856;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#24320;&#28304;&#39033;&#30446;&#65288;Alpaca&#12289;GPT4All&#21644;Chat-Doctor&#65289;&#30340;&#22823;&#35268;&#27169;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#36890;&#29992;&#21644;&#29305;&#23450;&#30340;&#21307;&#23398;&#39046;&#22495;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36234;&#21335;&#35821;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#22312;&#20004;&#20010;&#24320;&#25918;&#30340;LLMs&#19978;&#65306;Bloomz&#65288;&#22810;&#35821;&#35328;&#65289;&#21644;GPTJ-6B&#65288;&#36234;&#21335;&#35821;&#65289;&#65292;&#24471;&#21040;&#22235;&#20010;&#27169;&#22411;&#65306;Bloomz-Chat&#65292;Blo
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-4, PaLM, and LLaMa, have been shown to achieve remarkable performance across a variety of natural language tasks. Recent advancements in instruction tuning bring LLMs with ability in following user's instructions and producing human-like responses. However, the high costs associated with training and implementing LLMs pose challenges to academic research. Furthermore, the availability of pretrained LLMs and instruction-tune datasets for Vietnamese language is limited. To tackle these concerns, we leverage large-scale instruction-following datasets from open-source projects, namely Alpaca, GPT4All, and Chat-Doctor, which cover general domain and specific medical domain. To the best of our knowledge, these are the first instructional dataset for Vietnamese. Subsequently, we utilize parameter-efficient tuning through Low-Rank Adaptation (LoRA) on two open LLMs: Bloomz (Multilingual) and GPTJ-6B (Vietnamese), resulting four models: Bloomz-Chat, Blo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;QnotA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;NLP&#27169;&#22411;&#22312;&#27809;&#26377;&#30830;&#23450;&#31572;&#26696;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#21644;&#25552;&#20379;&#21512;&#29702;&#30340;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.04635</link><description>&lt;p&gt;
NLP&#27169;&#22411;&#33021;&#21542;&#8220;&#35782;&#21035;&#8221;&#65292;&#8220;&#21306;&#20998;&#8221;&#21644;&#8220;&#35777;&#26126;&#8221;&#27809;&#26377;&#30830;&#23450;&#31572;&#26696;&#30340;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can NLP Models 'Identify', 'Distinguish', and 'Justify' Questions that Don't have a Definitive Answer?. (arXiv:2309.04635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;QnotA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;NLP&#27169;&#22411;&#22312;&#27809;&#26377;&#30830;&#23450;&#31572;&#26696;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#21644;&#25552;&#20379;&#21512;&#29702;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#22312;&#21508;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#37027;&#20123;&#26377;&#27491;&#30830;&#21644;&#30830;&#23450;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#29992;&#25143;&#32463;&#24120;&#25552;&#20986;&#27809;&#26377;&#30830;&#23450;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#38169;&#35823;&#22320;&#22238;&#31572;&#36825;&#26679;&#30340;&#38382;&#39064;&#32943;&#23450;&#20250;&#25439;&#23475;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#33021;&#21542;&#20934;&#30830;&#35782;&#21035;&#36825;&#20123;&#38382;&#39064;&#24182;&#25552;&#20379;&#21512;&#29702;&#30340;&#22238;&#31572;&#65311;&#20026;&#20102;&#30740;&#31350;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;QnotA&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20116;&#20010;&#19981;&#21516;&#31867;&#22411;&#30340;&#27809;&#26377;&#30830;&#23450;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#20010;QnotA&#23454;&#20363;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#19968;&#20010;&#30456;&#24212;&#30340;QA&#23454;&#20363;&#65292;&#21363;&#19968;&#20010;&#8220;&#21487;&#20197;&#8221;&#22238;&#31572;&#30340;&#26367;&#20195;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19977;&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#20197;&#27979;&#35797;&#31995;&#32479;&#23545;QnotA&#38382;&#39064;&#30340;&#8220;&#35782;&#21035;&#8221;&#65292;&#8220;&#21306;&#20998;&#8221;&#21644;&#8220;&#35777;&#26126;&#8221;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#26159;&#21253;&#25324;GPT&#22312;&#20869;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20063;&#33021;&#22815;&#36827;&#34892;&#20934;&#30830;&#30340;&#35782;&#21035;&#21644;&#25552;&#20379;&#21512;&#29702;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though state-of-the-art (SOTA) NLP systems have achieved remarkable performance on a variety of language understanding tasks, they primarily focus on questions that have a correct and a definitive answer. However, in real-world applications, users often ask questions that don't have a definitive answer. Incorrectly answering such questions certainly hampers a system's reliability and trustworthiness. Can SOTA models accurately identify such questions and provide a reasonable response?  To investigate the above question, we introduce QnotA, a dataset consisting of five different categories of questions that don't have definitive answers. Furthermore, for each QnotA instance, we also provide a corresponding QA instance i.e. an alternate question that ''can be'' answered. With this data, we formulate three evaluation tasks that test a system's ability to 'identify', 'distinguish', and 'justify' QnotA questions. Through comprehensive experiments, we show that even SOTA models including GPT
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#26469;&#38142;&#25509;&#19981;&#21516;&#30340;&#30151;&#29366;&#28165;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#39044;&#35757;&#32451;&#30340;STS&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#28304;&#20013;&#39044;&#27979;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;74.8%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.04607</link><description>&lt;p&gt;
&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#38142;&#25509;&#30151;&#29366;&#28165;&#21333;
&lt;/p&gt;
&lt;p&gt;
Linking Symptom Inventories using Semantic Textual Similarity. (arXiv:2309.04607v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#26469;&#38142;&#25509;&#19981;&#21516;&#30340;&#30151;&#29366;&#28165;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#39044;&#35757;&#32451;&#30340;STS&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#28304;&#20013;&#39044;&#27979;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;74.8%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#22823;&#37327;&#30340;&#30151;&#29366;&#28165;&#21333;&#26469;&#34913;&#37327;&#20020;&#24202;&#30151;&#29366;&#65292;&#20294;&#36825;&#31181;&#22810;&#26679;&#24615;&#23548;&#33268;&#20102;&#20960;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26368;&#26174;&#33879;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#29615;&#22659;&#21644;&#30740;&#31350;&#30340;&#32467;&#26524;&#19981;&#21487;&#27604;&#36739;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#37325;&#22797;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;Semantic Textual Similarity&#65292;STS&#65289;&#26469;&#38142;&#25509;&#20808;&#21069;&#19981;&#30456;&#23481;&#30340;&#30151;&#29366;&#28165;&#21333;&#20013;&#30340;&#30151;&#29366;&#21644;&#35780;&#20998;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22235;&#20010;&#39044;&#35757;&#32451;&#30340;STS&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23545;&#26469;&#33258;16&#20010;&#22269;&#38469;&#25968;&#25454;&#28304;&#30340;6,607&#21517;&#21442;&#19982;&#32773;&#30340;&#22235;&#20010;&#19981;&#21516;&#28165;&#21333;&#20013;&#30340;&#25968;&#21315;&#20010;&#30151;&#29366;&#25551;&#36848;&#23545;&#36827;&#34892;&#30456;&#20851;&#20869;&#23481;&#30340;&#31579;&#26597; - &#36825;&#36890;&#24120;&#26159;&#19968;&#20010;&#38656;&#35201;&#19987;&#23478;&#23567;&#32452;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27169;&#22411;&#30340;&#20219;&#21153;&#26159;&#39044;&#27979;&#20845;&#39033;&#20219;&#21153;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#28165;&#21333;&#20013;&#30340;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#12290;STS&#26041;&#27861;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;74.8%&#30340;&#20934;&#30830;&#29575;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#34987;&#27979;&#35797;&#30340;&#27169;&#22411;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#32467;&#21512;&#35821;&#22659;&#21644;&#35821;&#20041;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#19987;&#23478;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#20135;&#29983;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
An extensive library of symptom inventories has been developed over time to measure clinical symptoms, but this variety has led to several long standing issues. Most notably, results drawn from different settings and studies are not comparable, which limits reproducibility. Here, we present an artificial intelligence (AI) approach using semantic textual similarity (STS) to link symptoms and scores across previously incongruous symptom inventories. We tested the ability of four pre-trained STS models to screen thousands of symptom description pairs for related content - a challenging task typically requiring expert panels. Models were tasked to predict symptom severity across four different inventories for 6,607 participants drawn from 16 international data sources. The STS approach achieved 74.8% accuracy across five tasks, outperforming other models tested. This work suggests that incorporating contextual, semantic information can assist expert decision-making processes, yielding gain
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#31350;&#20102;&#25968;&#25454;&#20462;&#21098;&#23545;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#22120;&#21644;&#20462;&#21098;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21518;&#35757;&#32451;&#30340;LLMs&#65292;&#20182;&#20204;&#21457;&#29616;&#22256;&#24785;&#24230;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#20248;&#20110;&#26356;&#21152;&#35745;&#31639;&#23494;&#38598;&#30340;&#35780;&#20998;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04564</link><description>&lt;p&gt;
&#24403;&#23569;&#23601;&#24847;&#21619;&#30528;&#26356;&#22810;&#65306;&#25506;&#31350;&#25968;&#25454;&#20462;&#21098;&#23545;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;( LLMS )&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale. (arXiv:2309.04564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04564
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#31350;&#20102;&#25968;&#25454;&#20462;&#21098;&#23545;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#22120;&#21644;&#20462;&#21098;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21518;&#35757;&#32451;&#30340;LLMs&#65292;&#20182;&#20204;&#21457;&#29616;&#22256;&#24785;&#24230;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#20248;&#20110;&#26356;&#21152;&#35745;&#31639;&#23494;&#38598;&#30340;&#35780;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22823;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMS )&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#36890;&#36807;&#20174;&#20114;&#32852;&#32593;&#19978;&#25235;&#21462;&#33719;&#21462;&#65292;&#23548;&#33268;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30001;&#22024;&#26434;&#30340;&#32593;&#32476;&#25991;&#26412;&#26500;&#25104;&#12290;&#36807;&#21435;&#65292;&#20026;&#20102;&#20943;&#23567;&#25968;&#25454;&#38598;&#24182;&#20351;&#20854;&#26356;&#39640;&#36136;&#37327;&#65292;&#37319;&#29992;&#20102;&#20197;&#35268;&#21017;&#20026;&#22522;&#30784;&#30340;&#25163;&#24037;&#21551;&#21457;&#24335;&#36807;&#28388;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#21487;&#20272;&#31639;&#30340;&#25968;&#25454;&#36136;&#37327;&#65292;&#20197;&#31995;&#32479;&#24615;&#22320;&#34913;&#37327;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27604;&#36739;&#65292;&#21253;&#25324;&#20351;&#29992;&#22256;&#24785;&#24230;&#30340;&#31616;&#21333;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;&#20197;&#21450;&#26356;&#22797;&#26434;&#21644;&#35745;&#31639;&#23494;&#38598;&#30340;&#38169;&#35823;L2-&#33539;&#25968;&#21644;&#35760;&#24518;&#21270;&#35780;&#20272;&#12290;&#36825;&#20123;&#25351;&#26631;&#29992;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#25490;&#24207;&#21644;&#20462;&#21098;&#65292;&#24182;&#38543;&#21518;&#27604;&#36739;&#22312;&#36825;&#20123;&#20462;&#21098;&#21518;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;LLMs&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22256;&#24785;&#24230;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#20248;&#20110;&#26356;&#21152;&#35745;&#31639;&#23494;&#38598;&#30340;&#35780;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring metho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#21253;&#21547;&#19977;&#20010;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04561</link><description>&lt;p&gt;
&#25913;&#36827;&#31264;&#23494;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#30340;&#19977;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Three Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding. (arXiv:2309.04561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#21253;&#21547;&#19977;&#20010;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#26159;&#25351;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#23450;&#20301;&#19977;&#32500;&#22330;&#26223;&#20013;&#34987;&#24341;&#29992;&#30340;&#29289;&#20307;&#30340;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#22312;&#33258;&#20027;&#23460;&#20869;&#26426;&#22120;&#20154;&#21040;AR/VR&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#30446;&#21069;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#26816;&#27979;&#26469;&#23436;&#25104;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#65292;&#21363;&#36890;&#36807;&#36793;&#30028;&#26694;&#26469;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#36827;&#34892;&#29289;&#29702;&#20132;&#20114;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36793;&#30028;&#26694;&#19981;&#36275;&#20197;&#25551;&#36848;&#29289;&#20307;&#30340;&#20960;&#20309;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31264;&#23494;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#30340;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#24341;&#29992;&#30340;&#19977;&#32500;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#20010;&#29420;&#31435;&#30340;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#36827;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#19979;&#32780;&#19978;&#30340;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#65292;&#26088;&#22312;&#28040;&#38500;&#23454;&#20363;&#38388;&#20851;&#31995;&#32447;&#32034;&#30340;&#27495;&#20041;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#36896;&#19968;&#20010;cont
&lt;/p&gt;
&lt;p&gt;
3D visual grounding is the task of localizing the object in a 3D scene which is referred by a description in natural language. With a wide range of applications ranging from autonomous indoor robotics to AR/VR, the task has recently risen in popularity. A common formulation to tackle 3D visual grounding is grounding-by-detection, where localization is done via bounding boxes. However, for real-life applications that require physical interactions, a bounding box insufficiently describes the geometry of an object. We therefore tackle the problem of dense 3D visual grounding, i.e. referral-based 3D instance segmentation. We propose a dense 3D grounding network ConcreteNet, featuring three novel stand-alone modules which aim to improve grounding performance for challenging repetitive instances, i.e. instances with distractors of the same semantic class. First, we introduce a bottom-up attentive fusion module that aims to disambiguate inter-instance relational cues, next we construct a cont
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#26410;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#26816;&#32034;&#21644;&#24635;&#32467;&#30456;&#20851;&#35777;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#35757;&#32451;LLM&#26469;&#25512;&#26029;&#24739;&#32773;&#26159;&#21542;&#24739;&#26377;&#29305;&#23450;&#30142;&#30149;&#65292;&#24182;&#19988;&#27169;&#22411;&#21487;&#20197;&#24635;&#32467;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34987;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04550</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#26816;&#32034;&#35777;&#25454;&#65306;&#21487;&#33021;&#24615;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges. (arXiv:2309.04550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#26410;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#26816;&#32034;&#21644;&#24635;&#32467;&#30456;&#20851;&#35777;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#35757;&#32451;LLM&#26469;&#25512;&#26029;&#24739;&#32773;&#26159;&#21542;&#24739;&#26377;&#29305;&#23450;&#30142;&#30149;&#65292;&#24182;&#19988;&#27169;&#22411;&#21487;&#20197;&#24635;&#32467;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34987;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#19982;&#24433;&#20687;&#25968;&#25454;&#20114;&#34917;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#21487;&#20197;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35786;&#26029;&#25552;&#20379;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#38480;&#21046;&#21644;&#19982;&#27599;&#20010;&#24739;&#32773;&#30456;&#20851;&#30340;&#22823;&#37327;&#31508;&#35760;&#20351;&#24471;&#25163;&#21160;&#27983;&#35272;&#27492;&#31867;&#25968;&#25454;&#20197;&#35782;&#21035;&#30456;&#20851;&#35777;&#25454;&#22312;&#23454;&#36341;&#20013;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#24335;&#26469;&#22788;&#29702;&#26410;&#32467;&#26500;&#21270;&#30340;EHR&#25968;&#25454;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26426;&#21046;&#26469;&#39640;&#25928;&#22320;&#26816;&#32034;&#21644;&#24635;&#32467;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20851;&#30340;&#26410;&#32467;&#26500;&#21270;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;LLM&#65288;Flan-T5 XXL&#65289;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35201;&#27714;LLM&#25512;&#26029;&#19968;&#20010;&#24739;&#32773;&#26159;&#21542;&#26377;&#25110;&#22788;&#20110;&#26576;&#31181;&#29305;&#23450;&#30142;&#30149;&#30340;&#39118;&#38505;&#65292;&#24182;&#22312;&#26159;&#30340;&#24773;&#20917;&#19979;&#25552;&#31034;&#27169;&#22411;&#24635;&#32467;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#24341;&#20837;&#25918;&#23556;&#31185;&#21307;&#29983;&#36827;&#34892;&#25163;&#21160;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#25552;&#20379;&#30340;&#36755;&#20986;&#22987;&#32456;&#20248;&#20110;&#26631;&#20934;&#30340;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unstructured Electronic Health Record (EHR) data often contains critical information complementary to imaging data that would inform radiologists' diagnoses. However, time constraints and the large volume of notes frequently associated with individual patients renders manual perusal of such data to identify relevant evidence infeasible in practice. Modern Large Language Models (LLMs) provide a flexible means of interacting with unstructured EHR data, and may provide a mechanism to efficiently retrieve and summarize unstructured evidence relevant to a given query. In this work, we propose and evaluate an LLM (Flan-T5 XXL) for this purpose. Specifically, in a zero-shot setting we task the LLM to infer whether a patient has or is at risk of a particular condition; if so, we prompt the model to summarize the supporting evidence. Enlisting radiologists for manual evaluation, we find that this LLM-based approach provides outputs consistently preferred to a standard information retrieval base
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20013;&#22269;&#32929;&#31080;&#25919;&#31574;&#26816;&#32034;&#25968;&#25454;&#38598;&#65288;CSPRD&#65289;&#65292;&#25552;&#20379;&#20102;700+&#26465;&#26631;&#27880;&#30340;&#25307;&#32929;&#35828;&#26126;&#20070;&#27573;&#33853;&#65292;&#36890;&#36807;&#35789;&#27719;&#12289;&#23884;&#20837;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#21452;&#32534;&#30721;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;CSPRD&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04389</link><description>&lt;p&gt;
CSPRD: &#20013;&#22269;&#32929;&#31080;&#24066;&#22330;&#37329;&#34701;&#25919;&#31574;&#26816;&#32034;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CSPRD: A Financial Policy Retrieval Dataset for Chinese Stock Market. (arXiv:2309.04389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20013;&#22269;&#32929;&#31080;&#25919;&#31574;&#26816;&#32034;&#25968;&#25454;&#38598;&#65288;CSPRD&#65289;&#65292;&#25552;&#20379;&#20102;700+&#26465;&#26631;&#27880;&#30340;&#25307;&#32929;&#35828;&#26126;&#20070;&#27573;&#33853;&#65292;&#36890;&#36807;&#35789;&#27719;&#12289;&#23884;&#20837;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#21452;&#32534;&#30721;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;CSPRD&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#24341;&#36215;&#20102;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#24182;&#22312;&#31264;&#23494;&#27573;&#33853;&#26816;&#32034;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20854;&#30446;&#30340;&#26159;&#26816;&#32034;&#32473;&#23450;&#38382;&#39064;&#30340;&#30456;&#20851;&#27573;&#33853;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#20351;&#29992;&#20102;&#36890;&#24120;&#24120;&#35782;&#30340;&#20107;&#23454;&#24615;&#26597;&#35810;&#26469;&#35780;&#20272;&#27169;&#22411;&#65292;&#32780;&#37329;&#34701;&#21644;&#32463;&#27982;&#31561;&#19987;&#19994;&#39046;&#22495;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#21644;&#19987;&#23478;&#27880;&#37322;&#32780;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#25919;&#31574;&#26816;&#32034;&#65292;&#36890;&#36807;&#24341;&#20837;&#20013;&#22269;&#32929;&#31080;&#25919;&#31574;&#26816;&#32034;&#25968;&#25454;&#38598;&#65288;CSPRD&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#30001;&#26377;&#32463;&#39564;&#30340;&#19987;&#23478;&#23545;&#26469;&#33258;&#25105;&#20204;&#25910;&#38598;&#30340;&#20013;&#22269;&#25919;&#31574;&#35821;&#26009;&#24211;&#20013;&#30340;10k+&#26465;&#30446;&#30340;&#30456;&#20851;&#25991;&#31456;&#36827;&#34892;&#26631;&#27880;&#30340;700+&#26465;&#25307;&#32929;&#35828;&#26126;&#20070;&#27573;&#33853;&#12290;&#23545;&#35789;&#27719;&#12289;&#23884;&#20837;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#21452;&#32534;&#30721;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;CSPRD&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20063;&#25552;&#31034;&#20102;&#25913;&#36827;&#30340;&#20016;&#23500;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, great advances in pre-trained language models (PLMs) have sparked considerable research focus and achieved promising performance on the approach of dense passage retrieval, which aims at retrieving relative passages from massive corpus with given questions. However, most of existing datasets mainly benchmark the models with factoid queries of general commonsense, while specialised fields such as finance and economics remain unexplored due to the deficiency of large-scale and high-quality datasets with expert annotations. In this work, we propose a new task, policy retrieval, by introducing the Chinese Stock Policy Retrieval Dataset (CSPRD), which provides 700+ prospectus passages labeled by experienced experts with relevant articles from 10k+ entries in our collected Chinese policy corpus. Experiments on lexical, embedding and fine-tuned bi-encoder models show the effectiveness of our proposed CSPRD yet also suggests ample potential for improvement. Our best performing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#25216;&#26415;&#23454;&#29616;&#20102;&#35821;&#38899;&#35821;&#20041;&#29702;&#35299;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#24494;&#35843;&#19979;&#28216;&#20219;&#21153;&#21518;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02780</link><description>&lt;p&gt;
GRASS: &#35821;&#38899;&#35821;&#20041;&#29702;&#35299;&#32479;&#19968;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GRASS: Unified Generation Model for Speech Semantic Understanding. (arXiv:2309.02780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#25216;&#26415;&#23454;&#29616;&#20102;&#35821;&#38899;&#35821;&#20041;&#29702;&#35299;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#24494;&#35843;&#19979;&#28216;&#20219;&#21153;&#21518;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;&#35821;&#38899;&#35821;&#20041;&#29702;&#35299;&#30340;&#25351;&#20196;&#24494;&#35843;&#25216;&#26415;&#65292;&#35813;&#26694;&#26550;&#26681;&#25454;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25552;&#31034;&#20026;&#38899;&#39057;&#25968;&#25454;&#29983;&#25104;&#35821;&#20041;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#25351;&#20196;-&#35821;&#38899;&#23545;&#26159;&#36890;&#36807;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#26500;&#24314;&#30340;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#19979;&#28216;&#20219;&#21153;&#21518;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#22312;&#35821;&#38899;&#21040;&#35821;&#20041;&#20219;&#21153;&#30340;&#25351;&#20196;&#24494;&#35843;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the instruction fine-tuning technique for speech semantic understanding by introducing a unified end-to-end (E2E) framework that generates semantic labels conditioned on a task-related prompt for audio data. We pre-train the model using large and diverse data, where instruction-speech pairs are constructed via a text-to-speech (TTS) system. Extensive experiments demonstrate that our proposed model significantly outperforms state-of-the-art (SOTA) models after fine-tuning downstream tasks. Furthermore, the proposed model achieves competitive performance in zero-shot and few-shot scenarios. To facilitate future work on instruction fine-tuning for speech-to-semantic tasks, we release our instruction dataset and code.
&lt;/p&gt;</description></item><item><title>CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.01940</link><description>&lt;p&gt;
CodeApex&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. (arXiv:2309.01940v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01940
&lt;/p&gt;
&lt;p&gt;
CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#30340;&#32534;&#31243;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#26085;&#30410;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CodeApex&#65292;&#19968;&#31181;&#21452;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#19987;&#27880;&#20110;LLM&#30340;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;CodeApex&#21253;&#25324;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65306;&#27010;&#24565;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#36339;&#25512;&#29702;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#32534;&#31243;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;CodeApex&#21033;&#29992;&#31639;&#27861;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;&#27979;&#35797;&#29992;&#20363;&#26469;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;14&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#65292;&#21253;&#25324;&#36890;&#29992;&#21644;&#19987;&#38376;&#21270;&#27169;&#22411;&#12290;GPT&#23637;&#29616;&#20986;&#26368;&#20339;&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;&#32422;50%&#21644;56%&#12290;&#32534;&#31243;&#20219;&#21153;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#24076;&#26395;CodeApex&#33021;&#22815;&#20026;&#35780;&#20272;&#32534;&#31243;&#33021;&#21147;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. We propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension and code generation abilities of LLMs. CodeApex comprises three types of multiple-choice questions: conceptual understanding, commonsense reasoning, and multi-hop reasoning, designed to evaluate LLMs on programming comprehension tasks. Additionally, CodeApex utilizes algorithmic questions and corresponding test cases to assess the code quality generated by LLMs. We evaluate 14 state-of-the-art LLMs, including both general-purpose and specialized models. GPT exhibits the best programming capabilities, achieving approximate accuracies of 50% and 56% on the two tasks, respectively. There is still significant room for improvement in programming tasks. We hope that CodeApex can serve as a reference for evaluating the co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25554;&#20540;&#26469;&#35299;&#20915;&#22312;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#24341;&#36215;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.01717</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#30340;&#30740;&#31350;&#25552;&#26696;&#20027;&#39064;&#25512;&#29702;&#20013;&#30340;&#36328;&#23398;&#31185;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#20855;&#26377;&#36873;&#25321;&#24615;&#25554;&#20540;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation. (arXiv:2309.01717v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25554;&#20540;&#26469;&#35299;&#20915;&#22312;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#24341;&#36215;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#26696;&#20027;&#39064;&#25512;&#29702;&#30340;&#30446;&#26631;&#26159;&#20174;&#36164;&#21161;&#26426;&#26500;&#23450;&#20041;&#30340;&#23398;&#31185;&#20307;&#31995;&#20013;&#33719;&#21462;&#26368;&#21512;&#36866;&#30340;&#23398;&#31185;&#21010;&#20998;&#65292;&#28982;&#21518;&#26426;&#26500;&#23558;&#26681;&#25454;&#36825;&#31181;&#21010;&#20998;&#20174;&#20854;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#21516;&#34892;&#35780;&#23457;&#19987;&#23478;&#12290;&#33258;&#21160;&#21270;&#30340;&#20027;&#39064;&#25512;&#29702;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#20027;&#39064;&#22635;&#20889;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24357;&#34917;&#36164;&#21161;&#26426;&#26500;&#21644;&#39033;&#30446;&#30003;&#35831;&#20154;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#12290;&#29616;&#26377;&#26041;&#27861;&#23558;&#20854;&#24314;&#27169;&#20026;&#23618;&#27425;&#24615;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36845;&#20195;&#22320;&#25512;&#29702;&#26368;&#21512;&#36866;&#30340;&#20027;&#39064;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#65292;&#23548;&#33268;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#23558;&#36328;&#23398;&#31185;&#25552;&#26696;&#24402;&#31867;&#20026;&#38750;&#36328;&#23398;&#31185;&#65292;&#36896;&#25104;&#22312;&#19987;&#23478;&#20998;&#37197;&#36807;&#31243;&#20013;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;&#25105;&#20204;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#21602;&#65311;
&lt;/p&gt;
&lt;p&gt;
The objective of topic inference in research proposals aims to obtain the most suitable disciplinary division from the discipline system defined by a funding agency. The agency will subsequently find appropriate peer review experts from their database based on this division. Automated topic inference can reduce human errors caused by manual topic filling, bridge the knowledge gap between funding agencies and project applicants, and improve system efficiency. Existing methods focus on modeling this as a hierarchical multi-label classification problem, using generative models to iteratively infer the most appropriate topic information. However, these methods overlook the gap in scale between interdisciplinary research proposals and non-interdisciplinary ones, leading to an unjust phenomenon where the automated inference system categorizes interdisciplinary proposals as non-interdisciplinary, causing unfairness during the expert assignment. How can we address this data imbalance issue und
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#23558;&#20219;&#21153;&#20449;&#24687;&#19982;MoE&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#26032;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15772</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Task-Based MoE for Multitask Multilingual Machine Translation. (arXiv:2308.15772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#23558;&#20219;&#21153;&#20449;&#24687;&#19982;MoE&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#26032;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#30340;&#22810;&#31181;&#24212;&#29992;&#20013;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;MoE&#23454;&#29616;&#26159;&#20219;&#21153;&#26080;&#20851;&#30340;&#65292;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#25152;&#26377;&#26631;&#35760;&#20197;&#30456;&#21516;&#26041;&#24335;&#22788;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#21160;&#24577;&#22522;&#20110;&#20219;&#21153;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;MoE&#27169;&#22411;&#30340;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#23558;&#20219;&#21153;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#19978;&#30340;&#20248;&#21183;&#12290;&#20511;&#21161;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#39640;&#25928;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-experts (MoE) architecture has been proven a powerful method for diverse tasks in training deep models in many applications. However, current MoE implementations are task agnostic, treating all tokens from different tasks in the same manner. In this work, we instead design a novel method that incorporates task information into MoE models at different granular levels with shared dynamic task-based adapters. Our experiments and analysis show the advantages of our approaches over the dense and canonical MoE models on multi-task multilingual machine translations. With task-specific adapters, our models can additionally generalize to new tasks efficiently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13916</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#20247;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#19981;&#23436;&#25972;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#19977;&#20803;&#32452;&#35270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#22270;&#35889;LLM&#65288;KG-LLM&#65289;&#65292;&#26469;&#23545;&#36825;&#20123;&#19977;&#20803;&#32452;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#31561;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;LLaMA-7B&#65292;ChatGLM-6B&#65289;&#20248;&#20110;&#26368;&#26032;&#30340;ChatGPT&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLLM-DataEngine&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#24369;&#28857;&#65292;&#29983;&#25104;&#36866;&#24403;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.13566</link><description>&lt;p&gt;
MLLM-DataEngine&#65306;&#19968;&#31181;MLLM&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MLLM-DataEngine: An Iterative Refinement Approach for MLLM. (arXiv:2308.13566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLLM-DataEngine&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#24369;&#28857;&#65292;&#29983;&#25104;&#36866;&#24403;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25351;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#29420;&#31435;&#24615;&#20351;&#24471;&#24403;&#21069;&#30340;MLLM&#24456;&#38590;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#20154;&#21147;&#25104;&#26412;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23553;&#38381;&#24490;&#29615;&#31995;&#32479;MLLM-DataEngine&#65292;&#23427;&#36830;&#25509;&#20102;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22312;&#27599;&#20010;&#24490;&#29615;&#36845;&#20195;&#20013;&#65292;MLLM-DataEngine&#39318;&#20808;&#26681;&#25454;&#35780;&#20272;&#32467;&#26524;&#20998;&#26512;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#28982;&#21518;&#29983;&#25104;&#21512;&#36866;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#29992;&#20110;&#19979;&#19968;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#19982;&#22522;&#20934;&#27979;&#35797;&#20998;&#31163;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#37117;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental datas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.13032</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 GPT&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 Large Language Model (LLM) &#23545;&#37329;&#34701;&#26032;&#38395;&#36827;&#34892;&#22810;&#20219;&#21153;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;PEFT/LoRA&#26041;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#20027;&#35201;&#21253;&#25324;&#20174;&#37329;&#34701;&#24066;&#22330;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#12289;&#31361;&#20986;&#25991;&#26412;&#30340;&#20027;&#35201;&#35266;&#28857;&#12289;&#23545;&#25991;&#26412;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20855;&#26377;&#36866;&#24403;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#31561;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#22810;&#20219;&#21153;&#30340;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;&#65292;&#20854;&#21709;&#24212;&#30340;&#32467;&#26500;&#21487;&#20197;&#37096;&#20998;&#20026;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#21478;&#19968;&#37096;&#20998;&#25968;&#25454;&#21487;&#20197;&#37319;&#29992;JSON&#26684;&#24335;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#23450;&#37327;&#30446;&#26631;&#21464;&#37327;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#36339;&#38382;&#31572;&#30340;&#33258;&#25105;&#36845;&#20195;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65288;HopPG&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#22788;&#29702;&#24322;&#26500;&#30693;&#35782;&#21644;&#22810;&#36339;&#38382;&#39064;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24182;&#21033;&#29992;&#20102;&#21069;&#20960;&#36339;&#30340;&#25191;&#34892;&#32467;&#26524;&#26469;&#29983;&#25104;&#19979;&#19968;&#36339;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2308.11257</link><description>&lt;p&gt;
HopPG&#65306;&#33258;&#25105;&#36845;&#20195;&#30340;&#24322;&#26500;&#30693;&#35782;&#22810;&#36339;&#38382;&#31572;&#31243;&#24207;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge. (arXiv:2308.11257v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#36339;&#38382;&#31572;&#30340;&#33258;&#25105;&#36845;&#20195;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65288;HopPG&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#22788;&#29702;&#24322;&#26500;&#30693;&#35782;&#21644;&#22810;&#36339;&#38382;&#39064;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24182;&#21033;&#29992;&#20102;&#21069;&#20960;&#36339;&#30340;&#25191;&#34892;&#32467;&#26524;&#26469;&#29983;&#25104;&#19979;&#19968;&#36339;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#35299;&#26512;&#26041;&#27861;&#26159;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#12290;&#23427;&#36890;&#24120;&#22522;&#20110;&#38382;&#39064;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#24211;&#36827;&#34892;&#25512;&#29702;&#24471;&#20986;&#31572;&#26696;&#12290;&#30001;&#20110;&#36825;&#31181;&#20869;&#22312;&#26426;&#21046;&#65292;&#23427;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35821;&#20041;&#35299;&#26512;&#26041;&#27861;&#36890;&#24120;&#22312;&#25191;&#34892;&#20043;&#21069;&#29983;&#25104;&#23436;&#25972;&#30340;&#31243;&#24207;&#65292;&#36825;&#22312;&#22788;&#29702;&#22810;&#36339;&#38382;&#39064;&#21644;&#24322;&#26500;&#30693;&#35782;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#39318;&#20808;&#65292;&#23436;&#25972;&#30340;&#22810;&#36339;&#31243;&#24207;&#20381;&#36182;&#20110;&#22810;&#20010;&#24322;&#26500;&#30340;&#25903;&#25345;&#20107;&#23454;&#65292;&#27169;&#22411;&#24456;&#38590;&#21516;&#26102;&#33719;&#21462;&#36825;&#20123;&#20107;&#23454;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#21069;&#20960;&#36339;&#25191;&#34892;&#32467;&#26524;&#19982;&#24403;&#21069;&#36339;&#31243;&#24207;&#29983;&#25104;&#20043;&#38388;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#30693;&#35782;&#30340;&#33258;&#25105;&#36845;&#20195;&#22810;&#36339;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65288;HopPG&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#21069;&#20960;&#36339;&#30340;&#25191;&#34892;&#32467;&#26524;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#29983;&#25104;&#19979;&#19968;&#36339;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The semantic parsing-based method is an important research branch for knowledge-based question answering. It usually generates executable programs lean upon the question and then conduct them to reason answers over a knowledge base. Benefit from this inherent mechanism, it has advantages in the performance and the interpretability. However,traditional semantic parsing methods usually generate a complete program before executing it, which struggles with multi-hop question answering over heterogeneous knowledge. Firstly,a complete multi-hop program relies on multiple heterogeneous supporting facts, and it is difficult for models to receive these facts simultaneously. Secondly,these methods ignore the interaction information between the previous-hop execution result and the current-hop program generation. To alleviate these challenges, we propose a self-iterative framework for multi-hop program generation (HopPG) over heterogeneous knowledge, which leverages the previous-hop execution res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25968;&#25454;&#19978;&#35299;&#20915;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;LLM&#22312;&#29702;&#35299;&#22270;&#25968;&#25454;&#12289;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#21644;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11224</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24615;&#33021;&#27934;&#23519;&#19982;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. (arXiv:2308.11224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25968;&#25454;&#19978;&#35299;&#20915;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;LLM&#22312;&#29702;&#35299;&#22270;&#25968;&#25454;&#12289;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#21644;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;LLM&#22312;&#22270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#20010;LLM&#22312;&#35299;&#20915;&#20960;&#20010;&#22270;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#29702;&#35299;&#33021;&#21147;&#12289;&#27491;&#30830;&#24615;&#12289;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;1) LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22270;&#25968;&#25454;&#65292;&#24182;&#25512;&#29702;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;2) GPT&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#36923;&#36753;&#21644;&#36830;&#36143;&#30340;&#32467;&#26524;&#65292;&#22312;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#12290;3) &#25152;&#26377;&#34987;&#26816;&#27979;&#30340;LLM&#22312;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#37117;&#38754;&#20020;&#25361;&#25112;&#65292;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31561;&#25216;&#26415;&#26174;&#31034;&#20986;&#25928;&#26524;&#19979;&#38477;&#12290;4) GPT&#27169;&#22411;&#22312;&#22810;&#31572;&#26696;&#20219;&#21153;&#20013;&#32463;&#24120;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#65292;&#24341;&#21457;&#30495;&#23454;&#24615;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;5) GPT&#27169;&#22411;&#23545;&#20854;&#36755;&#20986;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20449;&#24515;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#30699;&#27491;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-4&#26174;&#31034;&#20986;&#20102;&#19981;&#21516;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has dem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.03929</link><description>&lt;p&gt;
ChatGPT&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#25991;&#26412;&#20013;&#24314;&#31435;&#20449;&#20219;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#30340;&#21307;&#23398;&#25991;&#29486;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#26500;&#24314;&#20102;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998;&#20107;&#23454;&#20449;&#24687;&#21644;&#26410;&#32463;&#39564;&#35777;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#8220;&#20154;&#31867;&#30142;&#30149;&#21644;&#30151;&#29366;&#8221;&#26597;&#35810;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#32534;&#35793;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#27169;&#25311;&#25991;&#31456;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#65288;PubMed&#21644;ChatGPT&#65289;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;10&#32452;&#27599;&#32452;250&#20010;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#31181;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#65288;DOID&#65289;&#21644;&#30151;&#29366;&#26412;&#20307;&#65288;SYMP&#65289;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#20559;&#24046;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;GPT&#30142;&#30149;-&#30151;&#29366;&#38142;&#25509;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#22312;&#22122;&#22768;&#12289;&#20551;&#35774;&#21644;&#37325;&#35201;&#21457;&#29616;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#65306;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;ChatGPT&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;PubMed&#35745;&#25968;&#33719;&#24471;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
&lt;/p&gt;</description></item><item><title>CompLog&#26159;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;Kolmogorov&#22797;&#26434;&#24615;&#26367;&#20195;&#27010;&#29575;&#25512;&#29702;&#65292;&#23454;&#29616;&#35745;&#31639;&#26576;&#31181;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#25551;&#36848;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15453</link><description>&lt;p&gt;
&#20174;&#27010;&#29575;&#32534;&#31243;&#21040;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
From Probabilistic Programming to Complexity-based Programming. (arXiv:2307.15453v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15453
&lt;/p&gt;
&lt;p&gt;
CompLog&#26159;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;Kolmogorov&#22797;&#26434;&#24615;&#26367;&#20195;&#27010;&#29575;&#25512;&#29702;&#65292;&#23454;&#29616;&#35745;&#31639;&#26576;&#31181;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#25551;&#36848;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CompLog&#30340;&#26032;&#22411;&#35745;&#31639;&#26694;&#26550;&#30340;&#20027;&#35201;&#29305;&#28857;&#21644;&#21021;&#27493;&#23454;&#29616;&#12290;CompLog&#20511;&#37492;&#20102;&#27010;&#29575;&#32534;&#31243;&#31995;&#32479;&#65288;&#22914;ProbLog&#65289;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#24182;&#22522;&#20110;Simplicity&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#36890;&#36807;ASP&#31243;&#24207;&#30340;min-path&#25628;&#32034;&#35745;&#31639;&#20004;&#31181;Kolmogorov&#22797;&#26434;&#24615;&#65292;&#32780;&#19981;&#26159;&#27010;&#29575;&#25512;&#29702;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#25143;&#33021;&#22815;&#35745;&#31639;&#26576;&#20010;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;ex-post&#21644;ex-ante&#24230;&#37327;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#21518;&#39564;&#21644;&#20808;&#39564;&#20027;&#35266;&#27010;&#29575;&#12290;&#35745;&#31639;&#22522;&#20110;&#36890;&#36807;&#25551;&#36848;&#24615;&#35859;&#35789;&#20043;&#38388;&#30340;&#22240;&#26524;&#21644;&#25551;&#36848;&#24615;&#20851;&#31995;&#21152;&#26435;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#35268;&#33539;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#20960;&#20010;&#24212;&#29992;&#31034;&#20363;&#65306;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the main characteristics and a preliminary implementation of a novel computational framework named CompLog. Inspired by probabilistic programming systems like ProbLog, CompLog builds upon the inferential mechanisms proposed by Simplicity Theory, relying on the computation of two Kolmogorov complexities (here implemented as min-path searches via ASP programs) rather than probabilistic inference. The proposed system enables users to compute ex-post and ex-ante measures of unexpectedness of a certain situation, mapping respectively to posterior and prior subjective probabilities. The computation is based on the specification of world and mental models by means of causal and descriptive relations between predicates weighted by complexity. The paper illustrates a few examples of application: generating relevant descriptions, and providing alternative approaches to disjunction and to negation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.15217</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#30340;&#25216;&#26415;&#12290;RLHF&#24050;&#25104;&#20026;&#24494;&#35843;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26680;&#24515;&#26041;&#27861;&#12290;&#23613;&#31649;&#22914;&#27492;&#21463;&#27426;&#36814;&#65292;&#20294;&#31995;&#32479;&#24615;&#22320;&#31995;&#32479;&#21270;&#20854;&#32570;&#38519;&#30340;&#20844;&#24320;&#24037;&#20316;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#35843;&#26597;&#20102;RLHF&#21450;&#30456;&#20851;&#26041;&#27861;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65307;&#65288;2&#65289;&#27010;&#36848;&#20102;&#20102;&#35299;&#12289;&#25913;&#36827;&#21644;&#34917;&#20805;RLHF&#30340;&#23454;&#36341;&#25216;&#26415;&#65307;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20986;&#20102;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#20197;&#25913;&#36827;RLHF&#31995;&#32479;&#30340;&#31038;&#20250;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;RLHF&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20197;&#22810;&#26041;&#38754;&#26041;&#27861;&#24320;&#21457;&#26356;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11788</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#21363;&#20351;&#26159;&#26368;&#24494;&#23567;&#30340;&#36136;&#37327;&#25913;&#36827;&#20063;&#33021;&#20135;&#29983;&#24040;&#22823;&#20215;&#20540;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#37329;&#34701;&#26159;&#26089;&#26399;&#37327;&#23376;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#32773;&#12290;&#22312;&#36805;&#36895;&#21457;&#23637;&#30340;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;DisCoCat&#21644;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QNLP)&#36825;&#20004;&#31181;&#20013;&#24515;&#26041;&#27861;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ChatGPT&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21253;&#21547;1000&#22810;&#20010;&#30495;&#23454;&#21477;&#23376;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;QLSTM&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;DisCoCat&#24555;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#21487;&#29992;&#30340;&#36719;&#20214;&#23454;&#29616;&#20013;&#20063;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an application domain where the slightest qualitative improvements can yield immense value, finance is a promising candidate for early quantum advantage. Focusing on the rapidly advancing field of Quantum Natural Language Processing (QNLP), we explore the practical applicability of the two central approaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the problem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data generation approach, we conduct a case study with more than 1000 realistic sentences and find that QLSTMs can be trained substantially faster than DisCoCat while also achieving close to classical results for their available software implementations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32534;&#31243;&#38382;&#39064;&#20013;&#21435;&#38500;&#37325;&#22797;&#31243;&#24207;&#24182;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#20197;&#40723;&#21169;&#23398;&#20064;&#32773;&#21442;&#32771;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.07940</link><description>&lt;p&gt;
&#20026;&#25512;&#33616;&#21442;&#32771;&#35299;&#20915;&#26041;&#26696;&#32780;&#21435;&#37325;&#21644;&#25490;&#21517;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions. (arXiv:2307.07940v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32534;&#31243;&#38382;&#39064;&#20013;&#21435;&#38500;&#37325;&#22797;&#31243;&#24207;&#24182;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#20197;&#40723;&#21169;&#23398;&#20064;&#32773;&#21442;&#32771;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#65292;&#21442;&#32771;&#20854;&#20182;&#29992;&#25143;&#32534;&#20889;&#30340;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#23545;&#23398;&#20064;&#32773;&#24456;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22312;&#32447;&#35780;&#27979;&#31995;&#32479;&#21482;&#26159;&#21015;&#20986;&#29992;&#25143;&#25552;&#20132;&#30340;&#25152;&#26377;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#20379;&#21442;&#32771;&#65292;&#24182;&#26681;&#25454;&#25552;&#20132;&#26085;&#26399;&#12289;&#25191;&#34892;&#26102;&#38388;&#25110;&#29992;&#25143;&#35780;&#20998;&#36827;&#34892;&#25490;&#24207;&#65292;&#24573;&#35270;&#20102;&#31243;&#24207;&#33021;&#22815;&#25104;&#20026;&#21442;&#32771;&#30340;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22826;&#22810;&#37325;&#22797;&#21644;&#36817;&#20284;&#37325;&#22797;&#30340;&#31243;&#24207;&#65292;&#29992;&#25143;&#24456;&#38590;&#21442;&#32771;&#22810;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#20026;&#20102;&#28608;&#21169;&#23398;&#20064;&#32773;&#21442;&#32771;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27599;&#20010;&#32534;&#31243;&#38382;&#39064;&#20013;&#21435;&#37325;&#21644;&#25490;&#21517;&#24120;&#35265;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#26356;&#22810;&#37325;&#22797;&#30340;&#31243;&#24207;&#37319;&#29992;&#26356;&#24120;&#35265;&#30340;&#26041;&#27861;&#24182;&#21487;&#20316;&#20026;&#21442;&#32771;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#21024;&#38500;&#20102;&#36817;&#20284;&#37325;&#22797;&#30340;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#65292;&#24182;&#26681;&#25454;&#37325;&#22797;&#35745;&#25968;&#23545;&#21807;&#19968;&#30340;&#31243;&#24207;&#36827;&#34892;&#25490;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#21435;&#37325;&#21644;&#25490;&#21517;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Referring to the solution programs written by the other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the program can be a reference. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate the learners to refer to various solutions to learn the better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Based on the hypothesis that the more duplicated programs adopt a more common approach and can be a reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.03941</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65306;&#28085;&#20041;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#26368;&#21021;&#26159;&#30001;&#35895;&#27468;&#35199;&#29677;&#29273;&#19982;&#22467;&#20811;&#26031;&#20869;&#22612;&#32034;&#22996;&#21592;&#20250;(Mario Costeja Gonz\'alez)&#20043;&#38388;&#30340;&#23448;&#21496;&#32467;&#26524;&#32780;&#30830;&#31435;&#30340;&#65292;&#24182;&#19988;&#21518;&#26469;&#34987;&#20316;&#20026;&#27431;&#27954;&#32852;&#30431;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#19979;&#30340;&#21024;&#38500;&#26435;&#12290;RTBF&#20801;&#35768;&#20010;&#20154;&#21521;&#32452;&#32455;&#35831;&#27714;&#21024;&#38500;&#20010;&#20154;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#65292;&#20010;&#20154;&#21487;&#20197;&#21521;&#32452;&#32455;&#21457;&#36865;&#35831;&#27714;&#65292;&#25490;&#38500;&#20182;&#20204;&#30340;&#20449;&#24687;&#22312;&#26597;&#35810;&#32467;&#26524;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#20854;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;LLM&#21551;&#29992;&#30340;&#36719;&#20214;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#25490;&#38500;&#22312;RTBF&#20043;&#22806;&#12290;&#30456;&#27604;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;LLMs&#20197;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#22788;&#29702;&#20449;&#24687;&#65292;&#36825;&#20026;&#31526;&#21512;RTBF&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20197;&#31526;&#21512;RTBF&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of machine unle
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;$\alpha$-$\beta$-&#20998;&#35299;&#30340;&#27010;&#24565;&#65292;&#23558;Simon&#21516;&#20313;&#29305;&#24449;&#21270;&#20026;$1$-&#26222;&#36941;&#24615;&#21333;&#35789;&#65292;&#24182;&#24212;&#29992;&#20110;&#20108;&#20803;&#21333;&#35789;&#30340;&#23436;&#20840;&#21051;&#30011;&#21644;&#21516;&#20313;&#25351;&#25968;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.14192</link><description>&lt;p&gt;
$\alpha$-$\beta$-&#20998;&#35299;&#21450;Simon&#21516;&#20313;&#30340;&#20108;&#20803;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
$\alpha$-$\beta$-Factorization and the Binary Case of Simon's Congruence. (arXiv:2306.14192v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;$\alpha$-$\beta$-&#20998;&#35299;&#30340;&#27010;&#24565;&#65292;&#23558;Simon&#21516;&#20313;&#29305;&#24449;&#21270;&#20026;$1$-&#26222;&#36941;&#24615;&#21333;&#35789;&#65292;&#24182;&#24212;&#29992;&#20110;&#20108;&#20803;&#21333;&#35789;&#30340;&#23436;&#20840;&#21051;&#30011;&#21644;&#21516;&#20313;&#25351;&#25968;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1991&#24180;&#65292;H\'ebrard&#24341;&#20837;&#20102;&#19968;&#31181;&#21333;&#35789;&#30340;&#20998;&#35299;&#26041;&#27861;&#65292;&#34987;&#35777;&#26126;&#26159;&#30740;&#31350;&#21333;&#35789;&#30340;&#31163;&#25955;&#22240;&#23376;&#65288;&#20063;&#31216;&#20026;&#65288;&#31163;&#25955;&#65289;&#23376;&#20018;&#25110;&#23376;&#24207;&#21015;&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22522;&#20110;&#27492;&#65292;Karandikar&#21644;Schnoebelen&#39318;&#20808;&#24341;&#20837;&#20102;$k$-&#20016;&#23500;&#24615;&#30340;&#27010;&#24565;&#65292;&#38543;&#21518;Barker&#31561;&#20154;&#24341;&#20837;&#20102;$k$-&#26222;&#36941;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;2022&#24180;&#65292;Fleischmann&#31561;&#20154;&#36890;&#36807;&#20132;&#38598;&#21270;&#21333;&#35789;&#30340;&#25329;&#24418;&#20998;&#35299;&#21644;&#20854;&#36870;&#24207;&#26469;&#25512;&#24191;&#20102;&#25329;&#24418;&#20998;&#35299;&#12290;&#23613;&#31649;&#20316;&#32773;&#20165;&#20165;&#20351;&#29992;&#36825;&#31181;&#20998;&#35299;&#26041;&#27861;&#26469;&#30740;&#31350;&#26368;&#30701;&#30340;&#32570;&#22833;&#31163;&#25955;&#22240;&#23376;&#65292;&#20294;&#22312;&#26412;&#30740;&#31350;&#20013;&#25105;&#20204;&#23558;&#23545;&#36825;&#31181;&#26032;&#30340;$\alpha$-$\beta$-&#20998;&#35299;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;$k$-&#26222;&#36941;&#24615;&#21333;&#35789;&#30340;$\alpha$-$\beta$-&#20998;&#35299;&#20013;&#23558;&#33879;&#21517;&#30340;Simon&#21516;&#20313;&#29305;&#24449;&#21270;&#20026;$1$-&#26222;&#36941;&#24615;&#21333;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#20108;&#20803;&#21333;&#35789;&#12290;&#22312;&#36825;&#31181;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31867;&#21035;&#30340;&#23436;&#20840;&#21051;&#30011;&#24182;&#35745;&#31639;&#20102;&#21516;&#20313;&#30340;&#25351;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#19977;&#20803;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In 1991 H\'ebrard introduced a factorization of words that turned out to be a powerful tool for the investigation of a word's scattered factors (also known as (scattered) subwords or subsequences). Based on this, first Karandikar and Schnoebelen introduced the notion of $k$-richness and later on Barker et al. the notion of $k$-universality. In 2022 Fleischmann et al. presented a generalization of the arch factorization by intersecting the arch factorization of a word and its reverse. While the authors merely used this factorization for the investigation of shortest absent scattered factors, in this work we investigate this new $\alpha$-$\beta$-factorization as such. We characterize the famous Simon congruence of $k$-universal words in terms of $1$-universal words. Moreover, we apply these results to binary words. In this special case, we obtain a full characterization of the classes and calculate the index of the congruence. Lastly, we start investigating the ternary case, present a fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.14115</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#35299;&#37322;&#65306;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35299;&#37322;&#25104;&#20026;&#20102;&#36890;&#36807;&#36873;&#25321;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#38598;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#20013;&#20027;&#35201;&#21464;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#33258;&#25105;&#35299;&#37322;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#29255;&#27573;&#39640;&#24230;&#20114;&#30456;&#20851;&#32852;&#26102;&#26080;&#27861;&#35782;&#21035;&#30495;&#27491;&#30340;&#35299;&#37322;&#65292;&#22240;&#27492;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#31867;&#20284;&#30340;&#36129;&#29486;&#65292;&#25152;&#35859;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#26032;&#39062;&#22320;&#23558;&#20004;&#20010;&#22240;&#26524;&#26399;&#26395;&#20540;&#65288;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65289;&#24341;&#20837;&#20102;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#35299;&#37322;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#30340;&#22240;&#26524;&#27010;&#29575;&#65292;&#36890;&#36807;&#20854;&#29702;&#35770;&#37492;&#23450;&#65292;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35780;&#35770;&#21644;&#21307;&#30103;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CamChoice&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#65292;&#20026;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#25552;&#20379;&#20102;&#33258;&#21160;&#35780;&#20272;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.13047</link><description>&lt;p&gt;
CamChoice&#65306;&#19968;&#20221;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#20505;&#36873;&#31572;&#26696;&#20998;&#24067;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions. (arXiv:2306.13047v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CamChoice&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#65292;&#20026;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#25552;&#20379;&#20102;&#33258;&#21160;&#35780;&#20272;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#26159;&#29992;&#20110;&#34913;&#37327;&#20505;&#36873;&#20154;&#22312;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#33021;&#21147;&#30340;&#26222;&#36941;&#35780;&#20272;&#24418;&#24335;&#12290;&#25552;&#20986;&#30340;&#38382;&#39064;&#30340;&#36136;&#37327;&#23545;&#20110;&#27979;&#35797;&#35774;&#35745;&#20154;&#21592;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#27492;&#26032;&#25552;&#20986;&#30340;&#38382;&#39064;&#22312;&#37096;&#32626;&#21040;&#23454;&#38469;&#32771;&#35797;&#20043;&#21069;&#38656;&#35201;&#32463;&#36807;&#20960;&#20010;&#39044;&#27979;&#35797;&#35780;&#20272;&#38454;&#27573;&#12290;&#30446;&#21069;&#65292;&#36825;&#20010;&#36807;&#31243;&#26159;&#30456;&#24403;&#25163;&#21160;&#21270;&#30340;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38382;&#39064;&#24320;&#21457;&#21608;&#26399;&#30340;&#26102;&#38388;&#28382;&#21518;&#12290;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#23558;&#22823;&#22823;&#25552;&#39640;&#25928;&#29575;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#19981;&#21253;&#21547;&#36275;&#22815;&#30340;&#39044;&#27979;&#35797;&#20998;&#26512;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CamChoice&#65306;&#19968;&#20221;&#21253;&#21547;&#19981;&#21516;&#30446;&#26631;&#32423;&#21035;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#30340;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;RACE++&#19978;&#35757;&#32451;&#30340;&#33258;&#21160;&#31995;&#32479;&#21487;&#20197;&#23454;&#29616;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks. Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams. This process is currently quite manual, which can lead to time lags in the question development cycle. Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information. In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions. We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18365</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#21040;&#24213;&#26377;&#24590;&#26679;&#30340;&#24212;&#29992;&#65311;&#20843;&#20010;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#36719;&#20214;&#24037;&#31243;&#31561;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;LLM &#26159;&#21542;&#26377;&#33021;&#21147;&#25512;&#21160;&#21270;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#21547; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21517;&#31216;&#39044;&#27979;&#12289;&#23646;&#24615;&#39044;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#12289;&#21453;&#24212;&#39044;&#27979;&#12289;&#21453;&#21512;&#25104;&#65288;&#20174;&#20135;&#29289;&#39044;&#27979;&#21453;&#24212;&#29289;&#65289;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#23376;&#35774;&#35745;&#12289;&#20998;&#23376;&#23383;&#24149;&#21644;&#35797;&#21058;&#36873;&#25321;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#35748;&#21487;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; BBBP&#12289;Tox21&#12289;PubChem&#12289;USPTO &#21644; ChEBI&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#31034;&#20363;&#20013;&#65292;&#23545;&#19977;&#31181; GPT &#27169;&#22411;&#65288;GPT-4&#12289;GPT-3.5 &#21644; DaVinci-003&#65289;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been rapidly applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper,we establish a comprehensive benchmark containing 8 practical chemistry tasks, including 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6)text-based molecule design, 7) molecule captioning, and 8) reagent selection. Our analysis draws on widely recognized datasets including BBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Three GPT models (GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17256</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26159;&#25042;&#24816;&#30340;&#23398;&#20064;&#32773;&#65306;&#20998;&#26512;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20854;&#20013;LLM&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;-&#26631;&#31614;&#23545;&#65288;&#25552;&#31034;&#65289;&#30340;&#26465;&#20214;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#25105;&#20204;&#23545;&#24433;&#21709;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31283;&#20581;&#24615;&#30340;&#22240;&#32032;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;LLM&#23545;&#25552;&#31034;&#20869;&#25463;&#24452;&#25110;&#20551;&#30456;&#20851;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#24357;&#34917;&#36825;&#19968;&#30693;&#35782;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#31867;&#21644;&#25277;&#21462;&#20219;&#21153;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLM&#26159;&#8220;&#25042;&#24816;&#23398;&#20064;&#32773;&#8221;&#30340;&#20107;&#23454;&#65292;&#23427;&#24448;&#24448;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#26469;&#33719;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#21363;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are "lazy learners" that tend to exploit shortcuts in prompts for downstream tasks. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22635;&#34917;&#26041;&#27861;&#20026;&#25152;&#26377;&#27880;&#37322;&#32773;&#29983;&#25104;&#25152;&#26377;&#31034;&#20363;&#30340;&#24847;&#35265;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#19981;&#25490;&#26021;&#20219;&#20309;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#21457;&#29616;&#22635;&#34917;&#26041;&#27861;&#30340;&#36873;&#25321;&#23545;&#36719;&#26631;&#31614;&#21464;&#21270;&#21644;&#20998;&#24067;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.15070</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;&#30340;&#27880;&#37322;&#22635;&#34917;&#65306;&#20851;&#20110;&#20998;&#24067;&#21160;&#24577;&#21644;&#27169;&#22411;&#39044;&#27979;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Annotation Imputation to Individualize Predictions: Initial Studies on Distribution Dynamics and Model Predictions. (arXiv:2305.15070v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22635;&#34917;&#26041;&#27861;&#20026;&#25152;&#26377;&#27880;&#37322;&#32773;&#29983;&#25104;&#25152;&#26377;&#31034;&#20363;&#30340;&#24847;&#35265;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#19981;&#25490;&#26021;&#20219;&#20309;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#21457;&#29616;&#22635;&#34917;&#26041;&#27861;&#30340;&#36873;&#25321;&#23545;&#36719;&#26631;&#31614;&#21464;&#21270;&#21644;&#20998;&#24067;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20247;&#21253;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#38750;&#24120;&#36153;&#26102;&#36153;&#38065;&#12290;&#30001;&#20110;&#36825;&#20123;&#25104;&#26412;&#65292;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#32773;&#36890;&#24120;&#35753;&#27599;&#20010;&#27880;&#37322;&#32773;&#21482;&#23545;&#19968;&#23567;&#37096;&#20998;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#36825;&#23548;&#33268;&#20102;&#31232;&#30095;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#30340;&#31034;&#20363;&#21482;&#34987;&#23569;&#25968;&#27880;&#37322;&#32773;&#26631;&#35760;&#12290;&#36825;&#20010;&#36807;&#31243;&#30340;&#32570;&#28857;&#22312;&#20110;&#65292;&#22914;&#26524;&#19968;&#20010;&#27880;&#37322;&#32773;&#27809;&#26377;&#26631;&#27880;&#19968;&#20010;&#29305;&#23450;&#30340;&#31034;&#20363;&#65292;&#20182;&#20204;&#23545;&#23427;&#30340;&#30475;&#27861;&#23601;&#20250;&#34987;&#24573;&#35270;&#12290;&#36825;&#22312;&#20027;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#20013;&#23588;&#20026;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#27809;&#26377;&#19968;&#20010;&#27491;&#30830;&#30340;&#26631;&#31614;&#65306;&#20154;&#20204;&#21487;&#33021;&#20250;&#26377;&#19981;&#21516;&#30340;&#26377;&#25928;&#35266;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22635;&#34917;&#26041;&#27861;&#20026;&#25152;&#26377;&#31034;&#20363;&#29983;&#25104;&#25152;&#26377;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#19981;&#25490;&#26021;&#20219;&#20309;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22635;&#34917;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#35757;&#32451;&#21644;&#25552;&#31034;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21709;&#24212;&#21644;&#20010;&#21035;&#27880;&#37322;&#30340;&#20998;&#24067;&#12290;&#22312;&#25105;&#20204;&#23545;&#32467;&#26524;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22635;&#34917;&#26041;&#27861;&#30340;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#36719;&#26631;&#31614;&#30340;&#21464;&#21270;&#21644;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating data via crowdsourcing is time-consuming and expensive. Due to these costs, dataset creators often have each annotator label only a small subset of the data. This leads to sparse datasets with examples that are marked by few annotators. The downside of this process is that if an annotator doesn't get to label a particular example, their perspective on it is missed. This is especially concerning for subjective NLP datasets where there is no single correct label: people may have different valid opinions. Thus, we propose using imputation methods to generate the opinions of all annotators for all examples, creating a dataset that does not leave out any annotator's view. We then train and prompt models, using data from the imputed dataset, to make predictions about the distribution of responses and individual annotations.  In our analysis of the results, we found that the choice of imputation method significantly impacts soft label changes and distribution. While the imputation 
&lt;/p&gt;</description></item><item><title>DUDE&#25512;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#22522;&#20934;&#27979;&#35797;&#24182;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;</title><link>http://arxiv.org/abs/2305.08455</link><description>&lt;p&gt;
&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#65288;DUDE&#65289;
&lt;/p&gt;
&lt;p&gt;
Document Understanding Dataset and Evaluation (DUDE). (arXiv:2305.08455v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08455
&lt;/p&gt;
&lt;p&gt;
DUDE&#25512;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#22522;&#20934;&#27979;&#35797;&#24182;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21628;&#21505;&#25991;&#26723;AI&#31038;&#21306;&#37325;&#26032;&#35780;&#20272;&#24403;&#21069;&#30340;&#26041;&#27861;&#35770;&#65292;&#25317;&#25265;&#21019;&#24314;&#26356;&#23454;&#38469;&#21462;&#21521;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#65288;DUDE&#65289;&#26088;&#22312;&#32416;&#27491;&#22312;&#29702;&#35299;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#65288;VRD&#65289;&#26041;&#38754;&#30340;&#30740;&#31350;&#36827;&#23637;&#20572;&#28382;&#19981;&#21069;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#22810;&#34892;&#19994;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#39029;VRD&#30456;&#20851;&#30340;&#38382;&#39064;&#31867;&#22411;&#12289;&#31572;&#26696;&#21644;&#25991;&#26723;&#24067;&#23616;&#30340;&#21019;&#26032;&#65292;&#20855;&#26377;&#21508;&#31181;&#26469;&#28304;&#21644;&#26085;&#26399;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#35780;&#20272;&#35774;&#32622;&#26469;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#36825;&#20123;&#35774;&#32622;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#38656;&#35201;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#24378;&#22823;&#30340;&#27867;&#21270;&#21644;&#36866;&#24212;&#12290;DUDE&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#26356;&#23454;&#38469;&#12289;&#26356;&#38271;&#26399;&#30340;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#65292;&#24182;&#24076;&#26395;&#23427;&#20250;&#24341;&#39046;&#26410;&#26469;&#30340;&#25193;&#23637;&#21644;&#36129;&#29486;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35828;&#26126;&#20102;&#20197;&#19979;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.04940</link><description>&lt;p&gt;
&#26089;&#36215;&#30340;&#40479;&#20799;&#25417;&#21040;&#34411;&#65306;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#36827;&#34892;&#26356;&#26377;&#25928;&#30340;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#22914;&#28431;&#27934;&#26816;&#27979;&#21644;&#31867;&#22411;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#20013;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#30340;&#26368;&#20339;&#21033;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#26500;&#24314;&#20195;&#30721;&#30340;&#22797;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;12&#31181;&#21019;&#24314;&#22797;&#21512;&#34920;&#31034;&#30340;&#31574;&#30053;&#19982;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#20010;&#32534;&#30721;&#22120;&#23618;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#22312;CodeBERT&#27169;&#22411;&#19978;&#23454;&#35777;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#22312;4&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20960;&#20010;&#26089;&#26399;&#23618;&#30340;&#32452;&#21512;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#20135;&#29983;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19968;&#20123;&#32452;&#21512;&#21017;&#25913;&#36827;&#20102;&#22810;&#31867;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#24179;&#22343;&#26816;&#27979;&#22686;&#24378;2&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#22312;&#27880;&#37322;&#20013;&#26469;&#20248;&#21270;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#27604;&#36739;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04087</link><description>&lt;p&gt;
&#33258;&#25105;&#32534;&#36753;&#65306;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#22312;&#27880;&#37322;&#20013;&#26469;&#20248;&#21270;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#27604;&#36739;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#20013;&#29983;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#65292;&#20294;&#30001;&#20110;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#21463;&#20154;&#31867;&#32534;&#31243;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25191;&#34892;&#32467;&#26524;&#26469;&#25552;&#39640;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#38382;&#39064;&#20013;&#25552;&#20379;&#30340;&#31034;&#20363;&#27979;&#35797;&#29992;&#20363;&#19978;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#34917;&#20805;&#24615;&#27880;&#37322;&#20013;&#12290;&#21033;&#29992;&#36825;&#20010;&#27880;&#37322;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#30340;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#29992;&#20110;&#32416;&#27491;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#12290;&#19982;&#30452;&#25509;&#20174;LLMs&#29983;&#25104;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;APPS-dev&#19978;&#23558;pass@1&#30340;&#24179;&#22343;&#20540;&#25552;&#39640;89&#65285;&#65292;&#22312;APPS-test&#19978;&#25552;&#39640;31&#65285;&#65292;&#22312;HumanEval&#19978;&#25552;&#39640;48&#65285;&#65292;&#36229;&#36807;&#20102;&#20061;&#20010;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#21442;&#25968;&#22823;&#23567;&#33539;&#22260;&#20026;110M-t&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;ChatGPT&#30340;&#39044;&#27979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#21464;&#21270;&#65292;&#34920;&#26126;&#22797;&#26434;&#27169;&#22411;&#21487;&#39044;&#27979;&#33021;&#21147;&#30340;&#23835;&#36215;&#12290;&#36825;&#34920;&#26126;&#22312;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#20013;&#24341;&#20837;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#23450;&#37327;&#20132;&#26131;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07619</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#65311;&#22238;&#25253;&#21487;&#39044;&#27979;&#24615;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. (arXiv:2304.07619v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;ChatGPT&#30340;&#39044;&#27979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#21464;&#21270;&#65292;&#34920;&#26126;&#22797;&#26434;&#27169;&#22411;&#21487;&#39044;&#27979;&#33021;&#21147;&#30340;&#23835;&#36215;&#12290;&#36825;&#34920;&#26126;&#22312;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#20013;&#24341;&#20837;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#23450;&#37327;&#20132;&#26131;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#30340;&#28508;&#21147;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;ChatGPT&#20197;&#21450;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#21028;&#26029;&#26032;&#38395;&#26631;&#39064;&#23545;&#20844;&#21496;&#32929;&#31080;&#20215;&#26684;&#26159;&#22909;&#28040;&#24687;&#12289;&#22351;&#28040;&#24687;&#25110;&#26080;&#20851;&#28040;&#24687;&#12290;&#36890;&#36807;&#35745;&#31639;&#25968;&#23383;&#20998;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;"ChatGPT&#20998;&#25968;"&#21644;&#38543;&#21518;&#30340;&#26085;&#24120;&#32929;&#31080;&#24066;&#22330;&#22238;&#25253;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#24615;&#12290;&#32780;&#19988;&#65292;ChatGPT&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-1&#12289;GPT-2&#21644;BERT&#31561;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#22238;&#25253;&#65292;&#36825;&#34920;&#26126;&#22238;&#25253;&#21487;&#39044;&#27979;&#24615;&#26159;&#22797;&#26434;&#27169;&#22411;&#30340;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#32435;&#20837;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#24182;&#25552;&#39640;&#23450;&#37327;&#20132;&#26131;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the potential of ChatGPT, and other large language models, in predicting stock market returns using sentiment analysis of news headlines. We use ChatGPT to indicate whether a given headline is good, bad, or irrelevant news for firms' stock prices. We then compute a numerical score and document a positive correlation between these ``ChatGPT scores'' and subsequent daily stock market returns. Further, ChatGPT outperforms traditional sentiment analysis methods. We find that more basic models such as GPT-1, GPT-2, and BERT cannot accurately forecast returns, indicating return predictability is an emerging capacity of complex models. Our results suggest that incorporating advanced language models into the investment decision-making process can yield more accurate predictions and enhance the performance of quantitative trading strategies.
&lt;/p&gt;</description></item><item><title>ARNOLD&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#65292;&#22312;&#35821;&#35328;&#24341;&#23548;&#19979;&#24110;&#21161;&#26426;&#22120;&#20154;&#23398;&#20064;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.04321</link><description>&lt;p&gt;
ARNOLD&#65306;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#23454;&#29616;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#35821;&#35328;&#24341;&#23548;&#20219;&#21153;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes. (arXiv:2304.04321v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04321
&lt;/p&gt;
&lt;p&gt;
ARNOLD&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#65292;&#22312;&#35821;&#35328;&#24341;&#23548;&#19979;&#24110;&#21161;&#26426;&#22120;&#20154;&#23398;&#20064;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#29702;&#35299;&#29289;&#20307;&#30340;&#36830;&#32493;&#29366;&#24577;&#23545;&#20110;&#20219;&#21153;&#23398;&#20064;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20551;&#23450;&#30446;&#26631;&#29366;&#24577;&#26159;&#31163;&#25955;&#30340;(&#20363;&#22914;&#20108;&#36827;&#21046;&#29366;&#24577;)&#65292;&#36825;&#32473;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#21644;&#23558;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29366;&#24577;&#31163;&#25955;&#21270;&#38480;&#21046;&#20102;&#26426;&#22120;&#20154;&#26681;&#25454;&#21160;&#20316;&#21644;&#29366;&#24577;&#30340;&#24341;&#23548;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARNOLD&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;ARNOLD&#30001;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#32452;&#25104;&#65292;&#28041;&#21450;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#20419;&#36827;&#35821;&#35328;&#24341;&#23548;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#26495;&#29983;&#25104;&#30340;&#35821;&#35328;&#25551;&#36848;&#30340;&#19987;&#23478;&#28436;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#35821;&#35328;&#26465;&#20214;&#31574;&#30053;&#23398;&#20064;&#27169;&#22411;&#26469;&#35780;&#20272;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ARNOLD&#20026;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#30340;&#35821;&#35328;&#24341;&#23548;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#24182;&#21487;&#29992;&#20110;&#35780;&#20272;&#20174;&#27169;&#25311;&#22330;&#26223;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete(e.g., binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present ARNOLD, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results ind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#21322;&#30417;&#30563;&#37322;&#20041;&#29983;&#25104;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#22833;&#30446;&#26631;&#23545;&#24314;&#27169;&#20026;&#28508;&#22312;&#37322;&#20041;&#24207;&#21015;&#65292;&#24182;&#32467;&#21512;&#21452;&#21521;&#23398;&#20064;&#21644;&#25913;&#36827;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.02275</link><description>&lt;p&gt;
&#35821;&#35328;&#20316;&#20026;&#28508;&#22312;&#24207;&#21015;&#65306;&#29992;&#20110;&#21322;&#30417;&#30563;&#37322;&#20041;&#29983;&#25104;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language as a Latent Sequence: deep latent variable models for semi-supervised paraphrase generation. (arXiv:2301.02275v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#21322;&#30417;&#30563;&#37322;&#20041;&#29983;&#25104;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#22833;&#30446;&#26631;&#23545;&#24314;&#27169;&#20026;&#28508;&#22312;&#37322;&#20041;&#24207;&#21015;&#65292;&#24182;&#32467;&#21512;&#21452;&#21521;&#23398;&#20064;&#21644;&#25913;&#36827;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#21322;&#30417;&#30563;&#37322;&#20041;&#29983;&#25104;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#22833;&#30446;&#26631;&#23545;&#34987;&#24314;&#27169;&#20026;&#28508;&#22312;&#37322;&#20041;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21464;&#20998;&#24207;&#21015;&#33258;&#32534;&#30721;&#37325;&#26500;&#65288;VSAR&#65289;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#22312;&#32473;&#23450;&#35266;&#23519;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#28508;&#22312;&#24207;&#21015;&#25512;&#26029;&#12290;&#20026;&#20102;&#21033;&#29992;&#25991;&#26412;&#23545;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#21521;&#23398;&#20064;&#65288;DDL&#65289;&#30340;&#26032;&#22411;&#30417;&#30563;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26088;&#22312;&#19982;&#25105;&#20204;&#25552;&#20986;&#30340;VSAR&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;&#23558;VSAR&#19982;DDL&#65288;DDL+VSAR&#65289;&#32467;&#21512;&#36215;&#26469;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#32452;&#21512;&#27169;&#22411;&#23384;&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#22686;&#24378;&#23398;&#20064;&#65288;KRL&#65289;&#30340;&#26032;&#22411;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#32452;&#21512;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#31454;&#20105;&#21147;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores deep latent variable models for semi-supervised paraphrase generation, where the missing target pair for unlabelled data is modelled as a latent paraphrase sequence. We present a novel unsupervised model named variational sequence auto-encoding reconstruction (VSAR), which performs latent sequence inference given an observed text. To leverage information from text pairs, we additionally introduce a novel supervised model we call dual directional learning (DDL), which is designed to integrate with our proposed VSAR model. Combining VSAR with DDL (DDL+VSAR) enables us to conduct semi-supervised learning. Still, the combined model suffers from a cold-start problem. To further combat this issue, we propose an improved weight initialisation solution, leading to a novel two-stage training scheme we call knowledge-reinforced-learning (KRL). Our empirical evaluations suggest that the combined model yields competitive performance against the state-of-the-art supervised basel
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35780;&#20272;&#20154;&#26426;&#20132;&#20114;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;HALIE&#65292;&#35813;&#26694;&#26550;&#25429;&#25417;&#20102;&#20132;&#20114;&#36807;&#31243;&#12289;&#20027;&#35266;&#20307;&#39564;&#21644;&#20559;&#22909;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#20116;&#20010;&#20219;&#21153;&#26469;&#28085;&#30422;&#19981;&#21516;&#24418;&#24335;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2212.09746</link><description>&lt;p&gt;
&#35780;&#20272;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Evaluating Human-Language Model Interaction. (arXiv:2212.09746v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09746
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#20154;&#26426;&#20132;&#20114;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;HALIE&#65292;&#35813;&#26694;&#26550;&#25429;&#25417;&#20102;&#20132;&#20114;&#36807;&#31243;&#12289;&#20027;&#35266;&#20307;&#39564;&#21644;&#20559;&#22909;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#20116;&#20010;&#20219;&#21153;&#26469;&#28085;&#30422;&#19981;&#21516;&#24418;&#24335;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#20889;&#20316;&#36741;&#21161;&#21644;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#65292;&#28041;&#21450;&#21040;&#20154;&#26426;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#37117;&#26159;&#38750;&#20132;&#20114;&#24335;&#30340;&#65292;&#27169;&#22411;&#22312;&#27809;&#26377;&#20154;&#31867;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#36755;&#20986;&#12290;&#20026;&#20102;&#35780;&#20272;&#20154;&#26426;&#20132;&#20114;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20154;&#26426;&#35821;&#35328;&#20132;&#20114;&#35780;&#20272;&#65288;HALIE&#65289;&#65292;&#35813;&#26694;&#26550;&#23450;&#20041;&#20102;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;&#35774;&#35745;&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#32771;&#34385;&#30340;&#32500;&#24230;&#12290;&#19982;&#26631;&#20934;&#30340;&#38750;&#20132;&#20114;&#24335;&#35780;&#20272;&#30456;&#27604;&#65292;HALIE&#25429;&#25417;&#21040;&#20102;&#65288;i&#65289;&#20132;&#20114;&#36807;&#31243;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#32456;&#36755;&#20986;&#65307;&#65288;ii&#65289;&#31532;&#19968;&#20154;&#31216;&#20027;&#35266;&#20307;&#39564;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31532;&#19977;&#26041;&#35780;&#20272;&#65307;&#65288;iii&#65289;&#38500;&#20102;&#36136;&#37327;&#20043;&#22806;&#30340;&#20559;&#22909;&#27010;&#24565;&#65288;&#20363;&#22914;&#20139;&#21463;&#21644;&#25152;&#26377;&#26435;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20116;&#20010;&#20219;&#21153;&#65292;&#28085;&#30422;&#19981;&#21516;&#24418;&#24335;&#30340;&#20132;&#20114;&#65306;&#31038;&#20132;&#23545;&#35805;&#12289;&#38382;&#31572;&#12289;&#22635;&#23383;&#28216;&#25103;&#12289;&#25688;&#35201;&#21644;&#38544;&#21947;&#29983;&#25104;&#12290;&#20351;&#29992;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;LM&#65288;OpenAI&#30340;GPT-3&#30340;&#19977;&#20010;&#21464;&#20307;&#21644;AI21 Labs&#30340;Jurass&#65289;
&lt;/p&gt;
&lt;p&gt;
Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurass
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2211.09172</link><description>&lt;p&gt;
&#25991;&#23383;&#23545;&#35805;&#20013;&#30340;&#28145;&#24230;&#24773;&#24863;&#35782;&#21035;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#26032;&#30340;&#24212;&#29992;&#21644;&#23454;&#26045;&#22330;&#26223;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#21033;&#29992;&#23545;&#35805;&#35821;&#22659;&#12289;&#35828;&#35805;&#20154;&#21644;&#24773;&#24863;&#21160;&#24577;&#24314;&#27169;&#65292;&#35299;&#37322;&#24120;&#35782;&#34920;&#36798;&#12289;&#38750;&#27491;&#24335;&#35821;&#35328;&#21644;&#35773;&#21050;&#65292;&#24212;&#23545;&#23454;&#26102;&#24773;&#24863;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#35782;&#21035;&#24773;&#24863;&#21407;&#22240;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#31181;&#20998;&#31867;&#27861;&#65292;&#22810;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#20197;&#21450;&#35299;&#37322;&#24615;&#12290;&#26412;&#35843;&#30740;&#39318;&#20808;&#20171;&#32461;&#20102;&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#19982;&#27492;&#20219;&#21153;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#28982;&#21518;&#65292;&#23427;&#20171;&#32461;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#22810;&#31181;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#24773;&#24863;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#25551;&#36848;&#20102;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#65292;&#24182;&#35299;&#37322;&#20102;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#26368;&#21518;&#65292;&#23427;&#25552;&#20379;&#20102;&#23545;&#20110;&#26356;&#22909;&#30340;&#26694;&#26550;&#30340;&#24314;&#35758;&#24615;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#22788;&#29702;&#20027;&#35266;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Emotion Recognition in Conversations (ERC) has seen a tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker and emotion dynamics modelling, to interpreting common sense expressions, informal language and sarcasm, addressing challenges of real time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC to interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities pertaining to this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions of the most prominent works in ERC with explanations of the Deep Learning architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in ann
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#29255;&#27573;&#26816;&#27979;&#26694;&#26550;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36890;&#36807;&#21457;&#29616;&#12289;&#35299;&#37322;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#26410;&#26469;&#27169;&#22411;&#35774;&#35745;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.04476</link><description>&lt;p&gt;
&#21457;&#29616;&#12289;&#35299;&#37322;&#12289;&#25913;&#36827;&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#21160;&#29255;&#27573;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Discover, Explanation, Improvement: An Automatic Slice Detection Framework for Natural Language Processing. (arXiv:2211.04476v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#29255;&#27573;&#26816;&#27979;&#26694;&#26550;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36890;&#36807;&#21457;&#29616;&#12289;&#35299;&#37322;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#26410;&#26469;&#27169;&#22411;&#35774;&#35745;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#31995;&#32479;&#24615;&#30340;&#38169;&#35823;&#12290;&#19982;&#25163;&#21160;&#38169;&#35823;&#20998;&#26512;&#19981;&#21516;&#65292;&#23545;&#20110;&#33258;&#21160;&#35782;&#21035;&#34920;&#29616;&#19981;&#20339;&#30340;&#25968;&#25454;&#32452;&#30340;&#29255;&#27573;&#26816;&#27979;&#27169;&#22411;&#65288;SDM&#65289;&#30340;&#30740;&#31350;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#21487;&#20197;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#24182;&#20026;&#26410;&#26469;&#27169;&#22411;&#35757;&#32451;&#21644;&#35774;&#35745;&#25552;&#20379;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;NLP&#20219;&#21153;&#19978;&#65292;&#23545;SDM&#30340;&#30740;&#31350;&#21644;&#20854;&#26377;&#25928;&#24615;&#30340;&#23450;&#37327;&#35780;&#20272;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;&#8220;Discover, Explain, Improve(DEIM)&#8221;&#30340;NLP&#20998;&#31867;&#20219;&#21153;&#22522;&#20934;&#21644;&#19968;&#20010;&#26032;&#30340;SDM&#27169;&#22411;Edisa&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;Edisa&#21457;&#29616;&#20102;&#19968;&#33268;&#19988;&#34920;&#29616;&#19981;&#20339;&#30340;&#25968;&#25454;&#32452;&#65307;DEIM&#23558;&#23427;&#20204;&#32479;&#19968;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#23450;&#37327;&#25351;&#26631;&#12290;&#22312;DEIM&#30340;&#35780;&#20272;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;Edisa&#33021;&#22815;&#20934;&#30830;&#36873;&#25321;&#26131;&#20986;&#38169;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained natural language processing (NLP) models have achieved high overall performance, but they still make systematic errors. Instead of manual error analysis, research on slice detection models (SDM), which automatically identify underperforming groups of datapoints, has caught escalated attention in Computer Vision for both understanding model behaviors and providing insights for future model training and designing. However, little research on SDM and quantitative evaluation of their effectiveness have been conducted on NLP tasks. Our paper fills the gap by proposing a benchmark named "Discover, Explain, Improve (DEIM)" for classification NLP tasks along with a new SDM Edisa. Edisa discovers coherent and underperforming groups of datapoints; DEIM then unites them under human-understandable concepts and provides comprehensive evaluation tasks and corresponding quantitative metrics. The evaluation in DEIM shows that Edisa can accurately select error-prone datapoints with informati
&lt;/p&gt;</description></item><item><title>&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36880;&#23383;&#35760;&#24518;&#26080;&#27861;&#30495;&#27491;&#20445;&#25252;&#38544;&#31169;&#65292;&#26412;&#25991;&#35774;&#35745;&#30340;&#24067;&#38534;&#36807;&#28388;&#22120;&#34429;&#28982;&#38450;&#27490;&#20102;&#25152;&#26377;&#36880;&#23383;&#35760;&#24518;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#65292;&#23481;&#26131;&#34987;&#21512;&#29702;&#20462;&#25913;&#30340;&#8220;&#26679;&#24335;&#36716;&#25442;&#8221;&#25552;&#31034;&#32469;&#36807;&#12290;</title><link>http://arxiv.org/abs/2210.17546</link><description>&lt;p&gt;
&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#23383;&#35760;&#24518;&#20250;&#20135;&#29983;&#34394;&#20551;&#38544;&#31169;&#20445;&#25252;&#24863;
&lt;/p&gt;
&lt;p&gt;
Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17546
&lt;/p&gt;
&lt;p&gt;
&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36880;&#23383;&#35760;&#24518;&#26080;&#27861;&#30495;&#27491;&#20445;&#25252;&#38544;&#31169;&#65292;&#26412;&#25991;&#35774;&#35745;&#30340;&#24067;&#38534;&#36807;&#28388;&#22120;&#34429;&#28982;&#38450;&#27490;&#20102;&#25152;&#26377;&#36880;&#23383;&#35760;&#24518;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#65292;&#23481;&#26131;&#34987;&#21512;&#29702;&#20462;&#25913;&#30340;&#8220;&#26679;&#24335;&#36716;&#25442;&#8221;&#25552;&#31034;&#32469;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#25968;&#25454;&#35760;&#24518;&#30340;&#29616;&#35937;&#65292;&#26412;&#30740;&#31350;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#19982;&#38544;&#31169;&#25110;&#29256;&#26435;&#30456;&#20851;&#30340;&#39118;&#38505;&#65292;&#24182;&#26377;&#21161;&#20110;&#35780;&#20272;&#23545;&#31574;&#12290;&#28982;&#32780;&#36880;&#23383;&#35760;&#24518;&#23450;&#20041;&#36807;&#20110;&#20005;&#26684;&#65292;&#26410;&#33021;&#25429;&#25417;&#26356;&#20026;&#24494;&#22937;&#30340;&#35760;&#24518;&#24418;&#24335;&#12290;&#26412;&#25991;&#22522;&#20110;&#24067;&#38534;&#36807;&#28388;&#22120;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#35813;&#8220;&#23436;&#32654;&#8221;&#36807;&#28388;&#22120;&#24182;&#19981;&#33021;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on "verbatim memorization", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this "perfect" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified "style-transfer" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CA
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#20851;&#20110;&#26234;&#33021;&#12289;&#20154;&#31867;&#35821;&#35328;&#21644;&#20154;&#31867;&#25968;&#23398;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#20154;&#31867;&#35821;&#35328;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#33021;&#21542;&#23545;&#25105;&#20204;&#26080;&#27861;&#24819;&#35937;&#30340;&#20107;&#29289;&#26377;&#20219;&#20309;&#20102;&#35299;&#12290;</title><link>http://arxiv.org/abs/2208.03886</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#20102;&#35299;&#29978;&#33267;&#26080;&#27861;&#24819;&#35937;&#30340;&#20107;&#29289;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
What can we know about that which we cannot even imagine?. (arXiv:2208.03886v3 [physics.hist-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#20851;&#20110;&#26234;&#33021;&#12289;&#20154;&#31867;&#35821;&#35328;&#21644;&#20154;&#31867;&#25968;&#23398;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#20154;&#31867;&#35821;&#35328;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#33021;&#21542;&#23545;&#25105;&#20204;&#26080;&#27861;&#24819;&#35937;&#30340;&#20107;&#29289;&#26377;&#20219;&#20309;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#23558;&#32771;&#34385;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#21040;&#26234;&#33021;&#30340;&#29983;&#29289;&#23398;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#35748;&#30693;&#20041;&#32930;&#12290;&#36825;&#23558;&#24341;&#20986;&#20851;&#20110;&#20154;&#31867;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#20063;&#35768;&#26159;&#20154;&#31867;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#30340;&#26368;&#37325;&#35201;&#30340;&#35748;&#30693;&#20041;&#32930;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#23545;&#20154;&#31867;&#35821;&#35328;&#25152;&#21253;&#21547;&#30340;&#35748;&#30693;&#33021;&#21147;&#36827;&#34892;&#36190;&#32654;&#65292;&#20294;&#25105;&#23558;&#24378;&#35843;&#20154;&#31867;&#35821;&#35328;&#22810;&#20040;&#26377;&#38480;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#35748;&#30693;&#33021;&#21147;&#20063;&#26159;&#26377;&#38480;&#30340;&#65292;&#23613;&#31649;&#35821;&#35328;&#23545;&#20854;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#36825;&#23558;&#24341;&#20986;&#20851;&#20110;&#20154;&#31867;&#25968;&#23398;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26368;&#32456;&#26159;&#20197;&#20154;&#31867;&#35821;&#35328;&#30340;&#24418;&#24335;&#26469;&#34920;&#36848;&#30340;&#65292;&#25152;&#20197;&#20063;&#23384;&#22312;&#28145;&#23618;&#27425;&#30340;&#38480;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#23558;&#32467;&#21512;&#36825;&#20123;&#38382;&#39064;&#65292;&#23545;&#36825;&#31687;&#25991;&#31456;&#30340;&#26680;&#24515;&#38382;&#39064;&#25552;&#20986;&#19968;&#20010;&#37096;&#20998;&#24615;&#30340;&#12289;&#26377;&#28857;&#20391;&#38754;&#30340;&#31572;&#26696;&#65306;&#25105;&#20204;&#33021;&#22815;&#23545;&#25105;&#20204;&#29978;&#33267;&#26080;&#27861;&#26500;&#24819;&#30340;&#20107;&#29289;&#26377;&#20309;&#20102;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
In this essay I will consider a sequence of questions. The first questions concern the biological function of intelligence in general, and cognitive prostheses of human intelligence in particular. These will lead into questions concerning human language, perhaps the most important cognitive prosthesis humanity has ever developed. While it is traditional to rhapsodize about the cognitive power encapsulated in human language, I will emphasize how horribly limited human language is -- and therefore how limited our cognitive abilities are, despite their being augmented with language. This will lead to questions of whether human mathematics, being ultimately formulated in terms of human language, is also deeply limited. I will then combine these questions to pose a partial, sort-of, sideways answer to the guiding concern of this essay: what we can ever discern about that we cannot even conceive?
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#34920;&#29616;&#20316;&#20026;&#39044;&#27979;&#20799;&#31461;&#35789;&#35821;&#23398;&#20064;&#30340;&#38590;&#24230;&#30340;&#20195;&#29702;&#65292;&#25105;&#20204;&#21457;&#29616;&#20799;&#31461;&#33719;&#24471;&#19981;&#21516;&#31867;&#21035;&#30340;&#35789;&#35821;&#30340;&#24180;&#40836;&#19982;&#35270;&#35273;&#20998;&#31867;&#21644;&#23383;&#24149;&#31995;&#32479;&#30340;&#34920;&#29616;&#26377;&#20851;&#12290;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#35789;&#35821;&#19982;&#35270;&#35273;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2207.09847</link><description>&lt;p&gt;
&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#34920;&#29616;&#39044;&#27979;&#20799;&#31461;&#30340;&#35789;&#35821;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Predicting Word Learning in Children from the Performance of Computer Vision Systems. (arXiv:2207.09847v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#34920;&#29616;&#20316;&#20026;&#39044;&#27979;&#20799;&#31461;&#35789;&#35821;&#23398;&#20064;&#30340;&#38590;&#24230;&#30340;&#20195;&#29702;&#65292;&#25105;&#20204;&#21457;&#29616;&#20799;&#31461;&#33719;&#24471;&#19981;&#21516;&#31867;&#21035;&#30340;&#35789;&#35821;&#30340;&#24180;&#40836;&#19982;&#35270;&#35273;&#20998;&#31867;&#21644;&#23383;&#24149;&#31995;&#32479;&#30340;&#34920;&#29616;&#26377;&#20851;&#12290;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#35789;&#35821;&#19982;&#35270;&#35273;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20799;&#31461;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26469;&#35828;&#65292;&#23398;&#20064;&#19968;&#20010;&#35789;&#26368;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#23558;&#35813;&#35789;&#19982;&#25551;&#36848;&#30340;&#35270;&#35273;&#29616;&#35937;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#34920;&#29616;&#20316;&#20026;&#20174;&#35270;&#35273;&#32447;&#32034;&#23398;&#20064;&#19968;&#20010;&#35789;&#30340;&#38590;&#24230;&#30340;&#20195;&#29702;&#26469;&#25506;&#31350;&#35789;&#35821;&#23398;&#20064;&#30340;&#36825;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#20799;&#31461;&#33719;&#24471;&#19981;&#21516;&#31867;&#21035;&#30340;&#35789;&#35821;&#30340;&#24180;&#40836;&#19982;&#35270;&#35273;&#20998;&#31867;&#21644;&#23383;&#24149;&#31995;&#32479;&#30340;&#34920;&#29616;&#30456;&#20851;&#65292;&#36229;&#20986;&#20102;&#35789;&#35821;&#39057;&#29575;&#39044;&#26399;&#25928;&#24212;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#34920;&#29616;&#19982;&#35789;&#35821;&#30340;&#20855;&#20307;&#24615;&#30340;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#65292;&#32780;&#20855;&#20307;&#24615;&#21448;&#26159;&#20799;&#31461;&#35789;&#35821;&#23398;&#20064;&#30340;&#39044;&#27979;&#22240;&#32032;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#35789;&#35821;&#19982;&#35270;&#35273;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
For human children as well as machine learning systems, a key challenge in learning a word is linking the word to the visual phenomena it describes. We explore this aspect of word learning by using the performance of computer vision systems as a proxy for the difficulty of learning a word from visual cues. We show that the age at which children acquire different categories of words is correlated with the performance of visual classification and captioning systems, over and above the expected effects of word frequency. The performance of the computer vision systems is correlated with human judgments of the concreteness of words, which are in turn a predictor of children's word learning, suggesting that these models are capturing the relationship between words and visual phenomena.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KEPLMs&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#26368;&#21518;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2110.00269</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KEPLMs&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#26368;&#21518;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#23398;&#20064;&#20102;&#20449;&#24687;&#20016;&#23500;&#30340;&#35789;&#34920;&#31034;&#65292;&#22312;&#32454;&#35843;&#20043;&#21518;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#40065;&#26834;&#24615;&#24046;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#27880;&#20837;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#31216;&#20026;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(KEPLMs)&#12290;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#28145;&#20837;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;KEPLMs&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#31995;&#32479;&#22320;&#20998;&#31867;&#20102;&#29616;&#26377;&#30340;KEPLMs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models learn informative word representations on a large-scale text corpus through self-supervised learning, which has achieved promising performance in fields of natural language processing (NLP) after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. We refer to pre-trained language models with knowledge injection as knowledge-enhanced pre-trained language models (KEPLMs). These models demonstrate deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPLMs in NLP. We first discuss the advancements in pre-trained language models and knowledge representation learning. Then we systematically categorize existing KEPLMs from three different perspectives. Finally, we outline some potential directions of KEPLMs for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#25968;&#25454;&#24211;&#20013;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#21015;&#21517;&#20197;&#36741;&#21161;&#35843;&#20248;&#21644;&#20998;&#26512;&#24037;&#20316;&#12290;&#36890;&#36807;&#20998;&#26512;Kaggle&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#24615;&#20998;&#26512;&#22522;&#20934;&#65292;&#24182;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2107.04553</link><description>&lt;p&gt;
&#12298;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#36890;&#36807;&#21015;&#21517;&#39044;&#27979;&#25968;&#25454;&#30456;&#20851;&#24615;?&#12299;
&lt;/p&gt;
&lt;p&gt;
Can Deep Neural Networks Predict Data Correlations from Column Names?. (arXiv:2107.04553v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.04553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#25968;&#25454;&#24211;&#20013;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#21015;&#21517;&#20197;&#36741;&#21161;&#35843;&#20248;&#21644;&#20998;&#26512;&#24037;&#20316;&#12290;&#36890;&#36807;&#20998;&#26512;Kaggle&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#24615;&#20998;&#26512;&#22522;&#20934;&#65292;&#24182;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#25968;&#25454;&#24211;&#27169;&#24335;&#20803;&#32032;&#65292;&#20197;&#25351;&#23548;&#35843;&#20248;&#21644;&#20998;&#26512;&#24037;&#20316;&#12290;&#22522;&#26412;&#20551;&#35774;&#26159;&#65292;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#27169;&#24335;&#25991;&#26412;&#20013;&#25552;&#21462;&#26377;&#20851;&#25968;&#25454;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#22312;&#25968;&#25454;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#32972;&#26223;&#19979;&#32771;&#23519;&#20102;&#36825;&#19968;&#20551;&#35774;&#65306;&#36890;&#36807;&#20998;&#26512;&#21015;&#21517;&#65292;&#33021;&#21542;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25214;&#21040;&#20855;&#26377;&#30456;&#20851;&#25968;&#25454;&#30340;&#21015;&#23545;&#65311;&#39318;&#20808;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#30456;&#20851;&#24615;&#20998;&#26512;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;&#25968;&#21315;&#20010;Kaggle&#25968;&#25454;&#38598;&#21019;&#24314;(&#21487;&#19979;&#36733;)&#12290;&#20854;&#27425;&#65292;&#23427;&#21033;&#29992;&#35813;&#25968;&#25454;&#26469;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#21015;&#21517;&#39044;&#27979;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#12290;&#20998;&#26512;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#12289;&#21508;&#31181;&#30456;&#20851;&#24615;&#24230;&#37327;&#25351;&#26631;&#20197;&#21450;&#20247;&#22810;&#20934;&#30830;&#24230;&#25351;&#26631;&#12290;&#23427;&#30830;&#23450;&#20102;&#23548;&#33268;&#25104;&#21151;&#39044;&#27979;&#30340;&#22240;&#32032;&#65292;&#22914;&#21015;&#21517;&#38271;&#24230;&#21644;&#21333;&#35789;&#27604;&#20363;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent publications suggest using natural language analysis on database schema elements to guide tuning and profiling efforts. The underlying hypothesis is that state-of-the-art language processing methods, so-called language models, are able to extract information on data properties from schema text.  This paper examines that hypothesis in the context of data correlation analysis: is it possible to find column pairs with correlated data by analyzing their names via language models? First, the paper introduces a novel benchmark for data correlation analysis, created by analyzing thousands of Kaggle data sets (and available for download). Second, it uses that data to study the ability of language models to predict correlation, based on column names. The analysis covers different language models, various correlation metrics, and a multitude of accuracy metrics. It pinpoints factors that contribute to successful predictions, such as the length of column names as well as the ratio of words
&lt;/p&gt;</description></item><item><title>NewB&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;11&#20010;&#26032;&#38395;&#26469;&#28304;&#23545;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#30340;200,000&#22810;&#20010;&#21477;&#23376;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#21477;&#23376;&#30340;&#26032;&#38395;&#26469;&#28304;&#65292;&#24471;&#21040;&#27604;&#20256;&#32479;&#20998;&#31867;&#31995;&#32479;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#23545;&#23186;&#20307;&#23545;&#29305;&#26391;&#26222;&#30340;&#25551;&#32472;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2006.03051</link><description>&lt;p&gt;
&#26032;&#38395;&#20559;&#35265;&#26816;&#27979;&#30340;200,000+&#21477;&#23376;&#65306;NewB&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NewB: 200,000+ Sentences for Political Bias Detection. (arXiv:2006.03051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.03051
&lt;/p&gt;
&lt;p&gt;
NewB&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;11&#20010;&#26032;&#38395;&#26469;&#28304;&#23545;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#30340;200,000&#22810;&#20010;&#21477;&#23376;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#21477;&#23376;&#30340;&#26032;&#38395;&#26469;&#28304;&#65292;&#24471;&#21040;&#27604;&#20256;&#32479;&#20998;&#31867;&#31995;&#32479;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#23545;&#23186;&#20307;&#23545;&#29305;&#26391;&#26222;&#30340;&#25551;&#32472;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Newspaper Bias Dataset (NewB)&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;11&#20010;&#26032;&#38395;&#26469;&#28304;&#20851;&#20110;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#30340;200,000&#22810;&#20010;&#21477;&#23376;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#23558;&#21477;&#23376;&#26631;&#35760;&#20026;&#33258;&#30001;&#27966;&#25110;&#20445;&#23432;&#27966;&#19981;&#21516;&#65292;NewB&#28085;&#30422;&#20102;11&#23478;&#28909;&#38376;&#23186;&#20307;&#28304;&#30340;&#25919;&#27835;&#35266;&#28857;&#65292;&#25429;&#25417;&#21040;&#27604;&#20256;&#32479;&#30340;&#20108;&#20803;&#20998;&#31867;&#31995;&#32479;&#26356;&#32454;&#33268;&#30340;&#25919;&#27835;&#35266;&#28857;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#32473;&#23450;&#21477;&#23376;&#30340;&#26032;&#38395;&#26469;&#28304;&#65292;&#32467;&#26524;&#21457;&#29616;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;top-1&#12289;top-3&#21644;top-5&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;33.3%&#12289;61.4%&#21644;77.6%&#65292;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;18.3%&#12289;42.6%&#21644;60.8%&#12290;&#21033;&#29992;&#21477;&#23376;&#30340;&#26032;&#38395;&#26469;&#28304;&#26631;&#31614;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#22411;&#20998;&#26512;&#20102;&#21069;n&#20010;&#35789;&#32452;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#23186;&#20307;&#26469;&#28304;&#23545;&#29305;&#26391;&#26222;&#30340;&#25551;&#32472;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;&#21457;&#24067;&#33021;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Newspaper Bias Dataset (NewB), a text corpus of more than 200,000 sentences from eleven news sources regarding Donald Trump. While previous datasets have labeled sentences as either liberal or conservative, NewB covers the political views of eleven popular media sources, capturing more nuanced political viewpoints than a traditional binary classification system does. We train two state-of-the-art deep learning models to predict the news source of a given sentence from eleven newspapers and find that a recurrent neural network achieved top-1, top-3, and top-5 accuracies of 33.3%, 61.4%, and 77.6%, respectively, significantly outperforming a baseline logistic regression model's accuracies of 18.3%, 42.6%, and 60.8%. Using the news source label of sentences, we analyze the top n-grams with our model to gain meaningful insight into the portrayal of Trump by media sources.We hope that the public release of our dataset will encourage further research in using natural language 
&lt;/p&gt;</description></item><item><title>COVID-Q&#26159;&#19968;&#20010;&#21253;&#21547;1,690&#20010;&#20851;&#20110;COVID-19&#30340;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#23545;&#36825;&#20123;&#38382;&#39064;&#36827;&#34892;&#20998;&#31867;&#21644;&#32858;&#31867;&#65292;&#24182;&#20026;&#24320;&#21457;&#24212;&#29992;&#31995;&#32479;&#25110;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2005.12522</link><description>&lt;p&gt;
&#20154;&#20204;&#23545;COVID-19&#26377;&#21738;&#20123;&#38382;&#39064;&#65311;&#19968;&#20010;&#38382;&#39064;&#20998;&#31867;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
What Are People Asking About COVID-19? A Question Classification Dataset. (arXiv:2005.12522v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.12522
&lt;/p&gt;
&lt;p&gt;
COVID-Q&#26159;&#19968;&#20010;&#21253;&#21547;1,690&#20010;&#20851;&#20110;COVID-19&#30340;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#23545;&#36825;&#20123;&#38382;&#39064;&#36827;&#34892;&#20998;&#31867;&#21644;&#32858;&#31867;&#65292;&#24182;&#20026;&#24320;&#21457;&#24212;&#29992;&#31995;&#32479;&#25110;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;COVID-Q&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;13&#20010;&#26469;&#28304;&#30340;1,690&#20010;&#20851;&#20110;COVID-19&#30340;&#38382;&#39064;&#38598;&#21512;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#20998;&#20026;15&#20010;&#38382;&#39064;&#31867;&#21035;&#21644;207&#20010;&#38382;&#39064;&#32858;&#31867;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#26368;&#24120;&#35265;&#30340;&#38382;&#39064;&#28041;&#21450;COVID&#30340;&#20256;&#25773;&#12289;&#39044;&#38450;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35768;&#22810;&#22312;&#22810;&#20010;&#26469;&#28304;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#26410;&#34987;&#20219;&#20309;&#21487;&#38752;&#26426;&#26500;&#65288;&#22914;CDC&#21644;FDA&#65289;&#30340;FAQ&#32593;&#31449;&#22238;&#31572;&#12290;&#25105;&#20204;&#22312;https://github.com/JerryWeiAI/COVID-Q&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#23558;&#38382;&#39064;&#20998;&#31867;&#20026;15&#20010;&#31867;&#21035;&#65292;&#24403;&#27599;&#20010;&#31867;&#21035;&#35757;&#32451;20&#20010;&#26679;&#26412;&#26102;&#65292;BERT&#22522;&#20934;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;58.1%&#12290;&#23545;&#20110;&#38382;&#39064;&#32858;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;BERT+SiamLoss&#22522;&#20934;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;49.5%&#12290;&#25105;&#20204;&#24076;&#26395;COVID-Q&#33021;&#22815;&#30452;&#25509;&#29992;&#20110;&#24320;&#21457;&#23454;&#38469;&#31995;&#32479;&#25110;&#20316;&#20026;&#39046;&#22495;&#29305;&#23450;&#36164;&#28304;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present COVID-Q, a set of 1,690 questions about COVID-19 from 13 sources, which we annotate into 15 question categories and 207 question clusters. The most common questions in our dataset asked about transmission, prevention, and societal effects of COVID, and we found that many questions that appeared in multiple sources were not answered by any FAQ websites of reputable organizations such as the CDC and FDA. We post our dataset publicly at https://github.com/JerryWeiAI/COVID-Q. For classifying questions into 15 categories, a BERT baseline scored 58.1% accuracy when trained on 20 examples per category, and for a question clustering task, a BERT + triplet loss baseline achieved 49.5% accuracy. We hope COVID-Q can help either for direct use in developing applied systems or as a domain-specific resource for model evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#32447;&#24615;LSTM&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#34913;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#39033;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#30340;&#21033;&#29992;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#26356;&#22810;&#30340;&#23398;&#20064;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/1910.10294</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#32447;&#24615;LSTM&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unifying Framework of Bilinear LSTMs. (arXiv:1910.10294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.10294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#32447;&#24615;LSTM&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#34913;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#39033;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#30340;&#21033;&#29992;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#26356;&#22810;&#30340;&#23398;&#20064;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#21452;&#32447;&#24615;LSTM&#26694;&#26550;&#65292;&#21487;&#20197;&#34920;&#31034;&#21644;&#21033;&#29992;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#65292;&#20197;&#23454;&#29616;&#27604;&#32447;&#24615;LSTM&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#20250;&#22686;&#21152;&#26356;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30340;&#32479;&#19968;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#35843;&#25972;&#38544;&#34255;&#29366;&#24577;&#21521;&#37327;&#30340;&#22823;&#23567;&#19982;&#21452;&#32447;&#24615;&#39033;&#20013;&#26435;&#37325;&#30697;&#38453;&#30340;&#36924;&#36817;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#26469;&#24179;&#34913;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#39033;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#20248;&#21270;&#25105;&#20204;&#30340;&#21452;&#32447;&#24615;LSTM&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#20250;&#22686;&#21152;&#26356;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20110;&#35821;&#35328;&#30340;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#20013;&#23545;&#25105;&#20204;&#30340;&#21452;&#32447;&#24615;LSTM&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#20854;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel unifying framework of bilinear LSTMs that can represent and utilize the nonlinear interaction of the input features present in sequence datasets for achieving superior performance over a linear LSTM and yet not incur more parameters to be learned. To realize this, our unifying framework allows the expressivity of the linear vs. bilinear terms to be balanced by correspondingly trading off between the hidden state vector size vs. approximation quality of the weight matrix in the bilinear term so as to optimize the performance of our bilinear LSTM, while not incurring more parameters to be learned. We empirically evaluate the performance of our bilinear LSTM in several language-based sequence learning tasks to demonstrate its general applicability.
&lt;/p&gt;</description></item></channel></rss>