<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20154;&#31867;&#25512;&#29702;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#36890;&#36807;&#19968;&#20010;&#27969;&#27700;&#32447;&#21644;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27979;&#37327;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04461</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models. (arXiv:2309.04461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20154;&#31867;&#25512;&#29702;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#36890;&#36807;&#19968;&#20010;&#27969;&#27700;&#32447;&#21644;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27979;&#37327;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#33021;&#35299;&#26512;&#20851;&#20110;&#35270;&#35273;&#20869;&#23481;&#30340;&#33258;&#28982;&#26597;&#35810;&#24182;&#29983;&#25104;&#31867;&#20154;&#36755;&#20986;&#30340;&#35270;&#35273;&#21161;&#25163;&#65292;&#23637;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#21151;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#22522;&#20110;&#25152;&#24863;&#30693;&#20449;&#24687;&#30340;&#31867;&#20154;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#20851;&#20110;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#21040;&#24213;&#26377;&#22810;&#19968;&#33268;&#21644;&#26377;&#22810;&#22522;&#20110;&#23454;&#38469;&#30340;&#19968;&#20010;&#37325;&#35201;&#30097;&#34385;&#65292;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35780;&#20272;&#38656;&#35201;&#28085;&#30422;&#39640;&#23618;&#27425;&#25512;&#29702;&#21644;&#32454;&#33410;&#25512;&#29702;&#38142;&#30340;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;LLM-Human-in-the-Loop&#27969;&#27700;&#32447;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#27969;&#27700;&#32447;&#26174;&#33879;&#38477;&#20302;&#20102;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20010;&#27969;&#27700;&#32447;&#21644;&#29616;&#26377;&#30340;&#31895;&#31890;&#24230;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;CURE&#22522;&#20934;&#26469;&#21516;&#26102;&#27979;&#37327;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20013;&#22269;&#32929;&#31080;&#25919;&#31574;&#26816;&#32034;&#25968;&#25454;&#38598;&#65288;CSPRD&#65289;&#65292;&#25552;&#20379;&#20102;700+&#26465;&#26631;&#27880;&#30340;&#25307;&#32929;&#35828;&#26126;&#20070;&#27573;&#33853;&#65292;&#36890;&#36807;&#35789;&#27719;&#12289;&#23884;&#20837;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#21452;&#32534;&#30721;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;CSPRD&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04389</link><description>&lt;p&gt;
CSPRD: &#20013;&#22269;&#32929;&#31080;&#24066;&#22330;&#37329;&#34701;&#25919;&#31574;&#26816;&#32034;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CSPRD: A Financial Policy Retrieval Dataset for Chinese Stock Market. (arXiv:2309.04389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20013;&#22269;&#32929;&#31080;&#25919;&#31574;&#26816;&#32034;&#25968;&#25454;&#38598;&#65288;CSPRD&#65289;&#65292;&#25552;&#20379;&#20102;700+&#26465;&#26631;&#27880;&#30340;&#25307;&#32929;&#35828;&#26126;&#20070;&#27573;&#33853;&#65292;&#36890;&#36807;&#35789;&#27719;&#12289;&#23884;&#20837;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#21452;&#32534;&#30721;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;CSPRD&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#24341;&#36215;&#20102;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#24182;&#22312;&#31264;&#23494;&#27573;&#33853;&#26816;&#32034;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20854;&#30446;&#30340;&#26159;&#26816;&#32034;&#32473;&#23450;&#38382;&#39064;&#30340;&#30456;&#20851;&#27573;&#33853;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#20351;&#29992;&#20102;&#36890;&#24120;&#24120;&#35782;&#30340;&#20107;&#23454;&#24615;&#26597;&#35810;&#26469;&#35780;&#20272;&#27169;&#22411;&#65292;&#32780;&#37329;&#34701;&#21644;&#32463;&#27982;&#31561;&#19987;&#19994;&#39046;&#22495;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#21644;&#19987;&#23478;&#27880;&#37322;&#32780;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#25919;&#31574;&#26816;&#32034;&#65292;&#36890;&#36807;&#24341;&#20837;&#20013;&#22269;&#32929;&#31080;&#25919;&#31574;&#26816;&#32034;&#25968;&#25454;&#38598;&#65288;CSPRD&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#30001;&#26377;&#32463;&#39564;&#30340;&#19987;&#23478;&#23545;&#26469;&#33258;&#25105;&#20204;&#25910;&#38598;&#30340;&#20013;&#22269;&#25919;&#31574;&#35821;&#26009;&#24211;&#20013;&#30340;10k+&#26465;&#30446;&#30340;&#30456;&#20851;&#25991;&#31456;&#36827;&#34892;&#26631;&#27880;&#30340;700+&#26465;&#25307;&#32929;&#35828;&#26126;&#20070;&#27573;&#33853;&#12290;&#23545;&#35789;&#27719;&#12289;&#23884;&#20837;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#21452;&#32534;&#30721;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;CSPRD&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20063;&#25552;&#31034;&#20102;&#25913;&#36827;&#30340;&#20016;&#23500;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, great advances in pre-trained language models (PLMs) have sparked considerable research focus and achieved promising performance on the approach of dense passage retrieval, which aims at retrieving relative passages from massive corpus with given questions. However, most of existing datasets mainly benchmark the models with factoid queries of general commonsense, while specialised fields such as finance and economics remain unexplored due to the deficiency of large-scale and high-quality datasets with expert annotations. In this work, we propose a new task, policy retrieval, by introducing the Chinese Stock Policy Retrieval Dataset (CSPRD), which provides 700+ prospectus passages labeled by experienced experts with relevant articles from 10k+ entries in our collected Chinese policy corpus. Experiments on lexical, embedding and fine-tuned bi-encoder models show the effectiveness of our proposed CSPRD yet also suggests ample potential for improvement. Our best performing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;&#25511;&#21046;&#22120;(MoEController)&#30340;&#25351;&#20196;&#39537;&#21160;&#20219;&#24847;&#22270;&#20687;&#25805;&#20316;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#36866;&#37197;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#24320;&#25918;&#22495;&#22270;&#20687;&#25805;&#20316;&#20219;&#21153;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;MOE&#25216;&#26415;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#24212;&#24615;&#35757;&#32451;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.04372</link><description>&lt;p&gt;
MoEController: &#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#25511;&#21046;&#22120;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20219;&#24847;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers. (arXiv:2309.04372v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;&#25511;&#21046;&#22120;(MoEController)&#30340;&#25351;&#20196;&#39537;&#21160;&#20219;&#24847;&#22270;&#20687;&#25805;&#20316;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#36866;&#37197;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#24320;&#25918;&#22495;&#22270;&#20687;&#25805;&#20316;&#20219;&#21153;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;MOE&#25216;&#26415;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#24212;&#24615;&#35757;&#32451;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#36827;&#23637;&#65292;&#22312;&#24320;&#25918;&#22495;&#22270;&#20687;&#25805;&#20316;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#20196;&#20154;&#30528;&#36855;&#30340;&#32467;&#26524;&#12290; &#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#20687;&#25805;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#27169;&#22411;&#20855;&#26377;&#23436;&#20840;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#26082;&#21487;&#20197;&#36827;&#34892;&#20840;&#23616;&#25805;&#20316;&#65292;&#21448;&#21487;&#20197;&#36827;&#34892;&#23616;&#37096;&#22270;&#20687;&#32534;&#36753;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#25511;&#21046;&#22120;&#65288;MOE&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#24341;&#23548;&#33021;&#21147;&#19982;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#31867;&#25351;&#20196;&#30456;&#23545;&#40784;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22788;&#29702;&#21508;&#31181;&#24320;&#25918;&#22495;&#22270;&#20687;&#25805;&#20316;&#20219;&#21153;&#12290; &#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGPT&#65289;&#21644;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65288;ControlNet&#65289;&#29983;&#25104;&#22823;&#37327;&#20840;&#23616;&#22270;&#20687;&#36716;&#25442;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#30340;&#23616;&#37096;&#22270;&#20687;&#32534;&#36753;&#25968;&#25454;&#38598;&#12290; &#28982;&#21518;&#65292;&#20351;&#29992;MOE&#25216;&#26415;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#24212;&#24615;&#35757;&#32451;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23545;&#22270;&#20687;&#36827;&#34892;&#20840;&#23616;&#21644;&#23616;&#37096;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-model-based text-guided image generation has recently made astounding progress, producing fascinating results in open-domain image manipulation tasks. Few models, however, currently have complete zero-shot capabilities for both global and local image editing due to the complexity and diversity of image manipulation tasks. In this work, we propose a method with a mixture-of-expert (MOE) controllers to align the text-guided capacity of diffusion models with different kinds of human instructions, enabling our model to handle various open-domain image manipulation tasks with natural language instructions. First, we use large language models (ChatGPT) and conditional image synthesis models (ControlNet) to generate a large number of global image transfer dataset in addition to the instruction-based local image editing dataset. Then, using an MOE technique and task-specific adaptation training on a large-scale dataset, our conditional diffusion model can edit images globally and loc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20132;&#20114;&#30340;LLM&#35780;&#20272;&#26694;&#26550;&#65292;&#31361;&#30772;&#20102;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#35780;&#20272;LLM&#22312;&#21160;&#24577;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.04369</link><description>&lt;p&gt;
&#36229;&#36234;&#38745;&#24577;&#25968;&#25454;&#38598;&#65306;&#28145;&#24230;&#20132;&#20114;&#26041;&#27861;&#29992;&#20110;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation. (arXiv:2309.04369v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04369
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20132;&#20114;&#30340;LLM&#35780;&#20272;&#26694;&#26550;&#65292;&#31361;&#30772;&#20102;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#35780;&#20272;LLM&#22312;&#21160;&#24577;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36825;&#28608;&#21457;&#20102;&#23545;LLM&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;LLM&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#26080;&#27861;&#35780;&#20272;LLM&#22312;&#21160;&#24577;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#22330;&#26223;&#20013;&#28145;&#24230;&#20132;&#20114;&#24191;&#27867;&#23384;&#22312;&#12290;&#20854;&#20182;&#30340;LLM&#35780;&#20272;&#26041;&#27861;&#22522;&#20110;&#20154;&#24037;&#35780;&#20272;&#65292;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#65292;&#24182;&#19988;&#26080;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;LLM&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#20132;&#20114;&#30340;LLM&#35780;&#20272;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#35780;&#20272;&#20219;&#21153;&#65292;&#21487;&#20197;&#35780;&#20272;LLM&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19982;&#20854;&#20182;LLM&#30340;&#28145;&#24230;&#20132;&#20114;&#20013;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#35832;&#22810;&#23454;&#38469;&#20219;&#21153;&#65292;&#22914;&#26426;&#22120;&#32763;&#35793;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made progress in various real-world tasks, which stimulates requirements for the evaluation of LLMs. Existing LLM evaluation methods are mainly supervised signal-based which depends on static datasets and cannot evaluate the ability of LLMs in dynamic real-world scenarios where deep interaction widely exists. Other LLM evaluation methods are human-based which are costly and time-consuming and are incapable of large-scale evaluation of LLMs. To address the issues above, we propose a novel Deep Interaction-based LLM-evaluation framework. In our proposed framework, LLMs' performances in real-world domains can be evaluated from their deep interaction with other LLMs in elaborately designed evaluation tasks. Furthermore, our proposed framework is a general evaluation method that can be applied to a host of real-world tasks such as machine translation and code generation. We demonstrate the effectiveness of our proposed method through extensive experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Multi2SPE&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#26469;&#32534;&#30721;&#22810;&#39046;&#22495;&#31185;&#23398;&#35770;&#25991;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#31185;&#23398;&#39046;&#22495;&#65292;&#24182;&#22312;&#24341;&#25991;&#39044;&#27979;&#20219;&#21153;&#20013;&#20943;&#23569;&#20102;&#22810;&#36798;25&#65285;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.04333</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#38598;&#21512;&#23545;&#22810;&#39046;&#22495;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens. (arXiv:2309.04333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Multi2SPE&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#26469;&#32534;&#30721;&#22810;&#39046;&#22495;&#31185;&#23398;&#35770;&#25991;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#31185;&#23398;&#39046;&#22495;&#65292;&#24182;&#22312;&#24341;&#25991;&#39044;&#27979;&#20219;&#21153;&#20013;&#20943;&#23569;&#20102;&#22810;&#36798;25&#65285;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#25991;&#26723;&#19978;&#30340;&#26377;&#29992;&#20219;&#21153;&#65292;&#22914;&#20027;&#39064;&#20998;&#31867;&#21644;&#24341;&#25991;&#39044;&#27979;&#65292;&#28041;&#21450;&#36328;&#36234;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;Transformer&#30340;&#21333;&#20010;CLS&#26631;&#35760;&#30340;&#21521;&#37327;&#23884;&#20837;&#26469;&#23436;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#21487;&#20197;&#20351;Transformer&#26356;&#22909;&#22320;&#19987;&#27880;&#20110;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Multi2SPE&#65306;&#23427;&#40723;&#21169;&#22810;&#20010;CLS&#26631;&#35760;&#23398;&#20064;&#32858;&#21512;&#26631;&#35760;&#23884;&#20837;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30456;&#21152;&#20197;&#21019;&#24314;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26032;&#30340;&#22810;&#39046;&#22495;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;Multi-SciDocs&#65292;&#20197;&#27979;&#35797;&#22810;&#39046;&#22495;&#35774;&#32622;&#19979;&#30340;&#31185;&#23398;&#35770;&#25991;&#21521;&#37327;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;Multi2SPE&#22312;&#22810;&#39046;&#22495;&#24341;&#25991;&#39044;&#27979;&#20013;&#20943;&#23569;&#20102;&#22810;&#36798;25&#65285;&#30340;&#35823;&#24046;&#65292;&#21516;&#26102;&#38500;&#20102;&#19968;&#27425;BERT&#21069;&#21521;&#20256;&#36882;&#20043;&#22806;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#37327;&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many useful tasks on scientific documents, such as topic classification and citation prediction, involve corpora that span multiple scientific domains. Typically, such tasks are accomplished by representing the text with a vector embedding obtained from a Transformer's single CLS token. In this paper, we argue that using multiple CLS tokens could make a Transformer better specialize to multiple scientific domains. We present Multi2SPE: it encourages each of multiple CLS tokens to learn diverse ways of aggregating token embeddings, then sums them up together to create a single vector representation. We also propose our new multi-domain benchmark, Multi-SciDocs, to test scientific paper vector encoders under multi-domain settings. We show that Multi2SPE reduces error by up to 25 percent in multi-domain citation prediction, while requiring only a negligible amount of computation in addition to one BERT forward pass.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#27169;&#31946;&#25351;&#32441;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#65292;&#20197;&#33719;&#24471;&#26356;&#31616;&#21333;&#21644;&#26356;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;DailyDialog ERC&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04292</link><description>&lt;p&gt;
&#27169;&#31946;&#25351;&#32441;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition in Conversations. (arXiv:2309.04292v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#27169;&#31946;&#25351;&#32441;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#65292;&#20197;&#33719;&#24471;&#26356;&#31616;&#21333;&#21644;&#26356;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;DailyDialog ERC&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#25351;&#32441;&#24050;&#25104;&#21151;&#29992;&#20316;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#20294;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#25216;&#26415;&#19968;&#26679;&#65292;&#22312;&#24615;&#33021;&#19978;&#24050;&#34987;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;BERT&#25110;RoBERTa&#65289;&#22823;&#22823;&#36229;&#36234;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21363;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#35828;&#26126;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;ERC&#65292;&#20197;&#33719;&#24471;&#26356;&#31616;&#21333;&#21644;&#26356;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#35805;&#35821;&#21450;&#20854;&#20043;&#21069;&#30340;&#23545;&#35805;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;RoBERTa&#65292;&#24182;&#33719;&#21462;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#35805;&#35821;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#25552;&#20379;&#32473;&#36866;&#24212;&#30340;&#27169;&#31946;&#25351;&#32441;&#20998;&#31867;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;DailyDialog ERC&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#20351;&#29992;&#20102;&#26356;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Fingerprints have been successfully used as an interpretable text classification technique, but, like most other techniques, have been largely surpassed in performance by Large Pre-trained Language Models, such as BERT or RoBERTa. These models deliver state-of-the-art results in several Natural Language Processing tasks, namely Emotion Recognition in Conversations (ERC), but suffer from the lack of interpretability and explainability. In this paper, we propose to combine the two approaches to perform ERC, as a means to obtain simpler and more interpretable Large Language Models-based classifiers. We propose to feed the utterances and their previous conversational turns to a pre-trained RoBERTa, obtaining contextual embedding utterance representations, that are then supplied to an adapted Fuzzy Fingerprint classification module. We validate our approach on the widely used DailyDialog ERC benchmark dataset, in which we obtain state-of-the-art level results using a much lighter mode
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#23494;&#24230;&#38142;&#8221;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#23558;GPT-4&#29983;&#25104;&#30340;&#25688;&#35201;&#20174;&#31232;&#30095;&#36716;&#21270;&#20026;&#31264;&#23494;&#12290;&#20154;&#20204;&#26356;&#21916;&#27426;&#23494;&#24230;&#38142;&#25688;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#20855;&#27010;&#25324;&#24615;&#21644;&#34701;&#21512;&#24615;&#65292;&#24182;&#19988;&#20960;&#20046;&#19982;&#20154;&#24037;&#20889;&#20316;&#30340;&#25688;&#35201;&#19968;&#26679;&#23494;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.04269</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;&#21040;&#31264;&#23494;&#65306;&#20351;&#29992;&#23494;&#24230;&#38142;&#25552;&#31034;&#30340;GPT-4&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. (arXiv:2309.04269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04269
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#23494;&#24230;&#38142;&#8221;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#23558;GPT-4&#29983;&#25104;&#30340;&#25688;&#35201;&#20174;&#31232;&#30095;&#36716;&#21270;&#20026;&#31264;&#23494;&#12290;&#20154;&#20204;&#26356;&#21916;&#27426;&#23494;&#24230;&#38142;&#25688;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#20855;&#27010;&#25324;&#24615;&#21644;&#34701;&#21512;&#24615;&#65292;&#24182;&#19988;&#20960;&#20046;&#19982;&#20154;&#24037;&#20889;&#20316;&#30340;&#25688;&#35201;&#19968;&#26679;&#23494;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#22312;&#25688;&#35201;&#20013;&#21253;&#21547;&#30340;&#8220;&#27491;&#30830;&#8221;&#20449;&#24687;&#37327;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#19968;&#20010;&#22909;&#30340;&#25688;&#35201;&#24212;&#35813;&#26159;&#35814;&#32454;&#21644;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#65292;&#21516;&#26102;&#21448;&#19981;&#36807;&#20110;&#23494;&#38598;&#21644;&#38590;&#20197;&#29702;&#35299;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23494;&#24230;&#38142;&#8221;&#65288;CoD&#65289;&#25552;&#31034;&#30340;&#26041;&#24335;&#65292;&#26469;&#20135;&#29983;&#36234;&#26469;&#36234;&#31264;&#23494;&#30340;GPT-4&#25688;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GPT-4&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#21021;&#22987;&#30340;&#23454;&#20307;&#31232;&#30095;&#25688;&#35201;&#65292;&#28982;&#21518;&#36880;&#27493;&#21152;&#20837;&#32570;&#22833;&#30340;&#31361;&#20986;&#23454;&#20307;&#65292;&#32780;&#19981;&#22686;&#21152;&#38271;&#24230;&#12290; CoD&#29983;&#25104;&#30340;&#25688;&#35201;&#26356;&#20026;&#27010;&#25324;&#65292;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#34701;&#21512;&#65292;&#24182;&#19988;&#27604;&#20351;&#29992;&#26222;&#36890;&#25552;&#31034;&#29983;&#25104;&#30340;GPT-4&#25688;&#35201;&#20855;&#26377;&#26356;&#23569;&#30340;&#24341;&#23548;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;100&#31687;CNN DailyMail&#25991;&#31456;&#19978;&#36827;&#34892;&#20102;&#19968;&#20010;&#20154;&#31867;&#20559;&#22909;&#30740;&#31350;&#65292;&#21457;&#29616;&#20154;&#20204;&#26356;&#21916;&#27426;&#27604;&#26222;&#36890;&#25552;&#31034;&#29983;&#25104;&#30340;GPT-4&#25688;&#35201;&#26356;&#31264;&#23494;&#65292;&#20960;&#20046;&#19982;&#20154;&#24037;&#25776;&#20889;&#30340;&#25688;&#35201;&#19968;&#26679;&#23494;&#38598;&#12290;&#23450;&#24615;&#20998;&#26512;&#25903;&#25345;&#20102;&#20449;&#24687;&#37327;&#21644;&#21487;&#35835;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the ``right'' amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a ``Chain of Density'' (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between informativeness and readability. 500 annotated CoD summaries
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEX&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04213</link><description>&lt;p&gt;
UQ&#22312;#SMM4H 2023&#19978;&#30340;&#35770;&#25991;&#65306;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#36827;&#34892;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#30340;ALEX&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media. (arXiv:2309.04213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEX&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#19982;&#20844;&#20849;&#21355;&#29983;&#30456;&#20851;&#30340;&#27963;&#21160;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#30446;&#21069;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#25216;&#26415;&#28041;&#21450;&#21040;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#22914;BERT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#20026;&#20844;&#20849;&#21355;&#29983;&#22495;&#35757;&#32451;LLMs&#30340;&#25104;&#26412;&#23588;&#20854;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#36825;&#31181;&#22495;&#20869;&#25968;&#25454;&#38598;&#24448;&#24448;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#35774;&#32622;&#27169;&#22411;&#30340;&#24341;&#23548;&#26041;&#24335;&#26377;&#25928;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ALEX&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;ALEX&#27169;&#22411;&#22312;Social Media Mining for Health 2023 &#65288;SMM4H&#65289;&#30340;&#20219;&#21153;2&#21644;&#20219;&#21153;4&#20013;&#33719;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;&#20219;&#21153;1&#20013;&#24471;&#21040;&#20102;&#36739;&#39640;&#30340;&#35780;&#20998;[1]&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312; https:/ /github &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media becomes increasingly popular, more and more activities related to public health emerge. Current techniques for public health analysis involve popular models such as BERT and large language models (LLMs). However, the costs of training in-domain LLMs for public health are especially expensive. Furthermore, such kinds of in-domain datasets from social media are generally imbalanced. To tackle these challenges, the data imbalance issue can be overcome by data augmentation and balanced training. Moreover, the ability of the LLMs can be effectively utilized by prompting the model properly. In this paper, a novel ALEX framework is proposed to improve the performance of public health analysis on social media by adopting an LLMs explanation mechanism. Results show that our ALEX model got the best performance among all submissions in both Task 2 and Task 4 with a high score in Task 1 in Social Media Mining for Health 2023 (SMM4H)[1]. Our code has been released at https:// github
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;CALLA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;LLMs&#20174;&#20013;&#25991;&#21307;&#23398;&#25991;&#29486;&#20013;&#33719;&#21462;&#20132;&#20114;&#24335;&#30693;&#35782;&#12290;&#36890;&#36807;&#33258;&#30001;&#23545;&#35805;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;LLMs&#25484;&#25569;&#21307;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20107;&#23454;&#36319;&#38543;&#21709;&#24212;&#8221;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20154;&#24037;&#26500;&#24314;&#20102;&#20004;&#31181;&#35282;&#24230;&#30340;&#27979;&#35797;&#25968;&#25454;&#65306;&#19968;&#31181;&#19982;&#20107;&#23454;&#19968;&#33268;&#65292;&#19968;&#31181;&#19982;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2309.04198</link><description>&lt;p&gt;
CALLA&#25968;&#25454;&#38598;&#65306;&#20174;&#20013;&#25991;&#21307;&#23398;&#25991;&#29486;&#20013;&#25506;&#32034;LLMs&#30340;&#20132;&#20114;&#24335;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04198
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;CALLA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;LLMs&#20174;&#20013;&#25991;&#21307;&#23398;&#25991;&#29486;&#20013;&#33719;&#21462;&#20132;&#20114;&#24335;&#30693;&#35782;&#12290;&#36890;&#36807;&#33258;&#30001;&#23545;&#35805;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;LLMs&#25484;&#25569;&#21307;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20107;&#23454;&#36319;&#38543;&#21709;&#24212;&#8221;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20154;&#24037;&#26500;&#24314;&#20102;&#20004;&#31181;&#35282;&#24230;&#30340;&#27979;&#35797;&#25968;&#25454;&#65306;&#19968;&#31181;&#19982;&#20107;&#23454;&#19968;&#33268;&#65292;&#19968;&#31181;&#19982;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36890;&#36807;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#25351;&#23548;&#24494;&#35843;&#65288;IFT&#65289;&#25968;&#25454;&#65292;&#20197;&#20016;&#23500;LLMs&#30340;&#20132;&#20114;&#24335;&#21307;&#23398;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20016;&#23500;&#30340;&#21307;&#23398;&#30693;&#35782;&#26469;&#28304;&#30340;&#21307;&#23398;&#25991;&#29486;&#20173;&#26410;&#34987;&#24320;&#21457;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;CALLA&#25968;&#25454;&#38598;&#65292;&#20197;&#25506;&#32034;LLMs&#20174;&#20013;&#22269;&#21307;&#23398;&#25991;&#29486;&#20013;&#33719;&#21462;&#20132;&#20114;&#24335;&#30693;&#35782;&#12290;&#23427;&#36890;&#36807;&#33258;&#30001;&#23545;&#35805;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#35780;&#20272;LLMs&#25484;&#25569;&#21307;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#29616;&#35937;&#31216;&#20026;&#8220;&#20107;&#23454;&#36319;&#38543;&#21709;&#24212;&#8221;&#65292;LLMs&#20542;&#21521;&#20110;&#30830;&#35748;&#38382;&#39064;&#20013;&#25552;&#21040;&#30340;&#20107;&#23454;&#65292;&#24182;&#23545;&#25361;&#25112;&#36825;&#20123;&#20107;&#23454;&#34920;&#29616;&#20986;&#19981;&#24773;&#24895;&#12290;&#20026;&#28040;&#38500;&#36825;&#31181;&#29616;&#35937;&#23548;&#33268;&#30340;&#19981;&#20934;&#30830;&#35780;&#20272;&#65292;&#23545;&#20110;&#40644;&#37329;&#20107;&#23454;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#20154;&#24037;&#26500;&#24314;&#27979;&#35797;&#25968;&#25454;&#65306;&#19968;&#20010;&#19982;&#20107;&#23454;&#19968;&#33268;&#65292;&#19968;&#20010;&#19982;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;&#26681;&#25454;&#36825;&#20123;&#27979;&#35797;&#25968;&#25454;&#65292;&#25105;&#20204;&#20026;LLMs&#35780;&#20272;&#20854;&#23545;&#21307;&#23398;&#30693;&#35782;&#30340;&#25484;&#25569;&#33021;&#21147;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Large Language Models (LLMs) to the medical domain has stimulated the interest of researchers. Recent studies have focused on constructing Instruction Fine-Tuning (IFT) data through medical knowledge graphs to enrich the interactive medical knowledge of LLMs. However, the medical literature serving as a rich source of medical knowledge remains unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive knowledge acquisition from Chinese medical literature. It assesses the proficiency of LLMs in mastering medical knowledge through a free-dialogue fact-checking task. We identify a phenomenon called the ``fact-following response``, where LLMs tend to affirm facts mentioned in questions and display a reluctance to challenge them. To eliminate the inaccurate evaluation caused by this phenomenon, for the golden fact, we artificially construct test data from two perspectives: one consistent with the fact and one inconsistent with the fact. Drawing from the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#21307;&#23398;&#30693;&#35782;&#24211;&#36827;&#34892;&#30693;&#35782;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#21487;&#38752;&#21709;&#24212;&#29983;&#25104;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04175</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#21307;&#23398;&#30693;&#35782;&#24211;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#35843;&#25972;&#65292;&#23454;&#29616;&#20013;&#25991;&#21487;&#38752;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese. (arXiv:2309.04175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#21307;&#23398;&#30693;&#35782;&#24211;&#36827;&#34892;&#30693;&#35782;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#21487;&#38752;&#21709;&#24212;&#29983;&#25104;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#33324;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#38480;&#21046;&#65292;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#20855;&#26377;&#20851;&#20110;&#21307;&#23398;&#20107;&#23454;&#30340;&#24187;&#35273;&#30340;&#21709;&#24212;&#12290;&#36825;&#20123;&#32570;&#28857;&#22312;&#21307;&#23398;&#32972;&#26223;&#19979;&#20351;&#29992;LLMs&#26102;&#23384;&#22312;&#28508;&#22312;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#35843;&#25972;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#21307;&#23398;&#30693;&#35782;&#24211;&#26469;&#20351;LLMs&#33021;&#22815;&#39640;&#25928;&#25484;&#25569;&#39046;&#22495;&#30693;&#35782;&#24182;&#23454;&#29616;&#21487;&#38752;&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;cMedKnowQA&#65292;&#19968;&#20010;&#20174;&#21307;&#23398;&#30693;&#35782;&#24211;&#26500;&#24314;&#30340;&#20013;&#25991;&#21307;&#23398;&#30693;&#35782;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;LLMs&#30340;&#21307;&#23398;&#30693;&#35782;&#27700;&#24179;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;cMedKnowQA&#30340;&#30693;&#35782;&#35843;&#25972;&#30340;LLMs&#65292;&#22312;&#21709;&#24212;&#29983;&#25104;&#26041;&#38754;&#21487;&#20197;&#34920;&#29616;&#20986;&#27604;&#21407;&#22987;&#25351;&#23548;&#35843;&#25972;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;LLMs&#30340;&#39046;&#22495;&#36866;&#24212;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26032;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts. To address this challenge, we propose knowledge-tuning, which leverages structured medical knowledge bases for the LLMs to grasp domain knowledge efficiently and facilitate reliable response generation. We also release cMedKnowQA, a Chinese medical knowledge question-answering dataset constructed from medical knowledge bases to assess the medical knowledge proficiency of LLMs. Experimental results show that the LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of accuracy in response generation compared with vanilla instruction-tuning and offer a new reliable way for the domain adaptation of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#25928;&#26524;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2309.04174</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#30340;&#26080;&#35843;&#21442;&#25552;&#31034;&#20998;&#31867;&#30340;&#37325;&#26032;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification. (arXiv:2309.04174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#20998;&#31867;&#36890;&#36807;&#21033;&#29992;[MASK]&#26631;&#35760;&#30340;&#36951;&#28431;&#38382;&#39064;&#24418;&#24335;&#26469;&#36866;&#24212;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23558;&#22635;&#20805;&#30340;&#26631;&#35760;&#26144;&#23556;&#21040;&#26631;&#31614;&#19978;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26469;&#20943;&#23569;&#36825;&#19968;&#36807;&#31243;&#20013;&#30340;&#21171;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#38468;&#21152;&#21487;&#35757;&#32451;&#23884;&#20837;&#36827;&#34892;&#35843;&#21442;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#34920;&#31034;&#31354;&#38388;&#20013;&#28508;&#22312;&#30340;&#38750;&#32447;&#24615;&#27969;&#24418;&#65292;&#39640;&#32500;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#19981;&#24212;&#35813;&#20351;&#29992;&#27431;&#27663;&#36317;&#31163;&#26469;&#34913;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35843;&#21442;&#22522;&#20110;&#27969;&#24418;&#30340;&#31354;&#38388;&#37325;&#26032;&#23884;&#20837;&#26041;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#20869;&#31867;&#36817;&#37051;&#32422;&#26463;&#30340;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65288;LLE-INC&#65289;&#65292;&#29992;&#20110;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#65292;&#23427;&#20445;&#30041;&#20102;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#20316;&#20026;&#20998;&#31867;&#30340;&#24341;&#23548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#35843;&#20248;&#65292;&#25105;&#20204;&#30340;LLE-INC&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based classification adapts tasks to a cloze question format utilizing the [MASK] token and the filled tokens are then mapped to labels through pre-defined verbalizers. Recent studies have explored the use of verbalizer embeddings to reduce labor in this process. However, all existing studies require a tuning process for either the pre-trained models or additional trainable embeddings. Meanwhile, the distance between high-dimensional verbalizer embeddings should not be measured by Euclidean distance due to the potential for non-linear manifolds in the representation space. In this study, we propose a tuning-free manifold-based space re-embedding method called Locally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for verbalizer embeddings, which preserves local properties within the same class as guidance for classification. Experimental results indicate that even without tuning any parameters, our LLE-INC is on par with automated verbalizers with parameter 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GLS-CSC&#65292;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#20013;&#25991;STM&#27169;&#22411;&#23545;&#34920;&#38754;&#32447;&#32034;&#36807;&#24230;&#20381;&#36182;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#20998;&#26512;&#32534;&#36753;&#36317;&#31163;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35777;&#26126;GLS-CSC&#33021;&#22815;&#25552;&#39640;&#20013;&#25991;STM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04162</link><description>&lt;p&gt;
GLS-CSC: &#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#20943;&#23569;&#20013;&#25991;STM&#27169;&#22411;&#23545;&#34920;&#38754;&#32447;&#32034;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
GLS-CSC: A Simple but Effective Strategy to Mitigate Chinese STM Models' Over-Reliance on Superficial Clue. (arXiv:2309.04162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GLS-CSC&#65292;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#20013;&#25991;STM&#27169;&#22411;&#23545;&#34920;&#38754;&#32447;&#32034;&#36807;&#24230;&#20381;&#36182;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#20998;&#26512;&#32534;&#36753;&#36317;&#31163;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35777;&#26126;GLS-CSC&#33021;&#22815;&#25552;&#39640;&#20013;&#25991;STM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20013;&#25991;&#30701;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20381;&#36182;&#20110;&#34920;&#38754;&#32447;&#32034;&#65292;&#23548;&#33268;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20998;&#26512;&#21644;&#20943;&#23569;&#34920;&#38754;&#32447;&#32034;&#23545;STM&#27169;&#22411;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#32534;&#36753;&#36317;&#31163;&#29305;&#24449;&#23545;STM&#27169;&#22411;&#30340;&#36807;&#24230;&#20381;&#36182;&#24773;&#20917;&#65292;&#32534;&#36753;&#36317;&#31163;&#24120;&#34987;&#29992;&#26469;&#34913;&#37327;&#20013;&#25991;&#25991;&#26412;&#23545;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#34920;&#38754;&#32447;&#32034;&#12290;&#20026;&#20102;&#20943;&#23569;STM&#27169;&#22411;&#23545;&#34920;&#38754;&#32447;&#32034;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradually Learn Samples Containing Superficial Clue (GLS-CSC)&#30340;&#26032;&#30340;&#37325;&#37319;&#26679;&#35757;&#32451;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;&#39046;&#22495;&#20869;&#65288;I.D.&#65289;&#12289;&#40065;&#26834;&#24615;&#65288;Rob.&#65289;&#21644;&#39046;&#22495;&#22806;&#65288;O.O.D.&#65289;&#27979;&#35797;&#38598;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GLS-CSC&#22312;&#22686;&#24378;&#20013;&#25991;STM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20849;&#21516;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained models have achieved success in Chinese Short Text Matching (STM) tasks, but they often rely on superficial clues, leading to a lack of robust predictions. To address this issue, it is crucial to analyze and mitigate the influence of superficial clues on STM models. Our study aims to investigate their over-reliance on the edit distance feature, commonly used to measure the semantic similarity of Chinese text pairs, which can be considered a superficial clue. To mitigate STM models' over-reliance on superficial clues, we propose a novel resampling training strategy called Gradually Learn Samples Containing Superficial Clue (GLS-CSC). Through comprehensive evaluations of In-Domain (I.D.), Robustness (Rob.), and Out-Of-Domain (O.O.D.) test sets, we demonstrate that GLS-CSC outperforms existing methods in terms of enhancing the robustness and generalization of Chinese STM models. Moreover, we conduct a detailed analysis of existing methods and reveal their commonality.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#21457;&#35328;&#26465;&#20214;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#35821;&#38899;&#21512;&#25104;&#30340;&#38901;&#24459;&#65292;&#24182;&#30830;&#20445;&#33258;&#28982;&#35821;&#38899;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#36328;&#21457;&#35328;CVAE&#65292;&#36890;&#36807;&#25552;&#21462;&#21608;&#22260;&#21477;&#23376;&#30340;&#22768;&#23398;&#12289;&#35828;&#35805;&#20154;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#29983;&#25104;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#38901;&#24459;&#29305;&#24449;&#65292;&#26377;&#25928;&#27169;&#25311;&#20154;&#31867;&#38901;&#24459;&#29983;&#25104;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#23454;&#29992;&#31639;&#27861;&#65306;CUC-VAE TTS&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;CUC-VAE SE&#29992;&#20110;&#35821;&#38899;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2309.04156</link><description>&lt;p&gt;
&#36328;&#21457;&#35328;&#26465;&#20214;&#21270;VAE&#35821;&#38899;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cross-Utterance Conditioned VAE for Speech Generation. (arXiv:2309.04156v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#21457;&#35328;&#26465;&#20214;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#35821;&#38899;&#21512;&#25104;&#30340;&#38901;&#24459;&#65292;&#24182;&#30830;&#20445;&#33258;&#28982;&#35821;&#38899;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#36328;&#21457;&#35328;CVAE&#65292;&#36890;&#36807;&#25552;&#21462;&#21608;&#22260;&#21477;&#23376;&#30340;&#22768;&#23398;&#12289;&#35828;&#35805;&#20154;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#29983;&#25104;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#38901;&#24459;&#29305;&#24449;&#65292;&#26377;&#25928;&#27169;&#25311;&#20154;&#31867;&#38901;&#24459;&#29983;&#25104;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#23454;&#29992;&#31639;&#27861;&#65306;CUC-VAE TTS&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;CUC-VAE SE&#29992;&#20110;&#35821;&#38899;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#22312;&#22810;&#23186;&#20307;&#21046;&#20316;&#20013;&#26377;&#30528;&#28508;&#21147;&#65292;&#20294;&#24120;&#24120;&#38754;&#20020;&#20135;&#29983;&#26377;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#21644;&#26080;&#32541;&#32534;&#36753;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#21457;&#35328;&#26465;&#20214;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#35821;&#38899;&#21512;&#25104;(CUC-VAE S2)&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#38901;&#24459;&#24182;&#30830;&#20445;&#33258;&#28982;&#35821;&#38899;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#34920;&#29616;&#33021;&#21147;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAEs)&#30340;&#20877;&#34920;&#36798;&#33021;&#21147;&#12290;CUC-VAE S2&#26694;&#26550;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#36328;&#21457;&#35328;CVAE&#65292;&#23427;&#20174;&#21608;&#22260;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#22768;&#23398;&#12289;&#35828;&#35805;&#20154;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#20197;&#29983;&#25104;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#38901;&#24459;&#29305;&#24449;&#65292;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#20154;&#31867;&#38901;&#24459;&#29983;&#25104;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;&#19981;&#21516;&#35821;&#38899;&#21512;&#25104;&#24212;&#29992;&#30340;&#23454;&#29992;&#31639;&#27861;&#65306;CUC-VAE TTS&#20197;&#36827;&#34892;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;CUC-VAE SE&#20197;&#36827;&#34892;&#35821;&#38899;&#32534;&#36753;&#12290;CUC-VAE TTS&#26159;&#35813;&#26694;&#26550;&#30340;&#30452;&#25509;&#24212;&#29992;&#65292;&#20351;&#24471;&#33021;&#22815;&#23558;&#20219;&#24847;&#25991;&#26412;&#36716;&#25104;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, de
&lt;/p&gt;</description></item><item><title>NESTLE&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#24037;&#20855;&#65292;&#29992;&#20110;&#36827;&#34892;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#24182;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.04146</link><description>&lt;p&gt;
NESTLE&#65306;&#19968;&#31181;&#29992;&#20110;&#27861;&#24459;&#35821;&#26009;&#24211;&#32479;&#35745;&#20998;&#26512;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus. (arXiv:2309.04146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04146
&lt;/p&gt;
&lt;p&gt;
NESTLE&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#24037;&#20855;&#65292;&#29992;&#20110;&#36827;&#34892;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#24182;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27861;&#24459;&#35265;&#35299;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#20998;&#26512;&#65292;&#38656;&#35201;&#20351;&#29992;&#25991;&#26723;&#26816;&#32034;&#24037;&#20855;&#36873;&#25321;&#35821;&#26009;&#24211;&#30340;&#23376;&#38598;&#65292;&#20351;&#29992;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#31995;&#32479;&#23545;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#24182;&#23545;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#21270;&#20197;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#12290;&#27599;&#20010;&#36807;&#31243;&#37117;&#38656;&#35201;&#19987;&#19994;&#24037;&#20855;&#25110;&#32534;&#31243;&#25216;&#33021;&#65292;&#28982;&#32780;&#36824;&#27809;&#26377;&#20840;&#38754;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;&#21487;&#29992;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;IE&#65292;&#22914;&#26524;IE&#31995;&#32479;&#30340;&#26412;&#20307;&#20013;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#20449;&#24687;&#65292;&#37027;&#20040;&#38656;&#35201;&#33258;&#24049;&#26500;&#24314;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;NESTLE&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#32479;&#35745;&#20998;&#26512;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;&#12290;&#36890;&#36807;NESTLE&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#36741;&#21161;GUI&#36827;&#34892;&#32454;&#33268;&#32423;&#21035;&#30340;&#25511;&#21046;&#26469;&#21487;&#35270;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;NESTLE&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;IE&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#23558;&#21508;&#20010;&#32452;&#20214;&#36830;&#25509;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The statistical analysis of large scale legal corpus can provide valuable legal insights. For such analysis one needs to (1) select a subset of the corpus using document retrieval tools, (2) structuralize text using information extraction (IE) systems, and (3) visualize the data for the statistical analysis. Each process demands either specialized tools or programming skills whereas no comprehensive unified "no-code" tools have been available. Especially for IE, if the target information is not predefined in the ontology of the IE system, one needs to build their own system. Here we provide NESTLE, a no code tool for large-scale statistical analysis of legal corpus. With NESTLE, users can search target documents, extract information, and visualize the structured data all via the chat interface with accompanying auxiliary GUI for the fine-level control. NESTLE consists of three main components: a search engine, an end-to-end IE system, and a Large Language Model (LLM) that glues the who
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RST&#30340;&#35805;&#35821;&#35299;&#26512;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26723;&#32423;&#20869;&#23481;&#32467;&#26500;&#65292;&#20351;&#29992;&#32467;&#26500;&#24863;&#30693;&#30340;&#26032;&#38395;&#20869;&#23481;&#21477;&#23376;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#22823;&#25991;&#26412;&#33539;&#22260;&#35805;&#35821;&#20851;&#31995;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04141</link><description>&lt;p&gt;
&#30001;&#25991;&#26723;&#32423;&#20869;&#23481;&#32467;&#26500;&#25351;&#23548;&#30340;&#22522;&#20110;RST&#30340;&#35805;&#35821;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
RST-style Discourse Parsing Guided by Document-level Content Structures. (arXiv:2309.04141v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RST&#30340;&#35805;&#35821;&#35299;&#26512;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26723;&#32423;&#20869;&#23481;&#32467;&#26500;&#65292;&#20351;&#29992;&#32467;&#26500;&#24863;&#30693;&#30340;&#26032;&#38395;&#20869;&#23481;&#21477;&#23376;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#22823;&#25991;&#26412;&#33539;&#22260;&#35805;&#35821;&#20851;&#31995;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;(RST)&#22522;&#20110;&#35805;&#35821;&#35299;&#26512;&#25506;&#32034;&#20102;&#20174;&#20174;&#23376;&#21477;&#12289;&#21477;&#23376;&#21040;&#22823;&#25991;&#26412;&#33539;&#22260;&#22914;&#20309;&#32452;&#25104;&#19968;&#20010;&#25972;&#20307;&#35805;&#35821;&#65292;&#24182;&#20197;&#20998;&#23618;&#26641;&#30340;&#24418;&#24335;&#21576;&#29616;&#20462;&#36766;&#32467;&#26500;&#12290;&#29616;&#26377;&#30340;RST&#35299;&#26512;&#27969;&#27700;&#32447;&#22312;&#26500;&#24314;&#20462;&#36766;&#32467;&#26500;&#26102;&#32570;&#20047;&#23545;&#25991;&#26723;&#32423;&#20869;&#23481;&#32467;&#26500;&#30340;&#20102;&#35299;&#65292;&#23548;&#33268;&#22312;&#39044;&#27979;&#22823;&#25991;&#26412;&#33539;&#22260;&#30340;&#35805;&#35821;&#20851;&#31995;&#26102;&#24615;&#33021;&#30456;&#23545;&#36739;&#20302;&#12290;&#22522;&#20110;&#24847;&#35782;&#21040;&#39640;&#32423;&#20869;&#23481;&#30456;&#20851;&#20449;&#24687;&#22312;&#20419;&#36827;&#35805;&#35821;&#20851;&#31995;&#35782;&#21035;&#26041;&#38754;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RST-DP&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#34701;&#20837;&#20102;&#20174;&#26032;&#38395;&#35805;&#35821;&#24314;&#27169;&#20219;&#21153;&#20013;&#24471;&#20986;&#30340;&#32467;&#26500;&#24863;&#30693;&#30340;&#26032;&#38395;&#20869;&#23481;&#21477;&#23376;&#34920;&#31034;&#12290;&#36890;&#36807;&#20165;&#22686;&#21152;&#20960;&#20010;&#39069;&#22806;&#30340;&#23618;&#27425;&#65292;&#36825;&#20010;&#22686;&#24378;&#30340;&#27969;&#27700;&#32447;&#22312;&#21508;&#31181;RST&#35299;&#26512;&#25351;&#26631;&#19978;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rhetorical Structure Theory based Discourse Parsing (RST-DP) explores how clauses, sentences, and large text spans compose a whole discourse and presents the rhetorical structure as a hierarchical tree. Existing RST parsing pipelines construct rhetorical structures without the knowledge of document-level content structures, which causes relatively low performance when predicting the discourse relations for large text spans. Recognizing the value of high-level content-related information in facilitating discourse relation recognition, we propose a novel pipeline for RST-DP that incorporates structure-aware news content sentence representations derived from the task of News Discourse Profiling. By incorporating only a few additional layers, this enhanced pipeline exhibits promising performance across various RST parsing metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#30340;&#22343;&#22330;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20551;&#35774;&#31361;&#35302;&#26435;&#37325;&#36981;&#24490;&#33033;&#20914;&#21644;&#26001;&#28857;&#20998;&#24067;&#24182;&#21482;&#23545;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.04106</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#20803;&#39044;&#27979;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Meta predictive learning model of natural languages. (arXiv:2309.04106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#30340;&#22343;&#22330;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20551;&#35774;&#31361;&#35302;&#26435;&#37325;&#36981;&#24490;&#33033;&#20914;&#21644;&#26001;&#28857;&#20998;&#24067;&#24182;&#21482;&#23545;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#26412;&#36523;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#34920;&#29616;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#19981;&#21516;&#24615;&#36136;&#30340;&#20219;&#21153;&#20013;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35821;&#35328;&#22788;&#29702;&#65292;&#25105;&#20204;&#30340;&#20154;&#33041;&#21487;&#33021;&#19981;&#26159;&#25353;&#29031;&#21516;&#26679;&#30340;&#21407;&#29702;&#36816;&#20316;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37319;&#29992;&#30340;&#20154;&#24037;&#33258;&#25105;&#30417;&#30563;&#19982;&#33041;&#35745;&#31639;&#20043;&#38388;&#30340;&#32852;&#31995;&#24341;&#36215;&#20102;&#19968;&#22330;&#36777;&#35770;&#12290;&#22312;&#33041;&#35745;&#31639;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#20551;&#35774;&#20043;&#19968;&#26159;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25552;&#20986;&#36890;&#36807;&#23616;&#37096;&#23398;&#20064;&#26469;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#32534;&#30721;&#21644;&#30456;&#20851;&#30340;&#23398;&#20998;&#20998;&#37197;&#22312;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20316;&#29992;&#20173;&#28982;&#26410;&#30693;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#30340;&#22343;&#22330;&#23398;&#20064;&#27169;&#22411;&#65292;&#20551;&#35774;&#27599;&#20010;&#36830;&#25509;&#30340;&#31361;&#35302;&#26435;&#37325;&#36981;&#24490;&#33033;&#20914;&#21644;&#26001;&#28857;&#20998;&#24067;&#65292;&#21482;&#23545;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#20803;&#39044;&#27979;&#23398;&#20064;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#19978;&#24471;&#21040;&#20102;&#25104;&#21151;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models based on self-attention mechanisms have achieved astonishing performances not only in natural language itself, but also in a variety of tasks of different nature. However, regarding processing language, our human brain may not operate using the same principle. Then, a debate is established on the connection between brain computation and artificial self-supervision adopted in large language models. One of most influential hypothesis in brain computation is the predictive coding framework, which proposes to minimize the prediction error by local learning. However, the role of predictive coding and the associated credit assignment in language processing remains unknown. Here, we propose a mean-field learning model within the predictive coding framework, assuming that the synaptic weight of each connection follows a spike and slab distribution, and only the distribution is trained. This meta predictive learning is successfully validated on classifying handwritten digi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25972;&#20307;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#22810;&#25991;&#26723;&#25552;&#21462;&#24335;&#32508;&#36848;&#12290;&#36890;&#36807;&#23558;&#25972;&#20307;&#27874;&#26463;&#25628;&#32034;&#25512;&#29702;&#26041;&#27861;&#19982;&#21517;&#20026;SRI&#30340;&#25972;&#20307;&#24230;&#37327;&#30456;&#32467;&#21512;&#65292;&#24179;&#34913;&#20102;&#21516;&#19968;&#20027;&#39064;&#30340;&#25991;&#26723;&#20013;&#23376;&#38598;&#21477;&#23376;&#30340;&#37325;&#35201;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04087</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22810;&#25991;&#26723;&#32508;&#36848;&#30340;&#25972;&#20307;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Multi-document Summarization with Holistic Inference. (arXiv:2309.04087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25972;&#20307;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#22810;&#25991;&#26723;&#25552;&#21462;&#24335;&#32508;&#36848;&#12290;&#36890;&#36807;&#23558;&#25972;&#20307;&#27874;&#26463;&#25628;&#32034;&#25512;&#29702;&#26041;&#27861;&#19982;&#21517;&#20026;SRI&#30340;&#25972;&#20307;&#24230;&#37327;&#30456;&#32467;&#21512;&#65292;&#24179;&#34913;&#20102;&#21516;&#19968;&#20027;&#39064;&#30340;&#25991;&#26723;&#20013;&#23376;&#38598;&#21477;&#23376;&#30340;&#37325;&#35201;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#32508;&#36848;&#26088;&#22312;&#20174;&#19968;&#32452;&#20851;&#20110;&#21516;&#19968;&#20027;&#39064;&#30340;&#25991;&#26723;&#20013;&#33719;&#21462;&#26680;&#24515;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25972;&#20307;&#26694;&#26550;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#22810;&#25991;&#26723;&#25552;&#21462;&#24335;&#32508;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25972;&#20307;&#27874;&#26463;&#25628;&#32034;&#25512;&#29702;&#26041;&#27861;&#19982;&#21517;&#20026;&#23376;&#38598;&#20195;&#34920;&#25351;&#26631;&#65288;SRI&#65289;&#30340;&#25972;&#20307;&#24230;&#37327;&#30456;&#32467;&#21512;&#12290;SRI&#24179;&#34913;&#20102;&#26469;&#33258;&#28304;&#25991;&#26723;&#30340;&#21477;&#23376;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#21487;&#22312;&#26080;&#30417;&#30563;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#19979;&#35745;&#31639;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#22810;&#25991;&#26723;&#32508;&#36848;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#26080;&#30417;&#30563;&#21644;&#33258;&#36866;&#24212;&#35774;&#32622;&#12290;&#26681;&#25454;&#32467;&#26524;&#30340;ROUGE&#35780;&#20998;&#21644;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#32463;&#36807;&#26174;&#33879;&#30340;&#24046;&#36317;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#22810;&#26679;&#24615;&#23545;&#25552;&#39640;&#22810;&#25991;&#26723;&#32508;&#36848;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-document summarization aims to obtain core information from a collection of documents written on the same topic. This paper proposes a new holistic framework for unsupervised multi-document extractive summarization. Our method incorporates the holistic beam search inference method associated with the holistic measurements, named Subset Representative Index (SRI). SRI balances the importance and diversity of a subset of sentences from the source documents and can be calculated in unsupervised and adaptive manners. To demonstrate the effectiveness of our method, we conduct extensive experiments on both small and large-scale multi-document summarization datasets under both unsupervised and adaptive settings. The proposed method outperforms strong baselines by a significant margin, as indicated by the resulting ROUGE scores and diversity measures. Our findings also suggest that diversity is essential for improving multi-document summary performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#22833;&#35782;&#30151;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#32531;&#35299;&#30340;&#26694;&#26550;EMMA&#12290;&#36890;&#36807;&#31867;&#27604;&#31070;&#32463;&#24515;&#29702;&#23398;&#20013;&#30340;&#22833;&#35782;&#30151;&#29616;&#35937;&#65292;&#23450;&#20041;&#20102;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#21644;&#27835;&#30103;&#26041;&#27861;&#12290;&#35780;&#20272;&#27169;&#22359;&#36890;&#36807;&#21019;&#24314;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#38382;&#31572;&#31034;&#20363;&#26469;&#35780;&#20272;&#22833;&#35782;&#30151;&#31243;&#24230;&#65292;&#27835;&#30103;&#27169;&#22359;&#21017;&#37319;&#29992;&#20462;&#27491;&#21644;&#22686;&#24378;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#21644;&#32416;&#27491;&#22833;&#35782;&#30151;&#12290;</title><link>http://arxiv.org/abs/2309.04041</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#32531;&#35299;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22833;&#35782;&#30151;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Mitigation of Agnosia in Multimodal Large Language Models. (arXiv:2309.04041v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#22833;&#35782;&#30151;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#32531;&#35299;&#30340;&#26694;&#26550;EMMA&#12290;&#36890;&#36807;&#31867;&#27604;&#31070;&#32463;&#24515;&#29702;&#23398;&#20013;&#30340;&#22833;&#35782;&#30151;&#29616;&#35937;&#65292;&#23450;&#20041;&#20102;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#21644;&#27835;&#30103;&#26041;&#27861;&#12290;&#35780;&#20272;&#27169;&#22359;&#36890;&#36807;&#21019;&#24314;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#38382;&#31572;&#31034;&#20363;&#26469;&#35780;&#20272;&#22833;&#35782;&#30151;&#31243;&#24230;&#65292;&#27835;&#30103;&#27169;&#22359;&#21017;&#37319;&#29992;&#20462;&#27491;&#21644;&#22686;&#24378;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#21644;&#32416;&#27491;&#22833;&#35782;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#20294;&#35266;&#23519;&#21040;&#23427;&#20204;&#26377;&#26102;&#20250;&#35823;&#35299;&#35270;&#35273;&#36755;&#20837;&#65292;&#29978;&#33267;&#22312;&#31616;&#21333;&#24773;&#20917;&#19979;&#26410;&#33021;&#36981;&#24490;&#25991;&#26412;&#25351;&#20196;&#65292;&#23548;&#33268;&#26080;&#20851;&#30340;&#22238;&#22797;&#12289;&#38169;&#35823;&#21644;&#26080;&#26681;&#25454;&#30340;&#20027;&#24352;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#35266;&#23519;&#31867;&#27604;&#20110;&#31070;&#32463;&#24515;&#29702;&#23398;&#20013;&#30340;&#19968;&#31181;&#29616;&#35937;&#65292;&#21363;&#22833;&#35782;&#30151;&#65292;&#21363;&#26080;&#27861;&#27491;&#30830;&#22788;&#29702;&#24863;&#35273;&#27169;&#24577;&#21644;&#35748;&#35782;&#20107;&#29289;&#65288;&#20363;&#22914;&#65292;&#23545;&#35937;&#12289;&#39068;&#33394;&#12289;&#20851;&#31995;&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#36825;&#19968;&#31867;&#20284;&#30340;&#27010;&#24565;&#26469;&#23450;&#20041;&#8220;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#8221;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20840;&#38754;&#35780;&#20272;&#21644;&#32531;&#35299;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#12290;&#21463;&#21040;&#31070;&#32463;&#24515;&#29702;&#23398;&#20013;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;EMMA&#65288;&#35780;&#20272;&#21644;&#32531;&#35299;&#22810;&#27169;&#24577;&#22833;&#35782;&#30151;&#65289;&#12290;&#22312;EMMA&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#27169;&#22359;&#65292;&#29992;&#20110;&#33258;&#21160;&#21019;&#24314;&#32454;&#31890;&#24230;&#21644;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#38382;&#31572;&#31034;&#20363;&#65292;&#20840;&#38754;&#35780;&#20272;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#31243;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27835;&#30103;&#27169;&#22359;&#65292;&#20351;&#29992;&#20462;&#27491;&#21644;&#22686;&#24378;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#21644;&#32416;&#27491;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Multimodal Large Language Models (MLLMs) are widely used for a variety of vision-language tasks, one observation is that they sometimes misinterpret visual inputs or fail to follow textual instructions even in straightforward cases, leading to irrelevant responses, mistakes, and ungrounded claims. This observation is analogous to a phenomenon in neuropsychology known as Agnosia, an inability to correctly process sensory modalities and recognize things (e.g., objects, colors, relations). In our study, we adapt this similar concept to define "agnosia in MLLMs", and our goal is to comprehensively evaluate and mitigate such agnosia in MLLMs. Inspired by the diagnosis and treatment process in neuropsychology, we propose a novel framework EMMA (Evaluation and Mitigation of Multimodal Agnosia). In EMMA, we develop an evaluation module that automatically creates fine-grained and diverse visual question answering examples to assess the extent of agnosia in MLLMs comprehensively. We also d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20010;&#34920;&#31034;&#36716;&#31227;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#27492;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04031</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21040;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#30340;&#22810;&#37325;&#34920;&#31034;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems. (arXiv:2309.04031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20010;&#34920;&#31034;&#36716;&#31227;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#27492;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#26159;&#23558;&#35821;&#35328;&#30693;&#35782;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#21482;&#33021;&#36716;&#31227;LLM&#30340;&#21333;&#20010;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#39044;&#35757;&#32451;BERT&#30340;&#26368;&#21518;&#19968;&#23618;&#65289;&#65292;&#32780;&#25991;&#26412;&#30340;&#34920;&#31034;&#22312;&#26412;&#36136;&#19978;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#23618;&#12289;&#19978;&#19979;&#25991;&#21644;&#27169;&#22411;&#20197;&#21508;&#31181;&#26041;&#24335;&#33719;&#24471;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#33719;&#24471;&#21644;&#36716;&#31227;LLMs&#30340;&#22810;&#20010;&#34920;&#31034;&#21040;&#22522;&#20110;&#20256;&#23548;&#22120;&#30340;ASR&#31995;&#32479;&#20013;&#12290;&#23613;&#31649;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;LLMs&#30340;&#22810;&#20010;&#34920;&#31034;&#36716;&#31227;&#21487;&#20197;&#26159;&#36716;&#31227;&#21333;&#20010;&#34920;&#31034;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring the knowledge of large language models (LLMs) is a promising technique to incorporate linguistic knowledge into end-to-end automatic speech recognition (ASR) systems. However, existing works only transfer a single representation of LLM (e.g. the last layer of pretrained BERT), while the representation of a text is inherently non-unique and can be obtained variously from different layers, contexts and models. In this work, we explore a wide range of techniques to obtain and transfer multiple representations of LLMs into a transducer-based ASR system. While being conceptually simple, we show that transferring multiple representations of LLMs can be an effective alternative to transferring only a single representation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;TIDE&#65288;Textual Identity Detection&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#36523;&#20221;&#35789;&#27719;&#21644;&#35821;&#22659;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#24320;&#21457;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04027</link><description>&lt;p&gt;
TIDE: &#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36523;&#20221;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models. (arXiv:2309.04027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TIDE&#65288;Textual Identity Detection&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#36523;&#20221;&#35789;&#27719;&#21644;&#35821;&#22659;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#24320;&#21457;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#19981;&#20844;&#27491;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#24847;&#22806;&#20559;&#35265;&#12290;&#22312;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#65292;&#35780;&#20272;&#21644;&#21435;&#20559;&#36825;&#20123;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#23588;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#24615;&#21462;&#21521;&#31561;&#25935;&#24863;&#23646;&#24615;&#21487;&#33021;&#19981;&#21487;&#29992;&#12290;&#24403;&#36825;&#20123;&#27169;&#22411;&#25237;&#25918;&#21040;&#31038;&#20250;&#20013;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#23545;&#21382;&#21490;&#19978;&#24369;&#21183;&#32676;&#20307;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#36523;&#20221;&#35789;&#27719;&#34920;TIDAL&#65292;&#21253;&#25324;15,123&#20010;&#36523;&#20221;&#26415;&#35821;&#21644;&#30456;&#20851;&#30340;&#35821;&#22659;&#65292;&#28085;&#30422;&#20102;&#19977;&#20010;&#20154;&#21475;&#32479;&#35745;&#31867;&#21035;&#12290;&#25105;&#20204;&#21033;&#29992;TIDAL&#24320;&#21457;&#20102;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#36523;&#20221;&#35821;&#22659;&#30340;&#21487;&#29992;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#36129;&#29486;&#32773;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21435;&#20559;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. Evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. When these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. In this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. We create a new, more comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ML fairness techniques. We evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. Resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#26681;&#25454;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#29983;&#25104;&#19982;Gene Ontology&#20013;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#21516;&#26102;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24471;&#21040;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#22522;&#26412;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.04019</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of large language models for discovery of gene set function. (arXiv:2309.04019v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#26681;&#25454;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#29983;&#25104;&#19982;Gene Ontology&#20013;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#21516;&#26102;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24471;&#21040;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#22522;&#26412;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#38598;&#21512;&#20998;&#26512;&#26159;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#20294;&#23427;&#20381;&#36182;&#20110;&#25163;&#21160;&#21019;&#24314;&#30340;&#22522;&#22240;&#21151;&#33021;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#25968;&#25454;&#24211;&#30340;&#19981;&#23436;&#25972;&#21644;&#19981;&#20855;&#22791;&#29983;&#29289;&#23398;&#19978;&#19979;&#25991;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20854;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#20013;&#21457;&#23637;&#20986;&#26377;&#20851;&#24120;&#35265;&#22522;&#22240;&#21151;&#33021;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;GPT-4&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#29992;&#24635;&#32467;&#20854;&#20849;&#35782;&#21151;&#33021;&#30340;&#21517;&#31216;&#26631;&#35760;&#22522;&#22240;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#25991;&#26412;&#21644;&#24341;&#25991;&#36827;&#34892;&#35777;&#23454;&#12290;&#22312;&#19982;Gene Ontology&#20013;&#30340;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;GPT-4&#22312;50%&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20102;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#21017;&#24674;&#22797;&#20102;&#26356;&#19968;&#33324;&#27010;&#24565;&#30340;&#21517;&#31216;&#12290;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;&#19982;&#22522;&#22240;&#38598;&#21512;&#23500;&#38598;&#30456;&#27604;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#20854;&#25903;&#25345;&#24615;&#38472;&#36848;&#21644;&#24341;&#25991;&#22312;&#20154;&#24037;&#23457;&#26680;&#20013;&#24471;&#21040;&#20102;&#22522;&#26412;&#39564;&#35777;&#12290;&#24555;&#36895;&#32508;&#21512;&#24120;&#35265;&#22522;&#22240;&#21151;&#33021;&#30340;&#33021;&#21147;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gene set analysis is a mainstay of functional genomics, but it relies on manually curated databases of gene functions that are incomplete and unaware of biological context. Here we evaluate the ability of OpenAI's GPT-4, a Large Language Model (LLM), to develop hypotheses about common gene functions from its embedded biomedical knowledge. We created a GPT-4 pipeline to label gene sets with names that summarize their consensus functions, substantiated by analysis text and citations. Benchmarking against named gene sets in the Gene Ontology, GPT-4 generated very similar names in 50% of cases, while in most remaining cases it recovered the name of a more general concept. In gene sets discovered in 'omics data, GPT-4 names were more informative than gene set enrichment, with supporting statements and citations that largely verified in human review. The ability to rapidly synthesize common gene functions positions LLMs as valuable functional genomics assistants.
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.03992</link><description>&lt;p&gt;
ConDA: &#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03992
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#21508;&#31181;&#29992;&#36884;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#26032;&#38395;&#25253;&#36947;&#12290;&#37492;&#20110;&#36825;&#20123;LLMs&#21487;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#26469;&#22823;&#35268;&#27169;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#26032;&#30340;LLMs&#19981;&#26029;&#34987;&#24320;&#21457;&#65292;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#24335;&#26816;&#27979;&#22120;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#27809;&#26377;&#20851;&#20110;&#20854;&#29983;&#25104;&#22120;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27492;&#25968;&#25454;&#38382;&#39064;&#65292;&#21363;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#65292;&#24182;&#23558;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#20219;&#21153;&#12290;&#36825;&#37324;&#30340;&#22495;&#26159;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#22120;&#65292;&#21363;LLMs&#65292;&#25105;&#20204;&#20551;&#35774;&#21482;&#33021;&#35775;&#38382;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;ConDA&#30340;&#23545;&#27604;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#26631;&#20934;&#30340;&#22495;&#36866;&#24212;&#25216;&#26415;&#19982;&#34920;&#31034;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power 
&lt;/p&gt;</description></item><item><title>LanSER&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#24369;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26631;&#20934;SER&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#27169;&#25311;&#35821;&#38899;&#30340;&#38901;&#24459;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2309.03978</link><description>&lt;p&gt;
LanSER: &#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LanSER: Language-Model Supported Speech Emotion Recognition. (arXiv:2309.03978v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03978
&lt;/p&gt;
&lt;p&gt;
LanSER&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#24369;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26631;&#20934;SER&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#27169;&#25311;&#35821;&#38899;&#30340;&#38901;&#24459;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#65288;SER&#65289;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;&#23545;&#22823;&#22411;&#35821;&#38899;&#25968;&#25454;&#38598;&#21644;&#24494;&#22937;&#24773;&#32490;&#20998;&#31867;&#30340;&#25193;&#23637;&#26041;&#27861;&#22256;&#38590;&#37325;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LanSER&#65292;&#19968;&#31181;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24369;&#30417;&#30563;&#23398;&#20064;&#26469;&#25512;&#27979;&#24369;&#24773;&#32490;&#26631;&#31614;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;&#23545;&#20110;&#21463;&#21040;&#20998;&#31867;&#32422;&#26463;&#30340;&#24369;&#26631;&#31614;&#25512;&#27979;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#25991;&#26412;&#34164;&#28085;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25552;&#21462;&#30340;&#35821;&#38899;&#36716;&#24405;&#20013;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26368;&#39640;&#34164;&#28085;&#24471;&#20998;&#30340;&#24773;&#32490;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#24369;&#30417;&#30563;&#35757;&#32451;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26631;&#20934;SER&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#20986;&#25913;&#36827;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;&#23613;&#31649;&#21482;&#22312;&#25991;&#26412;&#19978;&#25512;&#27979;&#20986;&#30340;&#26631;&#31614;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#24471;&#21040;&#30340;&#34920;&#31034;&#20284;&#20046;&#33021;&#22815;&#27169;&#25311;&#35821;&#38899;&#30340;&#38901;&#24459;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition (SER) models typically rely on costly human-labeled data for training, making scaling methods to large speech datasets and nuanced emotion taxonomies difficult. We present LanSER, a method that enables the use of unlabeled data by inferring weak emotion labels via pre-trained large language models through weakly-supervised learning. For inferring weak labels constrained to a taxonomy, we use a textual entailment approach that selects an emotion label with the highest entailment score for a speech transcript extracted via automatic speech recognition. Our experimental results show that models pre-trained on large datasets with this weak supervision outperform other baseline models on standard SER datasets when fine-tuned, and show improved label efficiency. Despite being pre-trained on labels derived only from text, we show that the resulting representations appear to model the prosodic content of speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#12290;&#30740;&#31350;&#25351;&#20986;&#36825;&#19968;&#20559;&#24046;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PriDe&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03882</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Large Language Models' Selection Bias in Multi-Choice Questions. (arXiv:2309.03882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#12290;&#30740;&#31350;&#25351;&#20986;&#36825;&#19968;&#20559;&#24046;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PriDe&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQs&#65289;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30740;&#31350;&#20013;&#24120;&#35265;&#19988;&#37325;&#35201;&#30340;&#20219;&#21153;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;LLMs&#22312;MCQs&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#8220;&#36873;&#25321;&#20559;&#24046;&#8221;&#65292;&#21363;LLMs&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#65288;&#22914;&#8220;&#36873;&#39033;C&#8221;&#65289;&#12290;&#36825;&#31181;&#20559;&#24046;&#22312;&#21508;&#31181;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;MCQs&#20013;&#23545;&#36873;&#39033;&#20301;&#32622;&#21464;&#21270;&#30340;&#24615;&#33021;&#21464;&#24471;&#33030;&#24369;&#12290;&#25105;&#20204;&#21457;&#29616;&#23548;&#33268;&#36873;&#25321;&#20559;&#24046;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#21363;&#19982;&#36873;&#39033;&#30456;&#20851;&#30340;ID&#31526;&#21495;A/B/C/D&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#31216;&#20026;PriDe&#12290;PriDe&#39318;&#20808;&#23558;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#20998;&#35299;&#20026;&#23545;&#36873;&#39033;&#20869;&#23481;&#30340;&#20869;&#22312;&#39044;&#27979;&#21644;&#23545;&#36873;&#39033;ID&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#22312;&#23569;&#37327;&#27979;&#35797;&#26679;&#26412;&#19978;&#23545;&#36873;&#39033;&#20869;&#23481;&#36827;&#34892;&#25490;&#21015;&#32452;&#21512;&#26469;&#20272;&#35745;&#20808;&#39564;&#65292;&#20174;&#32780;&#29992;&#20110;&#28040;&#38500;&#21518;&#32493;&#27979;&#35797;&#26679;&#26412;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20316;&#20026;&#19968;&#31181;&#26080;&#26631;&#31614;&#12289;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;PriDe&#21487;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#19988;&#31283;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-choice questions (MCQs) serve as a common yet important task format in the research of large language models (LLMs). Our work shows that LLMs exhibit an inherent "selection bias" in MCQs, which refers to LLMs' preferences to select options located at specific positions (like "Option C"). This bias is prevalent across various LLMs, making their performance vulnerable to option position changes in MCQs. We identify that one primary cause resulting in selection bias is option numbering, i.e., the ID symbols A/B/C/D associated with the options. To mitigate selection bias, we propose a new method called PriDe. PriDe first decomposes the observed model prediction distribution into an intrinsic prediction over option contents and a prior distribution over option IDs. It then estimates the prior by permutating option contents on a small number of test samples, which is used to debias the subsequent test samples. We demonstrate that, as a label-free, inference-time method, PriDe achieves 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20174;&#25968;&#23398;&#38382;&#39064;&#20013;&#29983;&#25104;Prolog&#35859;&#35789;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;Prolog&#20195;&#30721;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03667</link><description>&lt;p&gt;
&#20174;&#25968;&#23398;&#38382;&#39064;&#20013;&#29983;&#25104;Prolog&#35859;&#35789;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring an LM to generate Prolog Predicates from Mathematics Questions. (arXiv:2309.03667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03667
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20174;&#25968;&#23398;&#38382;&#39064;&#20013;&#29983;&#25104;Prolog&#35859;&#35789;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;Prolog&#20195;&#30721;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;ChatGPT&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20852;&#36259;&#24613;&#21095;&#22686;&#21152;&#12290;ChatGPT&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20219;&#21153;&#19978;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#25512;&#29702;&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#36890;&#24120;&#34920;&#29616;&#36739;&#24046;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#36890;&#36807;&#24605;&#32500;&#38142;&#28608;&#21169;&#22312;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#29616;&#22312;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#26159;&#21542;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#26469;&#29983;&#25104;&#36923;&#36753;&#35821;&#35328;&#65288;Prolog&#65289;&#20195;&#30721;&#65292;&#24182;&#23558;&#36825;&#20123;&#20195;&#30721;&#20256;&#36882;&#32473;&#32534;&#35793;&#22120;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#24605;&#32500;&#38142;&#26469;&#24494;&#35843;LLaMA7B&#20316;&#20026;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20854;&#20182;&#29992;&#20110;&#29983;&#25104;Prolog&#20195;&#30721;&#12289;Prolog&#20195;&#30721;+&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#38142;+Prolog&#20195;&#30721;&#30340;&#24494;&#35843;LLaMA7B&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Prolog&#29983;&#25104;&#27169;&#22411;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#36824;&#20381;&#28982;&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a surge in interest in NLP driven by ChatGPT. ChatGPT, a transformer-based generative language model of substantial scale, exhibits versatility in performing various tasks based on natural language. Nevertheless, large language models often exhibit poor performance in solving mathematics questions that require reasoning. Prior research has demonstrated the effectiveness of chain-of-thought prompting in enhancing reasoning capabilities. Now, we aim to investigate whether fine-tuning a model for the generation of Prolog codes, a logic language, and subsequently passing these codes to a compiler can further improve accuracy. Consequently, we employ chain-of-thought to fine-tune LLaMA7B as a baseline model and develop other fine-tuned LLaMA7B models for the generation of Prolog code, Prolog code + chain-of-thought, and chain-of-thought + Prolog code, respectively. The results reveal that the Prolog generation model surpasses the baseline in performance, while the c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;One-to-All&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#35805;&#35821;&#19982;&#25152;&#26377;&#26631;&#31614;&#20505;&#36873;&#39033;&#26469;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#35821;&#20041;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.03563</link><description>&lt;p&gt;
&#20840;&#37096;&#26631;&#31614;&#22312;&#19968;&#36215;&#65306;&#22522;&#20110;&#39640;&#25928;&#30340;&#26631;&#31614;&#35821;&#20041;&#32534;&#30721;&#33539;&#24335;&#30340;&#20302;&#36164;&#28304;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
All Labels Together: Low-shot Intent Detection with an Efficient Label Semantic Encoding Paradigm. (arXiv:2309.03563v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03563
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;One-to-All&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#35805;&#35821;&#19982;&#25152;&#26377;&#26631;&#31614;&#20505;&#36873;&#39033;&#26469;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#35821;&#20041;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#24847;&#22270;&#26631;&#31614;&#30340;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20449;&#24687;&#23545;&#20110;&#23569;&#26679;&#26412;&#22330;&#26223;&#21487;&#33021;&#29305;&#21035;&#26377;&#30410;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#20102;&#24847;&#22270;&#26631;&#31614;&#65292;&#65288;&#20363;&#22914;&#23558;&#24847;&#22270;&#35270;&#20026;&#32034;&#24341;&#65289;&#65292;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65288;&#20363;&#22914;&#20165;&#20351;&#29992;&#37096;&#20998;&#24847;&#22270;&#26631;&#31614;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;One-to-All&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#35805;&#35821;&#19982;&#25152;&#26377;&#26631;&#31614;&#20505;&#36873;&#39033;&#36827;&#34892;&#27604;&#36739;&#12290;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#35821;&#20041;&#12290;&#22312;&#19977;&#20010;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#35757;&#32451;&#36164;&#28304;&#26497;&#20026;&#26377;&#38480;&#26102;&#65292;One-to-All&#29305;&#21035;&#26377;&#25928;&#65292;&#22312;1-shot&#12289;3-shot&#21644;5-shot&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#20102;&#20174;&#37322;&#20041;&#24471;&#21040;&#30340;&#38388;&#25509;&#30417;&#30563;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#23545;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#30340;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20301;&#20110;https://github.com/jian
&lt;/p&gt;
&lt;p&gt;
In intent detection tasks, leveraging meaningful semantic information from intent labels can be particularly beneficial for few-shot scenarios. However, existing few-shot intent detection methods either ignore the intent labels, (e.g. treating intents as indices) or do not fully utilize this information (e.g. only using part of the intent labels). In this work, we present an end-to-end One-to-All system that enables the comparison of an input utterance with all label candidates. The system can then fully utilize label semantics in this way. Experiments on three few-shot intent detection tasks demonstrate that One-to-All is especially effective when the training resource is extremely scarce, achieving state-of-the-art performance in 1-, 3- and 5-shot settings. Moreover, we present a novel pretraining strategy for our model that utilizes indirect supervision from paraphrasing, enabling zero-shot cross-domain generalization on intent detection tasks. Our code is at https://github.com/jian
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#26469;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22312;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36824;&#26080;&#27861;&#35299;&#20915;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#21407;&#22240;&#21487;&#33021;&#26159;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;</title><link>http://arxiv.org/abs/2309.00359</link><description>&lt;p&gt;
&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#29992;&#20110;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#26469;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22312;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36824;&#26080;&#27861;&#35299;&#20915;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#21407;&#22240;&#21487;&#33021;&#26159;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39321;&#20892;&#22312;&#24341;&#20837;&#20449;&#24687;&#29702;&#35770;&#30340;&#32463;&#20856;&#35770;&#25991;&#20013;&#23558;&#36890;&#20449;&#20998;&#20026;&#19977;&#20010;&#23618;&#27425;&#65306;&#25216;&#26415;&#23618;&#12289;&#35821;&#20041;&#23618;&#21644;&#25928;&#26524;&#23618;&#12290;&#25216;&#26415;&#23618;&#20851;&#27880;&#30340;&#26159;&#20934;&#30830;&#37325;&#26500;&#20256;&#36755;&#30340;&#31526;&#21495;&#65292;&#32780;&#35821;&#20041;&#23618;&#21644;&#25928;&#26524;&#23618;&#21017;&#28041;&#21450;&#25512;&#26029;&#20986;&#30340;&#24847;&#20041;&#21450;&#20854;&#23545;&#25509;&#25910;&#32773;&#30340;&#24433;&#21709;&#12290;&#24471;&#30410;&#20110;&#30005;&#20449;&#25216;&#26415;&#65292;&#31532;&#19968;&#23618;&#38382;&#39064;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#22823;&#30340;&#36827;&#27493;&#65292;&#22914;&#20114;&#32852;&#32593;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31532;&#20108;&#20010;&#30446;&#26631;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31532;&#19977;&#23618;&#20173;&#28982;&#22522;&#26412;&#19978;&#26410;&#34987;&#35302;&#21450;&#12290;&#31532;&#19977;&#20010;&#38382;&#39064;&#28041;&#21450;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#25509;&#25910;&#32773;&#34892;&#20026;&#12290;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#20043;&#19968;&#21487;&#33021;&#26159;LLM&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;&#34892;&#20026;&#26631;&#35760;&#23450;&#20041;&#20102;&#22312;&#19968;&#27425;&#36890;&#20449;&#20013;&#30340;&#25509;&#25910;&#32773;&#34892;&#20026;&#65292;&#22914;&#20998;&#20139;&#12289;&#28857;&#36190;&#12289;&#28857;&#20987;&#12289;&#36141;&#20080;&#12289;&#36716;&#25512;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shannon, in his seminal paper introducing information theory, divided the communication into three levels: technical, semantic, and effectivenss. While the technical level is concerned with accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Thanks to telecommunications, the first level problem has produced great advances like the internet. Large Language Models (LLMs) make some progress towards the second goal, but the third level still remains largely untouched. The third problem deals with predicting and optimizing communication for desired receiver behavior. LLMs, while showing wide generalization capabilities across a wide range of tasks, are unable to solve for this. One reason for the underperformance could be a lack of "behavior tokens" in LLMs' training corpora. Behavior tokens define receiver behavior over a communication, such as shares, likes, clicks, purchases, retweets, etc. W
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#35780;&#20272;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#26032;&#24314;&#31435;&#30340;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;DSTC11 Track 4&#20013;&#30340;&#31283;&#20581;&#21644;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;</title><link>http://arxiv.org/abs/2308.16797</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;LLM&#25552;&#31034;&#26159;&#31283;&#20581;&#19988;&#22810;&#35821;&#35328;&#23545;&#35805;&#35780;&#20215;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation. (arXiv:2308.16797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#35780;&#20272;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#26032;&#24314;&#31435;&#30340;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;DSTC11 Track 4&#20013;&#30340;&#31283;&#20581;&#21644;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#33258;&#21160;&#23545;&#35805;&#35780;&#20215;&#25351;&#26631;&#30340;&#24320;&#21457;&#19978;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#20294;&#23545;&#35780;&#20215;&#38750;&#33521;&#35821;&#23545;&#35805;&#30340;&#24605;&#32771;&#21364;&#24456;&#23569;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30830;&#20445;&#25351;&#26631;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#22238;&#31572;&#19981;&#21464;&#20063;&#26159;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#31283;&#20581;&#24615;&#21644;&#22810;&#35821;&#35328;&#24615;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#30340;&#26399;&#26395;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24403;&#21069;&#35780;&#20272;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#26032;&#24314;&#31435;&#30340;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33539;&#24335;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#24179;&#22343;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#24471;&#20998;&#21019;&#36896;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;DSTC11 Track 4&#8220;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#8221;&#20013;&#30340;&#31283;&#20581;&#21644;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#35777;&#26126;&#20102;&#25552;&#31034;LLM&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 "Automatic Evaluation Metrics for Open-Domain Dialogue Systems", proving the evaluation capabilities of prompted LLMs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15363</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#25991;&#26412;&#21040;SQL&#30340;&#30740;&#31350;&#65306;&#19968;&#20010;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#22522;&#20934;&#38459;&#30861;&#20102;&#35774;&#35745;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#21644;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;&#38382;&#39064;&#34920;&#31034;&#12289;&#31034;&#20363;&#36873;&#25321;&#21644;&#31034;&#20363;&#32452;&#32455;&#65292;&#24182;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#35814;&#32454;&#38416;&#36848;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#65292;&#36798;&#21040;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#26438;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#65292;&#24182;&#22312;&#27492;&#24230;&#37327;&#19979;&#27604;&#36739;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24320;&#28304;LLMs&#65292;&#24182;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#30417;&#30563;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#25351;&#26631;&#65288;IFD&#65289;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12032</link><description>&lt;p&gt;
&#20174;&#25968;&#37327;&#21040;&#36136;&#37327;&#65306;&#21033;&#29992;&#33258;&#25105;&#24341;&#23548;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#25552;&#21319;LLM&#24615;&#33021;&#20197;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#25351;&#26631;&#65288;IFD&#65289;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#65292;&#25351;&#20196;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#24050;&#25104;&#20026;&#19968;&#20010;&#28966;&#28857;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#35782;&#21035;&#21644;&#36873;&#25321;&#22823;&#35268;&#27169;&#24320;&#28304;&#25968;&#25454;&#38598;&#20013;&#30340;&#31934;&#36873;&#26679;&#26412;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#25163;&#21160;&#31579;&#36873;&#21644;&#28508;&#22312;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#65288;IFD&#65289;&#25351;&#26631;&#65292;&#23427;&#25104;&#20026;&#20102;&#19968;&#20010;&#20915;&#23450;&#24615;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;&#27169;&#22411;&#26399;&#26395;&#21709;&#24212;&#21644;&#33258;&#20027;&#29983;&#25104;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#28789;&#27963;&#24212;&#29992;IFD&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#31934;&#36873;&#26679;&#26412;&#65292;&#20174;&#32780;&#22823;&#24133;&#25552;&#21319;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;Alpaca&#21644;WizardLM&#31561;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#39564;&#35777;&#25903;&#25345;&#25105;&#20204;&#30340;&#21457;&#29616;&#65307;&#20165;&#20351;&#29992;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;10%&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33258;&#25105;&#24341;&#23548;&#25361;&#36873;&#21644;IFD&#25351;&#26631;&#30340;&#32508;&#21512;&#24847;&#21619;&#30528;LLM&#20248;&#21270;&#30340;&#19968;&#20010;&#21464;&#38761;&#24615;&#39134;&#36291;&#65292;&#26377;&#26395;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#26041;&#24335;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22914;&#20309;&#24341;&#29992;&#29992;&#25143;&#20043;&#21069;&#30340;&#23545;&#35805;&#65292;&#24182;&#21457;&#29616;&#36880;&#23383;&#24341;&#29992;&#21644;&#37322;&#20041;&#24341;&#29992;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#26234;&#33021;&#21644;&#24341;&#20154;&#20837;&#32988;&#65292;&#20294;&#36880;&#23383;&#24341;&#29992;&#20250;&#24341;&#36215;&#38544;&#31169;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2308.04879</link><description>&lt;p&gt;
&#27604;&#36739;&#32842;&#22825;&#26426;&#22120;&#20154;&#22914;&#20309;&#24341;&#29992;&#29992;&#25143;&#20043;&#21069;&#32842;&#22825;&#20250;&#35805;&#30340;&#30740;&#31350;&#65306;&#23545;&#29992;&#25143;&#38544;&#31169;&#20851;&#27880;&#21644;&#24863;&#30693;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Comparing How a Chatbot References User Utterances from Previous Chatting Sessions: An Investigation of Users' Privacy Concerns and Perceptions. (arXiv:2308.04879v1 [cs.HC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#26041;&#24335;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22914;&#20309;&#24341;&#29992;&#29992;&#25143;&#20043;&#21069;&#30340;&#23545;&#35805;&#65292;&#24182;&#21457;&#29616;&#36880;&#23383;&#24341;&#29992;&#21644;&#37322;&#20041;&#24341;&#29992;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#26234;&#33021;&#21644;&#24341;&#20154;&#20837;&#32988;&#65292;&#20294;&#36880;&#23383;&#24341;&#29992;&#20250;&#24341;&#36215;&#38544;&#31169;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#35760;&#24518;&#21644;&#24341;&#29992;&#20043;&#21069;&#30340;&#23545;&#35805;&#65292;&#20294;&#36825;&#26159;&#21542;&#22686;&#24378;&#20102;&#29992;&#25143;&#30340;&#21442;&#19982;&#24230;&#36824;&#26159;&#20405;&#29359;&#20102;&#38544;&#31169;&#65311;&#20026;&#20102;&#25506;&#35752;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#22914;&#20309;&#24341;&#29992;&#29992;&#25143;&#20043;&#21069;&#23545;&#35805;&#30340;&#24418;&#24335;&#65292;&#20197;&#21450;&#23545;&#29992;&#25143;&#30340;&#24863;&#30693;&#21644;&#38544;&#31169;&#20851;&#27880;&#30340;&#24433;&#21709;&#12290;&#22312;&#20026;&#26399;&#19977;&#21608;&#30340;&#32437;&#21521;&#38543;&#26426;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35753;169&#21517;&#21442;&#19982;&#32773;&#21521;&#19968;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#35848;&#35770;&#20182;&#20204;&#30340;&#20351;&#29992;&#29273;&#32447;&#30340;&#20064;&#24815;&#65292;&#24182;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#20998;&#20026;&#19977;&#32452;&#65306;(1-None)&#65306;&#19981;&#26126;&#30830;&#24341;&#29992;&#20197;&#21069;&#29992;&#25143;&#30340;&#35805;&#35821;&#65292;(2-Verbatim)&#65306;&#36880;&#23383;&#24341;&#29992;&#20197;&#21069;&#30340;&#35805;&#35821;&#65292;(3-Paraphrase)&#65306;&#20351;&#29992;&#37322;&#20041;&#26469;&#24341;&#29992;&#20197;&#21069;&#30340;&#35805;&#35821;&#12290;&#21442;&#19982;&#32773;&#35748;&#20026;&#36880;&#23383;&#24341;&#29992;&#21644;&#37322;&#20041;&#24341;&#29992;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#26234;&#33021;&#21644;&#24341;&#20154;&#20837;&#32988;&#12290;&#28982;&#32780;&#65292;&#36880;&#23383;&#24341;&#29992;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20063;&#24341;&#36215;&#20102;&#21442;&#19982;&#32773;&#30340;&#38544;&#31169;&#20851;&#27880;&#12290;&#20026;&#20102;&#20102;&#35299;&#20026;&#20160;&#20040;&#20154;&#20204;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#26465;&#20214;&#25110;&#25285;&#24515;&#38544;&#31169;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65292;&#20849;&#26377;15&#21517;&#21442;&#19982;&#32773;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots are capable of remembering and referencing previous conversations, but does this enhance user engagement or infringe on privacy? To explore this trade-off, we investigated the format of how a chatbot references previous conversations with a user and its effects on a user's perceptions and privacy concerns. In a three-week longitudinal between-subjects study, 169 participants talked about their dental flossing habits to a chatbot that either, (1-None): did not explicitly reference previous user utterances, (2-Verbatim): referenced previous utterances verbatim, or (3-Paraphrase): used paraphrases to reference previous utterances. Participants perceived Verbatim and Paraphrase chatbots as more intelligent and engaging. However, the Verbatim chatbot also raised privacy concerns with participants. To gain insights as to why people prefer certain conditions or had privacy concerns, we conducted semi-structured interviews with 15 participants. We discuss implications from our finding
&lt;/p&gt;</description></item><item><title>ValiTex&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#23427;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20256;&#32479;&#65292;&#36890;&#36807;&#27010;&#24565;&#27169;&#22411;&#21644;&#21160;&#24577;&#26816;&#26597;&#34920;&#25552;&#20379;&#20102;&#39564;&#35777;&#30340;&#32467;&#26500;&#21644;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2307.02863</link><description>&lt;p&gt;
ValiTex -- &#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#24230;&#37327;&#30340;&#32479;&#19968;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ValiTex -- a uniform validation framework for computational text-based measures of social science constructs. (arXiv:2307.02863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02863
&lt;/p&gt;
&lt;p&gt;
ValiTex&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#23427;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20256;&#32479;&#65292;&#36890;&#36807;&#27010;&#24565;&#27169;&#22411;&#21644;&#21160;&#24577;&#26816;&#26597;&#34920;&#25552;&#20379;&#20102;&#39564;&#35777;&#30340;&#32467;&#26500;&#21644;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22914;&#20309;&#39564;&#35777;&#35745;&#31639;&#25991;&#26412;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#24230;&#37327;&#30340;&#25351;&#23548;&#26159;&#20998;&#25955;&#30340;&#12290;&#34429;&#28982;&#23398;&#32773;&#20204;&#26222;&#36941;&#35748;&#35782;&#21040;&#39564;&#35777;&#20182;&#20204;&#30340;&#25991;&#26412;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#20182;&#20204;&#36890;&#24120;&#32570;&#20047;&#20849;&#21516;&#30340;&#26415;&#35821;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ValiTex&#30340;&#26032;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#35813;&#26694;&#26550;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#20256;&#32479;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#26694;&#26550;&#20197;&#36866;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#20998;&#26512;&#30340;&#30446;&#30340;&#12290;ValiTex&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#19968;&#20010;&#26159;&#27010;&#24565;&#27169;&#22411;&#65292;&#19968;&#20010;&#26159;&#21160;&#24577;&#26816;&#26597;&#34920;&#12290;&#27010;&#24565;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#25351;&#23548;&#39564;&#35777;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#21160;&#24577;&#26816;&#26597;&#34920;&#23450;&#20041;&#20102;&#20855;&#20307;&#30340;&#39564;&#35777;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#20102;&#21738;&#20123;&#27493;&#39588;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#25512;&#33616;&#30340;&#65288;&#21363;&#25552;&#20379;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#39564;&#35777;&#35777;&#25454;&#65289;&#25110;&#21487;&#36873;&#30340;&#65288;&#21363;&#23545;&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#26377;&#29992;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Guidance on how to validate computational text-based measures of social science constructs is fragmented. Whereas scholars are generally acknowledging the importance of validating their text-based measures, they often lack common terminology and a unified framework to do so. This paper introduces a new validation framework called ValiTex, designed to assist scholars to measure social science constructs based on textual data. The framework draws on a long-established tradition within psychometrics while extending the framework for the purpose of computational text analysis. ValiTex consists of two components, a conceptual model, and a dynamic checklist. Whereas the conceptual model provides a general structure along distinct phases on how to approach validation, the dynamic checklist defines specific validation steps and provides guidance on which steps might be considered recommendable (i.e., providing relevant and necessary validation evidence) or optional (i.e., useful for providing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#27169;&#22411;&#30340;&#26465;&#20214;&#29983;&#25104;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#32467;&#21512;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#22120;&#37096;&#20998;&#20351;&#29992;&#23436;&#25972;&#30340;Transformer&#27169;&#22411;&#29983;&#25104;&#31572;&#26696;&#65292;&#37492;&#21035;&#22120;&#37096;&#20998;&#20165;&#20351;&#29992;Transformer&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24207;&#21015;&#27169;&#22411;&#22312;&#29983;&#25104;&#32842;&#22825;&#22238;&#31572;&#26102;&#20934;&#30830;&#24615;&#19981;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.02074</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#27169;&#22411;&#30340;&#26465;&#20214;&#29983;&#25104;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
A Conditional Generative Chatbot using Transformer Model. (arXiv:2306.02074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#27169;&#22411;&#30340;&#26465;&#20214;&#29983;&#25104;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#32467;&#21512;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#22120;&#37096;&#20998;&#20351;&#29992;&#23436;&#25972;&#30340;Transformer&#27169;&#22411;&#29983;&#25104;&#31572;&#26696;&#65292;&#37492;&#21035;&#22120;&#37096;&#20998;&#20165;&#20351;&#29992;Transformer&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24207;&#21015;&#27169;&#22411;&#22312;&#29983;&#25104;&#32842;&#22825;&#22238;&#31572;&#26102;&#20934;&#30830;&#24615;&#19981;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#20154;&#31867;&#29992;&#25143;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#27807;&#36890;&#24037;&#20855;&#65292;&#26088;&#22312;&#22522;&#20110;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#36866;&#24403;&#30340;&#22238;&#31572;&#12290;&#22312;&#26368;&#36817;&#30340;&#26041;&#27861;&#20013;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24207;&#21015;&#27169;&#22411;&#26469;&#26500;&#24314;&#29983;&#25104;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#23427;&#20204;&#30340;&#24207;&#21015;&#24615;&#36136;&#65292;&#23548;&#33268;&#32467;&#26524;&#19981;&#22815;&#20934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#26465;&#20214;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;&#34429;&#28982;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#29983;&#25104;&#22120;&#30001;&#23436;&#25972;&#30340;Transformer&#27169;&#22411;&#32452;&#25104;&#20197;&#29983;&#25104;&#31572;&#26696;&#65292;&#20294;&#37492;&#21035;&#22120;&#20165;&#21253;&#25324;Transformer&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#65292;&#21518;&#36319;&#19968;&#20010;&#20998;&#31867;&#22120;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#20351;&#29992;&#23884;&#20837;&#24335;Transformer&#27169;&#22411;&#26469;&#25552;&#20986;&#29983;&#25104;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20854;&#21516;&#26102;&#22312;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#20381;&#38752;Transformer&#27169;&#22411;&#30340;&#24182;&#34892;&#35745;&#31639;&#65292;&#32467;&#26524;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Chatbot serves as a communication tool between a human user and a machine to achieve an appropriate answer based on the human input. In more recent approaches, a combination of Natural Language Processing and sequential models are used to build a generative Chatbot. The main challenge of these models is their sequential nature, which leads to less accurate results. To tackle this challenge, in this paper, a novel architecture is proposed using conditional Wasserstein Generative Adversarial Networks and a transformer model for answer generation in Chatbots. While the generator of the proposed model consists of a full transformer model to generate an answer, the discriminator includes only the encoder part of a transformer model followed by a classifier. To the best of our knowledge, this is the first time that a generative Chatbot is proposed using the embedded transformer in both generator and discriminator models. Relying on the parallel computing of the transformer model, the resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36861;&#36394;&#23454;&#20307;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#32463;&#36807;&#22823;&#37327;&#20195;&#30721;&#39044;&#35757;&#32451;&#30340;GPT-3.5&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#21363;&#20351;&#35757;&#32451;&#21644;&#35780;&#20272;&#20013;&#20960;&#20046;&#27809;&#26377;&#35789;&#27719;&#37325;&#21472;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02363</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Entity Tracking in Language Models. (arXiv:2305.02363v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36861;&#36394;&#23454;&#20307;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#32463;&#36807;&#22823;&#37327;&#20195;&#30721;&#39044;&#35757;&#32451;&#30340;GPT-3.5&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#21363;&#20351;&#35757;&#32451;&#21644;&#35780;&#20272;&#20013;&#20960;&#20046;&#27809;&#26377;&#35789;&#27719;&#37325;&#21472;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36861;&#36394;&#25805;&#20316;&#23545;&#35937;&#30340;&#29366;&#24577;&#24182;&#36319;&#36394;&#23427;&#20204;&#38543;&#25991;&#26412;&#25110;&#23545;&#35805;&#30340;&#23637;&#24320;&#32780;&#21457;&#29983;&#30340;&#20851;&#31995;&#21464;&#21270;&#26159;&#29702;&#35299;&#35805;&#35821;&#30340;&#20851;&#38190;&#21069;&#25552;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36861;&#36394;&#35805;&#35821;&#23454;&#20307;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#24456;&#23569;&#30340;&#31995;&#32479;&#35843;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20219;&#21153;&#65292;&#20197;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;&#21021;&#22987;&#29366;&#24577;&#30340;&#33521;&#25991;&#25551;&#36848;&#21644;&#19968;&#31995;&#21015;&#29366;&#24577;&#26356;&#25913;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25512;&#26029;&#20986;&#23454;&#20307;&#30340;&#26368;&#32456;&#29366;&#24577;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keeping track of how states and relations of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Despite this fact, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task to probe to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even for splits with almost no lexical overlap between training and ev
&lt;/p&gt;</description></item><item><title>MQAG&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#36873;&#39064;&#31572;&#26696;&#29983;&#25104;&#21644;&#22238;&#31572;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#25688;&#35201;&#20013;&#30340;&#20449;&#24687;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#39044;&#27979;&#30340;&#31572;&#26696;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#36817;&#20284;&#21028;&#26029;&#28304;&#25991;&#26723;&#21644;&#25688;&#35201;&#20043;&#38388;&#30340;&#20449;&#24687;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12307</link><description>&lt;p&gt;
MQAG: &#29992;&#20110;&#35780;&#20272;&#25688;&#35201;&#20013;&#20449;&#24687;&#19968;&#33268;&#24615;&#30340;&#22810;&#36873;&#39064;&#31572;&#26696;&#29983;&#25104;&#21644;&#22238;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency in Summarization. (arXiv:2301.12307v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12307
&lt;/p&gt;
&lt;p&gt;
MQAG&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#36873;&#39064;&#31572;&#26696;&#29983;&#25104;&#21644;&#22238;&#31572;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#25688;&#35201;&#20013;&#30340;&#20449;&#24687;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#39044;&#27979;&#30340;&#31572;&#26696;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#36817;&#20284;&#21028;&#26029;&#28304;&#25991;&#26723;&#21644;&#25688;&#35201;&#20043;&#38388;&#30340;&#20449;&#24687;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#38750;&#24120;&#27969;&#30021;&#30340;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25688;&#35201;&#21487;&#33021;&#21253;&#21547;&#20107;&#23454;&#19978;&#30340;&#19981;&#19968;&#33268;&#25110;&#28304;&#25991;&#26723;&#20013;&#19981;&#23384;&#22312;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#25688;&#35201;&#36136;&#37327;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#30830;&#23450;&#28304;&#25991;&#26723;&#21644;&#25688;&#35201;&#20043;&#38388;&#30340;&#20449;&#24687;&#19968;&#33268;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#35789;&#27719;&#21305;&#37197;&#25110;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#30452;&#25509;&#27604;&#36739;&#28304;&#25991;&#26723;&#21644;&#25688;&#35201;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#36873;&#39064;&#31572;&#26696;&#29983;&#25104;&#21644;&#22238;&#31572;&#26694;&#26550;MQAG&#65292;&#36890;&#36807;&#35745;&#31639;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#36873;&#39064;&#19978;&#25688;&#35201;&#21644;&#28304;&#25991;&#26723;&#31572;&#26696;&#20998;&#24067;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#36817;&#20284;&#20449;&#24687;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#36873;&#39064;&#31572;&#26696;&#27010;&#29575;&#65292;&#22240;&#20026;&#39044;&#27979;&#30340;&#31572;&#26696;&#20998;&#24067;&#21487;&#20197;&#20849;&#21516;&#39044;&#27979;&#25688;&#35201;&#21644;&#28304;&#25991;&#26723;&#30340;&#20449;&#24687;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art summarization systems can generate highly fluent summaries. These summaries, however, may contain factual inconsistencies and/or information not present in the source. Hence, an important component of assessing the quality of summaries is to determine whether there is information consistency between the source and the summary. Existing approaches are typically based on lexical matching or representation-based methods. In this work, we introduce an alternative scheme based on standard information-theoretic measures in which the information present in the source and summary is directly compared. We propose a Multiple-choice Question Answering and Generation framework, MQAG, which approximates the information consistency by computing the expected statistical distance between summary and source answer distributions over automatically generated multiple-choice questions. This approach exploits multiple-choice answer probabilities, as predicted answer distributions can be co
&lt;/p&gt;</description></item><item><title>TikTalk&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#26234;&#33021;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#38386;&#32842;&#26426;&#22120;&#20154;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;&#27969;&#34892;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#30340;38K&#20010;&#35270;&#39057;&#21644;367K&#20010;&#29992;&#25143;&#23545;&#35805;&#12290;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#20026;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.05880</link><description>&lt;p&gt;
TikTalk: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#30495;&#23454;&#19990;&#30028;&#38386;&#32842;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World. (arXiv:2301.05880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05880
&lt;/p&gt;
&lt;p&gt;
TikTalk&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#26234;&#33021;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#38386;&#32842;&#26426;&#22120;&#20154;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;&#27969;&#34892;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#30340;38K&#20010;&#35270;&#39057;&#21644;367K&#20010;&#29992;&#25143;&#23545;&#35805;&#12290;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#20026;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20013;&#26234;&#33021;&#21644;&#20154;&#31867;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;TikTalk&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#27969;&#34892;&#30340;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#20102;38K&#20010;&#35270;&#39057;&#65292;&#20197;&#21450;&#29992;&#25143;&#22312;&#20854;&#19979;&#21457;&#24067;&#30340;367K&#20010;&#23545;&#35805;&#12290;&#29992;&#25143;&#26681;&#25454;&#20182;&#20204;&#35266;&#30475;&#35270;&#39057;&#26102;&#30340;&#22810;&#27169;&#24577;&#32463;&#39564;&#36827;&#34892;&#33258;&#21457;&#24615;&#23545;&#35805;&#65292;&#36825;&#26377;&#21161;&#20110;&#37325;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#38386;&#32842;&#29615;&#22659;&#12290;&#19982;&#20043;&#21069;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#20013;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#23548;&#33268;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#25429;&#25417;&#20154;&#31867;&#20852;&#36259;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#12290;&#36825;&#20123;&#20107;&#23454;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#23637;&#31034;&#20102;TikTalk&#30340;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#38386;&#32842;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#23545;&#35805;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To facilitate the research on intelligent and human-like chatbots with multi-modal context, we introduce a new video-based multi-modal dialogue dataset, called TikTalk. We collect 38K videos from a popular video-sharing platform, along with 367K conversations posted by users beneath them. Users engage in spontaneous conversations based on their multi-modal experiences from watching videos, which helps recreate real-world chitchat context. Compared to previous multi-modal dialogue datasets, the richer context types in TikTalk lead to more diverse conversations, but also increase the difficulty in capturing human interests from intricate multi-modal information to generate personalized responses. Moreover, external knowledge is more frequently evoked in our dataset. These facts reveal new challenges for multi-modal dialogue models. We quantitatively demonstrate the characteristics of TikTalk, propose a video-based multi-modal chitchat task, and evaluate several dialogue baselines. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#31227;&#38500;&#22797;&#26434;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20165;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#38388;&#25509;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#31687;&#31456;&#20998;&#26512;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#36731;&#37327;&#32423;&#32467;&#26500;&#21482;&#26377;&#20004;&#20010;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#21364;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25317;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#21644;&#22788;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2210.09537</link><description>&lt;p&gt;
Less is More: &#29992;&#20110;&#31687;&#31456;&#20998;&#26512;&#30340;&#36731;&#37327;&#32423;&#21644;&#40065;&#26834;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Less is More: A Lightweight and Robust Neural Architecture for Discourse Parsing. (arXiv:2210.09537v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#31227;&#38500;&#22797;&#26434;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20165;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#38388;&#25509;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#31687;&#31456;&#20998;&#26512;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#36731;&#37327;&#32423;&#32467;&#26500;&#21482;&#26377;&#20004;&#20010;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#21364;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25317;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#21644;&#22788;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#34987;&#24191;&#27867;&#29992;&#20110;&#26500;&#24314;&#25991;&#26412;&#34920;&#31034;&#65292;&#28982;&#32780;&#36825;&#20123;&#22797;&#26434;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#23545;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19968;&#20123;&#31687;&#31456;&#20998;&#26512;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#24615;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#31227;&#38500;&#20102;&#22810;&#20010;&#22797;&#26434;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21482;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#38388;&#25509;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#20445;&#30041;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#20110;&#19977;&#20010;&#24120;&#35265;&#30340;&#31687;&#31456;&#20998;&#26512;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#26368;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#32467;&#26500;&#20165;&#21253;&#21547;&#20004;&#20010;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#22312;&#36739;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#21644;&#36739;&#23569;&#30340;&#22788;&#29702;&#26102;&#38388;&#19979;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#29978;&#33267;&#26356;&#22909;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex feature extractors are widely employed for text representation building. However, these complex feature extractors make the NLP systems prone to overfitting especially when the downstream training datasets are relatively small, which is the case for several discourse parsing tasks. Thus, we propose an alternative lightweight neural architecture that removes multiple complex feature extractors and only utilizes learnable self-attention modules to indirectly exploit pretrained neural language models, in order to maximally preserve the generalizability of pre-trained language models. Experiments on three common discourse parsing tasks show that powered by recent pretrained language models, the lightweight architecture consisting of only two self-attention layers obtains much better generalizability and robustness. Meanwhile, it achieves comparable or even better system performance with fewer learnable parameters and less processing time.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#30740;&#31350;&#25991;&#26412;&#24418;&#24335;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#21333;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;Char BiLSTM&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2204.08975</link><description>&lt;p&gt;
&#26816;&#27979;&#25991;&#26412;&#24418;&#24335;&#24615;&#65306;&#19968;&#39033;&#20851;&#20110;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Detecting Text Formality: A Study of Text Classification Approaches. (arXiv:2204.08975v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08975
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#30740;&#31350;&#25991;&#26412;&#24418;&#24335;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#21333;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;Char BiLSTM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24335;&#24615;&#26159;&#25991;&#26412;&#25991;&#26723;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#12290;&#23545;&#25991;&#26412;&#24418;&#24335;&#24615;&#27700;&#24179;&#30340;&#33258;&#21160;&#26816;&#27979;&#23545;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26377;&#28508;&#22312;&#22909;&#22788;&#12290;&#20043;&#21069;&#65292;&#20026;&#22810;&#31181;&#35821;&#35328;&#24341;&#20837;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#24418;&#24335;&#24615;&#26631;&#27880;&#8212;&#8212;GYAFC&#21644;X-FORMAL&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#29992;&#20110;&#35757;&#32451;&#39118;&#26684;&#36716;&#31227;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#25991;&#26412;&#24418;&#24335;&#24615;&#30340;&#21333;&#29420;&#26816;&#27979;&#20063;&#21487;&#33021;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#24418;&#24335;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#32479;&#35745;&#12289;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;Transformer&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#24615;&#33021;&#26368;&#20339;&#30340;&#27169;&#22411;&#20379;&#20844;&#20247;&#20351;&#29992;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#23454;&#39564;&#8212;&#8212;&#21333;&#35821;&#35328;&#12289;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21333;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#24418;&#24335;&#24615;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;Char BiLSTM&#27169;&#22411;&#20248;&#20110;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formality is one of the important characteristics of text documents. The automatic detection of the formality level of a text is potentially beneficial for various natural language processing tasks. Before, two large-scale datasets were introduced for multiple languages featuring formality annotation -- GYAFC and X-FORMAL. However, they were primarily used for the training of style transfer models. At the same time, the detection of text formality on its own may also be a useful application. This work proposes the first to our knowledge systematic study of formality detection methods based on statistical, neural-based, and Transformer-based machine learning methods and delivers the best-performing models for public usage. We conducted three types of experiments -- monolingual, multilingual, and cross-lingual. The study shows the overcome of Char BiLSTM model over Transformer-based ones for the monolingual and multilingual formality classification task, while Transformer-based classifie
&lt;/p&gt;</description></item></channel></rss>