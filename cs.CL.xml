<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#26126;&#30830;&#23545;&#40784;&#35821;&#35328;&#20043;&#38388;&#30340;&#27010;&#24565;&#23545;&#24212;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20197;&#24378;&#21270;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#20165;&#26377;&#32534;&#30721;&#22120;&#36824;&#26159;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#21508;&#35821;&#35328;&#20869;&#30340;&#32467;&#26500;&#27010;&#24565;&#31354;&#38388;&#23545;&#40784;&#24230;&#39640;&#12290;&#36890;&#36807;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#23545;&#40784;&#19981;&#21516;&#35821;&#35328;&#30340;&#27010;&#24565;&#31354;&#38388;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.12794</link><description>&lt;p&gt;
&#32467;&#26500;&#27010;&#24565;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#20855;&#26377;&#26222;&#36866;&#24615;&#65311;&#36208;&#21521;&#21487;&#35299;&#37322;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization. (arXiv:2310.12794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#26126;&#30830;&#23545;&#40784;&#35821;&#35328;&#20043;&#38388;&#30340;&#27010;&#24565;&#23545;&#24212;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20197;&#24378;&#21270;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#20165;&#26377;&#32534;&#30721;&#22120;&#36824;&#26159;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#21508;&#35821;&#35328;&#20869;&#30340;&#32467;&#26500;&#27010;&#24565;&#31354;&#38388;&#23545;&#40784;&#24230;&#39640;&#12290;&#36890;&#36807;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#23545;&#40784;&#19981;&#21516;&#35821;&#35328;&#30340;&#27010;&#24565;&#31354;&#38388;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#23427;&#20204;&#36890;&#36807;&#38544;&#24335;&#30693;&#35782;&#20256;&#36755;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36716;&#31227;&#23545;&#20110;&#25152;&#26377;&#35821;&#35328;&#32780;&#35328;&#24182;&#19981;&#22343;&#34913;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65292;&#36825;&#26159;&#19968;&#20010;&#25345;&#32493;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#25105;&#20204;&#26159;&#21542;&#24050;&#32463;&#36798;&#21040;&#20102;&#38544;&#24335;&#36328;&#35821;&#35328;&#27867;&#21270;&#30340;&#26497;&#38480;&#65292;&#24182;&#19988;&#26126;&#30830;&#30340;&#30693;&#35782;&#20256;&#36755;&#26159;&#21542;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26126;&#30830;&#23545;&#40784;&#35821;&#35328;&#20043;&#38388;&#27010;&#24565;&#23545;&#24212;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#35821;&#27861;&#26041;&#38754;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#25105;&#20204;&#23545;43&#31181;&#35821;&#35328;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#26080;&#35770;&#26159;&#20165;&#26377;&#32534;&#30721;&#22120;&#36824;&#26159;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;LLMs&#65292;&#21508;&#31181;&#35821;&#35328;&#20869;&#30340;&#32467;&#26500;&#27010;&#24565;&#31354;&#38388;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#30340;&#23545;&#20934;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#19981;&#21516;&#35821;&#35328;&#30340;&#27010;&#24565;&#31354;&#38388;&#65292;&#20174;&#32780;&#20415;&#20110;&#22312;&#27010;&#24565;&#20998;&#31867;&#21644;&#23545;&#40784;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited considerable cross-lingual generalization abilities, whereby they implicitly transfer knowledge across languages. However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge. It is unclear whether we have reached the limits of implicit cross-lingual generalization and if explicit knowledge transfer is viable. In this paper, we investigate the potential for explicitly aligning conceptual correspondence between languages to enhance cross-lingual generalization. Using the syntactic aspect of language as a testbed, our analyses of 43 languages reveal a high degree of alignability among the spaces of structural concepts within each language for both encoder-only and decoder-only LLMs. We then propose a meta-learning-based method to learn to align conceptual spaces of different languages, which facilitates zero-shot and few-shot generalization in concept classification and al
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#25216;&#26415;&#35282;&#24230;&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#30340;&#19979;&#19968;&#27493;&#24037;&#20316;&#65292;&#21253;&#25324;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12289;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#21644;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15774</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19979;&#19968;&#27493;&#65306;&#25216;&#26415;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#25216;&#26415;&#35282;&#24230;&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#30340;&#19979;&#19968;&#27493;&#24037;&#20316;&#65292;&#21253;&#25324;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12289;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#21644;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#22797;&#36328;&#23398;&#31185;&#35752;&#35770;&#65292;&#25105;&#20204;&#20174;&#25216;&#26415;&#35282;&#24230;&#20026;&#20154;&#31867;&#20013;&#24515;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(HGAI)&#23450;&#20041;&#21644;&#25552;&#20986;&#20102;&#19979;&#19968;&#27493;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#36335;&#32447;&#22270;&#65292;&#27010;&#36848;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19977;&#20010;&#23618;&#38754;&#19978;&#30340;&#26410;&#26469;&#26041;&#21521;&#65306;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65307;&#36866;&#24212;&#20154;&#31867;&#30340;&#24847;&#22270;&#34920;&#36798;&#65307;&#22686;&#24378;&#20154;&#31867;&#22312;&#21327;&#20316;&#24037;&#20316;&#27969;&#20013;&#30340;&#33021;&#21147;&#12290;&#35813;&#36335;&#32447;&#22270;&#26088;&#22312;&#21560;&#24341;&#36328;&#23398;&#31185;&#30740;&#31350;&#22242;&#38431;&#23545;HGAI&#30340;&#26032;&#20852;&#24819;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#35752;&#35770;&#65292;&#21516;&#26102;&#20445;&#25345;&#26410;&#26469;&#24037;&#20316;&#26223;&#35266;&#30340;&#25972;&#20307;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through iterative, cross-disciplinary discussions, we define and propose next-steps for Human-centered Generative AI (HGAI) from a technical perspective. We contribute a roadmap that lays out future directions of Generative AI spanning three levels: Aligning with human values; Accommodating humans' expression of intents; and Augmenting humans' abilities in a collaborative workflow. This roadmap intends to draw interdisciplinary research teams to a comprehensive list of emergent ideas in HGAI, identifying their interested topics while maintaining a coherent big picture of the future work landscape.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#25506;&#27979;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#31034;&#26469;&#39044;&#27979;&#26631;&#31614;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#25351;&#20196;&#21464;&#21270;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#19988;&#22312;&#22810;&#26679;&#21270;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14171</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#27979;&#65306;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26500;&#24314;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Probing in Context: Toward Building Robust Classifiers via Probing Large Language Models. (arXiv:2305.14171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#25506;&#27979;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#31034;&#26469;&#39044;&#27979;&#26631;&#31614;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#25351;&#20196;&#21464;&#21270;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#19988;&#22312;&#22810;&#26679;&#21270;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#22312;&#25552;&#20379;&#25351;&#20196;&#21644;&#23569;&#37327;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#21462;&#20915;&#20110;&#25351;&#20196;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#23545;&#19978;&#19979;&#25991;&#30340;&#20381;&#36182;&#21487;&#33021;&#20197;&#19981;&#21487;&#39044;&#27979;&#30340;&#26041;&#24335;&#34920;&#29616;&#65292;&#20363;&#22914;&#65292;&#19968;&#20010;&#30475;&#20284;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#25351;&#20196;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#26356;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#27979;&#12290;&#31867;&#20284;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#25105;&#20204;&#29992;&#25351;&#20196;&#23545;&#36755;&#20837;&#30340;&#34920;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#65292;&#20294;&#26159;&#25105;&#20204;&#19981;&#26159;&#35299;&#30721;&#36755;&#20986;&#39044;&#27979;&#65292;&#32780;&#26159;&#25506;&#27979;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#31034;&#26469;&#39044;&#27979;&#26631;&#31614;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#22312;&#22810;&#26679;&#21270;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#27979;&#23545;&#25351;&#20196;&#21464;&#21270;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25506;&#27979;&#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#31454;&#20105;&#25110;&#26356;&#32988;&#19968;&#31609;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are able to learn new tasks in context, where they are provided with instructions and a few annotated examples. However, the effectiveness of in-context learning is dependent on the provided context, and the performance on a downstream task can vary considerably, depending on the instruction. Importantly, such dependency on the context can surface in unpredictable ways, e.g., a seemingly more informative instruction might lead to a worse performance. In this paper, we propose an alternative approach, which we term in-context probing. Similar to in-context learning, we contextualize the representation of the input with an instruction, but instead of decoding the output prediction, we probe the contextualized representation to predict the label. Through a series of experiments on a diverse set of classification tasks, we show that in-context probing is significantly more robust to changes in instructions. We further show that probing performs competitive or superior
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#39046;&#22495;&#20851;&#38190;&#35789;&#29983;&#25104;&#26041;&#38754;&#12290;ChatGPT&#20173;&#38754;&#20020;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.13001</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#27454;&#22909;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT A Good Keyphrase Generator? A Preliminary Study. (arXiv:2303.13001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#39046;&#22495;&#20851;&#38190;&#35789;&#29983;&#25104;&#26041;&#38754;&#12290;ChatGPT&#20173;&#38754;&#20020;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#37325;&#35270;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#20316;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#20197;&#29992;&#20110;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20854;&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20851;&#38190;&#35789;&#29983;&#25104;&#25552;&#31034;&#65292;&#20851;&#38190;&#35789;&#29983;&#25104;&#22810;&#26679;&#24615;&#65292;&#22810;&#39046;&#22495;&#20851;&#38190;&#35789;&#29983;&#25104;&#21644;&#38271;&#25991;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20110;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;OpenAI&#24314;&#35758;&#30340;&#25552;&#31034;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#20026;&#20845;&#20010;&#20505;&#36873;&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#20845;&#20010;&#20505;&#36873;&#25552;&#31034;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#35266;&#23519;&#21040;&#20102;&#36731;&#24494;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;ChatGPT&#26377;&#24456;&#22823;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;ChatGPT&#22312;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#22312;&#26368;&#21518;&#19968;&#33410;&#20013;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20123;&#38480;&#21046;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of ChatGPT has recently garnered significant attention from the computational linguistics community. To demonstrate its capabilities as a keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the keyphrase generation task. We evaluate its performance in various aspects, including keyphrase generation prompts, keyphrase generation diversity, multi-domain keyphrase generation, and long document understanding. Our evaluation is based on six benchmark datasets, and we adopt the prompt suggested by OpenAI while extending it to six candidate prompts. We find that ChatGPT performs exceptionally well on all six candidate prompts, with minor performance differences observed across the datasets. Based on our findings, we conclude that ChatGPT has great potential for keyphrase generation. Moreover, we discover that ChatGPT still faces challenges when it comes to generating absent keyphrases. Meanwhile, in the final section, we also present some limitations and futu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#12289;&#24341;&#23548;&#29983;&#25104;&#21644;&#21322;&#21442;&#25968;&#23494;&#38598;&#26816;&#32034;&#65292;&#21160;&#24577;&#29983;&#25104;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35777;&#26126;&#26641;&#65292;&#23454;&#29616;&#31185;&#23398;&#25512;&#29702;&#65292;&#24182;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.07662</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#19987;&#23478;&#31995;&#32479;&#20013;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#36923;&#36753;&#25512;&#29702;&#30340;&#21160;&#24577;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dynamic Generation of Grounded Logical Explanations in a Neuro-Symbolic Expert System. (arXiv:2209.07662v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#12289;&#24341;&#23548;&#29983;&#25104;&#21644;&#21322;&#21442;&#25968;&#23494;&#38598;&#26816;&#32034;&#65292;&#21160;&#24577;&#29983;&#25104;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35777;&#26126;&#26641;&#65292;&#23454;&#29616;&#31185;&#23398;&#25512;&#29702;&#65292;&#24182;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35777;&#26126;&#26641;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#21457;&#20102;&#32463;&#20856;&#30340;&#22522;&#20110; Prolog &#30340;&#25512;&#29702;&#24341;&#25806;&#65292;&#20854;&#20013;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#12289;&#24341;&#23548;&#29983;&#25104;&#21644;&#21322;&#21442;&#25968;&#23494;&#38598;&#26816;&#32034;&#26469;&#26367;&#25442;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479; NELLIE &#26469;&#28436;&#31034;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#21160;&#24577;&#22320;&#23454;&#20363;&#21270;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#35268;&#21017;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#30340;&#34164;&#21547;&#65288;&#21435;&#65289;&#32452;&#21512;&#36827;&#34892;&#25429;&#25417;&#21644;&#35780;&#20998;&#12290;&#36825;&#23548;&#33268;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#22312;&#31185;&#23398;&#25512;&#29702;&#39046;&#22495;&#23637;&#31034;&#20102;&#22914;&#20309;&#36923;&#36753;&#22320;&#20174;&#32463;&#36807;&#20154;&#24037;&#39564;&#35777;&#30340;&#20107;&#23454;&#30340;&#32452;&#21512;&#20013;&#25512;&#23548;&#20986;&#31572;&#26696;&#30340;&#25512;&#29702;&#30165;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for systematic reasoning that produces human interpretable proof trees grounded in a factbase. Our approach evokes classic Prolog-based inference engines, where we replace handcrafted rules by combining neural language modeling, guided generation, and semiparametric dense retrieval. We demonstrate this approach through a novel system, NELLIE, which dynamically instantiates interpretable inference rules that capture and score entailment (de)compositions over natural language statements. This leads to strong performance, as shown in the scientific reasoning domain, while also producing reasoning traces showing how answers derive logically from the composition of human-verified facts.
&lt;/p&gt;</description></item></channel></rss>